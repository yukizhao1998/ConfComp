Summary,Issue key,Issue id,Issue Type,Status,Project key,Project name,Project type,Project lead,Project description,Project url,Priority,Resolution,Assignee,Reporter,Creator,Created,Updated,Last Viewed,Resolved,Fix Version/s,Fix Version/s,Fix Version/s,Component/s,Component/s,Due Date,Votes,Labels,Labels,Labels,Labels,Description,Environment,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Watchers,Log Work,Log Work,Original Estimate,Remaining Estimate,Time Spent,Work Ratio,Σ Original Estimate,Σ Remaining Estimate,Σ Time Spent,Security Level,Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Inward issue link (Blocker),Outward issue link (Blocker),Inward issue link (Cloners),Inward issue link (Cloners),Outward issue link (Cloners),Inward issue link (Duplicate),Inward issue link (Duplicate),Inward issue link (Duplicate),Outward issue link (Duplicate),Inward issue link (Incorporates),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Inward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Outward issue link (Reference),Inward issue link (Regression),Outward issue link (Regression),Inward issue link (Supercedes),Outward issue link (Supercedes),Inward issue link (dependent),Outward issue link (dependent),Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Attachment,Custom field (Affects version (Component)),Custom field (Attachment count),Custom field (Authors),Custom field (Blog - New Blog Administrators),Custom field (Blog - New Blog PMC),Custom field (Blog - Write access),Custom field (Blog Administrator?),Custom field (Blogs - Admin for blog),Custom field (Blogs - Email Address),Custom field (Blogs - Existing Blog Access Level),Custom field (Blogs - Existing Blog Name),Custom field (Blogs - New Blog Write Access),Custom field (Blogs - Username),Custom field (Bug Category),Custom field (Bugzilla - Email Notification Address),Custom field (Bugzilla - List of usernames),Custom field (Bugzilla - PMC Name),Custom field (Bugzilla - Project Name),Custom field (Bugzilla Id),Custom field (Bugzilla Id),Custom field (Change Category),Custom field (Complexity),Custom field (Discovered By),Custom field (Docs Text),Custom field (Enable Automatic Patch Review),Custom field (Epic Link),Custom field (Estimated Complexity),Custom field (Evidence Of Open Source Adoption),Custom field (Evidence Of Registration),Custom field (Evidence Of Use On World Wide Web),Custom field (Existing GitBox Approval),Custom field (External issue ID),Custom field (External issue URL),Custom field (Fix version (Component)),Custom field (Git Notification Mailing List),Custom field (Git Repository Import Path),Custom field (Git Repository Name),Custom field (Git Repository Type),Custom field (GitHub Options),Custom field (Github Integration),Custom field (Github Integrations - Other),Custom field (Global Rank),Custom field (INFRA - Subversion Repository Path),Custom field (Initial Confluence Contributors),Custom field (Last public comment date),Custom field (Level of effort),Custom field (Machine Readable Info),Custom field (Mentor),Custom field (New-TLP-TLPName),Custom field (Original story points),Custom field (Parent Link),Custom field (Priority),Custom field (Project),Custom field (Protected Branch),Custom field (Rank),Custom field (Rank (Obsolete)),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Reproduced In),Custom field (Review Date),Custom field (Reviewer),Custom field (Reviewer),Custom field (Reviewers),Custom field (Severity),Custom field (Severity),Custom field (Since Version),Custom field (Skill Level),Custom field (Source Control Link),Custom field (Space Description),Custom field (Space Key),Custom field (Space Name),Custom field (Start Date),Custom field (Tags),Custom field (Target end),Custom field (Target start),Custom field (Team),Custom field (Test and Documentation Plan),Custom field (Testcase included),Custom field (Tester),Custom field (Tester),Custom field (Workaround),Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment,Comment
Failure to start after unclean shutdown - java.lang.IllegalArgumentException: bufferSize must be positive,CASSANDRA-6531,12686569,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,ngrigoriev,ngrigoriev,27/Dec/13 20:53,16/Apr/19 09:31,14/Jul/23 05:53,28/Dec/13 06:18,1.2.14,2.0.5,,,,,0,compression,,,,"We had a severe power outage in the lab that resulted in unclean shutdown of the Cassandra servers. After the power was back I tried to start the cluster. Two out of 6 nodes cannot start because of this exception:

{code}
 INFO 20:47:11,003 Initializing system.local
 INFO [main] 2013-12-27 20:47:11,003 ColumnFamilyStore.java (line 251) Initializing system.local
 INFO 20:47:11,006 Opening /hadoop/disk1/cassandra/data/system/local/system-local-jb-2478 (5836 bytes)
 INFO [SSTableBatchOpen:1] 2013-12-27 20:47:11,006 SSTableReader.java (line 223) Opening /hadoop/disk1/cassandra/data/system/local/system-local-jb-2478 (5836 bytes)
 INFO 20:47:11,006 Opening /hadoop/disk4/cassandra/data/system/local/system-local-jb-2479 (144 bytes)
 INFO [SSTableBatchOpen:2] 2013-12-27 20:47:11,006 SSTableReader.java (line 223) Opening /hadoop/disk4/cassandra/data/system/local/system-local-jb-2479 (144 bytes)
ERROR 20:47:12,366 Exception encountered during startup
java.lang.IllegalArgumentException: bufferSize must be positive
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:55)
        at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1363)
        at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
        at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1147)
        at org.apache.cassandra.db.RowIteratorFactory.getIterator(RowIteratorFactory.java:69)
        at org.apache.cassandra.db.ColumnFamilyStore.getSequentialIterator(ColumnFamilyStore.java:1526)
        at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1645)
        at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:137)
        at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:236)
        at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:1)
        at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:255)
        at org.apache.cassandra.db.SystemKeyspace.getUnfinishedCompactions(SystemKeyspace.java:206)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:261)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:461)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)
{code}

Collecting the logs now, will attach to the issue in a moment. ","Cassandra 2.0.3 with patches for CASSANDRA-6496 and CASSANDRA-6284, built from source; Linux; XFS on the data disks; 5 data disks; 6 nodes",mshuler,ngrigoriev,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Dec/13 23:02;jbellis;6531.txt;https://issues.apache.org/jira/secure/attachment/12620674/6531.txt","27/Dec/13 21:04;ngrigoriev;cassandra_jstack.txt;https://issues.apache.org/jira/secure/attachment/12620654/cassandra_jstack.txt","27/Dec/13 21:04;ngrigoriev;system.log.gz;https://issues.apache.org/jira/secure/attachment/12620653/system.log.gz","27/Dec/13 20:55;ngrigoriev;system.log.gz;https://issues.apache.org/jira/secure/attachment/12620651/system.log.gz",,,,,,,,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,365560,,,Sun Dec 29 02:49:47 UTC 2013,,,,,,,,,,"0|i1r0hz:",365868,2.0.3,,,,,,,,xedin,,xedin,Normal,,1.0.0,,,,,,,,,,,,,,,,"27/Dec/13 20:55;ngrigoriev;system log with default logging levels;;;","27/Dec/13 21:04;ngrigoriev;Startup log with default logging level set to TRACE (log4j.rootLogger=TRACE,stdout,R). Interestingly enough I do not see the original exception in the log when I change it to TRACE and Cassandra process does not terminate. So I include the thread dump just in case.;;;","27/Dec/13 23:02;jbellis;Patch to fsync compression metadata on close.  As near as I can tell this is a problem back to the introduction of compression in 1.0 (CASSANDRA-47). /cc [~xedin] [~tjake];;;","28/Dec/13 00:03;ngrigoriev;Do I understand correctly that this patch should eliminate/reduce the possibility of this to happen but won't help to start the node in my case? If so, I am going to try to abuse this JIRA a bit and ask if anyone could recommend a method of bringing this node back to life without wiping it? If such a method exists, of course. Thanks!;;;","28/Dec/13 00:42;jbellis;The only straightforward approach (i.e.: doesn't involve writing code to rebuild the corrupt offsets from the data component) is to remove the affected sstables and repair.;;;","28/Dec/13 01:18;xedin;[~jbellis] I think we should fsync twice in this case - first time whe the header is finalized and second time on close.;;;","28/Dec/13 01:51;jbellis;It's still -tmp at this point so a single sync is sufficient.;;;","28/Dec/13 02:03;ngrigoriev;I believe the log messages were somewhat misleading. After messing with debug logs for a while I have found that the problem was, apparently, not with system KS. I have removed all stuff \*compactions_in_progress\* and that helped one of these two nodes in question to start. Another one seems to have an identity crisis as it believes that another node with the same address has already joined the cluster...but that seems to be another story.;;;","28/Dec/13 02:06;xedin;Ah, I completely forgot about that, sure.;;;","28/Dec/13 06:18;jbellis;committed;;;","28/Dec/13 06:19;jbellis;(Incidentally, the fix for the compactions in progress bug will be committed soon for CASSANDRA-6086.);;;","28/Dec/13 16:25;ngrigoriev;Just one more suggestion for the fix. I think it is important to log the name of the problematic file every time an exception is thrown. This helps to correctly associate the exception with the file. In this case RandomAccessReader.java:67 throws an exception without this information. I have replaced that line with:

{code}
            throw new IllegalArgumentException(""bufferSize must be positive while reading "" + file.getAbsolutePath());
{code};;;","28/Dec/13 17:54;mshuler;committed patch causes:
http://cassci.datastax.com/job/cassandra-2.0_test/76/testReport/junit/org.apache.cassandra.io.compress/CompressedRandomAccessReaderTest/testResetAndTruncateCompressed/;;;","29/Dec/13 02:49;jbellis;Updated close method to be idempotent.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix logback configuration in scripts and debian packaging for trunk/2.1,CASSANDRA-6530,12686456,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mshuler,mshuler,mshuler,26/Dec/13 19:15,16/Apr/19 09:31,14/Jul/23 05:53,10/Jan/14 15:34,2.1 beta1,,,Legacy/Tools,,,0,qa-resolved,,,,,,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Dec/13 16:56;mshuler;logback_configurations_final.patch;https://issues.apache.org/jira/secure/attachment/12620621/logback_configurations_final.patch","10/Jan/14 15:39;mshuler;logback_configurations_final2.patch;https://issues.apache.org/jira/secure/attachment/12622398/logback_configurations_final2.patch",,,,,,,,,,,,,,,,,,,2.0,mshuler,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,365446,,,Fri Jan 10 15:39:21 UTC 2014,,,,,,,,,,"0|i1qzmv:",365747,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"26/Dec/13 19:34;mshuler;There is a log4j config in test/conf/ and a few places that is used - additions to patch to come.;;;","26/Dec/13 19:55;mshuler;Replaced my initial patch with one that includes removal of the test/conf/ log4j configuration and an update of conf/README.txt;;;","26/Dec/13 21:22;mshuler;log4 has -server and -tools configurations. -tools only adds WARN,stderr info to the output of the various tools and it appears that a complete logback change will likely need a new logback-tools.xml to add the same.

Scratch my patch for the moment, since it produces some ugly tool output.;;;","27/Dec/13 17:17;mshuler;logback_configurations_final.patch looks good!
Fixes debian package build for trunk and running through some of the bin/* scripts works fine for me.  I'm not exactly sure how to generate some stderr output to see if logback spits those out properly from tools like nodetool or cqlsh, so that might need some testing.;;;","30/Dec/13 19:02;brandon.williams;Looks like we need to suppress the HSHA disconnect warnings like we did in log4j:

{noformat}
WARN  19:01:51 Got an IOException in internalRead!
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcherImpl.read0(Native Method) ~[na:1.7.0_17]
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:39) ~[na:1.7.0_17]
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:225) ~[na:1.7.0_17]
        at sun.nio.ch.IOUtil.read(IOUtil.java:198) ~[na:1.7.0_17]
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:359) ~[na:1.7.0_17]
        at org.apache.thrift.transport.TNonblockingSocket.read(TNonblockingSocket.java:141) ~[libthrift-0.9.1.jar:0.9.1]
        at com.thinkaurelius.thrift.util.mem.Buffer.readFrom(Buffer.java:96) ~[thrift-server-0.3.2.jar:na]
        at com.thinkaurelius.thrift.Message.internalRead(Message.java:338) [thrift-server-0.3.2.jar:na]
        at com.thinkaurelius.thrift.Message.read(Message.java:141) [thrift-server-0.3.2.jar:na]
        at com.thinkaurelius.thrift.TDisruptorServer$SelectorThread.handleRead(TDisruptorServer.java:521) [thrift-server-0.3.2.jar:na]
        at com.thinkaurelius.thrift.TDisruptorServer$SelectorThread.processKey(TDisruptorServer.java:500) [thrift-server-0.3.2.jar:na]
        at com.thinkaurelius.thrift.TDisruptorServer$AbstractSelectorThread.select(TDisruptorServer.java:375) [thrift-server-0.3.2.jar:na]
        at com.thinkaurelius.thrift.TDisruptorServer$AbstractSelectorThread.run(TDisruptorServer.java:339) [thrift-server-0.3.2.jar:na]
{noformat};;;","01/Jan/14 15:48;mshuler;conf/log4j-server.properties :
{code}
# Adding this to avoid thrift logging disconnect errors.
log4j.logger.org.apache.thrift.server.TNonblockingServer=ERROR
{code};;;","02/Jan/14 23:25;mshuler;This appears to already be set up in the logback config:
{code}
(c169)mshuler@hana:~/git/cassandra$ grep thrift conf/logback.xml 
  <logger name=""org.apache.thrift.server.TNonblockingServer"" level=""ERROR""/>
{code}

Setting rpc_server_type: hsha and running stress and killing it a dozen times or so did not give the error output as above.;;;","03/Jan/14 16:17;brandon.williams;This line is bumping org.apache.thrift.server up to ERROR, but my error never goes through that, it comes from org.apache.thrift.transport.  Changing that line in logback does suppress the warnings for me.;;;","08/Jan/14 16:09;brandon.williams;I was wrong, this doesn't suppress the warnings, they just don't repro 100% of the time.  It seems that logback is ignoring us here, but as far as I can tell we have it configured correctly.;;;","09/Jan/14 19:40;brandon.williams;Looks like logback is picky about how you specify what to ignore, it won't 'drill down' past the top level, and not even down to the exact class.  If I use 'com.thinkaurelius.thrift' that suppresses all the warnings, but 'com.thinkaurelius.thrift.TDisruptorServer' does not.  We don't use anything else in c.t.t so I think using that is fine.;;;","10/Jan/14 15:34;brandon.williams;Committed, with the change I detailed above.;;;","10/Jan/14 15:39;mshuler;logback_configurations_final2.patch rebased and s/o.a.t.s.TNonblockingServer/c.t.t/ - that works for me as well (could swear I tried that  :) );;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader shows no progress or errors when pointed at a bad directory,CASSANDRA-6529,12686445,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,thobbs,thobbs,26/Dec/13 18:34,16/Apr/19 09:31,14/Jul/23 05:53,07/Jan/14 05:52,2.0.5,,,Legacy/Tools,,,0,,,,,"With sstableloader, the source directory is supposed to be in the format {{<keyspace_name>/<table_name>/}}.  If you incorrectly just put the sstables in a {{<keyspace_name>/}} directory, the sstableloader process will not show any progress, errors, or other output, it will simply hang.

This was initially reported on the user ML here: http://www.mail-archive.com/user@cassandra.apache.org/msg33916.html",,marcuse,mbulman,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/14 08:02;marcuse;0001-verify-that-the-keyspace-exists-in-describeRing-and-.patch;https://issues.apache.org/jira/secure/attachment/12621275/0001-verify-that-the-keyspace-exists-in-describeRing-and-.patch","02/Jan/14 12:14;marcuse;0001-verify-that-the-keyspace-exists-in-describeRing.patch;https://issues.apache.org/jira/secure/attachment/12621061/0001-verify-that-the-keyspace-exists-in-describeRing.patch",,,,,,,,,,,,,,,,,,,2.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,365435,,,Tue Jan 07 05:52:15 UTC 2014,,,,,,,,,,"0|i1qzkn:",365737,2.0.3,,,,,,,,thobbs,,thobbs,Low,,,,,,,,,,,,,,,,,,"26/Dec/13 18:42;thobbs;Server side, you'll see something like this in the logs:

{noformat}
ERROR 12:28:28,053 Exception in thread Thread[Thrift:1,5,main]
java.lang.AssertionError: Unknown keyspace foo
	at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:262)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
	at org.apache.cassandra.service.StorageService.describeRing(StorageService.java:1181)
	at org.apache.cassandra.service.StorageService.describeRing(StorageService.java:1168)
	at org.apache.cassandra.thrift.CassandraServer.describe_ring(CassandraServer.java:1389)
	at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.getResult(Cassandra.java:4094)
	at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.getResult(Cassandra.java:4078)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{noformat};;;","02/Jan/14 12:14;marcuse;verify that the given keyspace exists in describeRing;;;","02/Jan/14 17:05;thobbs;It would be nice to have an error message instead of a stack trace:

{noformat}
~/cassandra $ bin/sstableloader /foo/bar -d 127.0.0.1
Exception in thread ""main"" java.lang.RuntimeException: Could not retrieve endpoint ranges: 
	at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:239)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:149)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:79)
Caused by: InvalidRequestException(why:No such keyspace: foo)
	at org.apache.cassandra.thrift.Cassandra$describe_ring_result$describe_ring_resultStandardScheme.read(Cassandra.java:34055)
	at org.apache.cassandra.thrift.Cassandra$describe_ring_result$describe_ring_resultStandardScheme.read(Cassandra.java:34022)
	at org.apache.cassandra.thrift.Cassandra$describe_ring_result.read(Cassandra.java:33964)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_describe_ring(Cassandra.java:1251)
	at org.apache.cassandra.thrift.Cassandra$Client.describe_ring(Cassandra.java:1238)
	at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:215)
{noformat}

Can we wrap the describe_ring() call in a try/catch for InvalidRequestException and just print the ""why"" attribute and a brief message that suggests checking the usage?;;;","03/Jan/14 08:02;marcuse;adds a nicer message

we exited with a stack trace for certain errors before (like giving a host without c* running there), the errors wont be as scary now;;;","03/Jan/14 21:40;thobbs;+1, thanks very much for the change.;;;","07/Jan/14 05:52;marcuse;committed!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Random tombstones after adding a CF with sstableloader,CASSANDRA-6527,12686418,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,yukim,br1985,br1985,26/Dec/13 13:58,16/Apr/19 09:31,14/Jul/23 05:53,26/Dec/13 22:01,2.0.4,,,,,,0,,,,,"I've marked this bug as critical since it results in a data loss without any warnings.

Here's the scenario:

- create a fresh one-node cluster with cassandra 1.2.11
- add a sample row:

{code}
CREATE KEYSPACE keyspace1 WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};
use keyspace1;
create table table1 (key text primary key, value1 text);
update table1 set value1 = 'some-value' where key = 'some-key';
{code}

- flush, drain, shutdown the cluster - you should have a single sstable:

{code}
root@l1:~# ls /var/lib/cassandra/data/keyspace1/table1/
keyspace1-table1-ic-1-CompressionInfo.db  
keyspace1-table1-ic-1-Filter.db  
keyspace1-table1-ic-1-Statistics.db  
keyspace1-table1-ic-1-TOC.txt
keyspace1-table1-ic-1-Data.db             
keyspace1-table1-ic-1-Index.db   
keyspace1-table1-ic-1-Summary.db
{code}

with a perfectly correct content:

{code}
root@l1:~# sstable2json /var/lib/cassandra/data/keyspace1/table1/keyspace1-table1-ic-1-Data.db
[
{""key"": ""736f6d652d6b6579"",""columns"": [["""","""",1387822268786000], [""value1"",""some-value"",1387822268786000]]}
]
{code}

- create a new cluster with 2.0.3 (we've used 3 nodes with replication=2, but I guess it doesn't matter)

- copy sstable from the machine in the old cluster to one of the machines in the new cluster (we do not want to use old sstableloader)

- load sstables with sstableloader:

{code}
sstableloader -d 172.16.9.12 keyspace1/table1
{code}

- analyze the content of newly loaded sstable:

{code}
root@l13:~# sstable2json /var/lib/cassandra/data/keyspace1/table1/keyspace1-table1-jb-1-Data.db
[
{""key"": ""736f6d652d6b6579"",""metadata"": {""deletionInfo"": {""markedForDeleteAt"":294205259775,""localDeletionTime"":0}},""columns"": [["""","""",1387824835597000], [""value1"",""some-value"",1387824835597000]]}
]
{code}

There's a random tombstone inserted!

We've hit this bug in production. We never use delete for this column family, but the tombstones appeared for each row. The timestamp looks random. In our case it was mostly in past, but sometimes (about 3% rows) it was in the future. That's even worse than missing a row. In that case you cannot simply add it again - tombstone from the future will hide it.

Fortunately, we have noticed that quickly and canceled the migration. However, we were quite lucky. There are no warnings or errors during the whole process. Losing less than 3% of data may be hard to noticed at first sight for many kind of apps.
",,br1985,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Dec/13 21:16;yukim;6527-2.0.txt;https://issues.apache.org/jira/secure/attachment/12620552/6527-2.0.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,365407,,,Thu Dec 26 22:01:34 UTC 2013,,,,,,,,,,"0|i1qzef:",365709,2.0.3,,,,,,,,jbellis,,jbellis,Critical,,2.0 beta 1,,,,,,,,,,,,,,,,"26/Dec/13 19:17;yukim;Confirmed.

SSTable version 'ic' has the following in partition header.

Partition size: 8 bytes
Partition deletion time: 4 bytes + 8 bytes
Cell count: 4 bytes

When streaming 'ic' to 2.0, it is reading 4 bytes + 8 bytes for deletion time. Then, partition size and cell count are not needed above 2.0, so it skips those 8 bytes and 4 bytes.
It should be 1) skip partition size (8 bytes), 2) read deletion time, then 3) skip cell count (4 bytes).
;;;","26/Dec/13 21:16;yukim;(also: https://github.com/yukim/cassandra/commits/6527)

Patch as well as test case attached.
;;;","26/Dec/13 21:18;yukim;Note that this only happens when streaming ""ic"" version of SSTable.
;;;","26/Dec/13 21:25;jbellis;+1;;;","26/Dec/13 22:01;yukim;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQLSSTableWriter addRow(Map<String, Object> values) does not work as documented.",CASSANDRA-6526,12686352,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,yarix,yarix,25/Dec/13 13:09,16/Apr/19 09:31,14/Jul/23 05:53,06/Mar/14 09:14,2.0.6,,,,,,0,,,,,"There are 2 bugs in the method
{code}
addRow(Map<String, Object> values)
{code}
First issue is that the map <b>must</b> contain all the column names as keys in the map otherwise the addRow fails (with InvalidRequestException ""Invalid number of arguments, expecting %d values but got %d"").

Second Issue is that the keys in the map must be in lower-case otherwise they may not be found in the map, which will result in a NPE during decompose.
h6. SUGGESTED SOLUTION:
Fix the addRow method with:
{code}
public CQLSSTableWriter addRow(Map<String, Object> values)
    throws InvalidRequestException, IOException
{
    int size = boundNames.size();
    Map<String, ByteBuffer> rawValues = new HashMap<>(size);
    for (int i = 0; i < size; i++) {
        ColumnSpecification spec = boundNames.get(i);
        String colName = spec.name.toString();
        rawValues.put(colName, values.get(colName) == null ? null : ((AbstractType)spec.type).decompose(values.get(colName)));
    }
    return rawAddRow(rawValues);
}
{code}
When creating the new Map for the insert we need to go over all columns and apply null to missing columns.

Fix the method documentation add this line:
{code}
     * <p>
     * Keys in the map <b>must</b> be in lower case, otherwise their value will be null.
     *
{code}",,slebresne,thobbs,yarix,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Mar/14 11:06;slebresne;6526.txt;https://issues.apache.org/jira/secure/attachment/12632224/6526.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,365336,,,Thu Mar 06 09:14:19 UTC 2014,,,,,,,,,,"0|i1qyyn:",365638,,,,,,,,,thobbs,,thobbs,Normal,,2.0.2,,,,,,,,,,,,,,,,"25/Dec/13 13:10;yarix;below is a test case:
{code:java}
@Test
public void testAddRow() throws Exception
{
	String KS = ""cql_keyspace"";
	String TABLE = ""table1"";
	
	File tempdir = Files.createTempDir();
	File dataDir = new File(tempdir.getAbsolutePath() + File.separator + KS + File.separator + TABLE);
	assertTrue( dataDir.mkdirs());
	
	String schema = ""CREATE TABLE cql_keyspace.table1 (""
			+ ""  k int PRIMARY KEY,""
			+ ""  v1 text,""
			+ ""  v2 int""
			+ "")"";
	String insert = ""INSERT INTO cql_keyspace.table1 (k, v1, v2) VALUES (?, ?, ?)"";
	CQLSSTableWriter writer = CQLSSTableWriter.builder()
			.inDirectory(dataDir)
			.forTable(schema)
			.withPartitioner(StorageService.instance.getPartitioner())
			.using(insert).build();
	
	Map<String, Object> values = new HashMap<>();
	values.put( ""k"", 0 );
	values.put( ""v1"", ""test1"" );
	values.put( ""v2"", 24 );
	writer.addRow( values );
	
	values.clear();  
	values.put( ""k"", 1 );
	values.put( ""v1"", ""test2"" );
//    	values.put( ""v2"", null ); //commented intentionally so that v2 will get null by the writer.
	writer.addRow( values );
	
	values.clear();
	values.put( ""k"", 2 );
	values.put( ""V1"", ""test3 - will not be found, since V1 is not lowercase"" );
	values.put( ""v2"", 42 );
	values.put( ""v3"", ""some ignored key"" );
	writer.addRow( values );
	
	writer.close();
	
	SSTableLoader loader = new SSTableLoader(dataDir, new SSTableLoader.Client()
	{
		public void init(String keyspace)
		{
			for (Range<Token> range : StorageService.instance.getLocalRanges(""cql_keyspace""))
				addRangeForEndpoint(range, FBUtilities.getBroadcastAddress());
			setPartitioner(StorageService.getPartitioner());
		}
		
		public CFMetaData getCFMetaData(String keyspace, String cfName)
		{
			return Schema.instance.getCFMetaData(keyspace, cfName);
		}
	}, new OutputHandler.SystemOutput(false, false));
	
	loader.stream().get();
	
	UntypedResultSet rs = QueryProcessor.processInternal(""SELECT * FROM cql_keyspace.table1;"");
	assertEquals(3, rs.size());
	
	Iterator<UntypedResultSet.Row> iter = rs.iterator();
	UntypedResultSet.Row row;
	
	row = iter.next();
	assertEquals(0, row.getInt(""k""));
	assertEquals(""test1"", row.getString(""v1""));
	assertEquals(24, row.getInt(""v2""));
	
	row = iter.next();
	assertEquals(1, row.getInt(""k""));
	assertEquals(""test2"", row.getString(""v1""));
	assertFalse(row.has(""v2""));
	
	row = iter.next();
	assertEquals(2, row.getInt(""k""));
	assertFalse(row.has(""v1""));
	assertEquals(42, row.getInt(""v2""));
	assertFalse(row.has(""v3""));

}

{code};;;","25/Dec/13 16:45;jbellis;bq. First issue is that the map <b>must</b> contain all the column names as keys in the map 

This is a bug.

bq. the keys in the map must be in lower-case 

This is not a bug.;;;","29/Dec/13 20:33;yarix;I agree. Yet, I think that adding a documentation (JavaDoc) to the second issue can be very helpful. ;;;","03/Mar/14 11:06;slebresne;Attaching simple patch to fix the bug mentioned and add some doc on when you should lowercase column names.;;;","05/Mar/14 19:13;thobbs;+1;;;","06/Mar/14 09:14;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cannot select data which using ""WHERE""",CASSANDRA-6525,12686346,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,silence.chow,silence.chow,25/Dec/13 10:59,16/Apr/19 09:31,14/Jul/23 05:53,21/May/14 09:46,2.0.8,,,,,,0,,,,,"I am developing a system on my single machine using VMware Player with 1GB Ram and 1Gb HHD. When I select all data, I didn't have any problems. But when I using ""WHERE"" and it has just below 10 records. I have got this error in system log:

{noformat}
ERROR [ReadStage:41] 2013-12-25 18:52:11,913 CassandraDaemon.java (line 187) Exception in thread Thread[ReadStage:41,5,main]
java.io.IOError: java.io.EOFException
        at org.apache.cassandra.db.Column$1.computeNext(Column.java:79)
        at org.apache.cassandra.db.Column$1.computeNext(Column.java:64)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:88)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:37)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:82)
        at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:157)
        at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:140)
        at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:144)
        at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:87)
        at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:46)
        at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:120)
        at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:80)
        at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:72)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:297)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1487)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1306)
        at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1401)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1936)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readFully(Unknown Source)
        at java.io.RandomAccessFile.readFully(Unknown Source)
        at org.apache.cassandra.io.util.RandomAccessReader.readBytes(RandomAccessReader.java:348)
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:392)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
        at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:74)
        at org.apache.cassandra.db.Column$1.computeNext(Column.java:75)
        ... 27 more
{noformat}

E.g.
{{SELECT * FROM table;}}
Its fine.
{{SELECT * FROM table WHERE field = 'N';}}
field is the partition key.
Its said ""Request did not complete within rpc_timeout."" in cqlsh","Linux RHEL5
RAM: 1GB
Cassandra 2.0.3
CQL spec 3.1.1
Thrift protocol 19.38.0",chander,christianmovi,enigmacurry,jeromatron,mbligh,mishail,mshuler,pjc08,rcoli,recastrodiaz,shyamkg,silence.chow,slebresne,sungkyu,thobbs,vayasin,vkuptcov,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6981,,,,,,,,,,,,,,,,,,,,"16/May/14 18:32;thobbs;6525-2.0.txt;https://issues.apache.org/jira/secure/attachment/12645287/6525-2.0.txt","11/Apr/14 21:36;enigmacurry;6981_test.py;https://issues.apache.org/jira/secure/attachment/12639872/6981_test.py",,,,,,,,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,365330,,,Fri Jun 06 11:01:08 UTC 2014,,,,,,,,,,"0|i1qyxb:",365632,2.0.3,2.0.4,2.0.6,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"26/Dec/13 16:38;jbellis;Can you describe how to reproduce starting with a fresh Cassandra install?;;;","28/Dec/13 11:13;silence.chow;My table have 4 field only
For example
CREATE TABLE test (
  hidden text,
  field2 text,
  field3 text,
  field4 text,
  PRIMARY KEY (hidden, field2 , field3)
);

The query using CQL3: SELECT * FROM test WHERE hidden = 'N';;;;","30/Dec/13 07:21;silence.chow;I think I know how to reproduce my situation now.
I have created a new table same as that table. I can run the query something like SELECT * FROM test WHERE hidden = 'N'; at the beginning, After that I have run a stress test which made all the physical RAM and swap exhaust. Then, the problem happen again.
;;;","21/Jan/14 22:54;vayasin;I got the exact same error message and problem. But for me it was a small table of about 20 rows and i did not stress test.
running a compactions fixed the problem for me. I'm not sure what the cause was but i recently dropped and recreated the table. ;;;","14/Mar/14 14:34;enigmacurry;[~mshuler] can you reproduce?;;;","14/Mar/14 20:51;mshuler;I tested using cassandra-2.0 git branch on my laptop (16G), which ran fine. I tried on a 4G box and 2G box - both ran fine looping through the script below, while running cassandra-stress in another shell, also. I'm about 10 or so loops through, while looping stress read and write on a 1G virtualbox vm, and it's slow, but I've had no errors, so far. I'll let it keep running a while to see if I can get a timeout or error of some sort

*update* 2.5 hours of looping this while looping stress read/write on my vbox vm and all is well.
*update2* tried the same after dropping my vbox vm to 512M - 45k row load takes about 60 sec. to import and the read takes a little longer to output - vm starts swapping on the 45k read and the load avg nears 40, but it's still working.

{code}
#!/bin/sh

# create some data:
for i in $(seq 1 50); do echo ""N,text blah blah$i,text blah blah$i,text blah blah$i"" >> c6525_1-50.csv ; done
for i in $(seq 51 500); do echo ""N,text blah blah$i,text blah blah$i,text blah blah$i"" >> c6525_51-500.csv ; done
for i in $(seq 501 5000); do echo ""N,text blah blah$i,text blah blah$i,text blah blah$i"" >> c6525_501-5000.csv ; done
for i in $(seq 5001 50000); do echo ""N,text blah blah$i,text blah blah$i,text blah blah$i"" >> c6525_5001-50000.csv ; done

# create our cql to drop/create/import
cat << 'EOF' > c6525_run.cql
DROP KEYSPACE c6525;

CREATE KEYSPACE c6525 WITH replication = {'class': 'SimpleStrategy', 'replication_factor': '1'};

CREATE TABLE c6525.test (hidden text, field2 text, field3 text, field4 text, PRIMARY KEY (hidden, field2, field3));

COPY c6525.test (hidden, field2, field3, field4) FROM 'c6525_1-50.csv';
SELECT * from c6525.test WHERE hidden = 'N';

COPY c6525.test (hidden, field2, field3, field4) FROM 'c6525_51-500.csv';
SELECT * from c6525.test WHERE hidden = 'N';

COPY c6525.test (hidden, field2, field3, field4) FROM 'c6525_501-5000.csv';
SELECT * from c6525.test WHERE hidden = 'N';

COPY c6525.test (hidden, field2, field3, field4) FROM 'c6525_5001-50000.csv';
SELECT * from c6525.test WHERE hidden = 'N' LIMIT 51000;
EOF

echo; echo ""*** Hit CTL-C to stop looping..***""; echo
sleep 3

# loop it
while true; do echo ""SOURCE 'c6525_run.cql';"" | cqlsh ; sleep 1 ; done
{code};;;","15/Mar/14 01:40;jbellis;Thanks, Michael.;;;","11/Apr/14 11:36;shyamkg;I am still getting this error in DSE 2.0.5 and 2.0.6.. Tried in various machine mac & ubuntu. 

Steps :
1 -> CREATE TABLE DSQ (
        exchange text,
        sc_code int,
        load_date timeuuid, /* tried timestamp also but same behaviour */
        PRIMARY KEY (exchange, sc_code, load_date)
) 
2 -> Did SSTable load
writer.newRow(compositeColumn.builder().add(bytes(entry.stock_exchange)).add(bytes(entry.sc_code)).add(bytes(new com.eaio.uuid.UUID().toString())).build());
3 -> sstablesload 
Established connection to initial hosts
Opening sstables and calculating sections to stream
Streaming relevant part of stock/DSQ/stock-DSQ-ib-1-Data.db to [/127.0.0.1]
progress: [/127.0.0.1 1/1 (100%)] [total: 100% - 2147483647MB/s (avg: 2MB/s)                            
4 -> No errors in server log
5 -> Log into cqlsh and select * from DSQ; 
6 --> errors in Server log: 
Exception in thread Thread[ReadStage:51,5,main]
java.io.IOError: java.io.EOFException
	at org.apache.cassandra.db.Column$1.computeNext(Column.java:79)
	at org.apache.cassandra.db.Column$1.computeNext(Column.java:64)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:88)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:37)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:82)
	at org.apache.cassandra.db.columniterator.LazyColumnIterator.computeNext(LazyColumnIterator.java:82)
	at org.apache.cassandra.db.columniterator.LazyColumnIterator.computeNext(LazyColumnIterator.java:59)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.filter.QueryFilter$2.getNext(QueryFilter.java:157)
	at org.apache.cassandra.db.filter.QueryFilter$2.hasNext(QueryFilter.java:140)
	at org.apache.cassandra.utils.MergeIterator$Candidate.advance(MergeIterator.java:144)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.<init>(MergeIterator.java:87)
	at org.apache.cassandra.utils.MergeIterator.get(MergeIterator.java:46)
	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:120)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:80)
	at org.apache.cassandra.db.RowIteratorFactory$2.getReduced(RowIteratorFactory.java:101)
	at org.apache.cassandra.db.RowIteratorFactory$2.getReduced(RowIteratorFactory.java:75)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:115)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:98)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.ColumnFamilyStore$9.computeNext(ColumnFamilyStore.java:1607)
	at org.apache.cassandra.db.ColumnFamilyStore$9.computeNext(ColumnFamilyStore.java:1603)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1754)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1718)
	at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:137)
	at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1418)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1931)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1142)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:617)
	at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readUnsignedShort(RandomAccessFile.java:713)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:74)
	at org.apache.cassandra.db.Column$1.computeNext(Column.java:75)
	... 37 more
7 -> client shows Request did not complete within rpc_timeout.;;;","11/Apr/14 15:50;thobbs;https://issues.apache.org/jira/browse/CASSANDRA-6981 is a dupe of this.  I'm re-opening this to investigate further.  Besides this ticket and 6981, I've seen one other case of this: https://github.com/datastax/python-driver/issues/106;;;","11/Apr/14 15:57;thobbs;It's worth noting that in CASSANDRA-6981, setting {{disk_access_mode: standard}} seemed to fix the problem.;;;","11/Apr/14 17:50;mbligh;(copied from 6981)
I thought it was interesting how far apart these two numbers were:

""java.io.IOError: java.io.IOException: mmap segment underflow; remaining is 20402577 but 1879048192 requested""

And that the requested number is vaguely close to 2^^31 - did something do a negative number and wrap a 32 bit signed here?
To be fair, it's not that close to 2^^31, but still way off what was expected?;;;","11/Apr/14 20:58;enigmacurry;fwiw, I've written a multi-threaded test for this using the python-driver. It's attached above as 6981_test.py. I used the criteria stated in CASSANDRA-6981:

bq. created about 16 tables, all the same, each with about 5 text fields and 5 binary fields. Most of those fields had a secondary index. Then insert into all the tables in parallel.

I'm using 16 tables, each with 5 text fields, 5 blob fields, inserting 10,000 rows into each table in parallel, and then selecting that data out based on a single field (blob5) that has 5 diffent options.

I could not reproduce the error in this ticket, however I did get this error several times:

{code}
ERROR [ReadStage:136] 2014-04-11 16:55:36,312 CassandraDaemon.java (line 198) Exception in thread Thread[ReadStage:136,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1920)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.io.util.RandomAccessReader.getTotalBufferSize(RandomAccessReader.java:157)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.getTotalBufferSize(CompressedRandomAccessReader.java:159)
        at org.apache.cassandra.service.FileCacheService.get(FileCacheService.java:96)
        at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:36)
        at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1195)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:57)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1540)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1369)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:260)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:103)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1735)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:50)
        at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:556)
        at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1723)
        at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
        at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1374)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1916)
        ... 3 more
{code};;;","11/Apr/14 21:40;enigmacurry;Running this a few more times, I was able to get this on 2.0.5:

{code}
ERROR [ReadStage:90] 2014-04-11 17:37:57,768 CassandraDaemon.java (line 192) Exception in thread Thread[ReadStage:90,5,main]
java.lang.RuntimeException: org.apache.cassandra.io.sstable.CorruptSSTableException: java.io.EOFException: EOF after 46084 bytes out of 48857
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1935)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: org.apache.cassandra.io.sstable.CorruptSSTableException: java.io.EOFException: EOF after 46084 bytes out of 48857
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:82)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1560)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:166)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:105)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1754)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:53)
        at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:537)
        at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1742)
        at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
        at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1418)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1931)
        ... 3 more
Caused by: java.io.EOFException: EOF after 46084 bytes out of 48857
        at org.apache.cassandra.io.util.FileUtils.skipBytesFully(FileUtils.java:392)
        at org.apache.cassandra.utils.ByteBufferUtil.skipShortLength(ByteBufferUtil.java:382)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:70)
        ... 22 more
{code};;;","11/Apr/14 21:51;enigmacurry;This repros on git:cassandra-2.0 HEAD as well:

{code}
ERROR [ReadStage:82] 2014-04-11 17:49:50,903 CassandraDaemon.java (line 216) Exception in thread Thread[ReadStage:82,5,main]
org.apache.cassandra.io.sstable.CorruptSSTableException: java.io.EOFException: EOF after 35761 bytes out of 48857
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:82)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1540)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1369)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:164)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:103)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1735)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:50)
        at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:556)
        at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1723)
        at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
        at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1374)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1916)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.io.EOFException: EOF after 35761 bytes out of 48857
        at org.apache.cassandra.io.util.FileUtils.skipBytesFully(FileUtils.java:394)
        at org.apache.cassandra.utils.ByteBufferUtil.skipShortLength(ByteBufferUtil.java:382)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:70)
        ... 22 more
{code};;;","25/Apr/14 00:02;shyamkg;FYI... Same issue also exist in 2.0.7 version as well. ;;;","25/Apr/14 10:59;shyamkg;I tried couple of things this morning and would like to update

I changed the table definition to have COMPACT STORAGE with LevelCompactionStrategy and loaded the data. 

Server log: 
INFO 06:51:06,415 [Stream #80a0b380-cc67-11e3-a1a2-fb95dccb4714] Received streaming plan for Bulk Load
 INFO 06:51:06,416 [Stream #80a0b380-cc67-11e3-a1a2-fb95dccb4714] Prepare completed. Receiving 1 files(199308 bytes), sending 0 files(0 bytes)
 INFO 06:51:06,466 Enqueuing flush of Memtable-compactions_in_progress@2119789616(131/1310 serialized/live bytes, 7 ops)
 WARN 06:51:06,466 setting live ratio to maximum of 64.0 instead of Infinity
 INFO 06:51:06,466 CFS(Keyspace='system', ColumnFamily='compactions_in_progress') liveRatio is 64.0 (just-counted was 64.0).  calculation took 0ms for 0 cells
 INFO 06:51:06,467 Writing Memtable-compactions_in_progress@2119789616(131/1310 serialized/live bytes, 7 ops)
 INFO 06:51:06,467 [Stream #80a0b380-cc67-11e3-a1a2-fb95dccb4714] Session with /192.168.1.73 is complete
 INFO 06:51:06,468 [Stream #80a0b380-cc67-11e3-a1a2-fb95dccb4714] All sessions completed
 INFO 06:51:06,479 Completed flushing ***/apache-cassandra-2.0.7/data/data/system/compactions_in_progress/system-compactions_in_progress-jb-6-Data.db (158 bytes) for commitlog position ReplayPosition(segmentId=1398421195982, position=197721)
 INFO 06:51:06,483 Compacting [SSTableReader(path='***/apache-cassandra-2.0.7/data/data/stock/dailystockquote/stock-dailystockquote-jb-6-Data.db'), SSTableReader(path='***/apache-cassandra-2.0.7/data/data/stock/dailystockquote/stock-dailystockquote-jb-5-Data.db')]
 INFO 06:51:06,485 Enqueuing flush of Memtable-compactions_in_progress@729498316(0/0 serialized/live bytes, 1 ops)
 INFO 06:51:06,491 Writing Memtable-compactions_in_progress@729498316(0/0 serialized/live bytes, 1 ops)
 INFO 06:51:06,500 Completed flushing ***/apache-cassandra-2.0.7/data/data/system/compactions_in_progress/system-compactions_in_progress-jb-7-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1398421195982, position=197800)

Behavioral changes:
Able to query table with no errors at server log but no data was loaded. 
;;;","09/May/14 22:12;thobbs;This seems to require near-OOM conditions to occur.  So far I've only been able to reproduce this in a low-memory environment (~1GB), and it either occurs just before an OOM or when the JVM is on the brink of exhausting its heap space.;;;","09/May/14 23:00;thobbs;Interestingly, this doesn't seem to be reproduceable when the keyspace isn't dropped and recreated.  (Just modify the repro script to remove the ""DROP KEYSPACE"" and use ""IF NOT EXISTS"" on the create statements.);;;","09/May/14 23:37;thobbs;Considering that drop/recreate seems to be necessary to reproduce the issue and that using a disk_access_mode of ""standard"" with no compression seems to fix the issue, I believe the problem is that old FileCacheService entries are being reused with new SSTables.  The FileCacheService is only used for PoolingSegmentedFiles, which are used if compression or mmap disk access mode are enabled.  Since FileCacheService uses (String) file paths as keys, new SSTables with the same filename can lookup old entries.

The only question is why the old FileCacheService entries are not being invalidated; this basically means that SSTableReader.close() is not being called in some cases.;;;","13/May/14 22:28;thobbs;My initial guess about FileCacheService entries not being invalidated was wrong; they're all being invalidated correctly.  Furthermore, this isn't specific to compressed sstables (it reproduces with and without compression) or to a particular disk_access_mode (both standard and mmap have errors, although the specific errors are different).;;;","14/May/14 20:07;thobbs;The problem is that key cache entries stick around after the keyspace is dropped.  After it's recreated and read, there are key cache hits that return old positions.  I'm not sure why it only seems to be a problem for the secondary index tables; my guess is that the key-cache preheating that happens after compaction is replacing the old entries in the key cache for the data tables.

CASSANDRA-5202 is the correct permanent solution for this, but that's for 2.1.  For 2.0, perhaps we should do something similar to CASSANDRA-6351 and go through the key cache to invalidate all entries for the CF when it's dropped.;;;","15/May/14 07:51;slebresne;bq. For 2.0, perhaps we should do something similar to CASSANDRA-6351 and go through the key cache to invalidate all entries for the CF when it's dropped.

That makes sense to me.;;;","16/May/14 18:32;thobbs;Attached patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6525-2.0]) invalidates relevant key cache entries when a table is dropped.;;;","19/May/14 07:51;slebresne;Patch lgtm, but wouldn't make sense to do also invalidate for truncate in CFS.truncateBlocking, just to be on the safe side?;;;","20/May/14 19:35;thobbs;bq. wouldn't make sense to do also invalidate for truncate in CFS.truncateBlocking, just to be on the safe side?

Truncates don't reset the SSTable generation counter ({{CFS.fileIndexGenerator}}), so new tables will have different generation numbers (and hence different key cache keys).;;;","20/May/14 20:14;vkuptcov;We have a cluster with 5 nodes in one DC and a cluster with two nodes in the other without a replication between these datacenters. In all DC we use C* 2.0.5.

Today we've found a bug with similar messages but with the different result. We have dropped and recreated one table in the DC with 5 nodes and just truncated the same table in another DC.
After ~10 hours we have noticed appearing of the following messages in the first DC logs:
{code}
ERROR [ReadStage:231469] 2014-05-20 21:05:20,349 CassandraDaemon.java (line 192) Exception in thread Thread[ReadStage:231469,5,main]
java.io.IOError: java.io.EOFException
        at org.apache.cassandra.db.Column$1.computeNext(Column.java:79)
        at org.apache.cassandra.db.Column$1.computeNext(Column.java:64)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:88)
:
{code}

For the node, on which this messages started, we found several messages like on the other nodes
{code}
 INFO [GossipTasks:1] 2014-05-20 21:20:31,864 Gossiper.java (line 863) InetAddress /10.33.20.91 is now DOWN
 INFO [RequestResponseStage:10] 2014-05-20 21:20:32,186 Gossiper.java (line 849) InetAddress /10.33.20.91 is now UP
 INFO [GossipTasks:1] 2014-05-20 21:26:51,965 Gossiper.java (line 863) InetAddress /10.33.20.91 is now DOWN
{code}
and finally the node has stopped.


We found such effect only in the DC, where we have dropped and recreated table. In the DC with truncate everything is OK.
;;;","20/May/14 20:42;thobbs;[~vkuptcov] that seems consistent with what I found.  I suggest invalidating your key cache in the problematic DC.  You can use {{nodetool invalidatekeycache}} to do this.;;;","20/May/14 20:50;vkuptcov;Yes, it looks like this. We have deleted the data from /var/lib/cassandra/saved_caches/* and after nodes restarting we don't notice the mentioned exceptions.;;;","21/May/14 07:33;slebresne;bq. Truncates don't reset the SSTable generation counter

Fair enough (though it would still feel cleaner to invalidate the key cache entries, even if it don't result in a bug). But anyway, +1 on the patch.;;;","21/May/14 09:46;slebresne;Patch committed (I want to start a vote for 2.0.8), thanks.;;;","06/Jun/14 11:01;jeromatron;Darn jira hotkeys.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Unable to contact any seeds!"" with multi-DC cluster and listen != broadcast address",CASSANDRA-6523,12686197,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,cburroughs,cburroughs,23/Dec/13 19:48,16/Apr/19 09:31,14/Jul/23 05:53,09/Jun/14 19:29,2.0.9,2.1 rc2,,,,,0,,,,,"New cluster:
 * Seeds: list of 6 internal IPs
 * listen address: internal ip
 * broadcast: external ip

Two DC cluster, using GPFS where the external IPs are NATed.  Clusters fails to start with ""Unable to contact any seeds!""

 * Fail: Try to start a seed node
 * Fail: Try to start two seed nodes at the same time in the same DC
 * Success: Start two seed nodes at the same time in different DCs.

Presumably related to CASSANDRA-5768",1.2.13ish,cburroughs,garo5,jasobrown,mishail,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/14 15:54;brandon.williams;6523.txt;https://issues.apache.org/jira/secure/attachment/12647154/6523.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,365170,,,Mon Jun 09 19:29:19 UTC 2014,,,,,,,,,,"0|i1qxyf:",365475,,,,,,,,,jasobrown,,jasobrown,Low,,,,,,,,,,,,,,,,,,"13/Mar/14 21:43;jbellis;Is this still a problem with 5768 fixed?;;;","31/Mar/14 15:37;cburroughs;I believe it's a regression caused by CASSANDRA-5768;;;","22/May/14 14:43;cburroughs;Still present in 2.0.x series.  There are a variety of stack overflow and mailing list threads threads with ""Unable to contact any seeds"".

I think the problem is that CASSANDRA-5768 isn't checking ""have I contacted a seed"" but ""am I connected to one of these IP addresses"".  That ends up being  requirement that the seeds/broadcast/listen address line up in a particular way.;;;","22/May/14 21:44;brandon.williams;It's comparing the actual IP address it received contact from to the seed list, so if it's being contacted by the broadcast address, but the seed is listed with the internal address, there won't be a match.  So, you'd probably want a different seed list for DC 1 and DC 2, with the listen/broadcast switched for the seeds that aren't local.;;;","23/May/14 13:14;cburroughs;I don't think that's an unreasonable requirement and I can roll with it but if there is an easy way the check could be made contact based instead of string equality based that would be nice.  Starting a cluster is often the first thing someone will do with Cassandra and NATs, firewalls hijacking DNS requests, reconnecting snitches all make this complicated (Top results on stack overflow disagree for example [1] [2]).

Barring a more clever check I suggest changing the message to something like ""Seed check failed!  At least one boardcast_address must match IPs from seed list"".  Seeing ""Handshaking with foo"" from the seed list and then ""Unable to contact any seeds!"" is a confusing experience.

[1] http://stackoverflow.com/questions/21261098/cassandra-unable-to-contact-seeds-if-using-aws-elastic-ip-address-only-works
[2] http://stackoverflow.com/questions/20690987/apache-cassandra-unable-to-gossip-with-any-seeds;;;","23/May/14 15:36;brandon.williams;Part of the problem, is we can't know a seed's broadcast_address without, of course, them gossiping it to us.  We could possibly extend the 'talked to a seed' check to loop through the endpoints we know about and look for a matching broadcast_address, though that's a bit expensive and is kind of cheating since we want to know we talked to a seed directly, not that we were told about a seed, but I don't think that actually matters.  I'll have to think about it some.

bq. Barring a more clever check I suggest changing the message to something like ""Seed check failed! At least one boardcast_address must match IPs from seed list"".

Well, that's not entirely accurate either, since if you aren't using BCA at all, it's confusing.  We could print out the seeds with the error though, so at least between that and the handshaking messages you'd get a clue.;;;","28/May/14 15:53;brandon.williams;Patch to iterate through the endpoint map and compare both the endpoint address and INTERNAL_IP (if present) to the seeds list instead of tracking it on a per-contact basis.  Possibly expensive in a large cluster, but not called from the gossiper itself so that should be fine.

I don't actually have a NAT setup and don't really want to make one (give myself the disease to test the cure), so if you could test this out Chris that'd help.;;;","30/May/14 22:27;jasobrown;[~cburroughs] Not sure how usable this suggestion is, but you could use the external addresses in the seeds list instead of the internal (or mixed per-dc). As GPFS is a 'reconnecting snitch', once the connection of the IP addr is established, the nodes in the same DC will switch over to the internal IP (listen_address). The length of time from node startup to switching the connection to internal IP should be rather short, so your traffic shouldn't be 'unusual' for much time. However, what happens during that 'unusual' time may be a deal breaker.

I bring this up as that's what we did when running multi-region in ec2, and is what EC2MRS facilitates. ;;;","31/May/14 14:04;garo5;I have a similar issue described in CASSANDRA-7292 and in there I use the public ip addresses in the seeds list, but it does not help in my case. These two issues might or might not be connected.;;;","06/Jun/14 20:10;cburroughs;Jason, thanks for the suggestion.  I think there are a very different ways I could work around it.  At this point though I'd like to see it cleared up so others don't trip over it too.

Tested driftx's patch on top of 2.0.8 with two DCs and the same NAT setup.

 * Started a node in DC A with auto_bootrap:false and it came up happy.
 * Started a second node in DC A, and it successfully bootstrapped.
 * Stopped all DC A nodes, started a node in DC B with auto_bootrap:true.  It failed with ""Unable to contact any seeds!'
 * Started all DC A nodes, nodes in B could bootstrap.

So it looks like it works from my point of view.;;;","09/Jun/14 18:58;jasobrown;+1 to @driftx's patch;;;","09/Jun/14 19:29;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DroppableTombstoneRatio JMX value is 0.0 for all CFs,CASSANDRA-6522,12686076,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,dkador,dkador,22/Dec/13 19:59,16/Apr/19 09:31,14/Jul/23 05:53,06/Feb/14 07:59,2.0.6,,,,,,0,,,,,"We're seeing that the JMX value for DroppableTombstoneRatio for all our CFs is 0.0. On the face of it that seems wrong since we've definitely issued a ton of deletes for row keys to expire some old data that we no longer need (and it definitely hasn't been reclaimed from disk yet). Am I misunderstanding what this means / how to use it? We're on 1.2.8 and using leveled compaction for all our CFs.

gc_grace_seconds is set to 1 day and we've issued a series of deletes over a day ago, so gc_grace has elapsed.

Cluster is 18 nodes.  Two DCs, so 9 nodes in each DC.  Each node has capacity for 1.5TB or so and is sitting with about 1TB under management.  That's why we wanted to do deletes, obviously.  Most of that 1TB is a single CF (called ""events"") which represents intermediate state for us that we can delete.

Happy to provide any more info, just let me know.","Ubuntu 12.04 LTS, Cassandra 1.2.8",cburroughs,dkador,marcuse,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5159,,,,,,,,,,,,,,,"08/Jan/14 08:24;marcuse;0001-account-for-range-and-row-tombstones-in-tombstone-dr.patch;https://issues.apache.org/jira/secure/attachment/12621942/0001-account-for-range-and-row-tombstones-in-tombstone-dr.patch",,,,,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,365033,,,Thu Feb 06 13:03:43 UTC 2014,,,,,,,,,,"0|i1qx4n:",365341,1.2.8,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"26/Dec/13 23:11;jbellis;/cc [~yukim];;;","02/Jan/14 10:42;marcuse;This metric only counts deleted columns, I guess you have been doing entire row deletes?

[~jbellis] [~yukim] Should we try to estimate how many columns are affected by a row/range tombstone and include in this metric? ;;;","02/Jan/14 15:20;jbellis;Yes.;;;","02/Jan/14 17:18;dkador;Marcus Eriksson: Yeah, we were doing row deletes, not column deletes.

Thanks for getting this scheduled.;;;","08/Jan/14 08:24;marcuse;Patch that accounts for row and range tombstones in the histogram. Also prints the histogram in tools/bin/sstablemetadata.

Note that it is very hard to cheaply know how many live columns a range tombstone shadows, so this metric is only the tombstone count / number of columns, which in this case would be pretty pointless.

CQL deletes were not accounted for before this, so it should also help for tombstone-only compaction in those cases.;;;","31/Jan/14 15:47;jbellis;+1;;;","06/Feb/14 07:59;marcuse;Committed, thanks;;;","06/Feb/14 12:38;cburroughs;This a ""jmx value reporting"" bug or a ""sstables not getting compacted"" bug?;;;","06/Feb/14 13:03;marcuse;Both

It is the histogram used for deciding if we should do a tombstone-only compaction that is exposed over JMX;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift should validate SliceRange start and finish lengths,CASSANDRA-6521,12686065,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,benbromhead,benbromhead,benbromhead,22/Dec/13 16:54,16/Apr/19 09:31,14/Jul/23 05:53,22/Dec/13 17:11,1.2.14,2.0.4,,,,,0,,,,,"To quote [~benbromhead]:

bq. It appears that Cassandra does not check the length of a column name that is part of a range predicate for a *_slice query before it serialises the slice query to pass to the replicas. Names with a length greater than 0xFFFF cause an assertion error to occur in ByteBufferUtil.writeWithShortLength. This further causes subsequent reads on the node to fail until Cassandra is restarted",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6533,,,,,,,,,,,,,,,,,,,,"22/Dec/13 17:01;aleksey;6521.txt;https://issues.apache.org/jira/secure/attachment/12620094/6521.txt",,,,,,,,,,,,,,,,,,,,1.0,benbromhead,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,365017,,,Sun Dec 22 17:11:42 UTC 2013,,,,,,,,,,"0|i1qx1b:",365326,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"22/Dec/13 17:01;aleksey;Attaching Ben's patch with several modifications.;;;","22/Dec/13 17:11;aleksey;Committed, thanks, Ben.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix dropping columns,CASSANDRA-6520,12685917,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mishail,rhatch,rhatch,20/Dec/13 23:06,16/Apr/19 09:31,14/Jul/23 05:53,03/Jan/14 22:54,2.1 beta1,,,Legacy/CQL,,,0,,,,,"Using ccqlsh, I issue a statement to drop a column from a table, and the session appears to disconnect.

The statement was:
{noformat}
cqlsh:taskapp> alter table user_task drop task_order;
{noformat}
Here's the full setup I used:
{noformat}
ccm create test_cluster
ccm populate -n 3
ccm start
ccm node1 cqlsh

CREATE KEYSPACE taskapp WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '3'
};

use taskapp;

create table user (
    user_id timeuuid PRIMARY KEY,
    first_name text,
    last_name text,
    email text
);

create table user_task (
    task_id timeuuid PRIMARY KEY,
    user_id timeuuid,
    task_order int,
    task_description text,
    is_complete boolean,
    is_top_level boolean,
    subtask_ids list<timeuuid>
);
{noformat}
and then the statement which triggers the disconnect:
{noformat}
cqlsh:taskapp> alter table user_task drop task_order;
TSocket read 0 bytes
TSocket read 0 bytes
cqlsh:taskapp> describe table user_task;

[Errno 32] Broken pipe
{noformat}

The log for the active node shows this INFO, followed immediately by an exception (included below). The other nodes show no relevant messages:
{noformat}
INFO  [Thrift:4] 2013-12-20 16:04:58,668 MigrationManager.java:263 - Update ColumnFamily 'taskapp/user_task' From org.apache.cassandra.config.CFMetaData@15e4ed88[cfId=df7153ac-c309-3bd2-92c2-e05bb53153fb,ksName=taskapp,cfName=user_task,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(7375627461736b5f696473:org.apache.cassandra.db.marshal.ListType(org.apache.cassandra.db.marshal.TimeUUIDType))),comment=,readRepairChance=0.1,dclocalReadRepairChance=0.0,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.TimeUUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=subtask_ids, type=org.apache.cassandra.db.marshal.ListType(org.apache.cassandra.db.marshal.TimeUUIDType), kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=is_complete, type=org.apache.cassandra.db.marshal.BooleanType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=12 cap=12]=ColumnDefinition{name=is_top_level, type=org.apache.cassandra.db.marshal.BooleanType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=task_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=10 cap=10]=ColumnDefinition{name=task_order, type=org.apache.cassandra.db.marshal.Int32Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=user_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=16 cap=16]=ColumnDefinition{name=task_description, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtable_flush_period_in_ms=0,caching=KEYS_ONLY,defaultTimeToLive=0,speculative_retry=99.0PERCENTILE,indexInterval=128,populateIoCacheOnFlush=false,droppedColumns={},triggers={}] To org.apache.cassandra.config.CFMetaData@3568f812[cfId=df7153ac-c309-3bd2-92c2-e05bb53153fb,ksName=taskapp,cfName=user_task,cfType=Standard,comparator=org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.ColumnToCollectionType(7375627461736b5f696473:org.apache.cassandra.db.marshal.ListType(org.apache.cassandra.db.marshal.TimeUUIDType))),comment=,readRepairChance=0.1,dclocalReadRepairChance=0.0,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.TimeUUIDType,minCompactionThreshold=4,maxCompactionThreshold=32,columnMetadata={java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=subtask_ids, type=org.apache.cassandra.db.marshal.ListType(org.apache.cassandra.db.marshal.TimeUUIDType), kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=11 cap=11]=ColumnDefinition{name=is_complete, type=org.apache.cassandra.db.marshal.BooleanType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=12 cap=12]=ColumnDefinition{name=is_top_level, type=org.apache.cassandra.db.marshal.BooleanType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=task_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=PARTITION_KEY, componentIndex=null, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=7 cap=7]=ColumnDefinition{name=user_id, type=org.apache.cassandra.db.marshal.TimeUUIDType, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}, java.nio.HeapByteBuffer[pos=0 lim=16 cap=16]=ColumnDefinition{name=task_description, type=org.apache.cassandra.db.marshal.UTF8Type, kind=REGULAR, componentIndex=0, indexName=null, indexType=null}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={sstable_compression=org.apache.cassandra.io.compress.LZ4Compressor},bloomFilterFpChance=0.01,memtable_flush_period_in_ms=0,caching=KEYS_ONLY,defaultTimeToLive=0,speculative_retry=99.0PERCENTILE,indexInterval=128,populateIoCacheOnFlush=false,droppedColumns={task_order=1387580698664000},triggers={}]
ERROR [Thrift:4] 2013-12-20 16:04:58,672 CustomTThreadPoolServer.java:212 - Error occurred during processing of message.
java.lang.ClassCastException: java.lang.Long cannot be cast to java.util.Map
	at org.apache.cassandra.serializers.MapSerializer.serialize(MapSerializer.java:27) ~[main/:na]
	at org.apache.cassandra.db.marshal.AbstractType.decompose(AbstractType.java:71) ~[main/:na]
	at org.apache.cassandra.db.CFRowAdder.add(CFRowAdder.java:78) ~[main/:na]
	at org.apache.cassandra.db.CFRowAdder.addMapEntry(CFRowAdder.java:65) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.toSchemaNoColumnsNoTriggers(CFMetaData.java:1610) ~[main/:na]
	at org.apache.cassandra.config.CFMetaData.toSchemaUpdate(CFMetaData.java:1483) ~[main/:na]
	at org.apache.cassandra.service.MigrationManager.announceColumnFamilyUpdate(MigrationManager.java:264) ~[main/:na]
	at org.apache.cassandra.cql3.statements.AlterTableStatement.announceMigration(AlterTableStatement.java:217) ~[main/:na]
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:71) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:194) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:228) ~[main/:na]
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:218) ~[main/:na]
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1966) ~[main/:na]
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4486) ~[thrift/:na]
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4470) ~[thrift/:na]
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.1.jar:0.9.1]
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.1.jar:0.9.1]
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194) ~[main/:na]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_45]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_45]
	at java.lang.Thread.run(Thread.java:744) [na:1.7.0_45]
{noformat}","C* from trunk -- cassandra-2.0.3-709-g486f079
java 1.7.0_45 (on linux 64 bit)
[cqlsh 4.1.0 | Cassandra 2.1-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]

3 node cluster built on my machine using ccm",aleksey,mishail,rhatch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5417,,,,,,"21/Dec/13 04:08;mishail;trunk-6520.patch;https://issues.apache.org/jira/secure/attachment/12619961/trunk-6520.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,364986,,,Fri Jan 03 22:54:09 UTC 2014,,,,,,,,,,"0|i1qwuf:",365295,,,,,,,,,aleksey,,aleksey,Normal,,2.1 rc3,,,,,,,,,,,,,,,,"21/Dec/13 04:08;mishail;I guess we should use {{valueComparator}} for collection types in this case. Patch attached;;;","03/Jan/14 22:54;aleksey;Committed, thanks again [~mishail];;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loss of secondary index entries if nodetool cleanup called before compaction,CASSANDRA-6517,12685801,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,awerrch,awerrch,20/Dec/13 12:40,16/Apr/19 09:31,14/Jul/23 05:53,31/Jan/14 15:45,2.0.5,,,Feature/2i Index,Legacy/CQL,,1,,,,,"From time to time we had the feeling of not getting all results that should have been returned using secondary indexes. Now we tracked down some situations and found out, it happened:

1) To primary keys that were already deleted and have been re-created later on

2) After our nightly maintenance scripts were running

We can reproduce now the following szenario:

- create a row entry with an indexed column included
- query it and use the secondary index criteria -> Success
- delete it, query again -> entry gone as expected
- re-create it with the same key, query it -> success again

Now use in exactly that sequence

nodetool cleanup
nodetool flush
nodetool compact

When issuing the query now, we don't get the result using the index. The entry is indeed available in it's table when I just ask for the key. Below is the exact copy-paste output from CQL when I reproduced the problem with an example entry on on of our tables.

mwerrch@mstc01401:/opt/cassandra$ current/bin/cqlsh Connected to 14-15-Cluster at localhost:9160.
[cqlsh 4.1.0 | Cassandra 2.0.3 | CQL spec 3.1.1 | Thrift protocol 19.38.0] Use HELP for help.
cqlsh> use mwerrch;
cqlsh:mwerrch> desc tables;

B4Container_Demo

cqlsh:mwerrch> desc table ""B4Container_Demo"";

CREATE TABLE ""B4Container_Demo"" (
  key uuid,
  archived boolean,
  bytes int,
  computer int,
  deleted boolean,
  description text,
  doarchive boolean,
  filename text,
  first boolean,
  frames int,
  ifversion int,
  imported boolean,
  jobid int,
  keepuntil bigint,
  nextchunk text,
  node int,
  recordingkey blob,
  recstart bigint,
  recstop bigint,
  simulationid bigint,
  systemstart bigint,
  systemstop bigint,
  tapelabel bigint,
  version blob,
  PRIMARY KEY (key)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='demo' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=604800 AND
  index_interval=128 AND
  read_repair_chance=1.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

CREATE INDEX mwerrch_Demo_computer ON ""B4Container_Demo"" (computer);

CREATE INDEX mwerrch_Demo_node ON ""B4Container_Demo"" (node);

CREATE INDEX mwerrch_Demo_recordingkey ON ""B4Container_Demo"" (recordingkey);

cqlsh:mwerrch> INSERT INTO ""B4Container_Demo"" (key,computer,node) VALUES (78c70562-1f98-3971-9c28-2c3d8e09c10f, 50, 50); cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

 key                                  | node | computer
--------------------------------------+------+----------
 78c70562-1f98-3971-9c28-2c3d8e09c10f |   50 |       50

(1 rows)

cqlsh:mwerrch> DELETE FROM ""B4Container_Demo"" WHERE key=78c70562-1f98-3971-9c28-2c3d8e09c10f;
cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

(0 rows)

cqlsh:mwerrch> INSERT INTO ""B4Container_Demo"" (key,computer,node) VALUES (78c70562-1f98-3971-9c28-2c3d8e09c10f, 50, 50); cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

 key                                  | node | computer
--------------------------------------+------+----------
 78c70562-1f98-3971-9c28-2c3d8e09c10f |   50 |       50

(1 rows)

**********************************
Now we execute (maybe from a different shell so we don't have to close this session) from /opt/cassandra/current/bin directory:
./nodetool cleanup
./nodetool flush
./nodetool compact


Going back to our CQL session the result will no longer be available if queried via the index:
*********************************

cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

(0 rows)
",Ubuntu 12.0.4 with 8+ GB RAM and 40GB hard disk for data directory.,awerrch,Daniel Smedegaard Buus,mishail,mshuler,rhatch,samt,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jan/14 15:32;samt;0001-CASSANDRA-6517-Use-column-timestamp-to-check-for-del.patch;https://issues.apache.org/jira/secure/attachment/12622641/0001-CASSANDRA-6517-Use-column-timestamp-to-check-for-del.patch","13/Jan/14 15:32;samt;repro.sh;https://issues.apache.org/jira/secure/attachment/12622640/repro.sh",,,,,,,,,,,,,,,,,,,2.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,364987,,,Fri Jan 31 15:45:41 UTC 2014,,,,,,,,,,"0|i1qwun:",365296,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"20/Dec/13 14:45;jbellis;Can you reproduce [~mshuler]?;;;","20/Dec/13 15:17;awerrch;Some technical details about our environment: We use Murmur3Partitioner and NetworkTopologyStrategy with 3 data centers consisting each of 2 computers. We use VNodes and 256 tokens each. The column family from the above example has an RF=1 per data center. The server log files did not show any obvious errors (level = INFO, we might increase this) except this during a nodetool repair -pr job some days ago (so I don't think this is the main reason for something reproducible today):

""Comparison method violates its general contract.""

The machine kept running stable afterwards and was fully available for further requests. Still we restarted it to make sure there are no broken internal threads or something that nasty.;;;","20/Dec/13 18:20;mshuler;Thanks for the extra details, Christoph - I will see if I can reproduce.
;;;","06/Jan/14 17:55;mshuler;I reproduced on 2.0.4 with a simple 3-node RF=3 ccm cluster.  Repair did not change the results and querying a different node has the same result.
{code}
cqlsh:mwerrch> select key,node,computer from ""B4Container_Demo"" where computer=50;

(0 rows)

cqlsh:mwerrch> select * from ""B4Container_Demo"";

 key                                  | archived | bytes | computer | deleted | description | doarchive | filename | first | frames | ifversion | imported | jobid | keepuntil | nextchunk | node | recordingkey | recstart | recstop | simulationid | systemstart | systemstop | tapelabel | version
--------------------------------------+----------+-------+----------+---------+-------------+-----------+----------+-------+--------+-----------+----------+-------+-----------+-----------+------+--------------+----------+---------+--------------+-------------+------------+-----------+---------
 78c70562-1f98-3971-9c28-2c3d8e09c10f |     null |  null |       50 |    null |        null |      null |     null |  null |   null |      null |     null |  null |      null |      null |   50 |         null |     null |    null |         null |        null |       null |      null |    null

(1 rows)

cqlsh:mwerrch>
{code}
----
Update1: full cluster restart shows the same results
----
Update2: debug logs, query run on node1 ""... where computer=50"" resulting in (0 rows)
{code}
DEBUG [Thrift:1] 2014-01-06 12:11:42,122 CassandraServer.java (line 1954) execute_cql3_query
DEBUG [Thrift:1] 2014-01-06 12:11:42,136 AbstractReplicationStrategy.java (line 86) clearing cached endpoints
DEBUG [WRITE-/127.0.0.2] 2014-01-06 12:11:42,270 OutboundTcpConnection.java (line 290) attempting to connect to /127.0.0.2
 INFO [HANDSHAKE-/127.0.0.2] 2014-01-06 12:11:42,271 OutboundTcpConnection.java (line 386) Handshaking version with /127.0.0.2

==> .ccm/test/node2/logs/system.log <==
DEBUG [ACCEPT-/127.0.0.2] 2014-01-06 12:11:42,271 MessagingService.java (line 850) Connection version 7 from /127.0.0.1
DEBUG [Thread-7] 2014-01-06 12:11:42,272 IncomingTcpConnection.java (line 107) Upgrading incoming connection to be compressed
DEBUG [Thread-7] 2014-01-06 12:11:42,274 IncomingTcpConnection.java (line 115) Max version for /127.0.0.1 is 7
DEBUG [Thread-7] 2014-01-06 12:11:42,274 MessagingService.java (line 743) Setting version 7 for /127.0.0.1
DEBUG [Thread-7] 2014-01-06 12:11:42,274 IncomingTcpConnection.java (line 124) set version for /127.0.0.1 to 7
DEBUG [ReadStage:1] 2014-01-06 12:11:42,283 KeysSearcher.java (line 69) Most-selective indexed predicate is 'B4Container_Demo.computer EQ 50'
DEBUG [ReadStage:1] 2014-01-06 12:11:42,285 FileCacheService.java (line 70) Evicting cold readers for /home/mshuler/.ccm/test/node2/data/system/schema_columns/system-schema_columns-jb-7-Data.db
DEBUG [ReadStage:1] 2014-01-06 12:11:42,286 FileCacheService.java (line 115) Estimated memory usage is 11033316 compared to actual usage 0
DEBUG [ReadStage:1] 2014-01-06 12:11:42,286 FileCacheService.java (line 115) Estimated memory usage is 11164665 compared to actual usage 131349
DEBUG [ReadStage:1] 2014-01-06 12:11:42,286 FileCacheService.java (line 115) Estimated memory usage is 11296014 compared to actual usage 262698

==> .ccm/test/node1/logs/system.log <==
DEBUG [Thrift:1] 2014-01-06 12:11:42,287 Tracing.java (line 159) request complete
{code}

'select * ..."" query debug log is just:
{code}
DEBUG [Thrift:1] 2014-01-06 12:18:07,087 CassandraServer.java (line 1954) execute_cql3_query
DEBUG [Thrift:1] 2014-01-06 12:18:07,122 Tracing.java (line 159) request complete
{code}
----
Update4: went back to fresh cluster, inserted the data and queried ""... where computer=50""
{code}
DEBUG [Thrift:1] 2014-01-06 12:28:01,995 CassandraServer.java (line 1954) execute_cql3_query

==> .ccm/test/node2/logs/system.log <==
DEBUG [ReadStage:33] 2014-01-06 12:28:02,015 KeysSearcher.java (line 69) Most-selective indexed predicate is 'B4Container_Demo.computer EQ 50'

==> .ccm/test/node1/logs/system.log <==
DEBUG [Thrift:1] 2014-01-06 12:28:02,019 Tracing.java (line 159) request complete
{code};;;","06/Jan/14 18:54;jbellis;Do you have time to take a look, Sam?;;;","07/Jan/14 11:02;samt;I should have some time to look at this in the next few days;;;","09/Jan/14 09:10;awerrch;I just tried to reproduce the problem on an old cluster here that is still running Cassandra 1.2.3 version using RandomPartitioner. There everything succeeded properly. My guess is, the above problem is a side effect of the changes made for the 2.x versions.;;;","13/Jan/14 15:32;samt;Cleanup is irrelevant here and I was able to repro on a single node. The root cause is the incorrect use of CompactionManager.NO_GC (aka Long.MIN_VALUE) as a timestamp in PreCompactedRow.merge. The sequence of events is like so:

* update 0 inserts the row, so indexes the column value 
* update 1 deletes the row with a RangeTombstone, which deletes the value from the 2i, but leaves the original columns in the main cf's memtable
* update 2 re-inserts the row, now the main cf memtable still has the old col (which was being shadowed by the RT) so it calls SIM.update - which inserts the new col into the 2i and tries to delete the old column (which was already removed by update 1) - this results in another tombstone being written to the 2i memtable, but this has the timestamp of the column from the update 0 (the one who's 2i entry has already been removed) so it has no negative effect. This is why the 2i query contines to work as expected until we flush/compact.

When we flush, the RT is written to the sstable. This means that at compaction time, when we come to process the live column value from the sstable it is checked against the RT and ends up being removed from the 2i because of the incorrect timestamp passed into deletionInfo.isDeleted in PCR.merge. This index removal only hits the 2i memtable though, so although it prevents queries working correctly it only does so until the node is restarted (clearing the 2i memtable).

I've attached a bash script which repros the problem & a patch to fix it. The patch includes a new unit test and all the existing unit tests are still passing (though I didn't check any dtests).;;;","15/Jan/14 09:17;awerrch;Thanks Sam, for working this out and for the provided patch. I took the 2.0.4 source code and modified the PrecompactedRow class as suggested. Now we are running this patched version on some testing machines and there the described problem is gone. Have to do more testing and finally give it a try on our productive system, but at this moment things are looking very promising.;;;","31/Jan/14 15:45;slebresne;+1, committed (with s/{{isDeleted(column.name(), column.timestamp())}}/{{isDeleted(column)}} since that's equivalent), thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix validator lookup when converting cells to objects (pig),CASSANDRA-6515,12685746,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius,dbrosius,20/Dec/13 04:42,16/Apr/19 09:31,14/Jul/23 05:53,20/Dec/13 11:05,2.1 beta1,,,,,,0,,,,,"due to refactor, lookup using validators map would fail as the col name type was changed from ByteBuffer to String., use the byte buffer instead.",,dbrosius,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/13 04:43;dbrosius;6515.txt;https://issues.apache.org/jira/secure/attachment/12619747/6515.txt",,,,,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,364821,,,Fri Dec 20 11:05:54 UTC 2013,,,,,,,,,,"0|i1qvrr:",365121,,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,,"20/Dec/13 07:19;slebresne;+1;;;","20/Dec/13 11:05;dbrosius;committed to trunk as commit 0d695d4d1eac1241fa251dc45103953aeb977ce2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool Refresh / CFS.loadNewSSTables() can Lose New SSTables,CASSANDRA-6514,12685696,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,19/Dec/13 22:40,16/Apr/19 09:31,14/Jul/23 05:53,18/Mar/14 23:17,2.0.7,2.1 beta2,,Tool/nodetool,,,0,,,,,"When nodetool refresh / CFS.loadNewSSTables() renames the newly loaded SSTables, it doesn't check to make sure the new name doesn't already exist.  It's easy for one of the newly loaded files themselves to have one of these names, so the rename will wipe out one of the SSTables you intended to load.

For example, if you create a new, empty table, move two SSTables with generations 1 and 2 into the data directory, and then call {{nodetool refresh}}, you might see this:

{noformat}
INFO 15:37:42,587 Loading new SSTables for duration_test/ints...
 INFO 15:37:42,601 Renaming new SSTable /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-2 to /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-1
 INFO 15:37:42,605 Opening /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-1 (424005 bytes)
 INFO 15:37:42,614 Renaming new SSTable /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-1 to /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-2
 INFO 15:37:42,615 Opening /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-2 (424005 bytes)
 INFO 15:37:42,617 Loading new SSTables and building secondary indexes for duration_test/ints: [SSTableReader(path='/var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-2-Data.db')]
 INFO 15:37:42,618 Done loading load new SSTables for duration_test/ints
ERROR 15:38:09,428 Exception in thread Thread[ReadStage:40,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/duration_test/ints/duration_test-ints-jb-1-Data.db (No such file or directory)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1939)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)

{noformat}",,cburroughs,rcoli,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6245,,,,,,,,,,,"18/Mar/14 22:21;thobbs;6514-2.0-v2.patch;https://issues.apache.org/jira/secure/attachment/12635417/6514-2.0-v2.patch","19/Dec/13 22:43;thobbs;6514-2.0.patch;https://issues.apache.org/jira/secure/attachment/12619682/6514-2.0.patch",,,,,,,,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,364771,,,Tue Mar 18 23:17:53 UTC 2014,,,,,,,,,,"0|i1qvgn:",365071,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"19/Dec/13 22:43;thobbs;Attached patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6514]) checks for conflicts when renaming the loaded SSTables and includes a unit test that exercises this.;;;","19/Dec/13 22:50;jbellis;I'm not sure it's worth special-casing the rename, since flush/compaction can still error out here.  Either we need to solve it everywhere or tell people, ""come up with generation ids high enough that they won't conflict with new sstables until long after loading is done."";;;","19/Dec/13 22:57;jbellis;Maybe it would be best to add some kind of ""staging"" directory so we don't need to worry about conflicts at all.  Fundamentally it's kind of broken to encourage people to manually edit the live data directories.;;;","19/Dec/13 22:59;thobbs;bq. Maybe it would be best to add some kind of ""staging"" directory so we don't need to worry about conflicts at all. Fundamentally it's kind of broken to encourage people to manually edit the live data directories.

Yeah, I agree that that would be the proper fix, this was just a quick solution to avoid a (probably) more common case.;;;","19/Dec/13 23:01;thobbs;bq.  ""come up with generation ids high enough that they won't conflict with new sstables until long after loading is done.""

I did think about that, but in this case, the generations didn't conflict with newly flushed sstables, so it was particularly unexpected. ;;;","12/Mar/14 20:35;jbellis;+1, although I'd caution not to make it sounds like there is no race at all anyore in CHANGES;;;","18/Mar/14 22:21;thobbs;I ended up needing to adjust the tests a bit to ensure SSTables were cleared off disk and there were new duplicate hardlinks due to incremental backups.  The v2 patch includes those [changes|https://github.com/thobbs/cassandra/commit/d060d9ec9dc460c21aa151f37f59badf9d25150a].;;;","18/Mar/14 22:44;jbellis;+1;;;","18/Mar/14 23:17;thobbs;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't drop local mutations without a hint,CASSANDRA-6510,12685590,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,19/Dec/13 13:33,16/Apr/19 09:31,14/Jul/23 05:53,19/Dec/13 13:58,1.2.14,2.0.4,,,,,0,,,,,"SP.insertLocal() uses a regular DroppableRunnable, thus timed out local mutations get dropped without leaving a hint. SP.insertLocal() should be using LocalMutationRunnable instead.

Note: hints are the context here, not consistency.",,aleksey,cburroughs,rcoli,slebresne,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Dec/13 13:34;aleksey;6510.txt;https://issues.apache.org/jira/secure/attachment/12619553/6510.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,364665,,,Mon Feb 03 21:58:44 UTC 2014,,,,,,,,,,"0|i1qutb:",364965,,,,,,,,,jbellis,,jbellis,Normal,,1.2.1,,,,,,,,,,,,,,,,"19/Dec/13 13:38;jbellis;History: this was supposed to be fixed by CASSANDRA-4753 but inexplicably I only changed the counters write path.;;;","19/Dec/13 13:39;jbellis;+1;;;","19/Dec/13 13:58;aleksey;Committed, thanks.;;;","19/Dec/13 14:26;slebresne;bq. but inexplicably I only changed the counters write path.

We all know that it's because you love that path so much.;;;","03/Feb/14 21:30;rcoli;This bug seems to have the implication that no ConsistencyLevel has had its supposed meaning for the duration of the bug, because there is no guarantee that the acknowledged-to-the-client local write actually succeeds? Is that correct?

If so, this issue seems quite fundamental and serious; why did automated testing not surface it? Is there now a test which covers this case?

What is the ""since"" for this issue? Looks like at least 1.2.0?;;;","03/Feb/14 21:40;jbellis;Nope, that's not the implication.  You can see from the code that {{responseHandler.response}} only gets called after {{rm.apply}}.  That is, no write is acknowledged if it hasn't actually been applied.;;;","03/Feb/14 21:58;rcoli;Thanks for the clarification. Others who look to JIRA to understand impact will appreciate not having to try to deduce it from reading the patch.

What is the ""since"" for this issue? Looks like at least 1.2.0?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstables from stalled repair sessions become live after a reboot and can resurrect deleted data,CASSANDRA-6503,12685433,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,jjordan,jjordan,18/Dec/13 18:52,16/Apr/19 09:31,14/Jul/23 05:53,31/Jan/14 13:31,1.2.14,2.0.5,,,,,1,,,,,"The sstables streamed in during a repair session don't become active until the session finishes.  If something causes the repair session to hang for some reason, those sstables will hang around until the next reboot, and become active then.  If you don't reboot for 3 months, this can cause data to resurrect, as GC grace has expired, so tombstones for the data in those sstables may have already been collected.",,cburroughs,garyogasawara,jasobrown,jjordan,kohlisankalp,marcuse,mishail,rcoli,rlow,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5412,CASSANDRA-6756,,,CASSANDRA-10797,,,,,,,,,,,"31/Jan/14 01:59;yukim;6503-2.0-followup.txt;https://issues.apache.org/jira/secure/attachment/12626245/6503-2.0-followup.txt","23/Jan/14 13:42;jasobrown;6503_2.0-v2.diff;https://issues.apache.org/jira/secure/attachment/12624797/6503_2.0-v2.diff","29/Jan/14 21:07;yukim;6503_2.0-v3.diff;https://issues.apache.org/jira/secure/attachment/12625969/6503_2.0-v3.diff","06/Jan/14 14:48;jasobrown;6503_c1.2-v1.patch;https://issues.apache.org/jira/secure/attachment/12621600/6503_c1.2-v1.patch",,,,,,,,,,,,,,,,,4.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,364510,,,Fri Jan 31 23:44:46 UTC 2014,,,,,,,,,,"0|i1qtv3:",364810,1.1.7,,,,,,,,yukim,,yukim,Low,,0.7.0,,,,,,,,,,,,,,,,"18/Dec/13 18:53;jjordan;One thing I was thinking that might help with this is if we could leave the sstables named ""-tmp"" until we are ready to make them become active.  That way when you reboot, any files hanging around will get removed on restart, instead of becoming active.;;;","18/Dec/13 22:06;jbellis;I don't think we should be messing with repair code in 1.2.14, not for something that it took 3 years for someone to run across.  Suggest targetting 2.0.;;;","19/Dec/13 23:10;jbellis;(Not actually sure this is still an issue in 2.0.x but it seems possible.);;;","19/Dec/13 23:42;yukim;In 1.2 as well as 2.0, every time the node receives SSTable file, it starts writing as 'tmp', but at the end of receiving, it calls SSTableWriter#closeAndOpenReader that moves received file out of 'tmp'. At this point, SSTables are still not added to ColumnFamilyStore, though if we shutdown the node and restart, the node would recognize received files even though streaming session was not finished successfully.

We need little tweak to defer renaming of SSTable files to do so after receiving all streamed files.
 ;;;","06/Jan/14 14:48;jasobrown;Attached patch, 6503_c1.2-v1, defers the release of the sstables to the CFS until the session is complete. Note: that patch is only for 1.2.

For c* 2.0, I'd like [~yukim]'s advice. I have a WIP here: https://github.com/jasobrown/cassandra/tree/6503_c2.0. The problem I'm running into is that FileMessage.sstable is of type SSTableReader, which we need on the sender side, but on the receiver side we want SSTableWriter (if we are going to defer the release of the sstables. For hacking things up sake, I've just changed FileMessage.sstable to a plain SSTable and let the users do the casting - which is only in two places, one of which is the FileMessage.Serializer.serailize() method. Not very extensive, but perhaps a bit sloppy.

Yuki, do you think it's worthwhile to split up the FileMessage object into two classes like OutFileMessage (which has a SSTR) and InFileMessage (which has SSTW)? ;;;","06/Jan/14 16:32;yukim;[~jasobrown] What I'm thinking to do is to closeAndOpenReader without renaming from tmp as we receive, and rename them at once at the end. And for renaming multiple files at once we probably want some kind of lockfile(also resolve CASSANDRA-2900?).

Though finalizing SSTable write in closeAndOpenReader takes some time, so completely defer finalize as you do may be a good idea. I think splitting FileMessage is better than casting.;;;","07/Jan/14 01:28;jasobrown;bq. we probably want some kind of lockfile

Interesting. What problems do you see this solving (I'm probably missing something in my understanding)?

bq. splitting FileMessage is better than casting

Yeah, I knew you were gonna say that :);;;","07/Jan/14 15:01;jasobrown;[~yukim] I can see how modifying closeAndOpenReader() would help here, but there's one wrinkle (I think): if we defer renaming the files from closeAndOpenReader(), we would need to rename the *open* SSTR, and there's a note in FileUtils.renameWithConfirm():

{code}        // this is not FSWE because usually when we see it it's because we didn't close the file before renaming it,
        // and Windows is picky about that.{code}

So I'm not sure about the deferred rename of the SSTR, assuming we care about Windows.
;;;","10/Jan/14 18:53;yukim;I think you are right. We don't want to close SSTR again for renaming.
Let's stick with deferring closeAndOpenReader as you did.
For the patch, I think it would be nice to call SSTW.abort() to discard already received file when something bad happened.

bq. Interesting. What problems do you see this solving (I'm probably missing something in my understanding)?

If the node goes down during we are doing closeAndOpenReader to received files, there is a chance we have already renamed files.
So I wanted to make sure the node won't read those files when the node come up.
;;;","10/Jan/14 19:30;jasobrown;bq. call SSTW.abort() ...

Makes sense, will add that in.

bq. If the node goes down during we are doing closeAndOpenReader to received files, there is a chance we have already renamed files

Ok, got it. Will come up with an idea and drop into the next patch. Thanks;;;","10/Jan/14 21:24;rcoli;What is the ""since"" version for this ticket?

It seems rather serious that users were still exposed to zombie data even if following repair best practice, if a repair session ever stalled... especially because repair sessions tend to stall. In a way we are fortunate that the only way to restart a failed repair session is to restart the nodes (CASSANDRA-3486) because this workaround has protected some users from this bug.

{quote}I don't think we should be messing with repair code in 1.2.14, not for something that it took 3 years for someone to run across. Suggest targetting 2.0.{quote}
The analysis of impact here seems confused. What has happened is not that it took 3 years for someone to encounter this bug; rather, it took 3 years for someone who ran across this very serious bug to notice and care enough to file a JIRA.

Bugs which violate guarantees in the way this one does (""I repaired once every gc_grace_seconds and still ended up with zombie data!"") should IMO be considered Major or Critical and should be candidates for fixes at any point in the release cycle. If we can entirely re-write a broken feature like replace_node in 1.2.12, I fail to see what keeps us from fixing buggy code that erroneously treats temporary files as permanent, in 1.2.14.;;;","10/Jan/14 21:52;jasobrown;bq. What is the ""since"" version for this ticket?

I suspect it's been around for a while - I only (knowingly) ran into the problem with 1.1.7.

bq. ""3 years""

The original descriptions says ""3 months"", and that is how long we took to run into the problem (at a serious level). I think perhaps [~jbellis] misspoke when he mentioned ""3 years"" in an earlier comment. 

As for fix version, I would like to see this in 1.2, as well, but it depends on the changes needed. At this point, though, the patch I have now (still WIP) is not too dramatic, so it could reasonably go into 1.2, but we should judge when it's ready/accepted.;;;","10/Jan/14 22:29;jbellis;I'm counting 3 years since 0.7 which is at least as long as repair has worked this way.;;;","10/Jan/14 22:48;jasobrown;[~jbellis] Ahh, OK. Thank you for clarifying.;;;","10/Jan/14 22:52;jjordan;I think most people notice they have hung repairs, and restart the nodes to clear them, before it becomes a problem.;;;","23/Jan/14 13:42;jasobrown;Attached v2 patch has the following changes:

- Changed StreamReceiveTask to keep a collection of SSTW rather than SSTR. This allows us to do the conversion of SSTW to SSTR all together after we've gotten all the streamed files. Also fixed up the code paths to here so they pass SSTW.

- Also in StreamReceiveTask, added an abort() method, which will discard the SSTWs it has buffered up. Changed StreamSession so that when a session ends in failure, it calls the new STR.abort() method.

- Split FileMessage out into IncomingFileMessage and OutgoingFileMessage. I needed to do this since as each one has a different subclass of SSTable, but also because java generics doesn't allow me to return different subclasses from StreamMessage.Serializer<V extends StreamMessage>. This necessitated the changes in StreamMessage as I couldn't have one serializer for both IncomingFileMessage and OutgoingFileMessage.  As it didn't seem best to create a new StreamMessage.Type (something like FILE_IN and FILE_OUT) just to represent the FILE message type's behavior on inbound vs. outbound, I instead split the SM.Type.serializer into two variables: inSerializer and outSerializer. For all the other Type's, the in and out serializers are the same class; in the case of Type.FILE, this is where I'm referencing IncomingFileMessage.serializer and OutgoingFileMessage.serializer, respectively. This seemed the cleanest way to introduce the now-bifurcated life of Type.FILE/FileMessage.

- added StreamLockfile to satisfy [~yukim]'s request for a mechanism to remove, on restart, the subset of SSTRs that were successfully converted when others from it's stream session failed. Assumes the process crashed in the middle of converting the SSTWs to SSTRs.

In the first patch, I chose to write the lockfile out to the commitlog directory. I did this as it seems like overkill to add another yaml setting (and Config/DD change) just for this value. Thus, I wanted to piggyback off something else that we already have, and DD.getCommitLogDirectory seemed the least worst. I'm open to suggestions on this.

Once these changes are incorporated into 2.0 and trunk, I would still like to do something for 1.2 but I do not think we need to be as extensive as what we're doing for 2.0+. Perhaps leave out the lockfile and the abort(), and just leave the deferring of converting SSTW to SSTR until the end of the session (basically what the current 1.2 patch does, but I'll check it out again after the 2.0 stuff is good).
;;;","29/Jan/14 21:07;yukim;bq. In the first patch, I chose to write the lockfile out to the commitlog directory. 

How about using SSTable directory itself? You can access it using Directories object so it is easy to perform delete if we do it inside CFS.scrubDataDirectories. Attaching patch for this.

Other than that, it seems good.

bq.  ...I would still like to do something for 1.2 but I do not think we need to be as extensive as what we're doing for 2.0+. Perhaps leave out the lockfile and the abort(), and just leave the deferring of converting SSTW to SSTR until the end of the session...

I agree. We can commit attached 1.2 patch as well.;;;","30/Jan/14 18:01;jasobrown;Committed the 1.2 patch to 1.2, have a few questions for [~yukim] about 2.0 patch that I'll add here in a minute (after coffee);;;","30/Jan/14 19:32;jasobrown;bq. How about using SSTable directory itself?

I think that is legit as StreamReceiveTask is specific to a CF.;;;","31/Jan/14 01:59;yukim;[~jasobrown] Looks like we need to make a little tweak around complete message since we moved adding received SSTables to different thread.
It is breaking StreamingTransferTest.

Patch attach for fix.;;;","31/Jan/14 10:58;marcuse;[~yukim] i committed the followup patch to 2.0, revert if that was a bad idea :);;;","31/Jan/14 14:23;jasobrown;The followup patch is fine, but I'm not quite sure why it is needed. How does being in a different thread affect the sending of the CompleteMessage, which doesn't look like it was there before?;;;","31/Jan/14 23:44;yukim;[~jasobrown] Complete message exchange was actually fragile before and could leave streaming session to WAIT_COMPLETE state on one side.

Only bellow pattern worked, and it worked because we were sending complete as we receive FileMessage in the same thread.

{code}
(A) ---> File     ---> (B) ...1
(A) <--- Complete <--- (B) ...2
(A) ---> Complete ---> (B) ...3
{code}

But now finalizing all received files moved to another thread. So sending receiving complete from A(3) gets first and B terminates it session without sending back complete, leaving A as WAIT_COMPLETE. Thus we needed to make sure to send complete message.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Null pointer exception in custom secondary indexes,CASSANDRA-6498,12685142,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mafernandez,adelapena,adelapena,17/Dec/13 10:24,16/Apr/19 09:31,14/Jul/23 05:53,15/Jan/14 18:09,2.1 beta1,,,Feature/2i Index,,,2,2i,secondary_index,secondaryIndex,,"StorageProxy#estimateResultRowsPerRange raises a null pointer exception when using a custom 2i implementation that not uses a column family as underlying storage:
{code}
resultRowsPerRange = highestSelectivityIndex.getIndexCfs().getMeanColumns();
{code}
According to the documentation, the method SecondaryIndex#getIndexCfs should return null when no column family is used.",,aagea,adelapena,mafernandez,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/14 09:13;mafernandez;CASSANDRA-6498.patch;https://issues.apache.org/jira/secure/attachment/12622822/CASSANDRA-6498.patch",,,,,,,,,,,,,,,,,,,,1.0,mafernandez,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,364219,,,Wed Jan 15 18:09:28 UTC 2014,,,,,,,,,,"0|i1qs2n:",364519,2.1 rc3,,,,,,,,samt,,samt,Low,,,,,,,,,,,,,,,,,,"14/Jan/14 09:13;mafernandez;In order to avoid this null pointer exception, we shouldn't assume that highestSelectivityIndex (which is a SecondaryIndex) has a IndexCfs because that depends on the implementation type.
 
Therefore, a nice way to solve this issue would be to include an abstract method in the SecondaryIndex class, add a implementation of the method where we really know there is a IndexCfs and otherwise delegate the implementation of this method to those who are creating a custom 2i.

I submit a patch that implements this solution.;;;","14/Jan/14 17:45;jbellis;WDYT [~beobal]?;;;","15/Jan/14 14:23;adelapena;I have been assigned to this bug, the provided patch seems to work fine.
Could the reviewer please review (and, hopefully, commit) the patch?

Thanks;;;","15/Jan/14 14:36;aagea;+1;;;","15/Jan/14 14:45;samt;+1 patch looks good to me. 

In fact, the same issue affects Solr backed indexes in DSE and our approach is the same.;;;","15/Jan/14 17:35;mafernandez;+1

I've been running some tests with this patch and there were no errors.;;;","15/Jan/14 18:09;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Endless L0 LCS compactions,CASSANDRA-6496,12685087,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,ngrigoriev,ngrigoriev,17/Dec/13 03:00,16/Apr/19 09:31,14/Jul/23 05:53,17/Dec/13 20:35,2.0.4,,,,,,0,compaction,lcs,,,"I have first described the problem here: http://stackoverflow.com/questions/20589324/cassandra-2-0-3-endless-compactions-with-no-traffic

I think I have really abused my system with the traffic (mix of reads, heavy updates and some deletes). Now after stopping the traffic I see the compactions that are going on endlessly for over 4 days.

For a specific CF I have about 4700 sstable data files right now.  The compaction estimates are logged as ""[3312, 4, 0, 0, 0, 0, 0, 0, 0]"". sstable_size_in_mb=256.  3214 files are about 256Mb (+/1 few megs), other files are smaller or much smaller than that. No sstables are larger than 256Mb. What I observe is that LCS picks 32 sstables from L0 and compacts them into 32 sstables of approximately the same size. So, what my system is doing for last 4 days (no traffic at all) is compacting groups of 32 sstables into groups of 32 sstables without any changes. Seems like a bug to me regardless of what did I do to get the system into this state...
","Cassandra 2.0.3, Linux, 6 nodes, 5 disks per node",marcuse,ngrigoriev,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/13 14:39;jbellis;6496.txt;https://issues.apache.org/jira/secure/attachment/12619110/6496.txt","17/Dec/13 03:28;ngrigoriev;system.log.1.gz;https://issues.apache.org/jira/secure/attachment/12619027/system.log.1.gz","17/Dec/13 03:28;ngrigoriev;system.log.gz;https://issues.apache.org/jira/secure/attachment/12619028/system.log.gz",,,,,,,,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,364164,,,Tue Dec 17 20:35:58 UTC 2013,,,,,,,,,,"0|i1qrqf:",364464,,,,,,,,,marcuse,,marcuse,Normal,,2.0 beta 1,,,,,,,,,,,,,,,,"17/Dec/13 03:14;jbellis;Can you enable debug logging in o.a.c.db.compaction and post a log sample?;;;","17/Dec/13 03:28;ngrigoriev;Attaching the logs. I have enabled the compaction logging this morning to get a slight idea of what was going on. ;;;","17/Dec/13 08:26;marcuse;i think this might be a duplicate of CASSANDRA-6284;;;","17/Dec/13 11:47;ngrigoriev;One thing I forgot to mention about the logs - I have reduced the number of compactors to one when enabling the debugging. Since at that point it was clear that something was wrong I was looking for clarity, not performance :);;;","17/Dec/13 14:39;jbellis;Patch to remove sstable output size limit when we're supposed to be doing STCS in L0.

- Chose to use LCT w/ unlimited size instead of normal CT since that seems less fragile (e.g. if we decide CT.level() should return -1)
- Some churn to standardize on limiting in Bytes over MB
;;;","17/Dec/13 17:16;ngrigoriev;Cool!!!!! I have got the source tagged 2.0.3, applied the patch, recompiled, restarted the node. Clearly now it compacts the groups of 32 L0 sstables into  large ones. I see that it just did one round and created 8Gb sstable from 32 256Mb ones.

Thanks a lot for the patch! I will revert the compaction settings to give it enough resources and let it complete its job to see the end results before I restart the test traffic.;;;","17/Dec/13 18:51;marcuse;[~ngrigoriev] thanks for testing

and patch lgtm, +1;;;","17/Dec/13 20:35;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOCAL_SERIAL  use QUORUM consistency level to validate expected columns,CASSANDRA-6495,12685086,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,kohlisankalp,kohlisankalp,kohlisankalp,17/Dec/13 02:59,25/Oct/19 13:11,14/Jul/23 05:53,14/Jan/14 17:54,2.0.5,,,Feature/Lightweight Transactions,,,0,LWT,,,,"If CAS is done at LOCAL_SERIAL consistency level, only the nodes from the local data center should be involved. 
Here we are using QUORAM to validate the expected columns. This will require nodes from more than one DC. 
We should use LOCAL_QUORAM here when CAS is done at LOCAL_SERIAL. 

Also if we have 2 DCs with DC1:3,DC2:3, a single DC down will cause CAS to not work even for LOCAL_SERIAL. 

",,kohlisankalp,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6939,,,,,,,,,,,,,,,"14/Jan/14 08:07;kohlisankalp;trunk_6495.diff;https://issues.apache.org/jira/secure/attachment/12622812/trunk_6495.diff",,,,,,,,,,,,,,,,,,,,1.0,kohlisankalp,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,364163,,,Tue Jan 14 17:54:19 UTC 2014,,,,,,,,,,"0|i1qrq7:",364463,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"19/Dec/13 06:38;kohlisankalp;[~jbellis]
For this should we use LOCAL_QUORAM for LOCAL_SERIAL or use the consistency level of commit. I think adding a third CL for this will be too confusing. So I think we can use CL of commit for validating columns.  ;;;","19/Dec/13 09:25;slebresne;Pretty sure we *must* use LOCAL_QUORUM. LOCAL_SERIAL should still provided serializability within the local data-center and this require doing a LOCAL_QUORUM. Using the CL of the commit would be incorrect in most case.;;;","19/Dec/13 13:29;jbellis;I think that ""cas"" CL of SERIAL -> read at Q, LOCAL_SERIAL -> LQ.;;;","20/Dec/13 04:57;kohlisankalp;OK. Let me do that;;;","14/Jan/14 17:54;jbellis;committed the StorageProxy change.

Leaving the AbstractPaxosCallback timeout alone; the intention is that CasContentionTimeout is for ""the replicas are responding normally, but I couldn't get a ballot accepted within X seconds because I'm competing with other transactions.""  So using WriteTimeout for a response to a single prepare or propose is correct.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions when a second Datacenter is Added,CASSANDRA-6493,12685014,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,rspitzer,rspitzer,16/Dec/13 18:38,16/Apr/19 09:31,14/Jul/23 05:53,18/Dec/13 17:04,,,,,,,0,,,,,"On adding a second datacenter several exceptions were raised.

Test outline:
Start 25 Node DC1
Keyspace Setup Replication 3
Begin insert against DC1 Using Stress
While the inserts are occuring
Start up 25 Node DC2
Alter Keyspace to include Replication in 2nd DC
Run rebuild on DC2
Wait for stress to finish
Run repair on Cluster
... Some other operations

At the point when the second datacenter is added several warnings go off because nodetool status is not functioning, and a few moments later the start operation reports a failure because a node has not successfully turned on. 

The first start attempt yielded the following exception on a node in the second DC.

{code}
CassandraDaemon.java (line 464) Exception encountered during startup
java.lang.AssertionError: -7560216458456714666 not found in -9222060278673125462, -9220751250790085193, ..... ALL THE TOKENS ...,  9218575851928340117, 9219681798686280387
at org.apache.cassandra.locator.TokenMetadata.getPredecessor(TokenMetadata.java:752)
at org.apache.cassandra.locator.TokenMetadata.getPrimaryRangesFor(TokenMetadata.java:696)
at org.apache.cassandra.locator.TokenMetadata.getPrimaryRangeFor(TokenMetadata.java:703)
at org.apache.cassandra.locator.AbstractReplicationStrategy.getRangeAddresses(AbstractReplicationStrategy.java:187)
at org.apache.cassandra.dht.RangeStreamer.getAllRangesWithSourcesFor(RangeStreamer.java:147)
at org.apache.cassandra.dht.RangeStreamer.addRanges(RangeStreamer.java:121)
at org.apache.cassandra.dht.BootStrapper.bootstrap(BootStrapper.java:81)
at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:979)
at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:745)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:586)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:483)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
{code}

The test automatically tries to restart nodes if they fail during startup, The second attempt for this node succeeded but a 'nodetool status' still failed and a different node in the second DC logged the following and failed to start up.

{code}
ERROR [main] 2013-12-16 18:02:04,869 CassandraDaemon.java (line 464) Exception encountered during startup
java.util.ConcurrentModificationException
	at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1115)
	at java.util.TreeMap$KeyIterator.next(TreeMap.java:1169)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3382)
	at org.apache.commons.lang.StringUtils.join(StringUtils.java:3444)
	at org.apache.cassandra.locator.TokenMetadata.getPredecessor(TokenMetadata.java:752)
	at org.apache.cassandra.locator.TokenMetadata.getPrimaryRangesFor(TokenMetadata.java:696)
	at org.apache.cassandra.locator.TokenMetadata.getPrimaryRangeFor(TokenMetadata.java:703)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.getRangeAddresses(AbstractReplicationStrategy.java:187)
	at org.apache.cassandra.dht.RangeStreamer.getAllRangesWithSourcesFor(RangeStreamer.java:147)
	at org.apache.cassandra.dht.RangeStreamer.addRanges(RangeStreamer.java:121)
	at org.apache.cassandra.dht.BootStrapper.bootstrap(BootStrapper.java:81)
	at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:979)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:745)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:586)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:483)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
ERROR [StorageServiceShutdownHook] 2013-12-16 18:02:04,876 CassandraDaemon.java (line 191) Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopNativeTransport(StorageService.java:358)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:373)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:89)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:551)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:724)
{code}

","Ubuntu, EC2 M1.large",mishail,rspitzer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,364091,,,Wed Dec 18 16:53:11 UTC 2013,,,,,,,,,,"0|i1qra7:",364391,1.2.13,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"16/Dec/13 18:43;rspitzer;https://cassci.datastax.com/job/cassandra-addremovedc/25/console

The ""Node down Detected"" are messages from a thread which runs nodetool status every ~2 seconds and counts how many nodes report themselves as up, the lack of a command line output shows the command failed. ;;;","16/Dec/13 23:20;rspitzer;I was able to get the same results repeating the test.
https://cassci.datastax.com/job/cassandra-addremovedc/26/console;;;","17/Dec/13 00:21;jbellis;From chat, this does not reproduce when CASSANDRA-6488 is reverted.;;;","17/Dec/13 00:31;rspitzer;Correct I didn't see this over several runs over the weekend testing on the pre-6488 build. Head of the git log from that build

{code}
commit c133ff88982948fdb12669bf766e9848102a3496
Author: Russell Spitzer <Russell.Spitzer@gmail.com>
Date:   Fri Dec 13 12:00:53 2013 -0800

    Patch to fix NPE ( this is patch a3d91dc9d67572e16d9ad92f22b89eb969373899)

commit 11455738fa61c6eb02895a5a8d3fbbe4d8cb24b4
Author: Brandon Williams <brandonwilliams@apache.org>
Date:   Fri Dec 13 12:10:47 2013 -0600

    Pig: don't assume all DataBags are DefaultDataBags
    Patch by Mike Spertus, reviewed by brandonwilliams for CASSANDRA-6420
{code};;;","18/Dec/13 16:53;rspitzer;Fixed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Please delete old releases from mirroring system,CASSANDRA-6490,12684870,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,tjake,sebb,sebb,16/Dec/13 00:21,16/Apr/19 09:31,14/Jul/23 05:53,22/Jun/15 16:10,,,,,,,0,,,,,"To reduce the load on the ASF mirrors, projects are required to delete old releases [1]

Please can you remove all non-current releases?
Thanks!
[Note that older releases are always available from the ASF archive server]

Any links to older releases on download pages should first be adjusted to point to the archive server.

[1] http://www.apache.org/dev/release.html#when-to-archive",http://www.apache.org/dist/cassandra/,sebb,slebresne,tjake,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,tjake,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,363942,,,Wed Jun 24 14:55:04 UTC 2015,,,,,,,,,,"0|i1qqef:",364248,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"16/Dec/13 00:23;sebb;The download page [1] shows that the only supported releases are 2.0.x 1.2.x and 1.1.x

Please delete all the older releases from

http://www.apache.org/dist/cassandra/

[1] http://cassandra.apache.org/download/;;;","16/Dec/13 10:39;slebresne;Done ([~urandom], can you check the debian/dists/ directory and delete the 06x and 07x directories? I don't seem to have the right to do so and they don't point at anything existing anymore).;;;","16/Dec/13 14:24;sebb;There is a problem with the directory protections:

drwxrwxr-x  3 eevans     eevans     6 Aug 20  2012 06x
drwxrwxr-x  3 eevans     eevans     6 Aug 20  2012 07x
drwxr-xr-x  3 slebresne  cassandra  6 May 27  2013 11x
drwxr-xr-x  3 slebresne  cassandra  6 Nov 25 08:11 12x
drwxr-xr-x  3 slebresne  cassandra  6 Nov 25 08:40 20x
drwxrwxr-x  3 apbackup   cassandra  6 Sep 10  2012 sid
drwxrwxr-x  3 eevans     eevans     6 Aug 20  2012 unstable

Only 'sid' above is correct.

The file group should be cassandra, and files should be group-writable otherwise only the owner can change things.
Which is awkward when the individual is temporarily unavailable.

However please note that Infra are moving towards all projects using svnpubsub [1] for releases - which avoids all such issues.
I suggest you file an Infra request now so you are ready for the next release.

[1] http://www.apache.org/dev/release-publishing.html#distribution_dist;;;","16/Dec/13 15:39;urandom;bq. Done (Eric Evans, can you check the debian/dists/ directory and delete the 06x and 07x directories? I don't seem to have the right to do so and they don't point at anything existing anymore).

Done.;;;","16/Dec/13 16:00;sebb;There's still a problem with some of the protections:

drwxr-xr-x  3 slebresne  cassandra  6 May 27  2013 11x
drwxr-xr-x  3 slebresne  cassandra  6 Nov 25 08:11 12x
drwxr-xr-x  3 slebresne  cassandra  6 Nov 25 08:40 20x

These should be changed - by slebresne - to allow group-write;;;","16/Dec/13 16:37;slebresne;Right, right, fixed.;;;","21/Jun/15 13:45;sebb;There are several old releases still on the mirror system under

https://dist.apache.org/repos/dist/release/cassandra/

Please delete the following:

2.0.14
2.1.4
2.1.5

Also, please can you update the release process so that older releases are removed from the mirror system when a new release has been uploaded?
Deletes should be done a few days after the new release has been announced.;;;","22/Jun/15 16:10;tjake;I've cleaned up the system but I will say, I like to include the latest and previous releases since when a release happens we wait to update the website till the mirrors are updated (otherwise users get a 404). If we push delete the old release and add the new then users get the worst of both worlds during the transition. 404 on the old versions and 404 on the new till all mirrors are updated.  ;;;","24/Jun/15 14:55;sebb;The recommended way to do this is as follows:

Following a successful release vote:
- publish the release artifacts (add to dist/release/cassandra)
- publish Maven artifacts via Nexus (if relevant)
- wait a day for mirrors to catch up (most will do so in less than a day)
- update the website download page to point to the new releases; older releases links should point to the archive server (this is published immediately)
- send the announce message
- a few days later, delete the older releases from dist/release/cassandra

That should not result in any 404s.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batchlog writes consume unnecessarily large amounts of CPU on vnodes clusters,CASSANDRA-6488,12684588,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,rbranson,rbranson,rbranson,14/Dec/13 00:49,16/Apr/19 09:31,14/Jul/23 05:53,17/Dec/13 14:50,1.2.13,2.0.4,,,,,0,,,,,"The cloneTokenOnlyMap call in StorageProxy.getBatchlogEndpoints causes enormous amounts of CPU to be consumed on clusters with many vnodes. I created a patch to cache this data as a workaround and deployed it to a production cluster with 15,000 tokens. CPU consumption drop to 1/5th. This highlights the overall issues with cloneOnlyTokenMap() calls on vnodes clusters. I'm including the maybe-not-the-best-quality workaround patch to use as a reference, but cloneOnlyTokenMap is a systemic issue and every place it's called should probably be investigated.",,aleksey,jeromatron,mshuler,rbranson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Dec/13 14:25;aleksey;6488-fix.txt;https://issues.apache.org/jira/secure/attachment/12619106/6488-fix.txt","14/Dec/13 00:49;rbranson;6488-rbranson-patch.txt;https://issues.apache.org/jira/secure/attachment/12618736/6488-rbranson-patch.txt","14/Dec/13 15:38;jbellis;6488-v2.txt;https://issues.apache.org/jira/secure/attachment/12618777/6488-v2.txt","15/Dec/13 10:24;aleksey;6488-v3.txt;https://issues.apache.org/jira/secure/attachment/12618811/6488-v3.txt","14/Dec/13 00:51;rbranson;graph (21).png;https://issues.apache.org/jira/secure/attachment/12618737/graph+%2821%29.png",,,,,,,,,,,,,,,,5.0,rbranson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,363660,,,Tue Dec 17 16:18:16 UTC 2013,,,,,,,,,,"0|i1qonz:",363966,1.2.11,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"14/Dec/13 00:51;rbranson;CPU usage dropping on a production cluster after the attached patch is rolled out.;;;","14/Dec/13 15:38;jbellis;v2 to move the caching logic inside cloneOnlyTokenMap;;;","14/Dec/13 15:39;jbellis;NB: I'm not sure what the changes to candidates/chosenEndpoints do so I've left that out for now.;;;","15/Dec/13 10:24;aleksey;v3 merges both and has some minor (stylistic) changes to SP on top.;;;","15/Dec/13 10:31;aleksey;Committed in 4be9e6720d9f94a83aa42153c3e71ae1e557d2d9.;;;","16/Dec/13 16:08;mshuler;This introduced a failure in BootStrapperTest:

{code}
test:
     [echo] running unit tests
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/test/cassandra
    [mkdir] Created dir: /home/mshuler/git/cassandra/build/test/output
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/home/mshuler/git/cassandra/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.dht.BootStrapperTest
    [junit] Tests run: 4, Failures: 1, Errors: 0, Time elapsed: 6.177 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit]  WARN 09:47:46,135 No host ID found, created 9019bb70-4d6e-4cf6-b730-140ff5ae4be5 (Note: This should happen exactly once per node).
    [junit]  WARN 09:47:46,262 Generated random token [d9180feb2e806704effa4024e8f4c631]. Random tokens will result in an unbalanced ring; see http://wiki.apache.org/cassandra/Operations
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testSourceTargetComputation(org.apache.cassandra.dht.BootStrapperTest):   FAILED
    [junit] expected:<1> but was:<0>
    [junit] junit.framework.AssertionFailedError: expected:<1> but was:<0>
    [junit]     at org.apache.cassandra.dht.BootStrapperTest.testSourceTargetComputation(BootStrapperTest.java:212)
    [junit]     at org.apache.cassandra.dht.BootStrapperTest.testSourceTargetComputation(BootStrapperTest.java:173)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.dht.BootStrapperTest FAILED

BUILD FAILED
/home/mshuler/git/cassandra/build.xml:1113: The following error occurred while executing this line:
/home/mshuler/git/cassandra/build.xml:1078: Some unit test(s) failed.

Total time: 9 seconds
((4be9e67...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
4be9e6720d9f94a83aa42153c3e71ae1e557d2d9 is the first bad commit
commit 4be9e6720d9f94a83aa42153c3e71ae1e557d2d9
Author: Aleksey Yeschenko <aleksey@apache.org>
Date:   Sun Dec 15 13:29:56 2013 +0300

    Improve batchlog write performance with vnodes
    
    patch by Jonathan Ellis and Rick Branson; reviewed by Aleksey Yeschenko
    for CASSANDRA-6488

:100644 100644 e5865925f160faabc2506c3a5aac9985c17c1658 b55393b2ed138011bab52f95f2e9b52107709938 M      CHANGES.txt
:040000 040000 dea10aa8044e10eb60002e75f2586a9c8e94b647 7030c09f9713bd3e342e4e012c59b09c86b79a42 M      src
{code};;;","16/Dec/13 16:18;mshuler;I'm working on the cassandra-2.0 branch, since I didn't mention it above. Around the same time, LeaveAndBootstrapTest, MoveTest, and RelocateTest were new failures - I'm looking at those
- http://cassci.datastax.com/job/cassandra-2.0_test/49/console
;;;","16/Dec/13 16:22;aleksey;So, the caching part. [~jbellis] can you have a look? If not, I will, later, but it's potentially 1.2.13 vote-affecting.;;;","16/Dec/13 16:31;mshuler;Commit bb09d3c fully passed all the unit tests in cassandra-2.0 branch.
- http://cassci.datastax.com/job/cassandra-2.0_test/47/console;;;","16/Dec/13 16:47;mshuler;Those same tests look like new failures with this commit in cassandra-1.2 branch also
- http://cassci.datastax.com/job/cassandra-1.2_test/32/console
vs.
- http://cassci.datastax.com/job/cassandra-1.2_test/33/console

(edit for clarity) New unit test failures in c-2.0 and c-1.2 branches with this commit:
- BootStrapperTest
- LeaveAndBootstrapTest
- MoveTest
- RelocateTest;;;","17/Dec/13 14:25;aleksey;Separates TM.cloneOnlyTokenMap() and TM.cachedOnlyTokenMap() and only switched SP.getBatchlogEndpoints() and ARS.getNaturalEndpoints() to use the cached version.

They aren't the only methods that *don't* mutate the returned metadata, but going through the rest of the usages and optimizing those can wait.

Also fixes a regression from 6435 in TM.cachedOnlyTokenMap().;;;","17/Dec/13 14:50;jbellis;updated comments and committed;;;","17/Dec/13 16:18;mshuler;cassandra-1.2 branch, commit 13348c4, is passing these 4 unit tests:
- http://cassci.datastax.com/job/cassandra-1.2_test/35/console

cassandra-2.0 is passing these, also
- http://cassci.datastax.com/job/cassandra-2.0_test/50/console

Thanks all!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in calculateNaturalEndpoints,CASSANDRA-6485,12684529,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,rspitzer,rspitzer,13/Dec/13 19:41,16/Apr/19 09:31,14/Jul/23 05:53,14/Dec/13 04:10,1.2.13,2.0.4,,,,,0,,,,,"I was running a test where I added a new data center to an existing cluster. 

Test outline:
Start 25 Node DC1
Keyspace Setup Replication 3
Begin insert against DC1 Using Stress
While the inserts are occuring
Start up 25 Node DC2
Alter Keyspace to include Replication in 2nd DC
Run rebuild on DC2
Wait for stress to finish
Run repair on Cluster
... Some other operations

Although there are no issues with smaller clusters or clusters without vnodes, Larger setups with vnodes seem to consistently see the following exception in the logs as well as a write operation failing for each exception. Usually this happens between 1-8 times during an experiment. 

The exceptions/failures are Occurring when DC2 is brought online but *before* any alteration of the Keyspace. All of the exceptions are happening on DC1 nodes. One of the exceptions occurred on a seed node though this doesn't seem to be the case most of the time. 

While the test was running, nodetool was run every second to get cluster status. At no time did any nodes report themselves as down. 


{code}
ystem_logs-107.21.186.208/system.log-ERROR [Thrift:1] 2013-12-13 06:19:52,647 CustomTThreadPoolServer.java (line 217) Error occurred during processing of message.
system_logs-107.21.186.208/system.log:java.lang.NullPointerException
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:128)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:2624)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:375)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:190)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:866)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:849)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:749)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.getResult(Cassandra.java:3690)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.getResult(Cassandra.java:3678)
system_logs-107.21.186.208/system.log-	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
system_logs-107.21.186.208/system.log-	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
system_logs-107.21.186.208/system.log-	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
system_logs-107.21.186.208/system.log-	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
system_logs-107.21.186.208/system.log-	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
system_logs-107.21.186.208/system.log-	at java.lang.Thread.run(Thread.java:724)
{code}",,cburroughs,rbranson,rspitzer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Dec/13 19:51;jbellis;6485.txt;https://issues.apache.org/jira/secure/attachment/12618670/6485.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,363601,,,Mon Dec 16 16:19:33 UTC 2013,,,,,,,,,,"0|i1qoav:",363907,1.2.13,,,,,,,,rbranson,,rbranson,Normal,,1.2.13,,,,,,,,,,,,,,,,"13/Dec/13 19:52;jbellis;It's possible for TM to get nulled out after we check it.  Cache a reference to any non-null TM we observe to fix.;;;","14/Dec/13 00:56;rbranson;LGTM.;;;","14/Dec/13 04:10;jbellis;committed;;;","16/Dec/13 16:19;rspitzer;Patch worked on my test. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible Collections.sort assertion failure in STCS.filterColdSSTables,CASSANDRA-6483,12684385,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,graham sanderson,graham sanderson,13/Dec/13 00:42,16/Apr/19 09:31,14/Jul/23 05:53,13/Dec/13 23:13,2.0.4,,,,,,0,compaction,,,,"We have observed the following stack trace periodically:

{code}
java.lang.IllegalArgumentException: Comparison method violates its general contract!
        at java.util.TimSort.mergeLo(TimSort.java:747)
        at java.util.TimSort.mergeAt(TimSort.java:483)
        at java.util.TimSort.mergeCollapse(TimSort.java:410)
        at java.util.TimSort.sort(TimSort.java:214)
        at java.util.TimSort.sort(TimSort.java:173)
        at java.util.Arrays.sort(Arrays.java:659)
        at java.util.Collections.sort(Collections.java:217)
        at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.filterColdSSTables(SizeTieredCompactionStrategy.java:94)
        at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundSSTables(SizeTieredCompactionStrategy.java:59)
        at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:229)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:191)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{code}

The comparator ant SizeTieredCompactionStrategy line 94 breaks the assertions in the new JDK7 default sort algorithm, because (I think just) the hotness value (based on meter) may be modified concurrently by another thread

This bug appears to have been introduced in CASSANDRA-6109
",,graham sanderson,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-8885,CASSANDRA-6109,,,,,,,,,,"13/Dec/13 21:31;thobbs;6483-2.0-v1.patch;https://issues.apache.org/jira/secure/attachment/12618692/6483-2.0-v1.patch",,,,,,,,,,,,,,,,,,,,1.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,363457,,,Fri Dec 13 23:13:53 UTC 2013,,,,,,,,,,"0|i1qnfb:",363763,,,,,,,,,jbellis,,jbellis,Normal,,2.0.3,,,,,,,,,,,,,,,,"13/Dec/13 00:43;graham sanderson;Note the java option java.util.Arrays.useLegacyMergeSort could be used as a workaround, but it is unclear to me if that would produce desirable results;;;","13/Dec/13 00:46;graham sanderson;The simplest fix is probably just to precompute an IdentityMap to any of the mutable data, and use it from within the comparator (since the comparator happens to be non static)

Alternatively use a List of a new wrapper type and sort that instead.;;;","13/Dec/13 03:53;graham sanderson;Adding my questions from dev-email thread

Note that the CASSANDRA-6109 feature claims to be “off” by default, however it isn’t immediately clear to me from that patch how “off” is implemented, and whether it is supposed to go down that code path even when “off""

I’m guessing there is no actual downside (other than ERROR level messages in the logs which cause alerts), since it just fails a subset of compactions?;;;","13/Dec/13 21:31;thobbs;Attached patch 6483-2.0-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6483]) builds a map of hotness values prior to the sort and uses that for comparisons.  I also made {{filterColdSSTables()}} skip unneeded work if we won't be able to filter anything anyway.;;;","13/Dec/13 21:37;thobbs;bq. Note that the CASSANDRA-6109 feature claims to be “off” by default, however it isn’t immediately clear to me from that patch how “off” is implemented, and whether it is supposed to go down that code path even when “off""

I answered this on the dev ML, but I'll repeat it here for others who are interested.  The default max_cold_reads_ratio is 0.0, so {{filterColdSSTables()}} shouldn't filter any SSTables.  When writing this patch, I realized that even with that set to 0.0, SSTables that have no read activity at all would still be filtered out.  However, after this patch, that's no longer true, and a setting of 0.0 will prevent any filtering at all.

bq. I’m guessing there is no actual downside (other than ERROR level messages in the logs which cause alerts), since it just fails a subset of compactions?

That's correct, this shouldn't cause any other problems, only delay some compactions.;;;","13/Dec/13 23:13;jbellis;LGTM, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Batchlog endpoint candidates should be picked randomly, not sorted by proximity",CASSANDRA-6481,12684310,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,12/Dec/13 17:47,16/Apr/19 09:31,14/Jul/23 05:53,12/Dec/13 17:57,1.2.13,2.0.4,,,,,0,,,,,"Batchlog endpoint candidates should be picked randomly, not sorted by proximity. I'll be lazy and just copy-paste some lines from IRC:

[20:23:27] rbranson:	 is there an issue where batch logs tend to get written to a subset of the nodes?
[20:28:04] rbranson:	 I mean all the write batches are going thru 10% of the nodes
[20:28:16] rbranson:	 it means writes won't scale linearly w/the cluster size

Attaching a trivial patch.",,aleksey,cburroughs,jasobrown,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/13 17:48;aleksey;6481.txt;https://issues.apache.org/jira/secure/attachment/12618447/6481.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,363382,,,Thu Dec 12 18:03:06 UTC 2013,,,,,,,,,,"0|i1qmyv:",363688,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"12/Dec/13 17:51;jbellis;If dsnitch actually worked then I think this would be fine, but fixing dsnitch is probably out of scope here so +1;;;","12/Dec/13 17:54;jasobrown;+1 on the patch, as well.

[~jbellis] In what way do you think dsnitch doesn't work?;;;","12/Dec/13 17:57;aleksey;Committed, thanks.;;;","12/Dec/13 18:03;jbellis;bq. In what way do you think dsnitch doesn't work

CASSANDRA-6465;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion error in MessagingService.addCallback,CASSANDRA-6476,12684100,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,iconara,iconara,11/Dec/13 20:20,16/Apr/19 09:31,14/Jul/23 05:53,24/Apr/14 15:31,1.2.17,,,,,,0,,,,,"Two of the three Cassandra nodes in one of our clusters just started behaving very strange about an hour ago. Within a minute of each other they started logging AssertionErrors (see stack traces here: https://gist.github.com/iconara/7917438) over and over again. The client lost connection with the nodes at roughly the same time. The nodes were still up, and even if no clients were connected to them they continued logging the same errors over and over.

The errors are in the native transport (specifically MessagingService.addCallback) which makes me suspect that it has something to do with a test that we started running this afternoon. I've just implemented support for frame compression in my CQL driver cql-rb. About two hours before this happened I deployed a version of the application which enabled Snappy compression on all frames larger than 64 bytes. It's not impossible that there is a bug somewhere in the driver or compression library that caused this -- but at the same time, it feels like it shouldn't be possible to make C* a zombie with a bad frame.

Restarting seems to have got them back running again, but I suspect they will go down again sooner or later.","Cassandra 2.0.2 DCE, Cassandra 1.2.15",elreydetodo@gmail.com,iconara,kohlisankalp,mishail,rfwagner@gmail.com,rlow,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Apr/14 21:05;brandon.williams;6476.txt;https://issues.apache.org/jira/secure/attachment/12641331/6476.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,363172,,,Thu Apr 24 15:31:35 UTC 2014,,,,,,,,,,"0|i1qlof:",363478,,,,,,,,,benedict,,benedict,Normal,,1.2.11,,,,,,,,,,,,,,,,"12/Dec/13 11:38;slebresne;MessagingService ain't the native transport (fyi, the native transport code doesn't leak outside the org.apache.cassandra.transport package), it's the intra-cluster messaging. In fact the stack trace shows that the write that trigger it don't even come from the native protocol but from thrift (which means you either use thrift for some things or something is whack).

But truth is, given the stack trace, where the writes comes from doesn't matter.  The assertion that fails is the line
{noformat}
assert previous == null;
{noformat}
in MessagingService.addCallback. And that's where things stop to make sense to me. This means that we tried to add a new message to the callback map but there was one with the same messageId already. Except that messageId is very straighforwardly generated by an {{incrementAndGet}} on an static AtomicInteger. And as far as I can tell, no other code inserts in the callback map without grabing a new messageId this way (except setCallbackForTests, but it does is only use in a unit test).

Therefore, it seems the only way such messageId conflict could happen is that we've gone full cycle on the AtomicInteger and hit the same id again. But entries in callbacks expire after the rpc timeout, so that implies > 4 billions requests in about 10 seconds. Sounds pretty unlikely to me.

But I might be missing something obvious: [~jbellis], I believe you might be more familiar with MessagingService, any idea?
;;;","12/Dec/13 12:06;iconara;Sorry, there was another stack trace I meant to attach to the same gist that said something about the native transport. I've added it now: https://gist.github.com/iconara/7917438 (see the second file). Those errors started with ""ERROR [Native-Transport-Requests:7924]"" which made me make the connection between us changing to compressed requests and the errors (since cql-rb only runs over the CQL protocol).

I've looked at the logs but my untrained eyes don't find any more hints as to what happened. I can post the full logs if that helps you.;;;","12/Dec/13 15:41;jbellis;bq. that's where things stop to make sense to me. This means that we tried to add a new message to the callback map but there was one with the same messageId already. Except that messageId is very straighforwardly generated by an incrementAndGet on an static AtomicInteger

Right.  I'm not sure why that assert even exists TBH; I have some idea that in the super distant past SP used to manually inject callbacks in some cases but if so that code is long dead.

Is it possible a bug in native compression code is corrupting random crap elsewhere in the JVM?;;;","12/Dec/13 16:18;slebresne;bq. Is it possible a bug in native compression code is corrupting random crap elsewhere in the JVM?

I have no clue, that would be a pretty serious JVM bug imo if that was the case. It would also be uncanny for ""random corrupted crap"" to trigger the same assertion on different nodes (but well, everything is possible). All I can say in that matter is that the native protocol uses the same compression libs than sstable compression and in basically the same way.;;;","12/Dec/13 17:14;jbellis;[~iconara] did you see the MS asserts on multiple nodes?;;;","12/Dec/13 19:35;iconara;[~jbellis] Yes, two out of three nodes got the same assertion failures within a minute or two.

I've updated the gist (https://gist.github.com/iconara/7917438) with the full logs (10,000 lines) from the two nodes. The third node has nothing in its logs around the same time (it's all just INFO and nothing that stands out).;;;","12/Dec/13 20:31;jbellis;Huh.  Well, I added some extra detail to the assert in c4d3a313885f14e802247b9354aafa4caaae9804.  Maybe that will show a clue.;;;","18/Apr/14 10:12;kohlisankalp;We saw these asserts on one node for sometime and then it went away. This is 1.2.15. We had some network problem around the same time. Don't know whether that is related. 
java.lang.AssertionError
	at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:541)
	at org.apache.cassandra.service.StorageProxy.sendMessagesToOneDCInternal(StorageProxy.java:638)
	at org.apache.cassandra.service.StorageProxy.sendMessagesToOneDC(StorageProxy.java:603)
	at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:530)
	at org.apache.cassandra.service.StorageProxy$2.apply(StorageProxy.java:121)
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:384)
	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:191)
	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:866)
	at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:849)
	at org.apache.cassandra.thrift.CassandraServer.internal_remove(CassandraServer.java:813)
	at org.apache.cassandra.thrift.CassandraServer.remove(CassandraServer.java:834)
	at org.apache.cassandra.thrift.Cassandra$Processor$remove.getResult(Cassandra.java:3642)
	at org.apache.cassandra.thrift.Cassandra$Processor$remove.getResult(Cassandra.java:3630)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722);;;","22/Apr/14 04:41;rlow;If System.nanoTime() went backwards then it could cause this. Messages won't be expired from the map and once we've sent 2 billion messages we'll reuse IDs. The instance Sankalp pasted the error from had processed approx 1 billion messages so it's conceivable it has added 2bn callbacks.

System.nanoTime() shouldn't go backwards though. It might on really old kernels but not on ours. But it's possible a hardware issue or kernel bug could cause this. It would have to be something correlated between nodes to explain Theo's issue.;;;","22/Apr/14 20:30;benedict;Isn't this most likely a duplicate of CASSANDRA-6948? Hit by never bouncing your node between bootstrap and hitting 4B+ messages, coupled with some dropped messages along the way, caused by shutting down the expiringmap reaper during bootstrap;;;","22/Apr/14 20:43;rlow;Yes, most likely it is. We see it correlated across nodes that were bootstrapped at the same time, which makes sense.;;;","22/Apr/14 20:46;benedict;Want to backport your patch [~brandon.williams]?;;;","22/Apr/14 20:52;brandon.williams;Unfortunately, I don't think your analysis is correct, because in 1.2 we only spin MS up/down for replace, not bootstrap.  The bootstrap shadow round was added in CASSANDRA-5571 which is 2.0-only.;;;","22/Apr/14 20:56;brandon.williams;[~rlow] are you doing a bootstrap, or are you doing a replace?;;;","22/Apr/14 20:57;rlow;Sorry, the instances we've seen were replaced, not bootstrapped. This is on 1.2.15.

Theo, had you replaced instances too?;;;","22/Apr/14 21:05;brandon.williams;Half a backport from CASSANDRA-6948 to only affect replace.;;;","24/Apr/14 09:04;benedict;LGTM;;;","24/Apr/14 15:31;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node hangs when Drop Keyspace / Table is executed,CASSANDRA-6472,12683871,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,amorton,amorton,10/Dec/13 17:04,16/Apr/19 09:31,14/Jul/23 05:53,04/Feb/14 20:17,2.1 beta1,,,,,,1,,,,,"from http://www.mail-archive.com/user@cassandra.apache.org/msg33566.html

CommitLogSegmentManager.flushDataFrom() returns a FutureTask to wait on the flushes, but the task is not started in flushDataFrom(). 

The CLSM manager thread does not use the result and forceRecycleAll (eventually called when making schema mods) does not start it so hangs when calling get().

plan to patch so flushDataFrom() returns a Future. ",,boneill42,jasonwee,mishail,rhatch,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6519,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,362943,,,Tue Feb 04 20:15:19 UTC 2014,,,,,,,,,,"0|i1qk9r:",363249,2.1 rc3,,,,,,,,,,,Low,,2.1 rc3,,,,,,,,,,,,,,,,"07/Jan/14 13:00;benedict;This is fixed as a byproduct of the changes in CASSANDRA-5549;;;","14/Jan/14 20:39;rhatch;This still appears to be happening.

Steps to reproduce:
{noformat}
ccm create test_cluster
ccm populate -n 1
ccm start
ccm node1 cqlsh

Connected to test_cluster at 127.0.0.1:9160.
[cqlsh 4.1.1 | Cassandra 2.1-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> create keyspace test_table_dropping with replication = {'class':'SimpleStrategy', 'replication_factor':1} ;
cqlsh> use test_table_dropping ;
cqlsh:test_table_dropping> CREATE TABLE simple_table (
                       ...   id uuid PRIMARY KEY,
                       ...   sometext text);
cqlsh:test_table_dropping> DROP TABLE simple_table;
{noformat}

At this point the cql session hangs. I don't see any exceptions in the log, but this message appears:
{noformat}
INFO  [Thrift:1] 2014-01-14 13:23:40,341 MigrationManager.java:288 - Drop ColumnFamily 'user_type_dropping/simple_table'
{noformat};;;","14/Jan/14 20:43;rhatch;I just noticed that if I ctrl-C, ctrl-D to kill my cql session, then open a new cql session the table is in fact gone. So the main problem is just cqlsh hanging after the statement.;;;","04/Feb/14 06:04;jbellis;[~mishail] can you shed any light on the cqlsh hang?;;;","04/Feb/14 19:49;mishail;Hmm, I can't reproduce that locally using the latest trunk. I've tried steps from this issue as well as from CASSANDRA-6519 . [~rhatch] could you please double check?;;;","04/Feb/14 20:15;rhatch;I'm no longer seeing the issue. I can drop keyspace/table now without a problem (also ran a dtest which relies on dropping and it worked fine).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Executing a prepared CREATE KEYSPACE multiple times doesn't work,CASSANDRA-6471,12683854,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,10/Dec/13 15:26,16/Apr/19 09:31,14/Jul/23 05:53,18/Dec/13 10:15,1.2.14,,,,,,0,,,,,"See user reports on the java driver JIRA: https://datastax-oss.atlassian.net/browse/JAVA-223. Preparing CREATE KEYSPACE queries is not particularly useful but there is no reason for it to be broken.

The reason is that calling KSPropDef/CFPropDef.validate() methods are not idempotent. Attaching simple patch to fix.",,amichai,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Dec/13 15:27;slebresne;6471.txt;https://issues.apache.org/jira/secure/attachment/12618049/6471.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,362926,,,Wed Dec 18 10:15:47 UTC 2013,,,,,,,,,,"0|i1qk67:",363232,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"12/Dec/13 08:11;amichai;Does this apply to 2.0.x as well?;;;","12/Dec/13 08:19;slebresne;It does.;;;","17/Dec/13 23:13;jbellis;+1;;;","18/Dec/13 10:15;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException on range query from client,CASSANDRA-6470,12683839,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,enrico.scalavino,enrico.scalavino,10/Dec/13 13:39,16/Apr/19 09:31,14/Jul/23 05:53,29/Jan/14 18:25,2.0.5,,,,,,2,,,,,"schema: 
{noformat}
CREATE TABLE inboxkeyspace.inboxes(user_id bigint, message_id bigint, thread_id bigint, network_id bigint, read boolean, PRIMARY KEY(user_id, message_id)) WITH CLUSTERING ORDER BY (message_id DESC);
CREATE INDEX ON inboxkeyspace.inboxes(read);
{noformat}

query: 
{noformat}
SELECT thread_id, message_id, network_id FROM inboxkeyspace.inboxes WHERE user_id = ? AND message_id < ? AND read = ? LIMIT ? 
{noformat}

The query works if run via cqlsh. However, when run through the datastax client, on the client side we get a timeout exception and on the server side, the Cassandra log shows this exception: 

{noformat}
ERROR [ReadStage:4190] 2013-12-10 13:18:03,579 CassandraDaemon.java (line 187) Exception in thread Thread[ReadStage:4190,5,main]
java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1940)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.cassandra.db.filter.SliceQueryFilter.start(SliceQueryFilter.java:261)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.makePrefix(CompositesSearcher.java:66)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.getIndexedIterator(CompositesSearcher.java:101)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:53)
        at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:537)
        at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1669)
        at org.apache.cassandra.db.PagedRangeCommand.executeLocally(PagedRangeCommand.java:109)
        at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1423)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1936)
        ... 3 more
{noformat}",,aleksey,dmnorc,enigmacurry,enrico.scalavino,jjordan,lyubent,marcostrama,mishail,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/14 23:16;enigmacurry;6470-reproduced.tar.gz;https://issues.apache.org/jira/secure/attachment/12625763/6470-reproduced.tar.gz","29/Jan/14 14:50;slebresne;6470.txt;https://issues.apache.org/jira/secure/attachment/12625871/6470.txt",,,,,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,362911,,,Wed Jan 29 18:25:44 UTC 2014,,,,,,,,,,"0|i1qk2v:",363217,,,,,,,,,lyubent,,lyubent,Normal,,,,,,,,,,,,,,,,,,"12/Dec/13 19:32;jbellis;Can you reproduce, Ryan?;;;","12/Dec/13 20:19;enigmacurry;[~enrico.scalavino] What version of Cassandra and datastax driver are you using? I'll try and recreate this, but if you have a test already written that is easily decoupled from your project can you post that too?;;;","12/Dec/13 22:38;jjordan;[~enrico.scalavino] are you using version 2.0.X of C* and the driver?  The limit being a bind parameter isn't supported unless you are. It may be confusing the driver if you are using the 1.0.X version of the driver (not sure what error gets thrown if you do that).;;;","13/Dec/13 03:09;marcostrama;I get the same error. I dont know when it has been started. I'm using Cassandra 2.0.2 and Datastax Java Driver 2.0.0-beta2. Query works in cqlsh but fail when running in the client. I tried to re-create (DROP/CREATE) the column family, but the error stills.

=============
Table layout:

cqlsh:pollkan> desc table observed;

CREATE TABLE observed (
  observed timeuuid,
  observer timeuuid,
  blocked boolean,
  PRIMARY KEY (observed, observer)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='99.0PERCENTILE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

CREATE INDEX observedBlocked ON observed (blocked);

=============
Query in the cqlsh:

cqlsh:pollkan> SELECT observer FROM observed WHERE observed = fa93c210-4bff-11e3-b48f-5714d8c6f3b2 AND observer > 00000000-0000-1000-0000-000000000000 and blocked = false LIMIT 10000;

 observer
--------------------------------------
 43814f60-5bb1-11e3-97c8-ad396a9e8180

(1 rows)

=============
Query in the client log:

2013-12-13/00:53:03.039/BRST [timeline_1] DEBUG br.com.pollkan.batch.CqlCommands Execute query [SELECT observer FROM observed WHERE observed = ? AND observer > ? and blocked = ? LIMIT 10000;] arguments [[fa93c210-4bff-11e3-b48f-5714d8c6f3b2][00000000-0000-1000-0000-000000000000][false]]

=============
Error in cassandra:

ERROR [ReadStage:52] 2013-12-13 01:04:56,799 CassandraDaemon.java (line 187) Exception in thread Thread[ReadStage:52,5,main]
java.lang.RuntimeException: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1931)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.ArrayIndexOutOfBoundsException: 0
        at org.apache.cassandra.db.filter.SliceQueryFilter.start(SliceQueryFilter.java:261)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.makePrefix(CompositesSearcher.java:66)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.getIndexedIterator(CompositesSearcher.java:101)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:53)
        at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:537)
        at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1649)
        at org.apache.cassandra.db.PagedRangeCommand.executeLocally(PagedRangeCommand.java:109)
        at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1414)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1927)
        ... 3 more

=============
Error from driver log:

2013-12-13/01:05:06.798/BRST [timeline_1] ERROR br.com.pollkan.batch.CqlCommands Exception! [Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)]
com.datastax.driver.core.exceptions.ReadTimeoutException: Cassandra timeout during read query at consistency ONE (1 responses were required but only 0 replica responded)
        at com.datastax.driver.core.exceptions.ReadTimeoutException.copy(ReadTimeoutException.java:69)
        at com.datastax.driver.core.ResultSetFuture.extractCauseFromExecutionException(ResultSetFuture.java:271)
        at com.datastax.driver.core.ResultSetFuture.getUninterruptibly(ResultSetFuture.java:187)
        at com.datastax.driver.core.Session.execute(Session.java:126)
        at br.com.pollkan.batch.CqlCommands.executeQuery(CqlCommands.java:149)
        at br.com.pollkan.batch.BaseBatch.processChild(BaseBatch.java:364)
        at br.com.pollkan.batch.BaseBatch.run(BaseBatch.java:640)
        at java.lang.Thread.run(Thread.java:722)

If need more information, please let me know. Tks;;;","13/Dec/13 11:22;enrico.scalavino;[~jjordan] [~enigmacurry] I am using datastax 2.0.0-rc1 and Cassandra 2.0.3. The query works if I remove the where clause on either the read attribute or on the message_id attribute. 
Unfortunately the code calling the query is buried deep in the project, and the tests only test high level apis. ;;;","13/Dec/13 13:02;dmnorc;I get the same error too. I'm using Cassandra 2.0.2 and Datastax Java Driver 2.0.0-beta2;;;","18/Dec/13 23:56;marcostrama;I removed the ""blocked"" column from the query (indexed column) and now it works. This helps?;;;","19/Dec/13 10:47;enrico.scalavino;No, also my query works if I remove one of the conditions. That still does not explain why that happens, and why there is an unmanaged exception on the server. ;;;","28/Jan/14 23:16;enigmacurry;I uploaded an Eclipse project (6470-reproduced.tar.gz) that reproduces this issue.

Start up a ccm cluster: 

{code}
ccm create -v git:cassandra-2.0.2
ccm create -v git:cassandra-2.0.2 test
ccm populate -n 3:3
ccm start
{code}

Then run Bug6470Client.

This code actually works with driver version 1.0.3, so I wonder if that's not the cause instead of C*...

I can also confirm that the same queries issued in cqlsh do not repro this issue.;;;","28/Jan/14 23:24;enigmacurry;I also tried from the latest python driver, no problem there.;;;","29/Jan/14 09:07;slebresne;This is likely a problem with the paging over 2ndary indexes (which is why only the 2.0 version of the driver is running into it). I'll have a closer look.;;;","29/Jan/14 14:50;slebresne;We were not handling empty bounds in DataRange.sliceForKey() (that is indeed used by paging calls) which was returning an empty slice array (which was incorrect but hence the error).

Attached simple fix.;;;","29/Jan/14 17:50;lyubent;+1;;;","29/Jan/14 18:25;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Runaway thread for SSL socket,CASSANDRA-6468,12683783,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,mishail,mishail,10/Dec/13 07:00,16/Apr/19 09:31,14/Jul/23 05:53,17/Dec/13 22:39,2.0.4,,,,,,1,,,,,"* Start Cassandra with {{internode_encryption}} turned on. For example {{server_encryption_options:   internode_encryption: dc}}
* Open a telnet session to port 7001. Don't type anything. I used PuTTY in a RAW mode
* Shutdown the MessageService (for ex. by calling {{drain}} on {{StorageSevice}} MBean )
* Now type anything in the telnet session and press enter
* Observe an endless flood of ""Error reading the socket null. java.net.SocketException: Socket closed""",,mishail,oseiler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6349,,,,,,"10/Dec/13 18:44;mishail;CASSANDRA-2.0-6468.patch;https://issues.apache.org/jira/secure/attachment/12618074/CASSANDRA-2.0-6468.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,362855,,,Tue Dec 17 22:39:36 UTC 2013,,,,,,,,,,"0|i1qjqf:",363161,2.0.3,,,,,,,,jbellis,,jbellis,Low,,2.0.3,,,,,,,,,,,,,,,,"10/Dec/13 18:44;mishail;Patch: don't rely on exceptions to check the socket's state, use socket's {{isClosed()}} instead;;;","17/Dec/13 22:39;jbellis;Committed.  Also added (in a separate commit) some more cleanup of SocketThread.run.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DES scores fluctuate too much for cache pinning,CASSANDRA-6465,12683652,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,cburroughs,cburroughs,09/Dec/13 18:23,16/Apr/19 09:31,14/Jul/23 05:53,14/Jan/14 21:18,2.0.5,,,,,,0,gossip,,,,"To quote the conf:

{noformat}
# if set greater than zero and read_repair_chance is < 1.0, this will allow
# 'pinning' of replicas to hosts in order to increase cache capacity.
# The badness threshold will control how much worse the pinned host has to be
# before the dynamic snitch will prefer other replicas over it.  This is
# expressed as a double which represents a percentage.  Thus, a value of
# 0.2 means Cassandra would continue to prefer the static snitch values
# until the pinned host was 20% worse than the fastest.
dynamic_snitch_badness_threshold: 0.1
{noformat}

An assumption of this feature is that scores will vary by less than dynamic_snitch_badness_threshold during normal operations.  Attached is the result of polling a node for the scores of 6 different endpoints at 1 Hz for 15 minutes.  The endpoints to sample were chosen with `nodetool getendpoints` for row that is known to get reads.  The node was acting as a coordinator for a few hundred req/second, so it should have sufficient data to work with.  Other traces on a second cluster have produced similar results.
 * The scores vary by far more than I would expect, as show by the difficulty of seeing anything useful in that graph.
 * The difference between the best and next-best score is usually > 10% (default dynamic_snitch_badness_threshold).

Neither ClientRequest nor ColumFamily metrics showed wild changes during the data gathering period.

Attachments:
 * jython script cobbled together to gather the data (based on work on the mailing list from Maki Watanabe a while back)
 * csv of DES scores for 6 endpoints, polled about once a second
 * Attempt at making a graph

","1.2.11, 2 DC cluster",cburroughs,ianbarfield,jasobrown,rcoli,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/14 18:10;thobbs;6465-v1.patch;https://issues.apache.org/jira/secure/attachment/12622904/6465-v1.patch","14/Jan/14 17:59;thobbs;99th_latency.png;https://issues.apache.org/jira/secure/attachment/12622899/99th_latency.png","09/Dec/13 18:24;cburroughs;des-score-graph.png;https://issues.apache.org/jira/secure/attachment/12617871/des-score-graph.png","14/Jan/14 21:04;thobbs;des-scores-with-penalty.csv;https://issues.apache.org/jira/secure/attachment/12622977/des-scores-with-penalty.csv","14/Jan/14 21:04;thobbs;des-scores-without-penalty.csv;https://issues.apache.org/jira/secure/attachment/12622976/des-scores-without-penalty.csv","09/Dec/13 18:24;cburroughs;des.sample.15min.csv;https://issues.apache.org/jira/secure/attachment/12617870/des.sample.15min.csv","09/Dec/13 18:24;cburroughs;get-scores.py;https://issues.apache.org/jira/secure/attachment/12617869/get-scores.py","14/Jan/14 17:59;thobbs;throughput.png;https://issues.apache.org/jira/secure/attachment/12622900/throughput.png",,,,,,,,,,,,,8.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,362724,,,Tue Jan 14 21:18:45 UTC 2014,,,,,,,,,,"0|i1qivr:",363018,,,,,,,,,brandon.williams,,brandon.williams,Low,,1.2.0,,,,,,,,,,,,,,,,"17/Dec/13 00:51;rcoli;Are we sure that this mechanism of producing cache pinning is worth the complexity here, especially given speculative execution? ;;;","03/Jan/14 00:34;thobbs;I can reproduce Chris's results, and in my experimentation it looks like almost all of the variation is due to the ""timePenalty"", which is basically how long it has been since the last entry from an endpoint.  I can see why something like the time penalty might be useful for the phi FD, which expects messages on a periodic basis, but it doesn't make sense to me to use it in a load balancing measure.  My suggestion would be to remove the time penalty.

bq. Are we sure that this mechanism of producing cache pinning is worth the complexity here, especially given speculative execution?

Effective cache utilization is extremely important, so I would say it's well worth the additional complexity.  I don't think speculative execution should affect this greatly, but I might be missing something; care to expand on that?;;;","03/Jan/14 01:34;jbellis;This was introduced by CASSANDRA-3722.  It's not clear to me what that code is trying to do.  Or maybe I'm still grumpy about calling i/o activity ""severity."";;;","06/Jan/14 19:36;ianbarfield;I believe the purpose of time penalty was to more quickly detect problematic nodes. If a node was suddenly suffering severe issues, that wouldn't be reflected in its latency metric until the current outstanding queries resolved. That might take until the maximum duration timeout which can be arbitrarily long, and in many cases is a lot longer than you'd like. By using timeDelay, the snitch can somewhat immediately penalize problem nodes since the queries do not have to timeout first. That said, it has numerous flaws both conceptually and in its implementation.

I was working on this problem a couple weeks ago, but have been distracted since, so I might not be able to give the best summary. Here's a couple issues off the top of my head though:
- if the time delay values are low, then high jitter throws the scores way off. It isn't unreasonable to expect situations where the time delay shifts semi-randomly between 0 and 1 ms. This means very little in terms of whether a node is a suitable target but can cause a drastic difference in scores if there is no slow node to anchor the scores.
- if the node response periods aren't low; say they average around 50 ms. Then by definition they are highly random since the score could be calculated at any point along 0 to 50 ms.
- it has a lot of complex interactions outside of its original scope of detecting bad nodes
- when calculating scores, if there is no lastReceived value for a node (eg. the node has just been added to the cluster), then the logic defaults to using the current time (essentially 0 or maximum 'good'). You might instead take the view that an unproven, cache-cold node would be a bad selection.
- sensitive to local noise. Each time the score is calculated, the timePenalty is calculated fresh. Since there is no concept of persistance or scope, events that corrupt the scoring process are extra harmful. eg. GC, CPU load / thread scheduling, and concurrency shenanigans occuring between the lastReceived.get() and System.currentTimeMillis()

Some of these issues are somewhat alleviated by the switch to using nanos, and I've been tempted to back port that for this class at least for testing, but this logic fails in complex ways. I think at some point I was able to confirm some wildly fluctuating values of the subcomponents to the scores (specifically timePenalty) by checking the mbeans and working under the assumption that timePenalty was likely the only component to well rounded scores -- if you have at least one node with >> timePenalty then it gets cut off to UPDATE_INTERVAL_IN_MS which as a divisor makes for nicely formed floating point numbers.

There are also a lot of issues with the other score components, and some of the overall logic, but... some other time. Apologies if i've gotten something quite wrong; I've never really used Cassandra.;;;","10/Jan/14 20:08;thobbs;[~ianbarfield] thanks for the analysis, you make some excellent observations.

From the discussion in CASSANDRA-3722, it seems like the two motivations for the time penalty were these:
# When a node dies, the FD will not mark it down for a while; in the meantime, we'd like to stop sending queries to it
# In a multi-DC setup, we would like to penalize the remote DC, but not so much that we won't ever use it when local nodes become very slow

I suspect that rapid read protection (CASSANDRA-4705) does a good job of mitigating the #1 case until the FD marks the node down.  I'll do some testing to confirm this.

I don't feel like the #2 case needs special treatment from the dynamic snitch, especially with the badness_threshold in effect.  Latency to the remote DC should prevent it from being used under normal circumstances.  If users really want to guarantee that, the LOCAL consistency levels are always available.;;;","10/Jan/14 20:22;brandon.williams;The best way to test #1 is to run in foreground mode and then suspend (^Z) the JVM.;;;","14/Jan/14 17:59;thobbs;Attached are two graphs of throughput and 99th percentile latencies for four runs of stress.  Two runs kept the time penalty in DES, and two had it removed.  There was a normal stress read of 3 million rows with and without the time penalty, and a second run where one of the three nodes was suspended 30 seconds into the run and resumed 60 seconds into the run.

In short, there's no difference in throughput or median/95th/99th latencies when a node goes down with the time penalty removed, so it looks like rapid read protection does indeed dominate there.;;;","14/Jan/14 18:10;thobbs;6465-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6465]) removes the timePenalty component from the DES score.;;;","14/Jan/14 20:09;brandon.williams;Can we get some numbers on score fluctuation with the time penalty removed to be certain this fixes it?;;;","14/Jan/14 21:04;thobbs;Attached are the DES scores from a run with and without the time penalty.  This was done with a three node CCM cluster. node1 coordinated all reads, and node2 and node3 were the replicas for all reads.  In both runs, node2 served most of the reads (as reported by cfstats).;;;","14/Jan/14 21:18;brandon.williams;Committed, with mentions of time penalty removed from comments.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paging queries with IN on the partition key is broken,CASSANDRA-6464,12683632,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,09/Dec/13 17:00,16/Apr/19 09:31,14/Jul/23 05:53,18/Dec/13 10:24,2.0.4,,,,,,2,,,,,"Feels like MultiPartitionPager (which handles paging queries when there is a IN on the partition key) has completely missed CASSANDRA-5714's train. As a result, it completely broken and will typically loop infinitely.

Attaching patch to fix.",,aleksey,liqweed,shohou,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Dec/13 17:01;slebresne;6464.txt;https://issues.apache.org/jira/secure/attachment/12617857/6464.txt","17/Dec/13 20:44;aleksey;redundant-stuff.txt;https://issues.apache.org/jira/secure/attachment/12619163/redundant-stuff.txt",,,,,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,362704,,,Wed Dec 18 10:24:01 UTC 2013,,,,,,,,,,"0|i1qirb:",362998,,,,,,,,,aleksey,,aleksey,Normal,,2.0 beta 1,,,,,,,,,,,,,,,,"17/Dec/13 20:44;aleksey;+1. Attaching a minor nitty patch, removing some redundant constructors and variables.;;;","18/Dec/13 10:24;slebresne;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cleanup ClassCastException,CASSANDRA-6462,12683593,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,Andie78,Andie78,09/Dec/13 12:24,16/Apr/19 09:31,14/Jul/23 05:53,10/Dec/13 04:35,2.0.4,,,Legacy/Tools,,,0,cleanup,compaction,,,"I enlarged the cluseter from 4 to 8 nodes. During cleaning up the ""old"" nodes with ""nodetool cleanup"" it breaks up with exception. I started cleanup from a different computer to manage them sequentially.
{panel:title=cmd.exe}
Error occurred during cleanup
java.util.concurrent.ExecutionException: java.lang.ClassCastException: org.apach
e.cassandra.io.sstable.SSTableReader$EmptyCompactionScanner cannot be cast to or
g.apache.cassandra.io.sstable.SSTableScanner
        at java.util.concurrent.FutureTask.report(Unknown Source)
        at java.util.concurrent.FutureTask.get(Unknown Source)
        at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTabl
eOperation(CompactionManager.java:227)
        at org.apache.cassandra.db.compaction.CompactionManager.performCleanup(C
ompactionManager.java:265)
        at org.apache.cassandra.db.ColumnFamilyStore.forceCleanup(ColumnFamilySt
ore.java:1054)
        at org.apache.cassandra.service.StorageService.forceKeyspaceCleanup(Stor
ageService.java:2038)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.reflect.misc.Trampoline.invoke(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.reflect.misc.MethodUtil.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown So
urce)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown So
urce)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Unknown Source)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown
Source)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Sou
rce)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(Unknown Sour
ce)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run
(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(U
nknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
        at sun.rmi.transport.Transport$1.run(Unknown Source)
        at sun.rmi.transport.Transport$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Sou
rce)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Sour
ce)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.ClassCastException: org.apache.cassandra.io.sstable.SSTable
Reader$EmptyCompactionScanner cannot be cast to org.apache.cassandra.io.sstable.
SSTableScanner
        at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompact
ion(CompactionManager.java:563)
        at org.apache.cassandra.db.compaction.CompactionManager.access$400(Compa
ctionManager.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$5.perform(Compac
tionManager.java:274)
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(Compactio
nManager.java:222)
        at java.util.concurrent.FutureTask.run(Unknown Source)
        ... 3 more
{panel}",Windows 7 / Java 1.7.0.25,Andie78,mathijs,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,362665,,,Thu Dec 12 16:47:18 UTC 2013,,,,,,,,,,"0|i1qiin:",362959,2.0.3,,,,,,,,,,,Normal,,2.0.1,,,,,,,,,,,,,,,,"09/Dec/13 17:20;Andie78;Cleanup on that node not working anymore. There are already problems after restart of the cassandra service. Any ideas how to fix? If u don't have idea as well I will try to repair, but maybe cannot reproduce anymore.
{panel:title=system.log}
ERROR [CompactionExecutor:25] 2013-12-09 17:50:14,932 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:25,1,RMI Runtime]
java.util.concurrent.RejectedExecutionException: Task java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@1dbf287 rejected from org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor@10f8a54[Terminated, pool size = 0, active threads = 0, queued tasks = 0, completed tasks = 780]
	at java.util.concurrent.ThreadPoolExecutor$AbortPolicy.rejectedExecution(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor.delayedExecute(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor.schedule(Unknown Source)
	at java.util.concurrent.ScheduledThreadPoolExecutor.submit(Unknown Source)
	at org.apache.cassandra.io.sstable.SSTableDeletingTask.schedule(SSTableDeletingTask.java:66)
	at org.apache.cassandra.io.sstable.SSTableReader.releaseReference(SSTableReader.java:1105)
	at org.apache.cassandra.db.DataTracker.removeOldSSTablesSize(DataTracker.java:388)
	at org.apache.cassandra.db.DataTracker.postReplace(DataTracker.java:353)
	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:347)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:252)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:1078)
	at org.apache.cassandra.db.compaction.CompactionTask.replaceCompactedSSTables(CompactionTask.java:296)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:242)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [ValidationExecutor:1] 2013-12-09 17:53:16,411 Validator.java (line 242) Failed creating a merkle tree for [repair #65efe6e0-60f2-11e3-ac47-eb1c24a59bb8 on nieste/nfiles, (3837863087520363054,3847126934337600104]], /10.9.9.240 (see log for details)
ERROR [ValidationExecutor:1] 2013-12-09 17:53:16,421 CassandraDaemon.java (line 187) Exception in thread Thread[ValidationExecutor:1,1,main]
FSWriteError in D:\Programme\cassandra\data\nieste\nfiles\snapshots\65efe6e0-60f2-11e3-ac47-eb1c24a59bb8\nieste-nfiles-jb-19878-Index.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:120)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:382)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:378)
	at org.apache.cassandra.db.Directories.clearSnapshot(Directories.java:416)
	at org.apache.cassandra.db.ColumnFamilyStore.clearSnapshot(ColumnFamilyStore.java:1801)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:810)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.FileSystemException: D:\Programme\cassandra\data\nieste\nfiles\snapshots\65efe6e0-60f2-11e3-ac47-eb1c24a59bb8\nieste-nfiles-jb-19878-Index.db: Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird.

	at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(Unknown Source)
	at sun.nio.fs.AbstractFileSystemProvider.delete(Unknown Source)
	at java.nio.file.Files.delete(Unknown Source)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:116)
	... 11 more
ERROR [CompactionExecutor:3] 2013-12-09 17:54:16,810 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:3,1,RMI Runtime]
java.lang.ClassCastException: org.apache.cassandra.io.sstable.SSTableReader$EmptyCompactionScanner cannot be cast to org.apache.cassandra.io.sstable.SSTableScanner
	at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:563)
	at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$5.perform(CompactionManager.java:274)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:222)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel};;;","10/Dec/13 04:33;jbellis;Introduced by CASSANDRA-2524;;;","10/Dec/13 04:35;jbellis;should be fixed in 2.0 HEAD by 5d24c55335e91fde5470ce94306fde26272e2b44;;;","12/Dec/13 12:27;Andie78;What can I do now? repair doesn't help and major compact doesn't help. Cleanup still stops with SSTableReader / ClassCastException.
Reset node (delete all and start new)? Wait for 2.0.4? Tx.;;;","12/Dec/13 15:15;jbellis;That, or cherry pick the fix yourself.;;;","12/Dec/13 16:47;Andie78;I will continue in the 2nd week of January. Company will close and I have just one day left... Marry X-Mas to @ ! ;-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Concurrency issue in Directories.getOrCreate(),CASSANDRA-6459,12683214,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,rwfowler,rwfowler,rwfowler,06/Dec/13 22:52,16/Apr/19 09:31,14/Jul/23 05:53,07/Dec/13 10:33,1.2.13,2.0.4,,,,,0,,,,,"We're seeing an FSWriteError sometimes during repairs. I think it's because two threads are calling Directories.getOrCreate() on the same directory at about the same time. File.mkdirs() returns false if the directory already exists.

A patch is forthcoming.

Here's the exception we get:

ERROR 2013-12-06 09:31:45,217 [Thread-11051] CassandraDaemon Exception in thread Thread[Thread-11051,5,main]
FSWriteError in /data-1/cassandra/data/RedDeerCollege/Binaries/backups
        at org.apache.cassandra.db.Directories.getOrCreate(Directories.java:483)
        at org.apache.cassandra.db.Directories.getBackupsDirectory(Directories.java:242)
        at org.apache.cassandra.db.DataTracker.maybeIncrementallyBackup(DataTracker.java:165)
        at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
        at org.apache.cassandra.db.ColumnFamilyStore.addSSTables(ColumnFamilyStore.java:911)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:186)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:138)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
        at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)

Someone else appears to have seen the same thing a while back:

Here's someone that's had a similar problem:

http://mail-archives.apache.org/mod_mbox/cassandra-user/201206.mbox/%3CA65C3B25-2866-48BE-8584-AB048663611C@thelastpickle.com%3E
",1.2 2.0,aleksey,rwfowler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Dec/13 22:53;rwfowler;CASSANDRA-6459.txt;https://issues.apache.org/jira/secure/attachment/12617496/CASSANDRA-6459.txt",,,,,,,,,,,,,,,,,,,,1.0,rwfowler,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,362466,,,Sat Dec 07 10:33:12 UTC 2013,,,,,,,,,,"0|i1qhan:",362760,1.2.9,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"06/Dec/13 22:56;rwfowler;I attached a patch and included a test method.

For a concurrency bug, reproducing the problem wasn't actually too hard. It doesn't happen every time, but it happens often enough.;;;","07/Dec/13 10:33;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool getendpoints doesn't validate key arity,CASSANDRA-6458,12683140,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,philipthompson,yaitskov,yaitskov,06/Dec/13 15:40,16/Apr/19 09:31,14/Jul/23 05:53,15/Apr/15 17:48,2.1.5,,,Tool/nodetool,,,0,lhf,,,,"I have a complex row key.

$ create table b (x int, s text, ((x,s)) primary key);

In cqlsh I cannot fill row key partially:

{noformat}
$ insert into b (x) values(4);
Bad Request: Missing mandatory PRIMARY KEY part s
{noformat}

But nodetool can find hosts by incomplete key
{noformat}
$ nodetool -h cas3 getendpoints anti_portal b 12
192.168.4.4
192.168.4.5
192.168.4.6
{noformat}

No error is reported.

I found that columns are separated by "":"".
And If I pass to many elements then the error happens.

{noformat}
$ nodetool -h cas3 getendpoints anit_portal b 12:dd:dd
Exception in thread ""main"" org.apache.cassandra.serializers.MarshalException: unable to make int from '12:dd:dd'
    at org.apache.cassandra.db.marshal.Int32Type.fromString(Int32Type.java:69)
    at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:2495)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
    at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
    at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
    at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
    at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
    at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
    at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
    at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
    at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
    at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
    at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
    at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
    at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
    at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
    at java.lang.reflect.Method.invoke(Method.java:606)
    at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
    at sun.rmi.transport.Transport$1.run(Transport.java:177)
    at sun.rmi.transport.Transport$1.run(Transport.java:174)
    at java.security.AccessController.doPrivileged(Native Method)
    at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
    at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
    at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:744)
Caused by: java.lang.NumberFormatException: For input string: ""12:dd:dd""
    at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
    at java.lang.Integer.parseInt(Integer.java:492)
    at java.lang.Integer.parseInt(Integer.java:527)
    at org.apache.cassandra.db.marshal.Int32Type.fromString(Int32Type.java:65)
    ... 36 more
{noformat}

I think showing huge stack trace is not proper behavior.
Error message should be printer if arity of passed key and table key are not equal.",,jjordan,mishail,philipthompson,thobbs,xapharius,yaitskov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/15 20:31;philipthompson;6458-2.1.txt;https://issues.apache.org/jira/secure/attachment/12705427/6458-2.1.txt","15/Apr/15 17:16;philipthompson;6458-trunk-v2.txt;https://issues.apache.org/jira/secure/attachment/12725629/6458-trunk-v2.txt",,,,,,,,,,,,,,,,,,,2.0,philipthompson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,362392,,,Wed Apr 15 17:48:16 UTC 2015,,,,,,,,,,"0|i1qgu7:",362686,,,,,,,,,jjordan,,jjordan,Low,,2.0.2,,,,,,,,,,,,,,,,"14/Mar/14 04:22;jjordan;getendpoints operates on the partition key only.  we might want to update the help text for it.;;;","10/Jun/14 08:53;xapharius;Hello!
I started working on this issue, but would like to have some feedback on how to proceed. I am quite new to Cassandra and might be making some wrong assumptions about it's internals.
In order for NodeTool to check the arity, it has to get the number of columns in the partition key of a cf. As far as I understand it, I would have to get that from CFMetaData.
The problem is that NodeTool can only connect through NodeProbe, which has the proxies for the MBeans.
The closest I got was through the StorageServerProxy and ColumnFamilyStore proxy, but none of them specify a method in their interface to access a cfMetaData.

The options I see so far are to either add a method to ColumnFamilyStoreMBean or try to validate the key arity somewhere lower.

Or is there an easier way?

Thanks!
;;;","18/Mar/15 20:31;philipthompson;Attached patches for 2.1 and trunk;;;","26/Mar/15 15:35;philipthompson;Upon reflection, I think only the documentation needed expanded. The error message explains pretty sufficiently that the key passed in is unacceptable, and there is no earlier point in the stack to validate it.;;;","15/Apr/15 16:49;jjordan;+1;;;","15/Apr/15 17:11;jbellis;I think trunk patch fails to apply now.;;;","15/Apr/15 17:14;philipthompson;I'll rebase.;;;","15/Apr/15 17:17;philipthompson;New patch for trunk attached.;;;","15/Apr/15 17:48;thobbs;Committed as {{dac54976}}, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig support for hadoop CqlInputFormat,CASSANDRA-6454,12682961,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,05/Dec/13 18:12,16/Apr/19 09:31,14/Jul/23 05:53,23/Jul/14 15:32,2.0.10,2.1.1,,,,,1,,,,,"CASSANDRA-6311 adds new CqlInputFormat, we need add the Pig support for it",,alexliu68,liqweed,ShridharB,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6311,,,,,,,,,,,"10/Dec/13 03:22;alexliu68;6454-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12617971/6454-2.0-branch.txt","19/May/14 22:38;alexliu68;6454-v2-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12645661/6454-v2-2.0-branch.txt","11/Jul/14 20:25;alexliu68;6454-v3-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12655290/6454-v3-2.0-branch.txt","11/Jul/14 20:15;alexliu68;6454-v3-2.1-branch.txt;https://issues.apache.org/jira/secure/attachment/12655287/6454-v3-2.1-branch.txt","22/Jul/14 18:29;alexliu68;6454-v4-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12657153/6454-v4-2.0-branch.txt","22/Jul/14 18:29;alexliu68;6454-v4-2.1-branch.txt;https://issues.apache.org/jira/secure/attachment/12657152/6454-v4-2.1-branch.txt",,,,,,,,,,,,,,,6.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,362218,,,Wed Jul 23 15:32:19 UTC 2014,,,,,,,,,,"0|i1qfrr:",362513,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"10/Dec/13 03:22;alexliu68;The patch is on top of CASSANDRA-6311. It creates CqlNativeStorage for new CqlInputFormat.;;;","16/May/14 12:09;ShridharB;[~alexliu68] I  tried to run pig script to load data with CqlNativeStorage but getting problems, looks like jar conflicts or may be something else can you please let me know the required jars and their version needed to run CqlNativeStorage . Below are the things i tried.
1.Applied this patch on top of Cassandra 2.0.07. 
2.When i tried to run pig script with CqlNativeStorage it threw me ""ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2998: Unhandled internal error. com/datastax/driver/core/policies/LoadBalancingPolicy"" exception. 
3.Then  added ""cassandra-driver-core-2.0.1.jar"" in my classpath after this i got an exception ""ERROR org.apache.pig.tools.pigstats.SimplePigStats - ERROR: com.codahale.metrics.Metric .......
....
Caused by: java.lang.ClassNotFoundException: com.codahale.metrics.Metric"" 
4.Then  added ""metrics-core-3.0.2.jar"". After adding this jar file i was able to run the job but failed and my hadoop log shows me this exception
""Caused by: com.datastax.driver.core.exceptions.NoHostAvailableException: All host(s) tried for query failed (tried: slave1.tpgsi.com/xxxx.yyy.zzz.aa (com.datastax.driver.core.TransportException: [slave1/xxxx.yyy.zzz.aa] Error writing))
NOTE: metrics-core-2.2.0.jar already exists in C* 2.0.7 lib folder.;;;","16/May/14 18:53;alexliu68;The patch is a little of off, I will update it to the latest code. ;;;","19/May/14 18:21;alexliu68;[~minishri] Do you enable native port 9042 on all the nodes? If the native port is different from 9042, set it in url native_port=<native_port>;;;","19/May/14 22:44;alexliu68;The latest patch is attached. To use CqlNativeStorage, the following parameters need to be specified
{code}
input_cql=     (It must in the following format 
 * 1) select clause must include partition key columns (to calculate the progress based on the actual CF row processed)
 * 2) where clause must include token(partition_key1, ...  , partition_keyn) > ? and 
 *       token(partition_key1, ... , partition_keyn) <= ?  (in the right order) 

native_port=  (If it's not default port)

other parameters are
[&native_port=<native_port>][&core_conns=<core_conns>]
[&max_conns=<max_conns>][&min_simult_reqs=<min_simult_reqs>][&max_simult_reqs=<max_simult_reqs>]
[&native_timeout=<native_timeout>][&native_read_timeout=<native_read_timeout>][&rec_buff_size=<rec_buff_size>]
[&send_buff_size=<send_buff_size>][&solinger=<solinger>][&tcp_nodelay=<tcp_nodelay>]
[&reuse_address=<reuse_address>]
[&keep_alive=<keep_alive>][&auth_provider=<auth_provider>][&trust_store_path=<trust_store_path>]
[&key_store_path=<key_store_path>][&trust_store_password=<trust_store_password>]
[&key_store_password=<key_store_password>][&cipher_suites=<cipher_suites>][&input_cql=<input_cql>]]
{code}
;;;","28/May/14 15:25;ShridharB;[~alexliu68] I have applied the new patch. After this am not able to load data from Cassandra. It throws me an exception 

Caused by: InvalidRequestException(why:Keyspace '' does not exist)
        at org.apache.cassandra.thrift.Cassandra$set_keyspace_result$set_keyspace_resultStandardScheme.read(Cassandra.java:8906)
        at org.apache.cassandra.thrift.Cassandra$set_keyspace_result$set_keyspace_resultStandardScheme.read(Cassandra.java:8892)
        at org.apache.cassandra.thrift.Cassandra$set_keyspace_result.read(Cassandra.java:8842)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_set_keyspace(Cassandra.java:599)
        at org.apache.cassandra.thrift.Cassandra$Client.set_keyspace(Cassandra.java:586)
        at org.apache.cassandra.hadoop.pig.AbstractCassandraStorage.initSchema(AbstractCassandraStorage.java:493)


Table schema :
CREATE TABLE dogs (
  block_id int,
  breed text,
  color text,
  short_hair boolean,
  PRIMARY KEY ((block_id, breed), color, short_hair))

My pig load function would be like this
data =  load 'cql://excelsior/dogs' using org.apache.cassandra.hadoop.pig.CqlNativeStorage();

Also tried with this 
a = load 'cql://excelsior/dogs?input_cql=select block_id,breed from excelsior.dogs where token(block_id,breed) > ? and token(block_id,breed) <= ? and block_id=5 and breed='Bulldog' ' using org.apache.cassandra.hadoop.pig.CqlNativeStorage();



Still same exception. ;;;","28/May/14 16:11;alexliu68;There are a few unit testing examples at CqlTableTest. input_cql needs encoded. 

{code}
using org.apache.cassandra.hadoop.pig.CqlNativeStorage();
should be using CqlNativeStorage();
{code};;;","10/Jul/14 15:28;brandon.williams;Looks good, though some of those params feel like YAGNI to me, but I guess if we already have them we might as well put them in.  Can you post a 2.1 branch too?;;;","10/Jul/14 17:58;brandon.williams;For 2.1, it's probably a good idea to switch all the tests to native.;;;","11/Jul/14 16:49;alexliu68;I got some test errors for compact tables using CqlStorage in CASSANDRA-7059, I hope change them to CqlNativeStorage fixes the issue.;;;","11/Jul/14 16:51;brandon.williams;Yes, that's a bug in CPRR, I'm fine with moving away from it.;;;","11/Jul/14 20:02;alexliu68;I also got a lib conflict issue as following

{code}
   [junit] ------------- ---------------- ---------------
    [junit] Testcase: testCassandraStorageCompositeColumnCF(org.apache.cassandra.pig.ThriftColumnFamilyTest):	Caused an ERROR
    [junit] org.antlr.runtime.tree.BaseTree.insertChild(ILjava/lang/Object;)V
    [junit] java.lang.NoSuchMethodError: org.antlr.runtime.tree.BaseTree.insertChild(ILjava/lang/Object;)V
    [junit] 	at org.apache.pig.parser.QueryParser.paren_expr(QueryParser.java:17532)
    [junit] 	at org.apache.pig.parser.QueryParser.cast_expr(QueryParser.java:17005)
    [junit] 	at org.apache.pig.parser.QueryParser.multi_expr(QueryParser.java:15679)
    [junit] 	at org.apache.pig.parser.QueryParser.expr(QueryParser.java:15568)
    [junit] 	at org.apache.pig.parser.QueryParser.unary_cond(QueryParser.java:15324)
    [junit] 	at org.apache.pig.parser.QueryParser.not_cond(QueryParser.java:14951)
    [junit] 	at org.apache.pig.parser.QueryParser.and_cond(QueryParser.java:14828)
    [junit] 	at org.apache.pig.parser.QueryParser.cond(QueryParser.java:14728)
    [junit] 	at org.apache.pig.parser.QueryParser.filter_clause(QueryParser.java:10509)
    [junit] 	at org.apache.pig.parser.QueryParser.op_clause(QueryParser.java:7092)
    [junit] 	at org.apache.pig.parser.QueryParser.general_statement(QueryParser.java:2314)
    [junit] 	at org.apache.pig.parser.QueryParser.statement(QueryParser.java:1579)
    [junit] 	at org.apache.pig.parser.QueryParser.query(QueryParser.java:395)
    [junit] 	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:236)
    [junit] 	at org.apache.pig.parser.QueryParserDriver.parse(QueryParserDriver.java:179)
    [junit] 	at org.apache.pig.PigServer$Graph.parseQuery(PigServer.java:1678)
    [junit] 	at org.apache.pig.PigServer$Graph.registerQuery(PigServer.java:1625)
    [junit] 	at org.apache.pig.PigServer.registerQuery(PigServer.java:577)
    [junit] 	at org.apache.pig.PigServer.registerQuery(PigServer.java:590)
    [junit] 	at org.apache.cassandra.pig.ThriftColumnFamilyTest.testCassandraStorageCompositeColumnCF(ThriftColumnFamilyTest.java:624)
{code}

Pig 0.12 is built with antlr 3.4.
Cassandra uses antler 3.2

Other than that, CqlNativeStorage passes all the tests;;;","11/Jul/14 20:26;alexliu68;I comment out that test case, we can fix it later  in another ticket;;;","18/Jul/14 17:59;brandon.williams;Do we actually need cassandra_pig.yaml?  It seems like all we're doing is turning on the native proto which should be fine in the standard config.;;;","21/Jul/14 22:36;alexliu68;cassandra_pig.yaml uses Murmur3Partitioner instead of ByteOrderedPartitioner. ByteOrderedPartitioner is used for other unit tests.;;;","21/Jul/14 22:45;brandon.williams;Is there a reason we need to switch to it for this? (not that I disagree with it at all);;;","22/Jul/14 00:52;alexliu68;Most of Cassandra deployment uses Murmur3Partitioner, testing on Murmur3Partitioner covers the general case. Some unit tests still use old ByteOrderedPartitioner, so just update the cassanra.yaml to Murmur3Partitioner breaks other unit tests. That's the reason I create a new yaml file.;;;","22/Jul/14 06:47;jbellis;I'm -0 on adding extra yaml files, fwiw.;;;","22/Jul/14 14:40;brandon.williams;I'm +1 on switching to M3 in principle, but it seems highly unlikely that we're going to catch any problems here with the handful of records we use in the pig tests.

bq. I'm -0 on adding extra yaml files, fwiw.

I agree, this doesn't seem like the time or place to be worried about BOP vs M3.  Let's just take that out and use the default yaml for now.

;;;","22/Jul/14 18:29;alexliu68;V4 rollback cassandra_pig.yaml;;;","23/Jul/14 15:32;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tools error out if they can't make ~/.cassandra,CASSANDRA-6449,12682745,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,kirktrue,jjordan,jjordan,04/Dec/13 18:53,16/Apr/19 09:31,14/Jul/23 05:53,18/Jun/14 19:20,2.0.9,2.1 rc2,,Legacy/Tools,,,2,lhf,,,,"We shouldn't error out if we can't make the .cassandra folder for the new history stuff.

{noformat}
Exception in thread ""main"" FSWriteError in /usr/share/opscenter-agent/.cassandra
	at org.apache.cassandra.io.util.FileUtils.createDirectory(FileUtils.java:261)
	at org.apache.cassandra.utils.FBUtilities.getToolsOutputDirectory(FBUtilities.java:627)
	at org.apache.cassandra.tools.NodeCmd.printHistory(NodeCmd.java:1403)
	at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1122)
Caused by: java.io.IOException: Failed to mkdirs /usr/share/opscenter-agent/.cassandra
	... 4 more
{noformat}",,jblangston@datastax.com,jjordan,jpotter,kirktrue,mishail,sbassi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/14 17:11;kirktrue;trunk-6449.txt;https://issues.apache.org/jira/secure/attachment/12646548/trunk-6449.txt",,,,,,,,,,,,,,,,,,,,1.0,kirktrue,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,362002,,,Wed Jun 18 19:20:02 UTC 2014,,,,,,,,,,"0|i1qefr:",362297,1.2.11,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"29/Jan/14 17:53;jblangston@datastax.com;From a customer:

The culprit is: / src / java / org / apache / cassandra / utils / FBUtilities.java

File historyDir = new File(System.getProperty(""user.home""), "".cassandra"");

Setting an alternate environment variable HOME doesn't fix. I've tried patching the nodetool wrapper script to provide -Duser.home at runtime, but it seems when defining user.home, I get runtime errors with missing libraries. It would be nice if the tool just honoured $HOME (or let you specify a commandline override without hacking the script).;;;","29/Jan/14 18:15;jblangston@datastax.com;This is the error that occurs when manually defining -Duser.home in the nodetool shell script:

{code}
Exception in thread ""main"" java.lang.NoClassDefFoundError: com/google/common/collect/AbstractMultimap$WrappedSortedSet 
at com.google.common.collect.AbstractMultimap.wrapCollection(AbstractMultimap.java:374) 
at com.google.common.collect.AbstractMultimap.get(AbstractMultimap.java:363) 
at com.google.common.collect.AbstractSetMultimap.get(AbstractSetMultimap.java:59) 
at com.google.common.collect.AbstractSortedSetMultimap.get(AbstractSortedSetMultimap.java:65) 
at com.google.common.collect.TreeMultimap.get(TreeMultimap.java:74) 
at com.google.common.collect.AbstractSortedSetMultimap.get(AbstractSortedSetMultimap.java:35) 
at com.google.common.collect.Multimaps$UnmodifiableMultimap.get(Multimaps.java:563) 
at org.apache.cassandra.locator.TokenMetadata.getTokens(TokenMetadata.java:507) 
at org.apache.cassandra.service.StorageService.getTokens(StorageService.java:2048) 
at org.apache.cassandra.service.StorageService.getTokens(StorageService.java:2042) 
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
at java.lang.reflect.Method.invoke(Method.java:597) 
at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93) 
at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27) 
at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208) 
at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120) 
at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:264) 
at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836) 
at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:762) 
at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1454) 
at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:74) 
at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1295) 
at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1387) 
at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:818) 
at sun.reflect.GeneratedMethodAccessor32.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
at java.lang.reflect.Method.invoke(Method.java:597) 
at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:303) 
at sun.rmi.transport.Transport$1.run(Transport.java:159) 
at java.security.AccessController.doPrivileged(Native Method) 
at sun.rmi.transport.Transport.serviceCall(Transport.java:155) 
at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535) 
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790) 
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918) 
at java.lang.Thread.run(Thread.java:662)
{code};;;","29/Jan/14 19:25;mishail;NodeCmd honestly tries to ignore IOExceptions, the problem is that {{FBUtilities.getToolsOutputDirectory}} wraps IOExceptions in {{FSWriteError}},

{code:title=org.apache.cassandra.tools.NodeCmd.printHistory}
        FileWriter writer = null;
        try
        {
            final String outputDir = FBUtilities.getToolsOutputDirectory().getCanonicalPath();
.....
        }
        catch (IOException ioe)
        {
            //quietly ignore any errors about not being able to write out history
        }
        finally
        {
            FileUtils.closeQuietly(writer);
        }
{code};;;","24/Apr/14 16:17;sbassi;I've had another user run into this issue who would like this functionality. I can reproduce the problem just by restricting write access to the users home dir that is running nodetool.;;;","23/May/14 17:12;kirktrue;Simply catching IOError in addition to IOException.;;;","18/Jun/14 19:05;brandon.williams;We probably want to fix this in 1.2 or at least 2.0;;;","18/Jun/14 19:10;jbellis;Patch applies to 2.0 if you point it at src/java/org/apache/cassandra/tools/NodeCmd.java.;;;","18/Jun/14 19:20;brandon.williams;Good enough for me then, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SELECT someColumns FROM table results in AssertionError in AbstractQueryPager.discardFirst,CASSANDRA-6447,12682664,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,julien.ayme@gmail.com,julien.ayme@gmail.com,04/Dec/13 11:50,16/Apr/19 09:31,14/Jul/23 05:53,20/Dec/13 07:50,2.0.4,,,,,,0,,,,,"I have a query which must read all the rows from the table:
Query: ""SELECT key, col1, col2, col3 FROM mytable""

Here is the corresponding code (this is using datastax driver):
{code}
ResultSet result = session.execute(""SELECT key, col1, col2, col3 FROM mytable"");
for (Row row : result) {
     // do some work with row
}
{code}

Messages sent from the client to Cassandra:
* 1st: {{QUERY SELECT key, col1, col2, col3 FROM mytable([cl=ONE, vals=[], skip=false, psize=5000, state=null, serialCl=ONE])}}

* 2nd: {{QUERY SELECT key, col1, col2, col3 FROM mytable([cl=ONE, vals=[], skip=false, psize=5000, state=java.nio.HeapByteBuffer[pos=24 lim=80 cap=410474], serialCl=ONE])}}

On the first message, everything is fine, and the server returns 5000 rows.
On the second message, paging is in progress, and the server fails in AbstractQueryPager.discardFirst: AssertionError (stack trace attached).

Here is some more info (step by step debugging on reception of 2nd message):
{code}
AbstractQueryPager.fetchPage(int):
* pageSize=5000, currentPageSize=5001, rows size=5002, liveCount=5001
* containsPreviousLast(rows.get(0)) returns true

-> AbstractQueryPager.discardFirst(List<Row>):
* rows size=5002
* first=TreeMapBackedSortedColumns[with TreeMap size=1]

-> AbstractQueryPager.discardHead(ColumnFamily, ...):
* counter = ColumnCounter$GroupByPrefix
* iter.hasNext() returns true (TreeMap$ValueIterator with TreeMap size=1)
* Column c = DeletedColumn
* counter.count() -> c.isLive returns false (c is DeletedColumn)
* counter.live() = 0
* iter.hasNext() returns false
* Math.min(0, toDiscard==1) returns 0

<- AbstractQueryPager.discardFirst(List<Row>):
* discarded = 0;
* count = newCf.getColumnCount() = 0;
{code}
->  assert discarded == 1 *throws AssertionError*

","Cluster: single node server (ubuntu)
Cassandra version: 2.0.3 (server/client)
Client: Datastax cassandra-driver-core 2.0.0-rc1",aleksey,enigmacurry,julien.ayme@gmail.com,mishail,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Dec/13 17:36;slebresne;6447.txt;https://issues.apache.org/jira/secure/attachment/12618443/6447.txt","11/Dec/13 10:50;julien.ayme@gmail.com;cassandra-2.0-6447.patch;https://issues.apache.org/jira/secure/attachment/12618218/cassandra-2.0-6447.patch","04/Dec/13 11:56;julien.ayme@gmail.com;stacktrace.txt;https://issues.apache.org/jira/secure/attachment/12616973/stacktrace.txt",,,,,,,,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,361921,,,Fri Dec 20 07:50:27 UTC 2013,,,,,,,,,,"0|i1qdxr:",362216,,,,,,,,,aleksey,,aleksey,Normal,,2.0.3,,,,,,,,,,,,,,,,"04/Dec/13 11:56;julien.ayme@gmail.com;the stacktrace;;;","04/Dec/13 11:59;julien.ayme@gmail.com;I think this issue occurs because the row to discard has only one column, and this column is not live, but I may be wrong here.;;;","04/Dec/13 12:06;julien.ayme@gmail.com;Also, since newCf.getColumnCount() == 0, the rest of the code is still valid (the row will not be included in the returned Rows).
Therefore, I think that the assert statement should be transformed to:
{code}
assert discarded == 1 || discarded == 0;
{code}
Or that the assert statement should be dropped, since it was introduced in the last commit on AbstractQueryPager: 

See: https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=3c9760bdb986f6c2430adfc13c86ecb75c3246ac
;;;","11/Dec/13 10:50;julien.ayme@gmail.com;Proposed (trivial) patch;;;","11/Dec/13 10:51;julien.ayme@gmail.com;Updated fix version, since fix should be trivial;;;","12/Dec/13 17:36;slebresne;I believe the fix is a tiny bit less trivial than that. If the first row in discardFirst has no live data, we need to check the following rows until we find one to discard, otherwise paging would end up return twice the same result. Not sure why discardFirst is not handling that correctly since discardLast is, but anyway, attaching patch to fix (the patch also slightly modify discardLast because it was actually not handling the case where there was less live rows than we want to discard).;;;","13/Dec/13 06:14;julien.ayme@gmail.com;Thanks for looking into this issue, and sorry for the assumption that this was trivial (I am still not completely familiar with the architecture of Cassandra, but I am trying to dig into it as best as I can).;;;","19/Dec/13 17:06;aleksey;I think we are good this time. +1;;;","20/Dec/13 07:50;slebresne;Committed thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AbstractColumnFamilyInputFormat does not use start and end tokens configured via ConfigHelper.setInputRange(),CASSANDRA-6436,12682396,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,pauloricardomg,pauloricardomg,pauloricardomg,03/Dec/13 14:30,16/Apr/19 09:31,14/Jul/23 05:53,26/Mar/14 15:38,2.0.7,,,Legacy/Tools,,,0,hadoop,patch,,,"ConfigHelper allows to set a token input range via the setInputRange(conf, startToken, endToken) call (ConfigHelper:254).

We used this feature to limit a hadoop job range to a single Cassandra node's range, or even to single row key, mostly for testing purposes. 

This worked before the fix for CASSANDRA-5536 (https://github.com/apache/cassandra/commit/aaf18bd08af50bbaae0954d78d5e6cbb684aded9), but after this ColumnFamilyInputFormat never uses the value of KeyRange.start_token when defining the input splits (AbstractColumnFamilyInputFormat:142-160), but only KeyRange.start_key, which needs an order preserving partitioner to work.

I propose the attached fix in order to allow defining Cassandra token ranges for a given Hadoop job even when using a non-order preserving partitioner.

Example use of ConfigHelper.setInputRange(conf, startToken, endToken) to limit the range to a single Cassandra Key with RandomPartitioner: 

IPartitioner part = ConfigHelper.getInputPartitioner(job.getConfiguration());
Token token = part.getToken(ByteBufferUtil.bytes(""Cassandra Key""));
BigInteger endToken = (BigInteger) new BigIntegerConverter().convert(BigInteger.class, part.getTokenFactory().toString(token));
BigInteger startToken = endToken.subtract(new BigInteger(""1""));
ConfigHelper.setInputRange(job.getConfiguration(), startToken.toString(), endToken.toString());",,cburroughs,pauloricardomg,pkolaczk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Dec/13 14:36;pauloricardomg;cassandra-1.2-6436.txt;https://issues.apache.org/jira/secure/attachment/12616787/cassandra-1.2-6436.txt","03/Dec/13 14:36;pauloricardomg;cassandra-1.2-6436.txt;https://issues.apache.org/jira/secure/attachment/12616786/cassandra-1.2-6436.txt",,,,,,,,,,,,,,,,,,,2.0,pauloricardomg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,361653,,,Wed Mar 26 15:38:28 UTC 2014,,,,,,,,,,"0|i1qcbj:",361951,,,,,,,,,pkolaczk,,pkolaczk,Normal,,1.2.6,,,,,,,,,,,,,,,,"03/Dec/13 14:36;pauloricardomg;Fix patch attached.;;;","03/Dec/13 14:36;pauloricardomg;Fix patch attached.;;;","10/Jan/14 17:59;cburroughs;Did this actually get in 1.2.6?  I don't see it in CHANGES.txt;;;","10/Jan/14 18:21;pauloricardomg;My bad, sorry. It was supposed be ""Since Version"", not ""Fix version"". Updated description.;;;","11/Mar/14 17:57;jbellis;[~pkolaczk] can you review?;;;","13/Mar/14 10:40;pkolaczk;Yeah, sure.;;;","25/Mar/14 09:34;pkolaczk;LGTM +1;;;","26/Mar/14 15:38;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DELETE with IF <field>=<value> clause doesn't work properly if more then one row are going to be deleted,CASSANDRA-6430,12682177,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,dyx,dyx,02/Dec/13 17:20,16/Apr/19 09:31,14/Jul/23 05:53,17/Oct/14 09:45,2.0.11,2.1.1,,,,,1,,,,,"CREATE TABLE test(key int, sub_key int, value text, PRIMARY KEY(key, sub_key) );

INSERT INTO test(key, sub_key, value) VALUES(1,1, '1.1');
INSERT INTO test(key, sub_key, value) VALUES(1,2, '1.2');
INSERT INTO test(key, sub_key, value) VALUES(1,3, '1.3');

SELECT * from test;
 key | sub_key | value
-----+---------+-------
   1 |       1 |   1.1
   1 |       2 |   1.2
   1 |       3 |   1.3

DELETE FROM test WHERE key=1 IF value='1.2';
 [applied]
-----------
     False     <=============== I guess second row should be removed

SELECT * from test;
 key | sub_key | value
-----+---------+-------
   1 |       1 |   1.1
   1 |       2 |   1.2
   1 |       3 |   1.3
(3 rows) 

DELETE FROM test WHERE key=1;

SELECT * from test;
(0 rows)          <=========== all rows were removed: OK

",,dyx,liqweed,mishail,philipthompson,slebresne,sungkyu,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/14 20:51;thobbs;6430-2.0.txt;https://issues.apache.org/jira/secure/attachment/12675344/6430-2.0.txt",,,,,,,,,,,,,,,,,,,,1.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,361434,,,Fri Oct 17 09:45:39 UTC 2014,,,,,,,,,,"0|i1qayv:",361733,2.0.2,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"29/May/14 18:35;sungkyu;Looks like IF keyword is for lightweight transaction, and it's assumed that both keys are specified. Maybe it should support the primary key only case, or report proper error message.;;;","03/Jun/14 21:43;jbellis;/cc [~thobbs];;;","04/Jun/14 08:54;slebresne;Definitively something we need to fix. That said, the first step here is probably to refuse conditions when the delete statement applies to multiple rows, because the code currently doesn't handle it and the fact we let the query fly is mainly a lack of validation.

In the longer run, it's possible to make such query work, but it's unclear to me whether that's a good idea or not (to be read as 'my initial hunch is that it's a bad idea'). Mainly because such query means we'll have to read the full partition during the Paxos rounds, thus making it pretty easy to OOM the server. Also, if we do handle such query, the condition will not only be a condition on the application of the statement anymore, but potentially a selection on what gets deleted. And I'm not sure we have a good way to indicate in the resultSet what got deleted and what didn't,  which is something one would expect.;;;","16/Oct/14 17:24;thobbs;It sounds like the best option for now is to require that the full primary key be specified when IF conditions are used in DELETEs.  If we want to reconsider allowing the statement in the description, we can always do that later.;;;","16/Oct/14 20:51;thobbs;6430-2.0.txt validates that all PK columns are restricted when performing conditional deletes.  I've also pushed a [dtest|https://github.com/thobbs/cassandra-dtest/tree/CASSANDRA-6430] that covers this.;;;","16/Oct/14 20:52;thobbs;[~blerer] can you review this in time for 2.0.11 and 2.1.1?;;;","17/Oct/14 09:45;slebresne;Did a quick review so it doesn't block the releases. Patch lgtm, committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Counter writes shouldn't be resubmitted after timeouts,CASSANDRA-6427,12682005,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,01/Dec/13 15:44,16/Apr/19 09:31,14/Jul/23 05:53,06/Dec/13 15:29,1.2.14,2.0.4,,,,,0,,,,,"CASSANDRA-4753 made SP.counterWriteTask() return a LocalMutationRunnable instead of the usual DroppableRunnalbe, and LMR resubmits the original runnable in case of timing out instead of simply dropping it.

For counters this is not the right option since it would lead to overcounting if the mutation got dropped-then-resubmitted and then retried by the user.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Dec/13 15:47;aleksey;6427.txt;https://issues.apache.org/jira/secure/attachment/12616476/6427.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,361264,,,Fri Dec 06 15:29:03 UTC 2013,,,,,,,,,,"0|i1q9x3:",361563,,,,,,,,,jbellis,,jbellis,Low,,1.2.1,,,,,,,,,,,,,,,,"02/Dec/13 00:17;jbellis;But if it's dropped-then-resubmitted then the user would only retry if the resubmit times out, right?  So we're better off than we would be in at least some situations (dropped, resubmitted, not retried) and no worse off in others (succeeded after timeout, but retried).;;;","02/Dec/13 21:17;aleksey;But the coordinator (and thus the user) has no way of telling if the timeout was caused, by a leader node death or if the runnable had been resubmitted and successfully applied eventually.

It's subtle, and it's about control.

Consider the two possible timeout handling strategies:
1. Never lose a counter write - prefer overcounting to undercounting. In this scenario one would always retry, and with the current behavior there will be a lot more overcounting than there would've been had Cassandra not resubmitted them under the cover.

2. Never repeat a counter write - prefer undercounting to overcounting. One would never retry, and with the current behavior would actually get more accurate results than with pre-1.2.1 C* (no resubmitting).

So on balance LocalMutationRunnable and DroppableRunnable do yield the same score, but I expect 1) to be more common, and LMR here makes things worse (and it's probably not something people are aware of and expect).

So it's not a big deal, and I'm fine with not-a-problem'ing it, but I do believe that going back to DroppableRunnable and not resubmitting is less surprising to users and more preferable in most cases.;;;","02/Dec/13 21:21;aleksey;Also, timeout due to node death is probably less common than timing out b/c of overload, and that makes always-retrying less accurate in general.;;;","06/Dec/13 03:27;jbellis;+1;;;","06/Dec/13 15:29;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage should not assume all DataBags are DefaultDataBags,CASSANDRA-6420,12681825,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mps,mps,mps,29/Nov/13 02:41,16/Apr/19 09:31,14/Jul/23 05:53,13/Dec/13 18:15,1.2.14,2.0.4,,,,,0,pig,,,,"CassandraStorage improperly assumes all DataBags are DefaultDataBags. As a result, natural Pig code can't be used with CassandraStorage. For example:
{quote}
{{B = FOREACH A GENERATE $0, TOBAG(TOTUPLE($1, $2));}}
{{STORE B into  'cassandra://MyKeySpace/MyColumnFamily' using CassandraStorage();}}
{quote}
fails with a complaint that a {{NonSpillableDataBag}} can't be converted into a {{DefaultDataBag}}.

Since the {{CassandraStorage}} code only calls methods from {{DataBag}}, there is no need for this artifical restriction. After applying the attached patch, the above code works fine, making CassandraStorage much easier to use.

This is my first submission to Cassandra, so I apologize for any incorrect process. Please let me know what I should do differently. In particular, I am a little unclear where I should put the test. I am thinking I should put it in ThriftColumnFamilyTest.java. Is this correct or should it be somewhere else? I'll create a test as soon as I understand. ",All environments,mps,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Nov/13 02:43;mps;patch.txt;https://issues.apache.org/jira/secure/attachment/12616333/patch.txt",,,,,,,,,,,,,,,,,,,,1.0,mps,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,361089,,,Mon Dec 23 05:24:03 UTC 2013,,,,,,,,,,"0|i1q8u7:",361388,1.2.11,2.1 rc3,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"29/Nov/13 02:44;mps;I have attached the diff, but I am afraid  I am a raw newbie and am not following the correct conventions. I apologize for the inconvenience and would greatly appreciate any guidance in this regard. ;;;","30/Nov/13 19:53;mps;When I initially filed this issue, I wasn't sure whether it should be classified as a bug or and improvement. After reading http://www.datastax.com/docs/datastax_enterprise1.0/about_pig, I think it should be classified as a bug, because the data {{B}} in the issue description is in a form that this link says should be suitable for storing into Cassandra.;;;","13/Dec/13 18:15;brandon.williams;Nice catch, committed.;;;","23/Dec/13 05:24;mps;Thanks, Brandon!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting max_hint_window_in_ms explicitly to null causes problems with JMX view,CASSANDRA-6419,12681794,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,zznate,zznate,zznate,28/Nov/13 17:04,16/Apr/19 09:31,14/Jul/23 05:53,19/Dec/13 23:44,1.2.14,2.0.4,,Local/Config,,,0,,,,,"Setting max_hint_window_in_ms to null in cassandra.yaml makes the StorageProxy mbean inaccessable. 

Stack trace when trying to view the bean through MX4J:
{code}
Exception during http request
javax.management.RuntimeMBeanException: java.lang.NullPointerException
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrow(DefaultMBeanServerInterceptor.java:839)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.rethrowMaybeMBeanException(DefaultMBeanServerInterceptor.java:852)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:651)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at mx4j.tools.adaptor.http.MBeanCommandProcessor.createMBeanElement(MBeanCommandProcessor.java:119)
	at mx4j.tools.adaptor.http.MBeanCommandProcessor.executeRequest(MBeanCommandProcessor.java:56)
	at mx4j.tools.adaptor.http.HttpAdaptor$HttpClient.run(HttpAdaptor.java:980)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.config.DatabaseDescriptor.getMaxHintWindow(DatabaseDescriptor.java:1161)
	at org.apache.cassandra.service.StorageProxy.getMaxHintWindow(StorageProxy.java:1506)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor8.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	... 4 more
Exception during http request
{code}",,rcoli,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Nov/13 17:13;zznate;6419-1.2.patch;https://issues.apache.org/jira/secure/attachment/12616288/6419-1.2.patch","28/Nov/13 17:13;zznate;6419-2.0.patch;https://issues.apache.org/jira/secure/attachment/12616289/6419-2.0.patch",,,,,,,,,,,,,,,,,,,2.0,zznate,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,361058,,,Mon Dec 30 21:33:23 UTC 2013,,,,,,,,,,"0|i1q8nb:",361357,1.2.12,2.0.2,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"28/Nov/13 17:05;zznate;Attaching a trivial patch to throw a config exception with a 'don't do that' message. ;;;","28/Nov/13 17:12;zznate;Trivial patches for 1.2 and 2.0 (only line numbers differ);;;","19/Dec/13 23:44;jbellis;LGTM, committed;;;","30/Dec/13 21:33;rcoli;FWIW, this looks like a single case of the systemic issue described in CASSANDRA-4967.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
auto_snapshots are not removable via 'nodetool clearsnapshot',CASSANDRA-6418,12681657,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lyubent,jre,jre,27/Nov/13 20:57,11/Aug/21 22:18,14/Jul/23 05:53,17/Mar/14 18:05,2.0.5,,,Tool/nodetool,,,0,,,,,"Snapshots of deleted CFs created via the ""auto_snapshot"" configuration parameter appear to not be tracked.  The result is that 'nodetool clearsnapshot <keyspace with deleted CFs>' does nothing, and short of manually removing the files from the filesystem, deleted CFs remain indefinitely taking up space.

I'm not sure if this is intended, but it seems pretty counter-intuitive.  I haven't found any documentation that indicates ""auto_snapshots"" would be ignored by 'nodetool clearsnapshot'.",auto_snapshot: true,jre,laura@datastax.com,lyubent,mishail,nickmbailey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6540,CASSANDRA-6821,,,,,,,,CASSANDRA-16843,,,,,,,,,,,"29/Nov/13 13:12;lyubent;6418_cassandra-2.0.patch;https://issues.apache.org/jira/secure/attachment/12616383/6418_cassandra-2.0.patch","21/Dec/13 01:37;lyubent;6418_cassandra-2.0_v5.patch;https://issues.apache.org/jira/secure/attachment/12619946/6418_cassandra-2.0_v5.patch","05/Dec/13 12:18;lyubent;6418_v2.patch;https://issues.apache.org/jira/secure/attachment/12617156/6418_v2.patch","12/Dec/13 01:38;lyubent;6418_v3_cassandra-2.0.patch;https://issues.apache.org/jira/secure/attachment/12618337/6418_v3_cassandra-2.0.patch","21/Dec/13 00:05;mishail;CASSANDRA-2.0-6418_v4.patch;https://issues.apache.org/jira/secure/attachment/12619937/CASSANDRA-2.0-6418_v4.patch",,,,,,,,,,,,,,,,5.0,lyubent,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360921,,,Mon Mar 17 18:05:06 UTC 2014,,,,,,,,,,"0|i1q7t3:",361220,2.0.2,,,,,,,,mishail,,mishail,Low,,,,,,,,,,,,,,,,,,"29/Nov/13 13:11;lyubent;Because cassandra uses CFStores to clear the snapshots once the CF is dropped, it goes out of scope. Changed the responsibility of clearing the snapshot from CFStore to o.a.c.db.Keyspace where the KS will use DatabaseDescriptor#getAllDataFileLocations to find the data directories then append the KS name and find all the CF dirs inside that KS directory instead of using the sstables' directories. This can be extended to be more granular (per CF) where instead of using all the dirs inside the KS directory, the user can provide a list of CFs to be removed. ;;;","29/Nov/13 19:23;jbellis;Can you review [~mishail]?;;;","05/Dec/13 05:16;mishail;{code}    [javac] C:\Users\mishail\workspace\cassandra\src\java\org\apache\cassandra\db\compaction\CompactionManager.java:810: error: cannot find symbol
    [javac]                 cfs.clearSnapshot(snapshotName);
    [javac]                    ^
    [javac]   symbol:   method clearSnapshot(String)
    [javac]   location: variable cfs of type ColumnFamilyStore
{code}

For *getAllKSDirectories*
* {{List<File> snapshotDirs = new ArrayList();}} .It's a raw data type.  You probably meant either {{newArrayList()}} or {{new ArrayList<>()}}
* {{new File(dataDirectory + ""/""  + ksName)}} and {{File(dataDirectory + ""/""  + ksName + ""/"" + cfDir)}} - you should use {{org.apache.cassandra.db.Directories.join(String...)}} instead that concatenation.
* I think it would be better to use one of {{File.listFiles}} methods instead of {{File.list()}} ;;;","05/Dec/13 12:18;lyubent;v2 adds a CFS#clearSnapshot function, and addresses all the nits.;;;","06/Dec/13 06:45;mishail;* {{getCFDirectory}} assumes that CF resides in a single location. If I'm correct, a CF can be in multiple dirs from {{DatabaseDescriptor.getAllDataFileLocations()}}
* I suspect {{File.listFiles()}} can return NULL if a directory doesn't exist

Minor remarks
* {{cfDir.exists() && cfDir.isDirectory()}} can be reduced to just {{cfDir.isDirectory()}}
* You can use {{dataFileLocations}} instead of {{DatabaseDescriptor.getAllDataFileLocations()}};;;","12/Dec/13 01:38;lyubent;Changed getCFDirectory to return a list of directories and added a check to ensure that CFs exist in v3.;;;","21/Dec/13 00:05;mishail;Looking at that duplicated code in {{getKSDirectories}} and {{getCFDirectory}}, and in {{getCFDirectory}} and {{Directories}}'s constructor, it occured to me that we don't need to call {{getCFDirectory}} in a static context. The only place it's used is {{doValidationCompaction}} where we always have a reference to CFS.

I'm attaching V4 of the patch. [~lyubent] what do you think?;;;","21/Dec/13 01:37;lyubent;LGTM, attaching v5 which adds a space to the for statement in {{Directories#getKSChildDirectories}}... ocd ftw.;;;","21/Dec/13 02:25;mishail;+1;;;","21/Dec/13 06:36;jbellis;committed;;;","14/Mar/14 17:41;nickmbailey;This doesn't handle the entire keyspace no longer existing. Due to calling 'getValidKeyspace' in storage service.;;;","17/Mar/14 18:00;nickmbailey;Re-opening, let me know if I should create a separate ticket.;;;","17/Mar/14 18:05;mishail;Let's use CASSANDRA-6821 fro that;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MoveTest fails in 1.2+,CASSANDRA-6416,12681619,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mshuler,jbellis,jbellis,27/Nov/13 18:01,16/Apr/19 09:31,14/Jul/23 05:53,05/Dec/13 17:06,1.2.13,2.0.4,,Legacy/Testing,,,0,qa-resolved,,,,"One test fails in 1.2, two in 2.0/trunk.",,marcuse,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,mshuler,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360883,,,Thu Dec 05 17:19:15 UTC 2013,,,,,,,,,,"0|i1q7kv:",361182,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"05/Dec/13 17:05;mshuler;trunk looks OK:
    [junit] Testsuite: org.apache.cassandra.service.MoveTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 11.137 sec

2.0 looks OK:
    [junit] Testsuite: org.apache.cassandra.service.MoveTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 7.376 sec

and 1.2:
    [junit] Testsuite: org.apache.cassandra.service.MoveTest
    [junit] Tests run: 3, Failures: 0, Errors: 0, Time elapsed: 6.52 sec

Fix looks like a winner:
----
commit d8c4e89b3e85e8cb41a438963845cb10a923a3d6
Author: Marcus Eriksson <marcuse@spotify.com>
Date:   Wed Dec 4 20:17:30 2013 +0100

    fix MoveTest;;;","05/Dec/13 17:19;marcuse;oops, didnt see this;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Snapshot repair blocks for ever if something happens to the ""I made my snapshot"" response",CASSANDRA-6415,12681617,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,jjordan,jjordan,27/Nov/13 17:47,16/Apr/19 09:31,14/Jul/23 05:53,02/Dec/13 18:52,1.2.13,2.0.4,,,,,1,repair,,,,"The ""snapshotLatch.await();"" can be waiting for ever and block all repair operations indefinitely if something happens that another node doesn't respond.

{noformat}
            public void makeSnapshots(Collection<InetAddress> endpoints)
            {
                try
                {
                    snapshotLatch = new CountDownLatch(endpoints.size());
                    IAsyncCallback callback = new IAsyncCallback()
                    {
                        public boolean isLatencyForSnitch()
                        {
                            return false;
                        }

                        public void response(MessageIn msg)
                        {
                            RepairJob.this.snapshotLatch.countDown();
                        }
                    };
                    for (InetAddress endpoint : endpoints)
                        MessagingService.instance().sendRR(new SnapshotCommand(tablename, cfname, sessionName, false).createMessage(), endpoint, callback);
                    snapshotLatch.await();
                    snapshotLatch = null;
                }
                catch (InterruptedException e)
                {
                    throw new RuntimeException(e);
                }
            }
{noformat}",,ahattrell,cburroughs,cywjackson,jeromatron,jjordan,mishail,nickmbailey,tvachon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Dec/13 16:57;yukim;6415-1.2.txt;https://issues.apache.org/jira/secure/attachment/12616573/6415-1.2.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360881,,,Fri Apr 18 20:47:32 UTC 2014,,,,,,,,,,"0|i1q7kf:",361180,1.2.10,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"27/Nov/13 17:56;yukim;In detail, 'makeSnapshot' happens after receiving all TreeResponses which are handled on single-threaded 'ANTI_ENTROPY' stage. So when it stuck, receiving repair messages or Merkle tree comparisons are all blocked afterward.;;;","02/Dec/13 16:57;yukim;I think it is better to re-think use of executors and threads around repair, for example, right now Differencer runs on ANTI_ENTROPY stage one by one.

But IMO, that would be quite a change for 1.2.x, so I propose just changing Snapshot response message type to INTERNAL_RESPONSE instead of REQUEST_RESPONSE which is droppable by MessagingService. So at least snapshot request messages don't get lost in next 1.2 release.;;;","02/Dec/13 17:28;jbellis;+1;;;","02/Dec/13 18:52;yukim;Committed, thanks!;;;","05/Dec/13 23:39;jeromatron;See the longer term solution here: CASSANDRA-6455;;;","02/Jan/14 21:58;nickmbailey;This was also fixed in the 2.0 branch in 2.0.4 correct?;;;","02/Jan/14 22:01;jbellis;Yes.;;;","18/Apr/14 20:47;cywjackson;I ran into the stuck issue on 1.2.10

Upgraded to 1.2.16, I could see repair is not ""stuck"", in a sense I see multiple repair sessions/stages started and finished.

But, in the end (after waiting a long time), I see that there is no more activity from the log, and also compactionstats/netstats, but yet the tpstats still show Active and Pending count in the stages:

AntiEntropyStage                  1         2           5073         0                 0
AntiEntropySessions               1         1             44         0                 0
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlacklistingCompactionsTest fails in trunk,CASSANDRA-6414,12681615,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,27/Nov/13 17:16,16/Apr/19 09:31,14/Jul/23 05:53,16/Jan/14 14:29,2.1 beta1,,,Legacy/Testing,,,0,,,,,Passes in 2.0 HEAD.  Bisect should be relatively easy.,,mshuler,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Dec/13 17:33;mshuler;6414_out.txt;https://issues.apache.org/jira/secure/attachment/12617193/6414_out.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360879,,,Thu Jan 16 16:39:22 UTC 2014,,,,,,,,,,"0|i1q7jz:",361178,,,,,,,,,,,,Normal,,2.1 rc3,,,,,,,,,,,,,,,,"27/Nov/13 17:17;jbellis;I also note that {{ant test -Dtest.name=BlacklistingCompactionsTest}} dumps a TON of crap to stdout in trunk but not in 2.0.  Is this logback's fault [~dbrosius]?;;;","05/Dec/13 17:08;mshuler;quick fix for logback spam:
{code}
sed -i 's/commitlog_sync_batch_window_in_ms: 1.0/commitlog_sync_batch_window_in_ms: 100/' test/conf/cassandra.yaml
{code};;;","05/Dec/13 17:32;mshuler;The difference between the output without the above conf edit (first in 6414_out attachment) and with the edit (ran it a couple times at the end of the file) is interesting and hopefully helpful with regards to the java.io.EOFException errors being the cause, I think.  Setting the batch_window to 1s results in the test simply timing out, which isn't very helpful.

Bisecting next;;;","05/Dec/13 19:35;mshuler;a few different good/bad strategies - this one seems to be closest:

my last chain of bisect:
...
((99b5040...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
Bisecting: 32 revisions left to test after this (roughly 5 steps)
[6c5c12de6bd6f9b035501ea3e4dc53497a20b9fe] Merge branch 'cassandra-2.0' into trunk
...
((6c5c12d...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect good
Bisecting: 16 revisions left to test after this (roughly 4 steps)
[88a390064425171c617b66280a0a5961b234599b] Merge branch 'cassandra-2.0' into trunk
...
((88a3900...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
Bisecting: 7 revisions left to test after this (roughly 3 steps)
[5dabd1cc0c65b329ff518d7ad3f09e4c11494f18] fix latency displays caused by metrics-core now using nanoseconds
...
((5dabd1c...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
Bisecting: 3 revisions left to test after this (roughly 2 steps)
[e890b1f2ccee051d27ff3bec9b6e8ef63e7ff508] increase -Xss to 256k
...
((e890b1f...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
Bisecting: 1 revision left to test after this (roughly 1 step)
[873ce0cb3f05d55753f205092e681c963cd20fc4] clarify error messages for zero/multiple PKs patch by Lyben Todorov; reviewed by jbellis for CASSANDRA-5875
...
compile fail
...
((873ce0c...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect skip
Bisecting: 1 revision left to test after this (roughly 1 step)
[29605aedd9e19f2f07042cd0aa6b31b6c94a4aea] switch logging from log4j to logback patch by dbrosius reviewed by jbellis for cassandra-5883
...
compile fail
...
((29605ae...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect skip
Bisecting: 1 revision left to test after this (roughly 1 step)
[fa0b7cf8b279c568aa7b2fcbaad91510797d838f] add logback-classic jar
...
((fa0b7cf...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
There are only 'skip'ped commits left to test.
The first bad commit could be any of:
29605aedd9e19f2f07042cd0aa6b31b6c94a4aea
873ce0cb3f05d55753f205092e681c963cd20fc4
fa0b7cf8b279c568aa7b2fcbaad91510797d838f
We cannot bisect more!;;;","05/Dec/13 23:44;mshuler;(edit: added commit, since it's a one-liner)

I misunderstood the logback spam as the WARN entries from CASSANDRA-3578.  So ignoring all the ERROR traces in the output (which I was marking bad above), just straight SUCESSFUL/FAILED test gets me:

((f3dc188...)|BISECTING)mshuler@hana:~/git/cassandra$ git bisect bad
f3dc188e203b3db980ee81df05390968043cb601 is the first bad commit
commit f3dc188e203b3db980ee81df05390968043cb601
Author: Jonathan Ellis <jbellis@apache.org>
Date:   Fri Nov 22 17:33:07 2013 -0600

    move setting lastCompactedKey to before the return-if-nothing-added

:040000 040000 0c616036c3aac24861a0f43b21f5812faecf2c92 33fb0a0761168d605a224dc87522785afe8cb5a1 M      src
----
$ git show f3dc188e203b3db980ee81df05390968043cb601
commit f3dc188e203b3db980ee81df05390968043cb601 (HEAD, refs/bisect/bad)
Author: Jonathan Ellis <jbellis@apache.org>
Date:   Fri Nov 22 17:33:07 2013 -0600

    move setting lastCompactedKey to before the return-if-nothing-added

{code}
diff --git a/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java b/src/java/org/apache/cassandra/
index 76f51d1..232d1f7 100644
--- a/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java
+++ b/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java
@@ -142,6 +142,7 @@ public class LeveledManifest
             int thisLevel = remove(sstable);
             minLevel = Math.min(minLevel, thisLevel);
         }
+        lastCompactedKeys[minLevel] = SSTableReader.sstableOrdering.max(added).last;
 
         // it's valid to do a remove w/o an add (e.g. on truncate)
         if (added.isEmpty())
@@ -152,7 +153,6 @@ public class LeveledManifest
 
         for (SSTableReader ssTableReader : added)
             add(ssTableReader);
-        lastCompactedKeys[minLevel] = SSTableReader.sstableOrdering.max(added).last;
     }
 
     public synchronized void repairOverlappingSSTables(int level)
{code};;;","06/Dec/13 02:43;jbellis;That should be purely cosmetic, and indeed reverting f3dc188e203b3db980ee81df05390968043cb601 still leaves BCT erroring out.;;;","15/Jan/14 20:31;mshuler;Multiple git bisections for purely BUILD SUCCESSFUL / FAILED arrive at the above commit as the first bad commit.  When I am on commit f3dc188 and revert it (ending up on 6164a83), 'ant clean jar && ant test -Dtest.name=BlacklistingCompactionsTest' gives me BUILD SUCCESSFUL.  Reverting f3dc188 from current trunk also fixes this test for me.

That said, I *do* have tons of traceback output, even when successful at that point.  I'm going to see if there is a common initial trace message (maybe ""org.apache.cassandra.io.sstable.CorruptSSTableException: java.io.EOFException"") and see if I can find where those are starting.

git bisect run script:
{code}
#!/bin/bash
ant realclean
ant jar || exit 125
for i in {1..3}; do
  ant test -Dtest.name=BlacklistingCompactionsTest
  RET=$?
  if [ ""$RET"" -eq ""0"" ]; then
    break
  fi
done
exit $RET
{code}

consistently arrives at:

{noformat}
# bad: [63c490af9cbe392ba025f15854173eb4f7fbbfad] encapsulate SecondaryIndex.estimateResultRows patch by Miguel Angel Fernandez Diaz; reviewed by Sam Tunnicliffe for CASSANDRA-6498
# good: [7514e61b48e9456cf6591abaf6dbf17b52217883] update changes
git bisect start 'trunk' 'cassandra-2.0'
# good: [6c379343561766724e48d1d7cf98e282e8ec91dd] Merge branch 'cassandra-2.0' into trunk
git bisect good 6c379343561766724e48d1d7cf98e282e8ec91dd
# skip: [a13c6dcbb7a6ba74b27e50eef4bfd0f80eaea121] Merge branch 'cassandra-2.0' into trunk
git bisect skip a13c6dcbb7a6ba74b27e50eef4bfd0f80eaea121
# bad: [351d49b06bd6887447eecb149b3cdf1697ab41ac] validate directory permissions on startup Patch by Lyuben Todorov and Koray Sariteke; reviewed by Mikhail Stepura for CASSANDRA-5818
git bisect bad 351d49b06bd6887447eecb149b3cdf1697ab41ac
# good: [5c21711e76731a6f48d67ca956ea865fda4574b4] merge from 2.0
git bisect good 5c21711e76731a6f48d67ca956ea865fda4574b4
# good: [52cc7efb2bcd47285148da85c089d796cb20734a] Merge branch 'cassandra-2.0' into trunk
git bisect good 52cc7efb2bcd47285148da85c089d796cb20734a
# good: [40598efa6344333d4d4deee2c1ec3e71bd931066] Merge branch 'cassandra-2.0' into trunk
git bisect good 40598efa6344333d4d4deee2c1ec3e71bd931066
# skip: [1bfd062fdc9daa35fbabcebb3ac31e726504f1ff] Merge branch 'cassandra-2.0' into trunk
git bisect skip 1bfd062fdc9daa35fbabcebb3ac31e726504f1ff
# bad: [7ea5b40b7172ec9f3fdecca533e19d8a165de7df] don't leave replaced SSTRs around to break other tests patch by Tyler Hobbs
git bisect bad 7ea5b40b7172ec9f3fdecca533e19d8a165de7df
# bad: [9f3a7f8a698aaaa9be44bed01aaf526567df8aca] simple naming fixes
git bisect bad 9f3a7f8a698aaaa9be44bed01aaf526567df8aca
# bad: [8732fe938659232b3f0c63933b125131fa50fd2e] Merge branch 'cassandra-2.0' into trunk
git bisect bad 8732fe938659232b3f0c63933b125131fa50fd2e
# good: [87b39c8af3477c3b80f124da23b46de350a259e7] Merge branch 'cassandra-2.0' into trunk
git bisect good 87b39c8af3477c3b80f124da23b46de350a259e7
# good: [a10150542c662a4cc69ce1b88f48636d1e6884f7] merge from 2.0
git bisect good a10150542c662a4cc69ce1b88f48636d1e6884f7
# bad: [f3dc188e203b3db980ee81df05390968043cb601] move setting lastCompactedKey to before the return-if-nothing-added
git bisect bad f3dc188e203b3db980ee81df05390968043cb601
{noformat};;;","15/Jan/14 21:31;mshuler;git bisect run, looking just for the loads of trace output does looks like it's from the logback addition
{code}
#!/bin/bash
ant realclean
ant jar || exit 125
if ant test -Dtest.name=BlacklistingCompactionsTest | grep ""org.apache.cassandra.io.sstable.CorruptSSTableException: java.io.EOFException""; then
  RET=1
else
  RET=0
fi
exit $RET
{code}

{noformat}
# bad: [63c490af9cbe392ba025f15854173eb4f7fbbfad] encapsulate SecondaryIndex.estimateResultRows patch by Miguel Angel Fernandez Diaz; reviewed by Sam Tunnicliffe for CASSANDRA-6498
# good: [7514e61b48e9456cf6591abaf6dbf17b52217883] update changes
git bisect start 'trunk' 'cassandra-2.0'
# bad: [6c379343561766724e48d1d7cf98e282e8ec91dd] Merge branch 'cassandra-2.0' into trunk
git bisect bad 6c379343561766724e48d1d7cf98e282e8ec91dd
# bad: [5102e8d748289876964305292d75f5eba28a40dd] Merge branch 'cassandra-2.0' into trunk
git bisect bad 5102e8d748289876964305292d75f5eba28a40dd
# bad: [cd6aa2d1ef16a8af2e5e3de20e2389575d8021e1] switch to use parameterized logging
git bisect bad cd6aa2d1ef16a8af2e5e3de20e2389575d8021e1
# bad: [5dabd1cc0c65b329ff518d7ad3f09e4c11494f18] fix latency displays caused by metrics-core now using nanoseconds
git bisect bad 5dabd1cc0c65b329ff518d7ad3f09e4c11494f18
# good: [e718467ec471bc5c952c3eaeac8bdc2b3db9eac5] Merge branch 'cassandra-2.0' into trunk
git bisect good e718467ec471bc5c952c3eaeac8bdc2b3db9eac5
# good: [a69457c14a36c5eb3db523b3eb42b6d399993177] Merge branch 'cassandra-2.0' into trunk
git bisect good a69457c14a36c5eb3db523b3eb42b6d399993177
# skip: [873ce0cb3f05d55753f205092e681c963cd20fc4] clarify error messages for zero/multiple PKs patch by Lyben Todorov; reviewed by jbellis for CASSANDRA-5875
git bisect skip 873ce0cb3f05d55753f205092e681c963cd20fc4
# bad: [fa0b7cf8b279c568aa7b2fcbaad91510797d838f] add logback-classic jar
git bisect bad fa0b7cf8b279c568aa7b2fcbaad91510797d838f
# good: [6c5c12de6bd6f9b035501ea3e4dc53497a20b9fe] Merge branch 'cassandra-2.0' into trunk
git bisect good 6c5c12de6bd6f9b035501ea3e4dc53497a20b9fe
# skip: [29605aedd9e19f2f07042cd0aa6b31b6c94a4aea] switch logging from log4j to logback patch by dbrosius reviewed by jbellis for cassandra-5883
git bisect skip 29605aedd9e19f2f07042cd0aa6b31b6c94a4aea


# There are only 'skip'ped commits left to test.
# The first bad commit could be any of:
# 29605aedd9e19f2f07042cd0aa6b31b6c94a4aea
# 873ce0cb3f05d55753f205092e681c963cd20fc4
# fa0b7cf8b279c568aa7b2fcbaad91510797d838f
# We cannot bisect more!
# bisect run cannot continue any more
{noformat};;;","15/Jan/14 22:30;mshuler;running 10 iterations, pass 10x to get a ""good""
{code}
#!/bin/bash
ant realclean
ant jar || exit 125
for i in {1..10}; do
  ant test -Dtest.name=BlacklistingCompactionsTest
  RET=$?
  if [ ""$RET"" -ne ""0"" ]; then
    break
  fi
done
exit $RET
{code};;;","15/Jan/14 23:42;brandon.williams;I get a very consistent pointer to a552b305f3d1b17e394744b18efd7f40599f3c2e (CASSANDRA-5590).  A single iteration won't pass there, but all of them pass on the commit before.;;;","16/Jan/14 13:15;slebresne;bq. That should be purely cosmetic, and indeed reverting f3dc188e203b3db980ee81df05390968043cb601 still leaves BCT erroring out.

I'll call your bluff!

As far as I can tell, the actual error that makes the test error out is
{noformat}
    [junit] java.lang.RuntimeException: java.util.NoSuchElementException
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.runWithCompactionsDisabled(ColumnFamilyStore.java:2076)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.truncateBlocking(ColumnFamilyStore.java:2020)
    [junit] 	at org.apache.cassandra.db.compaction.BlacklistingCompactionsTest.testBlacklisting(BlacklistingCompactionsTest.java:153)
    [junit] 	at org.apache.cassandra.db.compaction.BlacklistingCompactionsTest.testBlacklistingWithLeveledCompactionStrategy(BlacklistingCompactionsTest.java:67)
    [junit] Caused by: java.util.NoSuchElementException
    [junit] 	at java.util.Collections$EmptyIterator.next(Collections.java:3006)
    [junit] 	at com.google.common.collect.Ordering.max(Ordering.java:536)
    [junit] 	at com.google.common.collect.Ordering.max(Ordering.java:555)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.replace(LeveledManifest.java:142)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:162)
    [junit] 	at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:469)
    [junit] 	at org.apache.cassandra.db.DataTracker.markObsolete(DataTracker.java:246)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.markObsolete(ColumnFamilyStore.java:1097)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.discardSSTables(ColumnFamilyStore.java:2359)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore$10.run(ColumnFamilyStore.java:2004)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.runWithCompactionsDisabled(ColumnFamilyStore.java:2072)
{noformat}
and as it happens, due to f3dc188e203b3db980ee81df05390968043cb601, {{SSTableReader.sstableOrdering.max(added).last}} will be called even when {{added}} is empty, which is clearly documented as wrong for Ordering.max(). And while BCT fails every time for me on trunk, it does work fine with f3dc188e203b3db980ee81df05390968043cb601 reverted (it does log a bunch of ERROR because compaction complains about the sstable being corrupted, which is correct, and I do have to bump the test timeout on my box to not have the test timeout, but it does pass).

Haven't reverted f3dc188e203b3db980ee81df05390968043cb601 yet because I'm not sure what was the initial intent of that commit tbh.;;;","16/Jan/14 14:29;jbellis;You're right, f3dc188e203b3db980ee81df05390968043cb601 is a regression.  Reverted.

And it passes now, so I guess there must have been two problems before, although I wouldn't rule out ""Jonathan smoking crack"" either.;;;","16/Jan/14 16:39;brandon.williams;I think what happened with a552b305f is, we did have a problem at some point, and later fixed it.  I didn't know how far back to go to find a 'good' spot for bisect, so I bisected from 2.0 to trunk and arrived there, since the bisect script couldn't tell the difference between NPE (f3dc188e) and timeout (a552b305) so I think we're fine.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Saved KeyCache prints success to log; but no file present",CASSANDRA-6413,12681604,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,cburroughs,cburroughs,27/Nov/13 15:53,16/Apr/19 09:31,14/Jul/23 05:53,05/Dec/13 15:44,1.2.13,2.0.4,,,,,0,,,,,"Cluster has a single keyspace with 3 CFs.  All used to have ROWS_ONLY, two were switched to KEYS_ONLY about 2 days ago.  Row cache continues to save fine, but there is no saved key cache file present on any node in the cluster.

{noformat}
6925: INFO [CompactionExecutor:12] 2013-11-27 10:12:02,284 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 118 ms
6941:DEBUG [CompactionExecutor:14] 2013-11-27 10:17:02,163 AutoSavingCache.java (line 233) Deleting old RowCache files.
6942: INFO [CompactionExecutor:14] 2013-11-27 10:17:02,310 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 146 ms
8745:DEBUG [CompactionExecutor:6] 2013-11-27 10:37:25,140 AutoSavingCache.java (line 233) Deleting old RowCache files.
8746: INFO [CompactionExecutor:6] 2013-11-27 10:37:25,283 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 143 ms
8747:DEBUG [CompactionExecutor:6] 2013-11-27 10:37:25,283 AutoSavingCache.java (line 233) Deleting old KeyCache files.
8748: INFO [CompactionExecutor:6] 2013-11-27 10:37:25,625 AutoSavingCache.java (line 289) Saved KeyCache (21181 items) in 342 ms
8749:DEBUG [CompactionExecutor:6] 2013-11-27 10:37:25,625 AutoSavingCache.java (line 233) Deleting old RowCache files.
8750: INFO [CompactionExecutor:6] 2013-11-27 10:37:25,759 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 134 ms
8751:DEBUG [CompactionExecutor:6] 2013-11-27 10:37:25,759 AutoSavingCache.java (line 233) Deleting old RowCache files.
8752: INFO [CompactionExecutor:6] 2013-11-27 10:37:25,893 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 133 ms
8753:DEBUG [CompactionExecutor:6] 2013-11-27 10:37:25,893 AutoSavingCache.java (line 233) Deleting old RowCache files.
8754: INFO [CompactionExecutor:6] 2013-11-27 10:37:26,026 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 133 ms
9915:DEBUG [CompactionExecutor:18] 2013-11-27 10:42:01,851 AutoSavingCache.java (line 233) Deleting old KeyCache files.
9916: INFO [CompactionExecutor:18] 2013-11-27 10:42:02,185 AutoSavingCache.java (line 289) Saved KeyCache (22067 items) in 334 ms
9917:DEBUG [CompactionExecutor:17] 2013-11-27 10:42:02,279 AutoSavingCache.java (line 233) Deleting old RowCache files.
9918: INFO [CompactionExecutor:17] 2013-11-27 10:42:02,411 AutoSavingCache.java (line 289) Saved RowCache (50000 items) in 131 ms
{noformat}

{noformat}
$ ll ~/shared/saved_caches/
total 3472
-rw-rw-r-- 1 cassandra cassandra 3551608 Nov 27 10:42 Foo-Bar-RowCache-b.db

{noformat}",1.2.11,cburroughs,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Dec/13 23:55;jbellis;6413-v2.txt;https://issues.apache.org/jira/secure/attachment/12617081/6413-v2.txt","04/Dec/13 06:42;mishail;CASSANDRA-1.2-6413.patch;https://issues.apache.org/jira/secure/attachment/12616946/CASSANDRA-1.2-6413.patch",,,,,,,,,,,,,,,,,,,2.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360868,,,Thu Dec 12 12:08:58 UTC 2013,,,,,,,,,,"0|i1q7hj:",361167,1.2.12,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"27/Nov/13 21:05;mishail;[~cburroughs] what's your {{key_cache_size_in_mb}} in _cassandra.yaml_ ?;;;","27/Nov/13 21:10;cburroughs;{noformat}
key_cache_size_in_mb: 48
key_cache_save_period: 900

# Number of keys from the key cache to save
# Disabled by default, meaning all keys are going to be saved
# key_cache_keys_to_save: 100
{noformat};;;","28/Nov/13 07:15;mishail;That's weird. Does the directory contain only that single file? Are there any suspicious messages in the logs?;;;","02/Dec/13 16:33;cburroughs; * Yes, on every node in the cluster `saved_caches_directory` contains only a single file.
 * I don't see anything else relevant in the logs.  The log clip in the ticket is with AutoSavingCache set to DEBUG logging.;;;","04/Dec/13 06:01;mishail;Silly me :) The bug is so obvious, but only manifests itself if both Key cache AND row cache are enabled

{code:title=org.apache.cassandra.cache.AutoSavingCache.java|borderStyle=solid}
        private void deleteOldCacheFiles()
        {
            File savedCachesDir = new File(DatabaseDescriptor.getSavedCachesLocation());

            if (savedCachesDir.exists() && savedCachesDir.isDirectory())
            {
                for (File file : savedCachesDir.listFiles())
                {
                    if (file.isFile() && file.getName().endsWith(cacheType.toString()))
                    {
                        if (!file.delete())
                            logger.warn(""Failed to delete {}"", file.getAbsolutePath());
                    }

                    if (file.isFile() && file.getName().endsWith(CURRENT_VERSION + "".db""))
                    {
                        if (!file.delete())
                            logger.warn(""Failed to delete {}"", file.getAbsolutePath());
                    }
                }
            }
        }
{code}

So, each cache deletes FILES FROM ALL CACHES from the {{saved_caches}} and then happily writes its files.
The last save wins, ;;;","04/Dec/13 06:42;mishail;Patch: each cache should delete only its own files;;;","04/Dec/13 06:53;mishail;Looks like it was introduced in https://github.com/apache/cassandra/commit/cfe585c2c420c6e8445eb4c3309b09db8cf134ac for CASSANDRA-3762;;;","04/Dec/13 22:31;jbellis;Hmm...  Maybe the 3762 line was supposed to be

{code}
if (file.isFile() && !file.getName().endsWith(CURRENT_VERSION + "".db""))
{code}

to clean out obsolete cache files?;;;","04/Dec/13 23:29;mishail;[~jbellis]
In that case the obsolete files  would be never deleted. Their filenames end with ""<cacheType>-CURRENT_VERSION.db"", so they would fall through those two conditions.;;;","04/Dec/13 23:55;jbellis;v2 attached to clean out both old- and new- format files of the correct type.;;;","05/Dec/13 00:03;mishail;[~jbellis]
Do you think there will be problems with the simple {{contains(cacheType.toString())}} approach?;;;","05/Dec/13 05:43;jbellis;Wouldn't that match CF or KS names or even other parts of the path?;;;","05/Dec/13 06:03;mishail;That will match for sure. But what are chances?
Nevertheless, I'm ok with your patch.;;;","05/Dec/13 14:27;cburroughs;Jira's priority guide thingy for minor says ""easy workaround is present"".  Is there a way to get both caches to persist before upgrading with the fix?;;;","05/Dec/13 15:41;jbellis;No.  ""Minor"" also means ""doesn't seriously affect cluster stability or ability to respond to client requests."";;;","05/Dec/13 15:44;jbellis;committed v2;;;","12/Dec/13 12:08;cburroughs;As a followup note, this bug appears to have prevented any of the system-* KeyCaches from being saved.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gossip performance improvement at node startup,CASSANDRA-6409,12681470,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,qconner,qconner,27/Nov/13 01:29,16/Apr/19 09:31,14/Jul/23 05:53,27/Nov/13 17:38,1.2.13,2.0.4,,,,,0,,,,,"With large clusters (> 500 nodes) and num_tokens > 255 we sometimes see a node have trouble starting up.  CPU usage for one thread is pegged.

We see this concurrent with Gossip flaps on the node trying to learn the ring topology.  Other nodes on the ring, that are already at steady state do not seem to suffer.  It is the node joining the large ring that has trouble.",,cburroughs,qconner,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6127,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/13 01:33;qconner;2013-11-26_17-40-08.png;https://issues.apache.org/jira/secure/attachment/12615967/2013-11-26_17-40-08.png","27/Nov/13 02:33;jbellis;endpointToTokenMapCPU.txt;https://issues.apache.org/jira/secure/attachment/12615981/endpointToTokenMapCPU.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360735,,,Wed Nov 27 17:38:40 UTC 2013,,,,,,,,,,"0|i1q6nz:",361034,1.2.11,,,,,,,,,,,Normal,,,,,,,,,,,,,,,qconner,,,"27/Nov/13 01:31;qconner;sub ticket for the cpu peg at startup symptom.;;;","27/Nov/13 01:33;qconner;Taken about 10 minutes after node startup.  Gossip should have settled down by now.;;;","27/Nov/13 02:33;jbellis;Patch attached to move the Multimap computation into only the block where it is used and not each gossip update.;;;","27/Nov/13 15:23;qconner;+1

Moving getEndpointToTokenMapForReading() did the trick.  I tested the patch against cassandra-1.2 branch and the persistent flapping node and high cpu use symptom disappeared in my 500 node cluster setup (num_tokens=512).
;;;","27/Nov/13 17:38;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL/Thrift request hangs forever when querying more than certain amount of data,CASSANDRA-6407,12681357,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,ngrigoriev,ngrigoriev,26/Nov/13 19:55,16/Apr/19 09:31,14/Jul/23 05:53,11/Jan/14 01:28,2.0.5,,,,,,1,,,,,"I have a table like this (slightly simplified for clarity):

{code}
CREATE TABLE my_test_table (
		uid  uuid,
		d_id	 uuid,
		a_id	 uuid,	
		c_id	text,
		i_id	blob,	
		data	text,
		PRIMARY KEY ((uid, d_id, a_id), c_id, i_id)
);
{code}

I have created about over a hundred (117 to be specific) of sample entities with the same row key and different clustering keys. Each has a blob of approximately 4Kb.

I have tried to fetch all of them with a query like this via CQLSH:

{code}
select * from my_test_table where uid=44338526-7aac-4640-bcde-0f4663c07572 and a_id=00000000-0000-4000-0000-000000000002 and d_id=00000000-0000-1e64-0000-000000000001 and c_id='list-2'
{code}

This query simply hangs in CQLSH, it does not return at all until I abort it.

Then I started playing with LIMIT clause and found that this query returns instantly (with good data) when I use LIMIT 55 but hangs forever when I use LIMIT 56.

Then I tried to just query all ""i_id"" values like this:

{code}
select i_id from my_test_table where uid=44338526-7aac-4640-bcde-0f4663c07572 and a_id=00000000-0000-4000-0000-000000000002 and d_id=00000000-0000-1e64-0000-000000000001 and c_id='list-2'
{code}

And this query returns instantly with the complete set of 117 values. So I started thinking that it must be something about the total size of the response, not the number of results or the number of columns to be fetches in slices. And I have tried another test:

{code}
select cdata from my_test_table where uid=44338526-7aac-4640-bcde-0f4663c07572 and a_id=00000000-0000-4000-0000-000000000002 and d_id=00000000-0000-1e64-0000-000000000001 and c_id='list-2' LIMIT 63
{code}

This query returns instantly but if I change the limit to 64 it hangs forever. Since my blob is about 4Kb for each entity it *seems* like the query hangs when the total size of the response exceeds 252..256Kb. Looks quite suspicious especially because 256Kb is such a particular number. I am wondering if this has something to do with the result paging.

I did not test if the issue is reproducible outside of CQLSH but I do recall that I observed somewhat similar behavior when fetching relatively large data sets.

I can consistently reproduce this problem on my cluster. I am also attaching the jstack output that I have captured when CQLSH was hanging on one of these queries.","Oracle Linux 6.4, JDK 1.7.0_25-b15, Cassandra 2.0.2",mishail,ngrigoriev,nickmbailey,weideng,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Nov/13 19:58;ngrigoriev;cassandra.jstack.gz;https://issues.apache.org/jira/secure/attachment/12615897/cassandra.jstack.gz","09/Jan/14 03:00;ngrigoriev;cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12622094/cassandra.yaml","09/Jan/14 02:59;ngrigoriev;cassandra6407test.cql.gz;https://issues.apache.org/jira/secure/attachment/12622093/cassandra6407test.cql.gz","09/Jan/14 23:23;xedin;disruptor-thrift-server-0.3.3-SNAPSHOT.jar;https://issues.apache.org/jira/secure/attachment/12622289/disruptor-thrift-server-0.3.3-SNAPSHOT.jar","09/Jan/14 03:19;ngrigoriev;system.log.gz;https://issues.apache.org/jira/secure/attachment/12622098/system.log.gz",,,,,,,,,,,,,,,,5.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360622,,,Sat Jan 11 01:28:46 UTC 2014,,,,,,,,,,"0|i1q5yv:",360921,2.0.4,,,,,,,,,,,Normal,,2.0.2,,,,,,,,,,,,,,,,"26/Nov/13 19:58;ngrigoriev;jstack output for Cassandra server process on the host where I run CQLSH;;;","09/Jan/14 00:38;ngrigoriev;Some additional details.

I can confirm that the problem is not limited to CQLSH, it can be reproduced via CQL/Thrift. Which does not surprise me, I was assuming that's what CQLSH is using today.

One of my coworkers has pointed out that he did not observe this problem in his small single-node cluster, even with larger amounts of data in one response. I was curious enough to try it so I have configured a single-node Cassandra 2.0.4 cluster on a spare Linux machine, loaded my schema there and generated the ""problematic"" test data set. I could not reproduce the problem, i.e. I was getting back much larger result set than in my larger cluster. After that I took my ""production"" cassandra.yaml, changed the cluster name to a dummy one, reinitialized that single-node cluster with new config, reloaded the data and I could immediately reproduce the problem. To keep long story short, I was comparing the parameters I changed in my config with the defaults and finally found THE parameter that is clearly responsible for this issue: rpc_server_type. If set to ""sync"", then I can query larger data set. If set to ""hsha"" - I can only query up to ~256Kb of data and then the connection gets stuck forever.

Anything obvious that I am missing about the limitations of hsha? ;;;","09/Jan/14 00:48;ngrigoriev;It sounds somewhat related to:

CASSANDRA-4573

CASSANDRA-6373

;;;","09/Jan/14 01:18;jbellis;/cc [~xedin];;;","09/Jan/14 01:59;xedin;[~ngrigoriev] Is it possible to you to give us at least part of the data that you have been testing with? It sounds like it could be a bug in hsha implementation of the thrift server.;;;","09/Jan/14 03:09;ngrigoriev;[~xedin] I have prepared a simple test that does demonstrate the problem even in a small single-node cluster. Interestingly enough, with this test and such a small cluster with no load at all sometimes it actually works.

So, here is how I use it:

1. Set the RPC server type to hsha
2. Load the attached CQL ile
3. Use CQLSH
   use cassandra6407test ;
   select * from my_test_table ;

In most of the cases this SELECT gets stuck forever. Sometimes if you interrupt it (after a while) and do it again it actually returns all the data on the second attempt. Sometimes it does not. If you restart CQLSH and do it again - it will get stuck again. Specifying a LIMIT above 24-25 demonstrates similar behavior.

If you switch  RPC server type to ""sync"" and restart, then ""select * from my_test_table ;"" works all the time.

It almost feels like some sort of race condition or a timing issue somewhere between the part that produces the query result and the part that streams it back to the client.

The server config I have attached is simplified, I have disabled JNA, JEMalloc etc to have a configuration that is as close as possible to the default installation.;;;","09/Jan/14 03:19;ngrigoriev;this is the DEBUG log -  I have tried that ""select *"" request 3 times after restarting the server with RPC server type set to hsha.;;;","09/Jan/14 03:57;xedin;Thank you, [~ngrigoriev]! I will start working on this asap.;;;","09/Jan/14 06:02;nickmbailey;This definitely sounds like the same thing as CASSANDRA-6373 given that it happens with vnode clusters which would make describe_ring return a fairly large response.;;;","09/Jan/14 06:11;xedin;Yeah, they look similar but at lease we can reproduce, can you try one more thing please while you are on it - disable vnodes and try hsha with your data?;;;","09/Jan/14 06:17;nickmbailey;I'm not sure if you are asking me to try that in CASSANDRA-6373. I've verified it does not happen with vnodes disabled. The steps I described there should also let you reproduce the issue in a unit test.;;;","09/Jan/14 07:25;xedin;I was asking you to test your queries with hsha and vnodes disabled on a single node, but you mentioned right now that it works without vnodes so we are good, I will take it from here, thanks!;;;","09/Jan/14 21:49;xedin;I identified the problem and fixing it right now, you won't need to wait for next Cassandra release as it's a problem with disruptor_thrift_server, so as a temporary solution you will be just need to drop in the updated jar into lib/.;;;","09/Jan/14 22:32;ngrigoriev;[~xedin] Source patch will be OK too, whichever is simpler for you. We are building our Cassandra from source with two patches that are scheduled for 2.0.5. I do not mind rebuilding another dependency :) Thanks!;;;","09/Jan/14 23:23;xedin;There is the updated jar, simply replace 0.3.2 (or remove 0.3.2) with this one in your Cassandra lib/.;;;","10/Jan/14 00:46;ngrigoriev;I have tested the updated Thrift server with a single-node cluster using my test case and in my larger cluster with my original test - it seems to be working correctly now with large responses! Thanks!!!;;;","10/Jan/14 01:12;xedin;Great! I will wait for confirmation from CASSANDRA-6373, I think Nick is on it, and bump disruptor_thrift_server version in build.xml;;;","11/Jan/14 01:28;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool cfstats doesn't handle index CFs,CASSANDRA-6406,12681334,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,mishail,mishail,26/Nov/13 19:12,16/Apr/19 09:31,14/Jul/23 05:53,26/Nov/13 21:42,,,,,,,0,,,,,"After CASSANDRA-5871 values cfstats are read from the metrics, the problem is that metrics for Index column families have different JMX type ( {{type=IndexColumnFamily}} vs {{type=ColumnFamily}} for regular ones)

{code}
$ bin/nodetool.bat cfstats stress
Starting NodeTool
Keyspace: stress
Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
        at com.sun.proxy.$Proxy16.getCount(Unknown Source)
        at org.apache.cassandra.tools.NodeCmd.printColumnFamilyStats(NodeCmd.java:829)
        at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1123)
Caused by: javax.management.InstanceNotFoundException: org.apache.cassandra.metrics:type=ColumnFamily,keyspace=stress,scope=t1.t1_num_idx,name=WriteLatency
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:643)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1464)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
        at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:657)
        at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:606)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177)
        at sun.rmi.transport.Transport$1.run(Transport.java:174)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
        at sun.rmi.transport.StreamRemoteCall.exceptionReceivedFromServer(StreamRemoteCall.java:275)
        at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:252)
        at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:161)
        at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl_Stub.getAttribute(Unknown Source)
        at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.getAttribute(RMIConnector.java:902)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:267)
        ... 3 more
{code}",,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5871,,,,,,"26/Nov/13 19:16;mishail;Oracle_Java_Mission_Control_2013-11-26_11-15-02.png;https://issues.apache.org/jira/secure/attachment/12615886/Oracle_Java_Mission_Control_2013-11-26_11-15-02.png","26/Nov/13 19:47;mishail;trunk-6406.patch;https://issues.apache.org/jira/secure/attachment/12615895/trunk-6406.patch",,,,,,,,,,,,,,,,,,,2.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360599,,,Tue Nov 26 21:42:06 UTC 2013,,,,,,,,,,"0|i1q5tr:",360898,2.1 rc3,,,,,,,,yukim,,yukim,Low,,2.1 rc3,,,,,,,,,,,,,,,,"26/Nov/13 19:47;mishail;Patch to use a different JMX type for index CFs;;;","26/Nov/13 21:42;yukim;Thanks for catching this!
Committed to trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When making heavy use of counters, neighbor nodes occasionally enter spiral of constant memory consumpion",CASSANDRA-6405,12681233,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,alienth,alienth,26/Nov/13 09:43,16/Apr/19 09:31,14/Jul/23 05:53,10/Apr/14 18:16,2.1 beta2,,,,,,0,,,,,"We're randomly running into an interesting issue on our ring. When making use of counters, we'll occasionally have 3 nodes (always neighbors) suddenly start immediately filling up memory, CMSing, fill up again, repeat. This pattern goes on for 5-20 minutes. Nearly all requests to the nodes time out during this period. Restarting one, two, or all three of the nodes does not resolve the spiral; after a restart the three nodes immediately start hogging up memory again and CMSing constantly.

When the issue resolves itself, all 3 nodes immediately get better. Sometimes it reoccurs in bursts, where it will be trashed for 20 minutes, fine for 5, trashed for 20, and repeat that cycle a few times.

There are no unusual logs provided by cassandra during this period of time, other than recording of the constant dropped read requests and the constant CMS runs. I have analyzed the log files prior to multiple distinct instances of this issue and have found no preceding events which are associated with this issue.

I have verified that our apps are not performing any unusual number or type of requests during this time.

This behaviour occurred on 1.0.12, 1.1.7, and now on 1.2.11.

The way I've narrowed this down to counters is a bit naive. It started happening when we started making use of counter columns, went away after we rolled back use of counter columns. I've repeated this attempted rollout on each version now, and it consistently rears its head every time. I should note this incident does _seem_ to happen more rarely on 1.2.11 compared to the previous versions.

This incident has been consistent across multiple different types of hardware, as well as major kernel version changes (2.6 all the way to 3.2). The OS is operating normally during the event.


I managed to get an hprof dump when the issue was happening in the wild. Something notable in the class instance counts as reported by jhat. Here are the top 5 counts for this one node:

{code}
5967846 instances of class org.apache.cassandra.db.CounterColumn 
1247525 instances of class com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$WeightedValue 
1247310 instances of class org.apache.cassandra.cache.KeyCacheKey 
1246648 instances of class com.googlecode.concurrentlinkedhashmap.ConcurrentLinkedHashMap$Node 
1237526 instances of class org.apache.cassandra.db.RowIndexEntry 
{code}

Is it normal or expected for CounterColumn to have that number of instances?

The data model for how we use counters is as follows: between 50-20000 counter columns per key. We currently have around 3 million keys total, but this issue also replicated when we only had a few thousand keys total. Average column count is around 1k, and 90th is 18k. New columns are added regularly, and columns are incremented regularly. No column or key deletions occur. We probably have 1-5k ""hot"" keys at any given time, spread across the entire ring. R:W ratio is typically around 50:1. This is the only CF we're using counters on, at this time. CF details are as follows:

{code}
    ColumnFamily: CommentTree
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.CounterColumnType
      Cells sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.LongType,org.apache.cassandra.db.marshal.LongType,org.apache.cassandra.db.marshal.LongType)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.01
      DC Local Read repair chance: 0.0
      Populate IO Cache on flush: false
      Replicate on write: true
      Caching: KEYS_ONLY
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.LeveledCompactionStrategy
      Compaction Strategy Options:
        sstable_size_in_mb: 160



                Column Family: CommentTree
                SSTable count: 30
                SSTables in each level: [1, 10, 19, 0, 0, 0, 0, 0, 0]
                Space used (live): 4656930594
                Space used (total): 4677221791
                SSTable Compression Ratio: 0.0
                Number of Keys (estimate): 679680
                Memtable Columns Count: 8289
                Memtable Data Size: 2639908
                Memtable Switch Count: 5769
                Read Count: 185479324
                Read Latency: 1.786 ms.
                Write Count: 5377562
                Write Latency: 0.026 ms.
                Pending Tasks: 0
                Bloom Filter False Positives: 2914204
                Bloom Filter False Ratio: 0.56403
                Bloom Filter Space Used: 523952
                Compacted row minimum size: 30
                Compacted row maximum size: 4866323
                Compacted row mean size: 7742
                Average live cells per slice (last five minutes): 39.0
                Average tombstones per slice (last five minutes): 0.0

{code}


Please let me know if I can provide any further information. I can provide the hprof if desired, however it is 3GB so I'll need to provide it outside of JIRA.","RF of 3, 15 nodes.
Sun Java 7 (also occurred in OpenJDK 6, and Sun Java 6).
Xmx of 8G.
No row cache.",aleksey,alienth,brandon.williams,cburroughs,jeromatron,mishail,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6506,,,,,,,,,,,,,,,,,"11/Dec/13 03:34;alienth;threaddump.txt;https://issues.apache.org/jira/secure/attachment/12618172/threaddump.txt",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360498,,,Thu Apr 10 18:16:30 UTC 2014,,,,,,,,,,"0|i1q57j:",360797,1.0.12,1.1.7,1.2.11,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"27/Nov/13 00:03;alienth;Just did some analysis under normal conditions. Typically, our nodes have less than 200k instances of org.apache.cassandra.db.CounterColumn. During this issue we had nearly 6 million instances, as shown above.;;;","27/Nov/13 04:40;alienth;I have verified that an instance which exhibited the high instance count of CounterColumn classes returned to a lower count (from 5.5m to 180k) after the issue resolved itself, without a restart.;;;","28/Nov/13 16:56;jbellis;It sounds like compaction tbh.;;;","02/Dec/13 23:43;alienth;[~jbellis] There are no compaction tasks pending during the incident. Additionally, on an earlier occurrence I disabled compaction on the CF to no avail.

Would a compaction pileup account for the huge number of class instances? I also find it somewhat unlikely that a compaction issue would appear on 3 nodes simultaneously, and then immediately resolve on 3 nodes simultaneously.;;;","11/Dec/13 01:33;alienth;We're experiencing this issue right this moment (in fact, reddit is down as a result). Compactionstats on the three nodes that are spiking is as follows:

pending tasks: 0
Active compaction remaining time :        n/a
;;;","11/Dec/13 02:29;mishail;[~alienth] can you take a thread dump when the issue happens?;;;","11/Dec/13 03:34;alienth;Thread dump during incident.;;;","11/Dec/13 03:34;alienth;[~mishail] I have just attached a thread dump to this issue.

Thanks!;;;","11/Dec/13 03:35;alienth;Also, GC details after a CMS occurred (immediately followedy by another CMS)

{code}
 Heap
  par new generation   total 276480K, used 31300K [0x00000005fae00000, 0x000000060da00000, 0x000000060da00000)
   eden space 245760K,   0% used [0x00000005fae00000, 0x00000005fae913e8, 0x0000000609e00000)
   from space 30720K, 100% used [0x000000060bc00000, 0x000000060da00000, 0x000000060da00000)
   to   space 30720K,   0% used [0x0000000609e00000, 0x0000000609e00000, 0x000000060bc00000)
  concurrent mark-sweep generation total 8081408K, used 1319539K [0x000000060da00000, 0x00000007fae00000, 0x00000007fae00000)
  concurrent-mark-sweep perm gen total 41060K, used 24529K [0x00000007fae00000, 0x00000007fd619000, 0x0000000800000000)
{code};;;","11/Dec/13 05:33;mishail;[~alienth] How big is your {{key_cache_size_in_mb}}? . And I assume you have {{compaction_preheat_key_cache: true}}, right?;;;","03/Jan/14 01:03;alienth;[~mishail] 100M currently. preheat is turned on.;;;","03/Jan/14 01:20;alienth;I should note, [~brandon.williams] took a peek at the heap dump and it was unfortunately caught just after a CMS, so it doesn't tell us much. I've been unable to get a heap dump from when the memory is full. Despite the thing constantly CMSing, every dump I've taken is what the heap looked like just after a CMS.

Only solid clue still remaining is that instance count of CounterColumn.;;;","03/Jan/14 08:16;slebresne;In and of itself, having lots of instances of CounterColumn is not abnormal when doing lots of counter operations as this the class for each counter value while inserting/reading them. If you do lots of normal operations, you'll similarly see a lot of Column object allocated. The behavior you are seeing is not particularly normal, but having very many CounterColumn objects is not a definitive sign of a problem.

That being said, do you do insertions at CL.ONE? If so, counters are kind of a time bomb in the sense that the read that is done as part of replication is done *after* we've answered the client. Which means that if you insert too fast, replication tasks will pill up behind the scenes, and those task will hold memory that cannot be GCed. In particular, one thing to look at is the replicate_on_write stage in JMX. If pendingTasks are accumulating, that's likely your problem (you're inserting faster than your cluster can actually handle). In which case the basic solution consists in rate limiting the insertions so pending tasks don't pill up.;;;","03/Jan/14 08:58;alienth;[~slebresne] When the issue is occurring, we have no pending ReplicateOnWrite threads. All pending threads are either reads or writes.

When the crazy CounterColumn instance counts are reached, the number of reads/writes occurring on the table are drastically reduced they're all moving extremely slowly. 

If the CounterColumn instance count was legitimate, wouldn't we expect a huge number of reads/writes to be occurring, rather than a small few? Even during peak hours, we don't do more than 150 reads / 10 writes a second per cassandra node. When this issue occurs, that drops down to 2-3 reads and writes a second.

Additionally, we allow up to 128 read threads concurrently. Most of the counter column rows have around 1k columns, with the 95th percentile having 18k columns. Even if every single read thread was dedicated to reading our largest countercolumn row (which they're not), that accounts for a maximum of ~2-3m counter columns being concurrently accessed.;;;","03/Jan/14 09:57;slebresne;bq. If the CounterColumn instance count was legitimate, wouldn't we expect a huge number of reads/writes to be occurring, rather than a small few?

I expressed myself badly. I'm not saying the exact number is normal, and you are definitively reaching a bad situtation. I was merely saying that it's likely a consequence, not a cause, and that it unfortunately does not allow to narrow what the cause may be a whole lot. I'm not suggesting to ignore that information though.

Now, sorry to insist, but are you doing CL.ONE inserts?

Because the fact is, if you do, we *know* that replicate on write tasks may easily pile up behind the scenes. Which would hold ColumnCounter objects in memory and might explain why neighboring nodes are affected together.

Granted the attached thread dump don't show a whole lot of activity on the ReplicateOnWriteStage, and the absence of pending task on that task would suggest it's not the problem. Nonetheless, it's the best lead I have to offer so far.
;;;","03/Jan/14 10:17;alienth;[~slebresne] Woops, thought i included that. We are doing QUORUM writes.

Just dug through all of our logs and verified that we have never seen a pending count on ReplicateOnWriteStage during these incidents on any server. (Not only verified in thread dumps, but via periodic tpstats dumps).;;;","03/Jan/14 10:25;alienth;I should also note a few other things I've tried on nodes experiencing this issue.

* Wiping the keycache with a restart.
* Disabling the keycache.
* Disabling thrift.
* Adjusting read thread concurrency down to 32 and up to 256.

All of these attempts resulted in no change on the affected nodes. They continued to operate in the manner described above until they randomly got better. I have tried all of the above methods with a restart on a single server, as well as a restart on all three ndoes.;;;","03/Jan/14 10:32;alienth;Just had a thought. One thing I haven't tried is disabling hinted handoff on the affected nodes. When this issue is occurring, the constant CMSs result in a bunch of piled up hints. Perhaps something triggers this behaviour, and the hints keep it rolling until all hints have been handed off?

Bit of a stretch, but I'm grasping for anything at this point. I'll try this next time to see if it changes the behaviour at all.;;;","05/Jan/14 00:43;alienth;Happened again a few more times today, taking the site down.

Pausing hinted handoff resulted in no change on the affected nodes.

I've also verified that there are no abnormal number of requests via the org.apache.cassandra.metrics:type=ClientRequest mbeans.;;;","05/Jan/14 01:33;alienth;We've started abandoning the use of the counter columns. This issue has taken the site down for several hours in the past few days, so I could not allow it to continue.

Unfortunately this also means I won't have any place to reproduce this for continued troubleshooting.;;;","07/Jan/14 20:28;aleksey;[~alienth] As Sylvain said, counters currently cause a lot of allocations. I can't say for sure if what I'm going to describe is the cause of your issues, but it definitely contributes to it.

One issue is that all the counter shards of a counter are stored in a single cell, as a blob with sorted tuples (see CounterContext class). And when we reconcile two counter cells, we have to allocate a third cell, large enough to hold the merged context. So unlike regular cells, where reconcile simply picks one of the two cells, reconcile for counter columns creates one more. This doesn't just affect reads, it also affects writes (to the memtable, including replication writes).

Another issues is that when we replicate the counter, we read, and then send, the whole thing to the neighbouring nodes, and not just the value local to the leader-node, and it makes issue #1 worse.

We are aware of it all, and will fix it in 2.1, with CASSANDRA-6506. The second issue is/will be fixed as part of CASSANDRA-6504.

Please note that while it was possible to deal with #2, partially, before, there was no way to make CASSANDRA-6506 happen - because of the supercolumns. However, with CASSANDRA-3237 resolved in 2.0, it is now possible, and I'm currently working on that ticket.;;;","08/Jan/14 07:39;alienth;[~iamaleksey] Thanks for the details on those issues.

It definitely *feels* as though there is an allocation leak, since we go from ~200k allocations, up to 6 million when the issue is happening, and then immediately back down to ~200k when it goes away. Obviously very hard to determine exactly why that is :/

Is there any way to empirically determine if the issues you described are a contributing factor here?;;;","08/Jan/14 16:15;aleksey;[~alienth] No built-in metrics that comes to mind. [~slebresne] any ideas?;;;","08/Jan/14 16:57;slebresne;Nothing coming to mind no, not by default at least. I suppose it wouldn't be too hard to add some instrumentation to count the number of times CounterColumn.reconcile() is called and see if the issue happening is linked to a sudden increase in those calls. That being said, that would still not tell us why there is a sudden increase of the calls... It's still mysterious to me why nodes would suddenly start allocating counters like crazy. ;;;","21/Feb/14 12:59;jbellis;bq. when we reconcile two counter cells, we have to allocate a third cell, large enough to hold the merged context. So unlike regular cells, where reconcile simply picks one of the two cells, reconcile for counter columns creates one more. This doesn't just affect reads, it also affects writes (to the memtable, including replication writes).

Contention within a counter (as multiple writers race to merge cells) makes this worse, because you will get this allocation for failed merges (that is, that lost the CAS race) that need to retry as well.;;;","21/Feb/14 13:00;jbellis;Closing as a duplicate of CASSANDRA-6506.  There's no reasonable way to fix this in earlier C* versions.;;;","10/Apr/14 18:16;aleksey;CASSANDRA-6506 has been delayed until 3.0, but this issues is now actually resolved in 2.1 by the combination of new memtable code and various counters++ commits (including, but not limited to, part of CASSANDRA-6506 and CASSANDRA-6953).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Division by zero Exception in HintedHandoff and CompactionExecutor,CASSANDRA-6403,12680902,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,gwicke,gwicke,23/Nov/13 21:53,16/Apr/19 09:31,14/Jul/23 05:53,13/Mar/14 21:49,2.0.4,,,,,,0,,,,,"In write load testing I'm getting division by zero exceptions after running for a while:

ERROR [HintedHandoff:2] 2013-11-23 20:44:41,411 CassandraDaemon.java (line 187) Exception in thread Thread[HintedHandoff:2,1,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero
        at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:464)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:309)
        at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:92)
        at org.apache.cassandra.db.HintedHandOffManager$4.run(HintedHandOffManager.java:530)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.util.concurrent.ExecutionException: java.lang.ArithmeticException: / by zero
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:460)
        ... 6 more

ERROR [CompactionExecutor:8] 2013-11-23 21:34:01,493 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:8,1,RMI Runtime]
java.lang.ArithmeticException: / by zero
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable.<init>(ParallelCompactionIterable.java:59)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:126)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:296)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)


The nodes that encounter this error seem to hold onto a lot of memory which is not freed even after the write load is stopped. With the write load continuing they eventually run out of heap. nodetool compact dies with the same exception.","Cassandra 2.0.3 RC, Linux (Ubuntu Precise), OpenJDK 7",gwicke,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360167,,,Sun Nov 24 01:44:57 UTC 2013,,,,,,,,,,"0|i1q36f:",360466,2.0.3,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"24/Nov/13 01:32;jbellis;Fixed in cassandra-2.0 HEAD.

Note that you probably shouldn't use multithreaded compaction.  (CASSANDRA-6142);;;","24/Nov/13 01:44;gwicke;Ahh, good to know. I suspected something along those lines, so started a run with multithreaded_compaction disabled. So far (four hours in) it is looking good.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
removenode outputs confusing non-error,CASSANDRA-6397,12680789,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,kirktrue,enigmacurry,enigmacurry,22/Nov/13 17:58,16/Apr/19 09:31,14/Jul/23 05:53,12/Jun/14 22:34,2.0.9,2.1 rc2,,Legacy/Tools,,,0,lhf,,,,"*{{nodetool removenode force}}* outputs a slightly confusing error message when there is nothing for it to do.

* Start a cluster, then kill one of the nodes.
* run *{{nodetool removenode}}* on the node you killed.
* Simultaneously, in another shell, run *{{nodetool removenode force}}*, see that it outputs a simple message regarding it's status.
* Run *{{nodetool removenode force}}* again after the firsrt removenode command finishes, you'll see this message and traceback:

{code}
$ ~/.ccm/test/node1/bin/nodetool -p 7100 removenode force
RemovalStatus: No token removals in process.
Exception in thread ""main"" java.lang.UnsupportedOperationException: No tokens to force removal on, call 'removetoken' first
	at org.apache.cassandra.service.StorageService.forceRemoveCompletion(StorageService.java:3140)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:235)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:250)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:791)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1486)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:96)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1327)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1419)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:847)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
{code}

Two issues I see with this traceback:

* ""No tokens to force removal on"" is telling me the same thing that the message before it tells me: ""RemovalStatus: No token removals in process."", So the entire traceback is redundant.
* ""call 'removetoken' first"" - removetoken has been deprecated according to the message output by removenode, so there is inconsistency in directions to the user.",,cburroughs,enigmacurry,kirktrue,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/14 01:07;kirktrue;trunk-6397.txt;https://issues.apache.org/jira/secure/attachment/12644530/trunk-6397.txt","23/May/14 16:50;kirktrue;trunk-6397.v2.txt;https://issues.apache.org/jira/secure/attachment/12646542/trunk-6397.v2.txt",,,,,,,,,,,,,,,,,,,2.0,kirktrue,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360054,,,Thu Jun 12 22:34:58 UTC 2014,,,,,,,,,,"0|i1q2hb:",360353,1.2.12,2.0.3,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"22/Nov/13 19:25;brandon.williams;Changed all removetoken references to removenode in da5ff080550.  The rest should be fairly easy in nodetool.;;;","23/Apr/14 17:06;kirktrue;Sorry, I'm a little confused by the steps to reproduce. The first step says to ""kill one of the nodes"" but the second step uses nodetool on that same node. When I attempt step 2:

{noformat}
$ ~/.ccm/CASSANDRA-6397/node1/bin/nodetool -p 7100 removenode
{noformat}

I get this:

{noformat}
nodetool: Failed to connect to '127.0.0.1:7100' - ConnectException: 'Connection refused'.
{noformat}

Is there a missing step or something I'm missing?

Thanks.;;;","23/Apr/14 17:19;brandon.williams;You have some kind of JMX problem.;;;","23/Apr/14 17:23;kirktrue;I killed the node using {{kill -9 <PID of node1>}}. Thus the JVM and its JMX hooks aren't around anymore. Previous to the kill I was able to access the node via JMX OK.;;;","23/Apr/14 17:34;brandon.williams;It doesn't mean run nodetool *against* the same node, it means 'remove the node you just killed';;;","16/May/14 14:16;jbellis;[~brandon.williams] to review;;;","16/May/14 14:35;jbellis;[~brandon.williams] to review;;;","16/May/14 16:58;brandon.williams;Why switch to a boolean that we never check?;;;","20/May/14 16:58;kirktrue;Yes, I agree that's ugly.

My concern is that this change turns an error case (forcing when nothing is being removed) into a non-error case. Do users of either nodetool or JMX rely on getting an error? Do they need to know that their request was a no-op when they ask to force a removenode?

If so, they can check the boolean, kind of like File.delete(). But if you want, I can just remove the boolean FYI flag altogether.;;;","20/May/14 17:05;brandon.williams;I think we're conflating a couple issues here.  I think Ryan's point is there shouldn't be a stacktrace printed, but if there is a problem we should report that and exit with a non-zero code.  Programmatically though, the boolean is completely unused as the patch stands, so we don't need that unless it's to enable solving the first problem.;;;","23/May/14 16:51;kirktrue;Removed boolean return value.;;;","12/Jun/14 22:34;brandon.williams;I don't think any has been relying on catching an exception here, and if they have they've been doing it wrong.  If anyone needs this they can open a new one.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
debian init searches for jdk6 explicitly,CASSANDRA-6396,12680783,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,22/Nov/13 17:41,16/Apr/19 09:31,14/Jul/23 05:53,22/Nov/13 18:05,1.2.13,2.0.4,,Packaging,,,0,,,,,"When JAVA_HOME isn't set, the init looks for jdk6 explicitly.  Obviously for 2.0+ this can cause problems.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/13 17:44;brandon.williams;6396.txt;https://issues.apache.org/jira/secure/attachment/12615361/6396.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,360048,,,Fri Nov 22 18:05:13 UTC 2013,,,,,,,,,,"0|i1q2fz:",360347,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"22/Nov/13 17:44;brandon.williams;I think the best thing to do is just search the default-jvm path.  If the user hasn't set up alternatives correctly, they can fix that.  If they want to use a specific non-default jvm, they can override JAVA_HOME in /etc/default/cassandra.;;;","22/Nov/13 18:00;jbellis;+1;;;","22/Nov/13 18:05;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
FD phi estimator initial conditions,CASSANDRA-6385,12680322,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,qconner,qconner,20/Nov/13 16:51,16/Apr/19 09:31,14/Jul/23 05:53,21/Nov/13 02:58,1.2.13,2.0.3,,,,,0,,,,,"phi estimates are calculated for newly discovered nodes from an un-filled (new, uninitialized) deque.

The inter-arrival time (elapsed time between gossip heartbeats) is stored in the o.a.c.gms.ArrivalWindow.arrivalIntervale deque for each received heartbeat, up to the maximum window size of 1000 samples.

In the o.a.c.gms.FailureDetector.interpret() method, phi is calculated for the node which uses a statistical measure called variance.  Like mean, variance on a population (a set of numbers or measurements) is not statistically relevant unless the population set size is 30 or greater. 

When a new node is discovered, the calculated variance is higher than normal, and causes phi to be higher than normal, resulting in a false positive failure detection.",,qconner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6127,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/13 17:49;jbellis;6385-v2.txt;https://issues.apache.org/jira/secure/attachment/12614933/6385-v2.txt","20/Nov/13 18:59;jbellis;6385-v3.txt;https://issues.apache.org/jira/secure/attachment/12614949/6385-v3.txt","20/Nov/13 17:21;qconner;6385.txt;https://issues.apache.org/jira/secure/attachment/12614921/6385.txt","20/Nov/13 22:11;jbellis;6835-v4.txt;https://issues.apache.org/jira/secure/attachment/12615010/6835-v4.txt",,,,,,,,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,359587,,,Thu Nov 21 02:58:20 UTC 2013,,,,,,,,,,"0|i1pzmf:",359886,1.2.11,1.2.9,2.0.2,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"20/Nov/13 16:52;qconner;breaking out FD phi initial condition to related issue;;;","20/Nov/13 17:21;qconner;initial condition patch that paints a rosy picture until 30 rounds of gossip have completed and a statistically valid sample set has accumulated.;;;","20/Nov/13 17:49;jbellis;Thinking about it more, I'm not comfortable with saying that a dead node will *never* be detected if it dies before it hits the cutoff.  v2 changes it to sqrt(phi) until it hits 30.

We could probably do something more sophisticated that narrows the fudge factor as we approach our threshold of confidence.

(Both of these break ArrivalWindowTest, btw.);;;","20/Nov/13 18:59;jbellis;Thinking about it more, I think the main problem is using too low of an initial value to seed the Window.  Interval / 2 is always smaller then the actual mean will be, and it will be increasingly too small as the cluster size grows.

Picking a nice large value there gives us the ""large fudge to start that ""decays"" (by being averaged with real values) as we get more data"" behavior that we want.

v3 attached.;;;","20/Nov/13 19:15;brandon.williams;I like v3's logic much better, however on startup you can trip the assert (infinitely, if you kill a node very quickly after a round a gossip):

{noformat}
ERROR 19:10:48,249 Exception in thread Thread[GossipTasks:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.gms.ArrivalWindow.phi(FailureDetector.java:331)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:220)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:612)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:57)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:163)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{noformat};;;","20/Nov/13 19:24;jbellis;What am I missing?  We got rid of clear, and I added code so we don't put the AW into the Map until it has at least one data point.;;;","20/Nov/13 19:33;brandon.williams;bq. We got rid of clear

Well, not in v3 :);;;","20/Nov/13 19:46;jbellis;Time to pull :);;;","20/Nov/13 20:09;brandon.williams;Well crap, I pulled and still get it. :/;;;","20/Nov/13 20:19;brandon.williams;I think the clue lies here:

{noformat}
DEBUG 20:16:42,475 Ignoring interval time of 30000.0
 INFO 20:16:42,475 Node /10.179.65.102 is now part of the cluster
DEBUG 20:16:42,482 removing expire time for endpoint : /10.179.65.102
 INFO 20:16:42,482 InetAddress /10.179.65.102 is now UP
ERROR 20:16:43,377 Exception in thread Thread[GossipTasks:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.gms.ArrivalWindow.phi(FailureDetector.java:319)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:213)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:612)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:57)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:163)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
 INFO 20:16:43,377 Handshaking version with cassandra-1/10.179.65.102
{noformat}

Which I guess we need a backdoor method to inject, because I was going to go with 10s on CASSANDRA-4375.;;;","20/Nov/13 22:11;jbellis;v4;;;","20/Nov/13 22:54;brandon.williams;+1;;;","21/Nov/13 02:58;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTableReader.loadSummary may leave an open file,CASSANDRA-6380,12680110,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,mishail,mishail,20/Nov/13 05:25,16/Apr/19 09:31,14/Jul/23 05:53,20/Nov/13 13:26,2.0.3,,,,,,0,,,,,"When {{SSTableReader.loadSummary}} catches _IOException_ it tries to delete {{summariesFile}}, but the {{iStream}} is still open and the file is locked, so {{FileUtils.deleteWithConfirm}} fails, at least on Windows",,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/13 05:27;mishail;CASSANDRA-2.0-6380.patch;https://issues.apache.org/jira/secure/attachment/12614793/CASSANDRA-2.0-6380.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,359467,,,Wed Nov 20 13:26:51 UTC 2013,,,,,,,,,,"0|i1pyvr:",359766,2.0.2,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"20/Nov/13 05:27;mishail;Attaching the patch to close the stream to unlock the file;;;","20/Nov/13 13:24;yukim;'finally' block does close iStream, so I think there isn't a problem in current code.;;;","20/Nov/13 13:26;jbellis;LGTM; committed

(Didn't see Yuki's comment.  The problem is that the catch block runs before the finally, so it tries to delete before the close, which works on linux but not windows.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader does not support client encryption on Cassandra 2.0,CASSANDRA-6378,12680009,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,dlaube,dlaube,19/Nov/13 18:44,16/Apr/19 09:31,14/Jul/23 05:53,19/Dec/13 10:02,2.0.4,,,,,,0,client,encryption,ssl,sstableloader,"We have been testing backup/restore from one ring to another and we recently stumbled upon an issue with sstableloader. When client_enc_enable: true, the exception below is generated. However, when client_enc_enable is set to false, the sstableloader is able to get to the point where it is discovers endpoints, connects to stream data, etc.

==============BEGIN EXCEPTION==============
sstableloader --debug -d x.x.x.248,x.x.x.108,x.x.x.113 /tmp/import/keyspace_name/columnfamily_name
Exception in thread ""main"" java.lang.RuntimeException: Could not retrieve endpoint ranges:
at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:226)
at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:149)
at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:68)
Caused by: org.apache.thrift.transport.TTransportException: Frame size (352518400) larger than max length (16384000)!
at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:137)
at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:362)
at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:284)
at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:191)
at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
at org.apache.cassandra.thrift.Cassandra$Client.recv_describe_partitioner(Cassandra.java:1292)
at org.apache.cassandra.thrift.Cassandra$Client.describe_partitioner(Cassandra.java:1280)
at org.apache.cassandra.tools.BulkLoader$ExternalClient.init(BulkLoader.java:199)
... 2 more
==============END EXCEPTION==============

",,dlaube,mbulman,mishail,mshuler,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Dec/13 09:01;samt;0001-CASSANDRA-6387-Add-SSL-support-to-BulkLoader.patch;https://issues.apache.org/jira/secure/attachment/12619282/0001-CASSANDRA-6387-Add-SSL-support-to-BulkLoader.patch",,,,,,,,,,,,,,,,,,,,1.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,359366,,,Mon Dec 30 21:00:40 UTC 2013,,,,,,,,,,"0|i1py9b:",359665,,,,,,,,,mishail,,mishail,Normal,,,,,,,,,,,,,,,,,,"17/Dec/13 14:17;samt;Updated patch to remove unnecessary exception handling from SSLTransportFactory;;;","17/Dec/13 22:30;jbellis;Can you review, [~mishail]?;;;","18/Dec/13 07:42;mishail;The only minor comment I have is that {{opts}} parameter for {{org.apache.cassandra.tools.BulkLoader.LoaderOptions.getTransportFactory()}} is never used.


;;;","18/Dec/13 09:01;samt;Sorry, missed that when refactoring. Attached updated patch with the extraneous parameter removed.;;;","18/Dec/13 19:51;mishail;LGTM ;;;","18/Dec/13 22:17;jbellis;committed;;;","18/Dec/13 22:50;mshuler;cassandra-2.0 and trunk both fail to build in the same manner:
{code}
build-project:
     [echo] apache-cassandra: /home/mshuler/git/cassandra/build.xml
    [javac] Compiling 43 source files to /home/mshuler/git/cassandra/build/classes/thrift
    [javac] Note: /home/mshuler/git/cassandra/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java uses or overrides a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] Compiling 847 source files to /home/mshuler/git/cassandra/build/classes/main
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/NativeAllocator.java:22: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac] import sun.misc.Unsafe;
    [javac]                ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/FastByteComparisons.java:25: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac] import sun.misc.Unsafe;
    [javac]                ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/sstable/IndexSummary.java:20: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac] import java.io.Closeable;
    [javac]             ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/Memory.java:29: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]     private static final Unsafe unsafe = NativeAllocator.unsafe;
    [javac]                          ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/NativeAllocator.java:26: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]     static final Unsafe unsafe;
    [javac]                  ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/NativeAllocator.java:31: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]             Field field = sun.misc.Unsafe.class.getDeclaredField(""theUnsafe"");
    [javac]                                   ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/io/util/NativeAllocator.java:33: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]             unsafe = (sun.misc.Unsafe) field.get(null);
    [javac]                               ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:461: error: cannot find symbol
    [javac]             if (transportFactory.supportedOptions().contains(SSLTransportFactory.TRUSTSTORE))
    [javac]                                                              ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:462: error: cannot find symbol
    [javac]                 options.put(SSLTransportFactory.TRUSTSTORE, opts.encOptions.truststore);
    [javac]                             ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:463: error: cannot find symbol
    [javac]             if (transportFactory.supportedOptions().contains(SSLTransportFactory.TRUSTSTORE_PASSWORD))
    [javac]                                                              ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:464: error: cannot find symbol
    [javac]                 options.put(SSLTransportFactory.TRUSTSTORE_PASSWORD, opts.encOptions.truststore_password);
    [javac]                             ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:465: error: cannot find symbol
    [javac]             if (transportFactory.supportedOptions().contains(SSLTransportFactory.PROTOCOL))
    [javac]                                                              ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:466: error: cannot find symbol
    [javac]                 options.put(SSLTransportFactory.PROTOCOL, opts.encOptions.protocol);
    [javac]                             ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:467: error: cannot find symbol
    [javac]             if (transportFactory.supportedOptions().contains(SSLTransportFactory.CIPHER_SUITES))
    [javac]                                                              ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:468: error: cannot find symbol
    [javac]                 options.put(SSLTransportFactory.CIPHER_SUITES, Joiner.on(',').join(opts.encOptions.cipher_suites));
    [javac]                             ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:470: error: cannot find symbol
    [javac]             if (transportFactory.supportedOptions().contains(SSLTransportFactory.KEYSTORE)
    [javac]                                                              ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:472: error: cannot find symbol
    [javac]                 options.put(SSLTransportFactory.KEYSTORE, opts.encOptions.keystore);
    [javac]                             ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:473: error: cannot find symbol
    [javac]             if (transportFactory.supportedOptions().contains(SSLTransportFactory.KEYSTORE_PASSWORD)
    [javac]                                                              ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/tools/BulkLoader.java:475: error: cannot find symbol
    [javac]                 options.put(SSLTransportFactory.KEYSTORE_PASSWORD, opts.encOptions.keystore_password);
    [javac]                             ^
    [javac]   symbol:   variable SSLTransportFactory
    [javac]   location: class LoaderOptions
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/FastByteComparisons.java:114: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]       static final Unsafe theUnsafe;
    [javac]                    ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/FastByteComparisons.java:120: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]         theUnsafe = (Unsafe) AccessController.doPrivileged(
    [javac]                      ^
    [javac] /home/mshuler/git/cassandra/src/java/org/apache/cassandra/utils/FastByteComparisons.java:125: warning: Unsafe is internal proprietary API and may be removed in a future release
    [javac]                   Field f = Unsafe.class.getDeclaredField(""theUnsafe"");
    [javac]                             ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 12 errors
    [javac] 10 warnings

BUILD FAILED
{code};;;","18/Dec/13 23:01;mishail;The entire {{SSLTransportFactory.java}} is missed in the commit;;;","19/Dec/13 00:01;jbellis;fixed;;;","30/Dec/13 21:00;dlaube;Thanks for all of the hard work and effort everyone put in to get this fixed!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig tests are failing,CASSANDRA-6376,12680000,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,alexliu68,slebresne,slebresne,19/Nov/13 17:32,16/Apr/19 09:31,14/Jul/23 05:53,14/Mar/14 19:55,1.2.16,2.0.7,2.1 beta2,,,,0,,,,,"On my box, all pig tests are failing with the following stack:
{noformat}
    [junit] Testcase: org.apache.cassandra.pig.CqlTableDataTypeTest:	Caused an ERROR
    [junit] null
    [junit] java.lang.ExceptionInInitializerError
    [junit] 	at org.apache.cassandra.pig.PigTestBase.startHadoopCluster(PigTestBase.java:104)
    [junit] 	at org.apache.cassandra.pig.CqlTableDataTypeTest.setup(CqlTableDataTypeTest.java:198)
    [junit] Caused by: java.lang.NullPointerException
    [junit] 	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:422)
    [junit] 	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:280)
    [junit] 	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
    [junit] 	at org.apache.pig.test.MiniCluster.setupMiniDfsAndMrClusters(MiniCluster.java:50)
    [junit] 	at org.apache.pig.test.MiniGenericCluster.<init>(MiniGenericCluster.java:49)
    [junit] 	at org.apache.pig.test.MiniCluster.<init>(MiniCluster.java:31)
    [junit] 	at org.apache.pig.test.MiniGenericCluster.<clinit>(MiniGenericCluster.java:45)
{noformat}
On CASSANDRA-6375, Brandon reported that it was the case on his box too, so I don't think it's a local to my machine. Seems to be a relatively basic setup thing though, not an actual test failure.

I'll also note that we have a specific target for pig tests and that this target uses a longer timeout than the one for the test target. If that's because pig tests typically don't finish within the test timeout, then it would be nice to exclude them from the normal test target (and maybe include them in the long-test target).",,alexliu68,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/13 23:43;alexliu68;6376-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12615034/6376-1.2-branch.txt","14/Mar/14 17:38;alexliu68;6376-v2.txt;https://issues.apache.org/jira/secure/attachment/12634759/6376-v2.txt",,,,,,,,,,,,,,,,,,,2.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,359357,,,Fri Mar 14 19:55:06 UTC 2014,,,,,,,,,,"0|i1py7b:",359656,1.2.11,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"20/Nov/13 23:41;alexliu68;Add ""exclude"" attribute to testmacro, default exclude pig test.;;;","21/Nov/13 08:21;slebresne;I'll note that while I'm +1 on this exclude patch, the pig test fails with that ExceptionInInitializerError above even if run alone through the pig-test target, which is the main problem I meant we should fix here.;;;","21/Nov/13 15:21;brandon.williams;Hmm, pig-test passes for me just fine.;;;","21/Nov/13 15:32;slebresne;bq. pig-test passes for me just fine.

Interesting. But you didn't saw the same stacktrace than in the description when running it with the main test target did you? If you did, that doesn't make a whole lot of sense because the pig-test target does nothing more than only running the pig tests with a different timeout.

I wonder if it's not some dependency that needs to be installed that I don't have but you two guys have because you've used pig and I never have. In any case, if it's just a weird local thing of my machine, then I suppose I don't really care, I'll just never run the pig-test. But can at least one more people check the pig-test target and say what he got?;;;","21/Nov/13 15:50;brandon.williams;I think the root cause originally wasn't an NPE, if you look at the log output slightly above that, is a NCDF error in hadoop itself.  I think this is because Cassandra is built against a different hadoop version than hadoop.minicluster is, and that's why running the pig tests in isolation works.;;;","13/Mar/14 19:40;jbellis;where does that leave us here?;;;","13/Mar/14 20:09;brandon.williams;Now they seem to execute fine, but have an error:

{noformat}
    [junit] Testcase: testCqlStorageCollectionColumnTable(org.apache.cassandra.pig.CqlTableTest):       FAILED
    [junit] expected:<3> but was:<2>
    [junit] junit.framework.AssertionFailedError: expected:<3> but was:<2>
    [junit]     at org.apache.cassandra.pig.CqlTableTest.testCqlStorageCollectionColumnTable(CqlTableTest.java:186)
{noformat};;;","14/Mar/14 17:38;alexliu68;Fix the failed test case. v2 is attached.;;;","14/Mar/14 19:55;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit test failures on 1.2 branch,CASSANDRA-6375,12679926,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,,slebresne,slebresne,19/Nov/13 11:21,16/Apr/19 09:31,14/Jul/23 05:53,19/Nov/13 17:36,1.2.12,,,,,,0,,,,,"On my box, I get a number of reproducible test failures:
# LeaveAndBootstrapTest
{noformat}
[junit] Testsuite: org.apache.cassandra.service.LeaveAndBootstrapTest
[junit] Tests run: 6, Failures: 2, Errors: 0, Time elapsed: 13.145 sec
[junit] 
[junit] ------------- Standard Error -----------------
[junit]  WARN 12:11:43,275 Node /127.0.0.3 'leaving' token mismatch. Long network partition?
[junit] ------------- ---------------- ---------------
[junit] Testcase: newTestWriteEndpointsDuringLeave(org.apache.cassandra.service.LeaveAndBootstrapTest):	FAILED
[junit] mismatched endpoint sets expected:<[/127.0.0.4, /127.0.0.5]> but was:<[/127.0.0.4]>
[junit] junit.framework.AssertionFailedError: mismatched endpoint sets expected:<[/127.0.0.4, /127.0.0.5]> but was:<[/127.0.0.4]>
[junit] 	at org.apache.cassandra.service.LeaveAndBootstrapTest.newTestWriteEndpointsDuringLeave(LeaveAndBootstrapTest.java:131)
{noformat}
# TokenMetadataTest
{noformat}
[junit] Testsuite: org.apache.cassandra.locator.TokenMetadataTest
[junit] Tests run: 3, Failures: 1, Errors: 0, Time elapsed: 0.76 sec
[junit] 
[junit] Testcase: testRingIterator(org.apache.cassandra.locator.TokenMetadataTest):	FAILED
[junit] [] expected:<2> but was:<0>
[junit] junit.framework.AssertionFailedError: [] expected:<2> but was:<0>
[junit] 	at org.apache.cassandra.locator.TokenMetadataTest.testRingIterator(TokenMetadataTest.java:55)
[junit] 	at org.apache.cassandra.locator.TokenMetadataTest.testRingIterator(TokenMetadataTest.java:63)
{noformat}
# ScrubTest
{noformat}
[junit] Testsuite: org.apache.cassandra.db.ScrubTest
[junit] Tests run: 4, Failures: 1, Errors: 0, Time elapsed: 12.499 sec
[junit] 
[junit] ------------- Standard Error -----------------
[junit]  WARN 12:16:27,799 Out of order row detected (DecoratedKey(63, 63) found after DecoratedKey(7a, 7a))
[junit]  WARN 12:16:27,801 Out of order row detected (DecoratedKey(79, 79) found after DecoratedKey(7a, 7a))
[junit]  WARN 12:16:27,802 Out of order row detected (DecoratedKey(64, 64) found after DecoratedKey(7a, 7a))
[junit]  WARN 12:16:28,289 3 out of order rows found while scrubbing SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard3/Keyspace1-Standard3-ia-1-Data.db'); Those have been written (in order) to a new sstable (SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard3/Keyspace1-Standard3-ic-3-Data.db'))
[junit] ------------- ---------------- ---------------
[junit] Testcase: testScrubOneRow(org.apache.cassandra.db.ScrubTest):	FAILED
[junit] expected:<1> but was:<10>
[junit] junit.framework.AssertionFailedError: expected:<1> but was:<10>
[junit] 	at org.apache.cassandra.db.ScrubTest.testScrubOneRow(ScrubTest.java:94)
{noformat}

While running the whole test suites I also ran into the following stack:
{noformat}
[junit] Testsuite: org.apache.cassandra.dht.BootStrapperTest
[junit] Tests run: 4, Failures: 0, Errors: 0, Time elapsed: 14.345 sec
[junit] 
[junit] ------------- Standard Error -----------------
[junit]  WARN 11:16:50,833 No host ID found, created cb1c4ca1-c451-42ae-b205-36258dfe4f96 (Note: This should happen exactly once per node).
[junit]  WARN 11:16:51,193 Generated random token [f368755beab4290b7e70895776c6e14e]. Random tokens will result in an unbalanced ring; see http://wiki.apache.org/cassandra/Operations
[junit] ERROR 11:16:51,724 Fatal exception in thread Thread[PendingRangeCalculator:1,5,main]
[junit] java.util.ConcurrentModificationException
[junit] 	at java.util.TreeMap$PrivateEntryIterator.nextEntry(TreeMap.java:1115)
[junit] 	at java.util.TreeMap$EntryIterator.next(TreeMap.java:1151)
[junit] 	at java.util.TreeMap$EntryIterator.next(TreeMap.java:1146)
[junit] 	at com.google.common.collect.AbstractMultimap$EntryIterator.findValueIteratorAndKey(AbstractMultimap.java:1152)
[junit] 	at com.google.common.collect.AbstractMultimap$EntryIterator.next(AbstractMultimap.java:1166)
[junit] 	at com.google.common.collect.AbstractMultimap$EntryIterator.next(AbstractMultimap.java:1136)
[junit] 	at java.util.Collections$UnmodifiableCollection$1.next(Collections.java:1067)
[junit] 	at com.google.common.collect.ForwardingIterator.next(ForwardingIterator.java:48)
[junit] 	at com.google.common.collect.Maps$UnmodifiableEntries$1.next(Maps.java:953)
[junit] 	at com.google.common.collect.Maps$UnmodifiableEntries$1.next(Maps.java:951)
[junit] 	at com.google.common.collect.AbstractMultimap.putAll(AbstractMultimap.java:272)
[junit] 	at com.google.common.collect.TreeMultimap.putAll(TreeMultimap.java:74)
[junit] 	at org.apache.cassandra.utils.SortedBiMultiValMap.create(SortedBiMultiValMap.java:60)
[junit] 	at org.apache.cassandra.locator.TokenMetadata.cloneOnlyTokenMap(TokenMetadata.java:598)
[junit] 	at org.apache.cassandra.locator.TokenMetadata.cloneAfterAllLeft(TokenMetadata.java:619)
[junit] 	at org.apache.cassandra.service.PendingRangeCalculatorService.calculatePendingRanges(PendingRangeCalculatorService.java:139)
[junit] 	at org.apache.cassandra.service.PendingRangeCalculatorService$PendingRangeTask.run(PendingRangeCalculatorService.java:67)
[junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
[junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
[junit] 	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
[junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
[junit] 	at java.lang.Thread.run(Thread.java:744)
{noformat}
This doesn't end up failing the test and I was actually not able to reproduce when running BootStrapperTest individually, but I don't know if we understand why that can happen during the test (and if it's just an artifact of testing or a real thing).
",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Nov/13 17:08;slebresne;0001-LeaveAndBootstrapTest.txt;https://issues.apache.org/jira/secure/attachment/12614650/0001-LeaveAndBootstrapTest.txt","19/Nov/13 17:16;slebresne;0002-TMD-ConcurrentModificationException.txt;https://issues.apache.org/jira/secure/attachment/12614654/0002-TMD-ConcurrentModificationException.txt",,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,359283,,,Tue Nov 19 17:36:03 UTC 2013,,,,,,,,,,"0|i1pxqv:",359582,,,,,,,,,,,,Critical,,,,,,,,,,,,,,,,,,"19/Nov/13 15:59;slebresne;Btw, also got all pig tests to fail with the following exceptions:
{noformat}
    [junit] Testcase: org.apache.cassandra.pig.CqlTableDataTypeTest:	Caused an ERROR
    [junit] null
    [junit] java.lang.ExceptionInInitializerError
    [junit] 	at org.apache.cassandra.pig.PigTestBase.startHadoopCluster(PigTestBase.java:104)
    [junit] 	at org.apache.cassandra.pig.CqlTableDataTypeTest.setup(CqlTableDataTypeTest.java:198)
    [junit] Caused by: java.lang.NullPointerException
    [junit] 	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:422)
    [junit] 	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:280)
    [junit] 	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:124)
    [junit] 	at org.apache.pig.test.MiniCluster.setupMiniDfsAndMrClusters(MiniCluster.java:50)
    [junit] 	at org.apache.pig.test.MiniGenericCluster.<init>(MiniGenericCluster.java:49)
    [junit] 	at org.apache.pig.test.MiniCluster.<init>(MiniCluster.java:31)
    [junit] 	at org.apache.pig.test.MiniGenericCluster.<clinit>(MiniGenericCluster.java:45)
{noformat}
I wouldn't block a release because of pig tests, but if it does not just fail for me, it would be nice to fix it too.;;;","19/Nov/13 16:13;brandon.williams;Those fail for me too, but that should be an easy bisect.;;;","19/Nov/13 16:23;slebresne;Looking at TokenMedataTest, it was just assuming the last test in the file was actually running last and apparently that wasn't happening on my box. Ninja-fixed that one in commit 0a5a766 to not depend on the tests execution order.;;;","19/Nov/13 17:08;slebresne;For LeaveAndBootstrapTest, this bisects to CASSANDRA-6244. So I think this is just a case of ""we've made things asynchronous so we now check the expected result before the computation is done"". Tried adding a few calls to PRCS.blockUntilFinished in the few places that were failing for me and that seems to fix the test. Attaching the resulting patch. [~brandon.williams] can you check it's not entirely stupid?;;;","19/Nov/13 17:13;brandon.williams;Looks pretty similar to what I did in 7de6f9666 to fix them in 1.1, except I used the shotgun method :) So if that fixes it, then +1;;;","19/Nov/13 17:16;slebresne;Regarding the ConcurrentModificationException, it seems that the only reason this could get triggered is due to TMD.clearUnsafe(). As this is called by tests, this is not a real problem, but what about making it grab the writeLock like any good citizen to avoid getting scarry stack traces (and don't discard a real bug later on because we've grown used to discarding such stack)? Attaching patch to do that.;;;","19/Nov/13 17:36;slebresne;Alright, ScrubTest was another instance of tests expecting to run in a particular order (don't know why my box don't run them in the order they are declared but well, expecting a particular order is a bad idea in any case) so ninja-fixed that. I've also committed the 2 patches attached above.

This fixes the failure I'm saying, except for the pig tests, but as those are clearly a setup thing I don't want to block 1.2.12 for that and I've open CASSANDRA-6376 to deal with them. Closing this issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError for rows with zero columns,CASSANDRA-6374,12679880,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,sv3k,sv3k,19/Nov/13 06:13,16/Apr/19 09:31,14/Jul/23 05:53,21/Nov/13 21:15,2.0.3,,,,,,3,,,,,"After upgrading from 1.2.5 to 1.2.9 and then to 2.0.2 we've got those exceptions:
{code}
ERROR [FlushWriter:1] 2013-11-18 16:14:36,305 CassandraDaemon.java (line 187) Exception in thread Thread[FlushWriter:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.io.sstable.SSTableWriter.rawAppend(SSTableWriter.java:198)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:186)
        at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:360)
        at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:315)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
{code}

Also found similar issue in this thread:
http://www.mail-archive.com/user@cassandra.apache.org/msg32875.html
There Aaron Morton said that its caused by leaving rows with zero columns - that's exactly what we do in some CFs (using Thrift & Astyanax).
",,aleksey,ash2k,binrush,gwicke,jasobrown,sv3k,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/13 05:00;jbellis;6374.txt;https://issues.apache.org/jira/secure/attachment/12614789/6374.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,359238,,,Fri Nov 22 15:37:20 UTC 2013,,,,,,,,,,"0|i1pxh3:",359537,2.0.2,,,,,,,,jasobrown,,jasobrown,Normal,,,,,,,,,,,,,,,,,,"19/Nov/13 13:53;jbellis;Under Thrift, a row with zero columns will be removed as soon as it compacts.  So this really is a bad idea.;;;","20/Nov/13 03:25;ash2k;[~jbellis] can you explain please what exactly is a bad idea here? We are ok with ""a row with zero columns will be removed as soon as it compacts"". We just didn't expected this usage to be a problem - is it documented somewhere? It works fine with 1.2.x and do not work with 2.0.x.;;;","20/Nov/13 03:30;jbellis;Rows that sort-of exist for a non-deterministic amount of time are not a feature I intend to support.  That it works in 1.2.x is a bug.;;;","20/Nov/13 03:45;ash2k;Our scenario of usage is as follows:
We delete specific columns in a specific row. Sometimes those columns are the last ones. How can we delete the whole row if they are the last ones? We cannot read-check-delete_row_or_only_columns because it has a race between check and delete.;;;","20/Nov/13 04:55;jbellis;We still support that.  The assertion only rejects no cells at all.  (A tombstone still counts as a cell.)

That said, it's really the memtable's job to not flush empty rows even if it's rather antisocial to give it a batch containing zero mutations.;;;","20/Nov/13 05:00;jbellis;Patch attached to do that.  I'm a bit nervous though that I'm not 100% sure if 1.2 was actually writing empty rows to sstables, or if it was rejecting them silently somewhere else.;;;","21/Nov/13 20:48;jasobrown;+1. Agreed that flushing a zero column row is not a good idea.;;;","21/Nov/13 21:15;jbellis;Committed;;;","22/Nov/13 03:27;ash2k;Can this be fixed for 2.0.3 please? We need to upgrade our testing cluster to 2.0.x but cannot because of this issue.;;;","22/Nov/13 15:37;aleksey;bq. Can this be fixed for 2.0.3 please? We need to upgrade our testing cluster to 2.0.x but cannot because of this issue.

2.0.3 vote is being restarted, so yes, it will be in 2.0.3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe_ring hangs with hsha thrift server,CASSANDRA-6373,12679842,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,nickmbailey,nickmbailey,18/Nov/13 23:25,16/Apr/19 09:31,14/Jul/23 05:53,11/Jan/14 01:29,2.0.5,,,,,,1,,,,,"There is a strange bug with the thrift hsha server in 2.0 (we switched to lmax disruptor server).

The bug is that the first call to describe_ring from one connection will hang indefinitely when the client is not connecting from localhost (or it at least looks like the client is not on the same host). Additionally the cluster must be using vnodes. When connecting from localhost the first call will work as expected. And in either case subsequent calls from the same connection will work as expected. According to git bisect the bad commit is the switch to the lmax disruptor server:

https://github.com/apache/cassandra/commit/98eec0a223251ecd8fec7ecc9e46b05497d631c6

I've attached the patch I used to reproduce the error in the unit tests. The command to reproduce is: 

{noformat}
PYTHONPATH=test nosetests --tests=system.test_thrift_server:TestMutations.test_describe_ring
{noformat}

I reproduced on ec2 and a single machine by having the server bind to the private ip on ec2 and the client connect to the public ip (so it appears as if the client is non local). I've also reproduced with two different vms though.",,cburroughs,mbulman,mishail,nickmbailey,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/13 23:26;nickmbailey;describe_ring_failure.patch;https://issues.apache.org/jira/secure/attachment/12614510/describe_ring_failure.patch","19/Dec/13 17:33;nickmbailey;jstack.txt;https://issues.apache.org/jira/secure/attachment/12619604/jstack.txt","19/Dec/13 20:36;nickmbailey;jstack2.txt;https://issues.apache.org/jira/secure/attachment/12619654/jstack2.txt",,,,,,,,,,,,,,,,,,3.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,359200,,,Sat Jan 11 01:29:13 UTC 2014,,,,,,,,,,"0|i1px8n:",359499,2.0.0,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"11/Dec/13 20:05;xedin;[~nickmbailey] Have you tried doing reads/writes with the same setup? I'm just wondering if it's *only* describe_ring or everything else too which displays the same behavior?...;;;","11/Dec/13 23:35;nickmbailey;I can check tomorrow. I *think* i saw it with describe keyspaces as well but I'm not positive. I have the ec2 machine i recreated on as well which I could probably give you access to if you want.;;;","13/Dec/13 16:53;nickmbailey;[~xedin] I've verified its *only* the describe_ring call. Other calls seem to work fine. Let me know if you want access to the machine I used to reproduce.;;;","18/Dec/13 23:43;nickmbailey;[~xedin] any insight yet?;;;","19/Dec/13 01:59;xedin;I had a look at the code to figure out why only that particular command is not working but no success this far, it would be very helpful if you could attach output of jstack of the server side taken in the situation when describe_ring hangs...;;;","19/Dec/13 17:33;nickmbailey;Attached the output of jstack.;;;","19/Dec/13 20:18;xedin;So from the thread stacks it looks like server side is just waiting for new packets from the client as both Acceptor and Selector threads are blocked on select() from the socket. Just to clarify, was this taken simultaneously with client being stuck waiting for describe_ring output?;;;","19/Dec/13 20:36;nickmbailey;Yes, It should have been. Just to be sure I got another one which definitely is. They look about the same though.;;;","19/Dec/13 20:44;xedin;Those looks the same, selector threads are waiting on the socket and all of the worker threads are waiting on the barrier for a new task... Server implementation could not distinguish between commands it's being sent so I don't see any reason why one would hang and another wouldn't, I think describe_ring itself should be a culprit in this case.;;;","19/Dec/13 20:51;nickmbailey;Yeah, it's a very strange bug. Like I said it's definitely the commit where we changed the hsha server though. And it only happens on the first call to describe ring. For example if you change the patch I attached to do the following in the test function, the test passes fine after catching the first socket timeout.

{noformat}
    def test_describe_ring(self):
+       print 'running desc ring'
+       try:
+            list(client.describe_ring('Keyspace1'))[0].endpoints == ['127.0.0.1']
+       except Exception, e:
+            print e
+        print 'running again'
         assert list(client.describe_ring('Keyspace1'))[0].endpoints == ['127.0.0.1']
{noformat};;;","19/Dec/13 22:17;xedin;Is the same happening with vnodes turned off? I just don't get why if you send any other command it works fine but if you send describe_ring it fails, it shouldn't be any different from the server perspective... Can you also try increasing min number of rpc threads from 2 to 4, wonder if it would make any difference.;;;","20/Dec/13 00:37;nickmbailey;It doesn't happen with vnodes off. In fact it doesn't happen with vnodes set to 128. So that would seem to indicate it has to do with the size of the response as more vnodes will make the describe_ring response much larger.

rpc_min_threads doesn't have any effect.;;;","20/Dec/13 01:10;xedin;Does it have any ERROR messages in the server log starting with ""invalid frame size""? I think default frame size should be set to 15 mb which is enough for most of the things, but please check just in case.;;;","20/Dec/13 01:21;nickmbailey;Nope, no errors in the logs at all.;;;","09/Jan/14 23:24;xedin;[~nickmbailey] I have attached updated disruptor server jar to the CASSANDRA-6407, can you please try your test with update jar?;;;","10/Jan/14 16:23;nickmbailey;If I use the patch I attached previously I get the timeout as I described before. When I copy your updated jar over the existing jar I get a different error:

{noformat}
automaton@ip-10-196-42-161:~/cassandra$ PYTHONPATH=test nosetests --tests=system.test_thrift_server:TestMutations.test_describe_ring
E
======================================================================
ERROR: system.test_thrift_server.TestMutations.test_describe_ring
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/usr/local/lib/python2.7/dist-packages/nose/case.py"", line 381, in setUp
    try_run(self.inst, ('setup', 'setUp'))
  File ""/usr/local/lib/python2.7/dist-packages/nose/util.py"", line 469, in try_run
    return func()
  File ""/home/automaton/cassandra/test/system/__init__.py"", line 114, in setUp
    self.define_schema()
  File ""/home/automaton/cassandra/test/system/__init__.py"", line 183, in define_schema
    self.client.system_add_keyspace(ks)
  File ""/home/automaton/cassandra/interface/thrift/gen-py/cassandra/Cassandra.py"", line 1783, in system_add_keyspace
    return self.recv_system_add_keyspace()
  File ""/home/automaton/cassandra/interface/thrift/gen-py/cassandra/Cassandra.py"", line 1794, in recv_system_add_keyspace
    (fname, mtype, rseqid) = self._iprot.readMessageBegin()
  File ""/usr/lib/python2.7/dist-packages/thrift/protocol/TBinaryProtocol.py"", line 126, in readMessageBegin
    sz = self.readI32()
  File ""/usr/lib/python2.7/dist-packages/thrift/protocol/TBinaryProtocol.py"", line 203, in readI32
    buff = self.trans.readAll(4)
  File ""/usr/lib/python2.7/dist-packages/thrift/transport/TTransport.py"", line 58, in readAll
    chunk = self.read(sz-have)
  File ""/usr/lib/python2.7/dist-packages/thrift/transport/TTransport.py"", line 272, in read
    self.readFrame()
  File ""/usr/lib/python2.7/dist-packages/thrift/transport/TTransport.py"", line 276, in readFrame
    buff = self.__trans.readAll(4)
  File ""/usr/lib/python2.7/dist-packages/thrift/transport/TTransport.py"", line 58, in readAll
    chunk = self.read(sz-have)
  File ""/usr/lib/python2.7/dist-packages/thrift/transport/TSocket.py"", line 94, in read
    buff = self.handle.recv(sz)
error: [Errno 104] Connection reset by peer

----------------------------------------------------------------------
Ran 1 test in 8.528s

FAILED (errors=1)
{noformat};;;","10/Jan/14 18:56;xedin;Did you replace old one or just copied it over, does system log say anything? I'm going to try it myself today too.;;;","10/Jan/14 19:08;nickmbailey;Actually I do see an error in the log:

{noformat}
ERROR [main] 2014-01-10 19:06:51,179 CassandraDaemon.java (line 478) Exception encountered during startup
java.lang.NoClassDefFoundError: com/lmax/disruptor/EventTranslator
        at com.thinkaurelius.thrift.TDisruptorServer.<init>(TDisruptorServer.java:192)
        at org.apache.cassandra.thrift.THsHaDisruptorServer.<init>(THsHaDisruptorServer.java:46)
        at org.apache.cassandra.thrift.THsHaDisruptorServer$Factory.buildTServer(THsHaDisruptorServer.java:90)
        at org.apache.cassandra.thrift.TServerCustomFactory.buildTServer(TServerCustomFactory.java:56)
        at org.apache.cassandra.thrift.ThriftServer$ThriftServerThread.<init>(ThriftServer.java:130)
        at org.apache.cassandra.thrift.ThriftServer.start(ThriftServer.java:56)
        at org.apache.cassandra.service.CassandraDaemon.start(CassandraDaemon.java:414)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:474)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)
Caused by: java.lang.ClassNotFoundException: com.lmax.disruptor.EventTranslator
        at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
        at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:425)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:308)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:358)
        ... 9 more
 INFO [StorageServiceShutdownHook] 2014-01-10 19:06:51,220 Gossiper.java (line 1238) Announcing shutdown
DEBUG [GossipTasks:1] 2014-01-10 19:06:51,722 DebuggableThreadPoolExecutor.java (line 245) Task cancelled
java.util.concurrent.CancellationException
        at java.util.concurrent.FutureTask.report(FutureTask.java:121)
        at java.util.concurrent.FutureTask.get(FutureTask.java:188)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.extractThrowable(DebuggableThreadPoolExecutor.java:237)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.logExceptionsAfterExecute(DebuggableThreadPoolExecutor.java:201)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor.afterExecute(DebuggableScheduledThreadPoolExecutor.java:46)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1153)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{noformat}

I copied your new jar over the existing disruptor jar:

{noformat}
cp disruptor-thrift-server-0.3.3-SNAPSHOT.jar lib/disruptor-3.0.1.jar
{noformat};;;","10/Jan/14 19:11;nickmbailey;Perhaps I was supposed to just copy the jar into the lib folder and not replace the old jar? I just tried that and it looks like everything is fixed.;;;","10/Jan/14 20:14;xedin;You have replaced the wrong jar :) there are two of them - disruptor and disruptor_thrift_server, can you bring disruptor jar back and replace disruptor_thrift_server instead?;;;","10/Jan/14 21:36;nickmbailey;Got it:

{noformat}
mv disruptor-thrift-server-0.3.3-SNAPSHOT.jar lib/thrift-server-0.3.2.jar
{noformat}

That appears to fix the issue.;;;","10/Jan/14 21:52;xedin;Thanks! I will release disruptor_thrift_server and close both issues once build.xml is changed.;;;","11/Jan/14 01:29;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updating cql created table through cassandra-cli transform it into a compact storage table,CASSANDRA-6370,12679734,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,arodrime,arodrime,18/Nov/13 14:23,16/Apr/19 09:31,14/Jul/23 05:53,18/Nov/13 16:46,1.2.12,,,,,,0,,,,,"To reproduce :

echo ""CREATE TABLE test (aid int, period text, event text, viewer text, PRIMARY KEY (aid, period, event, viewer) );"" | cqlsh -kmykeyspace;

echo ""describe table test;"" | cqlsh -kmykeyspace;

Output >
CREATE TABLE test (
  aid int,
  period text,
  event text,
  viewer text,
  PRIMARY KEY (aid, period, event, viewer)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

Then do :

echo ""update column family test with dclocal_read_repair_chance = 0.1;"" | cassandra-cli -kmykeyspace

And finally again : echo ""describe table test;"" | cqlsh -kmykeyspace;

Output >

CREATE TABLE test (
  aid int,
  column1 text,
  column2 text,
  column3 text,
  column4 text,
  value blob,
  PRIMARY KEY (aid, column1, column2, column3, column4)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

This is quite annoying in production. If it is happening to you: 
UPDATE system.schema_columnfamilies SET column_aliases = '[""period"",""event"",""viewer""]' WHERE keyspace_name='mykeyspace' AND columnfamily_name='test'; should help restoring the table. (Thanks Sylvain for this information.)",,aleksey,arodrime,devdazed,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Nov/13 15:42;slebresne;6370.txt;https://issues.apache.org/jira/secure/attachment/12614406/6370.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,359092,,,Mon Nov 18 16:46:33 UTC 2013,,,,,,,,,,"0|i1pwhj:",359382,1.2.11,,,,,,,,jbellis,,jbellis,Critical,,1.2.2,,,,,,,,,,,,,,,,"18/Nov/13 14:41;jbellis;Why do you think cli does not list cql-created tables?;;;","18/Nov/13 15:13;slebresne;I'm going to reopen because while I agree that you should absolutely stick to cqlsh when dealing with CQL3 tables, I think it doesn't cost us much to either make sure it's not too easy to shoot yourself in the foot or at least disallow modifications of CQL3 table from thrift if that screw them up (especially since it's pretty damn hard to get back on your feet afterwards unless you're very familiar with the schema code).;;;","18/Nov/13 15:15;devdazed;I tend to agree. If there is any unexpected behavior that could arise then it should be prevented from happening, a big warning like ""THIS WILL ALTER YOUR TABLE WITH COMPACT STORAGE ... Continue Y/N?"", so the user is aware of what is happening.  Simply saying ""it's hidden when you list it"" is not a solution IMO.;;;","18/Nov/13 15:37;arodrime;I didn't try to list any table here.

I heard was thrift / cql were abstractions of the same data and that we could continuing to use both. I thought cassandra-cli was also compatible with cql tables. Even more, I had no error or warning while running my cassandra-cli command.

In my point of view (which was wrong at this time, I agree now) this was a ""normal"" usage of cassandra that resulted into a bug in my productions servers. My error is now fixed as you can read at the end of the description. My point here is to help the Cassandra team to make Cassandra more robust to avoid more issues of this kind.

Do whatever you want with this report, but there is no need of bashing me, this wasn't trivial, and the fact you hide the cql-created tables doesn't help in any way since I didn't list them. 

This happen to me for being an early Cassandra adopter who use to change the schema using cassandra-cli.;;;","18/Nov/13 15:42;slebresne;Let's keep it simple, attaching a patch that just refuse modifications to CQL3 tables from thrift. We don't allow to create them or list them so there's no good reason to allow modifying them and that way we make sure to avoid subtle screw-ups. And if you really want to shoot yourself in the foot by messing up with the underlying schema layout, that's what the System tables are for.;;;","18/Nov/13 15:53;jbellis;+1;;;","18/Nov/13 16:32;aleksey;Edit: d-oh.;;;","18/Nov/13 16:46;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix prepared statement size computation,CASSANDRA-6369,12679712,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,18/Nov/13 11:33,16/Apr/19 09:31,14/Jul/23 05:53,18/Nov/13 16:46,1.2.12,2.0.3,,,,,0,,,,,"When computed the size of CQLStatement to limit the prepared statement cache (CASSANDRA-6107), we overestimate the actual memory used because the statement include a reference to the table CFMetaData which measureDeep counts. And as it happens, that reference is big: on a simple test preparing a very trivial select statement, I was able to only prepare 87 statements before some started to be evicted because each statement was more than 93K big and more than 92K of that was the CFMetaData object. As it happens there is no reason to account the CFMetaData object at all since it's in memory anyway whether or not there is prepared statements or not.

Attaching a simple (if not extremely elegant) patch to remove what we don't care about of the computation. Another solution would be to use the MemoryMeter.withTrackerProvider option as we do in Memtable, but in the QueryProcessor case we currently use only one MemoryMeter, not one per CF, so it didn't felt necessarilly cleaner. We could create one-shot MemoryMeter object each time we need to measure a CQLStatement but that doesn't feel a lot simpler/cleaner either. But if someone feels religious about some other solution, I don't care.
",,jumar,lyubent,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6592,,,,,,,,,,,"18/Nov/13 11:33;slebresne;6369.txt;https://issues.apache.org/jira/secure/attachment/12614371/6369.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,359070,,,Mon Nov 18 16:46:20 UTC 2013,,,,,,,,,,"0|i1pwcn:",359360,,,,,,,,,lyubent,,lyubent,Normal,,1.2.11,,,,,,,,,,,,,,,,"18/Nov/13 15:03;lyubent;LGTM.;;;","18/Nov/13 16:46;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bisect unit test failures on 2.0 branch,CASSANDRA-6365,12679586,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mshuler,jbellis,jbellis,16/Nov/13 20:14,16/Apr/19 09:32,14/Jul/23 05:53,06/Dec/13 00:12,,,,Legacy/Testing,,,0,qa-resolved,,,,"Unit tests pass in 2.0.1.

They do not in 2.0.2.

Let's find where the failures were introduced.",,mishail,mshuler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Nov/13 01:52;jbellis;2.0.1-utest.txt;https://issues.apache.org/jira/secure/attachment/12614263/2.0.1-utest.txt","16/Nov/13 21:29;mshuler;C-2.0.1_tag_utests.txt;https://issues.apache.org/jira/secure/attachment/12614249/C-2.0.1_tag_utests.txt",,,,,,,,,,,,,,,,,,,2.0,mshuler,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358946,,,Fri Dec 06 00:11:00 UTC 2013,,,,,,,,,,"0|i1pvl3:",359236,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"16/Nov/13 21:29;mshuler;Unit Test results from c-2.0.1 git tag (full output attached):

{code}
((cassandra-2.0.1) *)mshuler@mana:~/DataStax/repos/cassandra$ egrep '.*Testcase.*ERROR' ../../tmp/C-2.0.1_tag_utests.txt
    [junit] Testcase: org.apache.cassandra.cli.CliTest:testCli: Caused an ERROR
    [junit] Testcase: org.apache.cassandra.service.EmbeddedCassandraServiceTest:BeforeFirstTest:        Caused an ERROR
    [junit] Testcase: org.apache.cassandra.streaming.StreamingTransferTest:testRandomSSTableTransfer:   Caused an ERROR
{code};;;","17/Nov/13 01:52;jbellis;You may be seeing a heisenbug or two.  Passing test run attached.

;;;","17/Nov/13 03:12;mishail;http://buildbot.datastax.com:8020/builders/cassandra-2.0/builds/210 is the latest ""green"" build for 2.0.x
http://buildbot.datastax.com:8020/waterfall?last_time=1381334941&show=cassandra-2.0

It was triggered by https://github.com/apache/cassandra/commit/01a57eea841e51fb4a97329ab9fa0f59d0b826f6 which is *after* 2.0.1 but before 2.0.2;;;","17/Nov/13 03:58;jbellis;01a57eea841e51fb4a97329ab9fa0f59d0b826f6 passes for me too :-| [Edit: twice in a row]

My takeaway is we've had heisenbugs for a while but at some point we introduced some that reproduce more readily because 2.0 HEAD hasn't passed in a long time.;;;","06/Dec/13 00:11;mshuler;Latest several 2.0 branch unit test builds have passed successfully - good work!
http://cassci.datastax.com/job/cassandra-2.0_test/;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader does not free off-heap memory for index summary,CASSANDRA-6359,12679499,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,thobbs,thobbs,15/Nov/13 21:45,16/Apr/19 09:32,14/Jul/23 05:53,16/Nov/13 16:31,2.0.3,,,Legacy/Tools,,,0,,,,,"Although sstableloader tells {{SSTableReaders}} to release their references to the {{IndexSummary}} objects, the summary's {{Memory}} is never {{free()}}'d, causing an off-heap memory leak.",,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/13 21:48;thobbs;0001-Free-off-heap-memory-when-releasing-index-summary.patch;https://issues.apache.org/jira/secure/attachment/12614140/0001-Free-off-heap-memory-when-releasing-index-summary.patch",,,,,,,,,,,,,,,,,,,,1.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358859,,,Sat Nov 16 16:31:45 UTC 2013,,,,,,,,,,"0|i1pv1r:",359149,,,,,,,,,jbellis,,jbellis,Low,,2.0.0,,,,,,,,,,,,,,,,"15/Nov/13 21:48;thobbs;Attached patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6359]) properly closes the {{IndexSummary}} before releasing the reference.;;;","16/Nov/13 16:31;jbellis;LGTM, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSTable read meter sync not cancelled when reader is closed,CASSANDRA-6358,12679494,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,thobbs,thobbs,15/Nov/13 21:32,16/Apr/19 09:32,14/Jul/23 05:53,16/Nov/13 16:34,2.0.3,,,,,,0,,,,,We run a fixed-schedule task to sync the read meter for every SSTableReader periodically.  These tasks are not cancelled when the SSTR is closed.,,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/13 21:36;thobbs;0001-Cancel-SSTR-read-meter-syncer-on-close.patch;https://issues.apache.org/jira/secure/attachment/12614135/0001-Cancel-SSTR-read-meter-syncer-on-close.patch",,,,,,,,,,,,,,,,,,,,1.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358854,,,Sat Nov 16 16:34:43 UTC 2013,,,,,,,,,,"0|i1pv0n:",359144,2.0.2,,,,,,,,jbellis,,jbellis,Normal,,2.0.2,,,,,,,,,,,,,,,,"15/Nov/13 21:36;thobbs;Attached patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6358]) cancel the scheduled task when the SSTableReader is closed.;;;","16/Nov/13 16:34;jbellis;LGTM, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"When dropping a CF, row cache is not invalidated",CASSANDRA-6351,12679404,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,frousseau,frousseau,frousseau,15/Nov/13 14:48,16/Apr/19 09:32,14/Jul/23 05:53,16/Nov/13 17:11,1.2.12,2.0.3,,,,,0,,,,,"When dropping a ColumnFamily with row cache enabled, then row cache is not invalidated for this CF.

This can be a bit annoying if the ColumnFamily is recreated because it will be empty, but row cache won't.
Note : this is similar to a ""TRUNCATE"" command (and TRUNCATE does invalidate the cache...)

Attached is patch which removes the rows of the currently dropped CF from row cache.",,frousseau,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/13 14:49;frousseau;0001-invalidate-row-cache-when-dropping-CF.patch;https://issues.apache.org/jira/secure/attachment/12614067/0001-invalidate-row-cache-when-dropping-CF.patch",,,,,,,,,,,,,,,,,,,,1.0,frousseau,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358764,,,Sat Nov 16 17:11:20 UTC 2013,,,,,,,,,,"0|i1pugn:",359054,1.2.10,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"16/Nov/13 17:11;jbellis;LGTM, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IOException in MessagingService.run() causes orphaned storage server socket,CASSANDRA-6349,12679323,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mishail,cl1cker,cl1cker,15/Nov/13 01:47,16/Apr/19 09:32,14/Jul/23 05:53,16/Nov/13 16:26,2.0.3,,,,,,0,,,,,"The refactoring of reading the message header in MessagingService.run() vs IncomingTcpConnection seems to mishandle IOException as the loop is broken and MessagingService.SocketThread never seems to get reinitialized.

To reproduce: telnet to port 7000 and send random data. This then prevents any new or restarting node in the cluster from handshaking with this defunct storage port.",cassandra 2.0+,cl1cker,mishail,oseiler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-10816,,,,,,CASSANDRA-6468,,,,,"16/Nov/13 03:49;mishail;CASSANDRA-2.0-6349.patch;https://issues.apache.org/jira/secure/attachment/12614197/CASSANDRA-2.0-6349.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358685,,,Wed Dec 11 00:57:15 UTC 2013,,,,,,,,,,"0|i1ptz3:",358975,2.0.2,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"16/Nov/13 03:49;mishail;One of the options is to close the socket on IOException and continue;;;","16/Nov/13 16:26;jbellis;LGTM, committed;;;","09/Dec/13 16:40;oseiler;I suspect these changes introduced an infinite loop if the ServerSocket gets closed (not sure how that is happening though). We've been seeing some major problems with Cassandra 2.0.3 when a new cluster is coming up for the first time, and it seems to be a result of this. With logging set to debug, system.log is getting pummelled with these exception messages:

{noformat}
DEBUG [ACCEPT-localhost-grid/10.96.99.178] 2013-12-06 22:55:39,759 MessagingService.java (line 905) Error reading the socket null
java.net.SocketException: Socket closed
        at java.net.PlainSocketImpl.socketAccept(Native Method)
        at java.net.AbstractPlainSocketImpl.accept(Unknown Source)
        at java.net.ServerSocket.implAccept(Unknown Source)
        at sun.security.ssl.SSLServerSocketImpl.accept(Unknown Source)
        at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:865)
{noformat}

It looks like once in this state, nothing will break it out; prior to this change the IOException catch block was throwing another exception, now it just keeps looping, using the (seemingly closed) ServerSocket. Restarting Cassandra seems to be the only way to resolve this. I'll probably be recommending we drop back to 2.0.2 until this problem is fixed (or we can understand why the ServerSocket is closed...)
;;;","09/Dec/13 18:41;mishail;The current SocketThread's code detects whether the ServerSocket is closing by catching AsynchronousCloseException/ClosedChannelException and breaking the endless loop.
And it looks like SSLServerSocketImpl throws a different exception (SocketException) which the thread doesn't handle

Do we really need {{while(true)}} there? Why can't we use {{while (!server.isClosed())}} instead?
;;;","10/Dec/13 07:01;mishail;CASSANDRA-6468;;;","11/Dec/13 00:57;cl1cker;Please also note that handling the protocol magic and version handshake in the while loop allows an attacker to open a connection and not send any data, preventing any further connections. Prior revisions handled all the handshaking in the resulting thread where it might be more appropriate.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOCAL_ONE code in the native protocol is not the same in C* 1.2 and C* 2.0,CASSANDRA-6347,12679251,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,14/Nov/13 17:26,16/Apr/19 09:32,14/Jul/23 05:53,19/Nov/13 10:58,1.2.12,,,,,,0,,,,,"When LOCAL_ONE was added (CASSANDRA-6202), it was unfortunately not given the same code (the one used by the native protocol) in C* 1.2 and C* 2.0.  In 1.2 it's 8 (even though the specification document pretends it's 10) while it's 10 in 2.0.

This basically breaks backward compatibility for the v1 protocol between C* 1.2 and C* 2.0. Now, we could ""fix"" 2.0 adding special cases for the v1 protocol but that's going to be a bit of a pain, so instead I suggest to just switch to 10 in 1.2. Since the spec was wrong anyway and nobody complained so far this suggest no-one has really added support for LOCAL_ONE in the native protocol against 1.2.11, so if we change it now we can just say to people to upgrade to 1.2.12 directly if they want to use LOCAL_ONE with the native protocol. Attaching simple patch for that.
",,jasobrown,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Nov/13 17:27;slebresne;6347.txt;https://issues.apache.org/jira/secure/attachment/12613880/6347.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358616,,,Tue Nov 19 10:58:26 UTC 2013,,,,,,,,,,"0|i1ptjr:",358906,,,,,,,,,jasobrown,,jasobrown,Normal,,,,,,,,,,,,,,,,,,"15/Nov/13 06:15;jasobrown;Crap, I thought I fixed all the id madness across 1.2 vs 2.0, thrift  vs native protocol, native protocol 1 vs 2. <sigh> . Thanks for discovering and fixing. 

+1
;;;","19/Nov/13 10:58;slebresne;Committed (a few days ago but forgot to close), thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Endpoint cache invalidation causes CPU spike (on vnode rings?),CASSANDRA-6345,12679044,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,rbranson,rbranson,13/Nov/13 16:06,16/Apr/19 09:32,14/Jul/23 05:53,26/Nov/13 20:11,1.2.13,2.0.4,,,,,2,,,,,"We've observed that events which cause invalidation of the endpoint cache (update keyspace, add/remove nodes, etc) in AbstractReplicationStrategy result in several seconds of thundering herd behavior on the entire cluster. 

A thread dump shows over a hundred threads (I stopped counting at that point) with a backtrace like this:

        at java.net.Inet4Address.getAddress(Inet4Address.java:288)
        at org.apache.cassandra.locator.TokenMetadata$1.compare(TokenMetadata.java:106)
        at org.apache.cassandra.locator.TokenMetadata$1.compare(TokenMetadata.java:103)
        at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:351)
        at java.util.TreeMap.getEntry(TreeMap.java:322)
        at java.util.TreeMap.get(TreeMap.java:255)
        at com.google.common.collect.AbstractMultimap.put(AbstractMultimap.java:200)
        at com.google.common.collect.AbstractSetMultimap.put(AbstractSetMultimap.java:117)
        at com.google.common.collect.TreeMultimap.put(TreeMultimap.java:74)
        at com.google.common.collect.AbstractMultimap.putAll(AbstractMultimap.java:273)
        at com.google.common.collect.TreeMultimap.putAll(TreeMultimap.java:74)
        at org.apache.cassandra.utils.SortedBiMultiValMap.create(SortedBiMultiValMap.java:60)
        at org.apache.cassandra.locator.TokenMetadata.cloneOnlyTokenMap(TokenMetadata.java:598)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:104)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:2671)
        at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:375)

It looks like there's a large amount of cost in the TokenMetadata.cloneOnlyTokenMap that AbstractReplicationStrategy.getNaturalEndpoints is calling each time there is a cache miss for an endpoint. It seems as if this would only impact clusters with large numbers of tokens, so it's probably a vnodes-only issue.

Proposal: In AbstractReplicationStrategy.getNaturalEndpoints(), cache the cloned TokenMetadata instance returned by TokenMetadata.cloneOnlyTokenMap(), wrapping it with a lock to prevent stampedes, and clearing it in clearEndpointCache(). Thoughts?","30 nodes total, 2 DCs
Cassandra 1.2.11
vnodes enabled (256 per node)",cburroughs,jjordan,mishail,rbranson,tupshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6127,,,,,,,,,,,,,,,"15/Nov/13 07:28;rbranson;6345-rbranson-v2.txt;https://issues.apache.org/jira/secure/attachment/12614030/6345-rbranson-v2.txt","13/Nov/13 21:52;rbranson;6345-rbranson.txt;https://issues.apache.org/jira/secure/attachment/12613706/6345-rbranson.txt","13/Nov/13 21:26;jbellis;6345-v2.txt;https://issues.apache.org/jira/secure/attachment/12613697/6345-v2.txt","18/Nov/13 03:52;jbellis;6345-v3.txt;https://issues.apache.org/jira/secure/attachment/12614329/6345-v3.txt","20/Nov/13 04:40;jbellis;6345-v4.txt;https://issues.apache.org/jira/secure/attachment/12614786/6345-v4.txt","22/Nov/13 14:29;jbellis;6345-v5.txt;https://issues.apache.org/jira/secure/attachment/12615340/6345-v5.txt","13/Nov/13 21:06;jbellis;6345.txt;https://issues.apache.org/jira/secure/attachment/12613688/6345.txt","13/Nov/13 21:48;rbranson;half-way-thru-6345-rbranson-patch-applied.png;https://issues.apache.org/jira/secure/attachment/12613704/half-way-thru-6345-rbranson-patch-applied.png",,,,,,,,,,,,,8.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358409,,,Mon Dec 09 23:50:02 UTC 2013,,,,,,,,,,"0|i1ps9r:",358699,1.2.11,,,,,,,,rbranson,,rbranson,Normal,,,,,,,,,,,,,,,,,,"13/Nov/13 17:05;jbellis;Interesting.  Could we optimize cOTM instead?  That would definitely be the simplest solution.

E.g. it looks like TreeMultimap.putAll actually loops over each entry and calls put one at a time which is the worst-case scenario for binary tree rebalancing -- quadratic time.;;;","13/Nov/13 17:30;jbellis;bq. it looks like TreeMultimap.putAll actually loops over each entry and calls put one at a time

I don't see a way around this: https://code.google.com/p/guava-libraries/issues/detail?id=1579;;;","13/Nov/13 17:34;benedict;A Treap? Can be cheaply built, cheaply merged and cheaply cloned.

Also, anything cheaply cloneable would work for that operation. A SnapTree that is wrapped to support multi-map functionality would also work.;;;","13/Nov/13 18:01;jbellis;Yes.  Too bad the implementation classes like AbstractSortedKeySortedSetMultimap are package-private.;;;","13/Nov/13 20:47;jbellis;Actually I think a STM multimap would still get messy quickly since you need to do a ""deep"" clone -- cloning the top-level Map would leave the values (sub-collections) sharing a reference.;;;","13/Nov/13 20:52;benedict;Just have the wrapper make any updates to a collection replace the collection instead of modifying it.

[NB: I haven't looked to see if this would have any negative performance implications on the update side, I'm assuming the reads are more frequent and/or collections small... if not, a Treap is probably the better choice as my old Jjoost implementation (IIRC) supports snapshotting and multiple values are dealt with inside the tree itself, not as a collection];;;","13/Nov/13 21:06;jbellis;bq. Proposal: In AbstractReplicationStrategy.getNaturalEndpoints(), cache the cloned TokenMetadata instance returned by TokenMetadata.cloneOnlyTokenMap(), wrapping it with a lock to prevent stampedes, and clearing it in clearEndpointCache().

Why not just use a sharded lock to prevent stampedes directly w/o the caching complexity, as in the attached?;;;","13/Nov/13 21:26;jbellis;I see, with vnodes we have enough ranges that we can have a thundering herd even if each range only clones once.

v2 attached with the approach you described originally.;;;","13/Nov/13 21:44;rbranson;Attached a patch we deployed to production that fixed the issue.;;;","13/Nov/13 21:48;rbranson;CPU user% graph during the rollout of the patch I attached on 1 DC (15 nodes) of the cluster. Around ~21:05 the patch starts to roll out and spikes are seen. The node in question receives the patch at ~21:30, and afterwards the spikes are gone. The rollout finishes at ~21:45.;;;","13/Nov/13 23:26;jbellis;I have to admit I like it better without the custom wrapper class. :);;;","14/Nov/13 00:25;rbranson;Well, I started writing the patch this morning and I don't write multi-threaded Java code every day, so I'm overly careful ;) The only theoretical advantage to my patch is that it allows concurrent readers.;;;","15/Nov/13 04:43;rbranson;Unfortunately both of the patches suffer from a deadlock, since the invalidation and fill are wrapped up in TokenMetadata's locks.

T1 acquires cache read lock
T2 acquires TokenMetadata write lock
T1 acquires cache write lock on miss
T2 is blocked on cache write lock trying to invalidate
T1 is blocked on TokenMetadata read lock trying to cloneOnlyTokenMap to fill the cache

Trying to work on a fix.;;;","15/Nov/13 07:28;rbranson;Attached a new patch with the deadlock fixed. We're running this on a production cluster.

The primary issue was the callback for invalidation from TokenMetadata to all of the registered AbstractReplicationStrategy instances. This was asking for it anyway, so in the patch I replaced the ""push"" invalidation with simple versioning of the TokenMetadata endpoints. TokenMetadata bumps it's version number each time the cache would need to be invalidated, and AbstractReplicationStrategy checks it's version when it needs to do a read, invalidating if necessary. This gets the invalidation out of the gossip threads and into the RPC threads, which is probably a good thing. The only thing I'm not super crazy about is the extra hot path read lock acquisition on TokenMetadata.getEndpointVersion(), which might be avoidable.;;;","18/Nov/13 03:52;jbellis;I think we can craft a simpler solution (v3) by using an AtomicReference to the TM clone.  This removes the possibility of deadlock since clearEndpointCache now only makes non-blocking calls.

I've also refined it to use a Striped<Lock> per-keyToken, as well as synchronizing the TM clone itself, since concurrent endpoint computation is fine.;;;","18/Nov/13 18:51;rbranson;I like the simpler approach. I still think the callbacks for invalidation are asking for it ;) I also think perhaps the stampede lock should be more explicit than a synchronized lock on ""this"" to prevent unintended blocking from future modifications.

Either way, I think the only material concern I have is the order that TokenMetadata changes get applied to the caches in AbstractReplicationStrategy instances. Shouldn't the invalidation take place on all threads in all instances of AbstractReplicationStrategy before returning from an endpoint-mutating write operation in TokenMetadata? It seems as if just setting the cache to empty would allow a period of time where TokenMetadata write methods had returned but not all threads have seen the mutation yet because they are still holding onto the old clone of TM. This might be alright though, I'm not sure. Thoughts?;;;","20/Nov/13 04:25;jbellis;bq. It seems as if just setting the cache to empty would allow a period of time where TokenMetadata write methods had returned but not all threads have seen the mutation yet

I'm not 100% sure this is what you're talking about, but I see this problem with the existing code (and my v3):

{noformat}
Thread 1                 Thread 2        
getNaturalEndpoints      
cloneOnlyTokenMap        
                         invalidateCachedTokenEndpointValues
endpoints = calculate
cacheEndpoint [based on the now-invalidated token map]
{noformat}

So it doesn't quite work.  We'd need to introduce another AtomicReference on the cache, so that invalidate could create a new Map (so it doesn't matter if someone updates the old one).  But I think you're right that getting rid of the callback approach entirely is better.;;;","20/Nov/13 04:40;jbellis;v4 attached that uses a versioning approach like yours.  I dropped the readLock acquire on version read since it's not necessary to block callers during the update.  (A few extra over-broad replica set operations won't hurt.);;;","20/Nov/13 20:49;rbranson;+100 at removing those pub/sub callbacks :)

The concurrency issues I bring up are probably because I'm unfamiliar with the ""guarantees"" needed by TokenMetadata updates. It looks like the current release code is subject to the issue I brought up, where method calls on TokenMetadata that change state return successfully before all threads applying mutations have ""seen"" the update. There will be some mutations in progress that are using ""stale"" token data to apply writes even after TokenMetadata write methods returns as successful. So this does not appear to be a regression, but I'm just being overly cautious having been burned by these sort of double-caching scenarios before. You bring up the point that over-broad operations are ok, and I agree, but I'm more concerned about operations that are too narrow. It seems that unless I'm missing something either is possible with the current release code, and thus these patches as well (including mine).

TokenMetadata#updateNormalTokens is (implicitly) relying on the removeFromMoving call to bump the version, but the tokenToEndpointMap is updated afterwards, which means internal data is updated after the version is bumped. IMHO to be defensive, any time the write lock is acquired in TokenMetadata, the version should be bumped in the finally block before the lock is released. I don't think this is exposing a bug in the existing patch though, because cloneOnlyTokenMap will be blocked until the write lock is released in the finally block.

Is the idea with the striped lock on the endpoint cache in AbstractReplicationStrategy to help smooth out the stampede effect when the ""global"" lock on the cached TM gets released after the fill? How much do you think it's worth the extra complexity? FWIW, my v2 patch suffers from this issue and it hasn't reared itself in production. The write load for the machines in the cluster I've been looking at is comparatively low though compared to many others at 6-7k/sec peak on an 8-core box.;;;","22/Nov/13 14:29;jbellis;bq. It seems that unless I'm missing something either is possible with the current release code, and thus these patches as well

Technically correct, but in practice we're in pretty good shape.  The sequence is:

# Add the changing node to pending ranges
# Sleep for RING_DELAY so everyone else starts including the new target in their writes
# Flush data to be transferred
# Send over data for writes that happened before (1)

Step 1 happens on every coordinator.  2-4 only happen on the node that is giving up a token range.

The guarantee we need is that any write that happens before the pending range change, completes before the subsequent flush.

Even if we used TM.lock to protect the entire ARS sequence (guaranteeing that no local write is in progress once the PRC happens) we could still receive writes from other nodes that began their PRC change later.  

So we rely on the RING_DELAY (30s) sleep.  I suppose a GC pause for instance at just the wrong time could theoretically mean a mutation against the old state gets sent out late, but I don't see how we can improve it.

bq. IMHO to be defensive, any time the write lock is acquired in TokenMetadata, the version should be bumped in the finally block before the lock is released

Haven't thought this through as much.  What are you saying we should bump that we weren't calling invalidate on before?

bq. Is the idea with the striped lock on the endpoint cache in AbstractReplicationStrategy to help smooth out the stampede effect when the ""global"" lock on the cached TM gets released after the fill?

I'm trying to avoid a minor stampede on calculateNaturalEndpoints (CASSANDRA-3881) but it's probably premature optimization.  v5 attached w/o that.;;;","26/Nov/13 17:33;rbranson;Thanks for taking the time to explain the consistency story. It makes perfect sense. 

My defensiveness comment suggested bumping the version number each time the TM write lock is released, which would be in addition to the existing invalidations. You're probably a much better gauge on the usefulness of this, so up to you.

Really nice that the v5 patch is so compact. Two minor comments: the endpointsLock declaration is still in there, and not to be all nitpicky but there are two typos in the comments (""wo we keep"" and ""clone got invalidted"").;;;","26/Nov/13 20:11;jbellis;bq. My defensiveness comment suggested bumping the version number each time the TM write lock is released, which would be in addition to the existing invalidations.

Okay.  I'm going to leave this be then, because I don't want to accidentally start invalidating the cache unnecessarily because one of those operations was more common than I thought.  Could address in trunk if you want to open a ticket.

Committed v5 w/ nits fixed.;;;","26/Nov/13 22:26;rbranson;LGTM!;;;","09/Dec/13 21:24;cburroughs;{noformat}
private volatile long ringVersion = 0;

ringVersion++;
{noformat}

If there is something tricky here that makes an increment on a volatile okay then it deserves a comment.;;;","09/Dec/13 23:50;jbellis;We don't care about keeping an accurate count, only that once it's done with that block it's higher than it was before.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
C* throws AssertionError when using paging and reverse ordering,CASSANDRA-6343,12679011,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,ach,ach,13/Nov/13 13:33,16/Apr/19 09:32,14/Jul/23 05:53,13/Nov/13 17:02,2.0.3,,,,,,0,,,,,"We have a table with CLUSTERING ORDER BY (date DESC). We try to do a query C* with paging and ORDER BY date ASC. 
This leads to the following exception in C* when pager goes to the last page:
{quote}
ERROR [Native-Transport-Requests:1287744] 2013-10-30 01:53:14,720 ErrorMessage.java (line 210) Unexpected exception during request
java.lang.AssertionError: Added column does not sort as the first column
        at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:115)
        at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:116)
        at org.apache.cassandra.service.pager.AbstractQueryPager.discardLast(AbstractQueryPager.java:238)
        at org.apache.cassandra.service.pager.AbstractQueryPager.discardLast(AbstractQueryPager.java:182)
        at org.apache.cassandra.service.pager.AbstractQueryPager.fetchPage(AbstractQueryPager.java:100)
        at org.apache.cassandra.service.pager.SliceQueryPager.fetchPage(SliceQueryPager.java:33)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:179)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:56)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:101)
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:235)
        at org.apache.cassandra.transport.messages.ExecuteMessage.execute(ExecuteMessage.java:139)
        at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:296)
        at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:45)
        at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:69)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{quote}

","Last time issue has been reproduced with C* version is 2.0.0, DataStax CQL driver version 2.0.0-beta1 on a single node.",ach,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/13 15:32;slebresne;6343.txt;https://issues.apache.org/jira/secure/attachment/12613626/6343.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358376,,,Wed Nov 13 17:02:22 UTC 2013,,,,,,,,,,"0|i1ps2f:",358666,2.0.0,,,,,,,,aleksey,,aleksey,Normal,,2.0.0,,,,,,,,,,,,,,,,"13/Nov/13 15:32;slebresne;Seems reversed completely slipped through the cracks somehow. Attaching patch to fix (includes a unit test).;;;","13/Nov/13 16:11;aleksey;+1;;;","13/Nov/13 17:02;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra document errata,CASSANDRA-6342,12678969,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lyubent,knight76,knight76,13/Nov/13 09:05,16/Apr/19 09:32,14/Jul/23 05:53,25/Nov/13 23:32,1.2.13,2.0.3,,,,,0,,,,,"Hi
I think a sample cql statement of cassandra document (http://cassandra.apache.org/doc/cql3/CQL.html) is wrong. Please change it.
------------------------------------------------------
Note that TTLs are allowed for both INSERT and UPDATE, but in both case the TTL set only apply to the newly inserted/updated values. In other words,

// Updating (or inserting)
UPDATE users USING TTL 10 SET favs['color'] = 'green' WHERE id = 'jsmith'
will only apply the TTL to the { 'color' : 'green' } record, the rest of the map remaining unaffected.

Deleting a map record is done with:

DELETE favs['author'] FROM plays WHERE id = 'jsmith'




upper DELETE cql statement is changed to below. On context of document,  'plays' table might be changed  'users' table. 

DELETE favs['author'] FROM users WHERE id = 'jsmith'
",,aleksey,knight76,lyubent,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Nov/13 21:57;lyubent;6342.patch;https://issues.apache.org/jira/secure/attachment/12615683/6342.patch",,,,,,,,,,,,,,,,,,,,1.0,lyubent,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358335,,,Mon Nov 25 23:32:18 UTC 2013,,,,,,,,,,"0|i1prtb:",358625,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"25/Nov/13 21:56;lyubent;Just renamed the table in the docs. Patch is for 1.2;;;","25/Nov/13 23:32;aleksey;Ninja-d, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"After executing abnormal cql statement, not working  (hang)",CASSANDRA-6341,12678966,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,knight76,knight76,13/Nov/13 08:44,16/Apr/19 09:32,14/Jul/23 05:53,14/Nov/13 17:49,1.2.12,2.0.3,,,,,0,,,,," I use a set type in table like sample, but if awkward cql statement failed, cqlsh is not worked like belows.


serkeyspace> CREATE TABLE images (
                ...     name text PRIMARY KEY,
                ...     owner text,
                ...     date timestamp,
                ...     tags set<text>
                ... );

cqlsh:userkeyspace> delete tags['cuddly'] from images where name = 'cat.jpg'; // not allowd cql statement
(hang)
^C 
cqlsh:userkeyspace> select * from plays;
(hang)
^C
cqlsh:userkeyspace> describe table plays;
(hang)
^C
cqlsh:userkeyspace> quit


---------------------------------

cassandra log when hang is occured.

ERROR 16:59:57,653 Exception in thread Thread[Thrift:8,5,main]
java.lang.AssertionError
	at org.apache.cassandra.cql3.Lists$Discarder.execute(Lists.java:414)
	at org.apache.cassandra.cql3.statements.DeleteStatement.updateForKey(DeleteStatement.java:82)
	at org.apache.cassandra.cql3.statements.ModificationStatement.getMutations(ModificationStatement.java:506)
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:377)
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:363)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:101)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:117)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:108)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1933)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4394)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4378)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)


",2.0.1,aleksey,knight76,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/13 16:31;slebresne;6341.txt;https://issues.apache.org/jira/secure/attachment/12613636/6341.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358332,,,Thu Nov 14 17:49:27 UTC 2013,,,,,,,,,,"0|i1prsn:",358622,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"13/Nov/13 16:31;slebresne;That query is actually valid, but the fact that it triggers an AssertionError is obviously not and due to some bad typo. Attaching trivial patch to fix (I've pushed a dtests too).;;;","13/Nov/13 16:33;slebresne;Actually, seems to affect 1.2 too, so the patch is against 2.0 but it probably apply to 1.2 as well anyway and I'll make sure to commit to 1.2 first in any case.;;;","13/Nov/13 19:33;aleksey;+1;;;","14/Nov/13 17:49;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make gossip tolerate slow Gossip tasks,CASSANDRA-6338,12678906,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,12/Nov/13 23:38,16/Apr/19 09:32,14/Jul/23 05:53,13/Nov/13 15:39,2.0.3,,,,,,0,gossip,,,,"Currently if a single gossip task bogs down the gossip Stage, Gossip will mark everyone down because it hasn't seen updates from them (since they are all queued behind the slow one).

This means that full GCs can cause gossip ""flapping"" as well as any actually problematic tasks such as recomputing pending ranges.",,cburroughs,jasobrown,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6127,,,,,,,,,,,,,,,"12/Nov/13 23:42;jbellis;6338.txt;https://issues.apache.org/jira/secure/attachment/12613464/6338.txt","20/Nov/13 13:12;jbellis;gossip-slowdown.txt;https://issues.apache.org/jira/secure/attachment/12614884/gossip-slowdown.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358272,,,Wed Nov 20 13:12:29 UTC 2013,,,,,,,,,,"0|i1prfb:",358562,,,,,,,,,jasobrown,,jasobrown,Low,,,,,,,,,,,,,,,,,,"12/Nov/13 23:42;jbellis;Attached.

Of course this doesn't help if we have too much for one thread to handle and it just gets farther and farther behind, but I don't think we've seen a failure scenario like that yet (at least post-CASSANDRA-6244).  And even then, at least this gives users something obvious in the log to alert them as to the cause of the problem instead of ""your cluster mysteriously marked everyone down and started serving up UAE."";;;","13/Nov/13 13:05;jasobrown;overall, lgtm. I'd be a bit worried about over-logging but that's probably better than the opposite (what we have now). ;;;","13/Nov/13 13:57;cburroughs;Is the fix version intended to be 2.x only?;;;","13/Nov/13 14:02;jbellis;bq. I'd be a bit worried about over-logging

Me too, but like I said, I've only seen transitory spikes and not a persistent condition.  If overlogging exposes that we have a problem with the latter, then we can work on fixing that.

bq. Is the fix version intended to be 2.x only?

Yes, I'm pretty nervous about causing regressions here because something subtle depended on the existing behavior.;;;","13/Nov/13 14:08;jasobrown;bq. If overlogging exposes that we have a problem with the latter ....

Totally agreed, and that's the reason for this ticket (to point problems) :)
;;;","13/Nov/13 15:39;jbellis;committed;;;","20/Nov/13 13:12;jbellis;Attached patch should make it easy to test this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Set minTimestamp correctly to be able to drop expired sstables,CASSANDRA-6337,12678868,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,marcuse,marcuse,12/Nov/13 19:21,16/Apr/19 09:32,14/Jul/23 05:53,13/Nov/13 19:22,2.0.3,,,,,,0,,,,,"When calculating which sstables we can drop we set minTimestamp to Integer.MAX_VALUE, this is wrong since minTimestamp is a long, and in most cases minTimestamp on sstables is larger than Integer.MAX_VALUE.

We should set it to Long.MAX_VALUE, patch does that.",,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/13 19:22;marcuse;0001-Set-minTimestamp-correctly.patch;https://issues.apache.org/jira/secure/attachment/12613412/0001-Set-minTimestamp-correctly.patch",,,,,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358234,,,Wed Nov 13 19:22:10 UTC 2013,,,,,,,,,,"0|i1pr6v:",358524,,,,,,,,,,,,Low,,2.0 beta 1,,,,,,,,,,,,,,,,"12/Nov/13 20:50;jbellis;+1;;;","13/Nov/13 19:22;marcuse;committed!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"ArrayIndexOutOfBound when using count(*) with over 10,000 rows",CASSANDRA-6333,12678837,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,thattolleyguy,thattolleyguy,12/Nov/13 16:59,16/Apr/19 09:32,14/Jul/23 05:53,22/Nov/13 07:46,2.0.3,,,,,,0,,,,,"We've been getting a TSocket read 0 bytes error when we try and run SELECT count(*) FROM <table> if the table has over 10,000 rows.

I've been able to reproduce the problem by using cassandra-stress to insert different number of rows. When I insert under 10,000, the count is returned. When I insert exactly 10,000, I get a message that my results were limited to 10,000 by default. If insert 10,001, I get the exception below.

{code}
ERROR [Thrift:4] 2013-11-12 09:54:04,850 CustomTThreadPoolServer.java (line 212) Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: -1
	at java.util.ArrayList.elementData(ArrayList.java:371)
	at java.util.ArrayList.remove(ArrayList.java:448)
	at org.apache.cassandra.cql3.ResultSet.trim(ResultSet.java:92)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:848)
	at org.apache.cassandra.cql3.statements.SelectStatement.pageCountQuery(SelectStatement.java:196)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:163)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:57)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:129)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:145)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:136)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1936)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4394)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4378)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
{code}
","Cassandra 2.0.2, Ubuntu 12.04.3 LTS, Oracle Java 1.7.0_21",aleksey,slebresne,thattolleyguy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Nov/13 14:34;slebresne;6333.txt;https://issues.apache.org/jira/secure/attachment/12614064/6333.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,358204,,,Fri Nov 22 07:47:26 UTC 2013,,,,,,,,,,"0|i1pr07:",358494,2.0.2,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"13/Nov/13 16:14;slebresne;Either that has been fixed in the current cassandra-2.0 tip or I'll need complete information to reproduce. I've added the following dtest: https://github.com/riptano/cassandra-dtest/commit/971c4d629a8524205d7bedc1f49315be043a6ceb and tried doing:
{noformat}
> ccm node1 stress -n 10001
...
> ccm node1 cqlsh
Connected to test at 127.0.0.1:9160.
[cqlsh 4.1.0 | Cassandra 2.0.2-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.38.0]
Use HELP for help.
cqlsh> use ""Keyspace1"";
cqlsh:Keyspace1> SELECT COUNT(*) FROM ""Standard1"" LIMIT 10002;

 count
-------
 10001

(1 rows)
{noformat}
All of that seems to be working fine as far as I can tell.;;;","13/Nov/13 17:21;thattolleyguy;If I don't use a limit on the select statement throws the exception.

{noformat}
cqlsh:Keyspace1> select count(*) from ""Standard1"" limit 10002;

 count
-------
 10002

(1 rows)

cqlsh:Keyspace1> select count(*) from ""Standard1"";
TSocket read 0 bytes
cqlsh:Keyspace1> 
{noformat};;;","15/Nov/13 14:34;slebresne;Ok, this is due to the fact that SP.getRangeSlice might return more results than asked (due to reconciliation, you need > 1 node) which was confusing the pager logic. Attaching patch so that the pager trim the result in that case.;;;","22/Nov/13 01:07;aleksey;+1;;;","22/Nov/13 07:47;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LIMIT fetches one less than requested value,CASSANDRA-6330,12678626,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,BrandenVisser,BrandenVisser,11/Nov/13 18:28,16/Apr/19 09:32,14/Jul/23 05:53,12/Nov/13 18:13,1.2.12,,,,,,0,,,,,"Using Cassandra 1.2.11, the following sequence demonstrates the issue:

{code:sql}
CREATE TABLE blah (key text, column text, value text, PRIMARY KEY (key, column)) WITH COMPACT STORAGE;
INSERT INTO blah (key, column, value) VALUES ('a', 'a', 'a');
INSERT INTO blah (key, column, value) VALUES ('a', 'b', 'e');
INSERT INTO blah (key, column, value) VALUES ('a', 'c', 'e');
INSERT INTO blah (key, column, value) VALUES ('a', 'd', 'e');
INSERT INTO blah (key, column, value) VALUES ('a', 'e', 'e');
SELECT column FROM blah WHERE key = 'a' AND column < 'c' ORDER BY column DESC LIMIT 2;

 column
--------
      b
{code}

However I would expect columns b and a to both be returned. Only seems to be an issue if the range bound is an exact match, and only if ORDER BY column DESC is used.",,aleksey,BrandenVisser,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/13 08:31;slebresne;6330.txt;https://issues.apache.org/jira/secure/attachment/12613343/6330.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,357993,,,Tue Nov 12 18:13:18 UTC 2013,,,,,,,,,,"0|i1pppj:",358283,,,,,,,,,aleksey,,aleksey,Normal,,1.2.0,,,,,,,,,,,,,,,,"11/Nov/13 18:35;BrandenVisser;Should clarify, version is 1.2.11-SNAPSHOT as of this weekend.;;;","12/Nov/13 08:31;slebresne;This is indeed an oversight (that has been there basically forever). Attaching simple fix; I've pushed a dtests for it already.;;;","12/Nov/13 14:36;aleksey;+1;;;","12/Nov/13 18:13;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
example pig script doesn't run,CASSANDRA-6329,12678547,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,11/Nov/13 10:44,16/Apr/19 09:32,14/Jul/23 05:53,20/Nov/13 16:26,1.2.13,,,,,,0,pig,,,,"The following line in the examples/pig/example-script.pig will not run (using Pig 0.9.2):
{code}
state_footage = FOREACH state_grouped GENERATE GROUP AS State, SUM(state_flat.SquareFeet) AS TotalFeet:int;
{code}

It needs to have a lowercase 'group' because that's the variable name after doing the GROUP operation in the previous line:
{code}
state_footage = FOREACH state_grouped GENERATE group AS State, SUM(state_flat.SquareFeet) AS TotalFeet:int;
{code}

Also, I wonder if it would be good to separate the CassandraStorage and CqlStorage into two different scripts, just so you can run the CqlStorage example out of the box without having to comment out the CassandraStorage example.",,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/13 13:41;jeromatron;CASSANDRA-6329.txt;https://issues.apache.org/jira/secure/attachment/12613369/CASSANDRA-6329.txt",,,,,,,,,,,,,,,,,,,,1.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,357914,,,Wed Nov 20 16:26:16 UTC 2013,,,,,,,,,,"0|i1pp7z:",358204,1.2.11,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"11/Nov/13 17:00;brandon.williams;Care to submit a patch?;;;","12/Nov/13 13:27;jeromatron;Yep will do.  I just wanted to make a ticket so I didn't forget.;;;","12/Nov/13 13:40;jeromatron;Adding a patch for 1.2 branch.;;;","20/Nov/13 16:26;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"select with ""in"" clause wrongly returns empty result",CASSANDRA-6327,12678465,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,baldrick,baldrick,10/Nov/13 17:42,16/Apr/19 09:32,14/Jul/23 05:53,14/Nov/13 08:33,2.0.3,,,,,,0,,,,,"This query returns no result:

cqlsh:tick_data> select syd from current_prices where shard = 1 and syd in (1, 556129);

(0 rows)

However this query does return a result, showing that the previous query was wrong to return no result:

cqlsh:tick_data> select syd from current_prices where shard = 1 and syd in (556129);

 syd
--------
 556129

(1 rows)

This can be reproduced as follows:

(a) Create a keyspace tick_data:

create keyspace tick_data WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

(b) Create a table current_prices:

CREATE TABLE current_prices (
  shard int,
  syd int,
  ask decimal,
  bid decimal,
  currency ascii,
  when timeuuid,
  PRIMARY KEY (shard, syd)
);

(c) Stop Cassandra and untar the attached tar file in /var/lib/cassandra/data/tick_data/.  It populates the current_prices table.

(d) Restart Cassandra and perform the above selects.","Cassandra 2.0.2, x86-64 Ubuntu 13.10",aleksey,baldrick,cowardlydragon,marcuse,recastrodiaz,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Nov/13 20:40;marcuse;0001-check-if-any-of-the-slices-intersects.patch;https://issues.apache.org/jira/secure/attachment/12613679/0001-check-if-any-of-the-slices-intersects.patch","12/Nov/13 09:38;slebresne;6327.txt;https://issues.apache.org/jira/secure/attachment/12613348/6327.txt","10/Nov/13 17:44;baldrick;current_prices.tar;https://issues.apache.org/jira/secure/attachment/12613056/current_prices.tar",,,,,,,,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,357840,,,Mon Jun 09 13:06:21 UTC 2014,,,,,,,,,,"0|i1porj:",358130,,,,,,,,,aleksey,,aleksey,Normal,,2.0.0,,,,,,,,,,,,,,,,"10/Nov/13 17:44;baldrick;Untar this file to populate the current_prices table.;;;","12/Nov/13 09:38;slebresne;This is a bug in the column slice intersection logic from CASSANDRA-5514. More precisely, when there was multiple column slices, the logic was returning no intersection as soon as one of the slice was not intersecting, but it should do the exact reverse, it should consider it an intersection as one as any of the slice intersect but wait to have test all slices before saying it doesn't intersect.

Attaching simple patch to fix.;;;","12/Nov/13 09:38;slebresne;PS: I've pushed a dtests for this.;;;","12/Nov/13 14:49;aleksey;+1;;;","12/Nov/13 18:23;slebresne;Committed, thanks;;;","13/Nov/13 20:40;marcuse;the fix broke KeyspaceTest#testLimitSSTablesComposites

this patch tests all slices, but keeps old intersection logic - all components of the composite need to intersect;;;","13/Nov/13 21:08;mshuler;0001-check-if-any-of-the-slices-intersects.patch fixes KeyspaceTest for me on current cassandra-2.0 branch (1d3a4dd).  This is the only test I have run, at the moment :)

    [junit] Testsuite: org.apache.cassandra.db.KeyspaceTest
    [junit] Tests run: 12, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 23.567 sec;;;","13/Nov/13 21:23;aleksey;[~krummas] Oh. Right. Makes sense.Go ahead.;;;","14/Nov/13 08:33;slebresne;Correct (more precisely the new version is correct in term of behavior but not as efficient as can be in terms of avoiding sstables).
Committed a slight variation of Markus fixup: there is no need to continue testing a given slice when we know it doesn't intersect.;;;","25/Nov/13 20:38;cowardlydragon;Does this fix CASSANDRA-6137?

Nevermind, you put in a comment saying it probably was... 

I have an audit job looking for this, so if we see it come up, we'll let you know once we move prod to 2.0.3

Any idea why compaction run right after schema creation seemed to fix this?;;;","25/Nov/13 20:45;marcuse;there is logic in 2.0 to be able to skip sstables on the read path, a (major) compaction means there is only one sstable, and it wont be skipped since atleast one of slices will intersect it;;;","09/Jun/14 13:06;recastrodiaz;I noticed this bug on Cassandra version 1.X. Then we upgraded to 2.0.3 which solved this issue. It seems, however, to have come back in version 2.0.7.
Should I open a new issue?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError on startup reading saved Serializing row cache,CASSANDRA-6325,12678380,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,cburroughs,cburroughs,09/Nov/13 05:29,16/Apr/19 09:32,14/Jul/23 05:53,04/Feb/15 18:27,1.2.12,2.0.3,,,,,0,,,,,"I don't see any reason what this could have to do with the upgrade, but don't have a large enough non-prod cluster to just keep restarting on.  Occurred on roughly 2 out of 100 restarted nodes. 

{noformat}
ERROR [main] 2013-11-08 14:40:13,535 CassandraDaemon.java (line 482) Exception encountered during startup
java.lang.AssertionError
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:41)
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:37)
        at org.apache.cassandra.cache.SerializingCache.serialize(SerializingCache.java:118)
        at org.apache.cassandra.cache.SerializingCache.put(SerializingCache.java:176)
        at org.apache.cassandra.cache.InstrumentingCache.put(InstrumentingCache.java:44)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:156)
        at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:444)
        at org.apache.cassandra.db.Table.open(Table.java:114)
        at org.apache.cassandra.db.Table.open(Table.java:87)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:278)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:465)
{noformat}

I have the files if there is any useful analysis that can be run.  Looked 'normal' to a cursory `less` inspection.

Possibly related: CASSANDRA-4463",upgrade from 1.2.9ish to 1.2.11ish,cburroughs,mishail,ngrigoriev@gmail.com,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/13 16:10;jbellis;6325-v2.txt;https://issues.apache.org/jira/secure/attachment/12613172/6325-v2.txt","10/Nov/13 01:02;mishail;CASSANDRA-1.2-6325.patch;https://issues.apache.org/jira/secure/attachment/12613022/CASSANDRA-1.2-6325.patch",,,,,,,,,,,,,,,,,,,2.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,357755,,,Wed Feb 04 18:27:33 UTC 2015,,,,,,,,,,"0|i1po8v:",358045,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"10/Nov/13 01:02;mishail;_SerializingCache.serialize_ will return {{null}} for a {{null}} value;;;","11/Nov/13 16:10;jbellis;Hmm, that would work but since we're already using serialize() == null to mean ""out of memory"" I'd rather avoid using it to mean ""you passed a null value"" as well.

v2 attached to avoid passing a null value instead.;;;","12/Nov/13 14:57;jbellis;WDYT, Mikhail?;;;","12/Nov/13 17:49;mishail;+1;;;","12/Nov/13 21:51;jbellis;committed;;;","04/Feb/15 18:23;ngrigoriev@gmail.com;I have started seeing it recently. Not sure from which version but now it happens relatively often one some of my nodes.

{code}
 INFO [main] 2015-02-04 18:18:09,253 ColumnFamilyStore.java (line 249) Initializing duo_xxxxxxxxxxx
 INFO [main] 2015-02-04 18:18:09,254 AutoSavingCache.java (line 114) reading saved cache /var/lib/cassandra/saved_caches/duo_xxxxxxxxxxx-RowCach
e-b.db
ERROR [main] 2015-02-04 18:18:09,256 CassandraDaemon.java (line 513) Exception encountered during startup
java.lang.AssertionError
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:41)
        at org.apache.cassandra.cache.SerializingCacheProvider$RowCacheSerializer.serialize(SerializingCacheProvider.java:37)
        at org.apache.cassandra.cache.SerializingCache.serialize(SerializingCache.java:118)
        at org.apache.cassandra.cache.SerializingCache.put(SerializingCache.java:177)
        at org.apache.cassandra.cache.InstrumentingCache.put(InstrumentingCache.java:44)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:130)
        at org.apache.cassandra.db.ColumnFamilyStore.initRowCache(ColumnFamilyStore.java:592)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:119)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:92)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:305)
        at com.datastax.bdp.server.DseDaemon.setup(DseDaemon.java:419)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:496)
        at com.datastax.bdp.server.DseDaemon.main(DseDaemon.java:659)
 INFO [Thread-2] 2015-02-04 18:18:09,259 DseDaemon.java (line 505) DSE shutting down...
ERROR [Thread-2] 2015-02-04 18:18:09,279 CassandraDaemon.java (line 199) Exception in thread Thread[Thread-2,5,main]
java.lang.AssertionError
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1274)
        at com.datastax.bdp.gms.DseState.setActiveStatus(DseState.java:171)
        at com.datastax.bdp.server.DseDaemon.stop(DseDaemon.java:506)
        at com.datastax.bdp.server.DseDaemon$1.run(DseDaemon.java:408)
 INFO [main] 2015-02-04 18:18:49,144 CassandraDaemon.java (line 135) Logging initialized
 INFO [main] 2015-02-04 18:18:49,169 DseDaemon.java (line 382) DSE version: 4.6.0
{code}


Cassandra version: 2.0.11.83 (DSE 4.6.0);;;","04/Feb/15 18:27;brandon.williams;Please open in a new ticket instead of one that's over a year old.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"No ""echo off"" in cqlsh.bat",CASSANDRA-6324,12678346,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,mishail,mishail,08/Nov/13 23:33,16/Apr/19 09:32,14/Jul/23 05:53,11/Nov/13 17:56,1.2.12,,,Legacy/Tools,,,0,cqlsh,,,,https://github.com/apache/cassandra/commit/08a22729afedb53dfa988b3e2db34b7a743977de mistakenly deleted {{@echo off}} from {{cqlsh.bat}},,aleksey,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/13 23:37;mishail;CASSANDRA-1.2-6324.patch;https://issues.apache.org/jira/secure/attachment/12612929/CASSANDRA-1.2-6324.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,357721,,,Mon Nov 11 17:56:43 UTC 2013,,,,,,,,,,"0|i1po1b:",358011,,,,,,,,,brandon.williams,,brandon.williams,Low,,1.2.11,,,,,,,,,,,,,,,,"08/Nov/13 23:37;mishail;{{ECHO OFF}};;;","09/Nov/13 00:20;aleksey;(this should be ninja-d);;;","11/Nov/13 17:56;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Classcast Exception thrown when under load,CASSANDRA-6322,12678325,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,pookieman,pookieman,08/Nov/13 21:56,16/Apr/19 09:32,14/Jul/23 05:53,11/Nov/13 17:41,2.0.3,,,,,,1,,,,,"Saw this in the logs when running a load test:

ERROR [EXPIRING-MAP-REAPER:1] 2013-11-08 21:52:56,389 CassandraDaemon.java (line 187) Exception in thread Thread[EXPIRING-MAP-REAPER:1,5,main]
java.lang.ClassCastException: org.apache.cassandra.db.CounterMutation cannot be cast to org.apache.cassandra.db.RowMutation
	at org.apache.cassandra.net.MessagingService$5.apply(MessagingService.java:350)
	at org.apache.cassandra.net.MessagingService$5.apply(MessagingService.java:340)
	at org.apache.cassandra.utils.ExpiringMap$1.run(ExpiringMap.java:97)
	at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
","Linux Centos 64 bit, Sun JDK 1.7",aleksey,corwin,mishail,pookieman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/13 16:44;jbellis;6322.txt;https://issues.apache.org/jira/secure/attachment/12613176/6322.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,357700,,,Tue Nov 12 18:39:27 UTC 2013,,,,,,,,,,"0|i1pnwn:",357990,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"08/Nov/13 22:44;jbellis;Hmm, CounterMutations should not be marked with shouldHint.;;;","11/Nov/13 16:44;jbellis;Fix attached.;;;","11/Nov/13 16:56;aleksey;+1;;;","11/Nov/13 17:41;jbellis;committed;;;","12/Nov/13 18:39;pookieman;Great thanks.. any ideas when you'll be cutting 2.0.3?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh should handle 'null' as session duration,CASSANDRA-6317,12678204,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,exabytes18,exabytes18,08/Nov/13 11:36,16/Apr/19 09:32,14/Jul/23 05:53,09/Nov/13 00:08,1.2.12,2.0.3,,,,,0,,,,,"Mysteriously, tracing doesn't fail all the time. If I run the query multiple times at different consistency levels, tracing sometimes starts working.

{code}
cqlsh:some_keyspace> TRACING on;
cqlsh:some_keyspace> select * from appservers;

 key     | status
---------+--------
 server1 |      1
 server2 |      1
 server3 |      1

unsupported operand type(s) for /: 'NoneType' and 'float'
{code}",1.2.10,aleksey,exabytes18,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Nov/13 23:49;mishail;CASSANDRA-1.2-6317.patch;https://issues.apache.org/jira/secure/attachment/12612931/CASSANDRA-1.2-6317.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,357579,,,Sat Nov 09 00:08:57 UTC 2013,,,,,,,,,,"0|i1pn5r:",357869,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"08/Nov/13 13:20;jbellis;Sounds like cqlsh isn't handling an elapsed time of null/None which is what get logged when a replica responds after the result has been sent to the client.;;;","08/Nov/13 13:34;aleksey;bq. Sounds like cqlsh isn't handling an elapsed time of null/None which is what get logged when a replica responds after the result has been sent to the client.

Yup.;;;","08/Nov/13 23:49;mishail;Handle the case when {{duration == None}};;;","09/Nov/13 00:08;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json2sstable breaks on RangeTombstone,CASSANDRA-6316,12678197,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lyubent,exabytes18,exabytes18,08/Nov/13 11:08,16/Apr/19 09:32,14/Jul/23 05:53,28/Nov/13 02:14,1.2.13,2.0.3,,,,,0,,,,,"It seems that sstable2json writes out json which json2sstable has trouble reading.

{code}
java.lang.NumberFormatException: An hex string representing bytes must have an even length
	at org.apache.cassandra.utils.Hex.hexToBytes(Hex.java:52)
	at org.apache.cassandra.utils.ByteBufferUtil.hexToBytes(ByteBufferUtil.java:503)
	at org.apache.cassandra.tools.SSTableImport.stringAsType(SSTableImport.java:572)
	at org.apache.cassandra.tools.SSTableImport.access$000(SSTableImport.java:66)
	at org.apache.cassandra.tools.SSTableImport$JsonColumn.<init>(SSTableImport.java:158)
	at org.apache.cassandra.tools.SSTableImport.addColumnsToCF(SSTableImport.java:229)
	at org.apache.cassandra.tools.SSTableImport.addToStandardCF(SSTableImport.java:212)
	at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:361)
	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:318)
	at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:537)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
ERROR: An hex string representing bytes must have an even length
12345:2:!
{code}",1.2.10,aleksey,exabytes18,lyubent,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Nov/13 15:38;lyubent;6316.diff;https://issues.apache.org/jira/secure/attachment/12613167/6316.diff","14/Nov/13 03:24;lyubent;6316_cassandra-2.0.diff;https://issues.apache.org/jira/secure/attachment/12613776/6316_cassandra-2.0.diff",,,,,,,,,,,,,,,,,,,2.0,lyubent,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,357572,,,Thu Nov 28 02:14:40 UTC 2013,,,,,,,,,,"0|i1pn47:",357862,1.2.11,2.0.2,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"08/Nov/13 14:06;lyubent;[~exabytes18] Could you please post the schema of the cf you are trying to export/import? ;;;","08/Nov/13 19:38;exabytes18;Pretty trivial schema:

{code}
cqlsh> CREATE KEYSPACE test_range_tombstones WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};
cqlsh> use test_range_tombstones;
cqlsh:test_range_tombstones> CREATE TABLE tbl (
                         ...     a INT,
                         ...     b INT,
                         ...     c INT,
                         ...     d TIMESTAMP,
                         ...     e BOOLEAN,
                         ...     PRIMARY KEY(a, b, c)
                         ... ) WITH compaction = { 'class' : 'LeveledCompactionStrategy' };
cqlsh:test_range_tombstones> insert into tbl (a, b, c, d, e) values (1, 2, 3, 4, false);
cqlsh:test_range_tombstones> select * from tbl;

 a | b | c | d                        | e
---+---+---+--------------------------+-------
 1 | 2 | 3 | 1969-12-31 16:00:00-0800 | False

cqlsh:test_range_tombstones> delete from tbl where a = 5 and b = 2;
{code};;;","11/Nov/13 15:37;lyubent;When SSTableImport#JsonColumn tries to find the comparator for the tombstoned range it fails and defaults to a BytesType comparator. I special-cased the tombstoned columns to get their comparator from the 1st component of the cf. [Gist of the schemas used for verification|https://gist.github.com/lyubent/7415048].;;;","12/Nov/13 17:44;aleksey;[~lyubent] Could you attach a patch for 2.0? Keep in mind that CASSANDRA-5435 exists.;;;","14/Nov/13 03:24;lyubent;[~iamaleksey] This patch is fundamentally the same (maybe I'm missing something) but as far as my testing shows it works for thrift and cql in C* 2.0. ;;;","19/Nov/13 22:24;jbellis;So... whose ball is this? :);;;","19/Nov/13 22:30;aleksey;Mine, although Sylvain has already merged 1.2 into 2.0 and trunk some time ago.;;;","28/Nov/13 02:13;aleksey;Oh. My bad.

The original 1.2 patch was incorrect - we cannot/should not be getting comparator from the first component b/c
a) It does not import range tombstones with 2+ components correctly and
b) there is no need to, we already know the comparator from the metadata, and simple comparator.fromString() is enough.

Backported Sylvain's merge-commit version from 2.0 instead.;;;","28/Nov/13 02:14;aleksey;(69c1ee96025dd35ed37bce2a2ccc0c2ca5a3dfed);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig CqlStorage generates  ERROR 1108: Duplicate schema alias,CASSANDRA-6309,12677888,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,thunderstumpges,thunderstumpges,06/Nov/13 23:15,16/Apr/19 09:32,14/Jul/23 05:53,13/Dec/13 17:42,1.2.13,2.0.4,,,,,1,,,,,"In Pig after loading a simple CQL3 table from Cassandra 2.0.1, and dumping contents, I receive:
Caused by: org.apache.pig.impl.plan.PlanValidationException: ERROR 1108: Duplicate schema alias: author in ""cm""

 cm = load 'cql://thunder_test/cassandra_messages' USING CqlStorage;
 dump cm
ERROR org.apache.pig.tools.grunt.Grunt - org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias cm
...
Caused by: org.apache.pig.impl.plan.PlanValidationException: ERROR 1108: Duplicate schema alias: author in ""cm""
        at org.apache.pig.newplan.logical.visitor.SchemaAliasVisitor.validate(SchemaAliasVisitor.java:75)


running 'describe cm' gives:
cm: {message_id: chararray,author: chararray,author: chararray,body: chararray,message_id: chararray}

The original table schema in Cassandra is:
CREATE TABLE cassandra_messages (
  message_id text,
  author text,
  body text,
  PRIMARY KEY (message_id, author)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='null' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};

it appears that the code in CqlStorage.getColumnMetadata at ~line 478 takes the ""keys"" columns (in my case, message_id and author) and appends the columns from getColumnMeta (which has all three columns). Thus the keys columns are duplicated.

",,alexliu68,thunderstumpges,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Nov/13 22:07;alexliu68;6309-2.0.txt;https://issues.apache.org/jira/secure/attachment/12612718/6309-2.0.txt","05/Dec/13 21:58;alexliu68;6309-fix-pig-test-compiling.txt;https://issues.apache.org/jira/secure/attachment/12617246/6309-fix-pig-test-compiling.txt","11/Dec/13 05:27;alexliu68;6309-trunk-branch.txt;https://issues.apache.org/jira/secure/attachment/12618187/6309-trunk-branch.txt","22/Nov/13 22:20;alexliu68;6309-v2-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12615395/6309-v2-2.0-branch.txt","05/Dec/13 21:20;alexliu68;6309-v3.txt;https://issues.apache.org/jira/secure/attachment/12617243/6309-v3.txt","22/Nov/13 22:20;alexliu68;LOCAL_ONE-write-for-all-strategies-v2.txt;https://issues.apache.org/jira/secure/attachment/12615396/LOCAL_ONE-write-for-all-strategies-v2.txt","07/Nov/13 22:07;alexliu68;LOCAL_ONE-write-for-all-strategies.txt;https://issues.apache.org/jira/secure/attachment/12612719/LOCAL_ONE-write-for-all-strategies.txt",,,,,,,,,,,,,,7.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,357263,,,Fri Dec 13 17:42:47 UTC 2013,,,,,,,,,,"0|i1pl7j:",357553,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"06/Nov/13 23:52;thunderstumpges;just as a note, I'm using code from the tip of the cassandra-2.0 branch. I'm a little confused because the code which combines the list from ""getKeysMeta"" with that of ""getColumnMeta"" has been around a while. There was some refactoring of getKeysMeta recently, but not sure how that could have changed things. 

I have a naive patch which simply loops the columns in getColumnMeta and only adds them to ""keys"" collection if they don't already exist. Can add that as a patch if y'all think that's an appropriate way to solve this thing.
 ;;;","07/Nov/13 00:04;alexliu68;It works in 1.2 branch. It looks like some changes to table metadata implementation causes the issue.;;;","07/Nov/13 19:40;alexliu68;The partition and cluster keys are added to system.schema_columns table, so to do a quick fix, we just ignore those columns as for now. When all other system_schema_* changes are done in the later release, we will make the final changes accordingly.;;;","07/Nov/13 20:26;alexliu68;Some thrift column families tests are broken too

{code}
    [junit] Testcase: testCassandraStorageFullCopy(org.apache.cassandra.pig.ThriftColumnFamilyTest):	Caused an ERROR
    [junit] null
    [junit] NotFoundException()
    [junit] 	at org.apache.cassandra.thrift.Cassandra$get_result$get_resultStandardScheme.read(Cassandra.java:10059)
    [junit] 	at org.apache.cassandra.thrift.Cassandra$get_result$get_resultStandardScheme.read(Cassandra.java:1)
    [junit] 	at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:9942)
    [junit] 	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
    [junit] 	at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:615)
    [junit] 	at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:600)
    [junit] 	at org.apache.cassandra.pig.ThriftColumnFamilyTest.getColumnValue(ThriftColumnFamilyTest.java:808)
    [junit] 	at org.apache.cassandra.pig.ThriftColumnFamilyTest.testCassandraStorageFullCopy(ThriftColumnFamilyTest.java:392)
    [junit] 
{code};;;","07/Nov/13 21:50;alexliu68;This turns out to be written path needs ""LOCAL_ONE"" be supported by Simple Strategy;;;","07/Nov/13 22:08;alexliu68;If we can't get the patch for LOCAL_ONE write path in this ticket, I can post it to CASSANDRA-6238;;;","07/Nov/13 23:48;thunderstumpges;Thanks for the quick turn-around Alex! I appreciate it. The patch looks good to me. better than what I hacked together locally. And the patch for LOCAL_ONE I haven't run into because we're running 2.0.1 on our servers still, so I had to override my consistency for reads and writes to ONE anyway.

thanks again!
Thunder;;;","22/Nov/13 20:35;brandon.williams;Patch fixes the duplicate schema problem, but the thrift test fails.;;;","22/Nov/13 22:24;alexliu68;V2 patch is attached, which fix the unit tests.;;;","22/Nov/13 22:37;brandon.williams;v2 removes all the license headers from the new tests, can you rebase it?;;;","22/Nov/13 23:31;alexliu68;https://github.com/apache/cassandra/blob/cassandra-2.0/test/unit/org/apache/cassandra/pig/ThriftColumnFamilyDataTypeTest.java#L20

https://github.com/apache/cassandra/blob/cassandra-2.0/test/unit/org/apache/cassandra/pig/CqlTableDataTypeTest.java#L20

The patch removes the duplicate license header.;;;","05/Dec/13 21:20;alexliu68;V3 patch is based on the latest 2.0 branch;;;","05/Dec/13 21:59;alexliu68;6309-fix-pig-test-compiling.txt fixes the build issue for pig test

;;;","06/Dec/13 00:29;alexliu68;Trunk bumped Pig version to 0.11.0, the pig unit tests fail

{code}
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testCqlStorageListType(org.apache.cassandra.pig.CqlTableDataTypeTest):	Caused an ERROR
    [junit] Unable to open iterator for alias list_rows
    [junit] org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias list_rows
    [junit] 	at org.apache.pig.PigServer.openIterator(PigServer.java:836)
    [junit] 	at org.apache.cassandra.pig.CqlTableDataTypeTest.testCqlStorageListType(CqlTableDataTypeTest.java:332)
    [junit] Caused by: org.apache.pig.PigException: ERROR 1002: Unable to store alias list_rows
    [junit] 	at org.apache.pig.PigServer.storeEx(PigServer.java:935)
    [junit] 	at org.apache.pig.PigServer.store(PigServer.java:898)
    [junit] 	at org.apache.pig.PigServer.openIterator(PigServer.java:811)
    [junit] Caused by: org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobCreationException: ERROR 2017: Internal error creating job configuration.
    [junit] 	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:848)
    [junit] 	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.compile(JobControlCompiler.java:294)
    [junit] 	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:177)
    [junit] 	at org.apache.pig.PigServer.launchPlan(PigServer.java:1264)
    [junit] 	at org.apache.pig.PigServer.executeCompiledLogicalPlan(PigServer.java:1249)
    [junit] 	at org.apache.pig.PigServer.storeEx(PigServer.java:931)
    [junit] Caused by: java.io.IOException: Serialization error: org.apache.log4j.Level
    [junit] 	at org.apache.pig.impl.util.ObjectSerializer.serialize(ObjectSerializer.java:47)
    [junit] 	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.JobControlCompiler.getJob(JobControlCompiler.java:526)
    [junit] Caused by: java.io.NotSerializableException: org.apache.log4j.Level
    [junit] 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1180)
    [junit] 	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1528)
    [junit] 	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1493)
    [junit] 	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1416)
    [junit] 	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1174)
    [junit] 	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:346)
    [junit] 	at org.apache.pig.impl.util.ObjectSerializer.serialize(ObjectSerializer.java:43)
{code};;;","10/Dec/13 23:38;alexliu68;It looks like logback replacing log4j causes the issue. I updated Pig to 0.11.1 on cassandra-2.0 branch and the pig unit tests passed.;;;","11/Dec/13 03:55;alexliu68;6309-trunk-branch.txt is on tunk, which changes the build.xml to only add runtime log4j lib to pig-test, other tests stay with logback.;;;","13/Dec/13 17:42;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thread leak caused in creating OutboundTcpConnectionPool,CASSANDRA-6308,12677834,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,timiblossom,timiblossom,timiblossom,06/Nov/13 18:32,16/Apr/19 09:32,14/Jul/23 05:53,06/Nov/13 20:34,1.2.12,2.0.3,,,,,0,leak,thread,,,"We have seen in one of our large clusters that there are many OutboundTcpConnection threads having the same names.  From a thread dump, OutboundTcpConnection threads have accounted for the largest shares of the total threads (65%+) and kept growing.

Here is a portion of a grep output for threads in which names start with ""WRITE-"":

""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00002aaac4022000 nid=0x2cb5 waiting on condition [0x00002acfbacda000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00002aaac42fe000 nid=0x2cb4 waiting on condition [0x00002acfbacad000]
""WRITE-/10.30.142.49"" daemon prio=10 tid=0x0000000040840000 nid=0x2cb1 waiting on condition [0x00002acfbac80000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004083e000 nid=0x2cb0 waiting on condition [0x00002acfbac53000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004083b800 nid=0x2caf waiting on condition [0x00002acfbac26000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040839800 nid=0x2cae waiting on condition [0x00002acfbabf9000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040837800 nid=0x2cad waiting on condition [0x00002acfbabcc000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000404a3800 nid=0x2cac waiting on condition [0x00002acfbab9f000]
""WRITE-/10.30.142.49"" daemon prio=10 tid=0x00000000404a1800 nid=0x2cab waiting on condition [0x00002acfbab72000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004049f800 nid=0x2caa waiting on condition [0x00002acfbab45000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004049e000 nid=0x2ca9 waiting on condition [0x00002acfbab18000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004049c800 nid=0x2ca8 waiting on condition [0x00002acfbaaeb000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x000000004049a800 nid=0x2ca7 waiting on condition [0x00002acfbaabe000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040498800 nid=0x2ca6 waiting on condition [0x00002acfbaa91000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040496800 nid=0x2ca5 waiting on condition [0x00002acfbaa64000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040717800 nid=0x2ca4 waiting on condition [0x00002acfbaa37000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040716000 nid=0x2ca3 waiting on condition [0x00002acfbaa0a000]
""WRITE-/10.30.146.195"" daemon prio=10 tid=0x0000000040714800 nid=0x2ca2 waiting on condition [0x00002acfba9dd000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040712800 nid=0x2ca1 waiting on condition [0x00002acfba9b0000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040710800 nid=0x2ca0 waiting on condition [0x00002acfba983000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004070e800 nid=0x2c9f waiting on condition [0x00002acfba956000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004070d000 nid=0x2c9e waiting on condition [0x00002acfba929000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004070b800 nid=0x2c9d waiting on condition [0x00002acfba8fc000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004070a000 nid=0x2c9c waiting on condition [0x00002acfba8cf000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040827000 nid=0x2c9b waiting on condition [0x00002acfba8a2000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040825000 nid=0x2c9a waiting on condition [0x00002acfba875000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac488e000 nid=0x2c99 waiting on condition [0x00002acfba848000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040823000 nid=0x2c98 waiting on condition [0x00002acfba81b000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040821800 nid=0x2c97 waiting on condition [0x00002acfba7ee000]
""WRITE-/10.30.146.195"" daemon prio=10 tid=0x000000004081f000 nid=0x2c96 waiting on condition [0x00002acfba7c1000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004081d000 nid=0x2c95 waiting on condition [0x00002acfba794000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004081b000 nid=0x2c94 waiting on condition [0x00002acfba767000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac488b000 nid=0x2c93 waiting on condition [0x00002acfba73a000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040819000 nid=0x2c92 waiting on condition [0x00002acfba70d000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f9000 nid=0x2c91 waiting on condition [0x00002acfba6e0000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f7000 nid=0x2c90 waiting on condition [0x00002acfba6b3000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f5000 nid=0x2c8f waiting on condition [0x00002acfba686000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f3000 nid=0x2c8d waiting on condition [0x00002acfba659000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407f1800 nid=0x2c8c waiting on condition [0x00002acfba62c000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000407ef000 nid=0x2c8b waiting on condition [0x00002acfba5ff000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000407ed800 nid=0x2c8a waiting on condition [0x00002acfba5d2000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00000000407ec000 nid=0x2c89 waiting on condition [0x00002acfba5a5000]
""WRITE-/10.30.161.144"" daemon prio=10 tid=0x00000000407e9800 nid=0x2c88 waiting on condition [0x00002acfba578000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405f5000 nid=0x2c87 waiting on condition [0x00002acfba54b000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405f3000 nid=0x2c86 waiting on condition [0x00002acfba51e000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405f1000 nid=0x2c85 waiting on condition [0x00002acfba4f1000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405ef000 nid=0x2c83 waiting on condition [0x00002acfba4c4000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405ed800 nid=0x2c82 waiting on condition [0x00002acfba497000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405eb800 nid=0x2c81 waiting on condition [0x00002acfba46a000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405ea000 nid=0x2c80 waiting on condition [0x00002acfba43d000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405e8800 nid=0x2c7f waiting on condition [0x00002acfba40f000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405e7800 nid=0x2c7e waiting on condition [0x00002acfba3e2000]
""WRITE-/10.30.161.144"" daemon prio=10 tid=0x0000000040607000 nid=0x2c7d waiting on condition [0x00002acfba3b5000]
""WRITE-/10.30.161.144"" daemon prio=10 tid=0x0000000040605800 nid=0x2c7c waiting on condition [0x00002acfba388000]
""WRITE-/10.30.142.49"" daemon prio=10 tid=0x0000000040604000 nid=0x2c7b waiting on condition [0x00002acfba35b000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040602000 nid=0x2c7a waiting on condition [0x00002acfba32e000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405ff800 nid=0x2c79 waiting on condition [0x00002acfba301000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405fe000 nid=0x2c78 waiting on condition [0x00002acfba2d4000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405fc000 nid=0x2c77 waiting on condition [0x00002acfba2a7000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x00000000405fa800 nid=0x2c75 waiting on condition [0x00002acfba27a000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040af9800 nid=0x2c74 waiting on condition [0x00002acfba24d000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040af8000 nid=0x2c73 waiting on condition [0x00002acfba220000]
""WRITE-/10.30.161.144"" daemon prio=10 tid=0x0000000040af6000 nid=0x2c72 waiting on condition [0x00002acfba1f3000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x0000000040af4000 nid=0x2c71 waiting on condition [0x00002acfba1c6000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040af2000 nid=0x2c70 waiting on condition [0x00002acfba199000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040af0800 nid=0x2c6f waiting on condition [0x00002acfba16c000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040aef000 nid=0x2c6e waiting on condition [0x00002acfba13f000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040aed000 nid=0x2c6d waiting on condition [0x00002acfba112000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040aeb800 nid=0x2c6b waiting on condition [0x00002acfba0b8000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac46b9000 nid=0x2c6a waiting on condition [0x00002acfba08b000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407b3000 nid=0x2c69 waiting on condition [0x00002acfba05e000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407b1800 nid=0x2c68 waiting on condition [0x00002acfba031000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407af800 nid=0x2c66 waiting on condition [0x00002acfba004000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407ae000 nid=0x2c65 waiting on condition [0x00002acfb9fd7000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407ab800 nid=0x2c64 waiting on condition [0x00002acfb9faa000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407a9800 nid=0x2c63 waiting on condition [0x00002acfb9f7d000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407a8000 nid=0x2c62 waiting on condition [0x00002acfb9f50000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407a6800 nid=0x2c61 waiting on condition [0x00002acfb9f23000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408d2800 nid=0x2c60 waiting on condition [0x00002acfb9ef6000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408d1000 nid=0x2c5f waiting on condition [0x00002acfb9ec9000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408cf800 nid=0x2c5d waiting on condition [0x00002acfb9e9c000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408cd800 nid=0x2c5c waiting on condition [0x00002acfb9e6f000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000408cc000 nid=0x2c5b waiting on condition [0x00002acfb9e42000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004088d800 nid=0x2c5a waiting on condition [0x00002acfb9e15000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004088b000 nid=0x2c59 waiting on condition [0x00002acfb9de8000]
""WRITE-/10.157.10.134"" daemon prio=10 tid=0x0000000040889000 nid=0x2c58 waiting on condition [0x00002acfb9dbb000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040887800 nid=0x2c57 waiting on condition [0x00002acfb9d8e000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x0000000040720000 nid=0x2c56 waiting on condition [0x00002acfb9d61000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x000000004071f000 nid=0x2c55 waiting on condition [0x00002acfb9d34000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000407c3000 nid=0x2c54 waiting on condition [0x00002acfb9d07000]
""WRITE-/10.78.95.30"" daemon prio=10 tid=0x00000000407c1800 nid=0x2c53 waiting on condition [0x00002acfb9cda000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00000000407c0000 nid=0x2c52 waiting on condition [0x00002acfb9cac000]
""WRITE-/10.28.131.195"" daemon prio=10 tid=0x00000000407be000 nid=0x2c51 waiting on condition [0x00002acfb9c7f000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000405cc000 nid=0x2c50 waiting on condition [0x00002acfb9c52000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000405ca800 nid=0x2c4f waiting on condition [0x00002acfb9c24000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000405c8800 nid=0x2c4e waiting on condition [0x00002acfb9bf7000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00000000405c6800 nid=0x2c4d waiting on condition [0x00002acfb9bca000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac5010800 nid=0x2c4c waiting on condition [0x00002acfb9b9c000]
""WRITE-/10.6.222.233"" daemon prio=10 tid=0x00002aaac4cd9800 nid=0x2c4b waiting on condition [0x00002acfb9b6f000]
""WRITE-/10.11.15.209"" daemon prio=10 tid=0x0000000040756800 nid=0x2c4a waiting on condition [0x00002acfb9b42000]
""WRITE-/10.11.15.209"" daemon prio=10 tid=0x0000000040754800 nid=0x2c49 waiting on condition [0x00002acfb9b15000]

 

We have patched this https://issues.apache.org/jira/browse/CASSANDRA-5175 but I don't this fix solves the issue totally.  I will attach  a patch soon. 

",,timiblossom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Nov/13 20:26;timiblossom;patch.txt;https://issues.apache.org/jira/secure/attachment/12612434/patch.txt",,,,,,,,,,,,,,,,,,,,1.0,timiblossom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,357209,,,Wed Nov 06 20:34:58 UTC 2013,,,,,,,,,,"0|i1pkvj:",357499,1.2.10,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"06/Nov/13 20:34;jbellis;LGTM; committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
make CqlPagingRecordReader more robust to failures,CASSANDRA-6302,12677638,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,05/Nov/13 20:22,16/Apr/19 09:32,14/Jul/23 05:53,05/Nov/13 21:29,1.2.12,2.0.3,,,,,0,,,,,"CPPR currently bails if the first location fails for any reason, and generates invalid CQL if only the row key is specified in the column list.",,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/13 20:23;brandon.williams;0001-Try-connecting-to-more-than-the-first-location.txt;https://issues.apache.org/jira/secure/attachment/12612241/0001-Try-connecting-to-more-than-the-first-location.txt","05/Nov/13 20:23;brandon.williams;0002-avoid-generating-broken-cql-when-only-the-row-key-is-s.txt;https://issues.apache.org/jira/secure/attachment/12612242/0002-avoid-generating-broken-cql-when-only-the-row-key-is-s.txt",,,,,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,357013,,,Tue Nov 05 21:29:56 UTC 2013,,,,,,,,,,"0|i1pjo7:",357303,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"05/Nov/13 21:19;jbellis;LGTM modulo brace-on-newline.;;;","05/Nov/13 21:29;brandon.williams;Committed with brace fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool fails with java.lang.UnsatisfiedLinkError (cannot find libjemalloc.so) when JEMalloc is configured,CASSANDRA-6301,12677619,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,ngrigoriev,ngrigoriev,05/Nov/13 18:30,16/Apr/19 09:32,14/Jul/23 05:53,06/Nov/13 00:56,2.0.3,,,Tool/nodetool,,,0,,,,,"{code}
>/opt/apache-cassandra-2.0.2/bin/nodetool ring
Note: Ownership information does not include topology; for complete information, specify a keyspace

Datacenter: DC1
==========
Address      Rack        Status State   Load            Owns                Token
                                                                            9208241795664305161
10.3.45.160  r1          Up     Normal  125.88 GB       16.41%              -9222548266947385654
10.3.45.160  r1          Up     Normal  125.88 GB       16.41%              -9177629719965963707
10.3.45.160  r1          Up     Normal  125.88 GB       16.41%              -9039272433194428886
10.3.45.160  r1          Up     Normal  125.88 GB       16.41%              -9037742357058937987
...
Exception in thread ""main"" java.lang.UnsatisfiedLinkError: Unable to load library 'jemalloc': libjemalloc.so: cannot open shared object file: No such file or directory
        at com.sun.jna.NativeLibrary.loadLibrary(NativeLibrary.java:164)
        at com.sun.jna.NativeLibrary.getInstance(NativeLibrary.java:237)
        at com.sun.jna.Library$Handler.<init>(Library.java:140)
        at com.sun.jna.Native.loadLibrary(Native.java:375)
        at com.sun.jna.Native.loadLibrary(Native.java:359)
        at org.apache.cassandra.io.util.JEMallocAllocator.<init>(JEMallocAllocator.java:36)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:526)
        at java.lang.Class.newInstance(Class.java:374)
        at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:488)
        at org.apache.cassandra.utils.FBUtilities.newOffHeapAllocator(FBUtilities.java:438)
        at org.apache.cassandra.config.DatabaseDescriptor.applyConfig(DatabaseDescriptor.java:442)
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:104)
        at org.apache.cassandra.tools.NodeCmd.printRing(NodeCmd.java:286)
        at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1092)
{code}

In my conf/cassandra-env.sh I have:

{code}
# Configure the following for JEMallocAllocator and if jemalloc is not available in the system
# library path (Example: /usr/local/lib/). Usually ""make install"" will do the right thing.
export LD_LIBRARY_PATH=/usr/local/lib
JVM_OPTS=""$JVM_OPTS -Djava.library.path=/usr/local/lib/""
{code}

I believe this file is not sourced by the tools, this is why a tool that might need that library cannot find it.
","Linux
Cassandra 2.0.2
libjemalloc.so in /usr/local/lib",ngrigoriev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6273,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356994,,,Wed Nov 06 00:55:08 UTC 2013,,,,,,,,,,"0|i1pjk7:",357284,2.0.2,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"05/Nov/13 19:07;brandon.williams;It is sourced by the tools, however the don't use JVM_OPTS (and it probably doesn't make sense to.)  I don't think we need a fancy allocator for any of the tools.;;;","05/Nov/13 20:27;ngrigoriev;Hmmm...I do not think it is sourced by nodetool.

{code}
/opt/apache-cassandra-2.0.2 >grep cassandra-env.sh bin/* conf/*
bin/cassandra:if [ -f ""$CASSANDRA_CONF/cassandra-env.sh"" ]; then
bin/cassandra:    . ""$CASSANDRA_CONF/cassandra-env.sh""
bin/debug-cql:if [ -f ""$CASSANDRA_CONF/cassandra-env.sh"" ]; then
bin/debug-cql:    . ""$CASSANDRA_CONF/cassandra-env.sh""
conf/cassandra-env.sh:        echo ""please set or unset MAX_HEAP_SIZE and HEAP_NEWSIZE in pairs (see cassandra-env.sh)""
conf/cassandra-topology.properties:# in cassandra-env.sh
conf/cassandra.yaml:# modify cassandra-env.sh as directed in the file.
{code}

I see that nodetool invokes JAVA directly, so it is sourced only by cassandra itself and debug-cql. And it seems to me that sourcing cassandra-env.sh from the tools would not be appropriate - it contains a number of settings that are server-specific all packed in JVM_OPTS. Or I am missing something...

The allocator is probably not needed, but, in general, a tool might need a native library that is used by the server too. Maybe I am using wrong example but something like native snappy library...so probably it may make sense to use the same set of native libraries for the tools as for the server itself. Even if not really needed now this may be more useful in the future. Just my two cents.;;;","05/Nov/13 20:45;brandon.williams;Actually it does after CASSANDRA-6273, so you might see if that patch helps.;;;","06/Nov/13 00:55;ngrigoriev;[~brandon.williams]

Indeed, your fix indirectly fixes this problem because by sourcing cassandra-end.sh it effectively does this:

{code}
export LD_LIBRARY_PATH=/usr/local/lib
{code}

I have applied your patch to test it to confirm that it does help. I took the liberty of closing this issue and linking it to CASSANDRA-6273. Thanks!
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Serialization bug in PagedRangeCommand,CASSANDRA-6299,12677534,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,05/Nov/13 09:34,16/Apr/19 09:32,14/Jul/23 05:53,05/Nov/13 15:35,2.0.3,,,,,,0,,,,,,,aleksey,incubos,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/13 09:40;slebresne;6299.txt;https://issues.apache.org/jira/secure/attachment/12612141/6299.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356909,,,Tue Nov 05 15:35:38 UTC 2013,,,,,,,,,,"0|i1pj1b:",357199,,,,,,,,,aleksey,,aleksey,Normal,,2.0.0,,,,,,,,,,,,,,,,"05/Nov/13 09:40;slebresne;There is a typo in the serialization of PagedRangeCommand so that if it has index expressions, it serializes their value with a 4-bytes length but read them with short 2-bytes length. In practice, this ends up breaking the underlying connection and results in query timeouts as described in [this thread|https://groups.google.com/a/lists.datastax.com/d/msg/java-driver-user/ao1ohSLpjRM/4rfCNk1OUM0J].

Patch attached. The patch also fixes a few problem in the serializedSize() method (though I don't think it's used in practice).
;;;","05/Nov/13 13:13;aleksey;+1;;;","05/Nov/13 15:35;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper blocks when updating tokens and turns node down,CASSANDRA-6297,12677445,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,sbtourist,sbtourist,04/Nov/13 20:30,16/Apr/19 09:32,14/Jul/23 05:53,04/Nov/13 21:26,1.2.12,2.0.3,,,,,0,,,,,"The GossipStage call to SystemTable.updateTokens causes a blocking memtable flush that may get stuck in the postFlushExecutor queue while waiting for other memtables to flush; as a consequence, the Gossiper itself ""blocks"" and the node is turned down.",,jeromatron,rcoli,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6127,,,,,,,,,,,,,,,"04/Nov/13 20:51;jbellis;6297-v2.txt;https://issues.apache.org/jira/secure/attachment/12612025/6297-v2.txt","04/Nov/13 20:48;jbellis;6297.txt;https://issues.apache.org/jira/secure/attachment/12612024/6297.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356820,,,Mon Nov 04 21:26:53 UTC 2013,,,,,,,,,,"0|i1pihj:",357110,1.2.11,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"04/Nov/13 20:48;jbellis;Thinking it through, the simplest solution is to just remove the blocking flush.  This does mean that we have a longer potential window of non-durability for the peer information under Periodic CommitLog, but this does not make things qualitatively worse -- e.g., if we were down entirely during the node addition we would also have to deal with not having the peer information on restart.

I see alternatives to removing the blocking flush as falling into two categories:
# semantically equivalent solutions with more complex implementations (e.g. moving updateTokens into another thread or executor)
# dramatically complex gymnastics that aren't worth the small extra benefit, such as adding a special commitlog sync instead of the blocking flush;;;","04/Nov/13 20:51;jbellis;v2 removes some other blocking flushes that are not critical;;;","04/Nov/13 21:05;brandon.williams;+1;;;","04/Nov/13 21:25;sbtourist;Works for me.;;;","04/Nov/13 21:26;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Murmur3Partitioner doesn't yield proper ownership calculation,CASSANDRA-6289,12677192,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,cywjackson,cywjackson,02/Nov/13 01:47,16/Apr/19 09:32,14/Jul/23 05:53,21/Nov/13 20:19,2.0.4,,,Legacy/Tools,,,0,,,,,"In a new 1.2 install with Murmur3 as default, I setup a test cluster with N=RF=3 for the cluster size and RF for a keyspace

but when I look at the ring output (with the keyspace name), to my surprise it shows RF=2.

Further investigate shows the ""total replica"" is an addition of the float value from the effectiveOwnership. But that results in < 1 for the setup:
{panel}
#bean is set to org.apache.cassandra.db:type=StorageService
$>run effectiveOwnership Keyspace1
#calling operation effectiveOwnership of mbean org.apache.cassandra.db:type=StorageService
#operation returns: 
\{ 
  /127.0.0.1 = 0.9999989;
  /127.0.0.2 = 0.9999989;
  /127.0.0.3 = 0.9999989;
 \}
{panel}

{panel}
$ ./bin/nodetool -h 0 -p 7100 ring Keyspace1

Datacenter: datacenter1
==========
Replicas: 2

Address    Rack        Status State   Load            Owns                Token                                       
                                                                          3074457345618258602                         
127.0.0.1  rack1       Up     Normal  1.02 GB         100.00%             -9223372036854775808                        
127.0.0.2  rack1       Up     Normal  996.38 MB       100.00%             -3074457345618258603                        
127.0.0.3  rack1       Up     Normal  980.55 MB       100.00%             3074457345618258602 
{panel}

{panel}
Keyspace: Keyspace1:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:3]
{panel}
The println would simply class the float value to int, so i guess that's round down.

When using RandomPartitioner, the effectiveOwnership will return 1.0 

So I guess the real question is, is the Murmur3 calculation correct? Or is it losing precision? If it is correct, then I guess we need to force the float -> int to round up? (is that even the right thing to do?)",,cburroughs,cywjackson,jeromatron,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Nov/13 14:45;jbellis;6289-v2.txt;https://issues.apache.org/jira/secure/attachment/12613374/6289-v2.txt","12/Nov/13 05:46;mishail;cassandra-1.2-6289.patch;https://issues.apache.org/jira/secure/attachment/12613324/cassandra-1.2-6289.patch",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356567,,,Thu Nov 21 20:19:03 UTC 2013,,,,,,,,,,"0|i1pgx3:",356855,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"12/Nov/13 05:46;mishail;Use {{Math.round}} instead of truncating;;;","12/Nov/13 13:31;jeromatron;Might this be a similar problem in RandomPartitioner?  Just wondering if CASSANDRA-6320 might have a similar fix.;;;","12/Nov/13 14:26;jbellis;6320 is about ownership changes which is not the case here.;;;","12/Nov/13 14:45;jbellis;I think the real problem is that displaying replica count derived from sum(ownership) is fundamentally broken.  Consider using SimpleStrategy instead of NTS -- you'll just get nonsense.

I think the right solution is to rip this out instead of trying to compensate for one inaccuracy by applying more on an ad hoc basis until it looks right in some cases.  But, I don't want to break anyone parsing nodetool output in 1.2.x.  Patch attached for 2.0.;;;","21/Nov/13 20:15;brandon.williams;+1;;;","21/Nov/13 20:19;jbellis;committed after IRC +1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid flushing compaction_history after each operation,CASSANDRA-6287,12677184,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Nov/13 00:46,16/Apr/19 09:32,14/Jul/23 05:53,02/Nov/13 14:11,2.0.3,,,,,,0,compaction,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Nov/13 00:47;jbellis;6287.txt;https://issues.apache.org/jira/secure/attachment/12611724/6287.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356559,,,Sat Nov 02 14:11:00 UTC 2013,,,,,,,,,,"0|i1pgvb:",356847,,,,,,,,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,,"02/Nov/13 01:12;yukim;+1.
It's just stats so we don't need to flush every inserts.;;;","02/Nov/13 14:11;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2.0 HSHA server introduces corrupt data,CASSANDRA-6285,12677127,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,xedin,davedamoon,davedamoon,01/Nov/13 18:45,16/Apr/19 09:32,14/Jul/23 05:53,19/May/14 21:39,2.0.8,,,,,,4,,,,,"After altering everything to LCS the table OpsCenter.rollups60 amd one other none OpsCenter-Table got stuck with everything hanging around in L0.
The compaction started and ran until the logs showed this:
ERROR [CompactionExecutor:111] 2013-11-01 19:14:53,865 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:111,1,RMI Runtime]
java.lang.RuntimeException: Last written key DecoratedKey(1326283851463420237, 37382e34362e3132382e3139382d6a7576616c69735f6e6f72785f696e6465785f323031335f31305f30382d63616368655f646f63756d656e74736c6f6f6b75702d676574426c6f6f6d46696c746572537061636555736564) >= current key DecoratedKey(954210699457429663, 37382e34362e3132382e3139382d6a7576616c69735f6e6f72785f696e6465785f323031335f31305f30382d63616368655f646f63756d656e74736c6f6f6b75702d676574546f74616c4469736b5370616365557365640b0f) writing into /var/lib/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-tmp-jb-58656-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:141)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:296)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)

Moving back to STC worked to keep the compactions running.
Especialy my own Table i would like to move to LCS.
After a major compaction with STC the move to LCS fails with the same Exception.","4 nodes, shortly updated from 1.2.11 to 2.0.2",appodictic,blynch,brandon.kearby,brevilo,cburroughs,chander,chris.wirt,colinkuo,cscetbon,davedamoon,enigmacurry,gwicke,incubos,jeromatron,kohlisankalp,kvaster,marcuse,mishail,mshang,mshuler,ngrigoriev,ngrigoriev@gmail.com,philipthompson,psanford,ravilr,rbfblk,rbranson,rcoli,rhatch,slebresne,sterligovak,thobbs,xedin,yarin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/14 20:29;mshuler;6285_testnotes1.txt;https://issues.apache.org/jira/secure/attachment/12633219/6285_testnotes1.txt","04/Mar/14 19:40;xedin;CASSANDRA-6285-disruptor-heap.patch;https://issues.apache.org/jira/secure/attachment/12632583/CASSANDRA-6285-disruptor-heap.patch","10/Mar/14 09:42;kvaster;cassandra-attack-src.zip;https://issues.apache.org/jira/secure/attachment/12633659/cassandra-attack-src.zip","28/Jan/14 18:24;rhatch;compaction_test.py;https://issues.apache.org/jira/secure/attachment/12625617/compaction_test.py","10/Mar/14 09:33;kvaster;disruptor-high-cpu.patch;https://issues.apache.org/jira/secure/attachment/12633656/disruptor-high-cpu.patch","10/Mar/14 09:33;kvaster;disruptor-memory-corruption.patch;https://issues.apache.org/jira/secure/attachment/12633657/disruptor-memory-corruption.patch","09/May/14 19:50;brandon.williams;enable_reallocate_buffers.txt;https://issues.apache.org/jira/secure/attachment/12644174/enable_reallocate_buffers.txt",,,,,,,,,,,,,,7.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356503,,,Mon Jan 26 21:39:11 UTC 2015,,,,,,,,,,"0|i1pgiv:",356791,2.0.6,,,,,,,,brandon.williams,,brandon.williams,Critical,,2.0.0,,,,,,,,,,,,,rbranson,,,"02/Nov/13 09:15;davedamoon;After removing all the Data from the OpsCenters Keyspace (and using LCS) and collectiong new Data for a night, the command nodetool compact OpsCenter rollups60 failed with this Exception:

Error occurred during compaction
java.util.concurrent.ExecutionException: java.lang.RuntimeException: Last written key DecoratedKey(-6663228376520744598, 37382e34362e3132382e3139382d6a7576616c69735f6e6f72785f6c6f6767696e672d706572666f726d616e63655f67726f757065642d67657452656164436f756e740b0f0000000100000009726f6c6c75707336) >= current key DecoratedKey(-6896470603826733036, 37382e34362e3132382e3139382d6d65646970726569735f7365617263685f696e6465785f323031335f31305f30382d6d756c7469776f7264735f70686f6e656d732d6765744c69766553535461626c65436f756e74) writing into /var/lib/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-tmp-jb-14-Data.db
	at java.util.concurrent.FutureTask.report(FutureTask.java:122)
	at java.util.concurrent.FutureTask.get(FutureTask.java:188)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:281)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1845)
	at org.apache.cassandra.service.StorageService.forceKeyspaceCompaction(StorageService.java:2167)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
	at sun.reflect.GeneratedMethodAccessor35.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:556)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:811)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:670)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.RuntimeException: Last written key DecoratedKey(-6663228376520744598, 37382e34362e3132382e3139382d6a7576616c69735f6e6f72785f6c6f6767696e672d706572666f726d616e63655f67726f757065642d67657452656164436f756e740b0f0000000100000009726f6c6c75707336) >= current key DecoratedKey(-6896470603826733036, 37382e34362e3132382e3139382d6d65646970726569735f7365617263685f696e6465785f323031335f31305f30382d6d756c7469776f7264735f70686f6e656d732d6765744c69766553535461626c65436f756e74) writing into /var/lib/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-tmp-jb-14-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:141)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:296)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	... 3 more;;;","29/Nov/13 14:47;brevilo;Just in case it helps: I'm getting almost identical exceptions while running a single node stress test using {{cassandra-stress}} with Cassandra 2.0.2 (DSC) on Debian Wheezy with 2 GB RAM. I'm running 10e7 write ops on a single HDD, using (more or less) Cassandra's default configuration, specifically STC.

{noformat}
ERROR [CompactionExecutor:14] 2013-11-29 15:33:39,978 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:14,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(-3658992336117051287, 3033353732383438) >= current key DecoratedKey(-4078405136366838408, 3033353634323236) writing into /srv3/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-tmp-jb-106-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:141)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{noformat};;;","15/Jan/14 18:06;brandon.kearby;I'm getting it as well on 2.0.4. I'm testing a new cluster. I don't get the error with one node, but when I add two or more I get the same error.
{code}
ERROR [CompactionExecutor:6] 2014-01-15 17:13:13,395 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:6,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(-1983406872803353678, 545749545445523a333535383030333439353835353830303334) >= current key DecoratedKey(-7683510718755081698, 545749545445523a333639333235363931383339333238323537) writing into /BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-tmp-jb-121-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code}

Here's the schema I'm testing with:
{code}
create column family signal
  with column_type = 'Standard'
  and comparator = 'UTF8Type'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'UTF8Type'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 432000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'
  and caching = 'ALL'
  and compaction_strategy_options = {'sstable_size_in_mb' : '160'}
  and comment = 'A store of information about each individual signal.'
  and column_metadata = [
    {column_name : 'type',
    validation_class : UTF8Type},
    {column_name : 'foo_id',
    validation_class : LongType},
    validation_class : UTF8Type}]
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.LZ4Compressor'};
{code};;;","15/Jan/14 18:16;jbellis;Can you enable snapshot_before_compaction and post the sstables that it's trying to compact?  I can get you a private upload place if necessary.;;;","15/Jan/14 18:24;brandon.kearby;Sure,

BTW, I tried changing to SizeTieredCompactionStrategy and got the same error. I'll enable snapshot_before_compaction.;;;","15/Jan/14 18:47;brandon.kearby;Snapshot of compaction before failing
Added attachment: system-compactions_in_progress-jb-25-Data.db

Logs before failing
{code}
INFO [CompactionExecutor:6] 2014-01-15 18:38:47,690 ColumnFamilyStore.java (line 740) Enqueuing flush of Memtable-compactions_in_progress@856586691(847/8470 serialized/live bytes, 35 ops)
 INFO [FlushWriter:3] 2014-01-15 18:38:47,691 Memtable.java (line 333) Writing Memtable-compactions_in_progress@856586691(847/8470 serialized/live bytes, 35 ops)
 INFO [FlushWriter:3] 2014-01-15 18:38:47,700 Memtable.java (line 373) Completed flushing /BigData/lib/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-jb-24-Data.db (304 bytes) for commitlog position ReplayPosition(segmentId=1389810508756, position=33429429)
 INFO [CompactionExecutor:6] 2014-01-15 18:38:47,703 CompactionTask.java (line 115) Compacting [SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-45-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-46-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-67-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-72-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-78-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-75-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-56-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-62-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-66-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-49-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-47-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-57-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-61-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-79-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-65-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-50-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-58-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-77-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-54-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-53-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-48-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-64-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-68-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-76-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-55-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-74-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-60-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-52-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-69-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-71-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-73-Data.db'), SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-63-Data.db')]
 INFO [CompactionExecutor:6] 2014-01-15 18:39:05,208 ColumnFamilyStore.java (line 740) Enqueuing flush of Memtable-compactions_in_progress@2046507929(0/0 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2014-01-15 18:39:05,208 Memtable.java (line 333) Writing Memtable-compactions_in_progress@2046507929(0/0 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2014-01-15 18:39:05,218 Memtable.java (line 373) Completed flushing /BigData/lib/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-jb-25-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1389810508756, position=33430117)
ERROR [CompactionExecutor:6] 2014-01-15 18:39:05,220 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:6,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(-5705444534806265577, 0000000000000000000000000000000000000000000000000000) >= current key DecoratedKey(-7490754936938484492, 00ab1b0000000000000000000000000000000000000000000000) writing into /BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-tmp-jb-80-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
 INFO [MemoryMeter:1] 2014-01-15 18:39:08,865 Memtable.java (line 451) CFS(Keyspace='system', ColumnFamily='sstable_activity') liveRatio is 14.596825396825396 (just-counted was 14.596825396825396).  calculation took 1ms for 84 cells
 INFO [MemoryMeter:1] 2014-01-15 18:43:44,575 Memtable.java (line 451) CFS(Keyspace='system', ColumnFamily='sstable_activity') liveRatio is 14.591111111111111 (just-counted was 14.585396825396826).  calculation took 4ms for 210 cells
(END) 
{code};;;","15/Jan/14 18:50;jbellis;can you tar up all the components, not just .db?;;;","15/Jan/14 18:54;jbellis;... for all the sstables in the ""Compacting"" list;;;","15/Jan/14 19:50;brandon.kearby;Hi Jonathan,

Here's a link to what you need: http://bhorne.test.s3.amazonaws.com/cassandra.tar.gz
;;;","15/Jan/14 21:11;brandon.kearby;So it seems like it might be related to the hsa server. BTW, I was getting https://issues.apache.org/jira/browse/CASSANDRA-6373 where it would hang describing the ring. So I upgraded to thrift-server-0.3.3.jar. When running with the sync server, I don't get the error above.

A little more context, I'm using pig and the CassandraStorage class to drive the writes. Running as a map task with 12 concurrent mappers creates 2773 connections!

 lsof -i tcp:9160 | wc -l
2773
;;;","15/Jan/14 22:47;jbellis;The  .tar.gz does not contain the sstables mentioned in the error message;;;","15/Jan/14 22:53;brandon.kearby;Correct. The tar contains a full log file with another example of the error.;;;","15/Jan/14 22:57;jbellis;[~enigmacurry] Can your team reproduce w/ the schema above and the sstables from the tarball?

{noformat}
Compacting [SSTableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-2-Data.db'), STableReader(path='/BigData/lib/cassandra/data/SocialData/signal/SocialData-signal-jb-1-Data.db')]
{noformat};;;","15/Jan/14 23:34;brandon.kearby;BTW, It happens when we use hsha. The schema above is abbreviated as I've left out a lot of the payload from the signal table. Ping me if you need the full signal table definition.;;;","15/Jan/14 23:40;brandon.kearby;When I cranked up the  number of rpc_max_threads and changed to sync, it stopped happening.;;;","27/Jan/14 18:55;brandon.kearby;After doing some more digging, looks like my issue is the same as https://issues.apache.org/jira/browse/CASSANDRA-4687
;;;","27/Jan/14 19:10;jbellis;[~rhatch] would still be useful to try to repro w/ Brandon's instructions since we don't have a way to repro 4687 yet.;;;","27/Jan/14 20:39;rhatch;[~jbellis] -- I was able to get the exception to occur by doing the following:

create a new cluster with ccm, and populate with 3 nodes
{noformat}
create keyspace SocialData with placement_strategy='org.apache.cassandra.locator.SimpleStrategy' and strategy_options = {replication_factor:3};
{noformat}
create signal Column Family (I had to modify schema above a little bit to make it work):
{noformat}
create column family signal
  with column_type = 'Standard'
  and comparator = 'UTF8Type'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'UTF8Type'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 432000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'
  and caching = 'ALL'
  and compaction_strategy_options = {'sstable_size_in_mb' : '160'}
  and comment = 'A store of information about each individual signal.'
  and column_metadata = [
    {column_name : 'type', validation_class : UTF8Type},
    {column_name : 'foo_id', validation_class : LongType}]
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.LZ4Compressor'}; 
{noformat}
stopped the nodes
copied all the files from the provided tar's /data/SocialData/ directory to one of my nodes
started the nodes up again
At this point I didn't find any data in the signal column family (using 'list signal;')
The exception appeared in the node's log
{noformat}
ERROR [CompactionExecutor:10] 2014-01-27 12:45:26,734 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:10,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(4322717900587903123, 706f737431353834373031323038270903ae0022076d9f) >= current key DecoratedKey(-7009815163526224622, 545749545445523a333533343836323333393032363439333437) writing into /home/rhatch/.ccm/test_cluster_1390845354/node1/data/SocialData/signal/SocialData-signal-tmp-jb-7-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:141)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask.run(FutureTask.java:262)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{noformat}
I was curious if repair would have any bearing, so I ran repair on one node (after which I can see data in the signal table), then I stopped and started the nodes again -- a similar exception appears in the log for all 3 nodes ('Last written key DecoratedKey ...').

I'm not 100% certain if my procedure for using the provided tar's test data was correct, so let me know if there's anything obvious I missed and I'll run through it again.;;;","27/Jan/14 20:45;jbellis;Was that 2.0 HEAD or 2.0.4?;;;","27/Jan/14 20:50;rhatch;oh sorry, forgot that detail. I reproduced from the cassandra-2.0.2 tag.;;;","27/Jan/14 20:52;jbellis;Can you try 2.0 HEAD as well just to be sure?;;;","27/Jan/14 21:11;rhatch;OK, appears we have the same issue on 2.0 HEAD as well (8bbb6e...) -- exception appears on startup using the procedure I included earlier.;;;","27/Jan/14 21:17;rhatch;I'm going to attempt to condense this down to a simple dtest as well.;;;","27/Jan/14 21:45;brandon.kearby;Hi [~rhatch], 

Here's the full schema I'm using to test with:

create keyspace SocialData
  with placement_strategy = 'NetworkTopologyStrategy'
  and strategy_options = {DC-Analytics : 3}
  and durable_writes = true;

use SocialData;

create column family signal
  with column_type = 'Standard'
  and comparator = 'UTF8Type'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'UTF8Type'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and gc_grace = 432000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.LeveledCompactionStrategy'
  and caching = 'NONE'
  and compaction_strategy_options = {'sstable_size_in_mb' : '160'}
  and comment = 'A store of information about each individual signal.'
  and column_metadata = [
    {column_name : 'type',
    validation_class : UTF8Type},
    {column_name : 'department_id',
    validation_class : LongType},
    {column_name : 'ecosystem_account_id',
    validation_class : UTF8Type},
    {column_name : 'content_type',
    validation_class : UTF8Type},
    {column_name : 'rating_count',
    validation_class : LongType},
    {column_name : 'service_account_id',
    validation_class : UTF8Type},
    {column_name : 'time',
    validation_class : LongType},
    {column_name : 'organization_id',
    validation_class : LongType},
    {column_name : 'conversation_id',
    validation_class : UTF8Type},
    {column_name : 'favorites_count',
    validation_class : LongType},
    {column_name : 'dislike_count',
    validation_class : LongType},
    {column_name : 'url',
    validation_class : UTF8Type},
    {column_name : 'impressions',
    validation_class : LongType},
    {column_name : 'network_strength',
    validation_class : LongType},
    {column_name : 'parent_signal_id',
    validation_class : UTF8Type},
    {column_name : 'account_snapshot_id',
    validation_class : UTF8Type},
    {column_name : 'region_id',
    validation_class : LongType},
    {column_name : 'time_bucket',
    validation_class : LongType},
    {column_name : 'enriched_on',
    validation_class : LongType},
    {column_name : 'dachis_account_id',
    validation_class : UTF8Type},
    {column_name : 'text',
    validation_class : UTF8Type},
    {column_name : 'sentiment',
    validation_class : LongType},
    {column_name : 'like_count',
    validation_class : LongType},
    {column_name : 'industry_id',
    validation_class : LongType},
    {column_name : 'service',
    validation_class : UTF8Type},
    {column_name : 'cloned_from',
    validation_class : UTF8Type},
    {column_name : 'constituent_type',
    validation_class : UTF8Type},
    {column_name : 'listings_count',
    validation_class : LongType},
    {column_name : 'network_size',
    validation_class : LongType},
    {column_name : 'analyzed',
    validation_class : Int32Type},
    {column_name : 'username',
    validation_class : UTF8Type},
    {column_name : 'service_signal_id',
    validation_class : UTF8Type},
    {column_name : 'language',
    validation_class : UTF8Type},
    {column_name : 'brand_id',
    validation_class : LongType},
    {column_name : 'rating',
    validation_class : LongType},
    {column_name : 'relationship_id',
    validation_class : UTF8Type}]
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.LZ4Compressor'};

;;;","28/Jan/14 18:22;rhatch;I was able to repro with the data provided in the tar (as noted above). Unfortunately my attempts to use dtest to reproduce the issue ""from scratch"" haven't been successful. First I tried testing with dtest w/CQL, but had no luck, though I may have missed something when trying to translate everything into CQL.

Next I tried dtest w/thrift but similarly wasn't able to trigger the issue in this way. I tried using the hsha rpc_server and was getting what appeared to be an unrelated error:
{noformat}
ERROR [Thrift-Selector_0] 2014-01-28 11:09:16,762 Message.java (line 153) Read an invalid frame size of 0. Are you using TFramedTransport on the client side?
{noformat}

I'll attach my basic dtest here in case it's useful later, but as for now I can't repro the issue without the provided db.;;;","31/Jan/14 22:06;thobbs;[~brandon.kearby] It looks like there may be a few things going on here.

The first is that some of your column names are not valid UTF-8.  I'm not terribly familiar with the UTF-8 specs, but they seem to fail validation in different ways, and Python seems to agree that they are not valid UTF-8, so I don't think it's a problem with our validation code.  Did you change the comparator from BytesType to UTF8Type at some point?  It might not be relevant to this ticket, but you may want to check on that on your end.

The second problem is that SocialData-signal-jb-2-Data.db has some out-of-order rows.  It looks like about 9 rows are randomly out of place in the sstable.  Running scrub would fix this, but I think it's erroring on UTF8 validation.  If I change the comparator to BytesType, the scrub completes and the rows are written in order.  So the problem is not necessarily with compaction itself but with out-of-order rows being written to sstables.

Given that switching from hsha to sync seemed to fix the problem, I wonder if that's part of the original cause.;;;","06/Feb/14 17:32;brandon.kearby;[~thobbs], The odd thing is that with hsha, it works with one node. When we have two or more nodes in the cluster, it starts getting these errors.;;;","06/Feb/14 18:59;ravilr;cc [~xedin]
we were also seeing such random out of place partitions/rows in sstables (rows not hashing to the node) while using disruptor based hsha thrift server, causing compaction to fail with out of order keys. this used to happen on freshly flushed sstables in L0.  We also used to see thrift validation failing on some columns while reading back.  We don't see these after switching back to sync server.


 ;;;","06/Feb/14 20:29;xedin;[~ravilr] Can you try with the most recent release of hsha, version 0.3.3? Just remove the old jar and drop in new one, that should be sufficient.;;;","06/Feb/14 20:58;brandon.kearby;[~xedin], I was running with 0.3.3. The previous version would hang on describe ring for me.;;;","06/Feb/14 22:19;xedin;[~rhatch] The 0 frame size you are seeing is a known Thrift problem which happens even with stock server implementations, they are working on it but it shouldn't cause any problems as such frames are ignored (it could also happen if something does e.g. telnet to the thrift port). I'm not sure that this is a problem with HsHa directly but might be unveiled by the increased throughput you can get with HsHa comparing to sync, it looks exactly like https://issues.apache.org/jira/browse/CASSANDRA-4687 (as [~brandon.kearby] mentioned) so can you try disabling key_cache and try uploading again with hsha?;;;","06/Feb/14 23:12;brandon.kearby;[~xedin], I've tried disabling the key_cache and it didn't help. That was the last thing I tried.;;;","06/Feb/14 23:26;xedin;I see that you have it set to NONE in CF schema but have you also tried disabling it all together in yaml? I'm not saying that it would help but trying to eliminate all possibilities. It's just not obvious to me if it's a hsha problem how Thrift could actually be correctly interpreting erroneous data from the socket, dispatching it the right Thrift handler and deserializing whole mutation (and meta information) to insert it into storage...;;;","07/Feb/14 02:56;ravilr;Also, one more factor with disruptor based hsha is direct memory/Unsafe versus heap-based message buffers. When we encountered this issue, we were running with jna,  hence was using direct memory buffers. I didn't test with heap-based message buffers. ;;;","17/Feb/14 19:00;ngrigoriev;I have started seeing these too. Surprisingly...after adding OpsCenter CE to my cluster. I do not see these associated with my own data.

{code}
java.lang.RuntimeException: Last written key DecoratedKey(3542937286762954312, 31302e332e34352e3135382d676574466c757368657350656e64696e67) >= current
key DecoratedKey(-2152912038130700738, 31302e332e34352e3135362d77696e7465726d7574655f6a6d657465722d776d5f6170706c69636174696f6e732d676574526563656e744
26c6f6f6d46) writing into /hadoop/disk1/cassandra/data/OpsCenter/rollups300/OpsCenter-rollups300-tmp-jb-5055-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{code};;;","18/Feb/14 00:48;kohlisankalp;This issue is also in logs attached in CASSANDRA-6716. ;;;","20/Feb/14 20:06;ngrigoriev;Can confirm on my side. I have switched to ""sync"" RPC server and after few scrubs/restarts I am running my load tests on a 6-node 2.0.5 cluster without a single exception in last ~8 hours.

I tried to correlate the moment I started getting large number of FileNotFoundException's with other events in my cluster....realized that it was not exactly 2.0.5 upgrade. It seems to correlate mostly with a moment when my jmeter server went out of free space and a bunch of tests crashed. Obviously, these crashes have terminated a few hundreds of client connections to Cassandra.

Not sure if it is related but it seems that from that moment it was some sort of snowball effect.;;;","03/Mar/14 09:37;marcuse;I think we can conclude that this only happens with HSHA

I brought back TThreadedSelectorServer instead of the Disruptor based server and have been running it for a few hours without the bug happening.

Could someone ([~kvaster] ?) try out https://github.com/krummas/cassandra/commits/marcuse/hsha and see if you can break it?

Note that i had to make a change in thrift 0.9.1 to get it to build and work, I'll follow up on that if this seems to solve the issue.;;;","03/Mar/14 11:06;kvaster;https://www.dropbox.com/s/3ldg10zh7qvva27/cassandra-attack.jar

Schema is located inside jar file - cassandra.txt

1. Start cassandra
2. java -jar cassandra-attack.jar
3. Stop cassandra
4. Start cassandra - commit logs will be corrupted.;;;","03/Mar/14 13:38;ngrigoriev;[~krummas]

I think using HSHA makes it easier to reproduce but...I am running SYNC for over a week now and recently I have experienced the same issue again.

We had another unclean shutdown (hrrr...some people are smarter than the UPSes ;) ) and after bringing the nodes back I have found that  on one node my compactions constantly fail with FileNotFoundException. Even worse, I can't scrub the keyspace/CF in question because ""scrub"" fails instantly with ""RuntimeException: Tried to hard link to file that does not exist..."". I have reported that one too. It is impossible to scrub. The only way to fix that issue I have found so far is to restart Cassandra on that node, stop compactions as soon as it starts (well, I could disable them differently, I assume) and then scrub. Sometimes I have to do it in several iterations to complete the process. Once I scrub all problematic KS/CFs I see no more exceptions.;;;","03/Mar/14 13:42;kvaster;Bug with FileNotFoundException is not related to HsHa problem.
And about several iterations for scrub:
https://issues.apache.org/jira/browse/CASSANDRA-6791
;;;","03/Mar/14 15:31;jbellis;According to http://mail-archives.apache.org/mod_mbox/cassandra-user/201402.mbox/%3C038601cf28ea$a2e504d0$e8af0e70$@struq.com%3E the 0.9 TThreadSelectorServer works well, although I'm not sure if he means that it performs better than 0.8 TTSS or just that it doesn't cause corruption. :);;;","03/Mar/14 16:19;kvaster;I've tried to investigate problem with HsHaDistruptorServer, but with no luck. Telling the truth, I see no reason for that server to corrupt data.
Also HsHaDistruptor do not corrupt data in case useHeapBasedAllocation is turned on.
More over if you look at disruptor-thrift-server code - Message.reallocateDataBuffer and turn on heap based allocation only for dataBuffer then you will not see corruption.;;;","03/Mar/14 18:14;xedin;[~ngrigoriev] and [~brandon.kearby] can you try setting useHeapBasedAllocation to ""true"" ? I'm fine with switch back to TThreadedSelectorServer if that helps.;;;","03/Mar/14 19:22;ngrigoriev;[~xedin]

That seems to be a parameter of the Thrift server...How do I control this parameter? Or I should just disable JNA?;;;","03/Mar/14 19:45;xedin;You can do it via JMX or disable JNA, I can also make a patch with would set it explicitly in Cassandra code.;;;","04/Mar/14 17:18;chris.wirt;[~Jonathan Ellis] [~Marcus Eriksson] That was my post from the user mail list.
After our 1.2.14 -> 2.0.5 upgrade and failure to get the new HsHa stable in our system, we moved to using the thrift 0.9.1 TTSS with reasonable success. We've now been running for two weeks under a relatively high read load.
We haven't seen any ""DecoratedKey != ..."" errors.
We have seen some warnings on start up about SSTable rows being out of order.
We have seen commit logs starting to build up with ""All time blocked"" on the FlushWriter incrementing.

Performance comparisons might be a little unfair, but certainly our p95, p99 have overall improved.

Obviously very keen to not be running a custom build of C*.;;;","04/Mar/14 17:26;marcuse;[~chris.wirt] could you paste those startup log lines?;;;","04/Mar/14 17:45;chris.wirt;These have since disappeared. I just restarted this node just now to check. We haven't run a scrub.

 WARN [main] 2014-02-16 23:23:02,032 LeveledManifest.java (line 171) At level 1, SSTableReader(path='/disk2/cassandra/data/struqrealtime/impressionstorev2/struqrealtime-impressionstorev2-jb-118905-Data.db') [DecoratedKey(-1513272878957942943, c41c955b40274acfa466ccb6079a21e5), DecoratedKey(6301362410765453237, 43ae61aacbc446be92c8bdea1d43e342)] overlaps SSTableReader(path='/disk2/cassandra/data/struqrealtime/impressionstorev2/struqrealtime-impressionstorev2-jb-116400-Data.db') [DecoratedKey(3953001739649874864, 5811ce41b7014917ab82eb32e8861ca5), DecoratedKey(9190609424240623933, 4e5b00a5a7594289924674974f44a995)].  This could be caused by a bug in Cassandra 1.1.0 .. 1.1.3 or due to the fact that you have dropped sstables from another node into the data directory. Sending back to L0.  If you didn't drop in sstables, and have not yet run scrub, you should do so since you may also have rows out-of-order within an sstable
;;;","04/Mar/14 18:41;marcuse;[~chris.wirt] those entries are unrelated (and fixed in CASSANDRA-6688);;;","04/Mar/14 19:40;xedin;The patch sets heap based allocation by default in disruptor server, should make it easier for people to test that scenario...;;;","06/Mar/14 19:05;jbellis;[~mshuler] Can you test hsha with Viktor's jar above?  (https://issues.apache.org/jira/browse/CASSANDRA-6285?focusedCommentId=13917950&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13917950)

I want to know if
# you can reproduce with a single node
# if not, if you can reproduce with multiple nodes
# assuming either 1 or 2, if you can still reproduce after applying Pavel's heap allocation path;;;","06/Mar/14 19:08;mshuler;Sure - let me see what I can find out.;;;","06/Mar/14 20:29;mshuler;6285_testnotes1.txt attached.

Neither a single node with hsha, nor a 3 node ccm cluster with hsha gave me any interesting errors with the attack jar.  Should I go back and try some of the previous repro steps and check yay/nay on the patch fixing this for those?;;;","06/Mar/14 20:37;xedin;[~mshuler] Can you try the same on the machine running Linux (if you haven't done that yet)? 

Edit: from the log it looks like Disruptor wasn't using the off-heap memory because JNA is disabled, ""Off-heap allocation couldn't be used as JNA is not present in classpath or broken, using on-heap instead."" So it would be great if you could test this on Linux with jna enabled.

Thanks!;;;","06/Mar/14 20:38;mshuler;I'm using a linux machine  :)  - and will link in JNA - good suggestion.;;;","06/Mar/14 20:51;mshuler;With jna enabled, yes, on a single node, after running the attack jar and restarting c*, I get:
{noformat}
 INFO [main] 2014-03-06 14:46:51,272 ColumnFamilyStore.java (line 254) Initializing tmp.CF
 INFO [main] 2014-03-06 14:46:51,277 ColumnFamilyStore.java (line 254) Initializing system_traces.sessions
 INFO [main] 2014-03-06 14:46:51,280 ColumnFamilyStore.java (line 254) Initializing system_traces.events
 INFO [main] 2014-03-06 14:46:51,281 CassandraDaemon.java (line 291) completed pre-loading (5 keys) key cache.
 INFO [main] 2014-03-06 14:46:51,288 CommitLog.java (line 130) Replaying /var/lib/cassandra/commitlog/CommitLog-3-1394138577628.log, /var/lib/
cassandra/commitlog/CommitLog-3-1394138577629.log
 INFO [main] 2014-03-06 14:46:51,311 CommitLogReplayer.java (line 184) Replaying /var/lib/cassandra/commitlog/CommitLog-3-1394138577628.log (C
L version 3, messaging version 7)
ERROR [main] 2014-03-06 14:46:51,432 CommitLogReplayer.java (line 306) Unexpected error deserializing mutation; saved to /tmp/mutation77387084
28696995512dat and ignored.  This may be caused by replaying a mutation against a table with the same name but incompatible schema.  Exception
 follows: 
org.apache.cassandra.serializers.MarshalException: Invalid version for TimeUUID type.
        at org.apache.cassandra.serializers.TimeUUIDSerializer.validate(TimeUUIDSerializer.java:39)
        at org.apache.cassandra.db.marshal.AbstractType.validate(AbstractType.java:172)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:276)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer.recover(CommitLogReplayer.java:97)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:151)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:131)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:312)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:471)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:560)
{noformat}

I'll double-check a 3 node cluster, then patch and see where I get.

(edit) this looks quite different than the previously posted errors - not sure if I'm on the right track, here..;;;","06/Mar/14 21:35;mshuler;Both a single local hsha node and 3x node ccm cluster with hsha (jna on both) throw the above errors after attack.jar run and restart.  The patch does appear to fix both single and ccm cluster.  My pre-patch ccm cluster never fully restarted, but do we need logs or anything from before/after?;;;","06/Mar/14 21:39;xedin;I don't think we need logs, [~jbellis] I'm thinking of commiting attached patch which should help meanwhile I working on resolving off-heap problem, WDYT?;;;","06/Mar/14 21:56;benedict;So, I think there may potentially be at least two races in the off heap deallocation. I suspect that may not be everything, though, as these two races probably won't cause the problem often. These are predicated on the assumption that thrift doesn't copy data from the DirectByteBuffer that the hsha server provides to it, so could be wrong, but anyway:

1) CL appends can be lagged behind the memtable update and, as a result, the acknowledgment to the client of success writing. If the CL record contains the ByteBuffer when it is freed, and that address is then reused in another allocation, it will write incorrect data to the commit log.
2) I believe thrift calls are two stage. If this is the case, and the client disconnects in between sending the first stage and receiving the result in the second stage, the buffer could be freed whilst still in flight to the memtable/CL

These are just quick ideas for where it might be, I haven't familiarised myself fully with thrift, the disruptor etc. to be certain if these are plausible, but it may turn out to be useful so thought I'd share.;;;","06/Mar/14 23:21;rbranson;Unfortunately this hit us during our 2.0.5 upgrade and 'sync' is not an option for the # of connections we have per node (tried this). We've been running Marcus' patch in prod and limping along on it, but it looks like the requestInvoke() override is causing the requests to get executed on the selector pool (which is limited by CPU #) instead of the executor service so our response times are pretty bad. The lack of anything showing up in the JMX for the executor service definitely points towards this. ;;;","06/Mar/14 23:57;xedin;[~rbranson] Which Marcus' patch are you talking about? Also I want to clarify one thing - disruptor server doesn't use requestInvoke(FrameBuffer) but dispatchInvoke(Message) which schedules message to executor pool based on ring buffer (WorkerPool) so actual execution is done in the separate thread. I can attach a patch which would switch back to TThreadedSelectorServer which is packed with Thrift (the only different between it and disruptor is that it schedules to classic thread pool), maybe disruptor server wasn't as good an idea for all of the real world use cases...;;;","07/Mar/14 00:34;benedict;It looks like thrift doesn't retain the DirectByteBuffer, just reads straight from them. The only possible window for corruption is during the construction of the thrift args object, which is a fairly narrow window.;;;","07/Mar/14 02:27;rbranson;We put in the TThreadedSelectorServer patch from Marcus. On top of that, to get our response times down from the 10x what they should be, I rolled out a larger hard-coded selector thread pool size of 256 (instead of the # of processors -- a measly 16). This is shaping up nicely.;;;","07/Mar/14 02:38;xedin;[~rbranson] So what you are saying is that after problem with disruptor you never tried it again with on-heap buffers but switched to TThreadedSelectorServer and increased selector pool size and the requestInvoke() is the problem with TThreadedSelectorServer?;;;","07/Mar/14 02:53;rbranson;[~xedin]: We had perf issues with the disruptor as well (sudden spikes of CPU to 100%) + this so I just wanted to get production away from it ASAP.;;;","07/Mar/14 03:22;xedin;[~rbranson] But the most important question for this ticket at least is - did you run with on or off heap buffers? I can bring TThreadedSelectorServer back in this ticket or just go with on-heap buffers and disruptor. [~mshuler] do you have any performance tests related to Thrift server? Maybe there is a low hanging fruit in there to fix up the spikes that Rick mentioned if we can reproduce.;;;","07/Mar/14 03:36;jbellis;bq. I'm thinking of commiting attached patch which should help meanwhile I working on resolving off-heap problem

Yeah, let's do this for now and roll 2.0.6 so we can stop the bleeding, then figure out whether doing more work on disruptor or TTSS is better for 2.0.7.;;;","07/Mar/14 03:59;rbranson;[~xedin]: off-heap for disruptor.

I think that we should really consider bringing back the old HSHA implementation from 1.2 as the ""hsha"" and allow switching to the disruptor implementation as another rpc_server_type for those that want to try it out.;;;","07/Mar/14 04:10;brandon.williams;bq. I think that we should really consider bringing back the old HSHA implementation from 1.2 as the ""hsha"" and allow switching to the disruptor implementation as another rpc_server_type for those that want to try it out.

I think I'm inclined to agree with this, aside from creating yaml creation problems in a minor.  If we just give up and effectively revert, what have we lost?  We need 2.0 stabilization sooner rather than later now, in all aspects.  If we can't trust disruptor except with a small change, let's just not trust it yet and worry about that in a future release.  With 2.1 beta already out, we can't tolerate much instability in the 2.0 branch.;;;","07/Mar/14 04:37;xedin;I'm not sure if there is a point of going all the way back to original HsHa when there is TThrededSelectorServer, but I'm fine with going with disruptor as a separate option, something like ""disruptor_hsha"" and making ""hsha"" - TThrededSelectorServer from Thrift, that's how I wanted it originally. Also I just want to mention that people have reported that disruptor works for them with on-heap buffers, so I am not sure if we need to go all paranoid about this...;;;","07/Mar/14 07:02;marcuse;My branch from above needed a tiny hack to thrift (https://github.com/krummas/thrift/commit/01ba2a3f3d386d0981371aab2494470e2a78e596), so if we want to roll with TTSS we should refactor our thrift usage a bit to avoid that hack;;;","07/Mar/14 09:35;xedin;Ah, so they have finally made transport a protected field in FrameBuffer... Well, that considerably complicates things with switching back TThreadedSelectorServer.;;;","07/Mar/14 10:21;slebresne;Alright, I've committed Pavel's patch above as a stopgap solution as discussed above because I want to start a vote on 2.0.6 asap (the changelog is getting pretty big). I've created CASSANDRA-6815 to decide what we want the followup for that to be for 2.0.7.;;;","07/Mar/14 20:53;mshang;To add to [~rbranson]'s input, we're also seeing the same stacktrace as [~mshuler] (TimeUUID MarshalException). I inspected the row mutations that caused it. Three ranges were nonsensical: the key, the column name, and the value. By nonsensical, I mean that they don't match my expectation of what we are inserting in production data. All other ranges seemed fine (timestamps, masks, sizes, cfid). The key, column name, and value were read successfully, so their length metadata was good. For our data, the column comparator is TimeUUID. Our client library is pycassa. Whereas pycassa generates tuuids like this: 913d7fea-a631-11e3-8080-808080808080, the nonsensical column names look like this: 22050aa4-de11-e380-8080-80808080800b and this: 10c326eb-86a4-e211-e380-808080808080. Most are of the first form. By shifting these nonsensical tuuids to the left or right by an octet, you get a reasonable tuuid. I don't have a similar insight into the nonsensical keys and values, but they could also be left or right shifted.;;;","10/Mar/14 03:55;enigmacurry;I'd like to be able to reproduce this in dtests to track this bug. Seeing as [~rhatch]'s python test wasn't able to repro this issue, and a [quick test I wrote|https://github.com/riptano/cassandra-dtest/blob/cassandra-6285/test_6285.py] doesn't either, does anyone have a simple way to reproduce this issue? 

[~kvaster] would you mind sharing the source code for your attack jar?;;;","10/Mar/14 09:33;kvaster;Attached patches for on-heap disruptor.

First pacth (disruptor-high-cpu.patch) turns off any key interests in case we're waiting for message to be processed. We need that cause processing may be delayed in case of high load and there may be something available to read from stream. In that case we'll have 100% cpu core usage.

Second patch (disruptor-memory-corruption.patch) makes copy from off-heap ByteBuffer when reading binary data. This binary data may be stored inside cassandra as is even after message processing. And binary data can be corrupted - cause it's memory may be already deallocated.;;;","10/Mar/14 09:42;kvaster;Attached cassandra-attack-src.zip - eclipse project for making high load test on cassandra.
This attack uses 100 threads to make writes, reads and deletes.;;;","10/Mar/14 09:51;benedict;Hmm. Just taking a look at Viktor's patch, I realised that my initial conclusions were actually quite plausible and probably (one of) the causes of the problem. When I dismissed them, I didn't realise we were using a custom TBinaryProtocol implementation. In particular (1) is definitely possible, and probably the cause of the issue, although the attack jar source would be helpful to figure out of there are any other potential causes. We should be able to force the problem to occur by artificially delaying the commit log write to prove this.

Either way, I don't think Viktor's patch is the best way to deal with this problem, as it leaves cleaning up the direct buffers to GC. Since we could be creating a lot of these, we could create an awful lot of artificial memory pressure. Honestly, I think the best solution is to simply avoid using direct buffers with thrift, at least until 2.1, which should fix this problem by ensuring the CL _write_ (if not commit) has happened before performing the memtable insertion.;;;","10/Mar/14 09:55;kvaster;My patch is defenetly NOT GOOD. Also for me that patch means anothe thing: it seems that we have 'success' answer  before data is passed to commitlog... I don't think that this is good.;;;","10/Mar/14 09:59;benedict;bq. we have 'success' answer before data is passed to commitlog

Yes, see my comment from a few days ago:

bq. 1) CL appends can be lagged behind the memtable update and, as a result, the acknowledgment to the client of success writing. If the CL record contains the ByteBuffer when it is freed, and that address is then reused in another allocation, it will write incorrect data to the commit log.

This is an absolutely plausible scenario since we do actually slice directly from the DirectByteBuffer, which I previously thought we did not.;;;","10/Mar/14 10:05;benedict;Has anybody tested this problem against 2.1? As if this is the only issue, it should be fixed there.;;;","10/Mar/14 19:17;enigmacurry;[~benedict] I haven't yet been able to reproduce this with anything other than Viktor's attack jar. I'm thinking Java's threading is beating Python's threading here, so I [created a dtest|https://github.com/riptano/cassandra-dtest/blob/master/thrift_hsha_test.py] that just run's his jar directly. This test is currently passing on cassandra-2.0 and cassandra-2.1 HEAD. ;;;","10/Mar/14 19:23;jbellis;You'd want to revert Pavel's patch from 2.1 to test Benedict's theory.;;;","10/Mar/14 21:17;kvaster;You may set threads count to only one in cass-atack jar and you will be still able to reproduce error.;;;","11/Mar/14 00:05;benedict;bq. You'd want to revert Pavel's patch from 2.1

beta1 should be fine to test against for this;;;","12/Mar/14 10:46;kvaster;I've tried my test with beta1 and I can confirm that I was not able to reproduce bug.
I think that it will be better to not use disruptor on 2.0.x even with on-heap allocation (we can still reuse buffer in case message will be of equal size when previous).
And it should be safe to use disruptor on 2.1 branch.

We'll be waiting for 2.1 release, cause it really impressed me over 2.0;;;","14/Mar/14 12:45;kvaster;Telling the truth, I don't think that this is really fixed in 2.0.6.
It's not easy to reproduce bug right now, but I think it can be. thrift-disruptor server does not allocate new Buffer for new message in case new message is of equal size with previous. In that case bug can be reproduced even with on-heap allocation.;;;","14/Mar/14 12:54;benedict;+1. That needs to be fixed as well.;;;","18/Mar/14 15:17;rcoli;{quote}
... Telling the truth, I don't think that this is really fixed in 2.0.6.
{quote}
If hsha is irrevocably broken with data corruption risk in 2.0 line, could we either get it wired off in the next point release, or some messaging in NEWS.txt that instructs people not to use it? My preference is the former to cover upgraders who are foolish enough to not read NEWS.txt; I am unable to see the benefit of leaving it usable if it is known broken.
;;;","20/Mar/14 19:49;appodictic;I read thought this. Does it make sense to call this HSHA2 and restore the old code and call it HSHA? I;;;","09/May/14 18:01;rbranson;This is not fixed. Still seeing the same exception running 2.0.6.

ERROR [CompactionExecutor:7] 2014-05-09 17:59:58,640 CassandraDaemon.java (line 196) Exception in thread Thread[CompactionExecutor:7,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(132126721345628486111245439753727165857, 0f3b67f2) >= current key DecoratedKey(37424530135488872684523334498941679307, 196b70ab) writing into /data/cassandra/data/redacted/Redacted/redacted-Redacted-tmp-jb-156533-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744);;;","09/May/14 19:31;brandon.williams;The line in question: https://github.com/xedin/disruptor_thrift_server/commit/77d6715af0eeba4c52f42fa6ba6549c8ae52ffa7#diff-18c889f19dc9fbeb73af99dcff152b6eR421;;;","09/May/14 19:50;brandon.williams;Patch to enable buffer reallocation.;;;","09/May/14 23:05;jbellis;I thought we were reallocating by default but I must have gotten that confused with on-heap buffers above.  If Viktor is right, reusing buffers is always potentially dangerous and should just be removed.  Can you comment, [~xedin]?;;;","09/May/14 23:45;xedin;No, by default it's turned off, because Thrift side expectation is that once the invocation is complete nobody else holds the buffers, but it seems like the problem is that on Cassandra side we actually never copy the buffer for the commit log (or was it something else?). So we need to set thrift server to alwayReallocate explicitly.

[~rbranson] I can give you updated jar so you don't have to wait for the release of Cassandra which would have alwaysReallocate set to true by default.;;;","09/May/14 23:55;xedin;So I can do two things, a). set alwaysReuse to true by default and release 0.3.5 today b). you can just switch to alwaysReallocate(true) in the configuration for 2.0.8, either works for me.;;;","10/May/14 04:23;jbellis;Yeah, we do treat BB as immutable so CL would understandably not expect Thrift to pull the rug out from under it.

I'm fine with calling alwaysReallocate on the Cassandra side in the interest of not changing things out from under any other users.;;;","10/May/14 16:10;brandon.williams;I have no issue with doing a) _AND_ b), just to be extra safe, if we know this puts the nail in this ticket's coffin.;;;","11/May/14 16:12;appodictic;I was poking around the dependency a bit

{quote}
Running com.thinkaurelius.thrift.OffHeapMultiRequestTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 2.515 sec
Running com.thinkaurelius.thrift.OnHeapMultiConnectionWithReallocateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.481 sec
Running com.thinkaurelius.thrift.OffheapMultiConnectionWithRellocateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.478 sec
Running com.thinkaurelius.thrift.OnHeapMultiConnectionTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.417 sec
Running com.thinkaurelius.thrift.OnHeapMultiRequestWithReallocateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.735 sec
Running com.thinkaurelius.thrift.OffHeapMultiRequestWithReallocateTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.437 sec
Running com.thinkaurelius.thrift.OffHeapMultiConnectionTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.282 sec
Running com.thinkaurelius.thrift.OnHeapMultiRequestTest
Tests run: 2, Failures: 0, Errors: 0, Skipped: 0, Time elapsed: 0.491 sec
{quote}

Q. Are a few tests that run in roughly 20 seconds enough to prove that this component fit for production? These highly concurrent off heap systems can have very subtle bugs as this ticket shows.

If I understand correct HSHA is not the default, the artifact has good test coverage, and only a handful of findbugs issues. Is there any piece that is going to run end-to-end or attempt to load/concurrently test these classes and be more rigorous then the previous system? Can that be made?;;;","11/May/14 20:57;xedin;[~brandon.williams] I have released 0.3.5 just now, with reallocation and on-heap buffers turned on by default. +1 on the change so it's either we commit 0.3.5 or your patch.

[~appodictic] Here is a definition of [unit testing|http://en.wikipedia.org/wiki/Unit_testing], in my tests I cover single/multi connection on-heap/off-heap +/- reallocation scenarios, basically everything to prove that server functions properly in all of the modes and returns correct results based on the operation being used. end-to-end tests are what systems which integrate project are supposed to do and that is done by stress and bpdlab testing. If you have been following discussion in this ticket you must have already realized that the problem is not caused by HsHa server directly but rather by the fact that Cassandra holds Thrift buffers even after blocking evaluation is finished.;;;","12/May/14 04:36;xedin;[~brandon.williams] 0.3.5 is already available on [maven central|http://search.maven.org/remotecontent?filepath=com/thinkaurelius/thrift/thrift-server/0.3.5/thrift-server-0.3.5.jar] so act as you think appropriate.;;;","12/May/14 17:52;appodictic;@Pavel I understand what you are saying. I understand what unit test is. Whenever I submit patches to the ASF they come with tests. ::cough:: ::cough::. In any case, what I was saying is the external dependency does not do load testing. Cassandra does not default to hsha. I DO NOT see any reference in this conversation into how exactly the HSHA server is now load/correctness tested. If such a test exists great, if not potentially should be added.;;;","12/May/14 18:17;appodictic;Also while we are on the topic.

{quote}
# The default is sync because on Windows hsha is about 30% slower.  On Linux,
# sync/hsha performance is about the same, with hsha of course using less memory.
#
# Alternatively,  can provide your own RPC server by providing the fully-qualified class name
# of an o.a.c.t.TServerFactory that can create an instance of it.
rpc_server_type: sync
{quote}

The logic behind this default confuses me. The the vast majority of the cassandra user base is linux. We chose 'sync' so the uncommon case is not slowed down. Clearly anyone using linux should switch to hsha because it uses less memory and is wiched fast according to github tests. But not being the default does it really get performance/correctness evaluated in any meaningful way?

;;;","12/May/14 20:07;benedict;For many workloads sync is faster than async on linux also (by a significant margin), so perhaps the docs should be updated.;;;","13/May/14 18:06;rbranson;I think what might help this specific quality issue out is just moving to the new HSHA implementation entirely in a later version and removing the choice. The new HSHA supposedly eliminates the performance issues that made it not a good default choice, so it appears as if there's no advantage to having the other choices.;;;","13/May/14 18:41;brandon.williams;For 2.1, I can get behind that I think, especially calling it 'disruptor' or pretty much anything besides 'HSHA.'  For 2.0 though it's hard to swallow in a minor.;;;","13/May/14 21:11;rbranson;I did some more digging around on our cluster that was running 2.0.6 when it saw the corruption: it took anywhere from a few hours to 48 hours for the first compaction with the out of order key exception to throw. These nodes are receiving thousands of writes per second, so it's not going to be trivially reproducible. We've been running one of the nodes with 2.0.8-tenative + enable_reallocate_buffers.txt and will report back once we've reached 72 hours and are comfortable rolling this out wide to our own clusters.;;;","19/May/14 18:36;rbranson;Haven't been able to repro in over 5 days. We're considering the enable_reallocate_buffers.txt patch fixed and production-ready.;;;","19/May/14 19:04;brandon.williams;I committed this patch to 2.0, but did not update the disruptor jar for fear of any further regressions, so the patch Rick tested is in there.  For 2.1, I committed both this patch and disruptor 0.3.5.;;;","19/May/14 19:13;brandon.williams;Oops, wait, I only changed the maven dependency.  [~mishail] could you clean up the 2.1+ side of things?;;;","19/May/14 21:35;mishail;[~brandon.williams] done.;;;","19/May/14 21:39;brandon.williams;Thanks.;;;","21/Oct/14 18:41;sterligovak;It looks like this is not fixed in 2.1.0. We have cassandra under heavy load through binary interface and only OpsCenter by thrift. OpsCenter rollups are corrupted in about an hour after scrub.

{quote}
ERROR [CompactionExecutor:71] 2014-10-21 22:16:39,950 CassandraDaemon.java:166 - Exception in thread Thread[CompactionExecutor:71,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(-7581200918995348250, 39352e3130382e3234322e32302d6973732d73686172645f696e666f2d676574426c6f6f6d46696c74657246616c7365506f73697469766573) >= current key DecoratedKey(-8301289422298317140, 800100010000000c62617463685f6d75746174650006d04a0d00010b0d0000000100000025) writing into /ssd/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-tmp-ka-9128-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:172) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:196) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:110) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:177) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
{quote}

We'll try to switch to sync and see what will happen.

Is it possible that streaming hangs because of that exception? Is it possible that this exception affect minor compactions of other keyspaces?;;;","21/Oct/14 18:51;ngrigoriev@gmail.com;[~sterligovak] I was always wondering why did I always see these problems appearing for OpsCenter keyspace. My keyspace had much more traffic but when I had this problem - it always manifested itself with OpsCenter keyspace. Even when I was also using Thrift (we use native protocol now).

I even remember disabling OpsCenter to prove the point :) 

;;;","21/Oct/14 23:37;sterligovak;Have you proven that it's really related to OpsCenter?

We've switched to ""sync"", but still get corrupted sstables. Now we get exception not during compaction, but at start:
{quote}
ERROR [SSTableBatchOpen:10] 2014-10-22 02:47:48,762 CassandraDaemon.java:166 - Exception in thread Thread[SSTableBatchOpen:10,5,main]
java.lang.IllegalStateException: SSTable first key DecoratedKey(4206305143314087741, 800100010000000c62617463685f6d7574617465000010250d00010b0d000000010000004e33372e3134302e3134312e3231322d6973732d736c6f745f636f6e66696775726174696f6e5f746172) > last key DecoratedKey(-4632241097675266745, 800100010000000c62617463685f6d7574617465000010260d00010b0d000000010000005133372e3134302e3134312e3231322d6973732d736c6f745f636f6e66696775726174696f6e5f746172676574)
        at org.apache.cassandra.io.sstable.SSTableReader.validate(SSTableReader.java:1083) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:398) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:294) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableReader$4.run(SSTableReader.java:430) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
{quote}

And nodetools scrub doesn't help. It finds no errors and after restart we get same exceptions.;;;","22/Oct/14 00:56;ngrigoriev@gmail.com;I think this is the error that you cannot fix by scrubbing. Corrupted sstable. I was fixing those by deleting the sstables and doing repairs. Unfortunately, if that happens on many nodes there is a risk of data loss.

As for the OpsCenter - do not get me wrong ;) I did not want to say that OpsCenter was directly responsible for these troubles. But I do believe that OpsCenter does something particular that reveals the bug in hsha server. At least this was my impression. After disabling OpsCenter and fixing the outstanding problems I do not recall seeing those errors anymore. And I was also using Thrift and I was writing and reading 100x more data than OpsCenter.

;;;","22/Oct/14 06:57;xedin;[~sterligovak] Did you get any WARN messages like this ""N out of order rows found while scrubbing <file>; Those have been written (in order) to a new sstable <new-file>"" while running scrub? Anyhow, you will have to delete affected files and repair from the neighbors, I'm also not sure how much of an involvement Thrift has in this because the only thing that could go wrong (shared buffers) was already fixed to be copied for every request and everything is allocated on-heap....

[~rbranson] Are you running HsHa with 2.1 or still on 2.0 ? ;;;","22/Oct/14 07:37;sterligovak;[~xedin] No, I've not seen such messages. sstablescrub failed with NPE. sstables were corrupted on all 17 nodes. I removed them manually and there was no errors overnight. It seems sync really impacted the problem. Maybe there are some another problem which hides with sync server. I still have problems - validation hangs on one table on all nodes :(.;;;","23/Oct/14 18:38;ngrigoriev@gmail.com;By the way, I am getting 

{code}
ERROR [CompactionExecutor:2333] 2014-10-23 18:29:53,590 CassandraDaemon.java (line 199) Exception in thread Thread[Compactio
nExecutor:2333,1,main]
java.lang.RuntimeException: Last written key DecoratedKey(1156541975678546868, 001000000000111100000000000003bc510f000010000
0000003bc510f00000000111100000000100000000000004000000000000000000100) >= current key DecoratedKey(36735936098318717, 001000
0000001111000000000000015feb8a00001000000000015feb8a00000000111100000000100000000000004000000000000000000100) writing into /
cassandra-data/disk2/myks/mytable/myks-mytable-tmp-jb-94445-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:142)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:165)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:160)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:198)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code}

with 2.0.10 release. I am using native protocol. I believe native protocol handler is based on HSHA, am I right? Anyway, I am getting those too.;;;","23/Oct/14 19:12;xedin;[~ngrigoriev] No, native protocol is not using Thrift, which further confirms that it's cross transport problem, I think we should create a separate ticket to handle it.
[~sterligovak] Do you still have stacktrace for the NPE you've got while scrubbing?;;;","24/Oct/14 09:25;sterligovak;[~xedin] That NPE happend once and unfortunatelly I have not saved it. If I'll get it once more I'll save this sstable.
I totally removed OpsCenter keyspace (with sstables) and recreated them. I don't get ""Last written key DecoratedKey"" any more. By the way, this error definetely causees streams to hang on 100%.

I have several strange things happening now:
  - I've noticed that it takes about 30 minutes between ""nodetool repair"" and first pending AntiEntropySession. Is that ok?
  - Repair is already running for 24 hours (~13GB per node, 17 nodes). What's the number of AntiEntropySessions to finish single repair? Number of key ranges?
{quote}
Pool Name                    Active   Pending      Completed   Blocked  All time blocked
CounterMutationStage              0         0              0         0                 0
ReadStage                         0         0         392196         0                 0
RequestResponseStage              0         0        5271906         0                 0
MutationStage                     0         0       19832506         0                 0
ReadRepairStage                   0         0           2280         0                 0
GossipStage                       0         0         453830         0                 0
CacheCleanupExecutor              0         0              0         0                 0
MigrationStage                    0         0              0         0                 0
ValidationExecutor                0         0          39446         0                 0
MemtableReclaimMemory             0         0          29927         0                 0
InternalResponseStage             0         0         588279         0                 0
AntiEntropyStage                  0         0        5325285         0                 0
MiscStage                         0         0              0         0                 0
CommitLogArchiver                 0         0              0         0                 0
MemtableFlushWriter               0         0          29927         0                 0
PendingRangeCalculator            0         0             30         0                 0
MemtablePostFlush                 0         0         135734         0                 0
CompactionExecutor               31        31         502175         0                 0
AntiEntropySessions               3         3           3446         0                 0
HintedHandoff                     0         0             44         0                 0

Message type           Dropped
RANGE_SLICE                  0
READ_REPAIR                  0
PAGED_RANGE                  0
BINARY                       0
READ                         0
MUTATION                     2
_TRACE                       0
REQUEST_RESPONSE             0
COUNTER_MUTATION             0
{quote}
  - Some validation compactions run for more than 100% (1923%). I thinks that it's CASSANDRA-7239, right?
  - the amount of sstables for some CFs is about 15 000 and continues to grow during repair.
  - There are several following exceptions during repair
{quote}
ERROR [RepairJobTask:80] 2014-10-24 13:27:31,717 RepairJob.java:127 - Error occurred during snapshot phase
java.lang.RuntimeException: Could not create snapshot at /37.140.189.163
        at org.apache.cassandra.repair.SnapshotTask$SnapshotCallback.onFailure(SnapshotTask.java:77) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.net.MessagingService$5$1.run(MessagingService.java:347) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
ERROR [AntiEntropySessions:141] 2014-10-24 13:27:31,724 RepairSession.java:303 - [repair #da2cb020-5b5f-11e4-a45e-d9cec1206f33] session completed with the following error
java.io.IOException: Failed during snapshot creation.
        at org.apache.cassandra.repair.RepairSession.failedSnapshot(RepairSession.java:344) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.repair.RepairJob$2.onFailure(RepairJob.java:128) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1172) ~[guava-16.0.jar:na]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
ERROR [AntiEntropySessions:141] 2014-10-24 13:27:31,724 CassandraDaemon.java:166 - Exception in thread Thread[AntiEntropySessions:141,5,RMI Runtime]
java.lang.RuntimeException: java.io.IOException: Failed during snapshot creation.
        at com.google.common.base.Throwables.propagate(Throwables.java:160) ~[guava-16.0.jar:na]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_51]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_51]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_51]
        at java.lang.Thread.run(Thread.java:744) [na:1.7.0_51]
Caused by: java.io.IOException: Failed during snapshot creation.
        at org.apache.cassandra.repair.RepairSession.failedSnapshot(RepairSession.java:344) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.repair.RepairJob$2.onFailure(RepairJob.java:128) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at com.google.common.util.concurrent.Futures$4.run(Futures.java:1172) ~[guava-16.0.jar:na]
        ... 3 common frames omitted
{quote};;;","29/Oct/14 15:43;marcuse;I think the cause of the latest exceptions in this ticket is CASSANDRA-8211;;;","26/Jan/15 21:39;rbfblk;I am getting this exception using Thrift HSHA in 2.1.0:

{quote}
 INFO [CompactionExecutor:8] 2015-01-26 13:32:51,818 CompactionTask.java (line 138) Compacting [SSTableReader(path='/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-2-Data.db'), SSTableReader(path='/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-1-Data.db')]
 INFO [CompactionExecutor:8] 2015-01-26 13:32:51,890 ColumnFamilyStore.java (line 856) Enqueuing flush of compactions_in_progress: 212 (0%) on-heap, 20 (0%) off-heap
 INFO [MemtableFlushWriter:8] 2015-01-26 13:32:51,892 Memtable.java (line 326) Writing Memtable-compactions_in_progress@1155018639(0 serialized bytes, 1 ops, 0%/0% of on/off-heap limit)
 INFO [MemtableFlushWriter:8] 2015-01-26 13:32:51,896 Memtable.java (line 360) Completed flushing /tmp/cass_test/cassandra/TestCassandra/data/system/compactions_in_progress-55080ab05d9c388690a4acb25fe1f77b/system-compactions_in_progress-ka-2-Data.db (42 bytes) for commitlog position ReplayPosition(segmentId=1422296630707, position=430226)
ERROR [CompactionExecutor:8] 2015-01-26 13:32:51,906 CassandraDaemon.java (line 166) Exception in thread Thread[CompactionExecutor:8,1,RMI Runtime]
java.lang.RuntimeException: Last written key DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000) >= current key DecoratedKey(14775611966645399672119169777260659240, 726f776b65793030385f31343232323937313537353835) writing into /tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-tmp-ka-3-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:172) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:196) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:110) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:177) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:74) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:235) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471) ~[na:1.7.0_40]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) ~[na:1.7.0_40]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_40]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
        at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
{quote}

I don't think it's caused by CASSANDRA-8211, because it happens during the first compaction that takes place between the first 2 SSTables to get flushed from an initially empty column family.

Also, I've only been able to reproduce it when using both *hsha* for the rpc server and *offheap_objects* for memtable allocation. If I switch either to sync or to offheap_buffers or heap_buffers then I cannot reproduce the problem. Also under the same circumstances I'm pretty sure I've seen incorrect data being returned to a client multiget_slice request before any SSTables had been flushed yet, so I presume this is corruption that happens before any flush/compaction takes place.

nodetool scrub yielded these errors:

{quote}
 INFO [CompactionExecutor:9] 2015-01-26 13:48:01,512 OutputHandler.java (line 42) Scrubbing SSTableReader(path='/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-2-Data.db') (168780 bytes)
 INFO [CompactionExecutor:10] 2015-01-26 13:48:01,512 OutputHandler.java (line 42) Scrubbing SSTableReader(path='/tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-ka-1-Data.db') (135024 bytes)
 WARN [CompactionExecutor:9] 2015-01-26 13:48:01,531 OutputHandler.java (line 52) Out of order row detected (DecoratedKey(14775611966645399672119169777260659240, 726f776b65793030385f31343232323937313537353835) found after DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000))
 WARN [CompactionExecutor:9] 2015-01-26 13:48:01,534 OutputHandler.java (line 57) Error reading row (stacktrace follows):
java.lang.RuntimeException: Last written key DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000) >= current key DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000) writing into /tmp/cass_test/cassandra/TestCassandra/data/test_ks/test_cf-1c45da40a58911e4826751fbbc77b187/test_ks-test_cf-tmp-ka-4-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:172) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:196) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.append(SSTableRewriter.java:110) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.io.sstable.SSTableRewriter.tryAppend(SSTableRewriter.java:141) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:186) ~[apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:592) [apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager.access$300(CompactionManager.java:100) [apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$3.execute(CompactionManager.java:315) [apache-cassandra-2.1.0.jar:2.1.0]
        at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:270) [apache-cassandra-2.1.0.jar:2.1.0]
        at java.util.concurrent.FutureTask.run(FutureTask.java:262) [na:1.7.0_40]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) [na:1.7.0_40]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) [na:1.7.0_40]
        at java.lang.Thread.run(Thread.java:724) [na:1.7.0_40]
 WARN [CompactionExecutor:9] 2015-01-26 13:48:01,534 OutputHandler.java (line 52) Row starting at position 25342 is unreadable; skipping to next
 WARN [CompactionExecutor:10] 2015-01-26 13:48:01,534 OutputHandler.java (line 52) Out of order row detected (DecoratedKey(29459452031265566667651334397450214244, 726f776b65793030355f31343232323936393033323837) found after DecoratedKey(131206587314004820534098544948237170809, 800100010000000c62617463685f6d7574617465000000))

etc...
{quote}

EDIT: I copied my comment to a new issue (CASSANDRA-8719) since this issue one is long closed;;;"
Wrong tracking of minLevel in Leveled Compaction Strategy causing serious performance problems,CASSANDRA-6284,12677083,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,JiriHorky,JiriHorky,JiriHorky,01/Nov/13 15:15,16/Apr/19 09:32,14/Jul/23 05:53,22/Nov/13 23:07,2.0.4,,,,,,0,lcs,,,,"Hi,

since version 2.0.0 (incl. beta), Leveled Compaction Strategy contains a hard-to-spot bug in choosing of sstable candidates to be compacted with tables in higher level. It always chooses first sstable in L1 and only first 1/10 of sstables in other higher levels.

This is caused by an error when determining ""minLevel"" of compacted tables in replace() function in LeveledManifest.java which is then used as an index to lastCompactedKeys array to ensure sort of ""round robin"" selection of SStables for compaction in each level. In the newer versions the minLevel is computed as the minimum of levels of newly created sstables instead of the old sstables. 
Typically compaction takes one table from L(X), compacts it with N tables in L(X+1) and produces M tables in L(X+1). Thus, the lastCompactedKey is improperly accounted to one level higher then it should be.

This causes serious performance problems as the uniform token range distribution across sstables in one level is broken.
In L1, the first SStable is always chosen to be compacted with overlapping tables in L2. Since a newly created tables in L0 contains practically whole range of keys of a given node, and the rest of ~9 tables in L1 are never pushed to the higher levels, they tend to contain higher and higher keys over time in very narrow token range. As a direct consequence, the first (the chosen) SStable in L1 (after a compaction of L1 tables with the L0 table) thus contains much wider range than anticipated ~1/10 , which forces compaction with many more tables in L2 than normally expected due to bigger overlap.
The similar problem appears in higher levels as well.

We noticed gradual performance degradation since we upgraded C* from 1.2.9 to 2.0.0 aprox. 1 month ago which we tracked down to increased compaction activity. We noticed that the number of sstables processed in one compaction is much higher than expected. The compaction IO activity in our case is more than 5 higher than in 1.2.9 version and only becomes worse.",,JiriHorky,rcoli,vjevdokimov,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Nov/13 15:23;JiriHorky;LeveledManifest.bug.6284.patch;https://issues.apache.org/jira/secure/attachment/12611617/LeveledManifest.bug.6284.patch",,,,,,,,,,,,,,,,,,,,1.0,JiriHorky,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356459,,,Fri Nov 22 23:07:58 UTC 2013,,,,,,,,,,"0|i1pg93:",356747,2.0.0,2.0.1,2.0.2,2.0 beta 1,2.0 beta 2,2.0 rc1,2.0 rc2,,jbellis,,jbellis,Normal,,2.0 beta 1,,,,,,,,,,,,,,,,"01/Nov/13 15:21;JiriHorky;The attached patch fixes this bug. The patch applies to 2.0.0. as well as to 2.0.2.;;;","01/Nov/13 16:52;jbellis;Patch LGTM.  Can you add a test case to LeveledCompactionStrategyTest?;;;","01/Nov/13 20:05;rcoli;Is 2.0.0 beta is the correct ""since"" for this ticket?;;;","01/Nov/13 20:30;jbellis;Probably.  Git annotate if you're not sure.;;;","01/Nov/13 21:26;rcoli;For the record :

git annotate says CASSANDRA-4872 introduces the MAX_LEVEL line fixed in the second part of the patch.

So ""since"" 2.0.1 beta 1 is correct. :);;;","04/Nov/13 09:56;JiriHorky;Regarding adding the test case - I think that the right test should  insert enough data to have something in L3, and then try to go through all tables in L1 and L2 and see 1) whether all sstables contain more or less the right token range (1/10 for L1 and 1/100 for L2) with some (possibly relatively high) tolerance and 2) that both levels effectively contain the whole token range the node is responsible for. Unfortunately, I won't be able to contribute that sooner than in a week and can't really promise that it will be good enough afterwards (I am not a Java programmer :-);;;","22/Nov/13 23:07;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows 7 data files kept open / can't be deleted after compaction.,CASSANDRA-6283,12677050,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,JoshuaMcKenzie,Andie78,Andie78,01/Nov/13 12:52,16/Apr/19 09:32,14/Jul/23 05:53,01/May/14 00:11,2.0.7,2.1.0,2.2.0 beta 1,Legacy/Local Write-Read Paths,Local/Compaction,,3,compaction,Windows,,,"Files cannot be deleted, patch CASSANDRA-5383 (Win7 deleting problem) doesn't help on Win-7 on Cassandra 2.0.2. Even 2.1 Snapshot is not running. The cause is: Opened file handles seem to be lost and not closed properly. Win 7 blames, that another process is still using the file (but its obviously cassandra). Only restart of the server makes the files deleted. But after heavy using (changes) of tables, there are about 24K files in the data folder (instead of 35 after every restart) and Cassandra crashes. I experiminted and I found out, that a finalizer fixes the problem. So after GC the files will be deleted (not optimal, but working fine). It runs now 2 days continously without problem. Possible fix/test:
I wrote the following finalizer at the end of class org.apache.cassandra.io.util.RandomAccessReader:

{code:title=RandomAccessReader.java|borderStyle=solid}
@Override
protected void finalize() throws Throwable {
	deallocate();
	super.finalize();
}
{code}

Can somebody test / develop / patch it? Thx.",Windows 7 (32) / Java 1.7.0.45,Andie78,bamboo82,chander,daniel.nuriyev,dominik.stadler@gmx.at,graham sanderson,JoshuaMcKenzie,jre,PuerTea,ravilr,vongocminh,wtmitchell3,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-7080,,,,,CASSANDRA-5383,CASSANDRA-5950,CASSANDRA-4050,CASSANDRA-7541,CASSANDRA-3613,,,,,,,,,,,"25/Feb/14 20:55;JoshuaMcKenzie;6283_StreamWriter_patch.txt;https://issues.apache.org/jira/secure/attachment/12631037/6283_StreamWriter_patch.txt","23/Nov/13 03:08;graham sanderson;leakdetect.patch;https://issues.apache.org/jira/secure/attachment/12615443/leakdetect.patch","05/Mar/14 18:38;Andie78;neighbor-log.zip;https://issues.apache.org/jira/secure/attachment/12632870/neighbor-log.zip","05/Mar/14 18:38;Andie78;root-log.zip;https://issues.apache.org/jira/secure/attachment/12632869/root-log.zip","07/Nov/13 15:26;Andie78;screenshot-1.jpg;https://issues.apache.org/jira/secure/attachment/12612628/screenshot-1.jpg","07/Nov/13 15:37;Andie78;system.log;https://issues.apache.org/jira/secure/attachment/12612629/system.log",,,,,,,,,,,,,,,6.0,JoshuaMcKenzie,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356426,,,Wed Oct 22 16:02:58 UTC 2014,,,,,,,,,,"0|i1pg1r:",356714,2.0.2,2.0.3,2.1 rc3,,,,,,jbellis,,jbellis,Normal,,2.0.1,,,,,,,,,,,,,,,,"07/Nov/13 15:26;Andie78;nodetool repair <ks> -pr causes neighbour-node to crash. Win7 file problem on neighbour node as well (screenshot-1.jpg). See following comment.;;;","07/Nov/13 15:37;Andie78;system.log after nodetool repair at neighbour (snipet):
{panel:title=system.log}
""FSWriteError ... Caused by: java.nio.file.FileSystemException: D:\Programme\cassandra\data\nieste\timezones\snapshots\dac98330-47bc-11e3-b167-eb1c24a59bb8\nieste-timezones-jb-8-Index.db: Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird.""
{panel}
Means, cassandra didn't close the file after last access. This Win7-file-access issue seems to affect important areas in cassandra. Is there a plan to fix it? Thx.;;;","12/Nov/13 15:09;Andie78;Another relying critical issue: Cassandra can't delete cache files.
{panel:title=system.log}
""WARN [CompactionExecutor:53] 2013-11-10 21:41:59,862 AutoSavingCache.java (line 277) Failed to delete D:\Programme\cassandra\saved_caches\system-schema_columns-KeyCache-b.db""

That leads to a crash on next start, system.log:
""ERROR [main] 2013-11-12 11:02:43,654 CassandraDaemon.java (line 478) Exception encountered during startup
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:394)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:355)
	at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:352)
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:119)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:264)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:409)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:386)
	at org.apache.cassandra.db.index.AbstractSimplePerColumnSecondaryIndex.init(AbstractSimplePerColumnSecondaryIndex.java:52)
	at org.apache.cassandra.db.index.SecondaryIndexManager.addIndexedColumn(SecondaryIndexManager.java:274)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:279)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:409)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:381)
	at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:314)
	at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:268)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
	at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:274)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:461)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)""
{panel};;;","14/Nov/13 16:13;Andie78;I have no Jira response yet. Do I need to put an assignee or somebody will join up? Is there a plan to fix this in 2.0.3 ? Thx for any information!;;;","19/Nov/13 03:35;mishail;It could be related to http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=4831749, which means the only way to ""fix"" it is to set {{disk_access_mode: standard}};;;","19/Nov/13 04:13;jbellis;That could be, but we try to munmap before deleting.  Is there a bug there?;;;","19/Nov/13 11:29;Andie78;Thx for reply. I  tried already disk_access_mode standard and mmap. Same result. Maybe WinXP and Linux don't care about closing files after reading but Win7?;;;","19/Nov/13 20:36;mishail;[~Andie78] Do you you see ""can't delete"" exceptions only of files like {{\snapshots\dac98330-47bc-11e3-b167-eb1c24a59bb8\}} (i.e. from a snapshots subfolder) or with other files as well?

I suspect that CASSANDRA-4050 causes {{doValidationCompaction}} to fail (and entire RepairSession as well). - as far as I understand REPAIR without ""parallel"" option (which is default  behavior) creates a snapshot, validates and then deletes the snapshot. The last step fails on WIndows;;;","19/Nov/13 21:27;mishail;It can be related to CASSANDRA-6275 as well;;;","20/Nov/13 10:42;Andie78;I see ""can't delete"" in regular compaction as well. It happens, when I make a lot of changes to a CF and Cassandra crashes. See my first comment. Without programming that finalizer, which calls ""deallocate()"", I couldn't use cassandra. I think in general, the file handling has to be programmed more accurately like my finalizer shows. Means closing all files direct after access. What about resource objects in java, which are responsible for closing files latest on GC?;;;","20/Nov/13 16:31;jre;There's a patch in CASSANDRA-6275 that should resolve the issue you're seeing.  Can you test it?;;;","20/Nov/13 20:22;mishail;[~Andie78] regarding your 1st comment
bq. nodetool repair <ks> -pr causes neighbour-node to crash

here is how I see it:

* Your nodes are running with {{disk_failure_policy=stop}}
* You started a repair on a node
* Repair tasks do a ""validation"" and creates a snapshot of a column familly
* After ""validation"" the repair tasks tries to clean up the snapshot and fails.
* The fail is a ""disk failure"" and causes the node to stop. (because {{disk_failure_policy=stop}} )

Do you still observe the problem with you GC-patch or with the patch for CASSANDRA-6275?;;;","21/Nov/13 11:52;Andie78;I will test ""2.0.3-tentative"".;;;","21/Nov/13 13:23;Andie78;I'm actuall importing logfiles (Heavy writing into one KS, Heavy changing into other KS (states of imported files)). The result with the pach for CASSANDRA-6275:

- After 100 imported logfiles no Delete-Error in System.log :-)
- After some delay old files are deleted. :-)

nodetool repair I didn't test yet.

Would like to test my (heavy) import over weekend. I will update my whole cluster to 2.0.3-tentative and set up nodetool repair jobs. I will inspect all system-log files and tell the result here.;;;","22/Nov/13 12:06;Andie78;During my importing I got the warnings:
{panel:title=system.log}
 WARN [CompactionExecutor:13] 2013-11-22 03:10:49,864 AutoSavingCache.java (line 277) Failed to delete D:\Programme\cassandra\saved_caches\nieste-nfiles-KeyCache-b.db
{panel}
and other cache files too. Later the same:
{panel:title=system.log}
 WARN [CompactionExecutor:22] 2013-11-22 07:10:49,896 AutoSavingCache.java (line 277) Failed to delete D:\Programme\cassandra\saved_caches\nieste-nfiles-KeyCache-b.db
{panel};;;","22/Nov/13 17:30;Andie78;I deployed now 2.0.3-tentative on the whole cluster. Result: With nodetool repair patch for CASSANDRA-6275 doen't work. Neighbour nodes crash again with disk_failure_policy=stop.
{panel:title=system.log}
ERROR [ValidationExecutor:3] 2013-11-22 18:21:49,591 FileUtils.java (line 417) Stopping gossiper
 WARN [ValidationExecutor:3] 2013-11-22 18:21:49,591 StorageService.java (line 279) Stopping gossip by operator request
ERROR [ValidationExecutor:4] 2013-11-22 18:21:50,361 Validator.java (line 242) Failed creating a merkle tree for [repair #923a7360-539a-11e3-8fde-eb1c24a59bb8 on nieste/evrangesdevice, (-787066926799647148,-773294852829911898]], /10.9.9.240 (see log for details)
ERROR [ValidationExecutor:4] 2013-11-22 18:21:50,371 CassandraDaemon.java (line 187) Exception in thread Thread[ValidationExecutor:4,1,main]
FSWriteError in D:\Programme\cassandra\data\nieste\evrangesdevice\snapshots\923a7360-539a-11e3-8fde-eb1c24a59bb8\nieste-evrangesdevice-jb-9-Index.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:120)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:382)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:378)
	at org.apache.cassandra.db.Directories.clearSnapshot(Directories.java:416)
	at org.apache.cassandra.db.ColumnFamilyStore.clearSnapshot(ColumnFamilyStore.java:1801)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:810)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.FileSystemException: D:\Programme\cassandra\data\nieste\evrangesdevice\snapshots\923a7360-539a-11e3-8fde-eb1c24a59bb8\nieste-evrangesdevice-jb-9-Index.db: Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird.

	at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(Unknown Source)
	at sun.nio.fs.AbstractFileSystemProvider.delete(Unknown Source)
	at java.nio.file.Files.delete(Unknown Source)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:116)
	... 11 more
ERROR [ValidationExecutor:4] 2013-11-22 18:21:50,371 FileUtils.java (line 417) Stopping gossiper
 WARN [ValidationExecutor:4] 2013-11-22 18:21:50,371 StorageService.java (line 279) Stopping gossip by operator request
ERROR [ValidationExecutor:2] 2013-11-22 18:21:51,221 FileUtils.java (line 423) Stopping RPC server
ERROR [ValidationExecutor:2] 2013-11-22 18:21:51,221 FileUtils.java (line 429) Stopping native transport
{panel};;;","22/Nov/13 17:56;Andie78;Actual I have only 4 nodes. But I want to enlarge my cluster with different racks (rooms with regular Desktop-PCs in different buildings in the same area). For that, I need to switch to NetworkTopologySnitch as well. Is repair with disk_failure_policy=ignore effective in that case? What can I do in my production cluster? Avoid repairing or repair and just ignore the delete-errors?;;;","22/Nov/13 19:18;mishail;bq. Avoid repairing or repair and just ignore the delete-errors?
That doesn't look like a viable approach. I would try to run repairs with {{-par}} option. If I read the code correctly then a snapshots will not be used in this case.

{code}
- name: repair [keyspace] [cfnames]
    help: |
      Repair one or more column families
         Use -pr to repair only the first range returned by the partitioner.
         Use -par to carry out a parallel repair.
{code}

But to be honest - I really don't know what are other differences between a sequential and parallel repair ;;;","22/Nov/13 19:37;mishail;[~Andie78] BTW, did you try to run a repair with your finalizer-patch? CASSANDRA-6275 fixed one leak and there might be others;;;","22/Nov/13 21:57;Andie78;Not yet. Just read some of cassandra source and Im not familiar with cassandra at all. That finalizer patch is only emergency and doesnt fix the source of the problem (lost file handels).;;;","22/Nov/13 22:10;Andie78;btw I read somewhere, in java it is not a nice program style to write finalizers (but in C++ destrutors are common, for example used in resource objects, which automatic always close resources). whats your opinion about finalizers in java?;;;","23/Nov/13 03:08;graham sanderson;For what it is worth, if you think there are other file leaks, this is the change I made locally to determine the allocation stack trace for CASSANDRA-6275 ... obviously from there you have some work to do.;;;","23/Nov/13 03:11;graham sanderson; But yes finalizers in java are a very bad idea, and not like destructors in any realistic way - so better to determine the source of your leak (maybe my attached patch will help, but don't keep that around when you are done - exception stack traces are very expensive) and fix that. Of course if the finalizer enables you to have a functioning cluster, then that is a good step, but it wouldn't be the correct fix in the codebase.;;;","28/Nov/13 12:29;graham sanderson;I am confused, did CASSANDRA-6275 make things worse? If not, or if in any way you still need the finalizer change to fix the problem, then apply my leakdetect.patch, and search logs for LEAK and post here. It has the same finalizer behavior it just logs where the RARs that have to be be cleaned up by finalizer were allocated.;;;","29/Nov/13 10:01;Andie78;{panel:title=Short brief}
- No compaction problems anymore (CASSANDRA-6275 fixed it),
- Still cache-deleting problem (See my comment at 22/Nov/13 13:06),
- Still repair problem when using snapshot-files (without -par option) (see comment of Mikhail Stepura at 20/Nov/13 21:22).
{panel}
I applied the leakdetect.patch. I will comment the results.;;;","29/Nov/13 11:31;Andie78;I think I can cancel repair without -par. After repairing one keyspace I got al lot of errors but not the leak-detect-messages. I think, not RAR is responsible during repair. The error on the Validation Node:
{panel:title=system.log}
ERROR [ValidationExecutor:1] 2013-11-29 12:15:38,370 Validator.java (line 242) Failed creating a merkle tree for [repair #92b6ccb0-58e7-11e3-aac3-b13a5fe180aa on nieste/niesteplants, (-5215786285174483271,-5206407297765302700]], /10.6.8.78 (see log for details)
ERROR [ValidationExecutor:1] 2013-11-29 12:15:38,370 CassandraDaemon.java (line 187) Exception in thread Thread[ValidationExecutor:1,1,main]
FSWriteError in D:\Programme\cassandra\data\nieste\niesteplants\snapshots\92b6ccb0-58e7-11e3-aac3-b13a5fe180aa\nieste-niesteplants-jb-19-Data.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:120)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:382)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:378)
	at org.apache.cassandra.db.Directories.clearSnapshot(Directories.java:416)
	at org.apache.cassandra.db.ColumnFamilyStore.clearSnapshot(ColumnFamilyStore.java:1801)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:810)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.FileSystemException: D:\Programme\cassandra\data\nieste\niesteplants\snapshots\92b6ccb0-58e7-11e3-aac3-b13a5fe180aa\nieste-niesteplants-jb-19-Data.db: Der Prozess kann nicht auf die Datei zugreifen, da sie von einem anderen Prozess verwendet wird.

	at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(Unknown Source)
	at sun.nio.fs.AbstractFileSystemProvider.delete(Unknown Source)
	at java.nio.file.Files.delete(Unknown Source)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:116)
	... 11 more
{panel}
The error on the repair-node:
{panel:title=system.log}
ERROR [AntiEntropySessions:1] 2013-11-29 12:15:38,419 RepairSession.java (line 278) [repair #92b6ccb0-58e7-11e3-aac3-b13a5fe180aa] session completed with the following error
org.apache.cassandra.exceptions.RepairException: [repair #92b6ccb0-58e7-11e3-aac3-b13a5fe180aa on nieste/niesteplants, (-5215786285174483271,-5206407297765302700]] Validation failed in /10.9.9.69
	at org.apache.cassandra.repair.RepairSession.validationComplete(RepairSession.java:152)
	at org.apache.cassandra.service.ActiveRepairService.handleMessage(ActiveRepairService.java:188)
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:59)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [AntiEntropySessions:1] 2013-11-29 12:15:38,429 CassandraDaemon.java (line 187) Exception in thread Thread[AntiEntropySessions:1,5,RMI Runtime]
java.lang.RuntimeException: org.apache.cassandra.exceptions.RepairException: [repair #92b6ccb0-58e7-11e3-aac3-b13a5fe180aa on nieste/niesteplants, (-5215786285174483271,-5206407297765302700]] Validation failed in /10.9.9.69
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: org.apache.cassandra.exceptions.RepairException: [repair #92b6ccb0-58e7-11e3-aac3-b13a5fe180aa on nieste/niesteplants, (-5215786285174483271,-5206407297765302700]] Validation failed in /10.9.9.69
	at org.apache.cassandra.repair.RepairSession.validationComplete(RepairSession.java:152)
	at org.apache.cassandra.service.ActiveRepairService.handleMessage(ActiveRepairService.java:188)
	at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:59)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
	... 3 more
{panel}
{panel:title=NodeTool}
Starting NodeTool
[2013-11-29 12:15:37,329] Starting repair command #1, repairing 256 ranges for keyspace nieste
[2013-11-29 12:19:46,219] Repair session 92b6ccb0-58e7-11e3-aac3-b13a5fe180aa for range (-5215786285174483271,-5206407297765302700] failed with error org.apache.cassandra.exceptions.RepairException: [repair #92b6ccb0-58e7-11e3-aac3-b13a5fe180aa on nieste/niesteplants, (-5215786285174483271,-5206407297765302700]] Validation failed in /10.9.9.69
[2013-11-29 12:19:46,219] Repair session 935b9830-58e7-11e3-aac3-b13a5fe180aa for range (8782854129978293476,8784492934430401685] failed with error org.apache.cassandra.exceptions.RepairException: [repair #935b9830-58e7-11e3-aac3-b13a5fe180aa on nieste/niesteplants, (8782854129978293476,8784492934430401685]] Validation failed in /10.9.9.69
[2013-11-29 12:19:46,219] Repair session 93b725b0-58e7-11e3-aac3-b13a5fe180aa for range (-5799639987122737930,-5786898749415113092] failed with error org.apache.cassandra.exceptions.RepairException: [repair #93b725b0-58e7-11e3-aac3-b13a5fe180aa on nieste/nfiles, (-5799639987122737930,-5786898749415113092]] Validation failed in /10.9.9.240
[2013-11-29 12:19:46,229] Repair session 94b16430-58e7-11e3-aac3-b13a5fe180aa for range (2664807773952357126,2669403005419855407] failed with error org.apache.cassandra.exceptions.RepairException: [repair #94b16430-58e7-11e3-aac3-b13a5fe180aa on nieste/nfiles, (2664807773952357126,2669403005419855407]] Validation failed in /10.9.9.240
[2013-11-29 12:19:46,229] Repair session 9503c9f0-58e7-11e3-aac3-b13a5fe180aa for range (-6420115574437655437,-6410524043851626540] finished
{panel}
Im curious, if cache-file-deleting will write leak-messages...
I'm using C* 2.0.3-release with leak-detection.patch.;;;","02/Dec/13 12:01;Andie78;Over weekend I got the following:
{panel:title=system.log}
ERROR [STREAM-IN-/10.6.8.78] 2013-11-29 17:57:20,266 StreamSession.java (line 410) [Stream #ea9bd2c0-5916-11e3-a5b5-b13a5fe180aa] Streaming error occurred
java.lang.RuntimeException: Outgoing stream handler has been closed
	at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(ConnectionHandler.java:175)
	at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:436)
	at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
	at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:293)
	at java.lang.Thread.run(Unknown Source)
 WARN [STREAM-IN-/10.6.8.78] 2013-11-29 17:57:20,266 StreamResultFuture.java (line 210) [Stream #ea9bd2c0-5916-11e3-a5b5-b13a5fe180aa] Stream failed
ERROR [NonPeriodicTasks:1] 2013-11-29 17:58:54,266 SSTableDeletingTask.java (line 81) Unable to delete D:\Programme\cassandra\data\system\schema_columns\system-schema_columns-jb-468-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR [NonPeriodicTasks:1] 2013-11-29 17:58:54,266 SSTableDeletingTask.java (line 81) Unable to delete D:\Programme\cassandra\data\system\schema_columns\system-schema_columns-jb-469-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR [NonPeriodicTasks:1] 2013-11-29 17:58:58,446 SSTableDeletingTask.java (line 81) Unable to delete D:\Programme\cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-jb-525-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR [NonPeriodicTasks:1] 2013-11-29 17:58:58,446 SSTableDeletingTask.java (line 81) Unable to delete D:\Programme\cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-jb-527-Data.db (it will be removed on server restart; we'll also retry after GC)
{panel}
The deleting problem occurs only once. So my GC patch seems to work. But there's no log of the leakdetect-patch.
{panel:title=system.log 10.6.8.78}
ERROR [CompactionExecutor:40] 2013-11-29 17:56:16,368 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:40,1,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:61)
	at java.util.concurrent.ThreadPoolExecutor.reject(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.execute(Unknown Source)
	at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:145)
	at java.util.concurrent.AbstractExecutorService.submit(Unknown Source)
	at org.apache.cassandra.db.ColumnFamilyStore.switchMemtable(ColumnFamilyStore.java:752)
	at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:817)
	at org.apache.cassandra.db.SystemKeyspace.forceBlockingFlush(SystemKeyspace.java:420)
	at org.apache.cassandra.db.SystemKeyspace.finishCompaction(SystemKeyspace.java:197)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:225)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel};;;","13/Jan/14 12:47;Andie78;Hello,
with 2.0.4 and leak detect patch I got expected errors
{panel:title=system.log}
ERROR [Finalizer] 2014-01-13 13:13:15,033 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-3418-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.<init>(CompressedThrottledReader.java:34)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.open(CompressedThrottledReader.java:48)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1355)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1161)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1173)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:244)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:250)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:126)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [Finalizer] 2014-01-13 13:13:15,053 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4984-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,073 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4984-Index.db allocated
...
ERROR [CompactionExecutor:2] 2014-01-13 13:13:15,073 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.IllegalArgumentException: bufferSize must be positive
...
ERROR [Finalizer] 2014-01-13 13:13:15,083 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5067-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,123 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5067-Index.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,133 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4302-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,153 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4302-Index.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,163 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4981-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,173 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4981-Index.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,183 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5068-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,193 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5068-Index.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,193 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4959-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:13:15,203 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4959-Index.db allocated
...
ERROR [CompactionExecutor:1] 2014-01-13 13:13:15,223 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.IllegalArgumentException: bufferSize must be positive
...
ERROR [Finalizer] 2014-01-13 13:13:15,243 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-4991-Index.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:13:15,243 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5064-Data.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:13:15,253 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5064-Index.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:13:15,263 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5062-Data.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:13:15,263 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5062-Index.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:13:15,273 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbydevice\events-eventsbydevice-jb-5066-Data.db allocated
...
ERROR [Finalizer] 2014-01-13 13:14:24,334 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\KSlogdata\CFlogdata\KSlogdata-CFlogdata-jb-26894-Data.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:14:54,544 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\evrangesncom\events-evrangesncom-jb-349-Data.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:14:54,554 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\evrangesdevice\events-evrangesdevice-jb-939-Data.db allocated
	...
ERROR [Finalizer] 2014-01-13 13:14:54,574 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbyplant\events-eventsbyplant-jb-795-Data.db allocated

and more...
{panel};;;","13/Jan/14 18:12;mishail;[~Andie78] Could you please post the full stack stack trace for this exception.
{quote}
ERROR [CompactionExecutor:2] 2014-01-13 13:13:15,073 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.IllegalArgumentException: bufferSize must be positive
...
{quote};;;","14/Jan/14 10:32;Andie78;Hello,

this problem is occurred, when node was not shutdown probably. As I know, that issue is known as CASSANDRA-6531. Here is the stack trace:
{panel:title=system.log}
ERROR [ReadStage:2385] 2014-01-14 10:57:11,875 CassandraDaemon.java (line 187) Exception in thread Thread[ReadStage:2385,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: bufferSize must be positive
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.lang.IllegalArgumentException: bufferSize must be positive
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:75)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:43)
	at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.createReader(CompressedPoolingSegmentedFile.java:48)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1195)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:57)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1516)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1335)
	at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:245)
	at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:105)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1710)
	at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:53)
	at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:537)
	at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1698)
	at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:39)
	... 4 more
ERROR [Finalizer] 2014-01-14 10:57:12,005 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\nieste\niesteinverters\nieste-niesteinverters-jb-2669-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:43)
	at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.createReader(CompressedPoolingSegmentedFile.java:48)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1195)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:57)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1516)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1335)
	at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:245)
	at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:105)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1710)
	at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:53)
	at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:537)
	at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1698)
	at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:39)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [CompactionExecutor:446] 2014-01-14 11:02:57,342 CassandraDaemon.java (line 187) Exception in thread Thread[CompactionExecutor:446,1,main]
java.lang.IllegalArgumentException: bufferSize must be positive
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:75)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.<init>(CompressedThrottledReader.java:34)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.open(CompressedThrottledReader.java:48)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1355)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1161)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1173)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:244)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:250)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:126)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [Finalizer] 2014-01-14 11:02:57,552 RandomAccessReader.java (line 398) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\KSlogdata\CFlogdata\KSlogdata-CFlogdata-jb-32763-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.<init>(CompressedThrottledReader.java:34)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.open(CompressedThrottledReader.java:48)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1355)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1161)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1173)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:244)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:250)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:126)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:60)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:197)
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel};;;","14/Jan/14 17:42;Andie78;Yesterday I updated one node with 2.0.4-rel incl. finalizer-patch (see results above). Nodetool repair -par caused node to repair ""endless"" and collecting about 65K Files in datafolder. I updated now to pre-2.0.5 from today (commit f6f50ddffe0821617fe29482f9ec918608560381). After starting, a lot of LEAK messages and File-Not-Found messages appeared in system.log. But files reduce.
{panel:title=system.log (pre-2.0.5)}
ERROR [SSTableBatchOpen:1] 2014-01-14 18:18:42,753 CassandraDaemon.java:139 - Exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.RuntimeException: java.io.FileNotFoundException: D:\Programme\cassandra\data\KSlogdata\CFlogdata\KSlogdata-CFlogdata-jb-27051-Index.db (Das System kann die angegebene Datei nicht finden)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:109) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:97) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader.buildSummary(SSTableReader.java:595) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:575) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:527) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:328) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:230) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader$4.run(SSTableReader.java:364) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[na:1.7.0_25]
	at java.lang.Thread.run(Unknown Source) ~[na:1.7.0_25]
Caused by: java.io.FileNotFoundException: D:\Programme\cassandra\data\KSlogdata\CFlogdata\KSlogdata-CFlogdata-jb-27051-Index.db (Das System kann die angegebene Datei nicht finden)
	at java.io.RandomAccessFile.open(Native Method) ~[na:1.7.0_25]
	at java.io.RandomAccessFile.<init>(Unknown Source) ~[na:1.7.0_25]
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:105) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	... 13 common frames omitted
...
ERROR [Finalizer] 2014-01-14 18:27:45,076 RandomAccessReader.java:401 - LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\system\compactions_in_progress\system-compactions_in_progress-ka-5012-Statistics.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:65) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:105) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:97) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:88) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.metadata.MetadataSerializer.deserialize(MetadataSerializer.java:98) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.sstable.SSTableReader.getApproximateKeyCount(SSTableReader.java:167) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:125) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:66) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:59) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:198) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at java.util.concurrent.Executors$RunnableAdapter.call(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.FutureTask.run(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source) ~[na:1.7.0_25]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) ~[na:1.7.0_25]
	at java.lang.Thread.run(Unknown Source) ~[na:1.7.0_25]
...
ERROR [main] 2014-01-14 18:27:45,446 CassandraDaemon.java:435 - Exception encountered during startup
java.lang.NullPointerException: null
	at org.apache.cassandra.db.Directories.<init>(Directories.java:192) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:487) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:211) [apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:418) [apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:505) [apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
>>Cassandra shutted down<<
{panel}
I deleted files in folder ""\data\system\compactions_in_progress"". Now its not shutting down anymore. Still leak failures, no ""FileNotFound"" yet. ""bufferSize must be positive"" is still appearing (CASSANDRA-6531 seems not to work).;;;","17/Feb/14 16:08;JoshuaMcKenzie;I've reproduced leaked file handles on repair in a lab on 2.0.5 w/leakfinalizer.patch.  Nodes start up without issue - I'm not seeing any LEAK or File-Not-Found on regular init, and the LEAK aren't showing up until repair kicks off.  W/the finalizer patch repair runs through to completion.  Andreas - have you had a chance to try out 2.0.5 w/the patch yet?

lastly - the leaks I'm seeing look like they're all isolated to a single case - streaming data outbound during the repair process:
{code:title=error|borderStyle=solid}
ERROR [Finalizer] 2014-02-17 09:21:52,922 RandomAccessReader.java (line 399) LEAK finalizer had to clean up
java.lang.Exception: RAR for C:\var\lib\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-jb-41-CRC.db allocated
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
        at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:106)
        at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:98)
        at org.apache.cassandra.io.util.DataIntegrityMetadata$ChecksumValidator.<init>(DataIntegrityMetadata.java:53)
        at org.apache.cassandra.io.util.DataIntegrityMetadata.checksumValidator(DataIntegrityMetadata.java:40)
        at org.apache.cassandra.streaming.StreamWriter.write(StreamWriter.java:76)
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:59)
        at org.apache.cassandra.streaming.messages.OutgoingFileMessage$1.serialize(OutgoingFileMessage.java:42)
        at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:45)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:383)
        at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:355)
        at java.lang.Thread.run(Thread.java:744)
{code}

Andreas - could you confirm whether or not this matches what you're seeing in your environment?  I'm curious if this is a dangling file handle like we've seen in other related tickets or if this is perhaps a race condition on access Windows is intolerant of.;;;","17/Feb/14 19:45;Andie78;Actually Im still running 2.0.5 Snapshot with my finalizer patch. During normal operation no fault, but I didn't check the logs in the last 2 weeks yet. The repair jobs I start with -par option. I will update to 2.0.5-rel with my patch and test repair w/o -par option and compare the logs to yours. Correct my plan, if necessary.;;;","17/Feb/14 23:58;JoshuaMcKenzie;With -par on the test.  Without -par I'm seeing the same errors you were seeing above indicating that snapshots have open handles:

{code:title=Non-par|borderStyle=solid}
ERROR [ValidationExecutor:3] 2014-02-17 17:52:57,092 Validator.java (line 242) Failed creating a merkle tree for [repair #99973cf0-982e-11e3-9370-639bcb1c8d6c on Keyspace1/Standard1, (-390084131511610885,-345083722760460
251]], /10.193.84.101 (see log for details)
ERROR [ValidationExecutor:3] 2014-02-17 17:52:57,092 CassandraDaemon.java (line 192) Exception in thread Thread[ValidationExecutor:3,1,main]
FSWriteError in \var\lib\cassandra\data\Keyspace1\Standard1\snapshots\99973cf0-982e-11e3-9370-639bcb1c8d6c\Keyspace1-Standard1-jb-37-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:120)
        at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:382)
        at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:378)
        at org.apache.cassandra.db.Directories.clearSnapshot(Directories.java:416)
        at org.apache.cassandra.db.ColumnFamilyStore.clearSnapshot(ColumnFamilyStore.java:1881)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:810)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
Caused by: java.nio.file.FileSystemException: \var\lib\cassandra\data\Keyspace1\Standard1\snapshots\99973cf0-982e-11e3-9370-639bcb1c8d6c\Keyspace1-Standard1-jb-37-Data.db: The process cannot access the file because it is
 being used by another process.
{code}

I want to confirm that repair w/-par on 2.0.5 doesn't give you much trouble, if any, and then tackle -par vs. non separately.;;;","18/Feb/14 02:45;graham sanderson;Unless I'm missing something, the code at StreamWriter.java:76

{code}
            validator = DataIntegrityMetadata.checksumValidator(sstable.descriptor);
{code}

creates a file reference that is never closed, which seems like it is a bug on all platforms - of course it might not manifest in the same way (especially if multiple threads/timing are involved).;;;","18/Feb/14 12:21;Andie78;I updated 2 of 8 nodes to 2.0.5-rel, with finalizer patch and LEAK-logging. Result: After Start a lot of LEAK-finalizer Errors like:
{panel:title=start-up node}
ERROR [Finalizer] 2014-02-18 12:53:42,388 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
ERROR [Finalizer] 2014-02-18 12:53:42,388 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\system\schema_keyspaces\system-schema_keyspaces-jb-433-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:55)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1362)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1147)
	at org.apache.cassandra.db.RowIteratorFactory.getIterator(RowIteratorFactory.java:69)
	at org.apache.cassandra.db.ColumnFamilyStore.getSequentialIterator(ColumnFamilyStore.java:1599)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1718)
	at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1656)
	at org.apache.cassandra.db.SystemKeyspace.serializedSchema(SystemKeyspace.java:767)
	at org.apache.cassandra.db.DefsTables.loadFromKeyspace(DefsTables.java:121)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:525)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:242)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
ERROR [Finalizer] 2014-02-18 12:53:42,388 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\system\local\system-local-jb-168-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:43)
	at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.createReader(CompressedPoolingSegmentedFile.java:48)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1195)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.<init>(SimpleSliceReader.java:57)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:250)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1560)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1379)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:327)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
	at org.apache.cassandra.cql3.statements.SelectStatement.readLocally(SelectStatement.java:232)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:250)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:58)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:255)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:526)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:233)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
{panel}
After Starting >>nodetool repair -par nieste<< I got again a lot of such messages (""nieste"" is my cql3 KS):
{panel:title=with -par}
ERROR [Finalizer] 2014-02-18 13:00:43,961 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\nieste\nfiles\nieste-nfiles-jb-1150-Index.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:103)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:90)
	at org.apache.cassandra.io.util.BufferedPoolingSegmentedFile.createReader(BufferedPoolingSegmentedFile.java:45)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:162)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:143)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:936)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:871)
	at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:783)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1186)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1174)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:252)
	at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:888)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:787)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [Finalizer] 2014-02-18 13:00:44,371 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\nieste\niesteinverters\nieste-niesteinverters-jb-1492-Index.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:103)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:95)
	at org.apache.cassandra.io.sstable.SSTableReader.openIndexReader(SSTableReader.java:1369)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:97)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1190)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1174)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getScanners(LeveledCompactionStrategy.java:194)
	at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:888)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:787)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel}
On the neighbour-node I got the same errors:
{panel:title=neighbour}
ERROR [Finalizer] 2014-02-18 13:03:18,203 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\nieste\niesteinverters\nieste-niesteinverters-jb-3271-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.<init>(CompressedThrottledReader.java:34)
	at org.apache.cassandra.io.compress.CompressedThrottledReader.open(CompressedThrottledReader.java:48)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1355)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:96)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1190)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1174)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getScanners(LeveledCompactionStrategy.java:194)
	at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:888)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:787)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
ERROR [Finalizer] 2014-02-18 13:03:27,543 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\nieste\niesteplants\nieste-niesteplants-jb-7-Index.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:103)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:90)
	at org.apache.cassandra.io.util.BufferedPoolingSegmentedFile.createReader(BufferedPoolingSegmentedFile.java:45)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:162)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:143)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:936)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:871)
	at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:783)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1186)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1174)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:252)
	at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:888)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:787)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel}
Repair succeed. So only lost filehandles. The strings ""delete"" or ""FSWriteError"" don't appear in any of these logs (w/-par).;;;","18/Feb/14 15:16;JoshuaMcKenzie;Thanks for checking that Andreas - I'll move forward with the assumption that what I've reproduced in the lab is just a small subset of the problems you're seeing in prod and go off your logs as reference.;;;","18/Feb/14 17:05;Andie78;You're welcome! :-);;;","18/Feb/14 21:30;JoshuaMcKenzie;A couple of thoughts - linux allows you to delete a file even though another process has a handle to it, keeping a reference in the /proc filesystem (http://archive09.linux.com/articles/58142) until application shutdown.  I'm thinking it's possible that on linux FileUtils.deleteWithConfirm is deleting a file that another thread has a handle to and we're none the wiser.

Commenting out the thrown exception in that function removes all LEAK messages and other errors for me when running nodetool.bat repair without -par, and after a run I have snapshot files laying around that can't be deleted but don't show up in the handle search in process explorer.  Stopping java makes them deletable so it's clearly a JVM handle thing...;;;","19/Feb/14 11:42;Andie78;I think, not stopping java in general allows deleting, but stopping the cassandra process which keeps file handles open. When I first studied the C* code, I found it difficult to see, how filehandles are spread around in different classes (?). Not easy, to keep the overview for me, but maybe its because of not enough java experience compared to C++ where I had to care for handles as well (file, memory). Later in C++, there were autopointers (boost/TR1) with a finalizer-like approach, where memory was deleted automatically (analog to closing files), when handle went out of scope or the programmer ""forgot"" to free/close. I don't know about the linux' process explorer, where filehandles are expected. But if I probably close every filehandle after use (even only reading), does it matter, what's in this process explorer, as long C* is able to delete? BTW if only a read process don't close the file handle, it makes already sense for me to forbid to delete, as long the os knows, the file is still used any way, except the user has to kill or restart the process, where the handles automatically disappear, mostly caused by problems. For me, it's a failure of the os as manager, to allow delete a file, as long there are any filehandles on it. This strict permission allows me as well, to detect failures in my program logic.;;;","19/Feb/14 14:55;JoshuaMcKenzie;There's pros and cons to either side of the debate - there's some benefits to being able to change file handles while in use such as being able to run updates, file recovery, etc.  Apparently this is a throwback in OS design to MS-DOS 3.3 as far as file-level locking is concerned.

Coming from a C++ background myself I know what you mean w/smart pointers and RAII-type resource management, but I believe the problem we're running into here is language independent.  We have atomic reference-counting implementations in the SSTableReaders to allow multiple concurrent read-only access to the data structures which would be a complicated implementation regardless of language choice.

I mentioned process explorer not seeing the file handle lock because I found it an oddity - my expectation is that anything from Sysinternals and Russinovich is bullet-proof, so I'm wondering how the OS got into a state where a file is locked yet I can't query the process that has said lock, even though stopping the JVM clearly released it.;;;","19/Feb/14 21:54;JoshuaMcKenzie;Referencing Mikhail's earlier comment - this looks like the same problem from CASSANDRA-4050 where hard links can't be deleted on NTFS if a process has the file or another hard link locked.  Code inspection for snapshot process looks clean and that would explain the intermittent locking issue where some snapshots remove without issue and others fail.

Referencing http://superuser.com/questions/301303/one-hardlink-is-locked-how-do-i-remove-the-other and the comments in the other ticket - on Windows we'll need to account for gracefully failing file deletion on snapshot cleanup, move those hard links to a $CWD/tmp folder for instance, and then clean up that folder on process shutdown.;;;","19/Feb/14 22:12;jbellis;So if we have a handle open to the original file still, we can't delete the snapshot hardlink either?;;;","19/Feb/14 23:29;JoshuaMcKenzie;Looks like that's the case, though the documentation for DeleteFile (http://msdn.microsoft.com/en-us/library/windows/desktop/aa363915(v=vs.85).aspx) doesn't indicate anything about hard links to open files being non-deletable.  Every use case I'm digging up shows that that's the case however:

http://superuser.com/questions/678357/how-to-delete-windows-ntfs-hard-link-mklink-h-while-original-is-in-use

There's an ecosystem of tools surrounding working around this type of thing on Windows - PendMoves and MoveFile from sysinternals, for instance

We could go a route similar to FileUtils.closeQuietly with a FileUtils.deleteQuietly or removeHardLink to more clearly communicate intent, try to delete it, move it to a tmp location if failure, and then either periodically retry delete on the links or remove them at shutdown.;;;","20/Feb/14 09:49;Andie78;So the repair issue (with snapshots) is another problem compared to the compaction issue, where my finalizer patch works perfectly in normal operation and shows, that handles are not closed from C* ? Relying to compaction issue, if C* generates a tmp folder, there will be thousands of files (I had for instance more than 64K files in one KS w/o finalizer patch). Is that a good idea for the operating system, when C* is running for weeks?;;;","20/Feb/14 16:17;JoshuaMcKenzie;It's possible the two are the same underlying problem.  We use hardlinks both during the snapshot process and during flushing of memtables - I'll have to think for a bit on the best way to test that theory out since your stack shows us the LEAK being cleaned up but doesn't (and can't) give us any indication of who it is that might still hold the reference to the handle.

As for the tmp folder and file growth - the hard links won't be kept open in the FD table for java so we shouldn't run the risk of an FD limit, however the drive space would still be used until the process was shut down.  I was thinking the addition of a periodic ""delete tmp/moved_hard_links/*"" type pass should help mitigate that, though it smells hacky to me.

Another option would be copying files instead of hard linking them during these operations on Windows only, but I think the performance ramifications of that make that option unworkable.;;;","20/Feb/14 23:16;JoshuaMcKenzie;As a final confirmation of this behavior I tested this locally.  I created and locked a file named test.txt (opened it), and then hard linked and attempted to delete it.

C:\Users\jmckenzie\Desktop\FileLocker>mklink /H link.txt test.txt
Hardlink created for link.txt <<===>> test.txt

C:\Users\jmckenzie\Desktop\FileLocker>del link.txt
C:\Users\jmckenzie\Desktop\FileLocker\link.txt
The process cannot access the file because it is being used by another process.

On Windows, if the original file is locked the hard link cannot be deleted even if it's not in use.

I'll start prototyping a sweep and delete model.  We have some unit tests that are failing on Windows with similar stacks to what Andreas is getting on startup so I'll look into those along with this.;;;","21/Feb/14 10:18;Andie78;Sounds interesting, I didn't work with such links yet. Can somebody check that behavior on Windows XP (32) as well? Before migrating to Win7, we didn't have that problems. Our nodes run on Win7 (32 Bit).;;;","21/Feb/14 12:30;jbellis;(Supporting XP is not a goal so it would be of academic interest only.);;;","21/Feb/14 12:40;Andie78;(Yep, but I thought, the difference can be the key to the solution :-) );;;","21/Feb/14 22:49;JoshuaMcKenzie;Regarding the LEAK on repair -par:  graham sanderson on the 17th appeared to be on the mark.  Adding FileUtils.closeQuietly(validator) in the finally block on StreamWriter.java plugged all LEAK messages from nodetool.bat repair -par.  I've started looking at the other leaks Andreas linked on here and the 1st I tracked down doesn't look like it should be leaking:
{code:title=LEAK message}
ERROR [Finalizer] 2014-02-18 12:53:42,388 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\system\schema_keyspaces\system-schema_keyspaces-jb-433-Data.db allocated
at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:55)
at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1362)
at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:67)
at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1147)
at org.apache.cassandra.db.RowIteratorFactory.getIterator(RowIteratorFactory.java:69)
at org.apache.cassandra.db.ColumnFamilyStore.getSequentialIterator(ColumnFamilyStore.java:1599)
at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1718)
at org.apache.cassandra.db.ColumnFamilyStore.getRangeSlice(ColumnFamilyStore.java:1656)
at org.apache.cassandra.db.SystemKeyspace.serializedSchema(SystemKeyspace.java:767)
at org.apache.cassandra.db.DefsTables.loadFromKeyspace(DefsTables.java:121)
at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:525)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:242)
at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:462)
at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:552)
{code}
Which should be closed at:
{code:title=ColumnFamilyStore.filter()}
...
        finally
        {
            try
            {
                rowIterator.close();
                Tracing.trace(""Scanned {} rows and matched {}"", total, matched);
            }
...
{code}

I'll look into the LEAK messages on some of the others you got Andreas and see if the code where the stack was captured looks like it leaks the handle.

Regarding the locked file handles on snapshots: It's not looking great for our options on hard links for the snashots on non-par repair.  On win7 I basically can't touch hard links to locked files which is different than the note mentioned on StackExchange:

{code:title=win7 move / delete}
C:\Users\jmckenzie\Desktop\FileLocker>mklink/H link.txt test.txt
Hardlink created for link.txt <<===>> test.txt
C:\Users\jmckenzie\Desktop\FileLocker>rename link.txt newlink.txt
The process cannot access the file because it is being used by another process.
C:\Users\jmckenzie\Desktop\FileLocker>move link.txt newlink.txt
The process cannot access the file because it is being used by another process.
        0 file(s) moved.
{code}

And as for behavior on winxp - it looks like this behavior was the same there as far as not being able to delete hard links to locked files:
{code:title=winxp delete hard link}
C:\Documents and Settings\jmckenzie\Desktop>fsutil hardlink create link.txt test.txt
Hardlink created for C:\Documents and Settings\jmckenzie\Desktop\link.txt <<===>
> C:\Documents and Settings\jmckenzie\Desktop\test.txt
C:\Documents and Settings\jmckenzie\Desktop>del link.txt
C:\Documents and Settings\jmckenzie\Desktop\link.txt
The process cannot access the file because it is being used by another process.
{code}

I believe this is why we have SSTableDeletingTask.java and the recurrent runs on CMS GC and rescheduling self.  We could follow the implementation pattern and have a SnapshotFileDeletingTask and register w/StorageService.tasks the same way we do on SSTableDeleting, but that doesn't solve the locked file handle move problems we have unfortunately.  It doesn't look like we use FileUtils.renameWithConfirm() in too many places in our code-base so it might be ok to document that as unsafe on Windows and deal with the other case.;;;","25/Feb/14 20:53;JoshuaMcKenzie;With our current io implementation, snapshots won't be deletable as long as the original sstable is locked.  It's going to take a writing of a new FileDataInput based on FileChannel w/jdk7 using the FILE_SHARE_DELETE flag to allow deletion of hard links while the original file is open (thanks for the heads up on that Jonathan).

Bug with behavior: http://bugs.java.com/view_bug.do?bug_id=6607535
jdk7 support: http://www.docjar.com/html/api/sun/nio/fs/WindowsChannelFactory.java.html

I'm attaching a patch to fix the repair -par leak and will work on tracking down some of those startup leaks as I'm seeing those as well.;;;","26/Feb/14 23:47;JoshuaMcKenzie;Andreas - do you have steps to reproduce the LEAK's traced to doValidationCompaction on 2.0.5 w/nodetool.bat repair -par <KS>?  The code surrounding that resource looks like it shouldn't have leaked the way your log indicates.;;;","27/Feb/14 15:56;JoshuaMcKenzie;Re: your other 2.0.5 leaks - A trace on the code surrounding loadFromKeyspace also looks clean, as does the one sourcing from cql3...readLocally.  Short of the JVM dying during a finally block, google's Cache not firing its removal event listener, or our MergeIterator close implementation leaking things - tag 2.0.5 looks like it shouldn't be producing these LEAKS.

If we had file handle leaks in our core select statements in CQL3 and Keyspace initialization I'd expect to be seeing them in local testing.;;;","05/Mar/14 12:57;Andie78;Hello,
since I don't know all code areas of C*, I describe, what I tested to reproduce: I cleaned system.log and used again C* 2.0.5-rel with LEAK detection and finalizer-patch in RAR.java. After starting again C* w/o doing anything I got a lot LEAK messages. I waited until C* finished his own work (mainly compacting I think). Now I started repair -par. Result are a lot of LEAK messages. Here the first one:

{panel:title=nodetool repair -par events}
ERROR [Finalizer] 2014-03-05 13:45:25,932 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\eventsbyproject\events-eventsbyproject-jb-2002-Index.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:103)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:90)
	at org.apache.cassandra.io.util.BufferedPoolingSegmentedFile.createReader(BufferedPoolingSegmentedFile.java:45)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:162)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:143)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:936)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:871)
	at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:783)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1186)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1174)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:252)
	at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:888)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:787)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel}
{panel:title=neighbor node}
ERROR [Finalizer] 2014-03-05 13:50:54,061 RandomAccessReader.java (line 394) LEAK finalizer had to clean up 
java.lang.Exception: RAR for D:\Programme\cassandra\data\events\evrangesdevice\events-evrangesdevice-jb-905-Index.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:63)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:103)
	at org.apache.cassandra.io.util.RandomAccessReader.open(RandomAccessReader.java:90)
	at org.apache.cassandra.io.util.BufferedPoolingSegmentedFile.createReader(BufferedPoolingSegmentedFile.java:45)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:162)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:143)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:936)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:871)
	at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:788)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1186)
	at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReader.java:1174)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:252)
	at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompactionIterable.<init>(CompactionManager.java:888)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:787)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{panel}
Repair successfully finished. If I can make more tests, let me know. After Thursday I will be on holiday for 3 weeks and in office again at Mon, 03/31/2014.;;;","05/Mar/14 15:01;JoshuaMcKenzie;Could you attach the most recent system.log from the root and neighbor nodes to this ticket?  Might help see if there's anything else going on there in the environment involved in this.;;;","05/Mar/14 18:38;Andie78;I attached logs (root-log.zip and neighbor-log.zip) during >nodetool repair -par events<. C* 2.0.5-rel with LEAK-log and finalizer-patch under Win-7.;;;","10/Mar/14 18:54;JoshuaMcKenzie;I've created CASSANDRA-6832 regarding the leak on repair -par to get a patch into the code-base for it without being blocked on this.

Andreas - those logs have 100+megs of the same stack traces repeatedly which unfortunately doesn't help me much.  I was hoping to see some information about process start-up in there.  Do you have a small dataset I could use to attempt to reproduce the errors you're seeing?  Alternatively - if you start up a clean test cluster on 2.0.5 in your environment do you see those errors?;;;","04/Apr/14 22:02;JoshuaMcKenzie;On CASSANDRA-4050 we're converting our RandomAccessReader to use nio which should fix the ""can't delete a hard-link while original file is open"" for most use-cases.  Unfortunately you cannot delete hard-linked files on Windows if you have a memory-mapped segment in the original file - I've done some benchmarking on CASSANDRA-6890 regarding removing memory mapped I/O and the performance cost / feature loss is high enough that we're going to keep it for now.

I'll put together a patch for this ticket to create something similar to an SSTableDeletingTask for a snapshot folder - walk the files and try to delete them, re-scheduling a job to try and clear this folder again after a GC if there's any failures due to access violations.  That combined with CASSANDRA-4050 should give us immediate and full clear on compressed cf's and partial / incrementally improving snapshot clearing on snapshots where there's memory mapped readers to the original sstables.

I don't like having partially cleared out snapshots floating around on the file-system though.  I'd guess this will cause some confusion for people in the future.;;;","07/Apr/14 13:12;Andie78;That means in short words, deletion during normal operation should work? And nodetool compact w/o -par should work as well? Tell me, if it's ready to test in my environment.;;;","07/Apr/14 15:18;JoshuaMcKenzie;Deletion during normal operation will fail gracefully and schedule retry if the original file is open.  This combined with CASSANDRA-4050 on compressed CF's should immediately delete the snapshot files and work on repair, otherwise the snapshots will hang around on disk until the original readers are closed / unmapped and sstables compacted.

Not ready to test yet - I'll post a patch when it's ready.;;;","07/Apr/14 21:07;jbellis;bq. That combined with CASSANDRA-4050 should give us immediate and full clear on compressed cf's and partial / incrementally improving snapshot clearing on snapshots where there's memory mapped readers to the original sstables.

I'd rather disallow mmaped i/o on Windows entirely pending making buffered i/o speed-competitive with mmap (at which point we can drop mmap entirely).

-1 on ""snapshot deletion scheduler"" band aids.;;;","07/Apr/14 21:43;JoshuaMcKenzie;I originally got the idea from the SSTableDeletingTask implementation (as mentioned prior) but that works too.

As of CASSANDRA-6907 we currently have snapshot-based repair disabled for Windows in 2.0.7+, so do we even need to disable the mmap'ed I/O path on Windows prior to optimizing buffered I/O and removing mmap as an option?;;;","07/Apr/14 21:55;jbellis;I'd love to get rid of SSTDT so at least I'm consistent. :)

Pretty sure we always use mmap on index files, so we'd need to fix that, and we'd also want to make sure we use buffered i/o even on uncompressed tables.;;;","07/Apr/14 22:13;JoshuaMcKenzie;Erring on the side of less complexity is never a bad thing in my book

To be safe leaving mmap in we'd have to disable snapshot deletion via nodetool to go along with removing it from repair which smells super-hacky - I'm on board (edit: with disabling mmap on Windows).  I'll create a ticket for removing mmap on index files and disabling mmap'ed I/O on Windows.  We reference 4050 and that from here and I think this ticket's done.  ;;;","01/May/14 00:11;JoshuaMcKenzie;Reference CASSANDRA-6993 and CASSANDRA-4050 for patches, CASSANDRA-6890 for performance #'s.;;;","14/Jul/14 09:34;bamboo82;Similar issue reappeared in Windows 2012, but fine in Windows 2008 R2. Could someone confirm it?   It seemed to be related to clearSnapshot during Repair phrase
Running 2.0.9

ERROR [ValidationExecutor:3] 2014-07-13 17:24:28,631 CassandraDaemon.java (line 199) Exception in thread Thread[ValidationExecutor:3,1,main]
FSWriteError in \cassandra\1\data\Commenter3\PostSource_Index\snapshots\3796e3c0-0aed-11e4-8d77-1f5ea6ee0a78\Commenter3-PostSource_Index-ic-21-Index.db
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:122)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:384)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:380)
	at org.apache.cassandra.db.Directories.clearSnapshot(Directories.java:488)
	at org.apache.cassandra.db.ColumnFamilyStore.clearSnapshot(ColumnFamilyStore.java:1877)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:811)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:63)
	at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:398)
	at java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
	at java.util.concurrent.FutureTask.run(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.nio.file.FileSystemException: \cassandra\1\data\Commenter3\PostSource_Index\snapshots\3796e3c0-0aed-11e4-8d77-1f5ea6ee0a78\Commenter3-PostSource_Index-ic-21-Index.db: The process cannot access the file because it is being used by another process.

	at sun.nio.fs.WindowsException.translateToIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsException.rethrowAsIOException(Unknown Source)
	at sun.nio.fs.WindowsFileSystemProvider.implDelete(Unknown Source)
	at sun.nio.fs.AbstractFileSystemProvider.delete(Unknown Source)
	at java.nio.file.Files.delete(Unknown Source)
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:118)
	... 12 more;;;","14/Jul/14 15:30;JoshuaMcKenzie;Was that repair on a range of keys rather than a keyspace?;;;","14/Jul/14 15:56;JoshuaMcKenzie;Created CASSANDRA-7541 to track this.;;;","21/Oct/14 20:43;daniel.nuriyev;Any hope to have this fixed in the current version? Inability to delete unused files on windows is critical and leads to accumulation of gigabytes on the disk. Waiting for version 3.0 is too long.;;;","21/Oct/14 23:49;JoshuaMcKenzie;[~daniel.nuriyev] What unused files are you unable to delete on 2.1 / how are they created?  Other than manually created snapshots we should have snapshot-based operations (repair specifically) bypassed which is where these problems originate.

The changes in CASSANDRA-4050 that truly fix this issue are very low-level and too high risk to put into the 2.X branch unfortunately.;;;","22/Oct/14 14:35;daniel.nuriyev;I had a bunch of these in the logs:
ERROR [NonPeriodicTasks:1] 2014-10-20 12:46:02,107 SSTableDeletingTask.java (line 81) Unable to delete var\lib\cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-jb-1-Data.db (it will be removed on server restart; we'll also retry after GC)

Which means that older files are not deleted after compacting into a bigger file.

Does the fix of this JIRA ticket cover this issue?
;;;","22/Oct/14 16:02;JoshuaMcKenzie;A combination of CASSANDRA-4050 and CASSANDRA-6993 should resolve this issue.  The inability to delete sstables without node restart is a known issue in the 2.X line and will be resolved in 3.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE in PrecompactedRow.update(PrecompactedRow.java:171),CASSANDRA-6277,12676860,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,vilda,vilda,31/Oct/13 14:15,16/Apr/19 09:32,14/Jul/23 05:53,31/Oct/13 14:24,2.0.3,,,,,,0,repair,,,,"Getting this AE on destination nodes during repair:

ERROR [ValidationExecutor:78] 2013-10-31 04:35:31,243 CassandraDaemon.java (line 187) Exception in thread Thread[ValidationExecutor:78,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.PrecompactedRow.update(PrecompactedRow.java:171)
        at org.apache.cassandra.repair.Validator.rowHash(Validator.java:198)
        at org.apache.cassandra.repair.Validator.add(Validator.java:151)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:799)
        at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:62)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:397)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
","Linux, 12 nodes, 3 AZ EC2
Cassandra version 2.0.2",rcoli,vilda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356236,,,Mon Nov 18 15:31:09 UTC 2013,,,,,,,,,,"0|i1pevj:",356524,2.0.2,,,,,,,,,,,Low,,2.0.0,,,,,,,,,,,,,,,,"31/Oct/13 14:24;jbellis;Fixed in cf8fa6e11bf16ffed84a8805c4bc63077d6a7bcf;;;","14/Nov/13 20:15;rcoli;Does this affect all repair in 2.0.0-2.0.2, or just certain cases? If just certain cases, how common do we estimate they are?;;;","18/Nov/13 15:31;vilda;My experience running 2.0.2: 21 AEs during repair on all 9 nodes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: Map can not be created with the same name as a previously dropped list,CASSANDRA-6276,12676851,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,blerer,gryphius,gryphius,31/Oct/13 12:37,16/Apr/19 09:32,14/Jul/23 05:53,11/Aug/14 16:24,2.0.10,,,,,,2,cql,,,,"If create a list, drop it and create a map with the same name, i get ""Bad Request: comparators do not match or are not compatible.""

{quote}
cqlsh:os_test1> create table thetable(id timeuuid primary key, somevalue text);
cqlsh:os_test1> alter table thetable add mycollection list<text>;  
cqlsh:os_test1> alter table thetable drop mycollection;
cqlsh:os_test1> alter table thetable add mycollection map<text,text>;  
Bad Request: comparators do not match or are not compatible.
{quote}


"," Cassandra 2.0.2 | CQL spec 3.1.0
centos 64 bit
Java(TM) SE Runtime Environment (build 1.7.0-b147)
",aleksey,blerer,Daniel Smedegaard Buus,gryphius,khahn,masumsoft,rcoli,slebresne,thebrenthaines,tupshin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Aug/14 15:35;slebresne;6272-2.0.txt;https://issues.apache.org/jira/secure/attachment/12660998/6272-2.0.txt","05/Aug/14 20:25;blerer;CASSANDRA-6276-V2.txt;https://issues.apache.org/jira/secure/attachment/12659938/CASSANDRA-6276-V2.txt","12/Jul/14 14:14;blerer;CASSANDRA-6276.txt;https://issues.apache.org/jira/secure/attachment/12655402/CASSANDRA-6276.txt",,,,,,,,,,,,,,,,,,3.0,blerer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356227,,,Tue Sep 20 01:36:43 UTC 2016,,,,,,,,,,"0|i1petj:",356515,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"31/Oct/13 13:15;brandon.williams;I think we're looking at CASSANDRA-5202 here.;;;","31/Oct/13 13:39;jbellis;5202 is drop/recreate of an entire table.;;;","23/Jan/14 10:23;Daniel Smedegaard Buus;This is quite frustrating when developing and debugging with large portions of data that need to be re-inserted every time you have a schema change because you have to drop an entire CF to make minor schema changes like this...

Is there a workaround one could use until this is resolved?

Thanks :);;;","05/Jun/14 20:52;jbellis;/cc [~slebresne];;;","31/Jul/14 12:53;aleksey;Unfortunately, we can't allow dropping a component from the comparator, including dropping individual collection columns from ColumnToCollectionType.

If we do allow that, and have pre-existing data of that type, C* simply wouldn't know how to compare those:

{code:title=ColumnToCollectionType.java}
    public int compareCollectionMembers(ByteBuffer o1, ByteBuffer o2, ByteBuffer collectionName)
    {
        CollectionType t = defined.get(collectionName);
        if (t == null)
            throw new RuntimeException(ByteBufferUtil.bytesToHex(collectionName) + "" is not defined as a collection"");

        return t.nameComparator().compare(o1, o2);
    }
{code}

A simple algorithm to hit the RTE:
1. create table test (id int primary key, col1 map<int, int>, col2 set<int>);
2. insert into test (id, col1, col2) VALUES ( 0, \{0:0, 1:1\}, \{0,1\});
3. flush
4. update test set col1 = col1 + \{2:2\}, col2 = col2 + \{2\} where id = 0;
5. flush
6. select * from test;

{noformat}
java.lang.RuntimeException: 636f6c31 is not defined as a collection
	at org.apache.cassandra.db.marshal.ColumnToCollectionType.compareCollectionMembers(ColumnToCollectionType.java:79) ~[main/:na]
	at org.apache.cassandra.db.composites.CompoundSparseCellNameType$WithCollection.compare(CompoundSparseCellNameType.java:296) ~[main/:na]
	at org.apache.cassandra.db.composites.AbstractCellNameType$1.compare(AbstractCellNameType.java:61) ~[main/:na]
	at org.apache.cassandra.db.composites.AbstractCellNameType$1.compare(AbstractCellNameType.java:58) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$Candidate.compareTo(MergeIterator.java:154) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$Candidate.compareTo(MergeIterator.java:131) ~[main/:na]
{noformat}

For this reason alone we can't allow getting rid of a comparator component.

However, even if we did, and allowed to create a different collection with the same name, we'd hit a different issue: the new collection's comparator would be used to compare potentially incompatible types. Now, your unit tests aren't failing b/c most of our comparators assume valid values and don't perform extra validation, then use something like ByteBufferUtil.compareUnsigned() to compare the values, which doesn't fail and will just stop once the shortest BB gets exhausted. One exception is tuples/usertypes - they *do* expect at least length to be there, and will throw an exception.

Example:
1. create table test (id int primary key, col set<boolean>);
2. insert into test (id, col) values (0, \{true,false\});
3. alter table test drop col;
4. create type test (f1 int);
5. alter table test add col set<test>;
6. update test set col = col + \{ \{f1 : 0 \} \} where id = 0;
7. select * from test;

{noformat}
java.nio.BufferUnderflowException: null
	at java.nio.Buffer.nextGetIndex(Buffer.java:498) ~[na:1.7.0_65]
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:355) ~[na:1.7.0_65]
	at org.apache.cassandra.db.marshal.TupleType.compare(TupleType.java:80) ~[main/:na]
	at org.apache.cassandra.db.marshal.TupleType.compare(TupleType.java:38) ~[main/:na]
	at org.apache.cassandra.db.marshal.ColumnToCollectionType.compareCollectionMembers(ColumnToCollectionType.java:81) ~[main/:na]
	at org.apache.cassandra.db.composites.CompoundSparseCellNameType$WithCollection.compare(CompoundSparseCellNameType.java:296) ~[main/:na]
	at org.apache.cassandra.db.composites.AbstractCellNameType$1.compare(AbstractCellNameType.java:61) ~[main/:na]
	at org.apache.cassandra.db.composites.AbstractCellNameType$1.compare(AbstractCellNameType.java:58) ~[main/:na]
	at org.apache.cassandra.utils.MergeIterator$Candidate.compareTo(MergeIterator.java:154) ~[main/:na]
{noformat};;;","31/Jul/14 12:58;aleksey;So the best we can do is provide a better exception message, unfortunately.

This also unfortunately complicates CASSANDRA-6717, since we don't store the comparator itself anymore and reconstruct it from the columns we have, and the current dropped_columns map only has names and drop timestamps in it. This means we must store the type of the dropped columns as well, in case some of those are collections, which is annoying.;;;","04/Aug/14 17:16;slebresne;You're right, I'm not sure there is much we can do about this.

bq. So the best we can do is provide a better exception message, unfortunately.

I agree. Let's detect that case and provide a meaningful error message for now.

In the longer run, one way to maybe fix this would be to push dropped_columns down to the sstable reading level. If we were skipping cells as soon as they are deserialized before they ever hit any comparator we would be free to update the comparator. Probably messy/inefficient to do with the current code, but might become feasible with 3.0 storage engine changes, we'll see. ;;;","04/Aug/14 17:21;aleksey;bq. In the longer run, one way to maybe fix this would be to push dropped_columns down to the sstable reading level. If we were skipping cells as soon as they are deserialized before they ever hit any comparator we would be free to update the comparator. Probably messy/inefficient to do with the current code, but might become feasible with 3.0 storage engine changes, we'll see. 

True. Would still be tricky somewhat, given that schema updates don't propagate instantly, but we'll see.;;;","04/Aug/14 17:22;benedict;3.0 storage engine won't be universal for a while (maybe never for thrift), but will index directly into columns (i.e. won't touch any not requested), so could trivially avoid retrieving data for dropped columns. The only problem is we'd need to track the range of sstables for which they were previously dropped (and maybe contains stale data), and which we now apply the new comparator too, which would be a bit ugly/annoying.;;;","05/Aug/14 20:25;blerer;This patch check if a collection of a different type and with the same name already existed and if it is the case it will send an error with the following message:
""Cannot add a collection with the name <collectionName> because a collection with the same name and a different type has already been used in the past""
;;;","11/Aug/14 15:35;slebresne;Patch looks good, though adding a method to {{CellNameType}} feels a tad overkill to me, I'd rather keep the validation in {{AlterTableStatement}} (it's arguably a personal preference). The other reason being that we should fix this in 2.0 and we don't have {{CellNameType}} there. So anyway, attaching a slightly simpler alternative for 2.0. If we're good with that, I'll push a simple dtest so 2.0 is covered and I'll include the tests from [~blerer] patch while merging with 2.1 (since CqlTester is not in 2.0).;;;","11/Aug/14 15:37;aleksey;2.0 patch LGTM;;;","11/Aug/14 16:24;slebresne;Alright, committed, thanks;;;","04/Aug/16 13:06;masumsoft;Facing the same problem in cassandra v3.7. Btw, it works when you drop the column, recreate a non collection column such as int with the same name and then drop that again. After that you can add the same name column with a different collection type and cassandra allows the operation. Dirty workaround though!;;;","20/Sep/16 01:36;aleksey;Dirty but also unsafe. There is a reason the limitation is there in the first place - try to 'work around it' and risk corruption.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2.0.x leaks file handles,CASSANDRA-6275,12676842,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,graham.sanderson,ash2k,ash2k,31/Oct/13 11:14,16/Apr/19 09:32,14/Jul/23 05:53,20/Nov/13 16:53,2.0.3,,,,,,6,,,,,"Looks like C* is leaking file descriptors when doing lots of CAS operations.

{noformat}
$ sudo cat /proc/15455/limits
Limit                     Soft Limit           Hard Limit           Units    
Max cpu time              unlimited            unlimited            seconds  
Max file size             unlimited            unlimited            bytes    
Max data size             unlimited            unlimited            bytes    
Max stack size            10485760             unlimited            bytes    
Max core file size        0                    0                    bytes    
Max resident set          unlimited            unlimited            bytes    
Max processes             1024                 unlimited            processes
Max open files            4096                 4096                 files    
Max locked memory         unlimited            unlimited            bytes    
Max address space         unlimited            unlimited            bytes    
Max file locks            unlimited            unlimited            locks    
Max pending signals       14633                14633                signals  
Max msgqueue size         819200               819200               bytes    
Max nice priority         0                    0                   
Max realtime priority     0                    0                   
Max realtime timeout      unlimited            unlimited            us 
{noformat}

Looks like the problem is not in limits.

Before load test:
{noformat}
cassandra-test0 ~]$ lsof -n | grep java | wc -l
166

cassandra-test1 ~]$ lsof -n | grep java | wc -l
164

cassandra-test2 ~]$ lsof -n | grep java | wc -l
180
{noformat}

After load test:
{noformat}
cassandra-test0 ~]$ lsof -n | grep java | wc -l
967

cassandra-test1 ~]$ lsof -n | grep java | wc -l
1766

cassandra-test2 ~]$ lsof -n | grep java | wc -l
2578
{noformat}

Most opened files have names like:
{noformat}
java      16890 cassandra 1636r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1637r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1638r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1639r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1640r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1641r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1642r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1643r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1644r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1645r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1646r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1647r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1648r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1649r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1650r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1651r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1652r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1653r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1654r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
java      16890 cassandra 1655r      REG             202,17 161158485     655420 /var/lib/cassandra/data/system/paxos/system-paxos-jb-255-Data.db
java      16890 cassandra 1656r      REG             202,17  88724987     655520 /var/lib/cassandra/data/system/paxos/system-paxos-jb-644-Data.db
{noformat}

Also, when that happens it's not always possible to shutdown server process via SIGTERM. Have to use SIGKILL.

p.s. See mailing thread for more context information https://www.mail-archive.com/user@cassandra.apache.org/msg33035.html","java version ""1.7.0_25""
Java(TM) SE Runtime Environment (build 1.7.0_25-b15)
Java HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)
Linux cassandra-test1 2.6.32-279.el6.x86_64 #1 SMP Thu Jun 21 15:00:18 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux",annesull,ash2k,baldrick,capncrunch4me,christianmovi,corwin,gianlucaborello,graham sanderson,jre,makhr,marcuse,mishail,mshuler,pieterc,rcoli,slebresne,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6392,,,,,,,,,,,,,,,,,,,,"20/Nov/13 05:07;jbellis;6275.txt;https://issues.apache.org/jira/secure/attachment/12614790/6275.txt","15/Nov/13 01:39;mshuler;c_file-descriptors_strace.tbz;https://issues.apache.org/jira/secure/attachment/12613994/c_file-descriptors_strace.tbz","31/Oct/13 11:16;ash2k;cassandra_jstack.txt;https://issues.apache.org/jira/secure/attachment/12611371/cassandra_jstack.txt","11/Nov/13 17:34;gianlucaborello;leak.log;https://issues.apache.org/jira/secure/attachment/12613184/leak.log","13/Nov/13 13:32;baldrick;position_hints.tgz;https://issues.apache.org/jira/secure/attachment/12613598/position_hints.tgz","31/Oct/13 15:49;baldrick;slog.gz;https://issues.apache.org/jira/secure/attachment/12611411/slog.gz",,,,,,,,,,,,,,,6.0,graham.sanderson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356218,,,Wed Nov 20 16:53:46 UTC 2013,,,,,,,,,,"0|i1perj:",356506,2.0.1,2.0.2,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"31/Oct/13 11:16;ash2k;Attaching thread dump of process when it is not possible to shutdown it cleanly.;;;","31/Oct/13 12:03;brandon.williams;This does look rather damning, and I know _something_ is up here because we can't run 2.0 with ccm on debug, so maybe this is it.;;;","31/Oct/13 15:49;baldrick;Part of system.log showing lots of memtable flush messages.;;;","31/Oct/13 15:50;baldrick;We had to downgrade from 2.0.2 to 1.2.11 due to C* leaking vast numbers of file descriptors (same issue in 2.0.1), approximately 50 a second.  We didn't have any trouble shutting down servers, we also were *not* using Paxos, so might be a different issue.  Like in this report, the same sstable files had been opened again and again (and again) only they were from one of our own column families, not from a system column family.  This table was being polled repeatedly (once per second per program) by about 20 programs using the native protocol; the polling was doing a range slice.  I haven't been able to reproduce the file descriptor leak in our test environment.

One thing I noticed is that the system logs were chock-a-block with memtable flushing messages, many many more than we would get with 1.2.11 (I've attached a snippet).;;;","31/Oct/13 19:29;mishail;bq. Also, when that happens it's not always possible to shutdown server process via SIGTERM. Have to use SIGKILL.

As far as I understand here *what* is happening

* {{SIGTERM handler}} waits for {{StorageServiceShutdownHook}} 
* {{StorageServiceShutdownHook}} waits (up to *3600 sec == 1hr*) for {{mutationStage}} threads to complete. 
* {{""MutationStage:2718""}} thread performs {{ColumnFamilyStore.forceBlockingFlush}} initiated by {{TruncateVerbHandler.doVerb}} and waits for {{""MemtablePostFlusher:1""}} 
* {{""MemtablePostFlusher:1""}} is waiting on {{CountDownLatch.await}} (in {{WrappedRunnable}} returned from {{ColumnFamilyStore.switchMemtable)}}.  It will wait until the latch is counted down to zero.

There is also another call to {{ColumnFamilyStore.forceBlockingFlush}} from {{""OptionalTasks:1"":BatchlogManager.cleanup()}} . 
;;;","31/Oct/13 20:10;jbellis;Hmm.  It's possible that we shouldn't be running Truncate on the Mutation stage.  But, I don't think Duncan or Mikhail mentioned running truncate so there is probably something else wrong as well.;;;","31/Oct/13 20:34;mishail;[~jbellis] still there is {{TruncateVerbHandler.doVerb}} in Mikhail's [^cassandra_jstack.txt];;;","31/Oct/13 20:42;brandon.williams;[~baldrick] are you using the native protocol?;;;","31/Oct/13 21:13;baldrick;Yes, I'm using the native protocol.  No use of truncate on my part.;;;","31/Oct/13 21:34;jbellis;Let's create a new ticket for the truncate hang then.;;;","01/Nov/13 02:51;ash2k;My load test uses TRUNCATE before it starts. I will check if that leak happens without it.;;;","01/Nov/13 03:14;ash2k;I tried without TRUNCATE - no leaking (test TRUNCATEs 3 tables before it starts).

Before test:
{noformat}
[root@cassandra-test0 ~]$ lsof -n | grep java | wc -l
169

[root@cassandra-test1 ~]$ lsof -n | grep java | wc -l
167

[root@cassandra-test2 ~]# lsof -n | grep java | wc -l
173
{noformat}

After test:
{noformat}
[root@cassandra-test0 ~]$ lsof -n | grep java | wc -l
172

[root@cassandra-test1 ~]$ lsof -n | grep java | wc -l
172

[root@cassandra-test2 ~]# lsof -n | grep java | wc -l
183
{noformat};;;","01/Nov/13 08:30;baldrick;In my case I didn't use truncate, however I did do one exotic operation: I used alter table to drop a no longer needed column a few days before I noticed this issue.

Before downgrading I made a copy of the entire contents of /var/lib/cassandra/, so I could try recreating the 2.0.2 cluster and the problem in some virtual machines using these.;;;","01/Nov/13 08:34;baldrick;I also changed sstable_compression from SnappyCompressor to LZ4Compressor.;;;","11/Nov/13 17:34;gianlucaborello;We are experiencing a similar issue in 2.0.2.

It started happening after we set a TTL for all our columns in a very limited datastore (just a few GBs).

We can easily see the fd count rapidly increase to 100000+, and the majority of fds are (from lsof):

{noformat}
java      13168       cassandra  267r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
java      13168       cassandra  268r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
java      13168       cassandra  269r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
java      13168       cassandra  270r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
java      13168       cassandra  271r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
java      13168       cassandra  272r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
java      13168       cassandra  273r      REG                9,0   273129  671089723 /raid0/cassandra/data/draios/process_counters_by_exe/draios-process_counters_by_exe-jb-231-Data.db (deleted)
{noformat}

I'm attaching the log of the exception (leak.log). You can see the exceptions, and then Cassandra eventually shuts down. We had to temporarily downgrade to 1.2.11
;;;","11/Nov/13 20:45;jbellis;[~ash2k] or others, can you verify if this is also a problem in 1.2.11?;;;","11/Nov/13 20:49;gianlucaborello;[~jbellis], FWIW 1.2.11 is working fine for us (I have one week of uptime so far since the downgrade).;;;","11/Nov/13 20:55;jbellis;Can you have a look [~krummas] ?;;;","12/Nov/13 06:51;baldrick;All our leaked fd's were for a large table that uses TTL.  We also don't have any problems with 1.2.11 (which we had to downgrade to, just like Gianluca).;;;","12/Nov/13 07:14;ash2k;[~jbellis] my test workload uses LWT so it cannot be run on 1.2.x. The written columns themself do not use TTL but AFAIK Paxos table uses TTL. And in my case I see a lot of open Paxos-related files. So, taking into account what others said above, looks like the problem is somehow connected to TTL.;;;","12/Nov/13 19:30;marcuse;been looking at this a bit and can't really reproduce, suspected CASSANDRA-5228 - but that seems to work (or, found a bug, but unrelated to this, CASSANDRA-6337)

slog.gz looks a bit like what [~mkjellman] reported in CASSANDRA-5241 (looping flushing of system tables) but that was resolved a long time ago.

does anyone have a way to reproduce? [~ash2k] would it be possible to post your load test?
;;;","13/Nov/13 02:11;rcoli;A brief note to mention that when durable_writes are disabled, handling of clean shutdown (via SIGTERM/StorageServiceShutdownHook) has additional blocking while waiting for drain. If one is investigating and/or modifying the behavior of the shutdown hook, they should be aware that there are two different cases to test. See CASSANDRA-2958.;;;","13/Nov/13 03:01;ash2k;[~krummas] sorry, I cannot post that code. The scenario is something like this:
{code}
TRUNCATE table1;
TRUNCATE table2;
TRUNCATE table3;
loop {
    SELECT FROM table1 WHERE key='xxx' (quorum);
    INSERT INTO table2 (quorum);
    INSERT INTO table3 IF NOT EXISTS; (should sometimes fail)
    UPDATE table1 WHERE  key = 'someid' IF column='zzz'; (should sometimes fail)
}
{code}
As I said, if I remove TRUNCATEs it do not leak.;;;","13/Nov/13 13:30;baldrick;OK, here is how you can reproduce.

1) Create this keyspace:

CREATE KEYSPACE all_production WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};

2) Create a table as follows:

use all_production;
CREATE TABLE position_hints (shard int, date text, when timeuuid, sequence bigint, syd int, broker uuid, engine uuid, confirmed bigint, open_buy bigint, open_sell bigint, PRIMARY KEY ((shard, date), when)) with clustering order by (when desc);

3) Stop Cassandra.  Untar the attached file in /var/lib/cassandra/data/all_production/ to populate the position_hints table.

4) Start Cassandra.

5) Prepare a large number of queries as follows:

for (( i = 0 ; i < 1000000 ; i = i + 1 )) ; do echo ""select * from position_hints where shard=1 and date='2013-10-30' and when>ba719c52-4182-11e3-a471-003048feded4 limit 1;"" ; done > /tmp/queries

6) In cqlsh:

use all_production;
source '/tmp/queries';

7) Enjoy watching the number of fd's used by Cassandra go up and up.;;;","13/Nov/13 13:32;baldrick;Untar this to populate the position_hints table.;;;","13/Nov/13 15:28;jbellis;Can you reproduce the above on 2.0.2 or 2.0.2 HEAD, [~mshuler]?;;;","14/Nov/13 23:29;mshuler;Reproduced in 2.0.2.
On m1.medium, running C*, the open files were about 910.  My query is still running, and I'm at >3500 open files.
(I'll work on cassandra-2.0 branch HEAD, next);;;","15/Nov/13 00:54;mshuler;Same results on cassandra-2.0 HEAD.;;;","15/Nov/13 01:39;mshuler;c_file-descriptors_strace.tbz is a strace of the C* java process and children while running the query to about 27k open files.  This was on the cassandra-2.0 branch in git.;;;","15/Nov/13 16:49;marcuse;ok, this is what i have so far, i can also reproduce on an m1.medium in EC2, ubuntu 13.10 which has 3.11.x kernel.

i cannot reproduce on my laptop (debian squeeze) or my server (rhel 6), both run kernel 2.6.x. (jdk7u45 on all)

it happens with trivial tables/data as well, so seems unrelated to TTL or truncate etc

just starting cassandra up shows ~50 open FDs for the same Data.db-file;;;","15/Nov/13 16:55;baldrick;I originally saw the issue on Ubuntu 10.04 (kernel 2.6.32) and reproduced it on Ubuntu 13.10 (kernel 3.11.0).;;;","15/Nov/13 16:56;pieterc;I also have the problem on Ubuntu 12.04 (Linux de-cass00 3.8.0-30-generic #44~precise1-Ubuntu SMP Fri Aug 23 18:32:41 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux)

Posted something om mailing list because I was not sure if it was a bug... (Here you can find more info, In some cases I hade a deleted file more than 50k times open)
http://www.mail-archive.com/user@cassandra.apache.org/msg32999.html

Temporary fix was to raise the nofile limit to 1kk...;;;","15/Nov/13 17:03;mshuler;My tests were on Ubuntu precise, same kernel as above, with JVM version 1.7_25.;;;","18/Nov/13 22:24;jre;We recently ran into this issue after upgrading to OpsCenter-4.0.0, it is quite easy to reproduce:
# Install Cassandra-2.0.2
# Install OpsCenter-4.0.0 on above cluster.

I upgraded OpsCenter on Friday, and by Sunday I had reached 1 Million open file handles.  I had to kill -9 the Cassandra processes as it wouldn't respond to sockets, DSC20 restart scripts reported successfully killing the processes but in fact did not.

{noformat}
[root@cassandra2 ~]# lsof -u cassandra|wc -l
175416
[root@cassandra2 ~]# lsof -u cassandra|grep -c OpsCenter
174474
{noformat}

Most of the handles show as ""deleted""
{noformat}
[root@cassandra2 ~]# lsof -u cassandra|grep -c deleted
174449
{noformat};;;","18/Nov/13 22:52;graham sanderson;Note also, that most if not all of the deleted files are of the form

{code}
java    14018 cassandra  586r   REG               8,33   8792499       1251 /data/1/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-4656-Data.db (deleted)
java    14018 cassandra  587r   REG               8,33  27303760       1254 /data/1/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-4655-Data.db (deleted)
java    14018 cassandra  588r   REG               8,33   8792499       1251 /data/1/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-4656-Data.db (deleted)
java    14018 cassandra  589r   REG               8,33  27303760       1254 /data/1/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-4655-Data.db (deleted)
java    14018 cassandra  590r   REG               8,33  10507214        936 /data/1/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-4657-Data.db (deleted)
{code}
We have 7 data disks per node (don't know if this contributes to the problem), and the number of such (open but) deleted files is very ill balanced with 93% on two of the 7 disks (on this particular node)... the distribution of live data file size for OpsCenter/rollups60 is a little uneven with the same data mounts that have more deleted files having more actual live data, but the deleted file counts per mount point vary by several order of magnitudes whereas the data size itself does not.;;;","19/Nov/13 14:00;jbellis;[~mshuler] Can you reproduce in 1.2?  How about 2.0.0?

If in the latter but not the former it's going to be a bitch to bisect but I don't have any better ideas.;;;","19/Nov/13 16:04;capncrunch4me;Environment Centos 6.4 ~ Kernel 3.10 (elrepo) ~ Cassandra 2.0.2 ~ Opscenter 4.0.

Noted that Opscenter extremely exacerbates the issue, as the roll-up CFs cause open files to grow at an incredible pace. Turning Opscenter off causes rate to slow and/or stop. After stopping opscenter, the opscenter open files on all C* nodes never release although the open files dont continue to grow.

There are 10's of thousands of these open:

/data/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-15491-Data.db (deleted)
/data/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-15491-Data.db (deleted)
/data/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-15491-Data.db (deleted)
/data/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-15491-Data.db (deleted)

A restart of cassandra and keeping opscenter off will keep file descriptors within a comfortable range. 

;;;","19/Nov/13 20:37;graham sanderson;Yes I believe we can mitigate the problem in the OpCenter case, however it is a good test bed since it makes the problem easy to spot - note it seems to be worse under high read/write activity on tracked keyspaces/CFs, however that makes sense.

Note I was poking (somewhat blindly) thru the (2.0.2) code (partly out of interest) looking for what might be leaking these file handles, and I also took a heap dump. I discovered what turned out to be CASSANDRA-6358 which leaks FileDescriptors though their refCounts all seemed to be 0. In any case there weren't enough (total FileDescriptors - in the heap dump) to account for the problem. They were also for mem-mapped files (the ifile in SSTableReader) and none of the leaked deleted file handles were mem-mapped (since they were compressed data files)

That said CASSANDRA-6358 was pinning the SSTableReaders in memory (since the Runnable was an anonymous inner class), so someone with more knowledge of the code might have a better idea if this might be a problem (other than the memory leak)

I don't have an environment yet where I can easily build and install code changes, though we could downgrade our system test environment to 2.0.0 to see if we can reproduce the problem there - unsure if we can downgrade to 1.2.X easily given our current testing.

Note while I was looking at the code I came across CASSANDRA-5555... What caught my eye was the interaction between FileCacheService and RAR.deallocate, but more specifically related to the fact that this change, added a concurrent structure inside another separate concurrent structure, and it seemed like there might be a case where a RAR was recycled into a concurrent queue that was already removed and drained, in which case it would get GCed without close, presumably causing a file handle leak on the native side. Though I couldn't come up with any significantly convincing interactions that would cause this to happen without some very very unlucky things happening (and my knowledge of the google cache implementation was even more limited!), so this is unlikely the cause of this issue (especially if the issue doesn't happen in the 1.2.7+ branch), because I think nearly all deleted data files are being leakd, and finally because there is no particular correlation with TTL.;;;","19/Nov/13 21:08;mishail;bq. What caught my eye was the interaction between FileCacheService and RAR.deallocate, but more specifically related to the fact that this change, added a concurrent structure inside another separate concurrent structure, and it seemed like there might be a case where a RAR was recycled into a concurrent queue that was already removed and drained, in which case it would get GCed without close, presumably causing a file handle leak on the native side

I might be wrong, but what are you saying correlates with observations from CASSANDRA-6283. [~Andie78] experiminted and 
bq. found out, that a finalizer fixes the problem. So after GC the files will be deleted (not optimal, but working fine). It runs now 2 days continously without problem. Possible fix/test:I wrote the following finalizer at the end of class org.apache.cassandra.io.util.RandomAccessReader: { deallocate(); super.finalize(); } };;;","19/Nov/13 21:23;mishail;I wonder what would happen with {{file_cache_size_in_mb: 0 }}. RAR should be explicitly deallocated then.;;;","19/Nov/13 23:50;graham sanderson;Trying that now (one node with that setting);;;","20/Nov/13 00:16;jre;[~mishail] We (Graham Sanderson and I work together) added 'file_cache_size_in_mb: 0' to cassandra.yaml on one of the nodes, and restarted that node plus another with the default (unspecified) file_cache_size_in_mb setting to run an A/B test.  Both nodes still leak file handles, however, the node with the default setting leaks much faster (about 3-4x the leak rate).

CASSANDRA-6283 appears to be an exact duplicate of this problem, Windows and Linux JVMs appear to exhibit the exact same file handle leak behavior.;;;","20/Nov/13 00:25;graham sanderson;Note that this would tend to imply that I was wrong (at least about the particular code path), and the change in leak rate may be attributable to less throughput without the file cache. Note the leak rate does seem quite related to how hard we are hitting the server as mentioned before, so a threading bug elsewhere might be the cause.

Note nominally buffer in RAR should be volatile, but then any code path thru close where buffer's latest value is stale would end up calling deallocate anyway (at least in the case that file_cache_size_in_mb is off; I didn't think through the other case.

So given the finalizer fix - which we can try and build here to test out (unless someone has it pre-built) - seems to imply that it is just someone failing to call close() under load conditions.;;;","20/Nov/13 04:00;graham sanderson;We were able to confirm the finalizer fix stopped the leak;;;","20/Nov/13 04:09;graham sanderson;Note I believe the problem is caused by CASSANDRA-5514 (2.0 beta 1)

I don't have a patch because I don't know the exact patch for reasons below

Here is CollationController.java starting at line 264: (as of current 2.0 branch and 2.0.2)

{code}
            // Check for row tombstone in the skipped sstables
            if (skippedSSTables != null)
            {
                for (SSTableReader sstable : skippedSSTables)
                {
                    if (sstable.getMaxTimestamp() <= minTimestamp)
                        continue;

                    sstable.incrementReadCount();
                    OnDiskAtomIterator iter = filter.getSSTableColumnIterator(sstable);
                    if (iter.getColumnFamily() == null)
                        continue;

                    ColumnFamily cf = iter.getColumnFamily();
                    // we are only interested in row-level tombstones here, and only if markedForDeleteAt is larger than minTimestamp
                    if (cf.deletionInfo().getTopLevelDeletion().markedForDeleteAt > minTimestamp)
                    {
                        includedDueToTombstones++;
                        iterators.add(iter);
                        returnCF.delete(cf.deletionInfo().getTopLevelDeletion());
                        sstablesIterated++;
                    }
                }
            }
{code}

Note if the last ""if"" test does not succeed, then ""iter"" is neither closed, nor is it added to the ""iterators"" list to be closed in the finally section at the end - it would have been easy for me to add it always to ""iterators"" list except that ""iterators"" is referenced lower in the function:

{code}
            if (iterators.isEmpty())
                return null;

            Tracing.trace(""Merging data from memtables and {} sstables"", sstablesIterated);
            filter.collateOnDiskAtom(returnCF, iterators, gcBefore);
{code}

Being new to the code, I cannot say whether it should be in ""iterators"" at that point, or just have been closed (quietly) above
;;;","20/Nov/13 04:32;graham sanderson;Also note stack trace for all leaked files we saw - someone can perhaps use this to help figure out what this actually affects (i.e. some of the iter's RARs may have been owned by someone else in which case AOK)

{code}
ERROR [Finalizer] 2013-11-20 03:43:42,129 RandomAccessReader.java (line 399) LEAK finalizer had to clean up
java.lang.Exception: RAR for /data/5/cassandra/OpsCenter/rollups60/OpsCenter-rollups60-jb-6882-Data.db allocated
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:66)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:76)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:43)
	at org.apache.cassandra.io.util.CompressedPoolingSegmentedFile.createReader(CompressedPoolingSegmentedFile.java:48)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:39)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1182)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader.setToRowStart(IndexedSliceReader.java:108)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader.<init>(IndexedSliceReader.java:84)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:65)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:42)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:167)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:273)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1467)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1286)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:65)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:47)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:744)
{code};;;","20/Nov/13 05:07;jbellis;Nailed it.  (It should be closed.)  Patch attached.;;;","20/Nov/13 05:47;jre;I accidentally hit the ""Testing"" button, and don't see a way to revert.  I've build cassandra-2.0.2 with just this patch applied, and we are testing it now, but I didn't mean to change the status.;;;","20/Nov/13 12:42;jbellis;(Reject takes it back to Open.);;;","20/Nov/13 14:19;mshuler;I will test this out this morning!;;;","20/Nov/13 14:35;jre;So we've been running this all night, have written a few hundred GB of data with some products we're developing, all the while OpsCenter 4.0.0 was doing TTL'd rollups and what not.  Deleted file count remained at 1 the entire time, never increasing, and total file count remained below 1000.

FYI, the single undeleted file looks like some temporary file randomly generated on Cassandra startup that gets deleted but not closed for the processes' duration, example:
{noformat}
java    1925 cassandra   44u   REG              253,4      4096         13 /tmp/ffi441Hpl (deleted)
{noformat};;;","20/Nov/13 14:54;brandon.williams;That's probably JNA, or snappy.;;;","20/Nov/13 16:04;slebresne;That patch lgtm, +1.;;;","20/Nov/13 16:11;mshuler;Patch works for me, too :);;;","20/Nov/13 16:53;jbellis;committed.  thanks everyone!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fixes for compacting larger-than-memory rows,CASSANDRA-6274,12676652,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,30/Oct/13 15:12,16/Apr/19 09:32,14/Jul/23 05:53,30/Oct/13 15:20,2.0.3,,,,,,0,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356084,,,Wed Oct 30 15:20:01 UTC 2013,,,,,,,,,,"0|i1pdy7:",356372,,,,,,,,,marcuse,,marcuse,Low,,,,,,,,,,,,,,,,,,"30/Oct/13 15:12;jbellis;https://github.com/jbellis/cassandra/commits/6142-2.0 already reviewed on CASSANDRA-6142, but splitting out here to avoid confusion from having 6142 in the 2.0 CHANGES.;;;","30/Oct/13 15:20;jbellis;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader data distribution is broken since 1.2.7,CASSANDRA-6272,12676594,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,samt,samt,samt,30/Oct/13 10:23,16/Apr/19 09:32,14/Jul/23 05:53,30/Oct/13 19:12,1.2.12,,,Legacy/Tools,,,0,,,,,"Running sstableloader on 1.2.10+ results in radically different distribution compared with earlier versions. It looks as though the 'bare-bones' IndexSummary created in SSTR.loadForBatch is the cause (CASSANDRA-5555); because it contains only a single entry, we end up with the wrong segment of the index file in SSTR.getPosition (its position is always 0),  so only the first segment of the index file is considered when searching for the range's position.

This doesn't affect  2.0/trunk",,jeromatron,mbulman,rcoli,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/13 18:46;samt;0001-CASSANDRA-6272-Load-or-build-a-real-IndexSummary-in-.patch;https://issues.apache.org/jira/secure/attachment/12611138/0001-CASSANDRA-6272-Load-or-build-a-real-IndexSummary-in-.patch","30/Oct/13 21:08;samt;SSTableReaderTest-for-2.0.patch;https://issues.apache.org/jira/secure/attachment/12611176/SSTableReaderTest-for-2.0.patch","30/Oct/13 21:08;samt;SSTableReaderTest-for-trunk.patch;https://issues.apache.org/jira/secure/attachment/12611175/SSTableReaderTest-for-trunk.patch",,,,,,,,,,,,,,,,,,3.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,356026,,,Wed Oct 30 21:12:53 UTC 2013,,,,,,,,,,"0|i1pdlb:",356314,1.2.7,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"30/Oct/13 18:46;samt;Sorry, generated the original patch against an older version, so it doesn't apply to the current 1.2 branch. Replaced with an updated version;;;","30/Oct/13 18:57;samt;Actually, this problem is present in earlier versions than 1.2.10, as CASSANDRA-5555 was actually included in 1.2.7;;;","30/Oct/13 19:12;brandon.williams;Committed.;;;","30/Oct/13 20:46;brandon.williams;Sam, can you rewrite the test for trunk?;;;","30/Oct/13 21:08;samt;Patches with SSTableReaderTest updated for 2.0 & trunk;;;","30/Oct/13 21:12;brandon.williams;Pushed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SERIAL consistency in errors to v1 protocol driver,CASSANDRA-6270,12676559,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,iconara,iconara,30/Oct/13 06:47,25/Oct/19 13:11,14/Jul/23 05:53,30/Oct/13 16:24,2.0.3,,,Feature/Lightweight Transactions,,,0,LWT,,,,"I'm the author of the Ruby driver for CQL, and I got a bug report about strange errors when running on C* 2.0 and using lightweight transaction queries. The bug report can be found here: https://github.com/iconara/cql-rb/issues/53

The client sent {{UPDATE table SET val = 42 WHERE row_id = 5 IF val = 41}} and when C* couldn't fulfill SERIAL consistency it sent an error back saying ""Operation timed out - received only -1 responses"".

So far so good, but it also set the {{consistency}} field in the error response to 8, corresponding to {{SERIAL}} in v2 of the binary protocol, even if the communication with the client was over v1 of the protocol. Since my driver doesn't yet support v2 it doesn't think that 8 is a valid consistency, and fails to parse the frame.

Is this the intended behaviour of C*, or an oversight in how that error is formulated? I could easily add {{SERIAL}} and accept it even if the communication is over v1 of the protocol, but the bigger issue is how C* handles drivers that do not speak the latest version of the protocol. People should be able to use a driver that worked correctly with C* X with C* X+1, right?

Do drivers have to be accepting in what they receive from C* because they might get consistencies, data types, etc. that are from future versions of the protocol, or does C* guarantee that frames will conform to the protocol that the driver says it understands?",Cassandra 2.0,adstage-david,aleksey,iconara,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Oct/13 15:34;slebresne;6270.txt;https://issues.apache.org/jira/secure/attachment/12611098/6270.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,355991,,,Wed Oct 30 16:24:57 UTC 2013,,,,,,,,,,"0|i1pddj:",356279,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"30/Oct/13 10:36;aleksey;v1 native protocol is not intended to support all the Cassandra 2.0 features. (If it could, we wouldn't need v2 native protocol in the first place).

To use 2.0 CAS with native proto you should be using the v2 protocol.;;;","30/Oct/13 13:22;iconara;In that case Cassandra should fail the request because it uses a CQL syntax that is not supported over that version of the protocol. Cassandra has accepted that the connection will use CQL v3.0.0 and the frame was sent with v1 of the protocol. The right thing to do is to fail the request because it uses features that are not in CQL v3.0.0, but instead it sends a frame back that a v1 driver cannot parse.

Drivers can't tell if the CQL is valid because they don't parse it, they can't tell if the CQL uses features not available in the version of the binary protocol or not. Cassandra has to say that the request is not valid, and it can't send frames back that are not valid in the protocol specified by the driver.;;;","30/Oct/13 15:07;adstage-david;I originally reported the bug on the Ruby project, just thought it might be useful to clarify a bit:

I could _sometimes_ successfully use lightweight transactions successfully over the v1 binary protocol the ruby driver is using (specifying a consistency of :quorum on writes), but sometimes it would throw this error. ;;;","30/Oct/13 15:34;slebresne;I agree that sending frames that are not valid is a tad anti-social. Attaching simple patch that throw an InvalidRequestException if the user tries to use CAS with the protocol v1.
;;;","30/Oct/13 16:00;aleksey;LGTM, although I'd slightly prefer protocol version be a enum (UNDEFINED, V1, V2), but not strongly.;;;","30/Oct/13 16:24;slebresne;Committed, thanks.

bq. I'd slightly prefer protocol version be a enum (UNDEFINED, V1, V2)

Bah, we use int for the encoding/decoding methods and perhaps more importantly, I'm lazy :)
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
restrict max num_tokens to something Gossip can handle,CASSANDRA-6267,12676467,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,29/Oct/13 19:19,16/Apr/19 09:32,14/Jul/23 05:53,30/Oct/13 15:46,1.2.12,2.0.3,,,,,0,,,,,,,qconner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Oct/13 19:19;jbellis;6267.txt;https://issues.apache.org/jira/secure/attachment/12610907/6267.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,355899,,,Wed Oct 30 15:46:25 UTC 2013,,,,,,,,,,"0|i1pct3:",356187,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"29/Oct/13 20:00;brandon.williams;+1;;;","29/Oct/13 23:53;qconner;+1;;;","30/Oct/13 15:46;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool compact throws an error after importing data with sstableloader,CASSANDRA-6262,12676233,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jblangston@datastax.com,jblangston@datastax.com,28/Oct/13 22:27,16/Apr/19 09:32,14/Jul/23 05:53,22/Jan/14 23:08,1.2.14,,,,,,0,,,,,"Exception when running nodetool compact:

{code}
Error occurred during compaction
java.util.concurrent.ExecutionException: java.lang.IndexOutOfBoundsException: index (2) must be less than size (2)
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:252)
	at java.util.concurrent.FutureTask.get(FutureTask.java:111)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:331)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1691)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:2198)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor10.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:138)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:252)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:819)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:801)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1487)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:848)
	at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.IndexOutOfBoundsException: index (2) must be less than size (2)
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305)
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284)
	at com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:81)
	at org.apache.cassandra.db.marshal.CompositeType.getComparator(CompositeType.java:94)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
	at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:128)
	at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:119)
	at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:114)
	at org.apache.cassandra.db.ColumnFamily.addAtom(ColumnFamily.java:219)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumnsFromSSTable(ColumnFamilySerializer.java:149)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:234)
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:114)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:98)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:160)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:76)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:203)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:145)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:352)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	... 3 more
{code}

Schema:

{code}
create table test (
 a uuid,
 b int,
 c timestamp,
 d text,
 e uuid,
 f blob,
 g uuid,
 h boolean,
 primary key (a, b)
);
{code}",,brandon.williams,jblangston@datastax.com,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jan/14 01:48;slebresne;6262.patch;https://issues.apache.org/jira/secure/attachment/12623879/6262.patch",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,355730,,,Wed Jan 22 23:08:05 UTC 2014,,,,,,,,,,"0|i1pbrj:",356018,,,,,,,,,brandon.williams,,brandon.williams,Normal,,1.2.4,,,,,,,,,,,,,,,,"28/Oct/13 22:35;jbellis;What do I need to insert to reproduce?  I assume it doesn't repro just after CREATE TABLE.;;;","28/Oct/13 22:38;brandon.williams;Stack without compaction:

{noformat}
ERROR [ReadStage:98] 2013-10-23 22:15:25,991 CassandraDaemon.java (line 191) Exception in thread Thread[ReadStage:98,5,main]
java.lang.IndexOutOfBoundsException: index (2) must be less than size (2)
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:305)
	at com.google.common.base.Preconditions.checkElementIndex(Preconditions.java:284)
	at com.google.common.collect.RegularImmutableList.get(RegularImmutableList.java:81)
	at org.apache.cassandra.db.marshal.CompositeType.getComparator(CompositeType.java:94)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:76)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
	at org.apache.cassandra.db.marshal.AbstractType$3.compare(AbstractType.java:69)
	at org.apache.cassandra.db.marshal.AbstractType$3.compare(AbstractType.java:66)
	at org.apache.cassandra.utils.MergeIterator$Candidate.compareTo(MergeIterator.java:151)
	at org.apache.cassandra.utils.MergeIterator$Candidate.compareTo(MergeIterator.java:128)
	at java.util.PriorityQueue.siftUpComparable(PriorityQueue.java:582)
	at java.util.PriorityQueue.siftUp(PriorityQueue.java:574)
	at java.util.PriorityQueue.offer(PriorityQueue.java:274)
	at java.util.PriorityQueue.add(PriorityQueue.java:251)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.advance(MergeIterator.java:123)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:96)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:157)
	at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:136)
	at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:84)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:291)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1391)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1126)
	at org.apache.cassandra.db.Table.getRow(Table.java:347)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)
	at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
{noformat};;;","28/Oct/13 23:19;brandon.williams;Confirmed that this was a case of schema mismatch.  It seems would could communicate this better to save time in the future, however.;;;","19/Nov/13 11:09;slebresne;bq. Confirmed that this was a case of schema mismatch.

You meant, the sstables weren't valid sstables for the schema they were loaded into? If so, I'm not sure the loader can validate that easily (short of reading the sstable just for that matter) since there is no information on the schema in the sstable itself.;;;","17/Jan/14 18:30;brandon.williams;I'm not saying we need to validate the schema per se (and I realize that's not easy) but throwing IOOBE doesn't make the problem immediately clear, some kind of hint about a possible schema mismatch could be useful.;;;","20/Jan/14 01:48;slebresne;Not against improving the error message. Attaching a tentative patch for this. I'll note though that CompositeType seems like the only logical place to catch this and rethrow with more info, but that also mean we can't be absolutely sure of what the problem is when we're there, so I've attempted to throw an helpful error message without being too specific that it would be confusing in some other cases (than the one of this issue). It's also possible that an incompatible schema would throw another kind of exception, so this may not be enough, but well, that's progress so we can start there I guess.;;;","20/Jan/14 02:25;brandon.williams;+1;;;","22/Jan/14 23:08;brandon.williams;Sylvain committed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra 2.0.1 OutOfMemoryError:  Requested array size exceeds VM limit,CASSANDRA-6260,12676213,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,prateek@bloomreach.com,prateek@bloomreach.com,28/Oct/13 20:57,16/Apr/19 09:32,14/Jul/23 05:53,31/Oct/13 13:43,2.0.3,,,,,,0,,,,,"I am running cassandra 2.0.1 server with cascading client https://github.com/ifesdjeen/cascading-cassandra/ (v 1.0.0-rc6). I am running a problem on restarting one of the nodes in the cassandra cluster. All other nodes in the cluster started properly without any issues. I had originally assigned 8G of RAM to heap space. I tried starting the node with 12G of RAM but it still fails with the following error. This is currently blocking a production release so appreciate your quick response.

[(bloomreach-ami) ubuntu@ip-10-179-26-169 :/mnt/cassandra_latest]# ERROR 20:55:58,738 Exception encountered during startup
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:394)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:355)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:352)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:119)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:267)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:411)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:383)
        at org.apache.cassandra.db.Keyspace.initCf(Keyspace.java:314)
        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:268)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
        at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:474)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:226)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:442)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:485)
 INFO 20:55:58,739 Initializing system.schema_triggers
java.lang.OutOfMemoryError: Requested array size exceeds VM limit
        at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:394)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:355)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:352)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:119)

",Cassandra server 2.0.1,iflatness,mishail,prateek@bloomreach.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Oct/13 05:42;mishail;cassandra-2.0-6260-keyLength.patch;https://issues.apache.org/jira/secure/attachment/12611255/cassandra-2.0-6260-keyLength.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,355710,,,Thu Oct 31 13:43:21 UTC 2013,,,,,,,,,,"0|i1pbn3:",355998,2.0.1,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"29/Oct/13 15:51;iflatness;I have gotten the same error with the nodes on my cluster.

The cluster is running 2.0.1, and the error started on 28.10.2013 for me as well.
My machines are each running with heaps of 512mb, and that was consistent with how they were before the restart. On initialization system memory drops extremely low right before the error output (after which java stops and the memory is freed). There are 4 machines in my cluster and the partitioner is Murmur3.

-On 2 of the nodes, a different error is thrown- Upgrading to 2.0.2 fixed the other error. The heap error is unchanged with the upgrade to 2.0.2

;;;","29/Oct/13 20:48;jbellis;Since the problem is reading the key cache, deleting it is an easy workaround.;;;","30/Oct/13 06:38;mishail;Patch to check if the length of array from DataInput doesn't exceed the limit;;;","30/Oct/13 15:26;jbellis;Seriously, people are corrupting their key cache files with a length between maxint and maxint-8?  Isn't that ridiculously unlikely?;;;","30/Oct/13 19:16;mishail;[~jbellis] Should we validate that key size is under 64K? (http://wiki.apache.org/cassandra/FAQ#max_key_size) ;;;","30/Oct/13 19:31;brandon.williams;Sounds reasonable to me.;;;","31/Oct/13 05:42;mishail;Validate that {{keyLength <= FBUtilities.MAX_UNSIGNED_SHORT}};;;","31/Oct/13 13:43;jbellis;committed; thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need the root clause in FBUtilities.classForName when there is exception loading class,CASSANDRA-6258,12676196,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,cywjackson,cywjackson,cywjackson,28/Oct/13 19:01,16/Apr/19 09:32,14/Jul/23 05:53,28/Oct/13 19:39,1.2.12,2.0.3,,,,,0,,,,,"We have a custom snitch that works in 1.1, but the same does not work in 1.2 . It throws a ConfigurationException:

{panel}
ERROR 11:39:37,936 Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Unable to find snitch class 'com.apigee.cassandra.OldEC2Snitch'
	at org.apache.cassandra.utils.FBUtilities.classForName(FBUtilities.java:432)
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:444)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:530)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:350)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:126)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:216)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Unable to find snitch class 'com.apigee.cassandra.OldEC2Snitch'
{panel}

However the above exception does not help us understand what's wrong (jar is in the classpath and readable). I've to add the root clause to the ConfigurationException to see the real problem:

{panel}
ERROR 11:42:20,020 Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Unable to find snitch class 'com.apigee.cassandra.OldEC2Snitch'
	at org.apache.cassandra.utils.FBUtilities.classForName(FBUtilities.java:432)
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:444)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:530)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:350)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:126)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:216)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Caused by: java.lang.NoClassDefFoundError: org/apache/cassandra/config/ConfigurationException
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:169)
	at org.apache.cassandra.utils.FBUtilities.classForName(FBUtilities.java:424)
	... 7 more
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.config.ConfigurationException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	... 10 more
Unable to find snitch class 'com.apigee.cassandra.OldEC2Snitch'
{panel}",,cywjackson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/13 19:03;cywjackson;cassandra-6258.patch;https://issues.apache.org/jira/secure/attachment/12610624/cassandra-6258.patch","28/Oct/13 19:12;cywjackson;cassandra-6258.patch.v2;https://issues.apache.org/jira/secure/attachment/12610625/cassandra-6258.patch.v2",,,,,,,,,,,,,,,,,,,2.0,cywjackson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,355693,,,Mon Oct 28 19:39:58 UTC 2013,,,,,,,,,,"0|i1pbjb:",355981,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"28/Oct/13 19:03;cywjackson;attaching diff (based off 1.2) to add the root clause when throwing ConfigurationException;;;","28/Oct/13 19:08;brandon.williams;Your patch has a bunch of unneeded yaml changes.;;;","28/Oct/13 19:12;cywjackson;ops, diff too much. This one should be clean;;;","28/Oct/13 19:39;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception count not incremented on OutOfMemoryError (HSHA),CASSANDRA-6255,12676130,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,dhendry,dhendry,28/Oct/13 14:49,16/Apr/19 09:32,14/Jul/23 05:53,17/Jan/14 16:09,1.2.14,,,,,,1,,,,,"One of our nodes decided to stop listening on 9160 (netstat -l was showing nothing and telnet was reporting connection refused). Nodetool status showed no hosts down and on the offending node nodetool info gave the following:

{noformat}
nodetool info
Token            : (invoke with -T/--tokens to see all 256 tokens)
ID               : (removed)
Gossip active    : true
Thrift active    : true
Native Transport active: false
Load             : 2.05 TB
Generation No    : 1382536528
Uptime (seconds) : 432970
Heap Memory (MB) : 8098.05 / 14131.25
Data Center      : DC1
Rack             : RAC2
Exceptions       : 0
Key Cache        : size 536854996 (bytes), capacity 536870912 (bytes), 41383646 hits, 1710831591 requests, 0.024 recent hit rate, 0 save period in seconds
Row Cache        : size 0 (bytes), capacity 0 (bytes), 0 hits, 0 requests, NaN recent hit rate, 0 save period in seconds
{noformat}

After looking at the cassandra log, I saw a bunch of the following:

{noformat}
ERROR [Selector-Thread-16] 2013-10-27 17:36:00,370 CustomTHsHaServer.java (line 187) Uncaught Exception: 
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:691)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:145)
        at org.apache.cassandra.thrift.CustomTHsHaServer.requestInvoke(CustomTHsHaServer.java:337)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.handleRead(CustomTHsHaServer.java:281)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.select(CustomTHsHaServer.java:224)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.run(CustomTHsHaServer.java:182)
ERROR [Selector-Thread-7] 2013-10-27 17:36:00,370 CustomTHsHaServer.java (line 187) Uncaught Exception: 
java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:691)
        at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:949)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1371)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.execute(DebuggableThreadPoolExecutor.java:145)
        at org.apache.cassandra.thrift.CustomTHsHaServer.requestInvoke(CustomTHsHaServer.java:337)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.handleRead(CustomTHsHaServer.java:281)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.select(CustomTHsHaServer.java:224)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.run(CustomTHsHaServer.java:182)
{noformat}

There wasn't anything else overtly suspicious in the logs except for the occasional 
{noformat}
ERROR [Selector-Thread-0] 2013-10-27 17:35:58,662 TNonblockingServer.java (line 468) Read an invalid frame size of 0. Are you using TFramedTransport on the client side?
{noformat}
but  that periodically comes up - I have looked into it before but it has never seemed to have any serious impact.

This ticket is not about *why* an OutOfMemoryError occurred - which is bad but I don't think I have enough information to reproduce or speculate on a cause. This ticket is about the fact that an OutOfMemoryError occurred and nodetool info was reporting Thrift active : true and Exceptions : 0. 

Our monitoring systems and investigation processes are both starting to rely on on the exception count. The fact that it was not accurate here is disconcerting.","Oracle java version ""1.7.0_15""

rpc_server_type: hsha",cburroughs,dhendry,luapmahp,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/14 05:42;mishail;CASSANDRA-1.2-6255.patch;https://issues.apache.org/jira/secure/attachment/12623583/CASSANDRA-1.2-6255.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,355627,,,Fri Jan 17 16:09:45 UTC 2014,,,,,,,,,,"0|i1pb4n:",355915,1.2.13,,,,,,,,brandon.williams,,brandon.williams,Low,,1.2.10,,,,,,,,,,,,,,,,"16/Jan/14 22:48;luapmahp;We've experienced almost precisely the same issue, both in the sense that cassandra stopped listening on 9160 due to an apparent out-of-memory problem, and also in observing the ""invalid frame size of 0"" error, which we've also observed to a lesser degree in the past (but trending up as of late) and haven't been able to root cause.

One other interesting data point is that all 3 of our nodes in our main datacenter experienced this same issue at the same time. Our working theory is that the trigger was the starting of many applications simultaneously was too much for the current cluster to handle (with both reads and writes using a consistency level of TWO). 

Relevant log snippet:
{code}
INFO [ScheduledTasks:1] 2014-01-16 19:01:45,208 GCInspector.java (line 119) GC for ConcurrentMarkSweep: 921 ms for 1 collections, 1389728032 used; max is 2105540608
ERROR [Selector-Thread-0] 2014-01-16 19:01:48,813 CustomTHsHaServer.java (line 187) Uncaught Exception:
java.lang.OutOfMemoryError: Java heap space
        at java.nio.HeapByteBuffer.<init>(Unknown Source)
        at java.nio.ByteBuffer.allocate(Unknown Source)
        at org.apache.thrift.server.TNonblockingServer$FrameBuffer.read(TNonblockingServer.java:491)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.handleRead(CustomTHsHaServer.java:273)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.select(CustomTHsHaServer.java:224)
        at org.apache.cassandra.thrift.CustomTHsHaServer$SelectorThread.run(CustomTHsHaServer.java:182)
 INFO [ScheduledTasks:1] 2014-01-16 19:01:48,822 GCInspector.java (line 119) GC for ConcurrentMarkSweep: 3125 ms for 2 collections, 1390753448 used; max is 2105540608
{code}

Strangely, you can see that we're not really close to our max heap limit, although it was inching close to our configured ""CMSInitiatingOccupancyFraction"" which is effectively 1.5/2GB.

We were ready to chalk this up to an increased load on the cassandra cluster (and a need for beefier/more nodes). But I'd also echo Dan's concern that the health reporting is not sufficient/accurate for this particular issue (which I think is the main ask for this bug). We were only able to detect this issue because we had a monit check checking for expected listening ports.

;;;","17/Jan/14 05:08;mishail;The problem is that {{CustomTHsHaServer.SelectorThread}} swallows all Throwables (including {{Error}} ) in its {{run}} method. Probably that was done for a reason. 
I guess it would be reasonable to increment the exceptions counter in that {{catch}} block, since the exception will be swallowed, and won't be handled by global {{UncaughtExceptionHandler}}
;;;","17/Jan/14 05:14;jbellis;If it's an easy fix let's go ahead and do it; otherwise, that class is replaced in 2.0 with the Disruptor server.;;;","17/Jan/14 05:18;jbellis;(Should probably rethrow the OOM so the global handler can shut down the server.);;;","17/Jan/14 05:42;mishail;Patch. 
* Don't swallow OOM
* Increase the exception counter in all other cases;;;","17/Jan/14 16:09;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift's prepare_cql*_query() should validate login,CASSANDRA-6254,12676108,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,28/Oct/13 12:39,16/Apr/19 09:32,14/Jul/23 05:53,28/Oct/13 13:28,1.2.12,2.0.3,,,,,0,,,,,"Non-logged in users shouldn't be able to prepare statements when authentication is enabled.

Native protocol is not affected by this, since it doesn't let you do anything unless you authenticate when auth is enabled.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/13 12:40;aleksey;6254.txt;https://issues.apache.org/jira/secure/attachment/12610547/6254.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,355605,,,Mon Oct 28 13:28:57 UTC 2013,,,,,,,,,,"0|i1pazr:",355893,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"28/Oct/13 12:40;aleksey;A trivial patch attached.;;;","28/Oct/13 13:13;slebresne;+1;;;","28/Oct/13 13:28;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAS updates should require P.MODIFY AND P.SELECT,CASSANDRA-6247,12675972,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,26/Oct/13 23:57,25/Oct/19 13:11,14/Jul/23 05:53,28/Oct/13 12:53,2.0.3,,,Feature/Lightweight Transactions,,,0,LWT,,,,"With CAS it is possible to simulate a SELECT query using conditional UPDATE IF. Hence all CAS updates should require P.SELECT permission, and not just P.MODIFY.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Oct/13 12:00;aleksey;6247.txt;https://issues.apache.org/jira/secure/attachment/12610541/6247.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,355469,,,Mon Oct 28 12:53:35 UTC 2013,,,,,,,,,,"0|i1pa5r:",355757,,,,,,,,,slebresne,,slebresne,Low,,2.0.0,,,,,,,,,,,,,,,,"28/Oct/13 11:39;aleksey;Attaching a trivial patch.;;;","28/Oct/13 12:00;aleksey;Re-attaching, with CassandraServer.cas() modified as well.;;;","28/Oct/13 12:44;slebresne;+1;;;","28/Oct/13 12:53;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL LIST USERS does nothing after a user is created.,CASSANDRA-6242,12675836,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,bensykes,bensykes,25/Oct/13 16:11,16/Apr/19 09:32,14/Jul/23 05:53,26/Oct/13 12:33,2.0.3,,,Legacy/Tools,,,0,,,,,"After using CREATE USER to create a new user, the LIST USERS command returns nothing to the console.
After removing this user again, the command works as expected.

{code}
$ ./cqlsh -u cassandra -p cassandra
Connected to Test Cluster at localhost:9160.
[cqlsh 4.0.1 | Cassandra 2.0.1 | CQL spec 3.1.1 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> LIST USERS;

 name      | super
-----------+-------
 cassandra |  True

cqlsh> CREATE USER bob WITH PASSWORD 'example' NOSUPERUSER;
cqlsh> LIST USERS;
cqlsh> SELECT * FROM system_auth.users;

 name      | super
-----------+-------
       bob | False
 cassandra |  True

(2 rows)

cqlsh> DROP USER bob;
cqlsh> LIST USERS;

 name      | super
-----------+-------
 cassandra |  True

cqlsh>
{code}","cqlsh 4.0.1 | Cassandra 2.0.1 | CQL spec 3.1.1 | Thrift protocol 19.37.0
java version ""1.6.0_43""
Java(TM) SE Runtime Environment (build 1.6.0_43-b01)
Java HotSpot(TM) 64-Bit Server VM (build 20.14-b01, mixed mode)
Windows 7 - CQL running in Cygwin.
Python 2.7.3",aleksey,bensykes,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Oct/13 05:44;mishail;cassandra-2.0-6242.patch;https://issues.apache.org/jira/secure/attachment/12610444/cassandra-2.0-6242.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,355334,,,Sat Oct 26 12:33:44 UTC 2013,,,,,,,,,,"0|i1p9br:",355622,2.0.1,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"26/Oct/13 05:38;mishail;Issue in cqlsh;;;","26/Oct/13 05:44;mishail;Patch to print out results for ""LIST"" commands as well;;;","26/Oct/13 12:33;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CLASSPATH logic from init script is unused, JNA isn't loaded",CASSANDRA-6240,12675560,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,paravoid,paravoid,24/Oct/13 20:47,16/Apr/19 09:32,14/Jul/23 05:53,25/Oct/13 20:19,,,,Packaging,,,0,,,,,"The init script has a classpath() function that collects all the jars and even includes this piece of code to work with the standard Debian/Ubuntu libjna-jar:
{code:none}
    # use JNA if installed in standard location
    [ -r /usr/share/java/jna.jar ] && cp=""$cp:/usr/share/java/jna.jar""
{code}

This seems very nice and correct, however the classpath() function is never called and is entirely unused :) Instead, /usr/bin/cassandra is called, which in turn includes /usr/share/cassandra/cassandra.in.sh, which has basically similar code to collect the jars for CLASSPATH but a) without the JNA standard path trick b) without using EXTRA_CLASSPATH (from /etc/default/cassandra) at all, so Cassandra boots without either JNA nor EXTRA_CLASSPATH, contrary to expectations.

There are various suggestions on the web to do ""ln -s /usr/share/java/jna.jar /usr/share/cassandra/lib/""; I suspect this bug to be the reason for that.

 /usr/share/cassandra/cassandra.in.sh seems smart enough to append but not overwrite CLASSPATH, so fixing the init script's classpath() to only include JNA + EXTRA_CLASSPATH (and making sure it's actually getting called :)) should be enough for a fix.",,paravoid,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6101,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,355137,,,Fri Oct 25 20:18:48 UTC 2013,,,,,,,,,,"0|i1p83z:",355425,2.0.1,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"25/Oct/13 20:18;urandom;This was fixed in the 2.0 branch as part of CASSANDRA-6101, and will be released as part of 2.0.2.

There is one remaining issue with the init script that has gotten hung in review.  [~paravoid] if you could have a look at CASSANDRA-6131 and comment on it there, I'd be very grateful (I'll buy you a cheesesteak sandwich in Portland next summer :)).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
LOCAL_ONE doesn't work for SimpleStrategy,CASSANDRA-6238,12675555,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,24/Oct/13 20:14,16/Apr/19 09:32,14/Jul/23 05:53,07/Nov/13 06:10,1.2.12,2.0.3,,,,,0,,,,,"LOCAL_ONE only works for NetworkTopologyStrategy which has DC specification. Any other strategy fails.

If there is no  DC specified in the strategy, we should treat LOCAL_ONE as ONE",,aleksey,alexliu68,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6151,,,,CASSANDRA-6151,,,,,,,,,,,"24/Oct/13 21:54;alexliu68;6238-v2.txt;https://issues.apache.org/jira/secure/attachment/12610170/6238-v2.txt","24/Oct/13 20:36;alexliu68;6238.txt;https://issues.apache.org/jira/secure/attachment/12610147/6238.txt",,,,,,,,,,,,,,,,,,,2.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,355132,,,Thu Nov 07 12:51:09 UTC 2013,,,,,,,,,,"0|i1p82v:",355420,,,,,,,,,jasobrown,,jasobrown,Normal,,,,,,,,,,,,,,,,,,"24/Oct/13 21:57;alexliu68;6238.txt change only for LOCAL_ONE.
6238-v2.txt change is for both LOCAL_ONE and LOCAL_QUORUM

I am not sure that we want LOCAL_QUORUM to support SimpleStrategy as well, so I attach v2 for reference.;;;","25/Oct/13 05:41;jasobrown;I used LOCAL_QUORUM's failing on non-NTS as my template I went by when making LOCAL_ONE only usable with NTS. I don't think it makes sense for the two CLs to behave differently: either they both should support non-NTS or not.  I'm not sure if I feel too strongly either way, but I'm more inclined to retain the current CL.LOCAL_* requiring NTS.;;;","25/Oct/13 07:46;alexliu68;I understand it comes as the same way as original LOCAL_QUORUM.

SimpleStrategy is for a cluster without multiple DCs. NTS is for multiple DC cluster. SimpleStrategy could be treated as a special NTS that is only for a one DC cluster.

The default CL of Cassandra Hadoop integration is LOCAL_ONE now, it should work for all types of strategy.  LOCAL_ONE looks like a better candidate as a default CL than ONE. When people first start using C*, it is common to be a one DC cluster, then later it may grow up to  a multiple DC cluster. If LOCAL_ONE work for all types of strategy,  manual change the default CL is not needed.  If we keep LOCAL_* requiring NTS, we has to update all the hadoop/pig examples and tests to use ONE. 

The main reason to make LOCAL_* work for all types of strategy is that LOCAL_* are better candidates as default CLs. The default CL should work for all types of strategy.

;;;","25/Oct/13 15:53;brandon.williams;I agree that having LOCAL_ONE be the hadoop default is advantageous, and having hadoop not work out of the box for those that just want to play with it (using SS) isn't very nice.;;;","07/Nov/13 05:52;jasobrown;I'm fine with making LOCAL_ONE work with SS, but we should also make LOCAL_QUORUM do so, as well. If there's no objections to making the LOCAL_*'s consistent, let's move forward. I'll review the patch by the morning.;;;","07/Nov/13 06:10;jasobrown;committed to 1.2, 2.0, and trunk.;;;","07/Nov/13 12:51;aleksey;If we made both LO and LQ work with SS, why not lift the restriction for EACH_QUORUM as well, for consistency-sake?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Authentication is broken for the protocol v1 on C* 2.0,CASSANDRA-6233,12675250,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,23/Oct/13 13:56,16/Apr/19 09:32,14/Jul/23 05:53,28/Oct/13 09:31,2.0.2,,,,,,3,,,,,"CASSANDRA-5664 simplified the decoding method of CredentialsMessage by using CBUtil.readStringMap (instead of duplicating the code). Unfortunately, that latter method turns his keys to uppercase (to provide some form of case insensitivity for keys), and in the case of CredentialsMessage this breaks PasswordAuthenticator that expect lowercased keys (besides, it's a bad idea to mess up with the case of the credentials map in general).

Making CBUtil.readStringMap uppercase keys was probably a bad idea in the first place (as nothing in the method name imply this), so attaching patch that remove this (and uppercase keys specifically in StartupMessage where that was done on purpose).",,aleksey,enigmacurry,slebresne,vchekan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6203,,,,,,,,,CASSANDRA-6243,,,,,,,,,,,"23/Oct/13 13:57;slebresne;6233.txt;https://issues.apache.org/jira/secure/attachment/12609856/6233.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,354870,,,Sun Oct 27 19:01:24 UTC 2013,,,,,,,,,,"0|i1p6gv:",355159,,,,,,,,,aleksey,,aleksey,Normal,,2.0.1,,,,,,,,,,,,,,,,"24/Oct/13 20:13;vchekan;It seems authenticated login is not covered by any unit tests...
Would it be better to use apache's CaseInsensitiveMap?
http://commons.apache.org/proper/commons-collections/javadocs/api-release/org/apache/commons/collections4/map/CaseInsensitiveMap.html;;;","25/Oct/13 18:11;enigmacurry;[~vchekan] there are [multiple authentication tests|https://github.com/riptano/cassandra-dtest/blob/master/auth_test.py] in cassandra-dtest.

[~slebresne] - What is protocol v1? Can you give me an example of how this breaks?

I tried [creating this test|https://github.com/EnigmaCurry/cassandra-dtest/commit/11042a79a3190211e40981271cfcca9c06aee456] and it's passing on cassandra-2.0, but also on 2.0.1, so I must not be exercising the same thing you're describing here. 
I also notice that the patch looks like it's already been committed to cassandra-2.0 branch (not sure if that was intentional or not.);;;","26/Oct/13 12:25;slebresne;I'm talking of the native protocol. cassandra-dtest uses CQL-over-thrift so there is no way to reproduce this bug with it. To produce, you'd need to for example use the Datastax java driver 1.0.4 against C* 2.0.1. The steps to reproduce are there: https://datastax-oss.atlassian.net/browse/JAVA-190;;;","26/Oct/13 13:02;aleksey;Committed by Sylvain in 86b26b67fe9dd804b84a56c2535726b966d28d13, so is part of 2.0.2. 'formal' +1 here. (needs updating CHANGES.txt).;;;","27/Oct/13 19:01;slebresne;Hum, I think I screwed up, didn't meant to commit it before the review. But well, I guess it's done now :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Installation shouldn't fail if /etc/sysctl.d/cassandra is deleted,CASSANDRA-6232,12675165,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,paravoid,paravoid,23/Oct/13 01:03,16/Apr/19 09:32,14/Jul/23 05:53,24/Oct/13 19:48,1.2.12,2.0.3,,Packaging,,,0,,,,,"The Debian package's postinst currently has this snippet code, under the ""configure"" action (i.e. what runs when installing or upgrading):
{code:none}
        if ! sysctl -p /etc/sysctl.d/cassandra.conf; then
            [...]
            rm -v /etc/sysctl.d/cassandra.conf
        fi
{code}

/etc/sysctl.d/cassandra.conf is a conffile and might be removed by the system administrator. The sysadmin might not want this sysctl setting or have an entirely different system of managing the /etc/sysctl.d hierarchy (in our case that would be puppet).

Since this piece of code doesn't check for the existence and doesn't use rm's ""-f"" argument, if the file doesn't exist the rm call fails and the package installation is aborted.

I'd propose checking for the file's existence instead of just ""sysctl -p"", so that you can avoid the nasty warnings too, but adding -f to rm shouldn't hurt either.

Note that this would probably fail on package upgrades under OpenVZ too, which according to the error message should be a supported configuration.",,paravoid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,354785,,,Thu Oct 24 20:35:31 UTC 2013,,,,,,,,,,"0|i1p5xz:",355074,2.0.1,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"24/Oct/13 19:48;brandon.williams;I don't see any harm with adding -f, done in 1f4070dbf2;;;","24/Oct/13 20:35;paravoid;-f works and is a good idea nevertheless, but the warnings mentioning OpenVZ are still echo'ed if the file doesn't exist, which can be confusing.

I'd suggest also making the
{code:none}
        if ! sysctl -p /etc/sysctl.d/cassandra.conf; then
{code}

line into
{code:none}
        if [ -r /etc/sysctl.d/cassandra.conf ] && ! sysctl -p /etc/sysctl.d/cassandra.conf; then
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add snapshot disk space to cfstats,CASSANDRA-6231,12675154,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,jbellis,jbellis,22/Oct/13 23:18,16/Apr/19 09:32,14/Jul/23 05:53,26/Nov/13 21:21,2.1 beta1,,,,,,0,lhf,,,,"As discussed in CASSANDRA-6179, this could help avoid some user confusion, especially when snapshots are autocreated for drop/truncate.",,cburroughs,jre,kohlisankalp,mishail,nickmbailey,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6179,,,"25/Nov/13 21:55;mishail;CASSANDRA-2.0-6231-v2.patch;https://issues.apache.org/jira/secure/attachment/12615682/CASSANDRA-2.0-6231-v2.patch","25/Nov/13 23:54;mishail;CASSANDRA-2.0-6231-v3.patch;https://issues.apache.org/jira/secure/attachment/12615731/CASSANDRA-2.0-6231-v3.patch","23/Nov/13 20:54;mishail;CASSANDRA-2.0-6231.patch;https://issues.apache.org/jira/secure/attachment/12615472/CASSANDRA-2.0-6231.patch","26/Nov/13 20:01;mishail;trunk-6231-v3.patch;https://issues.apache.org/jira/secure/attachment/12615898/trunk-6231-v3.patch",,,,,,,,,,,,,,,,,4.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,354774,,,Tue Nov 26 21:21:15 UTC 2013,,,,,,,,,,"0|i1p5vj:",355063,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"23/Nov/13 20:54;mishail;Patch: added a method to {{Directories}} to calculate an allocated size in ""snapshots"" directory;;;","25/Nov/13 18:03;jbellis;How hard would it be to break out ""space used by sstables that aren't live anymore?""  Could be confusing to double-count (or 10x-count or more with lots of snapshots).;;;","25/Nov/13 21:55;mishail;Patch v2. Separately report a space used by obsolete sstables. Assuming ""obsolete"" are those not reported by {{SSTableLister}};;;","25/Nov/13 22:19;jbellis;- Both numbers need to be ""de-duplicated"" across snapshots right?
- Looks like we could cut down on the file walking if we restricted it to the subdirectory of the CFS in question;;;","25/Nov/13 22:37;mishail;bq. Both numbers need to be ""de-duplicated"" across snapshots right?
I will fix that

bq. Looks like we could cut down on the file walking if we restricted it to the subdirectory of the CFS in question
We start walking from ""snapshots"" subdirectory of the CFS in question. Not sure how it can be reduced/restricted further.;;;","25/Nov/13 22:51;jbellis;Ah, right.  Then isn't the prefix check redundant?;;;","25/Nov/13 22:56;mishail;bq. Ah, right. Then isn't the prefix check redundant?

That is to separate the CF in question and its ""index"" CFs, they are reported separately in cfstats.  ;;;","25/Nov/13 23:06;mishail;And it seems that we only have to report that ""obsolete"" number. Because it makes no sense to count those from snapshots which are just links to existing files.;;;","25/Nov/13 23:12;jbellis;Makes sense to me.;;;","25/Nov/13 23:54;mishail;Patch v3. Count only the size of snapshot files (once for each file) which are not links for ""live"" SSTables.;;;","26/Nov/13 05:21;jbellis;LGTM!

Last thing, the size methods in CFSMBean are deprecated because we're moving to CFMetrics.  Not much reason to add this to both places; let's just do it in metrics.  (So it's probably easier to just do this for trunk since otherwise you'd have to do support both in nodecmd.);;;","26/Nov/13 19:22;nickmbailey;I'd just like to make a note that we've (opscenter) have seen issues with inspecting snapshots for LCS column families. LCS can create a very large number of sstable files (anywhere from 10k to 100k+ range) , and just storing strings for all the file names was giving us some issues. In our case we were dealing with much smaller heap sizes though. Also this is already handling duplicate sstables, but it doesn't sound unreasonable that the number of distinct files could get extremely large, even by just taking daily snapshots.;;;","26/Nov/13 19:37;jbellis;100k distinct filenames would be about 50MB of heap at 500 bytes each, and 16TB worth of data on disk.  I'm okay with those numbers.  (If you're still using 5MB sstables you should probably fix that before calling this, but you probably already have more important reasons to fix that.);;;","26/Nov/13 20:01;mishail;V3 of the patch with changes for trunk. (Use o.a.c.metrcis);;;","26/Nov/13 21:21;jbellis;committed!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
replace doesn't clean up system.peers if you have a new IP,CASSANDRA-6217,12674536,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,jjordan,jjordan,18/Oct/13 14:46,16/Apr/19 09:32,14/Jul/23 05:53,18/Oct/13 16:14,1.2.12,2.0.2,,,,,0,,,,,"When you use replace_token (or replace_node or replace_address) if the new node has a different IP, the old node will still be in system.peers",,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/13 15:52;brandon.williams;6217.txt;https://issues.apache.org/jira/secure/attachment/12609161/6217.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,354158,,,Fri Oct 18 16:14:31 UTC 2013,,,,,,,,,,"0|i1p24f:",354450,1.2.10,,,,,,,,jjordan,,jjordan,Normal,,1.2.0 beta 1,,,,,,,,,,,,,,,,"18/Oct/13 15:52;brandon.williams;Existing nodes will already have the correct peers state due to the token conflict(s) from the replacer, but the replacer will still have the dead node in its own peers table.  The simplest thing to do is finish replacing by removing the replace_address from the table, since either it will be our own (which should not appear there) or it will be the old node.  Trivial patch to do so.;;;","18/Oct/13 16:12;jjordan;+1 LGTM;;;","18/Oct/13 16:14;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updating Pig to 0.11.1 breaks the existing Pig driver,CASSANDRA-6213,12674372,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,17/Oct/13 18:45,16/Apr/19 09:32,14/Jul/23 05:53,17/Oct/13 19:06,,,,,,,0,,,,,"Current trunk upgrades Pig to 0.11.1 which causes the Pig storages code can't compile. Pig storages need implement the new API method cleanupOnSuccess(String,Job).",,alexliu68,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/13 18:55;alexliu68;6213-trunk.txt;https://issues.apache.org/jira/secure/attachment/12608999/6213-trunk.txt",,,,,,,,,,,,,,,,,,,,1.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353994,,,Tue Jan 07 15:09:17 UTC 2014,,,,,,,,,,"0|i1p13z:",354286,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"17/Oct/13 18:56;alexliu68;Fixed compiling error in trunk;;;","17/Oct/13 19:06;brandon.williams;Committed;;;","07/Jan/14 10:28;jeromatron;[~alexliu68] [~brandon.williams] Is there a reason this can't get merged into 2.0 so that 2.0 can use Pig 0.11.1 and 0.12.0?;;;","07/Jan/14 14:25;brandon.williams;We aren't going to change a lib version in a minor release.  We could backport this, but since we build with pig you'd have to roll your own custom release to use 0.11 or 0.12.;;;","07/Jan/14 14:42;jeromatron;ok.  unfortunate but makes sense since we've had issues with updating dependency libs in a minor release in the past.;;;","07/Jan/14 14:52;jeromatron;Can we update our dependency to 0.12 so that we're current with the pig stable releases before 2.1 is released?;;;","07/Jan/14 15:09;jeromatron;I created a separate ticket so we can revisit upgrading before going to beta with 2.1: CASSANDRA-6556.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
TimestampType doesn't support pre-epoch long,CASSANDRA-6212,12674282,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,simonhopkin,simonhopkin,17/Oct/13 09:15,16/Apr/19 09:32,14/Jul/23 05:53,21/Oct/13 07:21,2.0.2,,,Legacy/CQL,,,0,,,,,"org.apache.cassandra.db.marshal.TimestampType.dateStringToTimestamp() contains a regular expression that checks to see if the String argument contains a number.  If so it parses it as a long timestamp.  However pre-epoch timestamps are negative and the code doesn't account for this so it tries to parse it as a formatted Date.  A tweak to the regular expression in TimestampType.dateStringToTimestamp() would solve this issue.

I could use formatted date strings instead, but the TimestampType date parser uses ISO8601 patterns which would cause the timestamp to be rounded to the nearest second.

Currently I get the following exception message:

unable to coerce '-86400000' to a  formatted date (long)

",,aleksey,mishail,simonhopkin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/13 06:55;mishail;cassandra-2.0-6212.patch;https://issues.apache.org/jira/secure/attachment/12609370/cassandra-2.0-6212.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353904,,,Mon Oct 21 12:50:34 UTC 2013,,,,,,,,,,"0|i1p0jz:",354196,2.0.0,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"21/Oct/13 04:02;jbellis;Can you take a stab at this, Mikhail?;;;","21/Oct/13 06:55;mishail;Changed the regexp to accept a leading ""-"";;;","21/Oct/13 07:21;aleksey;Thanks, committed.

Haven't included the unit test, though - this particular one is better suited for the dtests.;;;","21/Oct/13 12:50;simonhopkin;Thanks for the quick turnaround on this issue guys.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in system.log,CASSANDRA-6211,12674276,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,ash2k,ash2k,17/Oct/13 08:06,16/Apr/19 09:32,14/Jul/23 05:53,23/Oct/13 12:30,2.0.2,,,,,,0,npe,nullpointerexception,,,"I wrote a stresstest to test C* and my code that uses CAS heavily. I see strange exception messages in logs:
{noformat}
ERROR [MutationStage:320] 2013-10-17 13:59:10,710 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:320,5,main]
java.lang.NullPointerException
ERROR [MutationStage:328] 2013-10-17 13:59:10,718 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:328,5,main]
java.lang.NullPointerException
ERROR [MutationStage:327] 2013-10-17 13:59:10,732 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:327,5,main]
java.lang.NullPointerException
ERROR [MutationStage:325] 2013-10-17 13:59:10,750 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:325,5,main]
java.lang.NullPointerException
ERROR [MutationStage:326] 2013-10-17 13:59:10,762 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:326,5,main]
java.lang.NullPointerException
ERROR [MutationStage:330] 2013-10-17 13:59:10,768 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:330,5,main]
java.lang.NullPointerException
ERROR [MutationStage:331] 2013-10-17 13:59:10,775 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:331,5,main]
java.lang.NullPointerException
ERROR [MutationStage:334] 2013-10-17 13:59:10,789 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:334,5,main]
java.lang.NullPointerException
ERROR [MutationStage:329] 2013-10-17 13:59:10,803 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:329,5,main]
java.lang.NullPointerException
ERROR [MutationStage:335] 2013-10-17 13:59:10,812 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:335,5,main]
java.lang.NullPointerException
ERROR [MutationStage:333] 2013-10-17 13:59:10,826 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:333,5,main]
java.lang.NullPointerException
ERROR [MutationStage:332] 2013-10-17 13:59:10,834 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:332,5,main]
java.lang.NullPointerException
ERROR [MutationStage:337] 2013-10-17 13:59:10,842 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:337,5,main]
java.lang.NullPointerException
ERROR [MutationStage:336] 2013-10-17 13:59:10,859 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:336,5,main]
java.lang.NullPointerException
ERROR [MutationStage:338] 2013-10-17 13:59:10,870 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:338,5,main]
java.lang.NullPointerException
ERROR [MutationStage:339] 2013-10-17 13:59:10,884 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:339,5,main]
java.lang.NullPointerException
ERROR [MutationStage:341] 2013-10-17 13:59:10,894 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:341,5,main]
java.lang.NullPointerException
ERROR [MutationStage:340] 2013-10-17 13:59:10,910 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:340,5,main]
java.lang.NullPointerException
ERROR [MutationStage:344] 2013-10-17 13:59:10,920 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:344,5,main]
java.lang.NullPointerException
{noformat}","java version ""1.7.0_25""
Java(TM) SE Runtime Environment (build 1.7.0_25-b15)
Java HotSpot(TM) 64-Bit Server VM (build 23.25-b01, mixed mode)

Linux hostname 2.6.32-279.el6.x86_64 #1 SMP Thu Jun 21 15:00:18 EDT 2012 x86_64 x86_64 x86_64 GNU/Linux
",ash2k,mishail,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/13 08:08;slebresne;6211.txt;https://issues.apache.org/jira/secure/attachment/12609820/6211.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353898,,,Wed Oct 23 12:30:35 UTC 2013,,,,,,,,,,"0|i1p0in:",354190,2.0.1,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"21/Oct/13 04:03;jbellis;I'm not sure how you've gotten an NPE logged without the rest of the stacktrace.  Are you doing anything unusual in deploying or configuring the log?;;;","21/Oct/13 04:30;ash2k;No, nothing special. Just a fresh deployment. By the way, client got no exceptions. All requests succeded and data was readable after that. So probably these exceptions are happening asynchronously to the request processing.;;;","21/Oct/13 04:32;ash2k;Also, I can add that these exceptions happen periodically in ""chunks"" of ~20.;;;","21/Oct/13 09:21;benedict;I've seen this before - especially with NPE the VM can optimise away the stack trace in certain cases (helpful, right?)   - seen it especially a problem in small highly parallelized workloads.

Try running with -XX:-OmitStackTraceInFastThrow to see if it helps
;;;","23/Oct/13 04:13;ash2k;{noformat}
ERROR [MutationStage:27] 2013-10-23 10:11:35,507 CassandraDaemon.java (line 185) Exception in thread Thread[MutationStage:27,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.serializers.UUIDSerializer.deserialize(UUIDSerializer.java:32)
	at org.apache.cassandra.serializers.UUIDSerializer.deserialize(UUIDSerializer.java:26)
	at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:142)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getUUID(UntypedResultSet.java:127)
	at org.apache.cassandra.db.SystemKeyspace.loadPaxosState(SystemKeyspace.java:812)
	at org.apache.cassandra.service.paxos.PaxosState.propose(PaxosState.java:94)
	at org.apache.cassandra.service.paxos.ProposeVerbHandler.doVerb(ProposeVerbHandler.java:34)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{noformat};;;","23/Oct/13 08:08;slebresne;When we load the paxos state, we assume that if the state is not empty then it must have an in_progress_ballot. But that's not guaranteed (at least not with the current code) since a node could get a commit without ever having seen the prepare/propose (it was dead for them or is just lagging behind). Besides, another reason (for having a non-empty state but no in_progress_ballot) is that we use TTL and the in_progress_ballot is the first thing that would TTL.

Anyway, attaching simple patch that test for the presence of in_progress_ballot.;;;","23/Oct/13 12:18;jbellis;+1;;;","23/Oct/13 12:30;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair hangs when a new datacenter is added to a cluster,CASSANDRA-6210,12674215,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,rspitzer,rspitzer,16/Oct/13 23:01,16/Apr/19 09:32,14/Jul/23 05:53,28/Jan/14 15:21,2.0.5,,,,,,0,,,,,"Attempting to add a new datacenter to a cluster seems to cause repair operations to break. I've been reproducing this with 20~ node clusters but can get it to reliably occur on 2 node setups.

{code}
##Basic Steps to reproduce
#Node 1 is started using GossipingPropertyFileSnitch as dc1
#Cassandra-stress is used to insert a minimal amount of data
$CASSANDRA_STRESS -t 100 -R org.apache.cassandra.locator.NetworkTopologyStrategy  --num-keys=1000 --columns=10 --consistency-level=LOCAL_QUORUM --average-size-values -
-compaction-strategy='LeveledCompactionStrategy' -O dc1:1 --operation=COUNTER_ADD
#Alter ""Keyspace1""
ALTER KEYSPACE ""Keyspace1"" WITH replication = {'class': 'NetworkTopologyStrategy', 'dc1': 1 , 'dc2': 1 };
#Add node 2 using GossipingPropertyFileSnitch as dc2
run repair on node 1
run repair on node 2
{code}

The repair task on node 1 never completes and while there are no exceptions in the logs of node1, netstat reports the following repair tasks
{code}
Mode: NORMAL
Repair 4e71a250-36b4-11e3-bedc-1d1bb5c9abab
Repair 6c64ded0-36b4-11e3-bedc-1d1bb5c9abab
Read Repair Statistics:
Attempted: 0
Mismatch (Blocking): 0
Mismatch (Background): 0
Pool Name                    Active   Pending      Completed
Commands                        n/a         0          10239
Responses                       n/a         0           3839
{code}

Checking on node 2 we see the following exceptions
{code}
ERROR [STREAM-IN-/10.171.122.130] 2013-10-16 22:42:58,961 StreamSession.java (line 410) [Stream #4e71a250-36b4-11e3-bedc-1d1bb5c9abab] Streaming error occurred
java.lang.NullPointerException
        at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(ConnectionHandler.java:174)
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:436)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:293)
        at java.lang.Thread.run(Thread.java:724)
...
ERROR [STREAM-IN-/10.171.122.130] 2013-10-16 22:43:49,214 StreamSession.java (line 410) [Stream #6c64ded0-36b4-11e3-bedc-1d1bb5c9abab] Streaming error occurred
java.lang.NullPointerException
        at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(ConnectionHandler.java:174)
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:436)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:293)
        at java.lang.Thread.run(Thread.java:724)
{code}

Netstats on node 2 reports
{code}
automaton@ip-10-171-15-234:~$ nodetool netstats
Mode: NORMAL
Repair 4e71a250-36b4-11e3-bedc-1d1bb5c9abab
Read Repair Statistics:
Attempted: 0
Mismatch (Blocking): 0
Mismatch (Background): 0
Pool Name                    Active   Pending      Completed
Commands                        n/a         0           2562
Responses                       n/a         0           4284

{code}
","Amazon Ec2
2 M1.large nodes",cburroughs,rcoli,rspitzer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jan/14 22:28;yukim;6210-2.0.txt;https://issues.apache.org/jira/secure/attachment/12623737/6210-2.0.txt","19/Dec/13 16:58;rspitzer;RepairLogs.tar.gz;https://issues.apache.org/jira/secure/attachment/12619601/RepairLogs.tar.gz","17/Jan/14 18:00;rspitzer;patch_1_logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12623694/patch_1_logs.tar.gz",,,,,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353837,,,Tue Jan 28 15:21:29 UTC 2014,,,,,,,,,,"0|i1p053:",354129,2.0.1,,,,,,,,rspitzer,,rspitzer,Normal,,2.0.0,,,,,,,,,,,,,,,,"17/Oct/13 08:53;jbellis;Is this just ""I can't repair until bootstrapping is finished?"";;;","17/Oct/13 17:13;rspitzer;I see this effect with both auto_bootstrap: false (in cassandra.yaml). Although if it is just a bootstrapping issue, I think we should have a better failure mode.  I'll try doing the keyspace alteration after adding the new node and see if that fixes things.;;;","17/Oct/13 17:16;rspitzer;Double checked that auto_bootstrap was false on both nodes. Re ran test

On node 1 (netstats)
{code}
Mode: NORMAL
Repair e50825a0-374e-11e3-854d-f97c22942f02
Read Repair Statistics:
Attempted: 0
Mismatch (Blocking): 0
Mismatch (Background): 0
Pool Name                    Active   Pending      Completed
Commands                        n/a         0           2562
Responses                       n/a         0           1284
{code}

On node 2 (System.log)
{code}
ERROR [STREAM-IN-/10.171.10.24] 2013-10-17 17:09:34,102 StreamSession.java (line 410) [Stream #e50825a0-374e-11e3-854d-f97c22942f02] Streaming error occurred
java.lang.NullPointerException
        at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(ConnectionHandler.java:174)
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:436)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:293)
        at java.lang.Thread.run(Thread.java:724){code};;;","17/Oct/13 17:28;rspitzer;-Moving the alter of the keyspace to after bringing up the second Node fixes this.-

I spoke to soon it failed on the second trial.;;;","17/Oct/13 19:03;brandon.williams;Can't repro on the 2.0 branch with these steps.

bq. Moving the alter of the keyspace to after bringing up the second Node fixes this.

That's because there's nothing to repair without the second replica defined.;;;","17/Oct/13 19:39;rspitzer;If I execute nodetool rebuild before doing the repair then repair completes successfully (because it doesn't have to stream anything?) ;;;","17/Oct/13 19:54;rspitzer;The same test works on 1.2.10 (DSE w/o vnodes);;;","17/Oct/13 20:17;rspitzer;[~brandon.williams], I moved the alter statement too after bringing up the second node, but not after running repair. So it still has data to repair.

So this sequence still has a bug
{code}
Node 1 up dc1
Stress
Node 2 up dc2
Alter keyspace
repair on node1
{code}

But this sequence seems to work fine
{code}
node 1 up
stress
node 2 up
Alter Keyspace
Nodetool rebuild on node 2
Repair
{code};;;","17/Oct/13 20:36;brandon.williams;Can you try 2.0 HEAD?;;;","17/Oct/13 21:12;rspitzer;Same error on 2.0 Head, when I try to repair without issuing the rebuild.

Last commit
{code}
commit 886f16ca1c14f1209151579408b59cd699832cbf
Author: Brandon Williams <brandonwilliams@apache.org>
Date:   Thu Oct 17 13:37:04 2013 -0500
{code}




;;;","17/Oct/13 22:16;rspitzer;The problem doesn't manifest when i'm not using vnodes on 2.0-HEAD;;;","17/Dec/13 21:10;yukim;I followed the steps bellow:

{code}
Node 1 up dc1
Stress
Node 2 up dc2
Alter keyspace
repair on node1
{code}

And with auto_bootstrap: false, I got the following and repair hung:

{code}
ERROR [AntiEntropyStage:1] 2013-12-17 15:03:08,945 CassandraDaemon.java (line 187) Exception in thread Thread[AntiEntropyStage:1,5,main]
java.lang.AssertionError: Unknown keyspace Keyspace1
        at org.apache.cassandra.db.Keyspace.<init>(Keyspace.java:262)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:110)
        at org.apache.cassandra.db.Keyspace.open(Keyspace.java:88)
        at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:46)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:744)
{code}

We should 'catch-all' in RepairVerbHandler to prevent hang at least.
It was not the same exception.

[~rspitzer], can you reproduce with 'log4j.logger.org.apache.cassandra.streaming=DEBUG' in your log4j-server.properties and attach the log here?;;;","18/Dec/13 05:26;rspitzer;Sure, It was a while back so things have most likely changed in the interim. I'll get the test running tomorrow with the DEBUG statements turned on. ;;;","18/Dec/13 23:36;rspitzer;Repair running on this node
{code}
 INFO [AntiEntropyStage:1] 2013-12-18 22:39:28,209 StreamResultFuture.java (line 82) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Executing streaming plan for Repair
 INFO [AntiEntropyStage:1] 2013-12-18 22:39:28,209 StreamResultFuture.java (line 86) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Beginning stream session with /10.171.81.22
DEBUG [StreamConnectionEstablisher:2] 2013-12-18 22:39:28,210 ConnectionHandler.java (line 78) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Sending stream init for incoming stream
DEBUG [StreamConnectionEstablisher:2] 2013-12-18 22:39:28,211 ConnectionHandler.java (line 84) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Sending stream init for outgoing stream
DEBUG [STREAM-OUT-/10.171.81.22] 2013-12-18 22:39:28,212 ConnectionHandler.java (line 356) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Sending Prepare (1 requests,  2 files}
{code}

On requested node
{code}
DEBUG [STREAM-IN-/10.172.27.174] 2013-12-18 22:39:28,296 ConnectionHandler.java (line 292) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Received Prepare (1 requests,  2 files}
ERROR [STREAM-IN-/10.172.27.174] 2013-12-18 22:39:28,314 StreamSession.java (line 410) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Streaming error occurred
java.lang.NullPointerException
        at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(ConnectionHandler.java:174)
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:436)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:293)
        at java.lang.Thread.run(Thread.java:724)
DEBUG [STREAM-IN-/10.172.27.174] 2013-12-18 22:39:28,316 ConnectionHandler.java (line 153) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Closing stream connection handler on /10.172.27.174
 INFO [STREAM-IN-/10.172.27.174] 2013-12-18 22:39:28,317 StreamResultFuture.java (line 181) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Session with /10.172.27.174 is complete
 WARN [STREAM-IN-/10.172.27.174] 2013-12-18 22:39:28,317 StreamResultFuture.java (line 210) [Stream #40d875d0-6835-11e3-a172-3729e500a0e7] Stream failed
{code};;;","19/Dec/13 16:58;rspitzer;Attached RepairLogs.tar.gz Which has logs from all the nodes involved

They contain data from two trial runs, one where counter data is attempted to be repaired, and one where standard inserts are repaired. The Shutdown indicates the separation between these tests. 

Null pointers are seen on 10.171.49.137 and 10.171.81.22;;;","23/Dec/13 15:56;yukim;Thanks Russell.

From the log, I can see streaming session on receiver side did not set up correctly, though sender thought we were ready. Sender wrote stream init message through SocketChannel API but the message was not sent/received.
I think there are two ways to fix this. 1) Sender waits for ACK from receiver or 2) Receiver holds response to sender until ready.
I'm tend to fix this using 2) rather than add another message for ACK, since the next message sender would send is only one stream PREPARE message.

Let me work on that patch as well as to fix AE I encountered above.;;;","16/Jan/14 23:32;yukim;I think the actual cause of this is StreamInitMessage is not completely written. We should have checked if SocketChannel#write does write out content of message buffer. Patch attached to fix this.

[~rspitzer] Can you test with the patch applied to 2.0 branch?;;;","17/Jan/14 17:51;rspitzer;Ran the test again last night and repair reported the following exceptions, I'll have the logs up in a moment.

Setup:
4 Nodes, 2 per DC

{code}
ERROR [AntiEntropySessions:2] 2014-01-17 06:59:13,320 RepairSession.java (line 278) [repair #def293b0-7f44-11e3-b180-d1c68624042f] session completed with the following error
org.apache.cassandra.exceptions.RepairException: [repair #def293b0-7f44-11e3-b180-d1c68624042f on Keyspace1/Standard1, (-4559856749309798061,-4559456353371206248]] Sync failed between /10.171.121.18 and /10.196.16.123
        at org.apache.cassandra.repair.RepairSession.syncComplete(RepairSession.java:200)
        at org.apache.cassandra.service.ActiveRepairService.handleMessage(ActiveRepairService.java:204)
        at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:59)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
ERROR [AntiEntropySessions:2] 2014-01-17 06:59:13,325 CassandraDaemon.java (line 192) Exception in thread Thread[AntiEntropySessions:2,5,RMI Runtime]
java.lang.RuntimeException: org.apache.cassandra.exceptions.RepairException: [repair #def293b0-7f44-11e3-b180-d1c68624042f on Keyspace1/Standard1, (-4559856749309798061,-4559456353371206248]] Sync failed between /10.171.121.18 and /10.196.16.123
        at com.google.common.base.Throwables.propagate(Throwables.java:160)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: org.apache.cassandra.exceptions.RepairException: [repair #def293b0-7f44-11e3-b180-d1c68624042f on Keyspace1/Standard1, (-4559856749309798061,-4559456353371206248]] Sync failed between /10.171.121.18 and /10.196.16.123
        at org.apache.cassandra.repair.RepairSession.syncComplete(RepairSession.java:200)
        at org.apache.cassandra.service.ActiveRepairService.handleMessage(ActiveRepairService.java:204)
        at org.apache.cassandra.repair.RepairMessageVerbHandler.doVerb(RepairMessageVerbHandler.java:59)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:60)
        ... 3 more
 INFO [AntiEntropySessions:4] 2014-01-17 06:59:13,328 RepairSession.java (line 236) [repair #df5d6370-7f44-11e3-b180-d1c68624042f] new session: will sync /10.171.121.18, /10.198.2.16 on range (-5516517151222322415,-5504449186624942606] for Keyspace1.[SuperCounter1, Super1, Counter3, Standard1, Counter1]
{code};;;","17/Jan/14 18:00;rspitzer;Logs from patched cluster running repair test;;;","17/Jan/14 22:28;yukim;[~rspitzer] thanks for the log, I'm still seeing the same NPE.

Updated patch attached. This version also enqueues message until connection for output is established. Can you test with it?;;;","17/Jan/14 22:36;rspitzer;Of course, Setting up now.;;;","28/Jan/14 02:21;rspitzer;Newest patch looks good to me. Passes extended repair tests.;;;","28/Jan/14 15:21;yukim;Thanks for testing.
Committed to 2.0 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE while saving cache,CASSANDRA-6208,12674161,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,brandon.williams,brandon.williams,16/Oct/13 17:38,16/Apr/19 09:32,14/Jul/23 05:53,16/Oct/13 18:47,1.2.11,,,,,,0,,,,,"After running stress with a 2i and leaving the node idle for a while, I receive this trace when the cache tries to autosave:

{noformat}
ERROR 16:07:05,499 Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.config.CFMetaData.<init>(CFMetaData.java:314)
        at org.apache.cassandra.config.CFMetaData.<init>(CFMetaData.java:307)
        at org.apache.cassandra.cache.AutoSavingCache$Writer.<init>(AutoSavingCache.java:213)
        at org.apache.cassandra.cache.AutoSavingCache.getWriter(AutoSavingCache.java:74)
        at org.apache.cassandra.cache.AutoSavingCache.submitWrite(AutoSavingCache.java:176)
        at org.apache.cassandra.cache.AutoSavingCache$1.run(AutoSavingCache.java:90)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{noformat}

CASSANDRA-5732 appears responsible.",,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Oct/13 18:43;samt;0001-CASSANDRA-6208-Removed-new-asserts-from-5732-in-CFMD.patch;https://issues.apache.org/jira/secure/attachment/12608774/0001-CASSANDRA-6208-Removed-new-asserts-from-5732-in-CFMD.patch",,,,,,,,,,,,,,,,,,,,1.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353783,,,Wed Oct 16 18:47:45 UTC 2013,,,,,,,,,,"0|i1oztr:",354075,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"16/Oct/13 17:40;brandon.williams;This affects the current tentative release of 1.2.11;;;","16/Oct/13 18:43;samt;Looks like a couple of the new asserts in CFMD constructor will cause this. They weren't essential to CASSANDRA-5732 anyway, so I've removed all of them just in case there are other cases we've not seen yet.
;;;","16/Oct/13 18:47;brandon.williams;Works for me, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader broken in 2.0 HEAD,CASSANDRA-6205,12673988,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,nickmbailey,nickmbailey,15/Oct/13 21:12,16/Apr/19 09:32,14/Jul/23 05:53,22/Oct/13 15:44,2.0.2,,,,,,0,,,,,"The code for tracking sstable coldness is also executing when running sstableloader which causes problems.

{noformat}
Exception in thread ""main"" java.lang.RuntimeException: Error validating SELECT * FROM sstable_activity WHERE keyspace_name='test_backup_restore' and columnfamily_name='cf0' and generation=1
at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:190)
at org.apache.cassandra.db.SystemKeyspace.getSSTableReadMeter(SystemKeyspace.java:907)
at org.apache.cassandra.io.sstable.SSTableReader.<init>(SSTableReader.java:337)
at org.apache.cassandra.io.sstable.SSTableReader.openForBatch(SSTableReader.java:160)
at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:112)
at java.io.File.list(File.java:1087)
at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:73)
at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:155)
at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:68)
Caused by: org.apache.cassandra.db.KeyspaceNotDefinedException: Keyspace system does not exist
{noformat}",,nickmbailey,slebresne,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Oct/13 22:08;thobbs;6205.patch;https://issues.apache.org/jira/secure/attachment/12609035/6205.patch",,,,,,,,,,,,,,,,,,,,1.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353611,,,Tue Oct 22 15:44:03 UTC 2013,,,,,,,,,,"0|i1oyrz:",353903,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"16/Oct/13 06:50;slebresne;For what that's worth, the patch at CASSANDRA-5894 fixes that too.;;;","17/Oct/13 19:22;thobbs;I haven't verified it, but I think the patch for CASSANDRA-5894 won't fix this because it only avoids opening an SSTR after closing the writer.;;;","17/Oct/13 22:08;thobbs;I verified that CASSANDRA-5894 doesn't fix this.

Attached patch 6205.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6205]) skips the read meter setup when in client mode.

I'm also looking into why the dtest that uses sstableloader didn't catch this.;;;","18/Oct/13 16:23;thobbs;sstableloader was indeed failing on the dtest, but the failure was being ignored by the test.  I've opened a [pull request|https://github.com/riptano/cassandra-dtest/pull/23] to fix the dtest once this is committed.;;;","21/Oct/13 09:27;slebresne;+1;;;","22/Oct/13 15:44;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add compaction, compression to cqlsh tab completion for CREATE TABLE",CASSANDRA-6196,12673630,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,jbellis,jbellis,13/Oct/13 22:25,16/Apr/19 09:32,14/Jul/23 05:53,21/Oct/13 19:18,1.2.12,2.0.2,,Legacy/Tools,,,0,,,,,,,aleksey,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/13 20:07;mishail;cassandra-1.2-6196.patch;https://issues.apache.org/jira/secure/attachment/12609512/cassandra-1.2-6196.patch","21/Oct/13 19:03;mishail;cassandra-2.0-6196.patch;https://issues.apache.org/jira/secure/attachment/12609504/cassandra-2.0-6196.patch",,,,,,,,,,,,,,,,,,,2.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353253,,,Tue Oct 22 06:31:35 UTC 2013,,,,,,,,,,"0|i1owkn:",353546,1.2.10,2.0.1,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"14/Oct/13 05:40;mishail;The completion for CREATE TABLE options is already there, but doesn't work because there are duplicate completers for Properties related stuff.

Attached the patch to remove static Keyspace-only completers, so the dynamic ones (keyspace/columnfamily) will work;;;","21/Oct/13 05:30;aleksey;[~mishail] The issue is present in 1.2, too, so the patch should be 1.2-based. Also, the patch does not fix auto completion for ALTER TABLE. Could you please handle that as well?;;;","21/Oct/13 19:03;mishail;New patch for 2.0 to address ""ALTER TABLE"" as well.

Fixing it for 1.2 will be trickier for me as 1.2 doesn't contain https://github.com/apache/cassandra/commit/7f6ac19efb9a9d51a3ebdb58197c8fe35476034f;;;","21/Oct/13 19:18;aleksey;Committed as is, thanks.

Re: 1.2 - if you break completion for CQL2/CQL3-beta while fixing it for CQL3-proper, I wouldn't object. So if that's the only thing that's stopping you, feel free to break it.;;;","21/Oct/13 20:07;mishail;Patch for 1.2. 
* Removed static KS-only completers
* Fix ""ALTER TABLE"";;;","22/Oct/13 06:31;aleksey;Committed the back port, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in error msg in cqlsh: Bad Request: Only superusers are allowed to perfrom CREATE USER queries,CASSANDRA-6195,12673629,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,harisekhon,harisekhon,13/Oct/13 21:47,16/Apr/19 09:32,14/Jul/23 05:53,13/Oct/13 22:56,1.2.11,2.0.2,,,,,0,,,,,"Typo in error message ""perfrom"" instead of ""perform"":

cqlsh
Connected to MyCluster1 at x.x.x.x:9160.
[cqlsh 4.0.1 | Cassandra 2.0.1 | CQL spec 3.0.0 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> create user hari with password 'mypass';
Bad Request: Only superusers are allowed to perfrom CREATE USER queries",,harisekhon,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353252,,,Sun Oct 13 22:56:22 UTC 2013,,,,,,,,,,"0|i1owkf:",353545,2.0.1,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"13/Oct/13 22:56;brandon.williams;Fixed in 4284d98;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
speculative retry can sometimes violate consistency,CASSANDRA-6194,12673619,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,13/Oct/13 18:56,16/Apr/19 09:32,14/Jul/23 05:53,23/Oct/13 15:52,2.0.2,,,,,,0,,,,,"This is most evident with intermittent failures of the short_read dtests.  I'll focus on short_read_reversed_test for explanation, since that's what I used to bisect.  This test inserts some columns into a row, then deletes a subset, but it performs each delete on a different node, with another node down (hints are disabled.)  Finally it reads the row back at QUORUM and checks that it doesn't see any deleted columns, however with speculative retry on this often fails.  I bisected this to the change that made 99th percentile SR the default reliably by looping the test enough times at each iteration to be sure it was passing or failing.",,aleksey,enigmacurry,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Oct/13 13:53;jbellis;6194.txt;https://issues.apache.org/jira/secure/attachment/12609652/6194.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353242,,,Wed Oct 23 15:52:09 UTC 2013,,,,,,,,,,"0|i1owi7:",353535,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"13/Oct/13 18:58;brandon.williams;Unfortunately I haven't been able to repro manually and ccm has a problem with logging at debug on 2.0+ currently.  10 iterations of the test should be enough to trigger, though I bisected again to the same point with 30.;;;","22/Oct/13 13:53;jbellis;Patch attached to wait for all the contacted replicas on DME.  Not sure how this could cause the test failure though so it's kind of a shot in the dark.;;;","22/Oct/13 17:10;slebresne;For the record, the error returned by the dtest is:
{noformat}
======================================================================
FAIL: short_read_reversed_test (consistency_test.TestConsistency)
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/home/mcmanus/Git/dtest-cassandra/consistency_test.py"", line 309, in short_read_reversed_test
    assert res[i][1] == 'value%d' % (5-i), 'Expecting value%d, got %s (%s)' % (5-i, res[i][1], str(res))
AssertionError: Expecting value5, got value8 ([[u'c000008', u'value8'], [u'c000005', u'value5'], [u'c000004', u'value4']])

----------------------------------------------------------------------
Ran 1 test in 130.035s
{noformat};;;","22/Oct/13 19:44;brandon.williams;Worth noting that there's no real pattern to the columns, other than they're wrong

{noformat}
AssertionError: Expecting value5, got value7 ([[u'c000007', u'value7'], [u'c000005', u'value5'], [u'c000004', u'value4']])
{noformat};;;","22/Oct/13 20:08;jbellis;The problem is that when we speculate, we do multiple data reads, and RowDigestResolver assumes there is only one data read.  (If there is more than one, it does not error out but silently drops all but one.)

So if the speculative read results in triggering the callback's ""we have enough replies to satisfy CL"" logic, and the speculative data read finished before the digest, we effectively do CL.ONE logic instead of CL.QUORUM.

https://github.com/jbellis/cassandra/commits/6194 includes a fix for this and also a fix for DigestMismatch logic with SR.;;;","22/Oct/13 20:36;brandon.williams;This fixes the test for me.;;;","23/Oct/13 15:52;aleksey;LGTM and committed (also made 99PERCENTILE the default again).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cassandra 2.0 won't start up with Java 7u40 with Client JVM.  (works on Server JVM, and both JVMs 7u25)",CASSANDRA-6190,12673612,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,slowenthal,slowenthal,13/Oct/13 17:30,16/Apr/19 09:32,14/Jul/23 05:53,25/Oct/13 20:30,1.2.14,2.0.3,,Local/Config,,,0,,,,,"Java 7u40 on some platforms do not recognize the the -XX:+UseCondCardMark JVM option.  7u40 on Macintosh works correctly,  If I use the tarball 7u40 version of 7, we encounter the error below. I tried 7u25 (the previous release) and it functioned correctly.

ubuntu@ubuntu:~$ Unrecognized VM option 'UseCondCardMark'
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

",Ubuntu 13.04 32- and 64-bit  JDK 7u40  (tried JRE 7u25),enigmacurry,jeromatron,pmcfadin,slowenthal,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Oct/13 13:35;brandon.williams;6190.txt;https://issues.apache.org/jira/secure/attachment/12608487/6190.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353235,,,Wed Jan 29 15:07:38 UTC 2014,,,,,,,,,,"0|i1owgn:",353528,,,,,,,,,urandom,,urandom,Normal,,,,,,,,,,,,,,,,,,"13/Oct/13 17:39;jbellis;So 7u40 doesn't work on Ubuntu, but does work on OS X?

Is this Oracle JDK or OpenJDK?;;;","13/Oct/13 17:57;brandon.williams;Oracle 7u40 runs just fine on Debian, I can't imagine it wouldn't work on Ubuntu.;;;","13/Oct/13 17:59;slowenthal;This is Oracle JDK.   It works on OSX.  I checked system.log and jconsole on mac to ensure that it's picked up, and we are running 7u40, and both are true.;;;","13/Oct/13 18:30;pmcfadin;Just tried c* 2.0.1 using Oracle JDK 7u40 on CentOS 6.4. Works without modification. ;;;","13/Oct/13 18:38;jbellis;I wonder if Steve has a 1.6 jvm installed that is being picked up instead.  (Especially since he mentions tarballs.);;;","13/Oct/13 18:45;brandon.williams;That would be my guess.  Steve, can you paste the output of 'dpkg -l | grep jre' and if you have results, trying removing all those packages so there can only be one java install on the system? (the oracle one, which I assume is from a tarball);;;","13/Oct/13 18:46;enigmacurry;[~slowenthal] Put an 'echo $JAVA' as the first line of the launch_service() function in bin/cassandra and you can verify that you're using the right java.;;;","13/Oct/13 22:06;slowenthal;I've even tried it by explicitly setting JAVA_HOME to point at my various javas.  Remember - if we aren't using Java 7, we don't fall into the code that adds that parameter.   Java 7 gets put in the system.log.;;;","14/Oct/13 01:47;slowenthal;Here is a simple test.  Not the ""FileNotFoundException"" is good - I ran C* with the wrong permissions, so that is the expected result.

Unix Info:
Linux ubuntu 3.8.0-19-generic #29-Ubuntu SMP Wed Apr 17 18:19:42 UTC 2013 i686 i686 i686 GNU/Linux
u


ubuntu@ubuntu:~$ export JAVA_HOME=~/Downloads/jre1.7.0_40/
ubuntu@ubuntu:~$ cassandra
xss =  -ea -javaagent:/usr/share/cassandra/lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1024M -Xmx1024M -Xmn100M -XX:+HeapDumpOnOutOfMemoryError -Xss256k
ubuntu@ubuntu:~$ Unrecognized VM option 'UseCondCardMark'
Error: Could not create the Java Virtual Machine.
Error: A fatal exception has occurred. Program will exit.

ubuntu@ubuntu:~$ export JAVA_HOME=~/Downloads/jre1.7.0_25/
ubuntu@ubuntu:~$ cassandra
xss =  -ea -javaagent:/usr/share/cassandra/lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1024M -Xmx1024M -Xmn100M -XX:+HeapDumpOnOutOfMemoryError -Xss256k
ubuntu@ubuntu:~$ log4j:ERROR setFile(null,true) call failed.
java.io.FileNotFoundException: /var/log/cassandra/system.log (Permission denied)
	at java.io.FileOutputStream.open(Native Method)
	at java.io.FileOutputStream.<init>(Unknown Source)
;;;","15/Oct/13 03:16;slowenthal;It does work on windows - set JAVA_HOME manually to a 7u40 home, and add the JVM option to cassandra.bat. Verified with jconsole.;;;","15/Oct/13 04:14;slowenthal;Mystery Solved:  In 7u40, the option is missing in the Client JVM.  In 7u25, it's available in both the client and server JVM.

ubuntu@ubuntu:~/Downloads$ cd jre1.7.0_40/
ubuntu@ubuntu:~/Downloads/jre1.7.0_40$ find . -type f -exec grep UseCondCard {} +
Binary file ./lib/i386/server/libjvm.so matches

ubuntu@ubuntu:~/Downloads/jre1.7.0_25$ find . -type f -exec grep UseCondCard {} +
Binary file ./lib/i386/server/libjvm.so matches
Binary file ./lib/i386/client/libjvm.so matches
ubuntu@ubuntu:~/Downloads/jre1.7.0_25$ 


;;;","15/Oct/13 05:46;slowenthal;Oracle Bug report filed 9007478 

Dear Java Developer,

Thank you for reporting this issue.

We have determined that this report is a new bug and have entered the bug into our bug tracking system under Bug Id: 9007478 . You can look for related issues on the Java Bug Database at http://bugs.sun.com.

We will try to process all newly posted bugs in a timely manner, but we make no promises about the amount of time in which a bug will be fixed. If you just reported a bug that could have a major impact on your project, consider using one of the technical support offerings available at Oracle Support.

Thanks again for your submission!

Regards,
Java Developer Support;;;","15/Oct/13 05:57;slowenthal;It looks like java defaults to client if the machine has only 1 core.  Our training VM was created with a single core.;;;","15/Oct/13 10:47;jbellis;It has to be 32bit as well. http://docs.oracle.com/javase/7/docs/technotes/guides/vm/server-class.html;;;","15/Oct/13 13:35;brandon.williams;Patch to also check for 64bit java when adding the UseConfCardMark flag.  We complain about 32bit JVMs later, so this at least gets us past the confusing error message.;;;","16/Oct/13 05:20;slowenthal;Here is a response from Oracle:

Hi Steve,
  I just wanted to close the loop on the bug you filled about UseCondCardMark not being available with the client JVM in 7u40.  The issue is that UseCondCardMark was never really available in client.  It was a NOP, and in 7u40 we moved the flag to work only with server as it should be.  Sorry for the confusion.


--
Azeem Jiva
@javawithjiva
;;;","16/Oct/13 05:23;slowenthal;Also 7u45 is out today, and it exhibits the same problem.;;;","16/Oct/13 12:27;brandon.williams;Can you test with my patch?;;;","25/Oct/13 20:22;urandom;+1, lgtm;;;","25/Oct/13 20:30;brandon.williams;Committed;;;","29/Jan/14 14:50;jeromatron;would this make sense to backport to the 1.2 line?  It's been reproduced with a 32 bit JDK 7 with 1.2.13.;;;","29/Jan/14 14:58;brandon.williams;Done.;;;","29/Jan/14 15:07;jeromatron;Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can't update int column to blob type.,CASSANDRA-6185,12673491,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,nickmbailey,nickmbailey,11/Oct/13 20:45,16/Apr/19 09:32,14/Jul/23 05:53,21/Oct/13 08:16,1.2.12,2.0.2,,,,,0,,,,,"Patch for dtests:

{noformat}
diff --git a/cql_tests.py b/cql_tests.py
index 11461e4..405c998 100644
--- a/cql_tests.py
+++ b/cql_tests.py
@@ -1547,35 +1547,35 @@ class TestCQL(Tester):
             CREATE TABLE test (
                 k text,
                 c text,
-                v text,
+                v int,
                 PRIMARY KEY (k, c)
             )
         """""")

-        req = ""INSERT INTO test (k, c, v) VALUES ('%s', '%s', '%s')""
+        req = ""INSERT INTO test (k, c, v) VALUES ('%s', '%s', %d)""
         # using utf8 character so that we can see the transition to BytesType
-        cursor.execute(req % ('ɸ', 'ɸ', 'ɸ'))
+        cursor.execute(req % ('ɸ', 'ɸ', 1))

         cursor.execute(""SELECT * FROM test"")
         cursor.execute(""SELECT * FROM test"")
         res = cursor.fetchall()
-        assert res == [[u'ɸ', u'ɸ', u'ɸ']], res
+        assert res == [[u'ɸ', u'ɸ', 1]], res

         cursor.execute(""ALTER TABLE test ALTER v TYPE blob"")
         cursor.execute(""SELECT * FROM test"")
         res = cursor.fetchall()
         # the last should not be utf8 but a raw string
-        assert res == [[u'ɸ', u'ɸ', 'ɸ']], res
+        assert res == [[u'ɸ', u'ɸ', '\x00\x00\x00\x01']], res

         cursor.execute(""ALTER TABLE test ALTER k TYPE blob"")
         cursor.execute(""SELECT * FROM test"")
         res = cursor.fetchall()
-        assert res == [['ɸ', u'ɸ', 'ɸ']], res
+        assert res == [['ɸ', u'ɸ', '\x00\x00\x00\x01']], res

         cursor.execute(""ALTER TABLE test ALTER c TYPE blob"")
         cursor.execute(""SELECT * FROM test"")
         res = cursor.fetchall()
-        assert res == [['ɸ', 'ɸ', 'ɸ']], res
+        assert res == [['ɸ', 'ɸ', '\x00\x00\x00\x01']], res

     @since('1.2')
     def composite_row_key_test(self):
{noformat}

",,aleksey,mbulman,nickmbailey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/13 08:30;slebresne;6185.txt;https://issues.apache.org/jira/secure/attachment/12608254/6185.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353114,,,Mon Oct 21 08:16:46 UTC 2013,,,,,,,,,,"0|i1ovof:",353401,,,,,,,,,aleksey,,aleksey,Low,,1.2.9,,,,,,,,,,,,,,,,"11/Oct/13 20:58;brandon.williams;CASSANDRA-5882 is responsible.;;;","14/Oct/13 08:30;slebresne;Currently, our type compatibility is based on comparison compatibility, and since 'blob' does not compare like 'int', this is refused. That being said, except for clustering columns, we can use a more permissive compatibilty check that don't take comparison order into account. Attaching patch that does that (the patch also slightly clean-up/improve the validation made by AlterTableStatement).
;;;","14/Oct/13 08:45;jbellis;Let's stick with 2.0 for this since it's new functionality and a bit involved.

Edit: or is this actually a regression?;;;","14/Oct/13 09:18;slebresne;bq. or is this actually a regression?

Kind of. In CASSANDRA-5882, we decided on purpose to stop allowing type changes that were not making sense. In doing so, we've been too restrictive, disallowing some type changes that stricly speaking make sense, which you could say is a restriction (of CASSANDRA-5882).

I'll note that the patch really only change code related to altering type, which is hardly mission critical, so I don't think there is too much risk in pushing it in 1.2. But personally I don't really care either way.
;;;","14/Oct/13 09:21;jbellis;All right, I'll set it back to 1.2 (1.2.12, which I just created).;;;","14/Oct/13 12:19;aleksey;+1

Comment typos: ATS.announceMigration(), the COLUMN_METADATA case comment ""so CFMetaData.validateCompatility ..."" - Complatility + the same typo in the COLUMN_ALIAS case.;;;","21/Oct/13 07:26;aleksey;^;;;","21/Oct/13 08:16;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Skip mutations that pass CRC but fail to deserialize,CASSANDRA-6183,12673465,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,11/Oct/13 17:22,16/Apr/19 09:32,14/Jul/23 05:53,14/Oct/13 19:26,1.2.11,2.0.2,,,,,1,,,,,"We've had a couple reports of CL replay failure that appear to be caused by dropping and recreating the same table with a different schema, e.g. CASSANDRA-5905.  While CASSANDRA-5202 is the ""right"" fix for this, it's too involved for 2.0 let alone 1.2, so we need a stopgap until then.",,johnny15676,lyubent,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6200,,,,,,,,,,,,,,,,,,,,"14/Oct/13 18:16;jbellis;6183-v2.txt;https://issues.apache.org/jira/secure/attachment/12608312/6183-v2.txt","11/Oct/13 17:27;jbellis;6183.txt;https://issues.apache.org/jira/secure/attachment/12608034/6183.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353088,,,Mon Oct 14 19:26:29 UTC 2013,,,,,,,,,,"0|i1ovin:",353375,,,,,,,,,lyubent,,lyubent,Low,,,,,,,,,,,,,,,,,,"11/Oct/13 17:27;jbellis;Patch will save skipped mutations out to separate files in case user wishes to perform forensics.;;;","11/Oct/13 17:29;jbellis;Lyuben, can you review?

To test, I suggest something like this:

# Flush, so only the new row is in the CL.
# Create a table and insert a row.  Copy the active CL segment.
# Drop table, recreate w/ incompatible schem (e.g., int -> double)
# Shutdown, move saved CL segment in; restart;;;","14/Oct/13 18:16;jbellis; v2 attached to validate cell names against comparator;;;","14/Oct/13 18:32;lyubent;v2 LGTM. Gist of the [test and output|https://gist.github.com/lyubent/6979885] ;;;","14/Oct/13 19:26;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to modify column_metadata via thrift,CASSANDRA-6182,12673463,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,nickmbailey,nickmbailey,11/Oct/13 17:08,16/Apr/19 09:32,14/Jul/23 05:53,25/Oct/13 17:11,2.0.3,,,,,,0,,,,,"Reproduced on 2.0 HEAD

{noformat}
[default@unknown] use opscenter;
Authenticated to keyspace: OpsCenter
[default@OpsCenter] create column family test with column_metadata = [{column_name: '1111', validation_class: LongType}];
637fffa1-a10f-3d89-8be6-8a316af05dd2
[default@OpsCenter] update column family test with column_metadata=[];
e49e435b-ba2a-3a08-8af0-32b897b872b8
[default@OpsCenter] show schema;

<other entries removed>

create column family test
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and populate_io_cache_on_flush = false
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and default_time_to_live = 0
  and speculative_retry = 'NONE'
  and column_metadata = [
    {column_name : '1111',
    validation_class : LongType}]
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.LZ4Compressor'}
  and index_interval = 128;
{noformat}
",,mbulman,mishail,nickmbailey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/13 14:23;slebresne;6182.txt;https://issues.apache.org/jira/secure/attachment/12609434/6182.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353086,,,Fri Oct 25 17:11:54 UTC 2013,,,,,,,,,,"0|i1ovi7:",353373,2.0.1,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"21/Oct/13 14:23;slebresne;Attaching patch. This is a regression from CASSANDRA-5579 for column definitions where the comparator is not UTF8Type (as is the case in this example).;;;","25/Oct/13 00:42;jbellis;+1;;;","25/Oct/13 17:11;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replaying a commit led to java.lang.StackOverflowError and node crash,CASSANDRA-6181,12673448,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,jdamick,jdamick,11/Oct/13 15:01,16/Apr/19 09:32,14/Jul/23 05:53,23/Oct/13 14:18,1.2.12,2.0.2,,,,,1,,,,,"2 of our nodes died after attempting to replay a commit.  I can attach the commit log file if that helps.
It was occurring on 1.2.8, after several failed attempts to start, we attempted startup with 1.2.10.  This also yielded the same issue (below).  The only resolution was to physically move the commit log file out of the way and then the nodes were able to start...  

The replication factor was 3 so I'm hoping there was no data loss...

{code}
 INFO [main] 2013-10-11 14:50:35,891 CommitLogReplayer.java (line 119) Replaying /ebs/cassandra/commitlog/CommitLog-2-1377542389560.log
ERROR [MutationStage:18] 2013-10-11 14:50:37,387 CassandraDaemon.java (line 191) Exception in thread Thread[MutationStage:18,5,main]
java.lang.StackOverflowError
        at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(TimeUUIDType.java:68)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:57)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:29)
        at org.apache.cassandra.db.marshal.AbstractType.compareCollectionMembers(AbstractType.java:229)
        at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:81)
        at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:439)
        at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
        at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
        at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
        at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
        at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
        at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)

.... etc.... over and over until ....

        at org.apache.cassandra.db.RangeTombstoneList.weakInsertFrom(RangeTombstoneList.java:472)
        at org.apache.cassandra.db.RangeTombstoneList.insertAfter(RangeTombstoneList.java:456)
        at org.apache.cassandra.db.RangeTombstoneList.insertFrom(RangeTombstoneList.java:405)
        at org.apache.cassandra.db.RangeTombstoneList.add(RangeTombstoneList.java:144)
        at org.apache.cassandra.db.RangeTombstoneList.addAll(RangeTombstoneList.java:186)
        at org.apache.cassandra.db.DeletionInfo.add(DeletionInfo.java:180)
        at org.apache.cassandra.db.AtomicSortedColumns.addAllWithSizeDelta(AtomicSortedColumns.java:197)
        at org.apache.cassandra.db.AbstractColumnContainer.addAllWithSizeDelta(AbstractColumnContainer.java:99)
        at org.apache.cassandra.db.Memtable.resolve(Memtable.java:207)
        at org.apache.cassandra.db.Memtable.put(Memtable.java:170)
        at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:745)
        at org.apache.cassandra.db.Table.apply(Table.java:388)
        at org.apache.cassandra.db.Table.apply(Table.java:353)
        at org.apache.cassandra.db.commitlog.CommitLogReplayer$1.runMayThrow(CommitLogReplayer.java:258)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{code}

",1.2.8 & 1.2.10 - ubuntu 12.04,exabytes18,frousseau,jdamick,johnny15676,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6223,,,,,,,,,,,,,,,,,,,,"22/Oct/13 09:44;slebresne;6181.txt;https://issues.apache.org/jira/secure/attachment/12609619/6181.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353071,,,Tue Nov 19 11:31:55 UTC 2013,,,,,,,,,,"0|i1ovev:",353358,,,,,,,,,frousseau,,frousseau,Critical,,1.2.8,,,,,,,,,,,,,,,,"11/Oct/13 15:46;jbellis;Pattern matching {{RangeTombstoneList}} to Sylvain.;;;","11/Oct/13 17:03;slebresne;[~jdamick] Actually, the commit log would be useful if you have it, though ideally I'd need the schema too. Feel free to send that to me in private if you prefer.;;;","11/Oct/13 18:25;jdamick;i sent you a link in email @ datastax.;;;","22/Oct/13 09:44;slebresne;I unfortunately haven't been to reproduce with the commit log from Jeffrey.

That being said, looking at the stacktrace more closely, I don't think that this is an infinite loop. Rather, in some insertion cases, we have to iterate over all (or a large part) of the range tombstones and that is currently done recursively so this can blow up the stack. The blow-up does reproduce rather easily in a unit test (with 3K range tombstone, which is not small, but not all that much). I though we would be unlikely to run into that case with the way range tombstones are used in practice, but I suppose that's still possible if you have multiple clustering columns so maybe that's just that.

Anyway, I don't really another fix than to rewrite the logic non-recursively.  Attaching a patch for this. This is probably a little bit more involved that what I'd like to push in 1.2 at this point, but at same I don't think there is any simpler way to fix this. On the bright side, RangeTombstoneList is relatively well covered by unit tests.

[~exabytes18], [~jdamick]: If you guys could check that the attached patch does fix this for you, that would be awesome.
;;;","22/Oct/13 12:41;frousseau;Don't know if this can help but, unit tests do not set ""-Xss"" parameter and uses the default value (1M in general, see http://www.oracle.com/technetwork/java/javase/tech/vmoptions-jsp-140102.html ) while default Xss is 180k for  Cassandra1.2

Here are some raw numbers using unit tests and setting Xss in the build.xml :
N = 193; // 180k
N = 313; // 228k
N = 394; // 256k
N = 1036; // 512k
N = 2321; // 1024k

This number represents the number of inserted tombstones before the tests starts failing (ie : incrementing this number by one : tests fails with SOException)
This should probably explain why it was not reproducible.
By the way, maybe the unit tests should set the Xss parameter in order to be as close as possible of a running cassandra instance ?

;;;","23/Oct/13 08:23;slebresne;Right, it's really not that hard to get into with the default stack trace, which probably explain why we already have 2 people reporting this.

bq. maybe the unit tests should set the Xss parameter in order to be as close as possible of a running cassandra instance ?

Not a bad idea. I've ninja-committed that, though I fixed it to 256k because that's the default in 2.0 and for 1.2, 180k is known to cause problem on some platforms and didn't wanted with that.;;;","23/Oct/13 13:51;frousseau;After reviewing patch, here are some changes in RangeTombstone.java:

L143: '(pos >= 0 ? pos : -pos-1)' should probably be '(pos >= 0 ? pos+1 : -pos-1)'
if pos is positive, it means that start == ends[pos], and the interval should be inserted at (pos+1)


Below is a simple test to reproduce this behaviour:

{noformat}
    @Test
    public void overlappingPreviousEndEqualsStartTest1()
    {
        RangeTombstoneList l = new RangeTombstoneList(cmp, 0);
        // add a RangeTombstone, so, last insert is not in insertion order
        l.add(rt(11, 12, 2));
        l.add(rt(1, 4, 2));
        l.add(rt(4, 10, 5));

        assertEquals(2, l.search(b(3)).markedForDeleteAt);
        assertEquals(5, l.search(b(4)).markedForDeleteAt);
        assertEquals(5, l.search(b(8)).markedForDeleteAt);
        assertEquals(3, l.size());
    }
{noformat}

Some very minor changes in comments:
L200: should replace 'insertFrom' to 'addInternal'
L394: should the commented line be : setInternal(i, start, ends[i], markedAt, delTime) ? (it seems more in the spirit)
L422: TODO can be removed, because implemented a few lines above

Otherwise, LGTM
;;;","23/Oct/13 14:17;slebresne;Correct about L143, thanks. Updated and committed, thanks.;;;","16/Nov/13 23:41;exabytes18;[~slebresne] Can you give a concrete cql schema + access pattern which causes this to occur? What would the effective usage limits be before and after this patch (i.e. how much improvement does this patch provide)?

Our application does a certain number of deletions which are unavoidable. It seems under our current design, cassandra cannot accommodate our workload and we're trying to understand what to do differently.;;;","18/Nov/13 13:45;slebresne;bq. What would the effective usage limits be before and after this patch

Before, it would crash with the exception in the description above, after it won't. This is really ""just"" a bug fix, if you don't run into it, there is nothing this patch will do for you. If you do, then it will fix the problem.;;;","18/Nov/13 23:00;exabytes18;Sorry, I mean what sort of DELETE statements cause this to happen? From reading these comments, it seems that there's some threshold at which too many deletions cause this to occur? Does the patch raise or eliminate this limit?;;;","19/Nov/13 09:32;slebresne;This is related to range tombstones. We mainly use range tombstone in two cases:
# you do a 'DELETE FROM X WHERE Y' (so the whole CQL3 row) *and* Y contains at least a clustering column (if it contains only the partition key for instance, we don't use a range tombstone).
# you set a collection (but not when you append/add/remove to it).

And this may be triggered if you reach some amount of such range tombstones within one partition. But 1) this is not guaranteed at all, as this depends on the ranges covered by those tombstones and the insertion order of those tombstones (and the exact conditions can't be easily characterize, sorry) and 2) the exact number of tombstones after which this may be triggered depends on the stack size (see Fabien's comment above).

In any case, the patch eliminate this entirely.;;;","19/Nov/13 11:31;exabytes18;Fantastic. Thanks for the clarification!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in CqlRecordWriter: Related to AbstractCassandraStorage handling null values,CASSANDRA-6180,12673418,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,hkropp,hkropp,11/Oct/13 11:56,16/Apr/19 09:32,14/Jul/23 05:53,25/Oct/13 16:13,1.2.12,2.0.3,,,,,0,,,,,"I encountered an issue with the {{CqlStorage}} and it's handling of null values. The {{CqlRecordWriter}} throws an NPE when a value is null. I found a related ticket CASSANDRA-5885 and applied the there stated fix to the {{AbstractCassandraStorage}}.
Instead of converting {{null}} values to {{ByteBuffer.wrap(new byte[0])}} {{AbstractCassandraStorage}} returns {{(ByteBuffer)null}}

This issue can be reproduced with the attached files: {{test_null.cql}}, {{test_null_data}}, {{null_test.pig}}

A fix can be found in the attached patch.

{code}
java.io.IOException: java.lang.NullPointerException
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:248)
Caused by: java.lang.NullPointerException
	at org.apache.thrift.protocol.TBinaryProtocol.writeBinary(TBinaryProtocol.java:194)
	at org.apache.cassandra.thrift.Cassandra$execute_prepared_cql3_query_args.write(Cassandra.java:41253)
	at org.apache.thrift.TServiceClient.sendBase(TServiceClient.java:63)
	at org.apache.cassandra.thrift.Cassandra$Client.send_execute_prepared_cql3_query(Cassandra.java:1683)
	at org.apache.cassandra.thrift.Cassandra$Client.execute_prepared_cql3_query(Cassandra.java:1673)
	at org.apache.cassandra.hadoop.cql3.CqlRecordWriter$RangeClient.run(CqlRecordWriter.java:232)
{code}","Pig, CqlStorage",alexliu68,hkropp,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Oct/13 08:24;alexliu68;6180-v2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12609824/6180-v2-1.2-branch.txt","11/Oct/13 11:57;hkropp;null_test.pig;https://issues.apache.org/jira/secure/attachment/12607982/null_test.pig","11/Oct/13 11:57;hkropp;patch.txt;https://issues.apache.org/jira/secure/attachment/12607985/patch.txt","11/Oct/13 11:57;hkropp;test_null.cql;https://issues.apache.org/jira/secure/attachment/12607983/test_null.cql","11/Oct/13 11:57;hkropp;test_null_data;https://issues.apache.org/jira/secure/attachment/12607984/test_null_data",,,,,,,,,,,,,,,,5.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,353041,,,Fri Oct 25 16:13:34 UTC 2013,,,,,,,,,,"0|i1ov87:",353328,1.2.10,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"11/Oct/13 11:57;hkropp;Files to reproduce this issue and a patch.;;;","13/Oct/13 16:57;jbellis;I don't think this is semantically correct.  ""UPDATE foo SET x = null"" means ""set x to a tombstone,"" which is not the same as setting it to an empty byte[].

/cc [~alexliu68];;;","14/Oct/13 05:36;alexliu68;Thrift API TBinaryProtocol.writeBinary method 

{code}
    public void writeBinary(ByteBuffer buffer) throws TException
    {
        writeI32(buffer.remaining());

        if (buffer.hasArray())
        {
            trans_.write(buffer.array(), buffer.position() + buffer.arrayOffset(), buffer.remaining());
        }
        else
        {
            byte[] bytes = new byte[buffer.remaining()];

            int j = 0;
            for (int i = buffer.position(); i < buffer.limit(); i++)
            {
                bytes[j++] = buffer.get(i);
            }

            trans_.write(bytes);
        }
    }
{code}

throws NPE if the variable is null instead of a BufferBuffer with empty byte. It looks like a bug for thrift execute_prepared_cql3_query which can't handle a null variable.

CASSANDRA-5081 may only work for binary protocol.;;;","14/Oct/13 05:55;alexliu68;If we do need use  ByteBuffer.wrap(new byte[0]) to fix the issue, this should only apply to CqlStorage, CassandraStorage should stay the old way.;;;","14/Oct/13 09:04;hkropp;I agree that {{byte[]}} is not semantically equivalent to {{null}}. The representation of {{null}} in Cassandra seems to me like an issue on it's own.

How about limiting it to {{CqlStorage}} and making it configurable? So the user can deliberately change the representation of {{null}}/empty values.

At least I can query easily for {{byte[]}} but not for {{null}}.
{code}
> CREATE INDEX null_idx ON null_table(null_value);
> DELETE null_value FROM null_table WHERE null_id = 'a';
> SELECT COUNT(*) FROM null_table WHERE null_value = '';
 count
-------
     6 
> SELECT * FROM null_table WHERE null_value = null;
Bad Request: Unsupported null value for indexed column null_value
{code};;;","23/Oct/13 08:25;alexliu68;6180-v2-1.2-branch.txt is attached to set it to empty byte array for CqlStorage, keep it null for CassandraStorage;;;","25/Oct/13 16:13;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
counter increment hangs,CASSANDRA-6175,12673100,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,09/Oct/13 19:57,16/Apr/19 09:32,14/Jul/23 05:53,09/Oct/13 21:55,2.0.2,,,,,,0,,,,,"In a three node cluster, a simple counter increment at quorum hangs the query indefinitely.  git blames our familiar friend CASSANDRA-6132 once again.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/13 21:10;jbellis;6175.txt;https://issues.apache.org/jira/secure/attachment/12607654/6175.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352723,,,Wed Oct 09 21:55:59 UTC 2013,,,,,,,,,,"0|i1ot9z:",353010,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"09/Oct/13 20:01;brandon.williams;Possible trace:

{noformat}
ERROR [ReplicateOnWriteStage:1] 2013-10-09 19:50:34,516 CassandraDaemon.java (line 191) Exception in thread Thread[ReplicateOnWriteStage:1,5,main]
java.lang.AssertionError
    at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:524)
    at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:572)
    at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:553)
    at org.apache.cassandra.service.StorageProxy.sendMessagesToOneDCInternal(StorageProxy.java:642)
    at org.apache.cassandra.service.StorageProxy.sendMessagesToOneDC(StorageProxy.java:624)
    at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:551)
    at org.apache.cassandra.service.StorageProxy$7$1.runMayThrow(StorageProxy.java:804)
    at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1631)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
{noformat}

I say possible because it didn't show up until I killed the process with prejudice.;;;","09/Oct/13 20:25;jbellis;Something's broke; sendMessagesToOneDC is only present in 1.2;;;","09/Oct/13 20:28;brandon.williams;That was somewhere in the bisect, so I don't know where it was.  It will totally hang on 2.0 though :);;;","09/Oct/13 21:05;jbellis;The actual problem:

{noformat}
java.lang.AssertionError
	at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:551)
	at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:606)
	at org.apache.cassandra.service.StorageProxy.mutateCounter(StorageProxy.java:1039)
	at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:502)
	at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:577)
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:379)
	at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:363)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:126)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:142)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:133)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1936)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4394)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4378)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{noformat};;;","09/Oct/13 21:10;jbellis;Assertion fix attached.;;;","09/Oct/13 21:15;brandon.williams;+1;;;","09/Oct/13 21:55;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
COPY TO command doesn't escape single quote in collections,CASSANDRA-6172,12673002,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,IvanMykhailov,IvanMykhailov,09/Oct/13 10:51,16/Apr/19 09:32,14/Jul/23 05:53,24/Nov/13 18:58,1.2.13,2.0.4,,Legacy/Tools,,,0,,,,,"{code}
CREATE TABLE test (key text PRIMARY KEY , testcollection set<text>) ;
INSERT INTO test (key, testcollection ) VALUES ( 'test', {'foo''bar'});
COPY test TO '/tmp/test.csv';
COPY test FROM '/tmp/test.csv';

Bad Request: line 1:73 mismatched character '<EOF>' expecting '''
Aborting import at record #0 (line 1). Previously-inserted values still present.
{code}

Content of generated '/tmp/test.csv':
{code}
test,{'foo'bar'}
{code}

Unfortunately, I didn't find workaround with any combination of COPY options ","Cassandra 2.0.1, Linux",aleksey,IvanMykhailov,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Nov/13 21:51;mishail;CASSANDRA-2.0-6172.patch;https://issues.apache.org/jira/secure/attachment/12615393/CASSANDRA-2.0-6172.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352625,,,Sun Nov 24 18:58:14 UTC 2013,,,,,,,,,,"0|i1osof:",352912,2.0.2,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"22/Nov/13 21:51;mishail;Patch: keep single quotes in a text value;;;","24/Nov/13 18:58;aleksey;Committed a *slightly* altered version (that only quotes the 's if we are formatting a collection). Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Too many splits causes a ""OutOfMemoryError: unable to create new native thread"" in AbstractColumnFamilyInputFormat",CASSANDRA-6169,12672932,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,patricioe,patricioe,patricioe,09/Oct/13 00:28,16/Apr/19 09:32,14/Jul/23 05:53,09/Oct/13 18:36,1.2.11,2.0.2,,,,,0,hadoop,,,,"The problem is caused by having 2300+ tokens due to vnodes.

In the client side I get this exception

{code}
Exception in thread ""main"" java.lang.OutOfMemoryError: unable to create new native thread
	at java.lang.Thread.start0(Native Method)
	at java.lang.Thread.start(Thread.java:691)
	at java.util.concurrent.ThreadPoolExecutor.addWorker(ThreadPoolExecutor.java:943)
	at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:1336)
	at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:132)
	at org.apache.cassandra.hadoop.AbstractColumnFamilyInputFormat.getSplits(AbstractColumnFamilyInputFormat.java:187)
	at org.apache.hadoop.mapred.JobClient.writeNewSplits(JobClient.java:1054)
	at org.apache.hadoop.mapred.JobClient.writeSplits(JobClient.java:1071)
	at org.apache.hadoop.mapred.JobClient.access$700(JobClient.java:179)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:983)
	at org.apache.hadoop.mapred.JobClient$2.run(JobClient.java:936)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1190)
	at org.apache.hadoop.mapred.JobClient.submitJobInternal(JobClient.java:936)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:550)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:580)
	at com.relateiq.hadoop.cassandra.etl.CassandraETLJob.run(CassandraETLJob.java:58)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at com.relateiq.hadoop.cassandra.etl.CassandraETLJob.main(CassandraETLJob.java:149)
{code}

The problem seem to be in AbstractColumnFamilyInputFormat line ~180 which has an unbounded upper limit (actually it is Integer.MAX_INT)
{code}
ExecutorService executor = Executors.newCachedThreadPool();
{code}

Followed by:
{code}
            for (TokenRange range : masterRangeNodes)
            {
                if (jobRange == null)
                {
                    // for each range, pick a live owner and ask it to compute bite-sized splits
                    splitfutures.add(executor.submit(new SplitCallable(range, conf)));
                }
                else
                .....
{code}

which gets called one time per token and creates one thread just as many times.

The easy fix unless there is a longer term fix I'm unaware of would be to set an upper limit to the thread pool.

Something like this:
{code}
ExecutorService executor = new ThreadPoolExecutor(0, ConfigHelper.getMaxConcurrentSplitsResolution(), 60L, TimeUnit.SECONDS, new LinkedBlockingQueue<Runnable>());
{code}


Shall I proceed with a patch ?","1.2.10
vnodes (server side)
Mac OS x (client)",patricioe,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/13 17:33;patricioe;CASSANDRA-6169.diff;https://issues.apache.org/jira/secure/attachment/12607604/CASSANDRA-6169.diff",,,,,,,,,,,,,,,,,,,,1.0,patricioe,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352555,,,Wed Oct 09 18:36:32 UTC 2013,,,,,,,,,,"0|i1os8v:",352842,,,,,,,,,jbellis,,jbellis,Low,,1.2.10,,,,,,,,,,,,,,,,"09/Oct/13 01:08;jbellis;Makes sense to me, although I'm not sure it needs to be configurable.  Picking something reasonable like 128 should be fine.;;;","09/Oct/13 18:36;jbellis;LGTM; committed!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool status should issue a warning when no keyspace is specified,CASSANDRA-6168,12672921,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,patricioe,patricioe,08/Oct/13 23:03,16/Apr/19 09:32,14/Jul/23 05:53,22/Mar/14 20:59,1.2.16,2.0.7,2.1 beta2,Tool/nodetool,,,1,lhf,,,,"Seen in 1.2.10.

Apologies if this is expected behavior. Nodetool status reports 0% ownership unless I add a keyspace name.

nodetool help docs says:
..."" status                 - Print cluster information (state, load, IDs, ...)""...

output without keyspace name
{code}
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens  Owns   Host ID                               Rack
UN  10.x.x.146  81.96 GB   256     0.0%   a70c59b3-a667-4d76-ba5b-ba849ad672da  r1
UN  10.x.x.63   95.32 GB   256     0.0%   f8cb7b10-4ebe-484a-a1c0-6cb2d053901b  r1
UN  10.x.x.184  89.54 GB   256     0.1%   cd86c420-55e2-4d99-8ed9-d9ee8d6a9d9c  r1
UN  10.x.x.190  79.68 GB   256     0.0%   544c3906-bc02-400d-9fd2-1e39ecadd6ff  r1
UN  10.x.x.168  93.44 GB   256     0.7%   33be316f-1276-475d-90cf-2667950d3a2c  r1
UN  10.x.x.132  84.4 GB    256     0.0%   b327d9f1-cab0-4583-8e5e-95c50b4074fd  r1
Datacenter: DCOFFLINE
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens  Owns   Host ID                               Rack
UN  10.x.x.62   56.09 GB   256     32.4%  c8994d27-767b-431f-bdc2-9196eeeb6f44  r1
UN  10.x.x.131  60.11 GB   256     32.8%  0b9d3314-039e-4f88-8ba6-d0f2885d9a30  r1
UN  10.x.x.167  56.45 GB   256     34.0%  ba76f4fe-4250-4839-a37d-c1a7c24e585d  r1
{code}

and with keyspace. Example: nodetool status MYKSPS

{code}
Datacenter: DC1
===============
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens  Owns (effective)  Host ID                               Rack
UN  10.x.x.184  89.51 GB   256     50.0%             cd86c420-55e2-4d99-8ed9-d9ee8d6a9d9c  r1
UN  10.x.x.146  81.96 GB   256     50.0%             a70c59b3-a667-4d76-ba5b-ba849ad672da  r1
UN  10.x.x.168  93.44 GB   256     50.0%             33be316f-1276-475d-90cf-2667950d3a2c  r1
UN  10.x.x.63   95.32 GB   256     50.0%             f8cb7b10-4ebe-484a-a1c0-6cb2d053901b  r1
UN  10.x.x.190  79.68 GB   256     50.0%             544c3906-bc02-400d-9fd2-1e39ecadd6ff  r1
UN  10.x.x.132  84.4 GB    256     50.0%             b327d9f1-cab0-4583-8e5e-95c50b4074fd  r1
Datacenter: DCOFFLINE
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address      Load       Tokens  Owns (effective)  Host ID                               Rack
UN  10.x.x.131  60.11 GB   256     32.8%             0b9d3314-039e-4f88-8ba6-d0f2885d9a30  r1
UN  10.x.x.167  56.45 GB   256     34.7%             ba76f4fe-4250-4839-a37d-c1a7c24e585d  r1
UN  10.x.x.62   56.09 GB   256     32.5%             c8994d27-767b-431f-bdc2-9196eeeb6f44  r1
{code}
",,jjordan,mishail,patricioe,vijay2win@yahoo.com,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/14 04:26;vijay2win@yahoo.com;0001-CASSANDRA-6168.patch;https://issues.apache.org/jira/secure/attachment/12636175/0001-CASSANDRA-6168.patch",,,,,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352544,,,Sat Mar 22 20:59:59 UTC 2014,,,,,,,,,,"0|i1os6f:",352831,,,,,,,,,brandon.williams,,brandon.williams,Low,,1.2.10,,,,,,,,,,,,,,,,"21/Oct/13 04:09;jbellis;Since we need KS to pick the right replication strategy, I vote we just leave Owns out for no-KS situation.  WDYT [~brandon.williams]?;;;","21/Oct/13 04:11;brandon.williams;Well, we do issue a warning when no keyspace is specified.;;;","20/Mar/14 05:07;jjordan;we actually don't for status, we need to make it act like ring;;;","20/Mar/14 05:14;brandon.williams;Care to take a stab, Vijay?;;;","20/Mar/14 15:36;vijay2win@yahoo.com;Hi Brandon, Sure, Thanks!;;;","22/Mar/14 04:26;vijay2win@yahoo.com;One line change.;;;","22/Mar/14 12:51;brandon.williams;+1;;;","22/Mar/14 20:59;vijay2win@yahoo.com;Committed Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
getOperationMode can thow NullPointerException,CASSANDRA-6166,12672898,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,Connor Warrington,Connor Warrington,Connor Warrington,08/Oct/13 20:47,16/Apr/19 09:32,14/Jul/23 05:53,09/Oct/13 00:42,2.0.2,,,,,,0,,,,,While the client or server is being initialized calling StorageService.getOperationMode() can throw a NullPointerException,,Connor Warrington,dbrosius,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/13 20:49;Connor Warrington;trunk-6166.txt;https://issues.apache.org/jira/secure/attachment/12607433/trunk-6166.txt",,,,,,,,,,,,,,,,,,,,1.0,Connor Warrington,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352521,,,Wed Oct 09 00:42:09 UTC 2013,,,,,,,,,,"0|i1os1b:",352808,,,,,,,,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,,"08/Oct/13 20:50;Connor Warrington;Add a value to Mode.
Set the operationMode to that Mode so we don't throw a NullPointerException;;;","09/Oct/13 00:42;dbrosius;+1, thanks!

committed to cassandra-2.0 as commit c198b76c46e4beae45e2a98910322a8761b73684;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE in hinted handoff delivery,CASSANDRA-6165,12672895,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,08/Oct/13 20:28,16/Apr/19 09:32,14/Jul/23 05:53,08/Oct/13 22:29,2.0.2,,,,,,0,,,,,"I suspect our old friend CASSANDRA-6132 is related to this as well:

{noformat}
ERROR [HintedHandoff:2] 2013-10-08 20:14:39,976 Caller+0     at org.apache.cassandra.service.CassandraDaemon$2.uncaughtException(CassandraDaemon.java:134)
 - Exception in thread Thread[HintedHandoff:2,1,main]
java.lang.AssertionError: null
    at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:543) ~[main/:na]
    at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:594) ~[main/:na]
    at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:573) ~[main/:na]
    at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:568) ~[main/:na]
    at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:433) ~[main/:na]
    at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:304) ~[main/:na]
    at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:92) ~[main/:na]
    at org.apache.cassandra.db.HintedHandOffManager$4.run(HintedHandOffManager.java:525) ~[main/:na]
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_17]
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_17]
    at java.lang.Thread.run(Thread.java:722) ~[na:1.7.0_17]
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/13 21:27;jbellis;6165.txt;https://issues.apache.org/jira/secure/attachment/12607440/6165.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352518,,,Tue Oct 08 22:30:23 UTC 2013,,,,,,,,,,"0|i1os0n:",352805,,,,,,,,,brandon.williams,,brandon.williams,Normal,,2.1 rc3,,,,,,,,,,,,,,,,"08/Oct/13 21:27;jbellis;Patch (against 2.0);;;","08/Oct/13 22:14;brandon.williams;+1;;;","08/Oct/13 22:30;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Switch CFS histograms to biased sampling,CASSANDRA-6164,12672868,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,08/Oct/13 19:16,16/Apr/19 09:32,14/Jul/23 05:53,09/Oct/13 13:29,1.2.11,2.0.2,,Legacy/Tools,,,0,jmx,,,,,,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/13 19:16;jbellis;6164.txt;https://issues.apache.org/jira/secure/attachment/12607414/6164.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352491,,,Tue Oct 08 20:42:23 UTC 2013,,,,,,,,,,"0|i1orun:",352778,,,,,,,,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,,"08/Oct/13 19:16;jbellis;Also cleans up the tombstone histograms from CASSANDRA-6057;;;","08/Oct/13 19:23;yukim;I don't think changing MBean API in minor release is good idea.;;;","08/Oct/13 19:27;jbellis;The method names being changed have not yet been released, so that should be fair game.;;;","08/Oct/13 20:26;yukim;yeah, that's right.
+1 from me.;;;","08/Oct/13 20:42;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
`service cassandra status` fails in Ubuntu 13.04 RUssian,CASSANDRA-6163,12672841,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,nsv,nsv,08/Oct/13 16:22,16/Apr/19 09:32,14/Jul/23 05:53,29/Jul/14 18:01,,,,,,,0,,,,,"> sudo service cassandra status

xss =  -ea -javaagent:/usr/share/cassandra/lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms4G -Xmx4G -Xmn800M -XX:+HeapDumpOnOutOfMemoryError -Xss256k
 * Cassandra is not running

> sudo netstat -anltp | grep 7199
tcp        0      0 0.0.0.0:7199            0.0.0.0:*               LISTEN      7589/java

Maybe it's linked to https://issues.apache.org/jira/browse/CASSANDRA-6162 - incomplete startup, some information not recorded (PID file ?), or again failure to parse localized output of system utilities.
",Ubuntu 13.04 RUssian (language seems important),nsv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6162,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352464,,,Tue Oct 08 16:45:08 UTC 2013,,,,,,,,,,"0|i1oron:",352751,2.0.1,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"08/Oct/13 16:45;brandon.williams;Probably CASSANDRA-6101 and CASSANDRA-6090;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra does not start on Ubuntu 13.04 RUssian,CASSANDRA-6162,12672840,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,dbrosius,nsv,nsv,08/Oct/13 16:12,16/Apr/19 09:32,14/Jul/23 05:53,08/May/14 04:41,2.0.9,2.1 rc1,,Packaging,,,0,,,,,"Output just after install:

vm.max_map_count = 1048575
expr: синтаксическая ошибка
^^^^ RU: syntax error
expr: синтаксическая ошибка
^^^^ RU: syntax error
/etc/init.d/cassandra: 59: [: Illegal number: 
/etc/init.d/cassandra: 63: [: Illegal number: 
/etc/init.d/cassandra: 67: [: Illegal number: 
expr: синтаксическая ошибка
^^^^ RU: syntax error
/etc/init.d/cassandra: 81: [: Illegal number: 
xss =  -ea -javaagent:/usr/share/cassandra/lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -XmsM -XmxM -XmnM -XX:+HeapDumpOnOutOfMemoryError -Xss256k","Ubuntu 13.04, Russian (!) ,java version ""1.7.0_40""",andreas.petter,dbrosius,nsv,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6163,,,,,,,,,,,,,,,,,,,,"08/May/14 03:11;dbrosius;6162.txt;https://issues.apache.org/jira/secure/attachment/12643894/6162.txt","08/Oct/13 16:17;nsv;CASSANDRA-6162.patch;https://issues.apache.org/jira/secure/attachment/12607375/CASSANDRA-6162.patch",,,,,,,,,,,,,,,,,,,2.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352463,,,Thu May 08 04:41:20 UTC 2014,,,,,,,,,,"0|i1orof:",352750,2.0.1,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"08/Oct/13 16:17;nsv;Uncommenting lines

#MAX_HEAP_SIZE=""4G""
#HEAP_NEWSIZE=""800M""

in /etc/cassandra/cassandra-env.sh makes it run.

This solution seems to be not right, the actual problem looks like parsing of localized output of system utilities during environment discovery.;;;","23/Jan/14 15:42;andreas.petter;confirmed, this is the case for German version, too. Installed 2.0.4 directly from the deb-package from the Apache site.

The problem lies in line 22 of cassandra-env.sh, in the awk expression which expects the result of  free -m to contain ""Mem:"" which is not the case for localized ubuntu versions.
Maybe this can be fixed (at least it works for german localization, but i suspect it is generic enough to work for others, too) by searching for "":"" only and returning only the first line with grep, i.e.: 
system_memory_in_mb=`free -m | awk '/:/ {print $2}' | grep -i """" --max-count=1`

However, i'm not a shell scripter and so this must be tested elsewhere, too. Sergey, can you test it for the russian version, please?;;;","08/May/14 03:11;dbrosius;fix for linux... similar fix can be applied to other os's if folks can provide the output of the other commands for each os.

6162.txt against 2.0;;;","08/May/14 03:20;brandon.williams;Why the 'exit' in awk?;;;","08/May/14 03:36;dbrosius;so you only print the first line;;;","08/May/14 04:20;brandon.williams;Ah. +1;;;","08/May/14 04:41;dbrosius;committed to cassandra-2.0 as commit 16fd1a4a89958595ca2ae44fdac2eb7aa1ad6be2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Throw errror when attempting to create a secondary index against counter,CASSANDRA-6160,12672782,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,ahattrell,ahattrell,08/Oct/13 10:24,16/Apr/19 09:32,14/Jul/23 05:53,09/Oct/13 14:53,1.2.11,,,Feature/2i Index,Legacy/CQL,,0,,,,,"Using CQL you can create a secondary index against a counter which is then non-functional.  

{code}
cqlsh:test> create table test2 (col1 int, col2 counter, primary key (col1)) ;
cqlsh:test> create index dodgy on test2(col2) ;
cqlsh:test> update test2 set col2 = col2 + 0 where col1 = 1 ;
cqlsh:test> select * from test2 ;

 col1 | col2
------+------
    1 |    0

cqlsh:t7088> select * from test2 where col2 = 0 ;
{code}

We should return an error to let users know they are in unsupported territory.
",,ahattrell,nff,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/13 13:21;slebresne;6160.txt;https://issues.apache.org/jira/secure/attachment/12607554/6160.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352405,,,Wed Jan 28 20:02:28 UTC 2015,,,,,,,,,,"0|i1orbj:",352692,1.2.10,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"09/Oct/13 13:21;slebresne;Trivial patch atttached;;;","09/Oct/13 13:29;jbellis;+1;;;","09/Oct/13 14:53;slebresne;Committed, thanks;;;","28/Jan/15 19:15;nff;Unfortunately this breaks secondary indexes on primary key dimensions:

{code}
cqlsh:ks1> create table test3 (col1 int, col2 int, col3 counter, primary key((col1, col2)));
cqlsh:ks1> update test3 set col3 = col3 + 3 where col1 = 1 and col2 = 2;
cqlsh:ks1> select * from test3;

 col1 | col2 | col3
------+------+------
    1 |    2 |    3

(1 rows)

cqlsh:ks1> create index expected on test3(col2);
Bad Request: Secondary indexes are not supported on counter tables
{code}

With RMW counters, we should be able to add both the counter write and the secondary index update in the same commit log entry. I couldn't find a JIRA for this; is there one?;;;","28/Jan/15 20:02;brandon.williams;Actually, I don't think we should be allowing mixing counters in the same table, but go ahead and make a new ticket [~nff] and [~slebresne] can decide.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Documentation cites three but only two rpc_server_types provided,CASSANDRA-6159,12672718,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,rektide,rektide,rektide,07/Oct/13 23:02,16/Apr/19 09:32,14/Jul/23 05:53,09/Oct/13 14:05,2.0.2,,,Legacy/Documentation and Website,,,0,,,,,,,rektide,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/13 23:05;rektide;0001-Out-of-the-box-rpc_server_type-set-is-sync-hsha-thes.patch;https://issues.apache.org/jira/secure/attachment/12607262/0001-Out-of-the-box-rpc_server_type-set-is-sync-hsha-thes.patch",,,,,,,,,,,,,,,,,,,,1.0,rektide,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352341,,,Wed Oct 09 14:05:31 UTC 2013,,,,,,,,,,"0|i1oqxj:",352629,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"07/Oct/13 23:06;rektide;jira, how does one work you? typically submit patch includes a form to link or include a patch. what are you jira?;;;","07/Oct/13 23:08;rektide;Patch attached and pull request submitted.  https://github.com/apache/cassandra/pull/22 . JIRA's ""submit patch"" button didn't give me any apparent ways to attach or link to any specific diffs. I have no idea wtf, whatever.;;;","08/Oct/13 00:32;brandon.williams;More->Attach files will let you attach files.  ""Submit patch"" is purely a workflow state, it's confusing I know.;;;","09/Oct/13 14:05;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Inserts are blocked in 2.1,CASSANDRA-6154,12672654,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,enigmacurry,enigmacurry,07/Oct/13 17:41,16/Apr/19 09:32,14/Jul/23 05:53,08/Oct/13 22:30,,,,,,,0,,,,,"With cluster sizes >1 inserts are blocked indefinitely:

{code}
$ ccm create -v git:trunk test
Fetching Cassandra updates...
Current cluster is now: test
$ ccm populate -n 2
$ ccm start
$ ccm node1 cqlsh
Connected to test at 127.0.0.1:9160.
[cqlsh 4.0.1 | Cassandra 2.1-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> CREATE KEYSPACE timeline WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh> USE timeline;
cqlsh:timeline> CREATE TABLE user_events (userid text, event timestamp, value text, PRIMARY KEY (userid, event));
cqlsh:timeline> INSERT INTO user_events (userid, event , value ) VALUES ( 'ryan', '2013-10-07', 'attempt');
{code}

The last INSERT statement never returns..",,enigmacurry,lizou,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Oct/13 21:16;jbellis;6154-v2.txt;https://issues.apache.org/jira/secure/attachment/12607242/6154-v2.txt","07/Oct/13 19:42;jbellis;6154.txt;https://issues.apache.org/jira/secure/attachment/12607223/6154.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352277,,,Tue Oct 08 22:15:29 UTC 2013,,,,,,,,,,"0|i1oqjb:",352565,2.1 rc3,,,,,,,,brandon.williams,,brandon.williams,Critical,,,,,,,,,,,,,,,,,,"07/Oct/13 19:11;brandon.williams;Bisect points at CASSANDRA-6132, specfically the ninja commit in 5440a0a6767544d6ea1ba34f5d2a3e223f260fb5;;;","07/Oct/13 19:42;jbellis;Looks like merge to trunk from 2.0 was syntactically correct but semantically broken.  Attached.;;;","07/Oct/13 20:14;brandon.williams;Not quite.

{noformat}
ERROR [GossipStage:1] 2013-10-07 20:09:15,849 Caller+0   at org.apache.cassandra.service.CassandraDaemon$2.uncaughtException(CassandraDaemon.java:134)
 - Exception in thread Thread[GossipStage:1,5,main]
java.lang.AssertionError: null
        at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:552) ~[main/:na]
        at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:576) ~[main/:na]
        at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:571) ~[main/:na]
        at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:808) ~[main/:na]
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:849) ~[main/:na]
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:934) ~[main/:na]
        at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:49) ~[main/:na]
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56) ~[main/:na]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_17]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_17]
{noformat};;;","07/Oct/13 21:16;jbellis;v2;;;","07/Oct/13 21:37;brandon.williams;+1;;;","07/Oct/13 21:47;jbellis;committed;;;","08/Oct/13 18:42;enigmacurry;Thanks, working now.;;;","08/Oct/13 20:03;brandon.williams;Actually, there's something wrong in the 2.0 branch that points at this commit too, but it's harder to trigger.  Using a batch such as:

{noformat}
            BEGIN BATCH
            INSERT INTO users (id, firstname, lastname) VALUES (0, 'Jack', 'Sparrow')
            INSERT INTO users (id, firstname, lastname) VALUES (1, 'Will', 'Turner')
            APPLY BATCH
{noformat}

at ONE with 2 of 3 nodes down should timeout, but never returns.;;;","08/Oct/13 20:59;jbellis;Can you set trace probability to 100% and get a trace?;;;","08/Oct/13 21:27;jbellis;Suspect patch for 6165 will fix this too.;;;","08/Oct/13 21:46;lizou;[~jbellis] I just pulled the Cassandra-2.0 source code and did a quick testing. Though slightly better than yesterday's trunk load. But there are still lots of assertion errors for  {{MessagingService.addCallback()}} without the argument of consistency level for MessageOut<RowMutation>. E.g.

{noformat}
   private void doDeliverHintsToEndpoint(InetAddress endpoint)
    {
     ...
                MessageOut<RowMutation> message = rm.createMessage();
     ...
                MessagingService.instance().sendRR(message, endpoint, responseHandler);
                responseHandlers.add(responseHandler);
{noformat}

and
{noformat}
    public static void sendToHintedEndpoints(final RowMutation rm,
                                             Iterable<InetAddress> targets,
                                             AbstractWriteResponseHandler responseHandler,
                                             String localDataCenter,
                                             ConsistencyLevel consistency_level)
    throws OverloadedException
    {
    ...
                    // belongs on a different server
                    if (message == null)
                        message = rm.createMessage();
                    String dc = DatabaseDescriptor.getEndpointSnitch().getDatacenter(destination);
                    // direct writes to local DC or old Cassandra versions
                    // (1.1 knows how to forward old-style String message IDs; updated to int in 2.0)
                    if (localDataCenter.equals(dc) || MessagingService.instance().getVersion(destination) < MessagingService.VERSION_20)
                    {
                        MessagingService.instance().sendRR(message, destination, responseHandler);
                    }
...
{noformat}

In the above examples, the {{sendRR()}} will call {{addCallback()}} which asserts as the message is of  type RowMutation.
;;;","08/Oct/13 21:53;jbellis;CASSANDRA-6165;;;","08/Oct/13 22:03;lizou;Yes, this has been observed in my test this afternoon after one node was killed.

I think that we should systematically check for this issue, as some other places can also have this {{MessagingService.addCallback()}} assertion issue.;;;","08/Oct/13 22:15;brandon.williams;CASSANDRA-6165 did indeed fix the batchlog problem here.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stress stopped calculating latency stats,CASSANDRA-6153,12672644,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,mishail,enigmacurry,enigmacurry,07/Oct/13 16:31,16/Apr/19 09:32,14/Jul/23 05:53,08/Oct/13 18:39,2.1 beta1,,,Legacy/Tools,,,0,,,,,"In trunk, cassandra-stress has stopped calculating all latency information:

From trunk:
{code}
$ ccm node1 stress
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency,95th,99.9th,elapsed_time
89995,8999,8999,0.0,0.0,0.0,10
304267,21427,21427,0.0,0.0,0.0,20
514791,21052,21052,0.0,0.0,0.0,30
727471,21268,21268,0.0,0.0,0.0,40
926467,19899,19899,0.0,0.0,0.0,50
1000000,7353,7353,0.0,0.0,0.0,54


Averages from the middle 80% of values:
interval_op_rate          : 21249
interval_key_rate         : 21249
latency median            : 0.0
latency 95th percentile   : 0.0
latency 99.9th percentile : 0.0
Total operation time      : 00:00:54
END
{code}

From 2.0:
{code}
$ ccm node1 stress
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency,95th,99.9th,elapsed_time
66720,6672,6672,0.2,25.6,201.6,10
289577,22285,22285,0.2,3.4,201.1,20
489105,19952,19952,0.2,1.8,201.2,30
660916,17181,17181,0.2,1.6,87.9,40
847452,18653,18653,0.2,1.6,108.8,50
1000000,15254,15254,0.2,1.6,108.9,59


Averages from the middle 80% of values:
interval_op_rate          : 19517
interval_key_rate         : 19517
latency median            : 0.2
latency 95th percentile   : 2.1
latency 99.9th percentile : 149.8
Total operation time      : 00:00:59
END
{code}",,enigmacurry,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/13 18:01;mishail;trunk-6153.patch;https://issues.apache.org/jira/secure/attachment/12607397/trunk-6153.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352267,,,Tue Oct 08 18:41:12 UTC 2013,,,,,,,,,,"0|i1oqh3:",352555,2.1 rc3,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"08/Oct/13 06:48;mishail;I believe it's related to https://github.com/apache/cassandra/commit/5dabd1cc0c65b329ff518d7ad3f09e4c11494f18
Not sure why we need to convert from nanos there;;;","08/Oct/13 10:30;brandon.williams;I think you're right about the cause.  Ping [~dbrosius];;;","08/Oct/13 17:41;mishail;There was an attempt to migrate to Metrics 3.0.1 (https://issues.apache.org/jira/browse/CASSANDRA-5838) which was later reverted.

* Initial commit: https://github.com/apache/cassandra/commit/c27a161920a2227cd04f8338a75732920694b1db
* Then [~dbrosius] updated Stress to work with new metrics: https://github.com/apache/cassandra/commit/5dabd1cc0c65b329ff518d7ad3f09e4c11494f18
* And then Metrics were reverted back to 2.2.0 https://github.com/apache/cassandra/commit/3205e5dbbc8fb8f365b72137cf1c1ea50f15cab6 but changes for Stress weren't rolled back

;;;","08/Oct/13 18:02;mishail;Reverted 5dabd1cc0c65b329ff518d7ad3f09e4c11494f18 ;;;","08/Oct/13 18:39;brandon.williams;Committed, thanks!;;;","08/Oct/13 18:41;enigmacurry;+1. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Assertion error in 2.0.1 at db.ColumnSerializer.serialize(ColumnSerializer.java:56),CASSANDRA-6152,12672552,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,ThinkerFeeler,ThinkerFeeler,06/Oct/13 20:54,16/Apr/19 09:32,14/Jul/23 05:53,14/Oct/13 17:06,1.2.11,,,,,,0,,,,,"{noformat}
ERROR [COMMIT-LOG-WRITER] 2013-10-06 12:12:36,845 CassandraDaemon.java (line 185) Exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:56)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:77)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.serialize(RowMutation.java:268)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:229)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:352)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.lang.Thread.run(Thread.java:722)
{noformat}","CentOS release 6.2 (Final)
With default set up on single node.
I also saw this exception in 2.0.0 on a three node cluster.",mishail,slebresne,ThinkerFeeler,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/13 07:39;slebresne;6152.txt;https://issues.apache.org/jira/secure/attachment/12608250/6152.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352175,,,Mon Oct 14 17:06:11 UTC 2013,,,,,,,,,,"0|i1opwn:",352463,1.2.10,2.0.1,,,,,,,jbellis,,jbellis,Normal,,1.2.0,,,,,,,,,,,,,,,,"06/Oct/13 20:56;ThinkerFeeler;The exception seems to happen first during a delete.  Let me know if you need more info.;;;","06/Oct/13 22:12;jbellis;That says you have an empty cell name, which is Not Supposed To Be Allowed.  Definitely need to know how you're doing this.;;;","06/Oct/13 22:29;ThinkerFeeler;I was running a functional test suite, which populates some tables after deleting the old rows for the same keys. 

I ran it by a command like:
{noformat}
    repeat 10 ./run-test.sh 
{noformat}
So, it was deleting and writing rows in quick succession.  

If you want to see more detail than that, I'll see what I can provide.
;;;","06/Oct/13 23:53;jbellis;As you can guess, we do have deletes in the unit and system tests, so yeah, details like ""here's the test script"" would be good!;;;","09/Oct/13 18:59;ThinkerFeeler;The test just runs our test suite repeatedly. After a few runs it gets the following, with TRACE level. I'll document more later.
{noformat}
DEBUG [Native-Transport-Requests:11] 2013-10-09 11:40:49,459 Message.java (line 302) Received: PREPARE INSERT INTO as_reports.data_report_details(report_id,item_name,item_value) VALUES (7bc2a570-a42b-4632-b245-f0db9255ccc3,?,?);, v=1
TRACE [Native-Transport-Requests:11] 2013-10-09 11:40:49,460 QueryProcessor.java (line 208) Stored prepared statement ef4c655e042ffab2c1f0eef1e53a573e with 2 bind markers
DEBUG [Native-Transport-Requests:11] 2013-10-09 11:40:49,460 Tracing.java (line 157) request complete
DEBUG [Native-Transport-Requests:11] 2013-10-09 11:40:49,460 Message.java (line 309) Responding: RESULT PREPARED ef4c655e042ffab2c1f0eef1e53a573e [item_name(as_reports, data_report_details), org.apache.cassandra.db.marshal.UTF8Type][item_value(as_reports, data_report_details), org.apache.cassandra.db.marshal.UTF8Type] (resultMetadata=[0 columns]), v=1
DEBUG [Native-Transport-Requests:13] 2013-10-09 11:40:49,464 Message.java (line 302) Received: EXECUTE ef4c655e042ffab2c1f0eef1e53a573e with 2 values at consistency ONE, v=1
TRACE [Native-Transport-Requests:13] 2013-10-09 11:40:49,464 QueryProcessor.java (line 232) [1] 'java.nio.HeapByteBuffer[pos=0 lim=0 cap=0]'
TRACE [Native-Transport-Requests:13] 2013-10-09 11:40:49,464 QueryProcessor.java (line 232) [2] 'java.nio.HeapByteBuffer[pos=36 lim=41 cap=43]'
TRACE [Native-Transport-Requests:13] 2013-10-09 11:40:49,464 QueryProcessor.java (line 97) Process org.apache.cassandra.cql3.statements.UpdateStatement@321baa4a @CL.ONE
ERROR [COMMIT-LOG-WRITER] 2013-10-09 11:40:49,465 CassandraDaemon.java (line 185) Exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.ColumnSerializer.serialize(ColumnSerializer.java:56)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:77)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.serialize(RowMutation.java:268)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:229)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:352)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.lang.Thread.run(Thread.java:722)
DEBUG [Native-Transport-Requests:13] 2013-10-09 11:40:49,466 Tracing.java (line 157) request complete
DEBUG [Native-Transport-Requests:13] 2013-10-09 11:40:49,466 Message.java (line 309) Responding: EMPTY RESULT, v=1
{noformat};;;","11/Oct/13 17:19;jbellis;I assume your test suite involves dropping and recreating the same tables?;;;","11/Oct/13 17:56;ThinkerFeeler;No, the test suite does *not* drop and create new tables (i.e., it does not call ""DROP TABLE"" and ""CREATE TABLE"").  It deletes rows from tables and re-inserts. I'm working right now on submitting a focused example that reproduces the bug.;;;","11/Oct/13 20:36;ThinkerFeeler;I found a *simple* example of the bug.  

If I insert an empty string ("""") into the table it causes the AssertionError. If I insert a non-empty string there's no AssertionError!

{noformat}
create keyspace if not exists bug with replication = {'class':'SimpleStrategy', 'replication_factor':1};


create table if not exists bug.bug_table ( -- compact; column values are ordered by item_name
        report_id   uuid,
        item_name   text,
        item_value  text,
primary key (report_id, item_name)) with compact storage;
}
{noformat}

BugMain.java:
{noformat}
package bug;

import java.io.IOException;
import java.util.ArrayList;
import java.util.List;

public class BugMain {
    private static String CASSANDRA_HOST = System.getProperty(""cassandraServer"",""172.17.1.169""); //""donalds01lx.uscorp.audsci.com"";
    private static BugInterface dao = new BugImpl(CASSANDRA_HOST);

    public static void bug() throws IOException {
        List<BugItem> items = new ArrayList<BugItem>();
        items.add(new BugItem("""",1,2,3));   // if you change the empty string """" to a non-empty string, the AssertionError goes away!
        items.add(new BugItem(""twp"",2,2,3));
        items.add(new BugItem(""three"",3,2,3));
        items.add(new BugItem(""four"",4,2,3));
        dao.saveReport(items);
    }
    
    public static void main(String [] args) throws IOException { 
       try {
           for(int i=0;i<1000;i++) {
               System.out.println(""\ndas: iteration "" + i + ""\n"");
               bug();
           }
       } finally {
           dao.shutdown();
       }
    }
}
{noformat}

BugItem.java:
{noformat}
package bug;

public class BugItem {
    public String name;
    public long long1; 
    public long long2;
    public long long3; 
    public BugItem(String string, long i, long j, long k) {
        name=string;
        long1 = i;
        long2= j;
        long3 = k;
    }
    public String toString() {return ""Item with name = "" + name + "", long1 = "" + long1 + "", long2 = "" + long2 + "", long3 = "" + long3;}
}
{noformat}

BugInterface.java:
{noformat}
package bug;

import java.util.List;


public interface BugInterface {
        public static final String VALUE_DELIMITER = "":"";
        public static final String HIERARCHY_DELIMITER = "" > "";
	void saveReport(List<BugItem> item);

	void connect();
	void shutdown();
}
{noformat}

BugImpl.java:
{noformat}
package bug;

import java.text.NumberFormat;
import java.util.List;
import java.util.UUID;

import org.apache.log4j.Logger;

import com.datastax.driver.core.Cluster;
import com.datastax.driver.core.PreparedStatement;
import com.datastax.driver.core.Session;
import com.datastax.driver.core.querybuilder.Insert;
import com.datastax.driver.core.querybuilder.QueryBuilder;

public class BugImpl implements BugInterface {
	private static final String CASSANDRA_NODE_PROPERTY=""CASSANDRA_NODE"";
	private static final Logger L = Logger.getLogger(new Throwable()
			.getStackTrace()[0].getClassName());
	private static final String KEYSPACE_NAME = ""bug"";
	private static final String REPORT_DATA_TABLE_NAME = ""bug_table"";
	private static NumberFormat numberFormat = NumberFormat.getInstance();
	private Cluster m_cluster;
	private Session m_session;
	private int m_writeBatchSize = 64;
	private String m_cassandraNode = ""<your cassandra hostname here>"";
	
	static {
		numberFormat.setMaximumFractionDigits(1);
	}

	public BugImpl() {
		m_cassandraNode=System.getProperty(CASSANDRA_NODE_PROPERTY, m_cassandraNode); // Get from command line
	}
	public BugImpl(String cassandraNode) {
		m_cassandraNode=cassandraNode;
	}
	@Override
	public void shutdown() {
		if (m_session!=null) {m_session.shutdown();}
		if (m_cluster!=null) {m_cluster.shutdown();}
	}
	@Override
	public void connect() {
		 m_cluster = Cluster.builder().addContactPoint(m_cassandraNode).build();
	     m_session = m_cluster.connect();
	}
	// ---------------------------------------------------------------------------------
	@Override
	public void saveReport(List<BugItem> items) {
		final long time1 = System.currentTimeMillis();
		if (m_session==null) {
			connect();
		}
		UUID reportId = UUID.randomUUID(); 
		saveReportAux(items,reportId);
		final long time2 = System.currentTimeMillis();
		L.info(""saveReport: t="" + numberFormat.format((double)(time2-time1) * 0.001) + "" seconds"");
	}
	
    public void saveReportAux(List<BugItem> items, UUID reportId) {
		int index = 0;
		int reportSize = items.size();
		int maxBatchedIndex = (int) (reportSize / m_writeBatchSize) * m_writeBatchSize;

		// prepare the batch statement
		final StringBuilder sb = new StringBuilder();
		sb.append(""BEGIN UNLOGGED BATCH "");
		for (int i = 0; i < m_writeBatchSize; i++) {
			sb.append(""insert into "" + KEYSPACE_NAME + "".""
					+ REPORT_DATA_TABLE_NAME
					+ "" (report_id, item_name, item_value) values ("" + reportId
					+ "",?,?);"");
		}
		sb.append(""APPLY BATCH;"");
		PreparedStatement insertBatchPrep = m_session.prepare(sb.toString());

		// prepare the single-insert statement for ""the rest"" that does not fit
		// in a whole batch
		final Insert writeReportData = QueryBuilder
				.insertInto(KEYSPACE_NAME, REPORT_DATA_TABLE_NAME)
				.value(""report_id"", reportId)
				.value(""item_name"", QueryBuilder.bindMarker())
				.value(""item_value"", QueryBuilder.bindMarker());
		final PreparedStatement writeReportDataPrep = m_session
				.prepare(writeReportData);

		// write the rows
		Object[] args = new Object[2 * m_writeBatchSize];
		int batchIndex = 0;
		int argsIndex = 0;
		for (final BugItem item : items) {
			if (index < maxBatchedIndex) {
				args[argsIndex++] = item.name;
				args[argsIndex++] = item.long1 + VALUE_DELIMITER
						+ item.long2 + VALUE_DELIMITER + item.long3;
				batchIndex++;
				if (batchIndex == m_writeBatchSize) {
					argsIndex = 0;
					batchIndex = 0;
					m_session.execute(insertBatchPrep.bind(args));
				}
			} else {
				m_session.execute(writeReportDataPrep.bind(item.name,
						item.long1 + VALUE_DELIMITER + item.long2 + VALUE_DELIMITER + item.long3));
			}
			index++;
		}
    }
   
    public void setWriteBatchSize(int size) {m_writeBatchSize=size;}
    public void setCassandraNode(String node) {m_cassandraNode=node;}
}
{noformat};;;","11/Oct/13 20:44;jbellis;Thanks, we'll have a look.;;;","11/Oct/13 21:49;ThinkerFeeler;I have a hunch that when the column name is """" and the Memtable flushes to an SSTable is when this bug bites.   I notice it happens at about the same iteration of the *for* loop in BugMain.java.;;;","14/Oct/13 07:42;slebresne;For some reason we weren't properly validating the cell names on inserts. In this case, it's a compact table with just one clustering column, so we cannot allow that clustering column to be the empty value. Patch attached to fix validation. This is not specific to 2.0 so the patch is against 1.2 (I've pushed a dtest for it too).;;;","14/Oct/13 08:44;jbellis;Tactically, I think renaming column to cell in 1.2 is a bit out of place w/ the rest of the code, but the fix LGTM.;;;","14/Oct/13 17:06;slebresne;Committed (with renaming only for 2.0), thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM in Cassandra 2.0.1,CASSANDRA-6149,12672509,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,depend,depend,06/Oct/13 04:30,16/Apr/19 09:32,14/Jul/23 05:53,07/Oct/13 19:21,2.0.2,,,,,,0,,,,,"I have a program to stress test Cassandra. What it does is remove/insert rows with a small set of row keys as fast as possible. Two CFs are involved. When I test against C* 1.2.3 with default configurations, it ran for 24 hours and C* doesn't having any issue. However after I upgraded to C* 2.0.1, C* crashes on OOM within 1-2 minutes. I can consistently reproduce this.

I built C* from the source and found out the last good changeset is cfa097cdd5e28d7fe8204248e246a1fae226d2c0. As soon as I include the next changeset 1e0d9513b748fae4ec0737283da71c65e9272102, C* starts to crash. What's interesting is although it seems the change was reverted by fc1a7206fe15882fd64e7ba8eb68ba9dc320275f. C* built from fc1a7206fe15882fd64e7ba8eb68ba9dc320275f has the same problem - OOM within minutes.

I didn't test against the official 2.0.0. But the C* built from 03045ca22b11b0e5fc85c4fabd83ce6121b5709b seems OK. I assume that's what 2.0.0 is.

I use default configurations in all cases. I didn't tune anything.",Windows 7 64 bit. Java 64-bit 1.7.0_25. Cassandra 2.0.1,depend,rcoli,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Oct/13 15:52;jbellis;6149-debug.txt;https://issues.apache.org/jira/secure/attachment/12607082/6149-debug.txt","06/Oct/13 18:47;jbellis;6149.txt;https://issues.apache.org/jira/secure/attachment/12607096/6149.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,352136,,,Mon Oct 07 19:21:03 UTC 2013,,,,,,,,,,"0|i1opnz:",352424,2.0.1,,,,,,,,xedin,,xedin,Normal,,2.0.1,,,,,,,,,,,,,,,,"06/Oct/13 06:24;jbellis;It doesn't entirely shock me that 1e0d9513b748fae4ec0737283da71c65e9272102 could cause problems (CASSANDRA-5661).

What does the heap dump show is using all the memory?  http://www.eclipse.org/mat/;;;","06/Oct/13 12:48;depend;MAT shows ""One instance of ""com.google.common.cache.LocalCache"" loaded by ""sun.misc.Launcher$AppClassLoader @ 0xd03530b0"" occupies 942,959,256 (91.01%) bytes.""

Another thing I notice is this OOM accompanoes with a LOT of ""Unable to delete ... (it will be removed on server restart; we'll also retry after GC)"" errors. I understand it might be OK on Windows. But the amount of such errors I see with 1e0d9513b748fae4ec0737283da71c65e9272102 seem unusual. In fact I don't see a single such error   prior to 1e0d9513b748fae4ec0737283da71c65e9272102.;;;","06/Oct/13 15:52;jbellis;I think we have a bug in cache size estimation, but I'm not sure how that turns into more-memory-used-than-1-2 which simply caches all readers.

Attached is a patch to add some debug logging.  Can you test after enabling debug on org.apache.cassandra.service.FileCacheService?;;;","06/Oct/13 17:43;depend;I got a bunch of ""less than"" estimation. Here is the part right before OOM:

...
DEBUG [ReadStage:32] 2013-10-06 13:33:36,610 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:22] 2013-10-06 13:33:36,614 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464191488
DEBUG [ReadStage:22] 2013-10-06 13:33:36,614 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
 INFO [ScheduledTasks:1] 2013-10-06 13:33:36,913 GCInspector.java (line 116) GC for ConcurrentMarkSweep: 962 ms for 4 collections, 1036218016 used; max is 1037959168
DEBUG [ReadStage:24] 2013-10-06 13:33:36,916 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464257024
DEBUG [ReadStage:24] 2013-10-06 13:33:36,916 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\Keyword\Keyspace1-Keyword-jb-1-Data.db
DEBUG [ReadStage:25] 2013-10-06 13:33:36,918 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464322560
DEBUG [ReadStage:25] 2013-10-06 13:33:36,919 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:19] 2013-10-06 13:33:36,920 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464388096
DEBUG [ReadStage:19] 2013-10-06 13:33:36,920 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:10] 2013-10-06 13:33:36,939 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464453632
DEBUG [ReadStage:10] 2013-10-06 13:33:36,940 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:10] 2013-10-06 13:33:36,941 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464519168
DEBUG [ReadStage:10] 2013-10-06 13:33:36,941 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:30] 2013-10-06 13:33:37,171 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464584704
DEBUG [ReadStage:30] 2013-10-06 13:33:37,171 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:7] 2013-10-06 13:33:37,171 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464584704
DEBUG [ReadStage:8] 2013-10-06 13:33:37,171 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464584704
DEBUG [ReadStage:8] 2013-10-06 13:33:37,171 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:11] 2013-10-06 13:33:37,171 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464584704
DEBUG [ReadStage:11] 2013-10-06 13:33:37,171 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:7] 2013-10-06 13:33:37,171 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
DEBUG [ReadStage:17] 2013-10-06 13:33:39,618 FileCacheService.java (line 127) Estimated memory usage is 327680 compared to actual usage 464846848
DEBUG [ReadStage:17] 2013-10-06 13:33:39,848 FileCacheService.java (line 137) Estimated memory usage 327680 is less than threshold 258998272; caching reader for D:\var\lib\cassandra\data\Keyspace1\MetaData\Keyspace1-MetaData-jb-1-Data.db
ERROR [ReadStage:23] 2013-10-06 13:33:39,848 CassandraDaemon.java (line 185) Exception in thread Thread[ReadStage:23,5,main]
java.lang.OutOfMemoryError: Java heap space
...

It does look like an estimation error.;;;","06/Oct/13 17:58;jbellis;How many requests/hits are in FileCacheMetrics?  (Use JConsole to look at o.a.c.metrics.FCM);;;","06/Oct/13 18:47;jbellis;While the size bug is the proximate cause of the OOM, it looks like it's never actually re-using the cached entries because it's comparing non-absolute with absolute paths.  Attached patch fixes both.;;;","06/Oct/13 23:41;depend;fix is confirmed. Thank you.;;;","07/Oct/13 18:39;xedin;+1;;;","07/Oct/13 19:21;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove multithreaded compaction (and precompactedrow),CASSANDRA-6142,12672169,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,03/Oct/13 15:38,16/Apr/19 09:32,14/Jul/23 05:53,30/Oct/13 15:20,2.1 beta1,,,,,,0,,,,,"There is at best a very small sweet spot for multithreaded compaction (ParallelCompactionIterable).  For large rows, we stall the pipeline and fall back to a single LCR pass.  For small rows, the overhead of the coordination outweighs the benefits of parallelization (45s to compact 2x1M stress rows with multithreading enabled, vs 35 with it disabled).",,blynch,dikanggu,kmueller,marcuse,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4182,CASSANDRA-6118,CASSANDRA-7249,,,CASSANDRA-7428,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351795,,,Wed Oct 30 15:20:24 UTC 2013,,,,,,,,,,"0|i1onkv:",352083,,,,,,,,,marcuse,,marcuse,Low,,,,,,,,,,,,,,,,,,"03/Oct/13 15:41;jbellis;I tried parallelizing at the OnDiskAtomIterator level instead (thread-per-iterator-per-partition, buffering into a queue) and for small partitions the performance is ridiculously bad, easily 100x worse than single threaded mode.

Any better ideas [~krummas] [~yukim] [~iamaleksey] [~slebresne]?  If not I will post a patch to rip out PCI.;;;","03/Oct/13 16:32;marcuse;i tried improving it a while back as well, got basically the same results, yes, we should remove it

concluded that the best way to improve the speed was to do more compactions in parallel (CASSANDRA-5936 - i should finish that up..);;;","03/Oct/13 22:36;jbellis;Pushed removal to https://github.com/jbellis/cassandra/commits/6142.

Also removes PrecompactedRow, which is no longer necessary, and fixes a couple existing bugs in LCR and Scrub that this revealed (last two commits).;;;","13/Oct/13 17:40;jbellis;Belated clicked Submit Patch.;;;","16/Oct/13 07:50;marcuse;CompactionsPurgeTest fails:
{noformat}
    [junit] Testsuite: org.apache.cassandra.db.compaction.CompactionsPurgeTest
    [junit] Tests run: 6, Failures: 2, Errors: 0, Skipped: 0, Time elapsed: 9.365 sec
    [junit] 
    [junit] Testcase: testMinTimestampPurge(org.apache.cassandra.db.compaction.CompactionsPurgeTest):   FAILED
    [junit] expected:<2> but was:<1>
    [junit] junit.framework.AssertionFailedError: expected:<2> but was:<1>
    [junit]     at org.apache.cassandra.db.compaction.CompactionsPurgeTest.testMinTimestampPurge(CompactionsPurgeTest.java:185)
    [junit] 
    [junit] 
    [junit] Testcase: testCompactionPurgeTombstonedRow(org.apache.cassandra.db.compaction.CompactionsPurgeTest):        FAILED
    [junit] expected:<10> but was:<5>
    [junit] junit.framework.AssertionFailedError: expected:<10> but was:<5>
    [junit]     at org.apache.cassandra.db.compaction.CompactionsPurgeTest.testCompactionPurgeTombstonedRow(CompactionsPurgeTest.java:313)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.CompactionsPurgeTest FAILED
{noformat}
and a few nits:
in LazilyCompactedRow:
* make reducer and merger final
* remove comment about reducer being null on row 123
;;;","17/Oct/13 21:49;jbellis;Damn, not sure how I missed that.  Suspect another existing bug.  Will investigate.;;;","19/Oct/13 20:02;jbellis;Pushed fixes for these to the same branch.  (They are indeed existing bugs in LCR.);;;","22/Oct/13 06:58;marcuse;looks good to me

regarding saveOutOfOrderRows, i guess a solution would be to flush a new sstable from the TreeSet when its size exceeds some limit? Unsure how common this is though.;;;","22/Oct/13 16:41;jbellis;I'm guessing not super common because the existing code will just break if it hits that case.  (A LCR object will throw errors if you try to use it after advancing the underlying stream to another row.)

I guess the next step is probably for me to pull the fixes out for application to 2.0.;;;","25/Oct/13 00:40;jbellis;Posted 2.0 fixes to https://github.com/jbellis/cassandra/commits/6142-2.0.  Note that b959e8ff3bccd3437de70d33da91307ab9c12a19 is a different, less-invasive approach than the one taken for trunk.;;;","30/Oct/13 07:40;marcuse;ok, looks good to me

how about 1.2?;;;","30/Oct/13 15:10;jbellis;The 2.0 backport was bad enough, I don't even want to think about 1.2.  They're all pretty rare corner cases, so I'm fine with telling people to upgrade to 2.0 if they care.;;;","30/Oct/13 15:13;jbellis;Split that out to CASSANDRA-6274 to keep CHANGES clean when i tag it 2.0.3.;;;","30/Oct/13 15:20;jbellis;Committed the MT and PCR removal to 2.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra-cli backward compatibility issue with Cassandra 2.0.1,CASSANDRA-6140,12672150,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,doanduyhai,doanduyhai,03/Oct/13 14:59,16/Apr/19 09:32,14/Jul/23 05:53,05/Nov/13 18:18,2.0.3,,,,,,0,,,,,"Currently we are using Cassandra 1.2.6 and we want to migrate to 2.0.1.

 We still use Thrift for some column families (migration to CQL3 is not done yet for them). We have cassandra-cli script to drop/create fresh keyspace, re-create column families and populate referential data:

*Schema creation script*
{code}
drop keyspace xxx;
create keyspace xxx with placement_strategy ...

create column family offers with 
key_validation_class = UTF8Type and
comparator = 'CompositeType(UTF8Type)'  and 
default_validation_class = UTF8Type;
{code}

*Data insertion script*:
{code}
set offers['OFFER1'][PRODUCT1']='test_product';
...
{code}

 When executing the data insertion script with Cassandra 2.0.1, we have the following stack trace:
{code}
Invalid cell for CQL3 table offers. The CQL3 column component (COL1) does not correspond to a defined CQL3 column
InvalidRequestException(why:Invalid cell for CQL3 table offers. The CQL3 column component (COL1) does not correspond to a defined CQL3 column)
	at org.apache.cassandra.thrift.Cassandra$insert_result$insert_resultStandardScheme.read(Cassandra.java:21447)
	at org.apache.cassandra.thrift.Cassandra$insert_result$insert_resultStandardScheme.read(Cassandra.java:21433)
	at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:21367)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:898)
	at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:882)
	at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:987)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:231)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:201)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:327)
{code}

 This data insertion script works pecfectly with Cassandra 1.2.6.

 We face the same issue with Cassandra 2.0.0. It looks like the cassandra-cli commands no longer works with Cassandra 2.0.0...

  

","Linux Ubuntu, Cassandra 2.0.0",aleksey,dbrosius,doanduyhai,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Nov/13 16:33;slebresne;6140.txt;https://issues.apache.org/jira/secure/attachment/12612202/6140.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351776,,,Tue Nov 05 18:18:47 UTC 2013,,,,,,,,,,"0|i1ongn:",352064,2.0.1,,,,,,,,aleksey,,aleksey,Normal,,2.0.0,,,,,,,,,,,,,,,,"21/Oct/13 15:39;jbellis;Can you try 2.0 HEAD?;;;","02/Nov/13 09:34;doanduyhai;I just pull Cassandra from github and tried with trunk (cassandra-2.1-SNAPSHOT) and also with branch cassandra-2.0, same result.

 The stack trace is the same;;;","03/Nov/13 05:42;dbrosius;Validation code thinks the table is a cql table, given this test
{code}
boolean isCQL3Table = metadata.hasCompositeComparator() && !metadata.isDense() && !metadata.isSuper();
{code}
and then fails, because there are no column names in the meta data
{code}
ColumnIdentifier columnId = new ColumnIdentifier(CQL3ColumnName, composite.types.get(columnIndex));
if (metadata.getColumnDefinition(columnId) == null)
     throw new org.apache.cassandra.exceptions.InvalidRequestException(String.format(""Invalid cell for CQL3 table %s. The CQL3 column component (%s) does not correspond to a defined CQL3 column"", metadata.cfName, columnId));
{code};;;","03/Nov/13 05:58;dbrosius;As an aside, is there a reason to allow a single component composite, as above:

CompositeType(UTF8Type);;;","03/Nov/13 09:24;doanduyhai;""As an aside, is there a reason to allow a single component composite""

 Yes, we want to be able to query with strict inequality on 'PRODUCT', they are ordered by name;;;","05/Nov/13 16:33;slebresne;The problem is that there is no definitive marker for a ""CQL3 table"" (in hindsight, adding a new CQL3 ColumnFamilyType for CQL3 table would have made things a lot easier, but it's possibly a bit late now). So we ""guess"" more than anything else if a table is really a CQL3 one and in that case we're wrong.  More precisely, the reason the code thinks it's a CQL3 table is because it looks exactly like the table that would have been defined by: {noformat}
CREATE TABLE (
  key text,
  column1 text,
  PRIMARY KEY (key, column1)
)
{noformat}

Anyway, I guess the simpler fix here is to only do validation on the thrift side when we're sure that the table can't have been created from thrift. That will be good enough in almost all cases, the case of tables having only a PRIMARY KEY and no other columns being the exception. Simple patch attached.
;;;","05/Nov/13 16:58;aleksey;+1;;;","05/Nov/13 18:18;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Cqlsh shouldn't display empty ""value alias""",CASSANDRA-6139,12672149,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,slebresne,slebresne,03/Oct/13 14:22,16/Apr/19 09:32,14/Jul/23 05:53,21/Oct/13 15:16,2.0.2,,,,,,0,,,,,"When someone creates:
{noformat}
CREATE TABLE foo (
   k int,
   v int,
   PRIMARY KEY (k, v)
) WITH COMPACT STORAGE
{noformat}
then we internally create a ""value alias"" (1.2)/""compact value definition"" (2.0) with an empty name. Seems that cqlsh don't recognize that fact and display that as:
{noformat}
cqlsh:ks> DESC TABLE foo;

CREATE TABLE foo (
  k int,
  v int,
  """" blob,
  PRIMARY KEY (k, v)
) WITH COMPACT STORAGE AND ...
{noformat}",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/13 08:49;aleksey;6139.txt;https://issues.apache.org/jira/secure/attachment/12609391/6139.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351775,,,Mon Oct 21 15:16:09 UTC 2013,,,,,,,,,,"0|i1ongf:",352063,2.0.1,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"21/Oct/13 08:50;aleksey;Trivial patch attached.;;;","21/Oct/13 14:58;brandon.williams;+1;;;","21/Oct/13 15:16;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL should not allow an empty string as column identifier,CASSANDRA-6136,12672018,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,mfiguiere,mfiguiere,02/Oct/13 20:32,16/Apr/19 09:32,14/Jul/23 05:53,03/Oct/13 14:31,2.0.2,,,,,,0,,,,,"CQL currently allows users to create a table with an empty string as column identifier:

{code}
CREATE TABLE t (k int primary key, """" int);
{code}

Which results in the following table:

{code}
CREATE TABLE t (
  k int,
  """" int,
  PRIMARY KEY (k)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};
{code}

Empty strings are not allowed for keyspace and table identifiers though.

I guess it's just a case that we haven't covered. Of course making it illegal in a future version would be a breaking change, but nobody serious would manually have chosen such an identifier...",,dbrosius,jjordan,mfiguiere,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/13 01:51;dbrosius;6136.txt;https://issues.apache.org/jira/secure/attachment/12606505/6136.txt","03/Oct/13 02:55;dbrosius;6136_v2.txt;https://issues.apache.org/jira/secure/attachment/12606512/6136_v2.txt","03/Oct/13 12:11;dbrosius;6136_v3.txt;https://issues.apache.org/jira/secure/attachment/12606573/6136_v3.txt",,,,,,,,,,,,,,,,,,3.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351644,,,Thu Oct 03 14:31:03 UTC 2013,,,,,,,,,,"0|i1omnj:",351932,,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,,"02/Oct/13 20:36;mfiguiere;Looks like it's used in {{system.""IndexInfo""}} actually:

{code}
cqlsh> DESC TABLE system.""IndexInfo""

CREATE TABLE ""IndexInfo"" (
  table_name text,
  index_name text,
  """" blob,
  PRIMARY KEY (table_name, index_name)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='indexes that have been completed' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=0 AND
  index_interval=128 AND
  read_repair_chance=0.000000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  default_time_to_live=0 AND
  speculative_retry='NONE' AND
  memtable_flush_period_in_ms=0 AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{code}

Would it be the reason for that to be allowed?;;;","03/Oct/13 02:55;dbrosius;version 1 disallows empty column names everywhere
version 2 disallows empty column names everywhere but in the system keyspace;;;","03/Oct/13 09:02;slebresne;Concerning IndexInfo, this is really a bug of the describe command of cqlsh. We internally use a empty column name to represent COMPACT tables that have not column outside the PK (which is generally allowed). So cqlsh should be not display that empty column.

Now the fact that we use it internally for that purpose is probably a good idea to refuse it otherwise indeed (the fact that it's not allowed for table and keyspace identifiers is less so, those are a lot more restricted than column names already).

But if we do it, I'd rather directly do it at the grammar level, and disallow empty quoted names altogether (again, that's *not* a problem for IndexInfo).;;;","03/Oct/13 12:10;dbrosius;version3 disallows at the parse layer.;;;","03/Oct/13 14:22;slebresne;Last patch lgtm, +1. I've also created CASSANDRA-6139 to fix cqlsh DESC command.;;;","03/Oct/13 14:31;dbrosius;committed to cassandra-2.0 as commit 27f4ea2bfd8831ee147ee1ed7a59be9c3308a558;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tracing should deal with write failure,CASSANDRA-6133,12671944,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Oct/13 14:34,16/Apr/19 09:32,14/Jul/23 05:53,02/Oct/13 16:10,1.2.11,2.0.2,,Legacy/Tools,,,0,,,,,,,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/13 14:35;jbellis;6133.txt;https://issues.apache.org/jira/secure/attachment/12606366/6133.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351570,,,Wed Oct 02 16:10:21 UTC 2013,,,,,,,,,,"0|i1om7b:",351859,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"02/Oct/13 14:35;jbellis;Attached.;;;","02/Oct/13 15:15;aleksey;Hmm. Shall we ever get UAE when writing with CL.ANY? I don't think so. Same re WTE, once CASSANDRA-6132 is resolved.

Only OE should be possible. I'd replace all those catch-es with a single catch (REE) with assert e instanseof OE and the log warning for 'too many nodes are overloaded to save trace events'.

Other than that +1.;;;","02/Oct/13 16:10;jbellis;committed w/ suggested changes;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CL.ANY writes can still time out,CASSANDRA-6132,12671942,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,02/Oct/13 14:33,16/Apr/19 09:32,14/Jul/23 05:53,03/Oct/13 00:12,2.0.2,,,,,,0,,,,,"If we know that all replicas are down at the beginning of a mutation, we will write a hint and return success.

But if we do not, we will attemp to write to replicas, time out, return failure, and then write a hint, violating our contract that (unless the coordinator goes down), writes at CL.ANY should always succeed.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Oct/13 19:11;jbellis;6132-v2.txt;https://issues.apache.org/jira/secure/attachment/12606418/6132-v2.txt","02/Oct/13 17:03;jbellis;6132.txt;https://issues.apache.org/jira/secure/attachment/12606386/6132.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351568,,,Fri Oct 04 21:33:26 UTC 2013,,,,,,,,,,"0|i1om6v:",351857,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"02/Oct/13 17:03;jbellis;Patch attached against 1.2.;;;","02/Oct/13 18:53;aleksey;Huh, are you sure? It doesn't build for me (WriteCallbackInfo?).

;;;","02/Oct/13 19:11;jbellis;v2;;;","02/Oct/13 23:38;aleksey;+1;;;","03/Oct/13 00:12;jbellis;committed w/ additional comment;;;","04/Oct/13 21:33;jbellis;This was messier than I thought and (four ninja commits later) I'm not confident all the corner cases are fixed.  Reverted from 1.2.x; will fix in 2.0.y only.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
JAVA_HOME on cassandra-env.sh is ignored on Debian packages,CASSANDRA-6131,12671937,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,urandom,sebastianlacuesta,sebastianlacuesta,02/Oct/13 14:24,16/Apr/19 09:32,14/Jul/23 05:53,02/Jan/14 23:13,2.0.5,,,Packaging,,,0,qa-resolved,,,,"I've just got upgraded to 2.0.1 package from the apache repositories using apt. I had the JAVA_HOME environment variable set in /etc/cassandra/cassandra-env.sh but after the upgrade it only worked by setting it on /usr/sbin/cassandra script. I can't configure java 7 system wide, only for cassandra.
Off-toppic: Thanks for getting rid of the jsvc mess.",,mshuler,sebastianlacuesta,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Dec/13 04:40;mshuler;6131-2.patch;https://issues.apache.org/jira/secure/attachment/12619746/6131-2.patch","14/Oct/13 13:35;urandom;6131.patch;https://issues.apache.org/jira/secure/attachment/12608279/6131.patch",,,,,,,,,,,,,,,,,,,2.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351563,,,Wed Apr 30 18:24:20 UTC 2014,,,,,,,,,,"0|i1om5r:",351852,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,mshuler,,,"03/Oct/13 15:33;urandom;Could you expound on this a bit?  Are you trying to _set_ {{JAVA_HOME}} from within {{cassandra-env.sh}}?;;;","03/Oct/13 16:32;sebastianlacuesta;Yes, I thought it was the way I should do this, in fact (due to time
constraints) I've just set it at /usr/sbin/cassandra (I know it's really
_awfull_). I used to set JAVA_HOME at /etc/cassandra/cassandra-env.sh and
it worked until 2.0.0. I need to use java 6 as the  default jvm on my
development machine, but jvm 7 just for cassandra, if I'm really wrong on
how I'm proceeding, I'd be glad to know what's the way to set it up.
Thanks a lot!


2013/10/3 Eric Evans (JIRA) <jira@apache.org>

;;;","03/Oct/13 19:39;urandom;If setting {{JAVA_HOME}} from {{cassandra-env.sh}} ever worked before (from the Debian package), it was probably by accident, but there is no reason we can't support it going forward.

For what it's worth, I'd probably recommend using {{/etc/default/cassandra}} for Debian/Ubuntu, but it will work with either.

[~sebastianlacuesta], could you test the attached patch and let me know if this solves it for you?;;;","07/Oct/13 19:04;sebastianlacuesta;Tried the patch with the 2.0.1 source tarball from the debian src file extracted from .dsc,
{code}
patching file debian/init
Hunk #4 FAILED at 53.
Hunk #5 FAILED at 95.
2 out of 5 hunks FAILED -- saving rejects to file debian/init.rej
{code}
content of debian/init.rej:
{code:title=debian/init.rej|borderStyle=solid}
--- debian/init
+++ debian/init
@@ -53,10 +29,6 @@
 # Depend on lsb-base (>= 3.0-6) to ensure that this file is present.
 . /lib/lsb/init-functions
 
-# If JNA is installed, add it to EXTRA_CLASSPATH
-#
-EXTRA_CLASSPATH=""/usr/share/java/jna.jar:$EXTRA_CLASSPATH""
-
 #
 # Function that returns 0 if process is running, or nonzero if not.
 #
@@ -95,7 +67,7 @@
     [ -e `dirname ""$PIDFILE""` ] || \
         install -d -ocassandra -gcassandra -m750 `dirname $PIDFILE`
 
-    export EXTRA_CLASSPATH
+
 
     start-stop-daemon -S -c cassandra -a /usr/sbin/cassandra -q -p ""$PIDFILE"" -t >/dev/null || return 1
{code};;;","14/Oct/13 13:35;urandom;Rebased to {{cassandra-2.0}} branch.
;;;","14/Oct/13 13:37;urandom;[~sebastianlacuesta]: that patch is meant to apply to the 2.0 branch (where it will it land); are you able to test against the 2.0 branch?;;;","19/Dec/13 23:17;jbellis;[~mshuler] can you test the above?;;;","20/Dec/13 04:40;mshuler;6131-2.patch has the first hunk modified for current cassandra-2.0 branch (JVM_SEARCH_DIRS was different and failed);;;","02/Jan/14 17:29;mshuler;Works fine for me with setting JAVA_HOME in the recommended location of /etc/default/cassandra or in cassandra-env.sh;;;","02/Jan/14 23:13;urandom;thanks Michael; committed;;;","30/Apr/14 18:24;sebastianlacuesta;Works for me. Thanks!



;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
get java.util.ConcurrentModificationException while bulkloading from sstable for widerow table,CASSANDRA-6129,12671715,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,ksaritek,ksaritek,02/Oct/13 06:12,16/Apr/19 09:32,14/Jul/23 05:53,05/Oct/13 00:43,2.0.2,,,Legacy/Tools,,,0,,,,,"I haven't faced that problem with cassandra 1.2.6

I have created widerow sstables with SSTableSimpleUnsortedWriter. When i tried to load sstables by sstableloader, I got java.util.ConcurrentModificationException after a while (not at the beggining of the streaming).

Exception is :
progress: [/192.168.103.5 0/39 (0%)] [/192.168.103.3 0/39 (0%)] [/192.168.103.1 0/39 (0%)] [total: 0% - 15MB/s (avg: 0MB/s)] INFO 00:45:23,542 [Stream #c0f53e00-2ae2-11e3-ab6b-99a3e9e32246] Session with /192.168.103.3 is complete
progress: [/192.168.103.5 0/39 (0%)] [/192.168.103.3 0/39 (0%)] [/192.168.103.1 0/39 (0%)] [total: 0% - 3MB/s (avg: 1MB/s)]Exception in thread ""STREAM-OUT-/192.168.103.3"" java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:894)
	at java.util.HashMap$EntryIterator.next(HashMap.java:934)
	at java.util.HashMap$EntryIterator.next(HashMap.java:932)
	at org.apache.cassandra.tools.BulkLoader$ProgressIndicator.handleStreamEvent(BulkLoader.java:129)
	at org.apache.cassandra.streaming.StreamResultFuture.fireStreamEvent(StreamResultFuture.java:198)
	at org.apache.cassandra.streaming.StreamResultFuture.handleProgress(StreamResultFuture.java:191)
	at org.apache.cassandra.streaming.StreamSession.progress(StreamSession.java:474)
	at org.apache.cassandra.streaming.StreamWriter.write(StreamWriter.java:105)
	at org.apache.cassandra.streaming.messages.FileMessage$1.serialize(FileMessage.java:73)
	at org.apache.cassandra.streaming.messages.FileMessage$1.serialize(FileMessage.java:45)
	at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:44)
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:384)
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:357)
	at java.lang.Thread.run(Thread.java:781)
progress: [/192.168.103.5 0/39 (3%)] [/192.168.103.3 0/39 (0%)] [/192.168.103.1 0/39 (2%)] [total: 1% - 2147483647MB/s (avg: 12MB/s)]Exception in thread ""STREAM-OUT-/192.168.103.1"" java.util.ConcurrentModificationException
	at java.util.HashMap$HashIterator.nextEntry(HashMap.java:894)
	at java.util.HashMap$KeyIterator.next(HashMap.java:928)
progress: [/192.168.103.5 0/39 (3%)] [/192.168.103.3 0/39 (0%)] [/192.168.103.1 0/39 (2%)] [total: 1% - 2147483647MB/s (avg: 12MB/s)]
	at org.apache.cassandra.streaming.StreamResultFuture.fireStreamEvent(StreamResultFuture.java:198)
	at org.apache.cassandra.streaming.StreamResultFuture.handleProgress(StreamResultFuture.java:191)
	at org.apache.cassandra.streaming.StreamSession.progress(StreamSession.java:474)
	at org.apache.cassandra.streaming.StreamWriter.write(StreamWriter.java:105)
	at org.apache.cassandra.streaming.messages.FileMessage$1.serialize(FileMessage.java:73)
	at org.apache.cassandra.streaming.messages.FileMessage$1.serialize(FileMessage.java:45)
	at org.apache.cassandra.streaming.messages.StreamMessage.serialize(StreamMessage.java:44)
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.sendMessage(ConnectionHandler.java:384)
	at org.apache.cassandra.streaming.ConnectionHandler$OutgoingMessageHandler.run(ConnectionHandler.java:357)
	at java.lang.Thread.run(Thread.java:781)
","three cassandra 2.0.1 node
jdk 7
linux - ubuntu",ksaritek,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Oct/13 21:03;jbellis;6129-CLQ.txt;https://issues.apache.org/jira/secure/attachment/12606890/6129-CLQ.txt","03/Oct/13 12:37;ksaritek;BulkLoader.diff;https://issues.apache.org/jira/secure/attachment/12606577/BulkLoader.diff","04/Oct/13 17:17;mishail;trunk-6129-synch-iter.patch;https://issues.apache.org/jira/secure/attachment/12606829/trunk-6129-synch-iter.patch",,,,,,,,,,,,,,,,,,3.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351425,,,Sat Oct 05 00:43:05 UTC 2013,,,,,,,,,,"0|i1olb3:",351714,2.0.0,2.0.1,,,,,,,mishail,,mishail,Normal,,2.0.0,,,,,,,,,,,,,ksaritek,,,"02/Oct/13 17:43;jbellis;Converted the map in question to CHM.  Thanks for the report!;;;","03/Oct/13 05:23;ksaritek;Is there a fix for that? What is CHM stand for? ;;;","03/Oct/13 05:43;ksaritek;Got it, have a looked commits at git. Convert from hashmap to concurrenthashmap.

Thanks :);;;","03/Oct/13 10:26;ksaritek;I have tested against 2.0.1 and got the problem. Seems that concurrent set is needed for progressByHost. 

I made some more changes and attached git diff file (BulkLoader.diff for BulkLoader.java)

wait for your comment.

Thanks
Koray ;;;","03/Oct/13 14:31;jbellis;done in 6ca9b4842942db6ff7a978f1054bb619f07a60ad;;;","04/Oct/13 09:05;ksaritek;At BulkLoad process, get exception from StreamFuture eventListeners list as:
INFO 17:27:46,987 [Stream #f92ac500-2c37-11e3-ad9c-99a3e9e32246] Prepare completed. Receiving 0 files(0 bytes), sending 1 files(115 bytes)
ERROR 17:27:47,005 Error in ThreadPoolExecutor
java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:819)
	at java.util.ArrayList$Itr.next(ArrayList.java:791)
	at org.apache.cassandra.streaming.StreamResultFuture.fireStreamEvent(StreamResultFuture.java:197)
	at org.apache.cassandra.streaming.StreamResultFuture.handleSessionPrepared(StreamResultFuture.java:175)
	at org.apache.cassandra.streaming.StreamSession.startStreamingFiles(StreamSession.java:620)
	at org.apache.cassandra.streaming.StreamSession.onInitializationComplete(StreamSession.java:400)
	at org.apache.cassandra.streaming.StreamSession$1.run(StreamSession.java:200)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:781)
ERROR 17:27:47,005 Error in ThreadPoolExecutor
java.util.ConcurrentModificationException
	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:819)
	at java.util.ArrayList$Itr.next(ArrayList.java:791)
	at org.apache.cassandra.streaming.StreamResultFuture.fireStreamEvent(StreamResultFuture.java:197)
	at org.apache.cassandra.streaming.StreamResultFuture.handleSessionPrepared(StreamResultFuture.java:175)
	at org.apache.cassandra.streaming.StreamSession.startStreamingFiles(StreamSession.java:620)
	at org.apache.cassandra.streaming.StreamSession.onInitializationComplete(StreamSession.java:400)
	at org.apache.cassandra.streaming.StreamSession$1.run(StreamSession.java:200)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:781)
Exception in thread ""StreamConnectionEstablisher:1"" Exception in thread ""StreamConnectionEstablisher:2"" java.util.ConcurrentModificationExceptionjava.util.ConcurrentModificationException

	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:819)	at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:819)

	at java.util.ArrayList$Itr.next(ArrayList.java:791)	at java.util.ArrayList$Itr.next(ArrayList.java:791)

	at org.apache.cassandra.streaming.StreamResultFuture.fireStreamEvent(StreamResultFuture.java:197)	at org.apache.cassandra.streaming.StreamResultFuture.fireStreamEvent(StreamResultFuture.java:197)

	at org.apache.cassandra.streaming.StreamResultFuture.handleSessionPrepared(StreamResultFuture.java:175)	at org.apache.cassandra.streaming.StreamResultFuture.handleSessionPrepared(StreamResultFuture.java:175)

	at org.apache.cassandra.streaming.StreamSession.startStreamingFiles(StreamSession.java:620)	at org.apache.cassandra.streaming.StreamSession.startStreamingFiles(StreamSession.java:620)

	at org.apache.cassandra.streaming.StreamSession.onInitializationComplete(StreamSession.java:400)	at org.apache.cassandra.streaming.StreamSession.onInitializationComplete(StreamSession.java:400)

	at org.apache.cassandra.streaming.StreamSession$1.run(StreamSession.java:200)	at org.apache.cassandra.streaming.StreamSession$1.run(StreamSession.java:200)

	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)

	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)

	at java.lang.Thread.run(Thread.java:781)	at java.lang.Thread.run(Thread.java:781)

Terminated;;;","04/Oct/13 17:17;mishail;Attached a patch. As per JavaDoc: {{It is imperative that the user manually synchronize on the returned list when iterating over it}};;;","04/Oct/13 21:03;jbellis;IMO, simpler to switch to CLQ since we don't actually need List API; attached.  WDYT?;;;","04/Oct/13 22:55;mishail;+1;;;","05/Oct/13 00:43;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add more data mappings for Pig,CASSANDRA-6128,12671672,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,01/Oct/13 22:05,16/Apr/19 09:32,14/Jul/23 05:53,07/Oct/13 19:07,1.2.11,2.0.2,,,,,0,,,,,"We need add more data mappings for
{code}
 DecimalType
 InetAddressType
 LexicalUUIDType
 TimeUUIDType
 UUIDType
{code}

Existing implementation throws exception for those data type",,alexliu68,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/13 22:19;alexliu68;6128-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12606207/6128-1.2-branch.txt",,,,,,,,,,,,,,,,,,,,1.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351382,,,Tue Oct 08 11:18:20 UTC 2013,,,,,,,,,,"0|i1ol27:",351674,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"01/Oct/13 22:21;alexliu68;Map

{code}
 DecimalType
 InetAddressType
 LexicalUUIDType
 TimeUUIDType
 UUIDType
{code}

to CHARARRAY;;;","07/Oct/13 15:28;brandon.williams;Shouldn't DecimalType map to a float or double instead of a string?;;;","07/Oct/13 18:27;alexliu68;Decimal has different precision than float/double, we will lose precision if we convert a decimal to a float/double. It is explained in this link http://stackoverflow.com/questions/5749615/losing-precision-converting-from-java-bigdecimal-to-double

If we don't need preserve the precision, we can use a double instead of a string. ;;;","07/Oct/13 19:07;brandon.williams;Well, crap: PIG-2764

I guess we'll have to use a string for now, otherwise we box people into the corner of precision loss with no way out.  At least with strings they can do something in a UDF, so +1 and committed.;;;","08/Oct/13 05:32;jeromatron;true, though we've been doing that all along anyway - we've always mapped Cassandra's BigInteger type to Pig's int and just accepted any possible loss.  That Pig ticket is nice to have for the future though.;;;","08/Oct/13 11:18;brandon.williams;Actually, we just let BigInteger overflow:

{noformat}
        else if (type instanceof IntegerType || type instanceof Int32Type) // IntegerType will overflow at 2**31, but is kept for compatibility until pig has a BigInteger
{noformat}

but no one has ever noticed that, which doesn't surprise me.  On the other hand I think losing precision in DecimalType is likely to be a bigger, more subtle problem, when encountered. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
vnodes don't scale to hundreds of nodes,CASSANDRA-6127,12671666,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,tupshin,tupshin,01/Oct/13 21:19,16/Apr/19 09:32,14/Jul/23 05:53,03/Dec/13 14:39,,,,,,,0,,,,,"There are a lot of gossip-related issues related to very wide clusters that also have vnodes enabled. Let's use this ticket as a master in case there are sub-tickets.

The most obvious symptom I've seen is with 1000 nodes in EC2 with m1.xlarge instances. Each node configured with 32 vnodes.

Without vnodes, cluster spins up fine and is ready to handle requests within 30 minutes or less. 

With vnodes, nodes are reporting constant up/down flapping messages with no external load on the cluster. After a couple of hours, they were still flapping, had very high cpu load, and the cluster never looked like it was going to stabilize or be useful for traffic.",Any cluster that has vnodes and consists of hundreds of physical nodes.,aleksey,alexliu68,brandon.williams,cburroughs,christianmovi,colinkuo,djatnieks,dkblinux98,enigmacurry,jasobrown,jeromatron,jjordan,johnny15676,mishail,mstump,qconner,rcoli,tupshin,wdhathaway,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6385,CASSANDRA-6386,CASSANDRA-6409,CASSANDRA-6410,,,,,,,,,,,,,,CASSANDRA-4288,CASSANDRA-6297,CASSANDRA-6244,CASSANDRA-6345,CASSANDRA-6338,,,,,,,"07/Nov/13 15:14;qconner;2013-11-05_18-04-03_no_compression_cpu_time.png;https://issues.apache.org/jira/secure/attachment/12612626/2013-11-05_18-04-03_no_compression_cpu_time.png","07/Nov/13 15:12;qconner;2013-11-05_18-09-38_compression_on_cpu_time.png;https://issues.apache.org/jira/secure/attachment/12612625/2013-11-05_18-09-38_compression_on_cpu_time.png","24/Oct/13 16:44;qconner;6000vnodes.patch;https://issues.apache.org/jira/secure/attachment/12610105/6000vnodes.patch","24/Oct/13 16:51;qconner;AdjustableGossipPeriod.patch;https://issues.apache.org/jira/secure/attachment/12610107/AdjustableGossipPeriod.patch","12/Nov/13 21:30;qconner;cpu-vs-token-graph.png;https://issues.apache.org/jira/secure/attachment/12613430/cpu-vs-token-graph.png","24/Oct/13 17:46;qconner;delayEstimatorUntilStatisticallyValid.patch;https://issues.apache.org/jira/secure/attachment/12610117/delayEstimatorUntilStatisticallyValid.patch","12/Nov/13 21:25;qconner;flaps-vs-tokens.png;https://issues.apache.org/jira/secure/attachment/12613427/flaps-vs-tokens.png","20/Nov/13 15:21;qconner;vnodes & gossip flaps.png;https://issues.apache.org/jira/secure/attachment/12614903/vnodes+%26+gossip+flaps.png",,,,,,,,,,,,,8.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351376,,,Tue Dec 03 14:39:28 UTC 2013,,,,,,,,,,"0|i1ol0v:",351668,1.2.6,1.2.9,,,,,,,,,,Normal,,1.2.0 beta 1,,,,,,,,,,,,,,,,"01/Oct/13 21:27;jbellis;bq. After a couple of hours, they were still flapping, had very high cpu load

To clarify, this is a bit of a mashup of multiple observations:

bq. When there was zero traffic on the cluster, we were seeing flapping without very high cpu. On smaller tests, we saw much higher cpu than expected when under load.;;;","24/Oct/13 16:28;qconner;*Background and Reproduction*

The symptom is evident with the presence of ""is now DOWN"" messages in the Cassandra system.log file.  The recording of a node DOWN is often followed by a node UP a few seconds later.  Users have coined this phenomenon ""gossip flap"" and the occurence of ""Gossip flaps"" has a machine and a human consequence.

Humans react strongly to the (temporary) marking of a node down.  Automated monitoring may trigger SNMP traps, etc.  A ""busy"" node that doesn't transmit heartbeat gossip messages on time will be marked as ""down"" though it may still be performing useful work.

Machine reactions include other C* nodes buffering of row mutations and storage of hints on disk when another node is marked down.  I have not explored the machine reactions but imagine the endpointSnitch could also be affected from the client frame of reference.

One piece of good news is that I was able to reproduce two different use cases that elicit the ""is now DOWN"" message in Log4J log files.

Use Case #1 is as follows:
  provision 256 or 512 nodes in EC2
  install Cassandra 1.2.9
  take defaults except specify num_tokens=256 in c*.yaml
  start one node at a time

Use Case #2 is as follows:
  provision 32 nodes in EC2
  install Cassandra 1.2.9
  take defaults in c*.yaml
  configure rack & datacenter
  start one node at a time
  when all nodes are up create about 1GB of data
    e.g. ""tools/bin/cassandra-stress -c 20 -l 3 -n 1000000""
  provision a 33rdxtra node in EC2
  install Cassandra 1.2.9
  take defaults except specify num_tokens=256
  configure different datacenter than first 32 nodes
  start the node (auto_bootstrap=true)


;;;","24/Oct/13 16:28;qconner;*Analysis*

My first experiments aimed to quantify the length of Gossip messages and determine what factors drive the message length.  I found the size of certain gossip messages increases proportionally with the number of vnodes (num_tokens in c.yaml).  I recorded message size over the num_tokens and number of nodes domains (64,128,256,512,...) for tokens and (32,64,128,256,512) for nodes.  I also made non-rigorous observation of User and Kernel CPU (Ubuntu 10.0.4 LTS).  My hunch is that both vnode count and node count have a mild effect on user CPU resource usage.

What is the rough estimate of bytes sent for certain Gossip messages and why does this matter?  The Phi Accrual Failure Detector (Hayashibara, et al) assumes fixed length heartbeat messages while Cassandra uses variable length messages.  I observed a correlation with larger messages, higher vnodes and false positive detections by the Gossip FailureDetector.  These observations, IMHO, are not explained by the research paper.  I formed a hypothesis that the false positives are due to jitter in the interval values.  I wondered if perhaps using a longer baseline to integrate over would reduce the jitter.  

I have a second theory to follow up on.  A newly added node will not have a long history of Gossip heartbeat interarrival times.  At least 40 samples are needed to compute mean, variance with any statistical significance.  It's possible the phi estimation algorithm is simply invalid for newly created nodes and that is why we see them flap shortly after creation.

In any case, the message of interest is the GossipDigestAck2 (GDA2) because it is the largest of the Gossip messages.  GDA2 contains the set of EndpointStateMaps (node metadata) for newly-discovered nodes, i.e. those nodes just added to an existing cluster.  When each node becomes aware of joining node, they Gossip it to three randomly-chosen other nodes.  The GDA2 message is tailored to contain the delta of new node metadata the receiving node is unaware of.

For a single node, the upper limit on GDA message size is roughly 3 * N * k * V
Where N is the number of nodes in the cluster,
V is the number of tokens (vnodes) per cluster,
k is a constant value, approximately 64 bytes, that represents a serialized token plus some other endpoint metadata.

If one is running hundreds of nodes in a cluster, the Gossip message traffic created when a node joins can be significant and increases with the number of nodes.  I believe this to be the first order effect and probably violates one of the assumptions of the PHI Accrual Failure Detection, that heartbeat messages are small enough not to consume a relevant amount of compute or communication resources.  The variable transmission time (due to variable length messages) is a clear violation of assumptions, if I've read the source code correctly.

On a related topic, there is a hard-coded limitation to the number of vnodes due to the serialization of the GDA messages.
No more than 1720 vnodes can be configured without creating a greater than 32K serialized String vnode message.  A patch is provided below for future use should this become an issue.

In clusters with hundreds of nodes, GDA2 messages can be 200 KB or 2 MB if many nodes join simultaneously.  This is not an issue if the computer experiences no latency from competing workloads.  In the real world, nodes are added because the cluster load has grown in terms of retained data, or in terms of a high transaction arrival rate.  This means node resources may be fully utilized when adding new nodes is typically attempted.

It occured to me that we have another use case to accomodate.  It is common to experience transient failure modes, even in modern data centers with disciplined maintenance practices.  Ethernet cables get moved, switches and routers rebooted.  BGP route errors and other temporary interruptions may occur with the network fabric in real world scenarios.  People make mistakes, plans change and preventative maintenance often causes short-lived interruptions occur with network, CPU and disk subsystems.
;;;","24/Oct/13 16:29;qconner;*Feature Suggestion*

The current Gossip failure detector is characterized by a sliding window of elapsed time, a heartbeat message period and a PHI threshold used to make the continuous random variable (lower case phi) into a dichotomous (binary) random variable.  That PHI (uppercase) threshold is called phi_convict_threshold.

I don't have a better mathmatical theory or derivation at this writing, but I do have an easy workaround for your consideration.  While phi_convict_threshold is adjustable, the period (or frequency) of Gossip messages is not.  Adjusting the gossip period to integrate over a longer time baseline reduced false positives from the Gossip failure detector.  The side effect increases the elapsed time to detect a legitimately-failed node.

Depending on user workload characteristics, and the related sources of latency (CPU, disk and network activity or transient delays) cited above, a System Architect could present a reasonable use case for controlling the Gossip message period.

The goal would be to set a detection window that accomodates common occurences for a given deployment scenario.  Not all data centers are created equal.

Patches and results from implementation will follow in subsequent posts.

*Potential Next Steps*
  Explore concern about sensitivity to gossip period.  Do the vnode gossip messages exceed capacity for peers to ingest?
  Explore concern about phi estimates from un-filled (new) deque.  See Patch #3.
  Explore concern about assuming Gaussian PDF.  Networks (not computers) generally characterize expected arrival time by Poisson distribution, not Gaussian.
;;;","24/Oct/13 16:44;qconner;Patch #1.  Increases number of allowed vnodes (num_tokens) from 1720 up to about 6000 with 512K stack size.

Because of CQL antlr grammar parser, stack is needed to parse token definitions.;;;","24/Oct/13 16:51;qconner;This is patch #2.  It adds a new configuration item in cassandra.yaml, ""gossip_period"".  Set it in milliseconds.

Setting the gossip period by JMX will be lost if stop/restart gossip so needs more work.  Reading from JMX seems fine.

This also isn't DRY.  Should set the value for intervalInMillis, from config, in a static initializer.  Wasn't sure if config object is available in that scope.;;;","24/Oct/13 17:46;qconner;Untested patch #3.  Delays output from FailureDetector until statistically valid number of samples have been obtained.;;;","24/Oct/13 17:49;qconner;First results with workaround patch #2.
No load.  No data.  Only system keyspace and Gossip on a 256 node m1.medium cluster in EC2.
Nodes started in rapid succession.

*phi=8, variable gossip_period*
1154 flaps for 1 sec
685 flaps for 2 sec
146 flaps for 3 sec
88 flaps for 4 sec
70 flaps for 5 sec
100 flaps for 10 sec

*phi=12*
1289 flaps for 1 sec
77 flaps for 2 sec
6 flaps for 3 sec
1 flaps for 4 sec
3 flaps for 5 sec
1 flaps for 6 sec
0 flaps for 8 sec
1 flaps for 10 sec
;;;","24/Oct/13 18:56;brandon.williams;It would be helpful to dump the interval times for a node that is flapping (dumpInterArrivalTimes on the FD) so we can see how long the heartbeats are taking.  If some are excessively long, we need to get threads dumps/debugger timings from the gossiper to see if something is blocking it or taking a long time before changing any fundamentals (gossip interval, FD formula) that we already know work in principle without vnodes.  Increasing the payload size to >32k shouldn't cause these problems, since that is only sent during initial state synchronization and isn't all that large to begin with.;;;","24/Oct/13 22:38;brandon.williams;Can you see if adding -Dcassandra.unsafesystem=true allows the cluster to stabilize at some point?;;;","25/Oct/13 12:29;cburroughs;> It would be helpful to dump the interval times for a node that is flapping (dumpInterArrivalTimes on the FD) so we can see how long the heartbeats are taking.

A per endpoint histogram of heartbeat arrival latency seems a worthwhile o.a.c.Metric to have all the time.

[~qconner]  On the topic of ""wait until there is enough data before doing stuff"" you might also be interested in the heuristic & report from CASSANDRA-4288;;;","25/Oct/13 23:50;qconner;I grabbed some sample log files from 10 nodes of 256 in a run today.  
[flap-intervals.tar.gz|http://qconner.s3.amazonaws.com/flap-intervals.tar.gz]

Convictions are happening with only 1 to 5 intervals recorded.  Patch #3 is looking like the winner but we should do the math by hand to be sure (volunteers?).

Also, I just tested [Patch #3|https://issues.apache.org/jira/secure/attachment/12610117/delayEstimatorUntilStatisticallyValid.patch] and found 0 flaps for the same setup as yesterday (256 nodes, phi=8, normal 1000 ms gossip period).


;;;","26/Oct/13 00:30;jbellis;Patch 1 will break things since later on we write the length of the string as two bytes.

I think we're fine with 1700 vnodes per machine TBH, although it would be better to limit that in the config instead of failing at an assert later on.;;;","26/Oct/13 00:37;tupshin;I'd just set a max of 1024. No one could ever need more than that. (Famous
last words)

;;;","26/Oct/13 01:22;brandon.williams;Patch #3 will make it take much longer for a rebooted node to know who's actually up or down, exacerbating CASSANDRA-4288.  I'd still like to know *why* things are taking longer with vnodes, and I'm especially hesitant to make any adjustments to the gossiper or FD since we know they work fine with single tokens, and also because they *have no knowledge about tokens*, it's just another opaque state to them.  I suspect something in StorageService is blocking the gossiper long enough to cause this, perhaps CASSANDRA-6244 or something similar.;;;","26/Oct/13 02:46;jbellis;Couldn't we tie the thrift/native server startup to ""I have enough gossip data now?"";;;","26/Oct/13 03:03;brandon.williams;That might confuse autodiscovery clients, at least without further changes.;;;","28/Oct/13 12:04;cburroughs;bq.  I'd just set a max of 1024. No one could ever need more than that. (Famous last words)

Isn't that equivalent to saying no one will have a heterogeneous cluster with more  than a 1024/256 = 4 performance delta between physical nodes?  SSD vs spinny could account for more than that. ;;;","28/Oct/13 14:16;jbellis;I'm okay with that limitation.  Intuitively it's reasonable that C* can't compensate for really ridiculous performance differences.

(Of course, you could also reduce the weak nodes below 256.);;;","29/Oct/13 18:17;qconner;Brandon,

You said Patch #3 will make it take much longer for a rebooted node to know who's actually up or down, exacerbating CASSANDRA-4288.  I've given this some thought and want to see if I understand your concern.

Patch #3 serves to send a zero value for phi, for newly-discovered nodes, until an accurate calculation of variance is complete.  This would be 40 seconds, applicable to new nodes only.

However (and this is what I'm looking for you to confirm) If a new node comes online, but is stopped again within 40 seconds of start-up, the FD will not ""convict"" it until the end of that 40 seconds.

I suspect this occurs less frequently than adding a node to a cluster, but probably depends on your use case (dev vs prod).

In my view, we can't escape the math, and the need to amass 40 samples.  That is why the bug exists today.  I agree we should look at tying thrift to a healthy startup as a compensating measure.

Instead of a fixed amount of time (gossip rounds), perhaps we should consider adding a hold-down timer based on a statistical measure?

This hold-down timer could be implemented for newly discovered nodes to suppress interaction until Gossip ""stabilizes"".  Just like we have a high-water mark for phi to denote failure, we could set a low-water mark and call it a trust threshold.  We wouldn't enable thrift communications to the new node until their phi value is below this low-water mark.

So the condition for ""recognizing"" a new node for thrift purposes could be two fold:
  1.  valid computation for variance (40 samples obtained in the 1000 sample window)
  2.  accurate phi value is indeed below the low-water mark;;;","29/Oct/13 19:15;brandon.williams;Let's move that discussion to CASSANDRA-4288, since that change is orthogonal to the actual problem we have here, regardless of whether it fixes it or just papers over the problem.  What we need to do next on this ticket is either correlate a thread dump to what is burning up CPU, or attach a debugger and see where the time is being spent.;;;","29/Oct/13 19:20;jbellis;bq. it would be better to limit that in the config instead of failing at an assert later on.

Split that out to CASSANDRA-6267.;;;","02/Nov/13 01:07;qconner;Monday (11/4) I will be start getting the CPU profiling captured with a 256 or 512 node cluster.  Plan is to capture with internode compression and without.
I was able to get semi-reproduction this week in a 256 node cluster -- one node had twice the cpu utilization of the others (20% user versus 10% user).  But I had too much logging enabled and that skewed results.
;;;","04/Nov/13 19:21;mstump;As another datapoint/use case create a 32 node ring with vnodes, decommission one of the nodes and observe the logs. Every node in the ring will be marked as down by the gossiper, then immediately be re-added again as up/available.;;;","04/Nov/13 19:31;brandon.williams;bq. Every node in the ring will be marked as down by the gossiper

In which node's view? (or all of them?);;;","04/Nov/13 19:42;mstump;We're observing the logs of a random sample of nodes and on all nodes observed the entire ring is marked as down, so I assume it's for all nodes.;;;","04/Nov/13 19:59;jbellis;How heavy is read/write load?;;;","04/Nov/13 20:10;mstump;Zero to minimal load. 177 writes/second, 0 reads against the entire ring. m2.4xlarge instances.;;;","04/Nov/13 21:35;brandon.williams;With CASSANDRA-6244 and CASSANDRA-6297 in 1.2 head, I think we need to re-verify this is still a problem.;;;","05/Nov/13 13:53;qconner;Good cpu profile results were obtained last night with the 1.2.9 code line.  Switching over to the cassandra-1.2 HEAD this morning for up-to-date analysis.
CPU profile of 1.2.9 showed bottleneck was computation of sum for the ArrivalWindow deque members (inter-arrival times of gossip messages).;;;","06/Nov/13 21:00;jbellis;ISTM that FD processing Gossip updates synchronously is a fundamental problem.  Any hiccup in processing will cause FD false positives.  (And even if our own code is perfect, GC pauses can still do this to us.)

Wouldn't it be better if we:
- time heartbeats based on when they arrive instead of when Gossip processes them
- teach FD to recognize that its information is only good up to the most recently processed message -- the absence of messages after that doesn't mean everyone is down unless the Gossip stage is empty;;;","06/Nov/13 21:10;tupshin;+1. Strongly agree with Jonathan's analysis and proposal.;;;","06/Nov/13 22:25;brandon.williams;At this point, I think we should:

* see if the flapping happens with vnodes in 1.2 head (maybe Quentin already knows from his last test)
* see if the flapping happens without vnodes in 1.2 head but the same number of nodes

Because if sum() in ArrivalWindow is burning the most CPU in the Gossiper task (note: not bottlenecking, each call was at most ~3ms, there were just lots of them) then that means that the problem is no longer tied to vnodes (if it ever was, since sum is per-node, not per-token) and we should probably open a new ticket (can't start a cluster of size >=X all at once, or similar) and discuss there.  We know that clusters much larger than any discussed on this ticket exist, but I don't think any of them have all rebooted at once.;;;","07/Nov/13 15:12;qconner;Run against the following commit on cassandra-1.2 branch:
8e7d7285cdeac4f2527c933280d595bbddd26935

This profile measures CPU time, not elapsed time.

Internode compression turned on.  

256 nodes w/ num_tokens=256.  
GossipPeriod 1000 ms.
ArrivalWindow.size() = 1000.
;;;","07/Nov/13 15:14;qconner;Run against the following commit on cassandra-1.2 branch:
8e7d7285cdeac4f2527c933280d595bbddd26935

This profile measures CPU time, not elapsed time.

Internode compression turned off.  

256 nodes w/ num_tokens=256.  
GossipPeriod 1000 ms.
ArrivalWindow.size() = 1000.
;;;","07/Nov/13 15:18;qconner;Good morning.  We saw the same CPU usage profile with cassandra-1.2 8e7d7285cdeac4f2527c933280d595bbddd26935 (which included the patch to not flush peers CF).  

CPU time was spent in looking up EndpointState or spent in PHI calculation.  No surprises were found.  No race conditions, no deadlocks or mutex/monitor contention.

I do not know if flapping happens in 1.2 head without vnodes.  I will find out today, if I can get the nodes (having trouble this morning allocating from EC2).  Will keep trying (Fridays seem better) but could slip into the weekend...
;;;","07/Nov/13 15:24;qconner;Tupshin, can you further quantify the CPU usage you observed, in terms of USER CPU and KERNEL CPU?
Also, can you confirm the number of nodes and vnodes for those observations.

I've seen about 25% user cpu @ 256 nodes and 60% @ 512 nodes.  Kernel cpu was under 5% for both in my trials.;;;","12/Nov/13 21:25;qconner;Flapping occurs with vnodes or without.  Please see below.

!flaps-vs-tokens.png!

Using vnodes appears to exacerbate, possibly with longer messages, probably with higher cpu utilization.  Either would delay the timestamp for the Failure Detector interarrival time.;;;","12/Nov/13 21:30;qconner;The num_tokens setting has a mild impact on average cpu utilization.  Please see the graph below for the trend with 256 nodes.

!cpu-vs-token-graph.png!

This graph does not characterize the bursty nature of any given node's CPU utilization.  It does average the utilization over a 200 second period, taken at 10 second intervals using ""sar -u"".

Since gossip heartbeat destinations are random, ""unlucky"" nodes will sometimes receive twice the gossip traffic and (10 second basis) CPU utilization has been casually observed @25% for N=256 and 60% for N=512.;;;","12/Nov/13 23:43;jbellis;bq. ISTM that FD processing Gossip updates synchronously is a fundamental problem. Any hiccup in processing will cause FD false positives.

I've pulled a fix for this out to CASSANDRA-6338.;;;","19/Nov/13 21:18;jbellis;bq. Untested patch #3. Delays output from FailureDetector until statistically valid number of samples have been obtained.

Did we ever find a scenario where we can demonstrate this patch making a difference?  Because I think it's a good idea in theory.;;;","20/Nov/13 15:18;qconner;Yes, both use case 1 and use case 2 (detailed in early comment above) were cured by patch #3.  Zero flaps were recorded in multiple trials in both use cases.  Patch #3 cures the flaps, but does not address the cpu usage symptom.

This was tested against the cassandra-1.2 branch.  I am conducting the same test today against use case 2 today, but using the current cassandra-2.0 branch of source.;;;","20/Nov/13 15:21;qconner;early results of testing with patch #3 were good

see
vnodes & gossip flaps.png

!vnodes\ \&\ gossip\ flaps.png!;;;","03/Dec/13 14:39;jbellis;I think we've addressed the major problems in the related tickets above.  No single culprit.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in Gossip propagation,CASSANDRA-6125,12671644,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,sbtourist,sbtourist,01/Oct/13 19:01,16/Apr/19 09:32,14/Jul/23 05:53,18/Sep/14 21:38,2.0.11,2.1.1,,,,,0,,,,,"Gossip propagation has a race when concurrent VersionedValues are created and submitted/propagated, causing some updates to be lost, even if happening on different ApplicationStatuses.
That's what happens basically:
1) A new VersionedValue V1 is created with version X.
2) A new VersionedValue V2 is created with version Y = X + 1.
3) V2 is added to the endpoint state map and propagated.
4) Nodes register Y as max version seen.
5) At this point, V1 is added to the endpoint state map and propagated too.
6) V1 version is X < Y, so nodes do not ask for his value after digests.

A possible solution would be to propagate/track per-ApplicationStatus versions, possibly encoding them to avoid network overhead.",,enigmacurry,jasobrown,jeromatron,kohlisankalp,mishail,mshuler,peter-librato,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5913,,,,,,,,,,,,,,,,,"18/Sep/14 20:34;brandon.williams;6125.txt;https://issues.apache.org/jira/secure/attachment/12669797/6125.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351354,,,Wed Jul 22 02:48:04 UTC 2015,,,,,,,,,,"0|i1okvz:",351646,,,,,,,,,jasobrown,,jasobrown,Normal,,,,,,,,,,,,,,,,,,"09/Oct/13 15:28;brandon.williams;Can you point me to a spot in code where step 5 happens?;;;","09/Oct/13 16:22;sbtourist;Step 5 happens in Gossiper#addLocalApplicationState: it's all about concurrent state changes, and even if the race condition window may seem very small, it's actually there, and if it bites, it bites hard (as effects are very difficult to trace back).;;;","29/Jul/14 18:57;mshuler;Setting fixVer to next 2.0/2.1 versions and assigning.;;;","18/Sep/14 20:34;brandon.williams;bq. A possible solution would be to propagate/track per-ApplicationStatus versions, possibly encoding them to avoid network overhead

That's a fairly invasive measure.  In practice there are very few instances where we update two application states together, and really only one case: STATUS and TOKENS.  Instead, we can just create a utility method that locks the gossip task, adds the states, and then releases it, so there can be no propagation in between.  Patch to do this.;;;","18/Sep/14 20:40;brandon.williams;There's still a small window where a remote node could gossip with us and receive a partial state, but that looks confined to node replacement where we'll be in a dead state and nobody will initiate a round with us, so we just need to isolate our own propagation.;;;","18/Sep/14 21:30;jasobrown;+1 on the patch, and I agree with [~brandon.williams]'s thoughts on keeping the protocol simple. As long as versioned values are added to the endpointState in the order in which you assign versions (which this patch provides), there should be no problem. This should be a low bar to clear for anyone who really wants to muck around in gossip-land :).
;;;","18/Sep/14 21:38;brandon.williams;Committed.;;;","22/Jul/15 02:48;peter-librato;We've seen this bug or something like it on 2.0.11 with 45 nodes in a fairly noisy AWS environment but other than CASSANDRA-8336 I don't see any fixes to gossip post 2.0.11.

The nodetool status command doesn't list the node that does't have status info. It's not up or down, it's simply not there and this impacts % ownership.
In a recent instance of this 4 nodes had the same ""status hole"" but only 2 of the 4 had different nodetool ring output compared to the other 41 ""no status hole"" members of the ring.

Restarting cassandra on the node that has a missing STATUS entry in gossip ""fixes"" the problem in that the hole goes away. This is something we used to see more commonly before 2.0.11 so it does appear this fix works but are there other places where a race might be happening?

{code}
/10.xx.yyy.169
  generation:1436544814
  heartbeat:2986679
  SEVERITY:0.0
  HOST_ID:7d22299f-b35b-4035-82bc-e2b603a655d7
  LOAD:2.555557836E11
  RACK:1e
  NET_VERSION:7
  DC:us-east
  RPC_ADDRESS:10.xx.yyy.169
  RELEASE_VERSION:2.0.11
  SCHEMA:0f72be52-2751-33a6-a172-8511e943b2ec
/10.xx.yyy.175
  generation:1419877470
  heartbeat:53496976
  SEVERITY:1.2787723541259766
  HOST_ID:c87ed8db-76b6-485a-ac2f-32c2822b1ef5
  LOAD:3.08812188602E11
  RACK:1e
  NET_VERSION:7
  STATUS:NORMAL,-1010822684895662807
  DC:us-east
  RPC_ADDRESS:10.xx.yyy.175
  RELEASE_VERSION:2.0.11
  SCHEMA:0f72be52-2751-33a6-a172-8511e943b2ec
{code}
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexedSliceReader can skip columns when fetching multiple contiguous slices,CASSANDRA-6119,12671547,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,frousseau,frousseau,frousseau,01/Oct/13 10:32,16/Apr/19 09:32,14/Jul/23 05:53,03/Oct/13 09:11,1.2.11,,,,,,0,,,,,"This was observed using SliceQueryFilter with multiple slices.

Let's have a row ""a"" having the following column list : ""colA"", ""colB"", ""colC"", ""colD""
Then select 2 ranges : [""colA"", ""colB""], [""colC"", ""colD""]
Expected result is the four columns
But only 3 are returned (""colA"", ""colB"", ""colD"")

To reproduce the above scenario in the unit tests, you can modify the test ""ColumnFamilyStoreTest.testMultiRangeIndexed"" by replacing the original line :
        String[] letters = new String[] { ""a"", ""b"", ""c"", ""d"", ""e"", ""f"", ""g"", ""h"", ""i"" };
by this one (""f"" letter has been removed) :
        String[] letters = new String[] { ""a"", ""b"", ""c"", ""d"", ""e"", ""g"", ""h"", ""i"" };

Anyway, a patch is attached which adds more unit tests, and modifies IndexedSliceReader.IndexedBlockFetcher & IndexedSliceReader.SimpleBlockFetcher 
",,frousseau,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Oct/13 16:40;slebresne;6119-v2.txt;https://issues.apache.org/jira/secure/attachment/12606130/6119-v2.txt","01/Oct/13 10:36;frousseau;6119.patch;https://issues.apache.org/jira/secure/attachment/12606075/6119.patch",,,,,,,,,,,,,,,,,,,2.0,frousseau,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351257,,,Thu Oct 03 09:11:21 UTC 2013,,,,,,,,,,"0|i1okaf:",351549,1.2.10,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"01/Oct/13 16:40;slebresne;Thanks Fabien, you're right, ISR can skip columns.

On the patch itself, it seems to me the actual bug if about dealing with the last column when exiting the main loop of both fetcher, the setNextSlice seem unrelated to the bug itself. And I believe those setNextSlice changes mainly inline in setNextSlice the tests that would be done by the next iterator of the main loop otherwise, so I'm not sure they are really optimisations. If I'm right about that, I'd really rather leave those changes away because the logic of ISR is already not the easier to follow, so I'd rather limit the number of cases as much as possible (unless there is a clear benefit performance wise).

As for the stopping conditions of the ""main"" BlockFetcher loops, I believe that for IndexedBlockFetcher it's enough to just not exit the loop until {{column != null}}, which avoids having to add a new case at the end. As for SimpleBlockFetcher, figured we could slightly refactor the loop to mimic the method of IndexedBlockFetcher too. Attaching a v2 with what I have in mind.
;;;","02/Oct/13 10:33;frousseau;After having reviewed your patch, it has definitely a better approach minimising code change and better readability

So I'm +1 for your patch;;;","03/Oct/13 09:11;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
UnsortedColumns.addAll(ColumnFamily) doesn't add the deletion info of the CF in argument,CASSANDRA-6115,12671458,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,sargun,sargun,30/Sep/13 22:17,16/Apr/19 09:32,14/Jul/23 05:53,01/Oct/13 15:39,2.0.2,,,,,,0,,,,,"Steps to reproduce:
https://gist.github.com/sargun/6771123

Doing an action outside of a BATCH results in different results than within the batch, even though the query is successful.",3 nodes on a same local host,aleksey,rcoli,sargun,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6221,,,,,,,,,,,,,,,,,,,,"01/Oct/13 08:08;slebresne;6115.txt;https://issues.apache.org/jira/secure/attachment/12606061/6115.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351164,,,Tue Oct 01 15:39:36 UTC 2013,,,,,,,,,,"0|i1ojpr:",351456,2.0.0,2.0.1,,,,,,,aleksey,,aleksey,Normal,,2.0.0,,,,,,,,,,,,,,,,"30/Sep/13 22:39;jbellis;Can you reproduce, Daniel?;;;","30/Sep/13 23:49;aleksey;1.2 is unaffected, this is a 2.0 regression. The issue is not batch-related, something's likely wrong with resolve().;;;","01/Oct/13 08:08;slebresne;The problem is that UnsortedColumns.addAll(ColumnFamily) doesn't add the deletion info of the CF in argument while it should. Attaching oneliner fix (I've pushed a dtest too). ;;;","01/Oct/13 10:40;aleksey;+1;;;","01/Oct/13 15:39;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig with widerows=true and batch size = 1 works incorrectly,CASSANDRA-6114,12671448,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,alexliu68,alexliu68,alexliu68,30/Sep/13 21:29,16/Apr/19 09:32,14/Jul/23 05:53,20/Oct/13 01:10,1.2.12,2.0.2,,,,,0,,,,,"If I run the demo pig scripts, I end up with a column family with 6 fairly wide rows.  If I load and dump those rows with widerows=true or set the cassandra.range.batch.size=1, the dump returns the correct values.  However, if I set both of those, it does not.  So in the case of a batch size of 1, wide rows support is broken.

So it's relatively simple to reproduce from the demo data:
{code}
grunt> SET cassandra.range.batch.size 1                                                
grunt> rows = LOAD 'cassandra://PigDemo/Scores' using CassandraStorage();                
grunt> dump rows;
...
(sylvain,{(4,),(7,),(10,),(21,),(24,),(46,),(47,),(49,),(51,),(52,),(67,),(68,),(72,),(73,),(82,),(83,),(86,),(98,),(101,),(105,),(108,),(112,),(114,),(124,),(125,),(136,),(139,),(145,),(150,),(151,),(153,),(165,),(167,),(171,),(178,),(182,),(202,),(211,),(212,),(215,),(226,),(237,),(242,),(243,),(255,),(261,),(273,),(282,),(300,),(307,),(308,),(311,),(312,),(313,),(316,),(317,),(332,),(337,),(338,),(348,),(355,),(360,),(361,),(373,),(375,),(377,),(384,),(401,),(404,),(412,),(418,),(429,),(436,),(441,),(451,),(453,),(461,),(473,),(478,),(483,),(485,),(486,),(489,),(509,),(511,),(516,),(517,),(521,),(536,),(541,),(543,),(545,),(550,),(583,),(587,),(592,),(611,),(613,),(622,),(625,),(627,),(633,),(648,),(649,),(651,),(659,),(665,),(668,),(670,),(672,),(679,),(688,),(692,),(700,),(703,),(707,),(709,),(730,),(731,),(738,),(740,),(744,),(750,),(759,),(764,),(766,),(768,),(774,),(776,),(778,),(779,),(788,),(795,),(796,),(813,),(821,),(825,),(830,),(831,),(835,),(843,),(846,),(847,),(848,),(851,),(862,),(863,),(872,),(878,),(881,),(883,),(884,),(888,),(905,),(906,),(916,),(921,),(926,),(928,),(944,),(946,),(947,),(952,),(954,),(972,),(973,),(974,),(976,),(978,),(982,),(991,)})
(brandon,{(6,),(7,),(14,),(15,),(25,),(36,),(37,),(38,),(46,),(53,),(57,),(65,),(74,),(75,),(84,),(91,),(104,),(120,),(128,),(137,),(148,),(159,),(171,),(174,),(176,),(179,),(183,),(192,),(195,),(201,),(205,),(210,),(216,),(222,),(223,),(243,),(255,),(264,),(271,),(287,),(290,),(308,),(309,),(326,),(343,),(347,),(356,),(359,),(360,),(363,),(367,),(368,),(378,),(398,),(400,),(402,),(410,),(412,),(419,),(427,),(429,),(447,),(449,),(462,),(464,),(468,),(470,),(472,),(480,),(482,),(506,),(511,),(520,),(521,),(522,),(524,),(535,),(548,),(553,),(565,),(569,),(571,),(573,),(575,),(583,),(584,),(595,),(597,),(606,),(608,),(634,),(646,),(650,),(654,),(667,),(673,),(677,),(686,),(690,),(692,),(713,),(715,),(721,),(723,),(736,),(737,),(752,),(753,),(758,),(759,),(764,),(766,),(767,),(776,),(778,),(786,),(812,),(816,),(818,),(823,),(826,),(832,),(838,),(842,),(860,),(873,),(879,),(918,),(919,),(935,),(941,),(942,),(948,),(956,),(961,),(966,),(973,),(974,),(977,),(979,),(983,),(984,),(986,),(995,),(997,)})
(jake,{(1,),(7,),(10,),(14,),(29,),(52,),(54,),(65,),(67,),(78,),(82,),(83,),(89,),(97,),(100,),(115,),(126,),(140,),(141,),(145,),(214,),(221,),(230,),(231,),(232,),(241,),(245,),(247,),(265,),(266,),(269,),(271,),(282,),(286,),(288,),(299,),(316,),(323,),(331,),(332,),(335,),(338,),(348,),(353,),(355,),(364,),(367,),(371,),(379,),(398,),(409,),(420,),(428,),(429,),(439,),(443,),(450,),(454,),(467,),(477,),(482,),(488,),(490,),(502,),(503,),(512,),(520,),(521,),(535,),(536,),(541,),(548,),(552,),(557,),(560,),(596,),(600,),(604,),(606,),(611,),(613,),(621,),(624,),(630,),(635,),(641,),(647,),(655,),(660,),(665,),(674,),(676,),(690,),(693,),(694,),(704,),(719,),(720,),(724,),(731,),(749,),(751,),(763,),(765,),(767,),(771,),(779,),(782,),(784,),(789,),(793,),(797,),(798,),(801,),(802,),(806,),(820,),(825,),(839,),(845,),(848,),(856,),(865,),(866,),(867,),(870,),(876,),(887,),(891,),(901,),(905,),(908,),(922,),(929,),(944,),(960,),(964,),(980,),(988,),(996,)})
(eric,{(14,),(17,),(23,),(25,),(26,),(34,),(42,),(43,),(57,),(64,),(68,),(80,),(88,),(93,),(100,),(114,),(131,),(132,),(134,),(143,),(146,),(147,),(156,),(157,),(170,),(171,),(172,),(177,),(186,),(197,),(198,),(206,),(209,),(223,),(224,),(233,),(236,),(241,),(251,),(252,),(255,),(263,),(266,),(267,),(268,),(272,),(277,),(280,),(289,),(293,),(294,),(297,),(301,),(306,),(310,),(312,),(321,),(326,),(333,),(334,),(335,),(345,),(357,),(362,),(363,),(370,),(380,),(389,),(392,),(393,),(401,),(420,),(431,),(462,),(464,),(465,),(471,),(484,),(486,),(490,),(493,),(504,),(505,),(509,),(515,),(521,),(534,),(538,),(547,),(554,),(557,),(561,),(564,),(572,),(573,),(578,),(582,),(584,),(590,),(598,),(599,),(603,),(605,),(609,),(618,),(634,),(636,),(639,),(648,),(656,),(661,),(667,),(671,),(674,),(675,),(687,),(713,),(721,),(733,),(736,),(763,),(767,),(776,),(785,),(787,),(809,),(813,),(826,),(829,),(830,),(832,),(840,),(841,),(844,),(846,),(854,),(855,),(876,),(890,),(892,),(902,),(910,),(930,),(934,),(938,),(940,),(943,),(955,),(959,),(965,),(966,),(968,),(972,),(980,),(985,),(989,)})
(jonathan,{(17,),(18,),(31,),(34,),(37,),(40,),(67,),(69,),(75,),(93,),(111,),(124,),(127,),(128,),(137,),(142,),(168,),(178,),(190,),(193,),(194,),(207,),(211,),(216,),(221,),(229,),(237,),(242,),(252,),(253,),(264,),(265,),(267,),(270,),(272,),(274,),(276,),(278,),(280,),(283,),(297,),(299,),(300,),(302,),(303,),(309,),(311,),(318,),(323,),(329,),(330,),(332,),(344,),(346,),(351,),(354,),(358,),(361,),(363,),(366,),(367,),(374,),(378,),(379,),(386,),(389,),(392,),(395,),(398,),(404,),(424,),(426,),(429,),(434,),(439,),(443,),(445,),(448,),(472,),(477,),(494,),(500,),(504,),(522,),(525,),(538,),(539,),(541,),(548,),(553,),(557,),(560,),(563,),(566,),(567,),(578,),(591,),(593,),(595,),(599,),(605,),(610,),(626,),(635,),(636,),(640,),(642,),(644,),(649,),(660,),(662,),(663,),(667,),(674,),(690,),(706,),(708,),(712,),(716,),(723,),(733,),(741,),(747,),(758,),(765,),(797,),(798,),(801,),(822,),(827,),(828,),(837,),(850,),(863,),(867,),(894,),(895,),(896,),(904,),(911,),(917,),(932,),(949,),(951,),(952,),(958,),(969,),(974,),(983,),(985,),(988,),(989,),(996,),(1000,)})
(gary,{(3,),(13,),(21,),(23,),(33,),(36,),(44,),(45,),(48,),(62,),(65,),(68,),(75,),(80,),(81,),(90,),(111,),(113,),(119,),(123,),(137,),(149,),(152,),(153,),(157,),(161,),(166,),(178,),(179,),(180,),(183,),(184,),(188,),(189,),(191,),(197,),(199,),(200,),(204,),(212,),(221,),(229,),(239,),(265,),(270,),(272,),(276,),(279,),(282,),(295,),(296,),(304,),(305,),(314,),(326,),(329,),(335,),(342,),(345,),(346,),(362,),(370,),(371,),(375,),(380,),(382,),(387,),(389,),(390,),(393,),(399,),(403,),(406,),(414,),(417,),(424,),(428,),(445,),(458,),(462,),(486,),(490,),(492,),(495,),(499,),(500,),(507,),(514,),(520,),(542,),(550,),(551,),(570,),(571,),(572,),(574,),(577,),(588,),(604,),(614,),(619,),(626,),(634,),(640,),(648,),(659,),(663,),(684,),(687,),(690,),(694,),(715,),(741,),(750,),(765,),(772,),(776,),(781,),(782,),(783,),(785,),(789,),(802,),(806,),(812,),(816,),(820,),(829,),(836,),(843,),(850,),(855,),(868,),(873,),(875,),(889,),(900,),(904,),(922,),(928,),(929,),(935,),(946,),(949,),(954,),(956,),(959,),(960,),(962,),(992,)})
{code}

{code}
grunt> SET cassandra.range.batch.size 1                                                
grunt> rows = LOAD 'cassandra://PigDemo/Scores?widerows=true' using CassandraStorage();
grunt> dump rows;
...
(jonathan,{(17,)})
(sylvain,{(4,)})
{code}

When I try with set batch size to something higher than 1 and it works fine.",,alexliu68,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/13 21:53;alexliu68;6114.txt;https://issues.apache.org/jira/secure/attachment/12605984/6114.txt",,,,,,,,,,,,,,,,,,,,1.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,351154,,,Sun Oct 20 01:10:35 UTC 2013,,,,,,,,,,"0|i1ojnj:",351446,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"30/Sep/13 21:35;alexliu68;if widerows=true, then it uses get_paged_slice which set the page size(number of columns per page) to {code} cassandra.range.batch.size {code}, so it's not good to make it to one. For efficiency, it needs to be at least 100 which is the default number of columns per page.

We shouldn't use it combining with widerows=true. it should be at least 100.

The reason why it ends with one column is that at ColumnFamilyRecordReader
{code}
                if (wideColumns.hasNext() && wideColumns.peek().right.keySet().iterator().next().equals(lastColumn))
                    wideColumns.next();
                if (!wideColumns.hasNext())
                    rows = null;
{code}

if batch size set to 1, then wideColumns returns 1 column, which then end the iterator.

I will throw exception if the widerows=true and batch size set to 1;;;","20/Oct/13 01:10;jbellis;I think batchSize==1 is broken for non-wide-rows as well.  Updated to throw InvalidArgument for either and committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memtable flushing should write any columns shadowed by partition/range tombstones if any 2i are present,CASSANDRA-6112,12671288,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,29/Sep/13 22:14,16/Apr/19 09:32,14/Jul/23 05:53,29/Sep/13 23:21,1.2.11,2.0.2,,,,,0,,,,,"We shouldn't be dropping any columns obsoleted by partition and/or range tombstones in case the table has secondary indexes, or else the stale entries wouldn't be cleaned up during compaction, and will only be dropped during 2i query read-repair, if that happens.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Sep/13 22:15;aleksey;6112.txt;https://issues.apache.org/jira/secure/attachment/12605830/6112.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350994,,,Sun Sep 29 23:30:25 UTC 2013,,,,,,,,,,"0|i1oinr:",351285,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"29/Sep/13 23:10;jbellis;+1;;;","29/Sep/13 23:21;aleksey;Committed, thanks.;;;","29/Sep/13 23:30;aleksey;Backported to 1.2.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 Batch statement memory leak,CASSANDRA-6107,12670953,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lyubent,cowardlydragon,cowardlydragon,27/Sep/13 15:18,16/Apr/19 09:32,14/Jul/23 05:53,06/Oct/13 16:29,1.2.11,,,Legacy/CQL,,,0,,,,,"We are doing large volume insert/update tests on a CASS via CQL3. 


Using 4GB heap, after roughly 750,000 updates create/update 75,000 row keys, we run out of heap, and it never dissipates, and we begin getting this infamous error which many people seem to be encountering:

WARN [ScheduledTasks:1] 2013-09-26 16:17:10,752 GCInspector.java (line 142) Heap is 0.9383457210434385 full.  You may need to reduce memtable and/or cache sizes.  Cassandra will now flush up to the two largest memtables to free up memory.  Adjust flush_largest_memtables_at threshold in cassandra.yaml if you don't want Cassandra to do this automatically
 INFO [ScheduledTasks:1] 2013-09-26 16:17:10,753 StorageService.java (line 3614) Unable to reduce heap usage since there are no dirty column families


8 and 12 GB heaps appear to delay the problem by roughly proportionate amounts of 75,000 - 100,000 rowkeys per 4GB. Each run of 50,000 row key creations sees the heap grow and never shrink again. 

We have attempted to no effect:
- removing all secondary indexes to see if that alleviates overuse of bloom filters 
- adjusted parameters for compaction throughput
- adjusted memtable flush thresholds and other parameters 

By examining heapdumps, it seems apparent that the problem is perpetual retention of CQL3 BATCH statements. We have even tried dropping the keyspaces after the updates and the CQL3 statement are still visible in the heapdump, and after many many many CMS GC runs. G1 also showed this issue.

The 750,000 statements are broken into batches of roughly 200 statements.","- CASS version: 1.2.8 or 2.0.1, same issue seen in both
- Running on OSX MacbookPro
- Sun JVM 1.7
- Single local cassandra node
- both CMS and G1 GC used
- we are using the cass-JDBC driver to submit our batches

",aleksey,cowardlydragon,lyubent,slebresne,tmatwood@pentucketcapital.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6293,,,,,"04/Oct/13 19:18;jbellis;6107-v4.txt;https://issues.apache.org/jira/secure/attachment/12606863/6107-v4.txt","02/Oct/13 23:30;lyubent;6107.patch;https://issues.apache.org/jira/secure/attachment/12606480/6107.patch","03/Oct/13 14:39;lyubent;6107_v2.patch;https://issues.apache.org/jira/secure/attachment/12606584/6107_v2.patch","04/Oct/13 15:15;lyubent;6107_v3.patch;https://issues.apache.org/jira/secure/attachment/12606805/6107_v3.patch","03/Oct/13 15:01;lyubent;Screen Shot 2013-10-03 at 17.59.37.png;https://issues.apache.org/jira/secure/attachment/12606586/Screen+Shot+2013-10-03+at+17.59.37.png",,,,,,,,,,,,,,,,5.0,lyubent,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350782,,,Thu Jan 30 16:14:44 UTC 2014,,,,,,,,,,"0|i1ohcv:",351073,1.2.10,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"27/Sep/13 15:22;cowardlydragon;- Further examination of several point-in-time heap dumps show that ALL cql statement batches are retained in the heap. Each statement has multiple collections such as ConcurrentHashMap and other data structures which will obviously consume huge amounts of resources.

- We have run a smaller run that does NOT batch our updates. It is obviously much slower, but the heap dumps show over time objects being garbage collected propertly.

;;;","27/Sep/13 15:58;cowardlydragon;It appears that since we are sending preparedStatements (this allows us to prep the statement and then set the consistency level), that the preparedStatements are never evicted from the prepared statement cache in org.apache.cassandra.cql3.QueryProcessor

There are no removes ever done to preparedStatements or thriftPreparedStatements...

This may technically be our fault for preparing every single batch statement, but shouldn't there be a limit on stored prep statements with LRU eviction?;;;","27/Sep/13 16:08;slebresne;bq. but shouldn't there be a limit on stored prep statements with LRU eviction?

There is (from QueryProcessor.java):
{noformat}
    public static final int MAX_CACHE_PREPARED = 100000; // Enough to keep buggy clients from OOM'ing us
    private static final Map<MD5Digest, CQLStatement> preparedStatements = new ConcurrentLinkedHashMap.Builder<MD5Digest, CQLStatement>()
                                                                               .maximumWeightedCapacity(MAX_CACHE_PREPARED)
                                                                               .build();
{noformat}
but it's possible that this limit was too high to prevent you from OOM'ing in that case. And maybe that hard-coded is too high

But really, you should not prepare every single statement, it is a client error.;;;","27/Sep/13 16:10;cowardlydragon;It appears you are using a MAX_CACHE_PREPARED of 100,000, and COncurrentLinkedHashMap should use that as an evictor.

If the individual keys for 200 line batch statements are large (say, 10k, which I think based on the heap dump they consist of 1 map per statement in the batch possibly, so that is easily possible). So 100000 x 100000 bytes per statement = 10 gigabytes... uhoh. 

I think 600,000 updates, which are 3000 batches of 200 statements each popped the heap for a 4GB. I figure 1 GB of that heap is used for filters/sstables/memtables/etc, so 3000 batches popped 3GB of heap, so a megabyte per batch.

Can we expose the MAX_CACHE_PREPARED as a config parameter?;;;","27/Sep/13 16:14;cowardlydragon;I agree, there is client dysfunction here... we're going to stop prepping the statements, if possible (I think the cass jdbc project may have required prepping to set the consistency level, which sucks, but let me verify).
;;;","27/Sep/13 16:22;cowardlydragon;We're using SpringJDBC on top of the cass-jdbc driver. In order to intercept the update and specify consistency, that is only convenient with a PreparedStatementCreator...

so we will not use SpringJDBC/PreparedStatementCreator and instead do a more manual JDBC call...

sorry for the ""critical"" spam...;;;","27/Sep/13 16:30;jbellis;Could we fix the OOM by adding a weight to the Map entry instead of assuming all entries are equal?;;;","27/Sep/13 16:48;slebresne;Given that prepared is not performance sensitive, I suppose we could even use jmeter to get the precise in-memory size, and then cap the prepared statements to some percentage of the heap.;;;","27/Sep/13 16:53;jbellis;That sounds reasonable.

I'd actually just pin it as something pretty small like 1MB; with CASSANDRA-4693 we shouldn't need to prepare monstrous batches.;;;","27/Sep/13 17:00;cowardlydragon;Yep, removing statement preparation looks good! Heap is GC'ing, and multiple runs can be done.;;;","02/Oct/13 23:29;lyubent;Added a MemoryMeter to QueryProcessor#getStatement to track the size of each ParsedStatement that is Prepared. If the statement is more than 1MB (1048576 bytes) than an IllegalArgumentsException is thrown. Also added the size of the prepared statement in the tracing line inside of QueryProcessor#getStatement. ;;;","02/Oct/13 23:42;aleksey;I don't think the issue here is (just) large individual prepared statements. It's the total size that all the prepared statements are occupying. That's what should be tracked and limited, not just the individual ones.;;;","02/Oct/13 23:59;jbellis;Right.  Use the size you're calculating as the weight in the cache Map.;;;","03/Oct/13 14:39;lyubent;Changed the MemoryMeter to measure the full map rather than enforcing restrictions on individual prepared statements. The hardcoded maximum is 100MB for the thrift cache and 100MB for the CQL cache. 

;;;","03/Oct/13 15:01;jbellis;Use the cache weigher/weightedCapacity api instead of re-measuring the entire cache each time.  then the cache will take care of evicting old ones to make room as needed.

suggest making the capacity 1/256 of heap size.

should probably have a separate setting for maximum single statement size.  if a single statement is under this threshold but larger than the cache, execute it but do not cache it.

finally, statementid size should be negligible, i'd leave that out.
;;;","03/Oct/13 15:01;lyubent;Memory Usage graph for batches of 'insert' statements with varying numbers of columns.  ;;;","04/Oct/13 15:15;lyubent;Set the total cache for the statements to 1/256 of the heap (for thrift and cql), set the individual statement limit to 1MB. If a statement is less than 1MB but too large for the cache, it's executed but not stored in the cache.;;;","04/Oct/13 19:18;jbellis;On second thought, rejecting really huge statements should be done at the protocol level.  I'll follow up with Sylvain to see if we're already doing that.

v4 attached that just does the weighing as discussed.  WDYT?;;;","06/Oct/13 10:18;lyubent;LGTM. But i was able to build some pretty big batch statements ( >4MB ) so I'm not sure about the rejection of large statements at protocol level.;;;","06/Oct/13 15:32;jbellis;bq. I'm not sure about the rejection of large statements at protocol level.

I think that's what CASSANDRA-5981 is open for, actually.;;;","06/Oct/13 16:29;jbellis;commented;;;","07/Oct/13 07:54;slebresne;CASSANDRA-5981 is indeed about limiting the size at the protocol level. However it's a global frame limitation. In particular this is the hard limit for queries with their values and for that reason the current hard-coded limit is relatively high (256MB). And we can bikeshed on the exact default to user and CASSANDRA-5981 will probably allow the user to play with that limit, but in any case, it will definitively have to be higher than the 1MB. The other detail is that the limit done by CASSANDRA-5981 is on the sent bytes, not the in-memory size of the query, but that probably don't matter much.

Anyway, provided that a prepared statement doesn't include values, it wouldn't be absurd to have a specific, lower limit on their size. Though my own preference would be to just leave it to a global limit on the preparedStatements cache map (but it could make sense to reject statements that blow up the entire limit on their own, so as to make sure to respect it). Too many hard-coded limitations make me nervous.;;;","03/Nov/13 16:48;mikeoz;This change appears to break code that uses EmbeddedCassandraService and PreparedStatements in version 1.2.11.  Please see CASSANDRA-6293 for details.;;;","30/Jan/14 16:14;jbellis;Note: this was reverted in 1.2.14 because of CASSANDRA-6592.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add additional limits in cassandra.conf provided by Debian package,CASSANDRA-6104,12670767,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,jblangston@datastax.com,jblangston@datastax.com,26/Sep/13 16:39,16/Apr/19 09:32,14/Jul/23 05:53,19/Mar/14 14:06,1.2.16,2.0.7,2.1 beta2,Packaging,,,0,,,,,"/etc/security/limits.d/cassandra.conf distributed with DSC deb/rpm packages should contain additional settings. We have found these limits to be necessary for some customers through various support tickets.

{code}
cassandra - memlock  unlimited
cassandra - nofile  100000
cassandra - nproc 32768
cassandra - as unlimited
{code}",,jblangston@datastax.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350596,,,Wed Mar 19 14:06:46 UTC 2014,,,,,,,,,,"0|i1og87:",350889,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"26/Sep/13 16:54;brandon.williams;We already have these two:

{noformat}
cassandra  -  memlock  unlimited
cassandra  -  nofile   100000
{noformat}

32k threads seems a little overboard.;;;","19/Mar/14 14:06;brandon.williams;Committed the address space change as unlimited, and nproc at the reasonable bound of 8096, since going beyond that would suggest that either you need HSHA, or mask a bug on our side.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException in TokenMetadata.cloneOnlyTokenMap,CASSANDRA-6103,12670748,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,mikeschrag,mikeschrag,26/Sep/13 15:32,16/Apr/19 09:32,14/Jul/23 05:53,30/Sep/13 18:56,1.2.11,2.0.2,,,,,0,,,,,"This isn't reproducible for me, but it happened to one of the servers in our cluster while starting up. It went away on a restart, but I figured it was worth filing anyway:

ERROR [main] 2013-09-26 08:04:02,478 CassandraDaemon.java (line 464) Exception encountered during startup
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
        at java.util.HashMap$EntryIterator.next(HashMap.java:834)
        at java.util.HashMap$EntryIterator.next(HashMap.java:832)
        at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:294)
        at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:286)
        at com.google.common.collect.AbstractBiMap.putAll(AbstractBiMap.java:160)
        at com.google.common.collect.HashBiMap.putAll(HashBiMap.java:42)
        at com.google.common.collect.HashBiMap.create(HashBiMap.java:72)
        at org.apache.cassandra.locator.TokenMetadata.cloneOnlyTokenMap(TokenMetadata.java:561)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getAddressRanges(AbstractReplicationStrategy.java:192)
        at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1711)
        at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1692)
        at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1461)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1228)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:949)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1116)
        at org.apache.cassandra.service.StorageService.setTokens(StorageService.java:214)
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:802)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)",,djatnieks,mikeschrag,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/13 03:44;mishail;cassandra-1.2-6103.patch;https://issues.apache.org/jira/secure/attachment/12605853/cassandra-1.2-6103.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350577,,,Mon Sep 30 18:56:07 UTC 2013,,,,,,,,,,"0|i1og3z:",350870,1.2.6,1.2.8,,,,,,,jbellis,,jbellis,Low,,1.2.0,,,,,,,,,,,,,,,,"26/Sep/13 16:09;brandon.williams;Hmm, it looks like endpointToHostIdMap is being mutated, but it's not immediately clear how since the gossiper is blocked here and can't notify anything else, not to mention all the locking in TMD.;;;","26/Sep/13 18:14;mikeschrag;Just to provide as much info as I can, the entire cluster was stopped. I then ran a script to start all of them back up at the same time. This particular cluster is 44 nodes. All 43 other nodes started fine. This one node died at startup with this exception. After a couple minutes I restarted the one node, and it came up fine. Priority ""Major"" is maybe a little aggressive for this one :);;;","26/Sep/13 19:16;mishail;Off the top of my head {{org.apache.cassandra.locator.TokenMetadata.updateHostId(UUID, InetAddress)}} calls {{endpointToHostIdMap.forcePut(endpoint, hostId);}} but doesn't acquire a write lock before that.  ;;;","28/Sep/13 02:08;djatnieks;I've run into the same error a couple times with a slightly different stack trace with DSE 3.1.3 (C* 1.2.6 I believe).

+1 on {{updateHostId}} as a likely source from my non-expert look at the code.

{noformat}
Exception encountered during startup
java.util.ConcurrentModificationException
at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
at java.util.HashMap$EntryIterator.next(HashMap.java:834)
at java.util.HashMap$EntryIterator.next(HashMap.java:832)
at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:294)
at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:286)
at com.google.common.collect.AbstractBiMap.putAll(AbstractBiMap.java:160)
at com.google.common.collect.HashBiMap.putAll(HashBiMap.java:42)
at com.google.common.collect.HashBiMap.create(HashBiMap.java:72)
at org.apache.cassandra.locator.TokenMetadata.cloneOnlyTokenMap(TokenMetadata.java:561)
at org.apache.cassandra.locator.TokenMetadata.cloneAfterAllLeft(TokenMetadata.java:582)
at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1708)
at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1686)
at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:1306)
at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1207)
at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:935)
at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1102)
at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:911)
at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:699)
at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554)
at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)
at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:342)
at com.datastax.bdp.server.DseDaemon.setup(DseDaemon.java:137)
at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:441)
at com.datastax.bdp.server.DseDaemon.main(DseDaemon.java:334)
{noformat}
;;;","28/Sep/13 03:07;mishail;I'm curious, each access to {{tokenToEndpointMap}} is guarded by either read or write lock, on other hand {{endpointToHostIdMap}} is accessed without any lock in most cases. Is it by design?;;;","30/Sep/13 03:44;mishail;Patch for 1.2. 
* Use read lock to read endpoints' collections, and return defensive copies  for them. 
* Use write lock in {{updateHostId}};;;","30/Sep/13 18:56;jbellis;LGTM; committed.  Thanks, Mikhail!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage broken for bigints and ints,CASSANDRA-6102,12670654,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,jalkanen,jalkanen,26/Sep/13 08:07,16/Apr/19 09:32,14/Jul/23 05:53,12/Oct/13 16:51,1.2.11,,,,,,0,,,,,"I am seeing something rather strange in the way Cass 1.2 + Pig seem to handle integer values.

Setup: Cassandra 1.2.10, OSX 10.8, JDK 1.7u40, Pig 0.11.1.  Single node for testing this. 

First a table:

{noformat}
> CREATE TABLE testc (
 key text PRIMARY KEY,
 ivalue int,
 svalue text,
 value bigint
) WITH COMPACT STORAGE;

> insert into testc (key,ivalue,svalue,value) values ('foo',10,'bar',65);
> select * from testc;

key | ivalue | svalue | value
-----+--------+--------+-------
foo |     10 |    bar |     65
{noformat}

For my Pig setup, I then use libraries from different C* versions to actually talk to my database (which stays on 1.2.10 all the time).

Cassandra 1.0.12 (using cassandra_storage.jar):

{noformat}
testc = LOAD 'cassandra://keyspace/testc' USING CassandraStorage();
dump testc
(foo,(svalue,bar),(ivalue,10),(value,65),{})
{noformat}

Cassandra 1.1.10:

{noformat}
testc = LOAD 'cassandra://keyspace/testc' USING CassandraStorage();
dump testc
(foo,(svalue,bar),(ivalue,10),(value,65),{})
{noformat}

Cassandra 1.2.10:

{noformat}
(testc = LOAD 'cassandra://keyspace/testc' USING CassandraStorage();
dump testc
foo,{(ivalue,
),(svalue,bar),(value,A)})
{noformat}


To me it appears that ints and bigints are interpreted as ascii values in cass 1.2.10.  Did something change for CassandraStorage, is there a regression, or am I doing something wrong?  Quick perusal of the JIRA didn't reveal anything that I could directly pin on this.

Note that using compact storage does not seem to affect the issue, though it obviously changes the resulting pig format.

In addition, trying to use Pygmalion 

{noformat}
tf = foreach testc generate key, flatten(FromCassandraBag('ivalue,svalue,value',columns)) as (ivalue:int,svalue:chararray,lvalue:long);
dump tf

(foo,
,bar,A)
{noformat}

So no help there. Explicitly casting the values to (long) or (int) just results in a ClassCastException.
","Cassandra 1.2.9 & 1.2.10, Pig 0.11.1, OSX 10.8.x",8forty@gmail.com,alexliu68,jalkanen,mishail,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Sep/13 05:52;alexliu68;6102-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12605644/6102-1.2-branch.txt","12/Oct/13 01:10;alexliu68;6102-2.0-branch.txt;https://issues.apache.org/jira/secure/attachment/12608123/6102-2.0-branch.txt","08/Oct/13 11:00;brandon.williams;6102-v2.txt;https://issues.apache.org/jira/secure/attachment/12607345/6102-v2.txt","09/Oct/13 02:56;alexliu68;6102-v3.txt;https://issues.apache.org/jira/secure/attachment/12607497/6102-v3.txt","11/Oct/13 19:45;alexliu68;6102-v4.txt;https://issues.apache.org/jira/secure/attachment/12608062/6102-v4.txt","11/Oct/13 19:53;alexliu68;6102-v5.txt;https://issues.apache.org/jira/secure/attachment/12608063/6102-v5.txt",,,,,,,,,,,,,,,6.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350483,,,Sat Oct 12 16:51:17 UTC 2013,,,,,,,,,,"0|i1ofj3:",350776,1.2.10,,,,,,,,brandon.williams,,brandon.williams,Normal,,1.2.9,,,,,,,,,,,,,,,,"26/Sep/13 15:38;brandon.williams;Hmm, ints, longs, and floats all work in the test_storage.pig script, but those are all populated by thrift.  We might have fixed this recently, but if not there's probably another bug with cql detection in ACS.;;;","26/Sep/13 18:09;alexliu68;[~jalkanen] Can you try it with CqlStorage which should work? We recommend to use CqlStorage unless you have to use CassandraStorage. There is some issue with CassandraStorage which can't get the right validator type for the columns based on system tables. 

We may needs fall back to thrift api to get the metadata for COMPACT STORAGE cql table.;;;","26/Sep/13 18:46;alexliu68;I propose that we implement the following
{code}
CqlStorage supports all kind of tables/column families including old thrift column families, 
new Cql tables with/without Compact storage. (this is already done)

CassandraStorage supports only old thrift column families PLUS Cql tables with Compact storage. 
It DOES NOT support other Cql tables. (I am changing code for this)
{code}

Any objection/thought?;;;","26/Sep/13 19:06;brandon.williams;I'm mostly ok with it, except I really want to get away from the cli for CASSANDRA-5709, and long-term I think we need to.  On the other hand, if we had CASSANDRA-5695 we could just get rid of those tests, so I'm still undecided.  Can you explain what the problem is here?;;;","26/Sep/13 21:57;alexliu68;Let me double check it, I may fix the issue.;;;","28/Sep/13 05:22;alexliu68;For regular Cql table e.g.
{code}
CREATE TABLE testm (
  m text,
  n text,
  o text,
  p text,
  q text,
  r text,
  PRIMARY KEY (m, n, o)
)

cqlsh:test> select * from testm;

 m  | n  | o  | p  | q  | r
----+----+----+----+----+----
  m |  n |  o |  p |  q |  r
 m1 | n1 | o1 | p1 | q1 | r1
{code}

the schema and result is 

{code}
(m,{((n,o,),),((n,o,p),p),((n,o,q),q),((n,o,r),r)})
(m1,{((n1,o1,),),((n1,o1,p),p1),((n1,o1,q),q1),((n1,o1,r),r1)})
grunt> describe test3;
test3: {key: chararray,columns: {(name: (),value: bytearray)}}
{code}

System.schema_columns

{code}
qlsh:test> select * from system.schema_columns where keyspace_name='test' and columnfamily_name='testm';  

 keyspace_name | columnfamily_name | column_name | component_index | index_name | index_options | index_type | validator
---------------+-------------------+-------------+-----------------+------------+---------------+------------+------------------------------------------
          test |             testm |           p |               2 |       null |          null |       null | org.apache.cassandra.db.marshal.UTF8Type
          test |             testm |           q |               2 |       null |          null |       null | org.apache.cassandra.db.marshal.UTF8Type
          test |             testm |           r |               2 |       null |          null |       null | org.apache.cassandra.db.marshal.UTF8Type
{code}

Because the column is composite, so we need use the last component of the composite columns to find the validator
;;;","28/Sep/13 05:23;alexliu68;For compact storage cql table, we just need treat it as thrift table.;;;","28/Sep/13 05:42;jalkanen;What is the patch against? Trying it on both trunk and cassandra-1.2 branches; on cassandra-1.2 it fails because of

    [javac] /Users/jalkanen/Eclipse/cassandra/src/java/org/apache/cassandra/hadoop/pig/CqlStorage.java:57: error: CqlStorage is not abstract and does not override abstract method getColumnMetadata(Client) in AbstractCassandraStorage

;;;","28/Sep/13 06:01;jalkanen;Confirmed working; result for the test case above is now (Pig 0.11.1 and patched 1.2 latest)

grunt> testc = LOAD 'cassandra://keyspace/testc' USING CassandraStorage();
grunt> dump testc;
(foo,(ivalue,10),(svalue,bar),(value,65),{})

Thank you!;;;","07/Oct/13 20:32;brandon.williams;This part:

{noformat}
+        // Don't want to create another TBase class, so use CfDef.populate_io_cache_on_flush 
+        // to store flag of compact storage cql table.
+        if (cql3Table && !(parseType(cfDef.comparator_type) instanceof AbstractCompositeType))
+            cfDef.setPopulate_io_cache_on_flush(true);
+        
+        // Don't want to create another TBase class, so use CfDef.replicate_on_write 
+        // to store flag of cql table.
+        if (cql3Table)
+            cfDef.setReplicate_on_write(true); 
{noformat}

Feels like a hack that is going to bite us down the road when those options really do get removed.;;;","07/Oct/13 21:25;alexliu68;Since thrift structs do not support inheritance, that is, a struct may not extend other structs, so the choice is to create a new struct CfInfo which has the following properties

{code}
struct CfInfo {
    1: required string keyspace,
    2: required string name,
    3: optional string column_type=""Standard"",
    4: optional string comparator_type=""BytesType"",
    5: optional string subcomparator_type,
    6: optional list<ColumnDef> column_metadata,
    7: optional string default_validation_class,
    8: optional string key_validation_class,
    9: optional boolean compact_cql_table,
    10: optional boolean cql3_table
}
{code}

CfInfo is a new struct, so it doesn't affect the other existing code.

or add two additional properties to CfDef
{code}
     39: optional boolean compact_cql_table,
    40: optional boolean cql3_table
{code}
It adds additional properties only used by Pig, so a little overhead.


;;;","07/Oct/13 21:27;alexliu68;[~brandon.williams] which one is better?;;;","08/Oct/13 10:58;brandon.williams;What if we just tracked them locally outside of the thrift structs, like this?;;;","08/Oct/13 16:58;alexliu68;It needs be serialized so that it can be transferred to the task nodes. We use Thrift serialization so the class need be generated by thrift interface(thrift structs). If we change to other serialization mechanism, we don't need TBase class.;;;","08/Oct/13 17:10;brandon.williams;Ah, right.  Well, I'm open to other options so we don't chain ourselves to thrift more than we have to, but between modifying CfDef just for this or creating CfInfo, I'd prefer CfInfo.;;;","09/Oct/13 02:25;alexliu68;A custom serialization format is implemented as followings

{code}
<compact><cql3table><CfDef>
{code}
e.g. 11<CfDef> as compact cql3 table
01<CfDef> as none compact cql3 table
where 1 as true, 0 as false.;;;","09/Oct/13 02:58;alexliu68;6102-v3.txt patch is attached which implemnts the new custom serialization format.;;;","11/Oct/13 19:46;alexliu68;6102-v4.txt patch is attached which reverts back the UUIDType mappings;;;","11/Oct/13 20:36;brandon.williams;Committed.;;;","11/Oct/13 22:44;brandon.williams;Can you post a version against 2.0 or trunk?  git is giving me all kinds of problems trying to merge this.;;;","11/Oct/13 23:41;alexliu68;6102-2.0-branch.txt patch is on top of cassandra-2.0 branch

;;;","12/Oct/13 05:08;mishail;It looks like https://github.com/apache/cassandra/commit/bc8e2475fa71f4bbbf95d4294d78b96a1aa1211c broke the trunk
{noformat}
    [javac] C:\Users\mishail\workspace\cassandra\src\java\org\apache\cassandra\hadoop\pig\AbstractCassandraStorage.java:135: error: variable validators is already defined in method columnToTuple(IColumn,AbstractCassandraStorage.CfInfo,AbstractType)
    [javac]             Map<ByteBuffer,AbstractType> validators = getValidatorMap(cfDef);
    [javac]                                          ^
    [javac] C:\Users\mishail\workspace\cassandra\src\java\org\apache\cassandra\hadoop\pig\AbstractCassandraStorage.java:157: error: cannot find symbol
    [javac]             for (IColumn subcol : col.getSubColumns())
    [javac]                  ^
    [javac]   symbol:   class IColumn
    [javac]   location: class AbstractCassandraStorage
{noformat};;;","12/Oct/13 16:51;brandon.williams;Committed the 2.0 patch and merged it up.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Debian init script broken,CASSANDRA-6101,12670650,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,awinter,awinter,26/Sep/13 07:12,16/Apr/19 09:32,14/Jul/23 05:53,04/Oct/13 13:45,2.0.2,,,,,,0,,,,,"The debian init script released in 2.0.1 contains 2 issues:

# The pidfile directory is not created if it doesn't already exist.
# Classpath not exported to the start-stop-daemon.

These lead to the init script not picking up jna.jar, or anything from the debian EXTRA_CLASSPATH environment variable, and the init script not being able to stop/restart Cassandra.",,awinter,pieterc,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6239,,,CASSANDRA-6240,,,,,,,,,,,,,,,,,"30/Sep/13 16:13;urandom;6101-classpath.patch;https://issues.apache.org/jira/secure/attachment/12605920/6101-classpath.patch","26/Sep/13 07:14;awinter;6101.txt;https://issues.apache.org/jira/secure/attachment/12605201/6101.txt",,,,,,,,,,,,,,,,,,,2.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350479,,,Fri Oct 04 13:45:39 UTC 2013,,,,,,,,,,"0|i1ofi7:",350772,,,,,,,,,,,,Low,,2.0.1,,,,,,,,,,,,,,,,"26/Sep/13 07:14;awinter;Attached patch with fixes.;;;","30/Sep/13 16:18;urandom;Good catch Anton.

I've applied the fix to ensure that the PID directory is created, but I think we can go a bit further with the classpath.

That function is a throwback to {{jsvc}} and duplicates the classpath construction we do elsewhere.  I've attached [6101-classpath.patch|https://issues.apache.org/jira/secure/attachment/12605920/6101-classpath.patch] which eliminates that function and uses {{EXTRA_CLASSPATH}} instead.

Can you give this a whirl and see if it (still) fixes the issues you were seeing?;;;","03/Oct/13 05:28;awinter;Yes, that works as well.;;;","03/Oct/13 12:23;pieterc;Not really important, but status isn't working with this patch..

root ~ # service cassandra status
 * could not access pidfile for Cassandra;;;","03/Oct/13 14:49;urandom;bq. Yes, that works as well.

Thanks Anton; Committed;;;","03/Oct/13 15:00;urandom;bq. Not really important, but status isn't working with this patch..

I'm not seeing that [~pieterc]; Which patch are you referring to, [6101.txt|https://issues.apache.org/jira/secure/attachment/12605201/6101.txt] or [6101-classpatch.patch|https://issues.apache.org/jira/secure/attachment/12605920/6101-classpath.patch]?

I wouldn't be surprised at this point to find there are more bugs with this, but only [6101.txt|https://issues.apache.org/jira/secure/attachment/12605201/6101.txt] should have had any impact on this, and it is definitely a change for the better.

When you see this error, does a PID file exist at {{/var/run/cassandra/cassandra.pid}}?  If so, what are the contents of the file?  Is Cassandra running, and if so, what is its PID (hint: try {{pgrep -f CassandraDaemon}})?

Could you attach the output of {{sh -x /etc/init.d/cassandra status}}?;;;","04/Oct/13 01:56;awinter;That {{service cassandra status}} problem is resolved by CASSANDRA-6090;;;","04/Oct/13 13:45;urandom;bq. That service cassandra status problem is resolved by CASSANDRA-6090

I think so too; Closing this issue;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Pig Storage classes should use IOException instead of RuntimeException,CASSANDRA-6099,12670574,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,alexliu68,alexliu68,alexliu68,25/Sep/13 21:02,16/Apr/19 09:32,14/Jul/23 05:53,25/Sep/13 21:31,1.2.11,,,,,,0,,,,,"If there is anything wrong in the script, a RuntimeException will results the following scripts to error out even though the following scripts are correct.

e.g.
{code}
grunt> newtable1 = LOAD 'cql://test_not_exist/test' USING CqlStorage();
2013-09-25 14:33:38,916 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. InvalidRequestException(why:Keyspace 'test_not_exist' does not exist)
Details at logfile: /home/automaton/pig_1380119534487.log
grunt> newtable2 = LOAD 'cql://test_exist/test' USING CqlStorage(); 
2013-09-25 14:33:48,083 [main] ERROR org.apache.pig.tools.grunt.Grunt - ERROR 2999: Unexpected internal error. InvalidRequestException(why:Keyspace 'test_not_exist' does not exist)
Details at logfile: /home/automaton/pig_1380119534487.log
{code}
",,alexliu68,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/13 21:18;alexliu68;6099-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12605105/6099-1.2-branch.txt",,,,,,,,,,,,,,,,,,,,1.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350403,,,Wed Sep 25 21:31:16 UTC 2013,,,,,,,,,,"0|i1of1b:",350696,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"25/Sep/13 21:03;alexliu68;To fix it, we need switch all RuntimeException to IOException;;;","25/Sep/13 21:31;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException causing query timeout,CASSANDRA-6098,12670548,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,lexlythius,lexlythius,25/Sep/13 18:51,16/Apr/19 09:32,14/Jul/23 05:53,01/Oct/13 07:02,2.0.2,,,,,,0,,,,,"A common SELECT query could not be completed failing with.

{noformat}
Request did not complete within rpc_timeout.
{noformat}

output.log showed this:
{noformat}
ERROR 15:38:04,036 Exception in thread Thread[ReadStage:170,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1867)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.index.composites.CompositesIndexOnRegular.isStale(CompositesIndexOnRegular.java:97)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:247)
        at org.apache.cassandra.db.index.composites.CompositesSearcher$1.computeNext(CompositesSearcher.java:102)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.db.ColumnFamilyStore.filter(ColumnFamilyStore.java:1651)
        at org.apache.cassandra.db.index.composites.CompositesSearcher.search(CompositesSearcher.java:50)
        at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:525)
        at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1639)
        at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
        at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1358)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1863)
{noformat}","CQLSH 4.0.0
Cassandra 2.0.0
Oracle Java 1.7.0_40
Ubuntu 12.04.3 x64",lexlythius,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Sep/13 18:53;slebresne;6098.txt;https://issues.apache.org/jira/secure/attachment/12605949/6098.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350377,,,Tue Oct 01 07:02:23 UTC 2013,,,,,,,,,,"0|i1oevj:",350670,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"25/Sep/13 19:11;lexlythius;This happens only when querying for some particular values on a secondary index. Cannot reproduce it querying by primary key.

Neither of the following did any good:
{noformat}
Restarting Cassandra
nodetool rebuild_index DB TABLE INDEX
nodetool invalidaterowcache
nodetool invalidatekeycache
nodetool cleanup
nodetool repair
{noformat};;;","25/Sep/13 19:25;lexlythius;Dropping the secondary index and creating it again solved the problem at hand.
I still think the issue is worth looking into, though.
;;;","26/Sep/13 12:11;slebresne;Right, that's a legit issue. Attaching simple patch to fix.;;;","26/Sep/13 13:27;jbellis;Patch looks good, can you add a unit test?;;;","30/Sep/13 18:53;slebresne;Update patch with some unit testing inside.;;;","30/Sep/13 19:22;jbellis;+1;;;","01/Oct/13 07:02;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool repair randomly hangs.,CASSANDRA-6097,12670530,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,jblangston@datastax.com,jblangston@datastax.com,25/Sep/13 17:54,16/Apr/19 09:32,14/Jul/23 05:53,09/Oct/13 15:40,1.2.11,,,,,,0,,,,,"nodetool repair randomly hangs. This is not the same issue where repair hangs if a stream is disrupted. This can be reproduced on a single-node cluster where no streaming takes place, so I think this may be a JMX connection or timeout issue. Thread dumps show that nodetool is waiting on a JMX response and there are no repair-related threads running in Cassandra. Nodetool main thread waiting for JMX response:

{code}
""main"" prio=5 tid=7ffa4b001800 nid=0x10aedf000 in Object.wait() [10aede000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7f90d62e8> (a org.apache.cassandra.utils.SimpleCondition)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.cassandra.utils.SimpleCondition.await(SimpleCondition.java:34)
	- locked <7f90d62e8> (a org.apache.cassandra.utils.SimpleCondition)
	at org.apache.cassandra.tools.RepairRunner.repairAndWait(NodeProbe.java:976)
	at org.apache.cassandra.tools.NodeProbe.forceRepairAsync(NodeProbe.java:221)
	at org.apache.cassandra.tools.NodeCmd.optionalKSandCFs(NodeCmd.java:1444)
	at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1213)
{code}

When nodetool hangs, it does not print out the following message:

""Starting repair command #XX, repairing 1 ranges for keyspace XXX""

However, Cassandra logs that repair in system.log:

1380033480.95  INFO [Thread-154] 10:38:00,882 Starting repair command #X, repairing X ranges for keyspace XXX

This suggests that the repair command was received by Cassandra but the connection then failed and nodetool didn't receive a response.

Obviously, running repair on a single-node cluster is pointless but it's the easiest way to demonstrate this problem. The customer who reported this has also seen the issue on his real multi-node cluster.

Steps to reproduce:

Note: I reproduced this once on the official DataStax AMI with DSE 3.1.3 (Cassandra 1.2.6+patches).  I was unable to reproduce on my Mac using the same version, and subsequent attempts to reproduce it on the AMI were unsuccessful. The customer says he is able is able to reliably reproduce on his Mac using DSE 3.1.3 and occasionally reproduce it on his real cluster. 

1) Deploy an AMI using the DataStax AMI at https://aws.amazon.com/amis/datastax-auto-clustering-ami-2-2

2) Create a test keyspace
{code}
create keyspace test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
{code}
3) Run an endless loop that runs nodetool repair repeatedly:

{code}
while true; do nodetool repair -pr test; done
{code}

4) Wait until repair hangs. It may take many tries; the behavior is random.",DataStax AMI,cburroughs,jblangston@datastax.com,jeffpotter,mishail,PuerTea,slebresne,wadey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Oct/13 16:34;yukim;6097-1.2.txt;https://issues.apache.org/jira/secure/attachment/12606597/6097-1.2.txt","01/Oct/13 14:08;jblangston@datastax.com;dse.stack;https://issues.apache.org/jira/secure/attachment/12606104/dse.stack","01/Oct/13 14:08;jblangston@datastax.com;nodetool.stack;https://issues.apache.org/jira/secure/attachment/12606103/nodetool.stack",,,,,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350359,,,Sun Oct 13 17:04:37 UTC 2013,,,,,,,,,,"0|i1oerr:",350652,,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,,"25/Sep/13 20:39;brandon.williams;It took a few thousands of iterations to reproduce, but it did happen (on my own machine, not in AWS or using the AMI)  Netstat shows one connection from nodetool to the jmx port that stays connected, and another one that connects, gets disconnected after a short while, then the process repeats.  This sounds like jmx doing it's double connection dance, but whatever reason the server rejects the second one.  I'm not sure there's anything we can do about this except say what we already know: jmx kinda sucks.

One odd thing I did notice occasionally get printed straight to stdout/err while running the loop:
{noformat}
Sep 25, 2013 7:55:52 PM ServerCommunicatorAdmin reqIncoming
WARNING: The server has decided to close this client connection.
{noformat}

However it didn't correlate to the iteration that hung.;;;","29/Sep/13 21:07;jblangston@datastax.com;If I'm reading [this|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/tools/NodeProbe.java#L1036-L1039] correctly, the condition that nodetool repair is waiting on won't get signaled if the status returned to the NotificationListener is SESSION_FAILED. Could that explain why it's hanging?;;;","29/Sep/13 22:25;yukim;Repair should send back FINISHED at the end of the repair even if session failed. https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/StorageService.java#L2414;;;","30/Sep/13 23:08;jblangston@datastax.com;I think the ease with which this can be reproduced is dependent on the number of keyspaces.  I started up a stock 3.1.3 AMI in hadoop mode so that DSE would create the cfs/HiveMetaStore/dse_system keyspaces and also created an additional keyspace using the customer's schema. Now I am able to reproduce the issue very readily by running nodetool repair -pr in a loop.  I thought that it might have been something to do with having hadoop enabled, so i disabled it again, but I am still able to reproduce the issue.  On the other hand, if I give repair a specific keyspace name, it takes much longer to reproduce, if at all.;;;","01/Oct/13 00:33;mishail;[~jblangston@datastax.com] do you have a full thread dump for the problem?;;;","01/Oct/13 14:08;jblangston@datastax.com;Stack trace for nodetool and cassandra attached.;;;","01/Oct/13 17:10;mishail;I suspect it could be infamous hang at {{SocketInputStream.socketRead0}}. And the only way I know to work around is to limit {{sun.net.client.defaultReadTimeout}}
{code}

""ClientNotifForwarder-5"" daemon prio=10 tid=0x00007f133c2e4000 nid=0x7549 runnable [0x00007f13389de000]
   java.lang.Thread.State: RUNNABLE
	at java.net.SocketInputStream.socketRead0(Native Method)
	at java.net.SocketInputStream.read(SocketInputStream.java:129)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
	- locked <0x00000000ff562dc0> (a java.io.BufferedInputStream)
	at java.io.DataInputStream.readByte(DataInputStream.java:248)
	at sun.rmi.transport.StreamRemoteCall.executeCall(StreamRemoteCall.java:195)
	at sun.rmi.server.UnicastRef.invoke(UnicastRef.java:142)
	at com.sun.jmx.remote.internal.PRef.invoke(Unknown Source)
	at javax.management.remote.rmi.RMIConnectionImpl_Stub.fetchNotifications(Unknown Source)
	at javax.management.remote.rmi.RMIConnector$RMINotifClient.fetchNotifs(RMIConnector.java:1306)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.fetchNotifs(ClientNotifForwarder.java:554)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.doRun(ClientNotifForwarder.java:437)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$NotifFetcher.run(ClientNotifForwarder.java:418)
	at com.sun.jmx.remote.internal.ClientNotifForwarder$LinearExecutor$1.run(ClientNotifForwarder.java:88)
{code};;;","01/Oct/13 20:05;jblangston@datastax.com;The JMX documentation [states|http://www.oracle.com/technetwork/java/javase/tech/best-practices-jsp-136021.html#mozTocId387765] that notifications are not guaranteed to always be delivered.  The API only guarantees that a client either receives all notifications for which it is listening, or can discover that notifications may have been lost. A client can discover when notifications are lost by registering a listener using JMXConnector.addConnectionNotificationListener. It looks like nodetool isn't doing this last part. Seems like we should register a ConnectionNotificationListener and if a notification fails, signal the condition so that nodetool doesn't hang. Maybe have nodetool query for the status of the repair at that point via separate JMX call, or just print a warning that ""The status of the repair command can't be determined, please check the log."" or something like that.

I would disagree with prioritizing this as trivial. It's not critical but I have had many customers express frustration with the  nodetool repair's proclivity for hanging.  It makes automating repairs painful because they can't count on nodetool to ever return.;;;","03/Oct/13 16:34;yukim;JB is right. We need to handle cases where connection got lost.
Attaching patch to listen to JMXConnectionNotification.

There are 2 cases handled. One is for ""notification lost"". In this case, We cannot tell if repair finished, so log message to check server log and go on to next keyspace for repair. The other one is for ""connection closed"", and in this case we throw exception to stop further processing of repair.;;;","05/Oct/13 05:37;jbellis;Tagging Sylvain for review since he also reviewed CASSANDRA-4767.;;;","09/Oct/13 14:47;slebresne;I've tried reproducing but lost patience when it didn't reproduce after > 1K iterations, so can't say I've ""tested"" the fix, but the JMX documentation makes it pretty clear we should handle those notifications and that it's very likely the source of the problem here. A few minor nits on the patch though:
* Is there a reason JMXConnectionNotification.FAILED is not handled? Feels like it would be a good to catch it too just in case.
* May want to mention in the error message for CLOSED/FAILED that we're not starting repair for the other keyspaces not yet started.

But lgtm otherwise;;;","09/Oct/13 15:40;yukim;Thanks!
Committed with above nits fix.;;;","09/Oct/13 20:49;jblangston@datastax.com;Customer compiled Cassandra from git and ran the resulting nodetool against his DSE installation. He reported that the hang is still reproducible.  I haven't tried to duplicate this myself yet.;;;","13/Oct/13 16:52;yukim;I think the issue they are having is platform specific.
Try setting sun.net.client.defaultReadTimeout system property (-Dsun.net.client.defaultReadTimeout=<timeout in millisec>) as suggested by Mikhail above to avoid stuck on socket read.
(http://docs.oracle.com/javase/6/docs/technotes/guides/net/properties.html);;;","13/Oct/13 17:00;brandon.williams;After this patch, I could not reproduce the problem after 10K iterations.;;;","13/Oct/13 17:04;jbellis;(Unless this was already fine in 2.0, we should tag fixversion appropriately.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cant migrate json manifest with multiple data directories,CASSANDRA-6093,12670431,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,25/Sep/13 08:06,16/Apr/19 09:32,14/Jul/23 05:53,25/Sep/13 12:59,2.0.2,,,,,,0,,,,,"http://mail-archives.apache.org/mod_mbox/cassandra-user/201309.mbox/%3C002401ceb980%2485a26d10%2490e74730%24%40struq.com%3E

most likely due to having multiple data dirs",,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/13 08:50;marcuse;0001-create-snapshot-directory-if-it-does-not-exist-when-.patch;https://issues.apache.org/jira/secure/attachment/12604981/0001-create-snapshot-directory-if-it-does-not-exist-when-.patch",,,,,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350260,,,Wed Sep 25 12:59:21 UTC 2013,,,,,,,,,,"0|i1oe5r:",350553,,,,,,,,,,,,Normal,,2.0 beta 1,,,,,,,,,,,,,,,,"25/Sep/13 08:36;marcuse;nope, works fine with multiple data dirs, need more info;;;","25/Sep/13 08:49;marcuse;found it, if the data dir is empty except for the json file, there is no directory to store the hardlink in

attached patch creates the dir;;;","25/Sep/13 12:06;jbellis;+1;;;","25/Sep/13 12:59;marcuse;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
init.d script not working under Ubuntu,CASSANDRA-6090,12670355,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,lr,lr,24/Sep/13 20:40,16/Apr/19 09:32,14/Jul/23 05:53,30/Sep/13 16:29,2.0.1,,,Packaging,,,0,,,,,"When installing the Cassandra package on Ubuntu, it starts up automatically without writing the PID file.
It renders the init.d script useless as it can't status or stop cassandra.

I submitted a PR on github to fix this:
https://github.com/apache/cassandra/pull/21",Ubuntu 12.04.2 LTS x64,lr,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350184,,,Mon Sep 30 16:29:52 UTC 2013,,,,,,,,,,"0|i1odp3:",350478,2.0.1,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"30/Sep/13 16:29;urandom;Partially committed, (the fix for directory creation was committed as part of CASSANDRA-6101); Thanks Laurent!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
client_only example does not work,CASSANDRA-6089,12670347,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mihasya,mihasya,mihasya,24/Sep/13 20:10,16/Apr/19 09:32,14/Jul/23 05:53,25/Sep/13 19:12,1.2.11,2.0.2,,,,,0,,,,,"client_only example fails out of the box with:

{code}
13/09/24 11:44:53 INFO gms.Gossiper: Node /127.0.0.1 is now part of the cluster
13/09/24 11:44:53 INFO gms.Gossiper: InetAddress /127.0.0.1 is now UP
13/09/24 11:44:53 ERROR concurrent.DebuggableThreadPoolExecutor: Error in ThreadPoolExecutor
java.lang.NullPointerException
  at org.apache.cassandra.service.MigrationManager.maybeScheduleSchemaPull(MigrationManager.java:113)
	at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:95)
	at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:803)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:846)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:931)
	at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:50)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
Exception in thread ""GossipStage:1"" java.lang.NullPointerException
	at org.apache.cassandra.service.MigrationManager.maybeScheduleSchemaPull(MigrationManager.java:113)
	at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:95)
	at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:803)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:846)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:931)
	at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:50)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
{code}
I've tested this both on the 1.2.8 tag and with 1.2 latest.

The line in question is 

{code:java}
if (Schema.instance.getVersion().equals(theirVersion) || !shouldPullSchemaFrom(endpoint))
{code}

Seems that {{Schema.instance.getVersion()}} returns null in the client_only case. Adding a {{Schema.instance.getVersion() == null}} as another {{||}} in the conditional appears to fix it, but I don't remember the codepaths well enough to confidently say that that's the correct thing to do.",,aleksey,mihasya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Sep/13 18:45;mihasya;cassandra-1.2-6089.txt;https://issues.apache.org/jira/secure/attachment/12605074/cassandra-1.2-6089.txt",,,,,,,,,,,,,,,,,,,,1.0,mihasya,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350176,,,Wed Sep 25 19:12:59 UTC 2013,,,,,,,,,,"0|i1odnb:",350470,1.2.10,1.2.8,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,mihasya,,,"25/Sep/13 18:37;mihasya;Actually, the change I mention above stops the errors, but also short-circuits schema propagation :D I've sorted out what the code actually does and made a change that properly fixes the issue. Patch incoming, please hold.;;;","25/Sep/13 18:44;mihasya;This patch correctly checks if ""own"" schema version is null before trying to compare versions.;;;","25/Sep/13 18:45;mihasya;Here's the patch..;;;","25/Sep/13 19:12;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"StorageProxy.truncateBlocking sends truncation messages to fat client hosts; making truncation timeout",CASSANDRA-6088,12670274,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,m0nstermind,m0nstermind,m0nstermind,24/Sep/13 13:41,16/Apr/19 09:32,14/Jul/23 05:53,24/Sep/13 15:14,1.2.11,2.0.2,,,,,0,,,,,"We have fat clients on our cluster. Trying to truncate CF gives timeout. This is because fat client dont respond to Truncation messages.

Fixed by sending Truncation only to storage endpoints.
",,m0nstermind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/13 13:42;m0nstermind;TruncateBlocking.txt;https://issues.apache.org/jira/secure/attachment/12604796/TruncateBlocking.txt",,,,,,,,,,,,,,,,,,,,1.0,m0nstermind,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350103,,,Tue Sep 24 15:14:46 UTC 2013,,,,,,,,,,"0|i1od73:",350397,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"24/Sep/13 15:04;jbellis;Patch does not apply to 1.2 or 2.0 or trunk, can you rebase?;;;","24/Sep/13 15:05;jbellis;May actually be your gratuitous import reformatting that's the problem; suggest matching http://wiki.apache.org/cassandra/CodeStyle;;;","24/Sep/13 15:14;jbellis;rebased, fixed code style, and committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node OOMs on commit log replay when starting up,CASSANDRA-6087,12670273,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,m0nstermind,m0nstermind,m0nstermind,24/Sep/13 13:37,16/Apr/19 09:32,14/Jul/23 05:53,24/Sep/13 15:48,1.2.11,2.0.2,,,,,0,,,,,"After some activity on batchlogs and hints and CFs restarted nodes without finished draining. On startup it OOMs. 

Investigating it found, that memtables occupied all available RAM, because MeteredFlusher is started later in setup code, so memtables cannot flush. 

Fixed by starting metered flusher before commit log replay starts",,m0nstermind,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/13 13:37;m0nstermind;CassandraDaemon.txt;https://issues.apache.org/jira/secure/attachment/12604795/CassandraDaemon.txt",,,,,,,,,,,,,,,,,,,,1.0,m0nstermind,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350102,,,Tue Sep 24 15:48:33 UTC 2013,,,,,,,,,,"0|i1od6v:",350396,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"24/Sep/13 15:48;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node refuses to start with exception in ColumnFamilyStore.removeUnfinishedCompactionLeftovers when find that some to be removed files are already removed,CASSANDRA-6086,12670272,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,m0nstermind,m0nstermind,24/Sep/13 13:30,16/Apr/19 09:32,14/Jul/23 05:53,30/Dec/13 20:05,2.0.5,,,,,,2,,,,,"Node refuses to start with
{code}
Caused by: java.lang.IllegalStateException: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.
      at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:544)
      at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:262)
{code}

IMO, there is no reason to refuse to start discivering files that must be removed are already removed. It looks like pure bug diagnostic code and mean nothing to operator (nor he can do anything about this).

Replaced throw of excepion with dump of diagnostic warning and continue startup.
",,annesull,cowardlydragon,graham sanderson,gwicke,jre,m0nstermind,ngrigoriev,rcoli,thobbs,vilda,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6008,,,,,,,,,,,,,,,"19/Dec/13 23:58;thobbs;6086-2.0-v3.txt;https://issues.apache.org/jira/secure/attachment/12619699/6086-2.0-v3.txt","19/Nov/13 17:14;yukim;6086-v2.txt;https://issues.apache.org/jira/secure/attachment/12614653/6086-v2.txt","24/Sep/13 13:31;m0nstermind;removeUnfinishedCompactionLeftovers.txt;https://issues.apache.org/jira/secure/attachment/12604794/removeUnfinishedCompactionLeftovers.txt",,,,,,,,,,,,,,,,,,3.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,350101,,,Mon Dec 30 20:05:28 UTC 2013,,,,,,,,,,"0|i1od6n:",350395,,,,,,,,,yukim,,yukim,Normal,,2.0.0,,,,,,,,,,,,,,,,"24/Sep/13 15:31;yukim;The primary reason we have removeUnfinishedCompacitonLeftovers check is to not overcount counters stored.
If we do not stop at the original exception, then we will have both compacted SSTables and leftovers which may produce unexpected counter value.

Though we have the report that ""this happens""(CASSANDRA-6008), we should fix somehow.
Maybe we can consider compaction is finished even if we have entry in compactions_in_progress but some or all of the input SSTables are missing.;;;","24/Sep/13 16:06;m0nstermind;Well, the common with CASSANDRA-6008 is that in my case node was dead before restart due to OOM. However, unlike 6008, CFs on which it reported this error was never truncated.;;;","24/Sep/13 16:09;m0nstermind;From the other hand, the leftovers about to remove are already missing (i.e. already removed) when exception is thrown. I am not sure how can it  influence counter value.;;;","08/Oct/13 17:21;yukim;bq. From the other hand, the leftovers about to remove are already missing (i.e. already removed) when exception is thrown.

Is this the case that, for example, you have SSTable 'C' that is produced from compacting SSTables 'A' and 'B'('C' has ancestors 'A' and 'B'), and 'A' and 'B' are already deleted, but removeUnfinishedCompactionLeftovers reports you have unfinished compaction of 'A' and 'B'?

In above case, you definitely don't want to proceed, since the code tries remove SSTable 'C' afterwards.
Patch v2 is attached to prevent deleting SSTable if its ancestors are already missing after printing warning.

(I'm still looking for the case why the above happened though. We supposed not to have entries in compaction_in_progress when those are removed.)
;;;","17/Oct/13 15:04;cowardlydragon;Well, how would one repair this for now, beyond reconstructing a virgin node and then replication? If there are counters, certainly you could provide an option for an admin to reset them or accept invalid values and get access to the rest of the data, or mark it in some (tombstone-ish?) way that other nodes with more accurate values for the counters can then replicate the correct state?

And yes, we just got this.;;;","17/Oct/13 15:08;ngrigoriev;@Constance,

I was able to repair my node - see CASSANDRA-6008. Used the suggestion I received + had to add some flavor to it :);;;","08/Nov/13 22:26;jre;Hi, we are able to consistently reproduce this issue:
{noformat}
ERROR 23:14:06,001 Exception encountered during startup
java.lang.IllegalStateException: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.
       at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:489)
       at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:264)
       at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:461)
       at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)
java.lang.IllegalStateException: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.
       at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:489)
       at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:264)
       at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:461)
       at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:504)
Exception encountered during startup: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.
{noformat}

Here are the two ways in which we have found to reproduce this issue:
# Buffer a large amount of CL.LOCAL_QUORUM writes into a Cassandra CF, then drop the CF while the writes are still buffered.
# Buffer a large amount of CL.LOCAL_QUORUM writes into a Cassandra CF, then restart some nodes while before the buffer has finished draining.;;;","19/Nov/13 17:14;yukim;Rebased patch against cassandra-2.0 attached.

Again, the patch changes log ERROR and stop starting C* to print WARN but prevent deleting SSTable if its ancestors are already missing.;;;","11/Dec/13 20:21;thobbs;[~yukim] the patch looks pretty good.  Can you add a test like {{ColumnFamilyStoreTest.testRemoveUnifinishedCompactionLeftovers()}} to cover this?  Also, warning logs should either suggest an action or let the user know that no action is required; in this case, we should tell the user to report the issue but that no corrective action is needed.;;;","17/Dec/13 20:23;yukim;[~thobbs] Added test and slightly changed message: https://github.com/yukim/cassandra/commits/6086

I'm not good at wording, so suggestion is welcome.;;;","19/Dec/13 23:58;thobbs;The v3 patch (and [branch|https://github.com/thobbs/cassandra/tree/6086]) builds on [~yukim]'s v2 patch and adds incremental deletions of entries in {{compactions_in_progress}} as discussed in CASSANDRA-6008.  Additionally, this move the warning log to debug level, since there's not anything the user can do or should do.;;;","30/Dec/13 20:05;yukim;Thanks Tyler, committed.
(I removed LegacyLeveledManifest related change in trunk since it no longer exists.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift validation refuses row markers on CQL3 tables,CASSANDRA-6081,12670045,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Sep/13 12:35,16/Apr/19 09:32,14/Jul/23 05:53,23/Sep/13 14:43,2.0.2,,,,,,0,,,,,CASSANDRA-5138 don't let row markers pass. It should.,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/13 12:36;slebresne;6081.txt;https://issues.apache.org/jira/secure/attachment/12604579/6081.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349875,,,Mon Sep 23 14:43:06 UTC 2013,,,,,,,,,,"0|i1obtb:",350173,,,,,,,,,jbellis,,jbellis,Low,,2.0.0,,,,,,,,,,,,,,,,"23/Sep/13 12:36;slebresne;Trivial patch attached.;;;","23/Sep/13 13:36;jbellis;+1;;;","23/Sep/13 14:43;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bad metadata returned for SELECT COUNT,CASSANDRA-6080,12670041,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Sep/13 12:20,16/Apr/19 09:32,14/Jul/23 05:53,25/Sep/13 12:01,2.0.2,,,,,,0,,,,,"The native protocol v2 returns the result metadata with a prepared statement, but for count queries this is currently incorrect, we return the metadata corresponding to a 'SELECT *' instead.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/13 12:22;slebresne;6080.txt;https://issues.apache.org/jira/secure/attachment/12604576/6080.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349871,,,Wed Sep 25 12:01:12 UTC 2013,,,,,,,,,,"0|i1obsf:",350169,,,,,,,,,aleksey,,aleksey,Low,,2.0 beta 1,,,,,,,,,,,,,,,,"23/Sep/13 12:22;slebresne;Trivial patch attached;;;","24/Sep/13 13:40;aleksey;+1;;;","25/Sep/13 12:01;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Memtables flush is delayed when having a lot of batchlog activity, making node OOM",CASSANDRA-6079,12670028,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,m0nstermind,m0nstermind,m0nstermind,23/Sep/13 11:20,16/Apr/19 09:32,14/Jul/23 05:53,23/Sep/13 14:22,1.2.11,2.0.2,,,,,0,,,,,"Both MeteredFlusher and BatchlogManager share the same OptionalTasks thread. So, when batchlog manager processes its tasks no flushes can occur. Even more, batchlog manager waits for batchlog CF compaction to finish.

On a lot of batchlog activity this prevents memtables from flush for a long time, making the node OOM.

Fixed this by moving batchlog to its own thread and not waiting for batchlog compaction to finish.",,m0nstermind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/13 11:20;m0nstermind;NoWaitBatchlogCompaction.diff;https://issues.apache.org/jira/secure/attachment/12604564/NoWaitBatchlogCompaction.diff",,,,,,,,,,,,,,,,,,,,1.0,m0nstermind,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349858,,,Wed Sep 25 10:09:02 UTC 2013,,,,,,,,,,"0|i1obpj:",350156,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"23/Sep/13 14:22;jbellis;Committed the executorservice change.  I'm less convinced by making cleanup not block for the compaction -- if batchlog is really behind, you don't want it to have to re-scan all the tombstoned rows you just replayed.;;;","23/Sep/13 14:32;jbellis;bq. Committed the executorservice change

... also to 1.2.11.;;;","23/Sep/13 16:42;m0nstermind;From the other hand, if all compaction pool threads are occupied by large CFs, batchlog manager stops replaying batches for a long time, which may not be acceptable.
Rescanning tombstones looks more acceptable to me than delaying of mutations. 

Ideally, batchlog should ensure compaction really happens before waiting. Either running it in its own executor or making some communication with CompactionManager. And if compaction cannot be done at the moment - just go and rescan tobmstones, trading efficiency for consistency.;;;","23/Sep/13 18:18;jbellis;Remember, this is the *replay* path.  If it's running, the mutation is already delayed, probably because the target node was down.  So delaying for compactions doesn't seem like a dealbreaker to me.  Especially since that leaves more iops to keep the rest of the system healthy and not go down ourselves. :);;;","23/Sep/13 20:13;m0nstermind;Not the target, but coordinator. This could be a target node at the same time of course, but even if it is, at least 2 other targets are ready to accept this mutation. Delaying mutation to quorum for more than rpc write timeout practically means it is lost.

;;;","23/Sep/13 20:33;jbellis;The coordinator doesn't block for compaction.  Only replayAllFailedBatches.;;;","23/Sep/13 20:43;m0nstermind;Exactly.
The case here could be:
1. Coordinator writes batchlog to chosen 2 nodes within the same dc
2. Coordinator crashes
3. Those 2 nodes supposed to run replayAllFailedBatches to replay this batch of crashed coordinator
3.1 but they cannot, because compaction pool is busy with ongoing large CF compaction and they are waiting on submitUsedDefined().get()

;;;","23/Sep/13 20:48;jbellis;Right.  I don't think it's worth sacrificing throughput in the common case, to reduce delay in the exceptional case.;;;","23/Sep/13 20:49;jbellis;(But, I'd be fine with avoiding the cleanup call unless there was actually something replayed.);;;","23/Sep/13 21:21;m0nstermind;Yeah, this makes stall on get() less probable. 

I could measure how big the impact on common case throughout due to rescan of thombstones, if you did not performed this measurement earlier.

Yet another idea is to reserve a special thread in CompactionManager for doing compactions on batchlog (and possibly hints) CF. This way it at least wouldn't delay for a long time.;;;","23/Sep/13 22:30;jbellis;I'm not a huge fan of making CompactionManager more complex, either. :);;;","24/Sep/13 16:29;m0nstermind;Well, I did some quick test. As i suspected there is no measurable performance impact on this.

3 node cluster, RF=3, separated client, writing batches of 3 inserts each to 3 different CFs. 8 core (not so) recent iron servers.
(no batches were replayed actually, so this pure common case). 

I measured this for 1 hour run in both cases.

Here is performance  WITH wait on get():
avg 3052 batches/sec, st deviance 131

WITHOUT waiting in get()
avg 3030 per sec, deviance 106

;;;","24/Sep/13 17:07;jbellis;I think you're missing that we need to replay not just for coordinator failure but for replica failure.  So ""only scan for last couple seconds"" is not correct; you need to scan every partition since the replica went down.

(Note that for typical deployments, replica failure will be 3x more likely than coordinator failure.);;;","24/Sep/13 17:28;m0nstermind;Do you mean the replica, which is target for one of batched mutations ? I see a hint is written by BM if it cannot deliver one of mutations serialized to batchlog record (in replaySerializedMutation and attemptDirectDelivery) , so as i understood HintManager bothers further about delivering it to failed replica when it is back online. So as soon as BM writes a hint(s) for (all of) failed replica(s), BM has nothing to do with batchlog record, so no all partitions scanning neccessary.

Am I missing something again ?;;;","24/Sep/13 20:14;jbellis;bq. as soon as BM writes a hint(s) for (all of) failed replica(s), BM has nothing to do with batchlog record, so no all partitions scanning neccessary

You are right.  Carry on!;;;","25/Sep/13 10:09;m0nstermind;ok then. started writing code ;-);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong memtable size estimation: liveRatio is not honored in edge cases,CASSANDRA-6078,12670024,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,m0nstermind,m0nstermind,23/Sep/13 11:08,16/Apr/19 09:32,14/Jul/23 05:53,25/Sep/13 01:51,1.2.11,2.0.2,,,,,0,,,,,"Memtable.getLiveSize does not honours liveRatio the correct way: 
allocator.get**Size() return sizes allocated only by name and columns data (i.e. no liveRatio applied); but conditions, which cap estimated size, compare it with estimatedSize, already multiplied by liveRatio.
If liveRatio is big enough (i've seen >11 on our dataset), this leads to huge estimation errors and even to OutOfMemory, because MeteredFlusher  underestimates memtables sizes.
",,jasobrown,m0nstermind,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/13 15:03;jbellis;6078-v2.txt;https://issues.apache.org/jira/secure/attachment/12604603/6078-v2.txt","23/Sep/13 11:08;m0nstermind;Memtable-getLiveSize.diff;https://issues.apache.org/jira/secure/attachment/12604558/Memtable-getLiveSize.diff",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349854,,,Wed Sep 25 01:51:48 UTC 2013,,,,,,,,,,"0|i1obon:",350152,,,,,,,,,jasobrown,,jasobrown,Low,,1.1.12,,,,,,,,,,,,,,,,"23/Sep/13 15:03;jbellis;This isn't 100% correct, either.  Consider the case where we've allocated a single 1-byte column from a 1MB slab: multiplying the 1MB by liveRatio is clearly incorrect.

The intent (CASSANDRA-5497) is to bound our error when liveRatio is incorrect.  I think where we get into trouble is with the upper bound, and I don't see a good way to fix that.  (On the bright side, overestimating the size is a lot less dangerous than underestimating it.)

Patch attached to remove the upper bound.;;;","24/Sep/13 13:38;jbellis;WDYT [~jasobrown]?;;;","24/Sep/13 23:11;jasobrown;I think this is a legit change to protect against the upper bound problem. I'd rather flush more frequently than OOM more frequently. 

lgtm;;;","25/Sep/13 01:51;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"SSTableMetadata.min(max)ColumnNames keep whole SlabAllocatior region referenced; wasting memory",CASSANDRA-6077,12670016,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,m0nstermind,m0nstermind,m0nstermind,23/Sep/13 09:58,16/Apr/19 09:32,14/Jul/23 05:53,23/Sep/13 14:14,2.0.2,,,,,,0,,,,,".. which could be a problem when there is a lot of sstables, when using LCS for example.

SSTableWriter calls SSTableMetadata.Collector.updateMin(Max)ColumnNames passing List of ByteByffers which reference a small byte array in slab region.
ColumnNameHelper.mergeMin(Max) just returns a reference of column name back to SSTableMetadata. So the latter keeps whole slab region referenced, preventing it from being GCed. 

Fixed it by making copies of column name bytebuffer, if its size more than column name itself.",,m0nstermind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/13 11:27;m0nstermind;ColumnNameHelper.diff;https://issues.apache.org/jira/secure/attachment/12604566/ColumnNameHelper.diff",,,,,,,,,,,,,,,,,,,,1.0,m0nstermind,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349846,,,Mon Sep 23 14:14:10 UTC 2013,,,,,,,,,,"0|i1obmv:",350144,,,,,,,,,jbellis,,jbellis,Normal,,2.0.0,,,,,,,,,,,,,,,,"23/Sep/13 14:14;jbellis;Nice catch; committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The token function should allow column identifiers in the correct order only,CASSANDRA-6075,12669792,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,blerer,mfiguiere,mfiguiere,21/Sep/13 02:06,16/Apr/19 09:32,14/Jul/23 05:53,24/Sep/14 02:10,2.0.11,2.1.1,,Legacy/CQL,,,0,cql,,,,"Given the following table:
{code}
CREATE TABLE t1 (a int, b text, PRIMARY KEY ((a, b)));
{code}

The following request returns an error in cqlsh as literal arguments order is incorrect:
{code}
SELECT * FROM t1 WHERE token(a, b) > token('s', 1);
Bad Request: Type error: 's' cannot be passed as argument 0 of function token of type int
{code}

But surprisingly if we provide the column identifier arguments in the wrong order no error is returned:
{code}
SELECT * FROM t1 WHERE token(a, b) > token(1, 'a'); // correct order is valid
SELECT * FROM t1 WHERE token(b, a) > token(1, 'a'); // incorrect order is valid as well
{code}",Cassandra 1.2.9,aleksey,blerer,mfiguiere,mishail,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/14 01:51;aleksey;6075-fix-v2.txt;https://issues.apache.org/jira/secure/attachment/12670873/6075-fix-v2.txt","23/Sep/14 08:53;blerer;CASSANDRA-2.0-6075-PART2.txt;https://issues.apache.org/jira/secure/attachment/12670668/CASSANDRA-2.0-6075-PART2.txt","23/Sep/14 08:53;blerer;CASSANDRA-2.1-6075-PART2.txt;https://issues.apache.org/jira/secure/attachment/12670669/CASSANDRA-2.1-6075-PART2.txt","18/Sep/14 10:08;blerer;CASSANDRA-2.1-6075.txt;https://issues.apache.org/jira/secure/attachment/12669692/CASSANDRA-2.1-6075.txt","03/Sep/14 11:49;blerer;CASSANDRA-6075.txt;https://issues.apache.org/jira/secure/attachment/12666209/CASSANDRA-6075.txt",,,,,,,,,,,,,,,,5.0,blerer,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349720,,,Wed Sep 24 02:10:05 UTC 2014,,,,,,,,,,"0|i1oauv:",350018,,,,,,,,,thobbs,,thobbs,Low,,,,,,,,,,,,,,,,,,"03/Sep/14 11:49;blerer;The patch add a test at the end of SelectStatement.processPartitionKeyRestrictions (once we are sure that no argument is missing) to check that the token function arguments have been specify in the good order.;;;","10/Sep/14 22:02;thobbs;I assume this is a problem for 2.1 as well.  Can you post a 2.1 patch as well?  The main thing that should change is the unit test (which can extend CQLTester in 2.1).;;;","18/Sep/14 10:08;blerer;This is the patch for the 2.1 branch. It differs from the one of the 2.0 branch because the 2.1 branch is using CFMetaData instead of CFDefinition. The test has also been migrated to CQLTester;;;","18/Sep/14 19:02;thobbs;If I specify the partition key items out of order, I get a somewhat confusing type error message:

{noformat}
cqlsh:ks1> create table foo (a int, b text, c int, d int, PRIMARY KEY ((a, b, c)));
cqlsh:ks1> select * from foo WHERE token(a, c, b) > token(0, 0, 'a');
Bad Request: Type error: 0 cannot be passed as argument 1 of function token of type text
{noformat}

We need to do the order check prior to the type check or find a way to do the type check properly.;;;","18/Sep/14 20:07;blerer;I am impressed Tyler. I would have never thought about testing such a crappy case.;;;","19/Sep/14 09:07;blerer;I had a look and it is not easy to solve that problem. 
The problem come from the fact that we build the restrictions at the same time that we do part of our validation. We will try to compute the value of token(0, 0, 'a') before we even know if we have all the columns required within the token function or if their is a mix of restrictions using token function that we do not support. If we want to do those check earlier we will end up duplicating the test that we do on Restrictions on Relations.
This seems to indicate a problem in the code. I believe that the code should first perform the validation checks on Relations then build Restrictions.
I planning to refactor SelectStatement and I will be able to tackle that problem properly at that time but I want to do that refactoring in 3.0 as it is not a trivial stuff to do.
So my proposal is to deliver the current fix in 2.0 and 2.1 (knowing that it is not perfect) and properly fix it for 3.0.
I opened #CASSANDRA-7981 for the SelectStatement refactoring were I keep track of this issue. 
What do you think Tyler?;;;","19/Sep/14 16:26;thobbs;Seems reasonable to me.

+1, committed.;;;","19/Sep/14 20:57;aleksey;This breaks pig tests on trunk, at least.

[~blerer] to run pig tests, do 'ant pig-test'. Ignore most the warnings - there are a lot of them.;;;","23/Sep/14 08:53;blerer;Completely forgot the case of the slice with start and end bound(e.g. token(key) >= token(1) and token(key) < token(2))
That is what broke the Pig-Tests. Sorry.
Here are the patchs to fix that problem on the 2.0 and 2.1+ branches.

I also dicover while trying to add more tests than the current approach to handle token functions is broken. With the current approach we cannot reject queries like:
{{SELECT * FROM %s WHERE token(a, b) > token(?, ?) and token(b) < token(?, ?)}} or {{SELECT * FROM %s WHERE token(a) > token(?, ?) and token(b) > token(?, ?)}} 
I will try to find another solution as part of #CASSANDRA-7981;;;","24/Sep/14 02:10;aleksey;Fixed in https://github.com/apache/cassandra/commit/b1166c09983b1678cbc4b241f1da860930c571a5

Used Guava's Iterators#cycle() instead of the null/hasnext check, and removed the (if cfDef.partitionKeyCount() > 0) condition, which is always true.

Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in Pig CassandraStorage,CASSANDRA-6072,12669761,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,jjordan,jjordan,20/Sep/13 22:35,16/Apr/19 09:32,14/Jul/23 05:53,25/Sep/13 21:11,1.2.11,2.0.2,,,,,0,,,,,"key_alias can be null for tables created from thrift.

Which causes an NPE here:
https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/hadoop/pig/AbstractCassandraStorage.java#L633",,aleksey,alexliu68,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/13 19:42;alexliu68;6072-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12604655/6072-1.2-branch.txt",,,,,,,,,,,,,,,,,,,,1.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349689,,,Wed Sep 25 21:11:02 UTC 2013,,,,,,,,,,"0|i1oanz:",349987,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"23/Sep/13 19:39;alexliu68;It's the issue when upgrade from 1.1 to 1.2.6, but the issue has been fixed by CASSANDRA-5800, so I use this ticket to clean up the code. We don't need check key_alias any more.;;;","23/Sep/13 20:35;aleksey;[~alexliu68] 5800 migrates the key_alias to key_aliases. The issue here is that there might not be a key_alias in the first place, so there is nothing to migrate.

Basically, the code should handle tables without a key alias, and it doesn't. 5800 wasn't covering that.;;;","23/Sep/13 20:49;alexliu68;The clean up patch just removes key_alias which should fix NPE as well.;;;","25/Sep/13 21:11;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CqlStorage loading compact table adds an extraneous field to the pig schema,CASSANDRA-6071,12669696,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,samt,samt,samt,20/Sep/13 17:07,16/Apr/19 09:32,14/Jul/23 05:53,26/Sep/13 18:54,1.2.11,2.0.2,,,,,0,,,,,"{code}
CREATE TABLE t (
  key text,
  field1 int,
  field2 int
  PRIMARY KEY (key, field1)
) WITH COMPACT STORAGE;

INSERT INTO t (key,field1,field2) VALUES ('key1',1,2);
INSERT INTO t (key,field1,field2) VALUES ('key2',1,2);
INSERT INTO t (key,field1,field2) VALUES ('key3',1,2);
{code}

{code}
grunt> t = LOAD 'cql://ks/t' USING CqlStorage();
grunt> describe t;                                 
t: {key: chararray,field1: int,field2: int,value: int}
dump t;
(key1,1,2,)
(key3,1,2,)
(key2,1,2,)
{code}
",,alexliu68,brandon.williams,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Sep/13 16:45;samt;6071-2.txt;https://issues.apache.org/jira/secure/attachment/12604830/6071-2.txt","26/Sep/13 18:40;samt;6071-3.txt;https://issues.apache.org/jira/secure/attachment/12605302/6071-3.txt","20/Sep/13 17:08;samt;6071.txt;https://issues.apache.org/jira/secure/attachment/12604269/6071.txt",,,,,,,,,,,,,,,,,,3.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349628,,,Thu Sep 26 18:54:56 UTC 2013,,,,,,,,,,"0|i1oaaf:",349926,,,,,,,,,alexliu68,,alexliu68,Low,,1.2.9,,,,,,,,,,,,,,,,"20/Sep/13 17:08;samt;This is caused by an additional ColumnDef being added in the CfDef used to construct the Pig schema. Where a value_alias is specified for the compact value column, CqlStorage.getKeysMeta adds a ColumnDef for it. The additional one is then added in AbstractCassandaStorage.getColumnMeta, so I've added an additional boolean arg to indicate whether the the value_alias has already been processed. I've also refactored ACS.getColumnMeta a bit to (hopefully) make the logic clearer. 
;;;","23/Sep/13 18:26;alexliu68;+1 except that I notice the patch is not based on current cassandra-1.2 branch, so it needs be updated to cassandra-1.2 branch.;;;","24/Sep/13 16:45;samt;Sorry about that, updated patch attached;;;","24/Sep/13 16:56;alexliu68;+1;;;","26/Sep/13 16:05;brandon.williams;Can you rebase [~beobal]? We committed a few pig things yesterday and it no longer applies :(;;;","26/Sep/13 18:40;samt;attached rebased patch;;;","26/Sep/13 18:54;brandon.williams;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sets not stored by INSERT with IF NOT EXISTS,CASSANDRA-6069,12669574,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,urandom,urandom,19/Sep/13 22:39,25/Oct/19 13:11,14/Jul/23 05:53,23/Sep/13 14:42,2.0.2,,,Feature/Lightweight Transactions,,,0,LWT,,,,An {{INSERT}} of a {{set}} column type is not stored when using {{IF NOT EXISTS}},,aleksey,sayap,slebresne,urandom,vongocminh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Sep/13 13:25;slebresne;6069.txt;https://issues.apache.org/jira/secure/attachment/12604589/6069.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349506,,,Mon Sep 23 14:42:51 UTC 2013,,,,,,,,,,"0|i1o9jb:",349804,2.0.0,,,,,,,,aleksey,,aleksey,Normal,,2.0.0,,,,,,,,,,,,,,,,"23/Sep/13 13:25;slebresne;This is a bit annoying. When we insert a new collection value, we also insert a range tombstone to remove the potential previous value. That range tombstone has a tombstone of t-1 (where t is the timestamp of the actual insert) so it doesn't conflict with the new inserts. But a CAS write rewrites all the timestamps to make sure they match the order decided by Paxos, which in this case mean the range tombstone ends up removing the data we're inserting.

Attaching a patch that implements a simple solution by making paxos use t-1 for row and range tombstones when rewriting the operation timestamp. In general this should be ok since tombstones wins over normal inserts so the tombstone will still always delete anything that has a timestamp < t. It does mean however that if someone does a row/range deletion and inserts in the same CAS operation, the deletions won't win contrarly to what happens with other operations. That being said:
# deleting something you are just inserting is a bit of an anti-social thing to do.
# I don't think users can actually do it today with CAS because we don't allow batches.
So feels like a reasonable fix.

The other solution would be to special case the paxos code for that specific CQL3 collections case, but that's going to be a tad more painful.
;;;","23/Sep/13 13:55;aleksey;+1;;;","23/Sep/13 14:42;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use non-pooling readers with openForBatch,CASSANDRA-6067,12669535,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,19/Sep/13 20:11,16/Apr/19 09:32,14/Jul/23 05:53,19/Sep/13 21:56,2.0.1,,,Legacy/Tools,,,0,,,,,Looks like CASSANDRA-5555 was incorrectly merged forward.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/13 20:29;jbellis;6067-v2.txt;https://issues.apache.org/jira/secure/attachment/12604114/6067-v2.txt","19/Sep/13 20:12;jbellis;6067.txt;https://issues.apache.org/jira/secure/attachment/12604110/6067.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349467,,,Thu Sep 19 21:56:39 UTC 2013,,,,,,,,,,"0|i1o9an:",349765,,,,,,,,,yukim,,yukim,Low,,2.0.0,,,,,,,,,,,,,,,,"19/Sep/13 20:22;yukim;BufferedSegmentedFile extends PoolingSegmentedFile. Still seeing the following error when bulk load:

{code}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:36)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:161)
	at org.apache.cassandra.io.util.SegmentedFile$SegmentIterator.next(SegmentedFile.java:142)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:904)
	at org.apache.cassandra.io.sstable.SSTableReader.getPosition(SSTableReader.java:839)
	at org.apache.cassandra.io.sstable.SSTableReader.getPositionsForRanges(SSTableReader.java:751)
	at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:122)
	at java.io.File.list(File.java:1087)
	at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:73)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:155)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:66)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.config.DatabaseDescriptor.getFileCacheSizeInMB(DatabaseDescriptor.java:1145)
	at org.apache.cassandra.service.FileCacheService.<clinit>(FileCacheService.java:41)
	... 11 more
{code};;;","19/Sep/13 20:29;jbellis;v2 fixes BSF to not extend PSF.

(Checked and this is fine in 1.2.);;;","19/Sep/13 20:36;yukim;+1;;;","19/Sep/13 21:56;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Heavy write load exhausts heap space,CASSANDRA-6056,12669278,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,arnaud-lb,arnaud-lb,18/Sep/13 16:17,16/Apr/19 09:32,14/Jul/23 05:53,20/Nov/13 00:10,,,,,,,0,,,,,"Issuing many INSERT or UPDATE queries cause cassandra to exhaust the heap space in a few minutes. It then gets stuck in full GCs, failing to recover.

Observed with the default config from Datastax Debian packages, with a 8GB heap. Query rate is around 35K queries per second, with 16 concurrent queries.","Debian, Cassandra 2.0",arnaud-lb,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/13 10:18;arnaud-lb;dominator-tree.png;https://issues.apache.org/jira/secure/attachment/12604021/dominator-tree.png","19/Sep/13 09:35;arnaud-lb;jvisualvm-class-instances.png;https://issues.apache.org/jira/secure/attachment/12604013/jvisualvm-class-instances.png",,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349210,,,Wed Nov 20 00:10:31 UTC 2013,,,,,,,,,,"0|i1o7pj:",349508,2.0.0,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"18/Sep/13 16:49;jbellis;This sounds a lot like CASSANDRA-5982.  You should have a look at that, especially if you're throwing large blobs around.  If not, you should put some effort into describing what makes your workload special.;;;","19/Sep/13 09:37;arnaud-lb;I can reproduce this with only UPDATES on two columns with small values (timestamp, int). Those columns were previously null. The entire rows are moderately small too, less than 200 bytes in average. This happens with only 4 concurrent queries after some times.

Attached a screenshot of jvisualvm's classes view: [^jvisualvm-class-instances.png]

The table being updated looks like this:

{code}
id timeuuid primary key
str1 text
str2 text
str3 text
str4 text
time1 timestamp (the column being updated)
time2 timestamp (mostly null)
time3 timestamp (mostly null)
n int           (the second column being updated)
time4 timestamp
time5 timestamp (mostly null)
{code}

Looking at CASSANDRA-5982, it could very well be related. I'm willing to try with the fix applied, is this merged in any 2.0 branch?;;;","19/Sep/13 10:18;arnaud-lb;Attached a screenshot of eclipse memory analyzer tool, showing the dominator tree report;;;","19/Sep/13 13:55;jbellis;5982 is applied to the only 2.0 branch in the repository.;;;","20/Nov/13 00:10;jbellis;between CASSANDRA-5982 and CASSANDRA-6059 I think we're good.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system.peers table not updated after decommissioning nodes in C* 2.0,CASSANDRA-6053,12669215,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,gumuz,gumuz,18/Sep/13 09:06,16/Apr/19 09:32,14/Jul/23 05:53,10/Jan/14 18:52,1.2.14,2.0.5,,,,,9,,,,,"After decommissioning my cluster from 20 to 9 nodes using opscenter, I found all but one of the nodes had incorrect system.peers tables.

This became a problem (afaik) when using the python-driver, since this queries the peers table to set up its connection pool. Resulting in very slow startup times, because of timeouts.

The output of nodetool didn't seem to be affected. After removing the incorrect entries from the peers tables, the connection issues seem to have disappeared for us. 

Would like some feedback on if this was the right way to handle the issue or if I'm still left with a broken cluster.

Attached is the output of nodetool status, which shows the correct 9 nodes. Below that the output of the system.peers tables on the individual nodes.
",Datastax AMI running EC2 m1.xlarge instances,cjbottaro,enigmacurry,gumuz,jeromatron,kenfailbus,krieb,psanford,rcoli,rssvihla,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Dec/13 22:49;thobbs;6053-v1.patch;https://issues.apache.org/jira/secure/attachment/12620992/6053-v1.patch","18/Sep/13 09:07;gumuz;peers;https://issues.apache.org/jira/secure/attachment/12603794/peers",,,,,,,,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349147,,,Tue Nov 03 14:30:09 UTC 2015,,,,,,,,,,"0|i1o7bj:",349445,1.2.9,2.0.3,,,,,,,brandon.williams,,brandon.williams,Normal,,2.0.0,,,,,,,,,,,,,,,,"18/Sep/13 09:07;gumuz;Output of nodetool status and the contents of the system.peers tables.;;;","18/Sep/13 14:35;gumuz;Just a heads up: I've just removed a broken node using 'nodetool removenode' and encountered the same problem again. The node wasn't removed from the other nodes' system.peers.;;;","26/Sep/13 21:28;cjbottaro;We're seeing this on a 1.2.9 cluster as well.;;;","27/Sep/13 18:56;cjbottaro;Is there a way to manually fix the system.peers table once it's in the messed up state?  Like a nodetool command or simply using CQL to delete the bad rows?;;;","27/Sep/13 19:11;brandon.williams;You can simply delete them all from system.peers if you want, they'll re-populate correctly.;;;","27/Sep/13 19:15;brandon.williams;Not sure how this can happen, given this code:

{noformat}
    private void removeEndpoint(InetAddress endpoint)
    {
        Gossiper.instance.removeEndpoint(endpoint);
        if (!isClientMode)
            SystemTable.removeEndpoint(endpoint);
    }
{noformat}

which is what decom and removetoken end up calling.;;;","27/Sep/13 21:16;cjbottaro;I did a ""truncate peers"" and it hasn't repopulated (I waited about 10 mins).  Do I need to restart one or more nodes to trigger repopulating of the table?;;;","27/Sep/13 23:32;brandon.williams;Yes.;;;","03/Oct/13 11:12;jeromatron;The load_ring_state=false directive should probably also clear out the peers table because otherwise, the state that you're trying to get rid of is still persisted there.;;;","07/Oct/13 16:56;brandon.williams;If a user knows enough to disable loading the state and that fixes the problem, they can clear the peers table manually.;;;","18/Dec/13 17:02;jbellis;[~enigmacurry] can you reproduce?;;;","18/Dec/13 18:03;enigmacurry;First attempt appears to work correctly on cassandra-2.0 HEAD and 1.2.9 : 

{code}
12:53 PM:~$ ccm create -v git:cassandra-1.2.9 t
Fetching Cassandra updates...
Current cluster is now: t
12:53 PM:~$ ccm populate -n 5
12:54 PM:~$ ccm start
12:54 PM:~$ ccm node1 stress
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency/95th/99th,elapsed_time
24994,2499,2499,9.5,55.2,179.0,10
103123,7812,7812,2.8,27.2,134.7,20
236358,13323,13323,1.7,15.4,134.7,30
329477,9311,9311,1.7,9.8,109.8,40
405667,7619,7619,1.8,9.2,6591.9,50
558989,15332,15332,1.5,6.6,6591.1,60
^C12:55 PM:~$ ccm node1 cqlsh
Connected to t at 127.0.0.1:9160.
[cqlsh 3.1.7 | Cassandra 1.2.9-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> select peer from system.peers;

 peer
-----------
 127.0.0.3
 127.0.0.2
 127.0.0.5
 127.0.0.4

cqlsh>
12:55 PM:~$ ccm node2 decommission
12:57 PM:~$ ccm node1 cqlsh
Connected to t at 127.0.0.1:9160.
[cqlsh 3.1.7 | Cassandra 1.2.9-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> select peer from system.peers;

 peer
-----------
 127.0.0.3
 127.0.0.5
 127.0.0.4

cqlsh>
12:58 PM:~$
{code}

All nodes show equivalent peers table.;;;","18/Dec/13 18:29;enigmacurry;OK, reproduced this by killing -9 one of the nodes and then doing a 'nodetool removenode':

{code}
01:20 PM:~$ kill -9 18961    (PID of node1)
01:21 PM:~$ ccm node1 status
Failed to connect to '127.0.0.1:7100': Connection refused
01:21 PM:~$ ccm node2 status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Owns   Host ID                               Token                                    Rack
DN  127.0.0.1  62.93 KB   20.0%  896644af-8640-4be6-a3ff-e8ed559d851c  -9223372036854775808                     rack1
UN  127.0.0.2  51.17 KB   20.0%  d3801466-d36d-428c-b4e5-05ff69fe36c0  -5534023222112865485                     rack1
UN  127.0.0.3  62.78 KB   20.0%  cb36c3ad-df45-4f77-bff5-ca93c504ec08  -1844674407370955162                     rack1
UN  127.0.0.4  51.17 KB   20.0%  89031a05-a3f6-4ac7-9d29-6caa0c609dbc  1844674407370955161                      rack1
UN  127.0.0.5  51.27 KB   20.0%  4909d856-a86e-493a-a7d0-7570d71eb9d8  5534023222112865484                      rack1

# Issue removenode on node3 :
01:21 PM:~$ ~/.ccm/t/node1/bin/nodetool -p 7300 removenode 896644af-8640-4be6-a3ff-e8ed559d851c

01:22 PM:~$ ccm node3 cqlsh
Connected to t at 127.0.0.3:9160.
[cqlsh 4.1.0 | Cassandra 2.0.3-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> select * from system.peers;

 peer      | data_center | host_id                              | preferred_ip | rack  | release_version | rpc_address | schema_version                       | tokens
-----------+-------------+--------------------------------------+--------------+-------+-----------------+-------------+--------------------------------------+--------------------------
 127.0.0.2 | datacenter1 | d3801466-d36d-428c-b4e5-05ff69fe36c0 |         null | rack1 |  2.0.3-SNAPSHOT |   127.0.0.2 | d133398f-f287-3674-83af-a1b04ee29f1f | {'-5534023222112865485'}
 127.0.0.5 | datacenter1 | 4909d856-a86e-493a-a7d0-7570d71eb9d8 |         null | rack1 |  2.0.3-SNAPSHOT |   127.0.0.5 | d133398f-f287-3674-83af-a1b04ee29f1f |  {'5534023222112865484'}
 127.0.0.4 | datacenter1 | 89031a05-a3f6-4ac7-9d29-6caa0c609dbc |         null | rack1 |  2.0.3-SNAPSHOT |   127.0.0.4 | d133398f-f287-3674-83af-a1b04ee29f1f |  {'1844674407370955161'}

(3 rows)

# Check node2 peers table:

01:23 PM:~$ ccm node2 cqlsh
Connected to t at 127.0.0.2:9160.
[cqlsh 4.1.0 | Cassandra 2.0.3-SNAPSHOT | CQL spec 3.1.1 | Thrift protocol 19.39.0]
Use HELP for help.
cqlsh> select * from system.peers;

 peer      | data_center | host_id                              | preferred_ip | rack  | release_version | rpc_address | schema_version                       | tokens
-----------+-------------+--------------------------------------+--------------+-------+-----------------+-------------+--------------------------------------+--------------------------
 127.0.0.3 | datacenter1 | cb36c3ad-df45-4f77-bff5-ca93c504ec08 |         null | rack1 |  2.0.3-SNAPSHOT |   127.0.0.3 | d133398f-f287-3674-83af-a1b04ee29f1f | {'-1844674407370955162'}
 127.0.0.1 |        null | 896644af-8640-4be6-a3ff-e8ed559d851c |         null |  null |            null |   127.0.0.1 |                                 null |                     null
 127.0.0.5 | datacenter1 | 4909d856-a86e-493a-a7d0-7570d71eb9d8 |         null | rack1 |  2.0.3-SNAPSHOT |   127.0.0.5 | d133398f-f287-3674-83af-a1b04ee29f1f |  {'5534023222112865484'}
 127.0.0.4 | datacenter1 | 89031a05-a3f6-4ac7-9d29-6caa0c609dbc |         null | rack1 |  2.0.3-SNAPSHOT |   127.0.0.4 | d133398f-f287-3674-83af-a1b04ee29f1f |  {'1844674407370955161'}

(4 rows)

# oh noes!... node2 still has an entry for node1 in peers table.

01:23 PM:~$ ccm node2 status
Datacenter: datacenter1
=======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address    Load       Owns   Host ID                               Token                                    Rack
UN  127.0.0.2  51.17 KB   40.0%  d3801466-d36d-428c-b4e5-05ff69fe36c0  -5534023222112865485                     rack1
UN  127.0.0.3  62.78 KB   20.0%  cb36c3ad-df45-4f77-bff5-ca93c504ec08  -1844674407370955162                     rack1
UN  127.0.0.4  51.17 KB   20.0%  89031a05-a3f6-4ac7-9d29-6caa0c609dbc  1844674407370955161                      rack1
UN  127.0.0.5  51.27 KB   20.0%  4909d856-a86e-493a-a7d0-7570d71eb9d8  5534023222112865484                      rack1

{code}

By issuing the removenode on node3, node3 seems to know about the node being removed and it's peers table is correct. node2, although it's status output shows node1 going away, it's peers table has not been updated.;;;","18/Dec/13 21:57;jbellis;Thanks, Ryan.;;;","30/Dec/13 13:54;brandon.williams;Can you provide debug logs from node2?;;;","31/Dec/13 18:39;thobbs;I was able to repro with Ryan's steps, so I can upload the logs if you'd like, but I should be able to figure this one out.;;;","31/Dec/13 22:49;thobbs;The problem was that state changes for the removed node were being handled after the system.peers row was deleted.  These state changes would result in update to the system.peers row, partially reviving it.

6053-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-6053]) avoids updating the system.peers table if the node is unknown or is in one of the ""dead"" states and adds some basic unit test coverage.;;;","10/Jan/14 18:52;brandon.williams;Committed.;;;","03/Nov/15 14:30;kenfailbus;[~brandon.williams] FYI - Even though, this was fixed it surfaced in 2.0.14 release that we have in production. We are going to follow the work-around as mentioned above.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Wrong toString() of CQL3Type.Collection,CASSANDRA-6051,12669211,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,alexander_radzin,alexander_radzin,18/Sep/13 08:35,16/Apr/19 09:32,14/Jul/23 05:53,18/Sep/13 12:27,1.2.10,,,,,,0,,,,,"{{Collection}} is an inner class of {{CQL3Type}} that represents column types that can contain several values (lists, sets, maps). Its {{toString()}} is very helpful: it generates a string representation of type that can be used for generating of {{CREATE TABLE}} statement. 

Unfortunately this method works incorrectly for maps. Instead of returning something like {{map<text, int>}} it returns {{set<text, int>}}.

Here is the appropriate code fragment:

{code:title=CQL3Type$Collection.java|borderStyle=solid}
       public String toString()
        {
            switch (type.kind)
            {
                case LIST:
                    return ""list<"" + ((ListType)type).elements.asCQL3Type() + "">"";
                case SET:
                    return ""set<"" + ((SetType)type).elements.asCQL3Type() + "">"";
                case MAP:
                    MapType mt = (MapType)type;
                    return ""set<"" + mt.keys.asCQL3Type() + "", "" + mt.values.asCQL3Type() + "">"";
            }
            throw new AssertionError();
        }
{code}


The obvious bug is here:
{code:java}
                case MAP:
                    MapType mt = (MapType)type;
                    return ""set<"" + mt.keys.asCQL3Type() + "", "" + 
{code}

It should be {{""map<"" + ...}} instead of {{""set<"" + ...}}",,alexander_radzin,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,300,300,,0%,300,300,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349143,,,Wed Sep 18 12:27:48 UTC 2013,,,,,,,,,,"0|i1o7an:",349441,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"18/Sep/13 12:27;slebresne;Fixed in commit fc9b3a7. Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"'Internal application error' on SELECT .. WHERE col1=val AND col2 IN (1,2)",CASSANDRA-6050,12669202,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,nsv,nsv,18/Sep/13 08:02,16/Apr/19 09:32,14/Jul/23 05:53,19/Sep/13 10:36,2.0.1,,,,,,0,cql3,,,,"Query with error: SELECT * FROM user WHERE login='nsv' AND st IN ('1','2') ALLOW FILTERING;

Query works:
SELECT * FROM user WHERE login='nsv' AND st IN ('1') ALLOW FILTERING;
-- Single item inside IN

Table definition: 
CREATE COLUMNFAMILY user (
     KEY uuid PRIMARY KEY,
     name text,
     avatar text,
     email text,
     phone text,
     login text,
     pw text,
     st text
);

From /var/log/cassandra/output.log:
ERROR 11:58:52,454 Internal error processing execute_cql3_query
java.lang.AssertionError
	at org.apache.cassandra.cql3.statements.SelectStatement.getIndexExpressions(SelectStatement.java:749)
	at org.apache.cassandra.cql3.statements.SelectStatement.getRangeCommand(SelectStatement.java:303)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:155)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:101)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:117)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:108)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1920)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4372)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4356)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)

","cqlsh, pdo_cassandra",aleksey,nsv,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/13 13:18;slebresne;6050.txt;https://issues.apache.org/jira/secure/attachment/12603819/6050.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349134,,,Thu Sep 19 10:36:31 UTC 2013,,,,,,,,,,"0|i1o78n:",349432,2.0.0,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"18/Sep/13 13:18;slebresne;We've apparently broken validation in 2.0 (1.2 is not affected): we don't support IN on non-primary key columns, even when there is an index (the only reason this work with only 1 element in the IN is that the code don't distinguish that from a EQ; that's probably a mistake in the first place, but for compatibility sake I think we should leave it that way). Attaching patch that fix the validation (I pushed a dtests too).;;;","18/Sep/13 19:14;aleksey;+1;;;","19/Sep/13 10:36;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memory leak when using snapshot repairs,CASSANDRA-6047,12669133,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,jblangston@datastax.com,jblangston@datastax.com,17/Sep/13 21:47,16/Apr/19 09:32,14/Jul/23 05:53,18/Sep/13 19:27,1.2.10,2.0.1,,,,,0,,,,,"Running nodetool repair repeatedly with the -snapshot parameter results in a native memory leak. The JVM process will take up more and more physical memory until it is killed by the Linux OOM killer.

The command used was as follows:

nodetool repair keyspace -local -snapshot -pr -st start_token -et end_token

Removing the -snapshot flag prevented the memory leak.  The subrange repair necessitated multiple repairs, so it made the problem noticeable, but I believe the problem would be reproducible even if you ran repair repeatedly without specifying a start and end token.

Notes from [~yukim]:

Probably the cause is too many snapshots. Snapshot sstables are opened during validation, but memories used are freed when releaseReferences called. But since snapshots never get marked compacted, memories never freed.

We only cleanup mmap'd memories when sstable is mark compacted. https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/io/sstable/SSTableReader.java#L974

Validation compaction never marks snapshots compacted.
",,jblangston@datastax.com,jjordan,mbulman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/13 16:05;yukim;6047-1.2.txt;https://issues.apache.org/jira/secure/attachment/12603851/6047-1.2.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,349065,,,Wed Sep 18 19:27:25 UTC 2013,,,,,,,,,,"0|i1o6tb:",349363,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"18/Sep/13 16:05;yukim;Patch attached to let SSTableReader implement Closeable and do clean up at #close. Validation compaction against snapshot calls #close at the end of validation.;;;","18/Sep/13 17:52;jbellis;+1;;;","18/Sep/13 19:27;yukim;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in Tracing,CASSANDRA-6041,12669008,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,17/Sep/13 13:01,16/Apr/19 09:32,14/Jul/23 05:53,17/Sep/13 13:34,2.0.1,,,,,,0,,,,,"Tracing in 2.0.0 can throw the following:
{noformat}
ERROR [TracingStage:1] 2013-09-17 14:38:41,486 CassandraDaemon.java (line 185) Exception in thread Thread[TracingStage:1,5,main]
java.lang.AssertionError: Added column does not sort as the last column
        at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:115)
        at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:116)
        at org.apache.cassandra.tracing.Tracing.addParameterColumns(Tracing.java:101)
        at org.apache.cassandra.tracing.Tracing$2.runMayThrow(Tracing.java:210)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)

{noformat}

That's because addParamaterColumns will insert cell with names we don't know in advance, so we can't use ArrayBackedSortedColumns. Attaching patch to switch to TreeMapBackedSortedColumns.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/13 13:10;slebresne;0002-Fix-AssertionError-in-Tracing.txt;https://issues.apache.org/jira/secure/attachment/12603589/0002-Fix-AssertionError-in-Tracing.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348940,,,Tue Sep 17 13:34:34 UTC 2013,,,,,,,,,,"0|i1o61r:",349238,,,,,,,,,aleksey,,aleksey,Low,,2.0.0,,,,,,,,,,,,,,,,"17/Sep/13 13:10;slebresne;I'll note that it would be possible technically to keep ArrayBackedSortedColumns, but we'd need to ensure addParameterColumns is a SortedMap with the correct order. So not sure it's worth bothering, especially since it feels using ABSC is a bit error prone in that code.;;;","17/Sep/13 13:23;aleksey;+1;;;","17/Sep/13 13:34;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paging filter empty rows a bit too agressively,CASSANDRA-6040,12669007,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,17/Sep/13 12:58,16/Apr/19 09:32,14/Jul/23 05:53,17/Sep/13 13:37,2.0.1,,,,,,0,,,,,See the attached patch.,,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/13 12:59;slebresne;0001-Correctly-filter-empty-rows-during-paging.txt;https://issues.apache.org/jira/secure/attachment/12603586/0001-Correctly-filter-empty-rows-during-paging.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348939,,,Tue Sep 17 13:37:21 UTC 2013,,,,,,,,,,"0|i1o61j:",349237,,,,,,,,,aleksey,,aleksey,Normal,,2.0.0,,,,,,,,,,,,,,,,"17/Sep/13 13:31;aleksey;+1;;;","17/Sep/13 13:37;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Parens around WHERE condition break query,CASSANDRA-6037,12668957,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,nsv,nsv,17/Sep/13 07:32,16/Apr/19 09:32,14/Jul/23 05:53,20/Sep/13 14:50,1.2.11,2.0.2,,,,,0,cql3,,,,"SELECT * FROM user WHERE (key=<UUID>);
Bad Request: line 1:25 no viable alternative at input '('

SELECT * FROM user WHERE key=<UUID>; -- No parens
-- Normal output

The example provided is minimal, bug was discovered with AND logic on indexed columns.

Parens-enclosed conditions is good SQL and so is produced by database abstraction layers in complex queries to avoid operation precedence problems.

Fixing this at application side is no option - this will open the can of logic bugs.","cqlsh, pdo_cassandra",dbrosius,nsv,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Sep/13 05:29;dbrosius;6037.txt;https://issues.apache.org/jira/secure/attachment/12604200/6037.txt",,,,,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348889,,,Fri Sep 20 14:50:55 UTC 2013,,,,,,,,,,"0|i1o5qf:",349187,,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,,"20/Sep/13 05:33;dbrosius;being as the conditions are 'and' conditions, probably not that important, but harmless.;;;","20/Sep/13 14:27;slebresne;+1;;;","20/Sep/13 14:50;dbrosius;committed as a0fa69715f7913804fbd55c1280e0d35edd3bf0f to cassandra-1.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepared statements are broken in native protocol v2,CASSANDRA-6035,12668840,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,blair,blair,16/Sep/13 18:48,16/Apr/19 09:32,14/Jul/23 05:53,17/Sep/13 13:48,2.0.1,,,,,,0,,,,,"Pulling this ticket over from: https://datastax-oss.atlassian.net/browse/JAVA-177

Using Java Driver 2.0.0-beta1, this fails:

{code}
Session#prepareStatement(""CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3}"")
{code}

This works with Java Driver 1.0.3.

The client side stack:

{code}
Caused by: com.datastax.driver.core.exceptions.DriverInternalError: An unexpected error occured server side on /127.0.0.3: java.lang.ArrayIndexOutOfBoundsException: 30
        at com.datastax.driver.core.Responses$Error.asException(Responses.java:85)
        at com.datastax.driver.core.Session.toPreparedStatement(Session.java:281)
        at com.datastax.driver.core.Session.prepare(Session.java:187)
{code}

The server side stack:

{code}
 INFO [FlushWriter:16] 2013-09-14 00:41:02,882 Memtable.java (line 422) Completed flushing /home/blair/.ccm/cassandra-2.0.0-168-gd2c67a1/node3
/data/system/local/system-local-jb-127-Data.db (80 bytes) for commitlog position ReplayPosition(segmentId=1379132442754, position=712508)
 INFO [CompactionExecutor:69] 2013-09-14 01:20:42,701 AutoSavingCache.java (line 250) Saved KeyCache (325 items) in 255 ms
 INFO [CompactionExecutor:70] 2013-09-14 05:20:42,665 AutoSavingCache.java (line 250) Saved KeyCache (325 items) in 218 ms
 INFO [CompactionExecutor:71] 2013-09-14 09:20:42,678 AutoSavingCache.java (line 250) Saved KeyCache (325 items) in 217 ms
ERROR [Native-Transport-Requests:3009] 2013-09-14 10:53:38,724 ErrorMessage.java (line 222) Unexpected exception during request
java.lang.ArrayIndexOutOfBoundsException: 30
        at org.jboss.netty.buffer.BigEndianHeapChannelBuffer.setInt(BigEndianHeapChannelBuffer.java:98)
        at org.jboss.netty.buffer.AbstractChannelBuffer.writeInt(AbstractChannelBuffer.java:422)
        at org.apache.cassandra.cql3.ResultSet$Metadata$Codec.encode(ResultSet.java:368)
        at org.apache.cassandra.cql3.ResultSet$Metadata$Codec.encode(ResultSet.java:320)
        at org.apache.cassandra.transport.messages.ResultMessage$Prepared$1.encode(ResultMessage.java:261)
        at org.apache.cassandra.transport.messages.ResultMessage$Prepared$1.encode(ResultMessage.java:239)
        at org.apache.cassandra.transport.messages.ResultMessage$1.encode(ResultMessage.java:49)
        at org.apache.cassandra.transport.messages.ResultMessage$1.encode(ResultMessage.java:39)
        at org.apache.cassandra.transport.Message$ProtocolEncoder.encode(Message.java:279)
        at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.doEncode(OneToOneEncoder.java:66)
        at org.jboss.netty.handler.codec.oneone.OneToOneEncoder.handleDownstream(OneToOneEncoder.java:59)
        at org.jboss.netty.handler.execution.ExecutionHandler.handleDownstream(ExecutionHandler.java:186)
        at org.jboss.netty.channel.Channels.write(Channels.java:704)
        at org.jboss.netty.channel.Channels.write(Channels.java:671)
        at org.jboss.netty.channel.AbstractChannel.write(AbstractChannel.java:248)
        at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:311)
        at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:43)
        at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:67)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{code}
",Ubuntu 13.04 with JDK 7u40 using ccm and the the cassandra-2.0 branch at d2c67a1cb95264e8a839bdcfc0a727c892f1fc1d.,aleksey,blair,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/13 07:07;slebresne;6035.txt;https://issues.apache.org/jira/secure/attachment/12603556/6035.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348774,,,Tue Sep 17 13:47:06 UTC 2013,,,,,,,,,,"0|i1o50v:",349072,2.0.1,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"17/Sep/13 07:07;slebresne;That's, well, a typo. Attaching trivial patch to fix.;;;","17/Sep/13 09:05;slebresne;PS: that does mean prepared statement are kind of broken with the v2 protocol. So updating the description to reflect that.;;;","17/Sep/13 10:20;slebresne;Actually, this doesn't affect 2.0.0 (I was pretty surprised since I'm sure I've tested prepared statement there), it's just there on the current 2.0 branch.;;;","17/Sep/13 11:22;aleksey;+1;;;","17/Sep/13 13:47;aleksey;Anonymous people, please stop moving issues to Testing.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL: allow names for bind variables,CASSANDRA-6033,12668796,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,16/Sep/13 15:58,16/Apr/19 09:32,14/Jul/23 05:53,19/Sep/13 10:59,2.0.1,,,,,,0,,,,,"Currently bind variables are ""anonymous"", they're just a question mark. What this means is that the only reliable way to reference those variables after preparation is through position their position in the query string, which is not excessively user friendly.

Of course some driver may be tempted to add their own version of named variables, but that forces said driver to parse the query string in the first place, which is something we've tried to avoid so far. Besides, this is useful enough that making it part of CQL would make this more consistent amongst driver rather than having everyone coming up with its own syntax.

I'll add that because we are already sending column names in the metadata, I believe we can support this without any change to the protocol. The idea would be to support queries like this (happy to discuss the exact syntax):
{noformat}
SELECT * FROM test WHERE key = ?my_key AND time > ?t_low AND time <= ?t_high
{noformat}

From the Cassandra side, the only thing that this would change is that in the result set returned to the client, the column names would be 'my_key', 't_low' and 't_high' respectively rather than 'key', 'time' and 'time' as they are now.  And so in particular using an anymous variable would be equivalent to using a name one with the name of the CQL column the variable is bound to.

Driver side, the driver would just have to keep a map of each name to their position in the metadata to provide reliable setter by names.
",,aleksey,jjordan,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/13 08:58;slebresne;6033.txt;https://issues.apache.org/jira/secure/attachment/12604011/6033.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348730,,,Thu Sep 19 10:59:45 UTC 2013,,,,,,,,,,"0|i1o4r3:",349028,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"17/Sep/13 22:32;jbellis;I've already wished for this a few times, but to play devil's advocate, isn't it a bit late to change this now that we have multiple drivers in production?  I submit that users have already made their peace with this.;;;","18/Sep/13 07:14;slebresne;I've seen at least a few users asked for something like this on the driver side, so not totally sure everyone is fully happy with the status quo. I mean, if I was convinced that everyone would say ""well, that's not supported, fair enough, I'll do without it, no biggy"", then I'd agree with you. But my fear is that people will be annoyed and start messing with query strings to try doing the equivalent of this. I'm afraid of drivers doing it all in their own way and being made more fragile by having to parse/search-and-replace the query string.

Do I wish we'd have done that sooner: sure. But typically my suggestion would be to do that for the protocol v2 only, i.e. change the spec of the protocol to say that the name in the result set is not the name of the CQL3 column, but rather the name of the bind variables. Which by default would be the CQL3 column name. We can do that spec change now, in 2.0.1, so client drivers can know what to expect. I strongly doubt anyone is in production with C* 2.0.0 and the native protocol v2 (if only because it's buggy in 2.0.0: CASSANDRA-6040). Then we can ship the syntax change in say 2.0.2: it's not a very complicated change to make.;;;","18/Sep/13 08:06;slebresne;Btw, let me add that the name we currently send in the metadata as result of a PREPARE is already not always a proper CQL3 column name. For instance, if you prepare:
{noformat}
UPDATE foo USING TTL ? SET m[?] = ? WHERE k = 0
{noformat}
then the bind variables will be respectively named '[ttl]', 'key(m)' and 'value(m)'.

So existing drivers already cannot really assume that the name returned is a true CQL3 column name. Just to say that I doubt we'll break any existing driver by making this change, if that is the fear.
;;;","18/Sep/13 15:45;slebresne;Attaching patch for this, with the small variation that the final syntax uses :name rather than ?name as that's a lot more common.

The CQL doc and protocol spec would probably need some updating but I'll do that upon commit if we're good on the patch.;;;","18/Sep/13 20:26;aleksey;Can you update for CASSANDRA-4210? I've ghetto-rebased with apply --reject, but that's not enough.;;;","19/Sep/13 08:58;slebresne;Rebased patch attached;;;","19/Sep/13 09:27;aleksey;+1;;;","19/Sep/13 10:59;slebresne;Committed (with doc updates), thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Default sstable output size not respected when running sstablesplit,CASSANDRA-6028,12668519,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,dmeyer,dmeyer,13/Sep/13 17:57,16/Apr/19 09:32,14/Jul/23 05:53,13/Sep/13 18:54,,,,,,,0,,,,,"Run sstablesplit -h:

usage: sstablessplit [options] <filename> [<filename>]*
--
Split the provided sstables files in sstables of maximum provided file
size (see option --size).
--
Options are:
    --debug         display stack traces
 -h,--help          display this help message
    --no-snapshot   don't snapshot the sstables before splitting
 -s,--size <size>   maximum size in MB for the output sstables (default:
                    50)
 -v,--verbose       verbose output

From the help message we expect the default size to be 50 MB.

repro steps:

supply any sstable of any size to sstablesplit but leave out the --size option.
Expected: should split sstable into 50MB sstables
Actual:
Pre-split sstables snapshotted into snapshot pre-split-1379086801768
Error splitting SSTableReader(path='./Keyspace1/Standard1/Keyspace1-Standard1-ic-15-Data.db'): Invalid target size for SSTables, must be > 0 (got: 0)","java version ""1.7.0_40""
Java(TM) SE Runtime Environment (build 1.7.0_40-b43)
Java HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)",dmeyer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348453,,,Fri Sep 13 18:54:49 UTC 2013,,,,,,,,,,"0|i1o31b:",348750,1.2.9,2.0.0,,,,,,,,,,Low,,,,,,,,,,,,,,,dmeyer,,,"13/Sep/13 18:54;brandon.williams;Fixed in b281dd1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
'null' error when running sstablesplit on valid sstable,CASSANDRA-6027,12668517,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,dmeyer,dmeyer,13/Sep/13 17:49,16/Apr/19 09:32,14/Jul/23 05:53,17/Sep/13 13:34,2.0.1,,,,,,0,,,,,"ccm create --cassandra-version git:cassandra-2.0 test
ccm populate -n 1
ccm start

ccm node1 stress -n 10000000 -o insert
ccm node1 compact
cd ~/.ccm/test/node1/data
../bin/sstablesplit -s 100 ./Keyspace1/Standard1/Keyspace1-Standard1-jb-12-Data.db

Expected: single sstable should be split into multiple sstables

Got:
Pre-split sstables snapshotted into snapshot pre-split-1379088385051
Error splitting SSTableReader(path='./Keyspace1/Standard1/Keyspace1-Standard1-jb-12-Data.db'): null

running du -h on the large compacted sstable showed it to be 2.4GB

This is probably related to CASSANDRA-6026; however, it is different.  In this bug the split does not occur, whereas in 6026 the split does occur though an error is thrown.","Environment: java version ""1.7.0_40""
Java(TM) SE Runtime Environment (build 1.7.0_40-b43)
Java HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)",aleksey,dmeyer,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Sep/13 09:08;slebresne;6027.txt;https://issues.apache.org/jira/secure/attachment/12603572/6027.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348451,,,Tue Sep 17 13:34:09 UTC 2013,,,,,,,,,,"0|i1o30v:",348748,2.0.0,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,dmeyer,,,"17/Sep/13 11:24;aleksey;+1 (<SSTableReader> is not needed here though).;;;","17/Sep/13 13:34;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when running sstablesplit on valid sstable,CASSANDRA-6026,12668515,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,dmeyer,dmeyer,13/Sep/13 17:45,16/Apr/19 09:32,14/Jul/23 05:53,17/Sep/13 07:12,1.2.10,,,,,,0,,,,,"#create cluster
ccm create --cassandra-version git:cassandra-1.2 test
ccm populate -n 1
ccm start

#run stress
ccm node1 stress -n 10000000 -o insert
ccm node1 compact

cd ~/.ccm/test/node1/data
../bin/sstablesplit -n 100 ./Keyspace1/Standard1/Keyspace1-Standard1-ic-16-Data.db

#Expected
single large sstable should be split into multiple sstables with max size 100 MB

#Actual
ERROR 10:14:06,992 Error in ThreadPoolExecutor
java.lang.NullPointerException
        at org.apache.cassandra.io.sstable.SSTableDeletingTask.run(SSTableDeletingTask.java:70)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)


Notes: It seems like the split occurs and can be recompacted.
Last known commit where split was working on 1.2 branch: 47b2cd6620894bf0c4c4584036eab49a2e14a50e
Have not bisected further.

sstablesplit is also broken on 2.0 branch; however, it fails differently.  Filing separate bug on that.
","Environment: java version ""1.7.0_40""
Java(TM) SE Runtime Environment (build 1.7.0_40-b43)
Java HotSpot(TM) 64-Bit Server VM (build 24.0-b56, mixed mode)

(Sun jvm on mac)",dmeyer,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/13 08:31;slebresne;6026.txt;https://issues.apache.org/jira/secure/attachment/12603303/6026.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348449,,,Tue Sep 17 07:12:51 UTC 2013,,,,,,,,,,"0|i1o30f:",348746,1.2.10,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,dmeyer,,,"13/Sep/13 17:59;dmeyer;This issue is probably related to CASSANDRA-6027; however, the actual behavior is different so two bugs filed.;;;","13/Sep/13 18:57;dmeyer;Will try to find the exact commit that introduced this error.  should have it this afternoon...;;;","13/Sep/13 22:07;dmeyer;f663a996c799bd93963a50b418ed5fcde2e48a40 is the first bad commit
commit f663a996c799bd93963a50b418ed5fcde2e48a40
Author: Aleksey Yeschenko <aleksey@apache.org>
Date:   Thu Sep 12 20:35:31 2013 +0300

    Add SSTableDeletingNotification to DataTracker

    patch by Piotr Kołaczkowski; reviewed by Aleksey Yeschenko for
    CASSANDRA-6010

:100644 100644 1c0958922018c8f05258ea6049617d5609466800 51be09daa65bd271ee9942dc97ee1547205049d4 M	CHANGES.txt
:040000 040000 a2dec4c205cfe751271801de219082d9551f2e9b 167f3f08984fa34dd35a1cbaf6ef13bde2f9d228 M	src;;;","16/Sep/13 08:31;slebresne;So, this is because CASSANDRA-6010 forgot to protect against the fact where the tracker was not set in SSTableDeletingTask (which is the case here). Trivial patch attached.

However CASSANDRA-6010 is marked as fixed in 1.2.10, so either the fix version is wrong there or this ticket lies about having reproduced in 1.2.9 :);;;","16/Sep/13 18:21;brandon.williams;+1;;;","16/Sep/13 19:07;dmeyer;+1 patch verified.;;;","17/Sep/13 07:12;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deleted row resurrects if was not compacted in GC Grace Timeout due to thombstone read optimization in CollactionController,CASSANDRA-6025,12668481,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,m0nstermind,m0nstermind,13/Sep/13 13:40,16/Apr/19 09:32,14/Jul/23 05:53,19/Sep/13 22:44,2.0.1,,,,,,0,,,,,"How to reproduce:
1. Insert column
2. Flush, so you'll have sstable-1
3. Delete just inserted column
4. Flush, now you have sstable-2 as well
5. Left it uncompacted for more then gc grace time or just use 0, so you dont have to wait
6. Read data form column. You'll read just deleted column


{code}
            /* add the SSTables on disk */
// This sorts sstables in the order sstable-2, sstable-1
            Collections.sort(view.sstables, SSTable.maxTimestampComparator);
//...
            for (SSTableReader sstable : view.sstables)
            {
//...
                if (iter.getColumnFamily() != null)
//...
                    while (iter.hasNext())
                    {
                        OnDiskAtom atom = iter.next();
// the problem is here. reading atom after gc grace time
// makes this condition false. so tombstone from sstable-2
// is not placed to temp container and is just thrown away.
// On next iteration of outer for statement an original
// data inserted in step 1 from sstable-1 will be read and
// placed to temp.
                        if (atom.getLocalDeletionTime() >= gcBefore)
                            temp.addAtom(atom);
//
                    }

// .. so at the end of the for statemet we resolve data from temp. which
// do not have tombstone at all -> data are resurrected.
               container.addAll(temp, HeapAllocator.instance);
 
}
{code}
",,m0nstermind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/13 13:43;m0nstermind;6025.diff;https://issues.apache.org/jira/secure/attachment/12603011/6025.diff",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348415,,,Thu Sep 19 22:44:14 UTC 2013,,,,,,,,,,"0|i1o2sv:",348712,,,,,,,,,jbellis,,jbellis,Normal,,2.0.0,,,,,,,,,,,,,,,,"13/Sep/13 13:43;m0nstermind;Fixed it by just deleting this condition. It seems that it is there by intention of minor optimization.
;;;","19/Sep/13 22:44;jbellis;This is caused by CASSANDRA-5577.  I've reverted that and added a test case to catch the problem.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAS should distinguish promised and accepted ballots,CASSANDRA-6023,12668469,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,13/Sep/13 11:48,25/Oct/19 13:10,14/Jul/23 05:53,16/Sep/13 06:48,2.0.1,,,Feature/Lightweight Transactions,,,0,LWT,,,,"Currently, we only keep 1) the most recent promise we've made and 2) the last update we've accepted. But we don't keep the ballot at which that last update was accepted. And because a node always promise to newer ballot, this means an already committed update can be replayed even after another update has been committed. Re-committing a value is fine, but only as long as we've not start a new round yet.

Concretely, we can have the following case (with 3 nodes A, B and C) with the current implementation:
* A proposer P1 prepare and propose a value X at ballot t1. It is accepted by all nodes.
* A proposer P2 propose at t2 (wanting to commit a new value Y). If say A and B receive the commit of P1 before the propose of P2 but C receives those in the reverse order, we'll current have the following states:
{noformat}
A: in-progress = (t2, _), mrc = (t1, X)
B: in-progress = (t2, _), mrc = (t1, X)
C: in-progress = (t2, X), mrc = (t1, X)
{noformat}
Because C has received the t1 commit after promising t2, it won't have removed X during t1 commit (but note that the problem is not during commit, that example still stand if C never receive any commit message).
* Now, based on the promise of A and B, P2 will propose Y at t2 (C don't see this propose in particular, not before he promise on t3 below at least). A and B accepts, P2 will send a commit for Y.
* In the meantime a proposer P3 submit a prepare at t3 (for some other irrelevant value) which reaches C before it receives P2 propose&commit. That prepare reaches A and B too, but after the P2 commit. At that point the state will be:
{noformat}
A: in-progress = (t3, _), mrc = (t2, Y)
B: in-progress = (t3, _), mrc = (t2, Y)
C: in-progress = (t3, X), mrc = (t2, Y)
{noformat}
In particular, C still has X as update because each time it got a commit, it has promised to a more recent ballot and thus skipped the delete. The value is still X because it has received the P2 propose after having promised t3 and has thus refused it.
* P3 gets back the promise of say C and A. Both response has t3 as in-progress ballot (and it is more recent than any mrc) but C comes with value X. So P3 will replay X. Assuming no more contention this replay will succeed and X will be committed at t3.

At the end of that example, we've comitted X, Y and then X again, even though only P1 has ever proposed X.

I believe the correct fix is to keep the ballot of when an update is accepted (instead of using the most recent promised ballot). That way, in the example above, P3 would receive from C a promise on t3, but would know that X was accepted at t1. And so P3 would be able to ignore X since the mrc of A will tell him it's an obsolete value.
",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/13 12:03;slebresne;0001-Distinguish-between-promised-and-accepted-ballots.txt;https://issues.apache.org/jira/secure/attachment/12603003/0001-Distinguish-between-promised-and-accepted-ballots.txt","13/Sep/13 12:03;slebresne;0002-Populate-commitsByReplica-in-PrepareCallback.txt;https://issues.apache.org/jira/secure/attachment/12603004/0002-Populate-commitsByReplica-in-PrepareCallback.txt",,,,,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348403,,,Mon Sep 16 06:48:16 UTC 2013,,,,,,,,,,"0|i1o2q7:",348700,,,,,,,,,jbellis,,jbellis,Normal,,2.0.0,,,,,,,,,,,,,,,,"13/Sep/13 12:03;slebresne;Attaching patch for the suggestion above. The patch also simplify slightly SK.savePaxosCommit: we used to not erase the update if the commit was older than in-progress. I believe that was a bit buggy and in any case unecessary since we write with the commit timestamp (so that there was no risk to erase a more recent update in fact). The other thing we were doing is to update the in-progress ballot if the commit was newer: I'm not sure that has any benefit and it makes me nervous to update in-progress outside of the prepare phase. Besides, if we remove that, we don't need to read the state to commit, which save a read and the lock acquisition on every commit.

I'm including a 2nd trivial patch that adds the population of commitsByReplica in PrepareCallback. It's partly unrelated to the problem of this ticket but it's clearly wrong and I'm not sure that warrant a separate ticket.
;;;","14/Sep/13 21:02;jbellis;I'm pretty sure we don't need to make everything volatile/concurrent in PrepareCallback, since the latch and synchronized establish happens-before for the StorageProxy thread.

I'm not crazy about PrepareResponse including two different values for inProgressCommit.  Can we clean that up somehow?

Might also be worth renaming in_progress_ballot to promised_ballot to be a little more clear on the distinction vs proposal_ballot.

Rest LGTM.;;;","15/Sep/13 13:11;jbellis;... Thinking about it more, I'm fine with the PrepareResponse, and it's not worth the backwards compatibility code for a column rename.  +1;;;","16/Sep/13 06:48;slebresne;Ok. Committed then, thanks.

(I did remove the volatile though. I got a ConcurrentModificationException during one run on commitsByReplica and instead of just fixing that I went overboard with the volatile. But you're right, the latch makes it unnecessary).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL identifier regex in documentation is wrong, (or cqlsh doesn't implement correctly)",CASSANDRA-6019,12668344,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,enigmacurry,enigmacurry,12/Sep/13 20:04,16/Apr/19 09:32,14/Jul/23 05:53,01/Apr/15 15:31,,,,Legacy/Documentation and Website,,,0,cql,,,,"The [CQL docs|http://cassandra.apache.org/doc/cql3/CQL.html#identifiers] state that a CQL identifier can be any characters [a-zA-Z0-9_]*. ,but in fact, they cannot start with an '_' or a number.

Empirically, it looks like the regex should be [a-zA-Z][a-zA-Z0-9_].*",,enigmacurry,matthewadams,philipthompson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348278,,,Wed Apr 01 15:31:13 UTC 2015,,,,,,,,,,"0|i1o1yn:",348574,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"01/Apr/15 15:31;philipthompson;This has been fixed in the docs file in the code for a while. The html merely needed regenerated.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update version in build.xml to reflect 2.1,CASSANDRA-6015,12668295,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,enigmacurry,enigmacurry,12/Sep/13 16:05,16/Apr/19 09:32,14/Jul/23 05:53,12/Sep/13 16:16,,,,,,,0,,,,,"The build.xml still marks the base.version as 2.0.0 in trunk.

This [breaks ccm|https://github.com/pcmanus/ccm/issues/73], as it's checking for this version number when it's doing things with 2.1 features (like logback logging, instead of log4j)
",,enigmacurry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348229,,,Thu Sep 12 16:16:02 UTC 2013,,,,,,,,,,"0|i1o1nr:",348525,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"12/Sep/13 16:16;brandon.williams;Done.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAS may return false but still commit the insert,CASSANDRA-6013,12668285,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,slebresne,slebresne,12/Sep/13 15:31,25/Oct/19 13:11,14/Jul/23 05:53,16/Sep/13 06:40,2.0.1,,,Feature/Lightweight Transactions,,,0,LWT,,,,"If a Paxos proposer proposes some value/update and that propose fail, there is no guarantee on whether this value will be accepted or not ultimately. Paxos guarantees that we'll agree on ""a"" value (for a given round in our case), but does not guarantee that the proposer of the agreed upon value will know it.  In particular, if for a given proposal at least one accepter has accepted it but not a quorum does, then that value might (but that's not guaranteed either) be replayed (and committed) by another proposer.

Currently, if a proposer A proposes some update U but it is rejected, A will sleep a bit and retry U. But if U was accepted by at least one acceptor, some other proposer B might replay U, succeed and commit it. If A does its retry after that happens, he will prepare, check the condition, and probably find that the conditions don't apply anymore since U has been committed already. It will thus return false, even though U has been in fact committed.

Unfortunately I'm not sure there is an easy way for a proposer whose propose fails to know if the update will prevail or not eventually. Which mean the only acceptable solution I can see would be to return to the user ""I don't know"" (through some exception for instance). Which is annoying because having a proposal rejected won't be an extremely rare occurrence, even with relatively light contention, and returning ""I don't know"" often is a bit unfriendly.",,aleksey,alexliu68,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/13 15:53;slebresne;6013-v2.txt;https://issues.apache.org/jira/secure/attachment/12603033/6013-v2.txt","14/Sep/13 12:24;jbellis;6013-v3.txt;https://issues.apache.org/jira/secure/attachment/12603192/6013-v3.txt","14/Sep/13 13:05;slebresne;6013-v4.patch;https://issues.apache.org/jira/secure/attachment/12603193/6013-v4.patch","13/Sep/13 07:25;jbellis;6013.txt;https://issues.apache.org/jira/secure/attachment/12602973/6013.txt",,,,,,,,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348219,,,Mon Sep 16 06:40:48 UTC 2013,,,,,,,,,,"0|i1o1lr:",348515,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"12/Sep/13 17:48;jbellis;bq. if for a given proposal at least one accepter has accepted it but not a quorum does, then that value might (but that's not guaranteed either) be replayed (and committed) by another proposer

Why not have the new leader require a quorum of replicas to say ""I have this unfinished business"" before replaying it?

(I'm pretty sure I had this logic in originally but you talked me out of it in the name of code simplification.);;;","13/Sep/13 07:25;jbellis;Okay, so the problem is not the retry per se, but when we have a ""split decision"" on the nodes that reply.  We can reduce the likelihood of that happening by waiting for all known live endpoints if that's required to hear from a majority.

If we still don't hear from a majority, we can return a timeout; it's valid for a transaction to be committed after a timeout.

Patch for the above attached.;;;","13/Sep/13 15:53;slebresne;Unfortunately, I think this is a little grimmer than that. The problem is that a proposer shouldn't move on unless the propose was successful (in which case it returns to the client) or it is sure that the propose will *not* be replayed (if it is sure of that, then retrying the proposed value with a newer ballot is safe; the current problem is that we retry with a newer ballot when we're not sure of that). In other words, we should timeout unless we are either successful or all nodes have answered and none have accepted. I'm attaching a v2 doing that (but still tries to timeout as little as possible without compromising correctness).

Unfortunately, this mean we'll timeout as soon as a proposer gets a propose reject but at least one acceptor had accepted it, which is not an extremely rare condition even with moderate contention. That being said, the current behavior is plain wrong, so unless someone has a much better idea that is easy to implement, we should probably go ahead with this for now.
;;;","14/Sep/13 12:23;jbellis;It looks to me like both uses of requiredTargets should actually be totalTargets.  v3 attached.;;;","14/Sep/13 13:05;slebresne;I don't follow. getSuccessful/getAcceptCount is supposed to returned how many successful accepts we got. So that's how much time we decremented remainingRequired, i.e. its initial value (requiredTargets) minus it's current value. Similarly, in isFullyRefused, we want to validate that remainingRequired was never decremented (no-one accepted), so we want to compare it's current value with its initial value, requiredTargets (comparing to totalTargets will in fact always fail).

I guess the code is more straightforward if we keep the number of accepts instead of the number of remaining accept: attaching v4 with that version (which is equivalent to v2, but with the updated comment of v3).
;;;","14/Sep/13 15:59;jbellis;+1;;;","16/Sep/13 06:40;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAS does not always correctly replay inProgress rounds,CASSANDRA-6012,12668284,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,12/Sep/13 15:29,25/Oct/19 13:11,14/Jul/23 05:53,13/Sep/13 12:32,2.0.1,,,Feature/Lightweight Transactions,,,0,LWT,,,,"Paxos says that on receiving the result of a prepare from a quorum of acceptors, the proposer should propose the value of the higher-number proposal accepted amongst the ones returned by the acceptors, and only propose his own value if no acceptor has send us back a previously accepted value.

But in PrepareCallback we only keep the more recent inProgress commit regardless of whether is has an update. Which means we could ignore a value already accepted by some acceptors if any of the acceptor send us a more recent ballot than the other acceptor but with no values. The net effect is that we can mistakenly accept two different values for the same round.
",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/13 15:46;slebresne;0001-Don-t-skip-paxos-old-round-replay-if-there-is-a-value-.txt;https://issues.apache.org/jira/secure/attachment/12602815/0001-Don-t-skip-paxos-old-round-replay-if-there-is-a-value-.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348218,,,Fri Sep 13 12:32:39 UTC 2013,,,,,,,,,,"0|i1o1lj:",348514,,,,,,,,,jbellis,,jbellis,Normal,,2.0.0,,,,,,,,,,,,,,,,"12/Sep/13 15:30;slebresne;Attaching fix: as far as checking if we should finish an inProgress round, we only need to keep the most recent inProgress commit that has a value. But so as to not break the optimization of CASSANDRA-5667, the patch also keep the most recent inProgress, regardless of whether it has a value or not.
;;;","12/Sep/13 21:44;jbellis;+1;;;","13/Sep/13 12:32;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in snapshot repair,CASSANDRA-6011,12668274,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,nickmbailey,nickmbailey,12/Sep/13 14:57,16/Apr/19 09:32,14/Jul/23 05:53,13/Sep/13 16:41,1.2.10,2.0.1,,,,,0,,,,,"When we do a snapshot/sequential repair, we use the repair session id as the snapshot name. Unfortunately in Directories.java when we delete a snapshot, we delete it for all column families, even when called on a specific cf store.

So what can happen is this:

Node B finishes validation compaction for CF1 and Notifies Node A
Node B *starts* to delete snapshot for CF1
Node A finishes repair of CF1 and starts repair of CF2
Node B takes snapshot of CF2 and starts validation compaction, but the previous validation compaction is still deleting snapshots, so the snapshot it wants to run a validation on gets deleted out from under it.

I've only reproduced on 1.2.6, but looking at the code this definitely looks like it exists in 1.2 HEAD. Not positive about 2.0.

I think the fix is just to update Directories.java to not delete the snapshot from all column families.",,mbulman,nickmbailey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/13 17:11;yukim;6011-1.2.txt;https://issues.apache.org/jira/secure/attachment/12602828/6011-1.2.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348208,,,Fri Sep 13 16:41:59 UTC 2013,,,,,,,,,,"0|i1o1jb:",348504,1.2.6,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"12/Sep/13 17:11;yukim;Patch attached to clear snapshot only for validated CF, not entire keyspace.;;;","12/Sep/13 18:31;jbellis;+1;;;","13/Sep/13 15:34;nickmbailey;if we end up having another 1.1 release, this is probably worth getting in there as well;;;","13/Sep/13 16:41;yukim;Committed, including 1.1 branch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migrate pre-2.0 key/value/column aliases to system.schema_columns,CASSANDRA-6009,12668231,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,ksaritek,ksaritek,12/Sep/13 10:53,16/Apr/19 09:32,14/Jul/23 05:53,18/Sep/13 12:15,2.0.1,,,Legacy/Tools,,,0,,,,,"upgrade cassandra from 1.2.6 to 1.2.9 first,
then upgrade from 1.2.9 to 2.0.0 as documented at cassandra upgrade doc

describe command is not giving table definition properly

cqlsh:datadb> DESCRIBE KEYSPACE demoks ;

CREATE KEYSPACE demoks WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '2'
};

USE demoks;

CREATE TABLE demodb (
  description text,
  symbol text,
list index out of range
  PRIMARY KEY (cqlsh:datadb>","cassandra 2.0, jdk 7",aleksey,ksaritek,slebresne,yeshvanthni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-8000,,,,,,,,,,,"17/Sep/13 17:51;aleksey;6009.txt;https://issues.apache.org/jira/secure/attachment/12603630/6009.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348165,,,Wed Sep 18 12:15:55 UTC 2013,,,,,,,,,,"0|i1o19r:",348461,2.0.0,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"12/Sep/13 16:58;aleksey;[~ksaritek] Need the original schema to reproduce this issue.;;;","13/Sep/13 07:31;ksaritek;here it is,
	
CREATE KEYSPACE demoks WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '1'
};

USE demoks;

CREATE TABLE demodb (
  id int,
  time text,
  price double,
  volume double,
  PRIMARY KEY (id, time)
) WITH compaction =
    { 'class' : 'LeveledCompactionStrategy',  'sstable_size_in_mb' : 10 };
;;;","13/Sep/13 19:49;aleksey;All right, this is legit. Fortunately, it's not a schema/migration issue. It's just cqlsh assuming a little too much. system.schema_columns doesn't have the values for clustering columns and partition key columns, if the tables are from pre-2.0 era.

DESCRIBE works fine with anything created in 2.0+, though.

Will fix.;;;","14/Sep/13 15:25;ksaritek;is there a workaround to get schema properly, like get snapshot and create table & keyspace then bulk import via sstable;;;","18/Sep/13 12:01;slebresne;+1 (but let's not commit in 2.1 since 2.0 will be an obligatory stop; startup is slow enough as it is)

[~ksaritek] Altering the tables on which cqlsh is unhappy should do the trick (you could alter the 'comment' property for instance).;;;","18/Sep/13 12:15;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replace the deprecated MapMaker with CacheLoader for compatibility with Guava 15.0+,CASSANDRA-6007,12668081,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,mpenet,mpenet,11/Sep/13 17:05,16/Apr/19 09:32,14/Jul/23 05:53,12/Sep/13 15:47,1.2.10,,,,,,1,,,,,"Attempting to load datastax/java-driver 2.0 beta1 and cassandra-all 2.0 in the same jvm causes some issues mainly because of clashes between guava versions  (15.0 in the driver vs 13.0.1 in c*). This makes automated testing using EmbeddedCassandraService problematic for instance.

Stacktrace from https://github.com/mpenet/alia/tree/2.0 running ""lein test""

Upgrading c* 2.0 to guava 15+ should help fix this issue. 

{code:java}
java.lang.IllegalAccessError: tried to access method com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; from class org.apache.cassandra.service.StorageProxy
	at org.apache.cassandra.service.StorageProxy.<clinit>(StorageProxy.java:87)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:190)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:447)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:426)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:344)
	at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:377)
	at org.apache.cassandra.service.EmbeddedCassandraService.start(EmbeddedCassandraService.java:52)
	at qbits.alia.test.embedded$start_service_BANG_.invoke(embedded.clj:20)
	at qbits.alia.test.embedded$eval10911.invoke(embedded.clj:24)
	at clojure.lang.Compiler.eval(Compiler.java:6619)
	at clojure.lang.Compiler.load(Compiler.java:7064)
	at clojure.lang.RT.loadResourceScript(RT.java:370)
	at clojure.lang.RT.loadResourceScript(RT.java:361)
	at clojure.lang.RT.load(RT.java:440)
	at clojure.lang.RT.load(RT.java:411)
	at clojure.core$load$fn__5018.invoke(core.clj:5530)
	at clojure.core$load.doInvoke(core.clj:5529)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invoke(core.clj:5336)
	at clojure.core$load_lib$fn__4967.invoke(core.clj:5375)
	at clojure.core$load_lib.doInvoke(core.clj:5374)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invoke(core.clj:619)
	at clojure.core$load_libs.doInvoke(core.clj:5413)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:621)
	at clojure.core$use.doInvoke(core.clj:5507)
	at clojure.lang.RestFn.invoke(RestFn.java:703)
	at qbits.alia.test.core$eval161$loading__4910__auto____162.invoke(core.clj:1)
	at qbits.alia.test.core$eval161.invoke(core.clj:1)
	at clojure.lang.Compiler.eval(Compiler.java:6619)
	at clojure.lang.Compiler.eval(Compiler.java:6608)
	at clojure.lang.Compiler.load(Compiler.java:7064)
	at clojure.lang.RT.loadResourceScript(RT.java:370)
	at clojure.lang.RT.loadResourceScript(RT.java:361)
	at clojure.lang.RT.load(RT.java:440)
	at clojure.lang.RT.load(RT.java:411)
	at clojure.core$load$fn__5018.invoke(core.clj:5530)
	at clojure.core$load.doInvoke(core.clj:5529)
	at clojure.lang.RestFn.invoke(RestFn.java:408)
	at clojure.core$load_one.invoke(core.clj:5336)
	at clojure.core$load_lib$fn__4967.invoke(core.clj:5375)
	at clojure.core$load_lib.doInvoke(core.clj:5374)
	at clojure.lang.RestFn.applyTo(RestFn.java:142)
	at clojure.core$apply.invoke(core.clj:619)
	at clojure.core$load_libs.doInvoke(core.clj:5413)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:619)
	at clojure.core$require.doInvoke(core.clj:5496)
	at clojure.lang.RestFn.applyTo(RestFn.java:137)
	at clojure.core$apply.invoke(core.clj:619)
	at user$eval85.invoke(NO_SOURCE_FILE:1)
	at clojure.lang.Compiler.eval(Compiler.java:6619)
	at clojure.lang.Compiler.eval(Compiler.java:6609)
	at clojure.lang.Compiler.eval(Compiler.java:6582)
	at clojure.core$eval.invoke(core.clj:2852)
	at clojure.main$eval_opt.invoke(main.clj:308)
	at clojure.main$initialize.invoke(main.clj:327)
	at clojure.main$null_opt.invoke(main.clj:362)
	at clojure.main$main.doInvoke(main.clj:440)
	at clojure.lang.RestFn.invoke(RestFn.java:421)
	at clojure.lang.Var.invoke(Var.java:419)
	at clojure.lang.AFn.applyToHelper(AFn.java:163)
	at clojure.lang.Var.applyTo(Var.java:532)
	at clojure.main.main(main.java:37)

{code}",,aleksey,ash2k,cburroughs,mpenet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Sep/13 11:30;aleksey;6007.txt;https://issues.apache.org/jira/secure/attachment/12602768/6007.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,348015,,,Thu Sep 12 15:47:41 UTC 2013,,,,,,,,,,"0|i1o0cf:",348311,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"12/Sep/13 05:54;ash2k;We use cassandra-maven-plugin 1.2.1, Astyanax 1.56.42 with cassandra jar version 1.2.5 (not possible to update due to [1] and [2]). With Guava 14.0.1 everything works fine, but if I update to Guava 15.0 then this exception happens when building project (integration tests fail):

{code}
[INFO ] 11:36:09.855 [main][] ERROR CassandraDaemon:430 - Exception encountered during startup
[INFO ] java.lang.IllegalAccessError: tried to access method com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; from class org.apache.cassandra.service.StorageProxy
[INFO ] 	at org.apache.cassandra.service.StorageProxy.<clinit>(StorageProxy.java:84) ~[cassandra-all-1.2.5.jar:1.2.5]
[INFO ] 	at java.lang.Class.forName0(Native Method) ~[na:1.7.0_25]
[INFO ] 	at java.lang.Class.forName(Class.java:190) ~[na:1.7.0_25]
[INFO ] java.lang.IllegalAccessError: tried to access method com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; from class org.apache.cassandra.service.StorageProxy
[INFO ] 	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:466) ~[cassandra-all-1.2.5.jar:1.2.5]
[INFO ] 	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:445) ~[cassandra-all-1.2.5.jar:1.2.5]
[INFO ] 	at org.apache.cassandra.service.StorageProxy.<clinit>(StorageProxy.java:84)
[INFO ] 	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:325) ~[cassandra-all-1.2.5.jar:1.2.5]
[INFO ] 	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:413) ~[cassandra-all-1.2.5.jar:1.2.5]
[INFO ] 	at java.lang.Class.forName0(Native Method)
[INFO ] 	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:456) ~[cassandra-all-1.2.5.jar:1.2.5]
[INFO ] 	at java.lang.Class.forName(Class.java:190)
[INFO ] 	at org.codehaus.mojo.cassandra.CassandraMonitor.main(CassandraMonitor.java:148) ~[cassandra-maven-plugin-1.2.1-1.jar:na]
[INFO ] 	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:466)
[INFO ] 	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:445)
[INFO ] Exception encountered during startup: tried to access method com.google.common.collect.MapMaker.makeComputingMap(Lcom/google/common/base/Function;)Ljava/util/concurrent/ConcurrentMap; from class org.apache.cassandra.service.StorageProxy
[INFO ] 	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:325)
[INFO ] 	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:413)
[INFO ] 	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:456)
[INFO ] 	at org.codehaus.mojo.cassandra.CassandraMonitor.main(CassandraMonitor.java:148)
{code}

MapMaker.makeComputingMap() was deprecated some time ago and removed in 15.0 that's why this exception happens. The code in StorageProxy class can trivially be fixed using Guava's LoadingCache.

[1]: https://github.com/Netflix/astyanax/issues/352
[2]: https://github.com/Netflix/astyanax/issues/391;;;","12/Sep/13 11:34;aleksey;Attaching a 1.2 based patch that replaces the deprecated MapMaker usage with CacheLoader (http://code.google.com/p/guava-libraries/wiki/MapMakerMigration)

Will update the jar and build.xml separately, and only for 2.0.1.;;;","12/Sep/13 14:58;jbellis;+1;;;","12/Sep/13 15:47;aleksey;Guava updated separately in 53f19c8495e37433d70bcfa19f8575b712d8b763;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Value of JVM_OPTS is partially lost when enabling JEMallocAllocator in cassandra-env.sh,CASSANDRA-6006,12668048,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,ngrigoriev,ngrigoriev,11/Sep/13 13:25,16/Apr/19 09:32,14/Jul/23 05:53,11/Sep/13 14:37,2.0.1,,,Local/Config,,,0,,,,,"In conf/cassandra-env.sh I see this:

{code}
# Configure the following for JEMallocAllocator and if jemalloc is not available in the system
# library path (Example: /usr/local/lib/). Usually ""make install"" will do the right thing.
# export LD_LIBRARY_PATH=<JEMALLOC_HOME>/lib/
# JVM_OPTS=""-Djava.library.path=<JEMALLOC_HOME>/lib/""
{code}


When I have enabled JEMalloc I have noticed that Cassandra complained about JAMM agent not being configured. Then I have realized that a bunch of JVM settings do not get passed to JVM, like heap size etc. This is because here the new argument replaces the previous value of JVM_OPTS instead of being added to it.

Here is the diff:
{code}
*** cassandra-env.sh.orig       2013-08-28 13:07:53.000000000 +0000
--- cassandra-env.sh    2013-09-11 13:25:12.904640141 +0000
***************
*** 227,233 ****
  # Configure the following for JEMallocAllocator and if jemalloc is not available in the system
  # library path (Example: /usr/local/lib/). Usually ""make install"" will do the right thing.
  # export LD_LIBRARY_PATH=<JEMALLOC_HOME>/lib/
! # JVM_OPTS=""-Djava.library.path=<JEMALLOC_HOME>/lib/""

  # uncomment to have Cassandra JVM listen for remote debuggers/profilers on port 1414
  # JVM_OPTS=""$JVM_OPTS -Xdebug -Xnoagent -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=1414""
--- 227,233 ----
  # Configure the following for JEMallocAllocator and if jemalloc is not available in the system
  # library path (Example: /usr/local/lib/). Usually ""make install"" will do the right thing.
  # export LD_LIBRARY_PATH=<JEMALLOC_HOME>/lib/
! # JVM_OPTS=""$JVM_OPTS -Djava.library.path=<JEMALLOC_HOME>/lib/""

  # uncomment to have Cassandra JVM listen for remote debuggers/profilers on port 1414
  # JVM_OPTS=""$JVM_OPTS -Xdebug -Xnoagent -Xrunjdwp:transport=dt_socket,server=y,suspend=n,address=1414""
{code}","Linux, cassandra 2.0.0",cburroughs,ngrigoriev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,347983,,,Wed Sep 11 14:37:28 UTC 2013,,,,,,,,,,"0|i1o05b:",348279,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"11/Sep/13 14:37;brandon.williams;Thanks, done in 1ae996d38259ad6d18fef7344b745eba8af56a4d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StandaloneScrubber assumes old-style json leveled manifest,CASSANDRA-6005,12667986,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,marcuse,marcuse,11/Sep/13 06:17,16/Apr/19 09:32,14/Jul/23 05:53,13/Sep/13 07:20,2.0.1,,,,,,0,lcs,,,,"With standalone scrubber in 2.0 we can encounter both the old-style json manifest and the new way, StandaloneScrubber needs to handle this.",,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Sep/13 06:18;marcuse;0001-Make-StandaloneScrubber-handle-new-leveled-manifest.patch;https://issues.apache.org/jira/secure/attachment/12602536/0001-Make-StandaloneScrubber-handle-new-leveled-manifest.patch",,,,,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,347921,,,Fri Sep 13 07:20:08 UTC 2013,,,,,,,,,,"0|i1nzrj:",348217,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"12/Sep/13 14:25;jbellis;+1;;;","13/Sep/13 07:20;marcuse;committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Performing a ""Select count(*)"" when replication factor < node count causes assertion error and timeout",CASSANDRA-6004,12667965,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,james0915,james0915,11/Sep/13 02:04,16/Apr/19 09:32,14/Jul/23 05:53,16/Sep/13 15:24,2.0.1,,,Legacy/CQL,,,0,,,,,"When performing a ""Select Count()"" query on a table belonging to a keyspace with a replication factor less than the total node count, the following error is encountered which ultimately results in an rpc_timeout for the request:

ERROR 18:47:54,660 Exception in thread Thread[Thread-5,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.filter.IDiskAtomFilter$Serializer.deserialize(IDiskAtomFilter.java:116)
	at org.apache.cassandra.db.RangeSliceCommandSerializer.deserialize(RangeSliceCommand.java:247)
	at org.apache.cassandra.db.RangeSliceCommandSerializer.deserialize(RangeSliceCommand.java:156)
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:99)
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:148)
	at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:125)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:73)

The issue is not encountered when the replication factor is >= node count

To replicate the issue:
1) Create the keyspace: CREATE KEYSPACE demodb WITH REPLICATION = {'class' : 'SimpleStrategy', 'replication_factor': 1};

2) Create the table CREATE TABLE users (
  user_name varchar,
  password varchar,
  gender varchar,
  session_token varchar,
  state varchar,
  birth_year bigint,
  PRIMARY KEY (user_name));

3) Do a CQL query: ""SELECT count( * ) FROM demodb.users"" ;

The issue is reproducible even if the table is empty. Both CQLSH and client (astyanax) api calls are affected. Tested on two different clusters (2-node and 8-node)","Two node setup
Ubuntu Server 12.04
Tested on JDK 1.6 and 1.7",james0915,slebresne,vongocminh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/13 07:38;slebresne;6004.txt;https://issues.apache.org/jira/secure/attachment/12603299/6004.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,347901,,,Wed Sep 25 19:36:35 UTC 2013,,,,,,,,,,"0|i1nznb:",348198,2.0.0,,,,,,,,jbellis,,jbellis,Normal,,2.0.0,,,,,,,,,,,,,,,,"11/Sep/13 17:47;brandon.williams;Reproduces well.  Looks like something related to CASSANDRA-4415, and IDAF is receiving a type of -1 it wasn't expecting.;;;","16/Sep/13 07:38;slebresne;Seems CASSANDRA-4415 had forgotten to register the verb handlers correctly. Attaching simple patch to fix.;;;","16/Sep/13 11:56;jbellis;+1;;;","16/Sep/13 15:24;slebresne;Committed, thanks;;;","17/Sep/13 10:26;vongocminh;Hello,

We got the same issue on our 4-node cluster on normal ""SELECT *"" query but changing replication_factor to 4 did not help.

I rolled back our dev cluster to 1.2.9 while waiting for the patch. Everything is working now but we don't have Batch for PreparedStatements (CASSANDRA-4693).

Could you please confirm that the bugfix is not related to the condition (replication_factor < node_count)? and not specific to ""SELECT COUNT(1)"" either?

Thanks for your help.
Best regards,
Minh;;;","17/Sep/13 10:38;slebresne;bq. could you please confirm that the bugfix is not related to the condition

I can. It's not related to the replication factor. And it will affect any select that requests a range of partition keys (select * without conditions is one) and that use the native protocol v2.;;;","17/Sep/13 23:00;james0915;Hi Minh,

For existing keyspaces, I was able to workaround the issue in our cluster by changing the replication factor to be the same as the number of nodes and then doing a ""nodetool repair"" on each node. I tested ""Select *"" and ""Select count"", both works.

Not sure, (Sylvain can confirm or not) but having (replication = nodecount) probably allows queries to be performed without having to request a range of partition keys which triggers the bug as Sylvain mentioned.
;;;","25/Sep/13 15:39;vongocminh;Hello,

I've deployed the new version v2.0.1 but it seems that the bug is not fixed. Here is how to reproduce the bug:
{code}

CREATE KEYSPACE mykeyspace WITH replication = {
    'class': 'SimpleStrategy',
    'replication_factor': '3'
};

USE mykeyspace;

CREATE TABLE mytable (
    id text,
    num int,
    str text,

    PRIMARY KEY (id, num)
);
CREATE INDEX ON mytable(str);
{code}

The following request always fails with rpc_timeout:
{code}
SELECT * FROM mytable WHERE str='test' AND num=1; -- NOT OK
{code}

But when we execute a ""should-be-the-same"" query, it works:
{code}
SELECT * FROM mytable WHERE num=1 AND str='test'; -- OK
{code}

And by miracle, the first query becomes functionals
{code}
SELECT * FROM mytable WHERE str='test' AND num=1; -- now is OK
{code}

Could you please have a look at the issue? It might be related to C* native secondary index?

Thanks for your help.
Best regards,
Minh;;;","25/Sep/13 19:36;brandon.williams;[~vongocminh] I can't reproduce on 2.0 HEAD, but if it is an issue it's not related to this one so please open a new ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CqlRecordWriter misses clusterClumns and partitionKeyColumns size issue for thrift tables,CASSANDRA-6002,12667934,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,alexliu68,alexliu68,alexliu68,10/Sep/13 22:17,16/Apr/19 09:32,14/Jul/23 05:53,12/Sep/13 18:26,1.2.10,,,,,,0,,,,,"For thrift tables, partitionKeyColumns size need be set to the right value and clusterColumns are missing",,alexliu68,jjordan,rspitzer,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/13 22:18;alexliu68;6002.txt;https://issues.apache.org/jira/secure/attachment/12602437/6002.txt",,,,,,,,,,,,,,,,,,,,1.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,347870,,,Thu Sep 12 18:26:05 UTC 2013,,,,,,,,,,"0|i1nzgf:",348167,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"12/Sep/13 18:26;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra-shuffle causes NumberFormatException,CASSANDRA-5995,12667824,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,lyubent,willymontaz,willymontaz,10/Sep/13 15:10,16/Apr/19 09:32,14/Jul/23 05:53,03/Apr/14 23:00,1.2.17,2.0.7,2.1 beta2,Legacy/Tools,,,0,,,,,"Using Cassandra-shuffle create, then Cassandra-shuffle en causes a NumbertFormatException :

Extract from output.log

 INFO 15:01:28,935 Enabling scheduled transfers of token ranges
 INFO 15:01:28,957 Initiating transfer of 3059156119944164299 (scheduled at Tue Sep 10 15:01:19 UTC 2013)
 WARN 15:01:28,962 Token 3059156119944164299 changing ownership from /10.36.194.173 to /10.39.67.29
 WARN 15:01:28,967 Token 3059156119944164299 changing ownership from /10.36.194.173 to /10.39.67.29
 INFO 15:01:28,968 RELOCATING: relocating [3059156119944164299] to 10.39.67.29
 INFO 15:01:28,968 RELOCATING: Sleeping 30000 ms before start streaming/fetching ranges
ERROR 15:01:29,331 Exception in thread Thread[GossipStage:8,5,main]
java.lang.NumberFormatException: For input string: """"
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Long.parseLong(Long.java:453)
        at java.lang.Long.valueOf(Long.java:540)
        at org.apache.cassandra.dht.Murmur3Partitioner$1.fromString(Murmur3Partitioner.java:183)
        at org.apache.cassandra.service.StorageService.handleStateRelocating(StorageService.java:1490)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1180)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:956)
        at org.apache.cassandra.gms.Gossiper.applyNewStates(Gossiper.java:947)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:905)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:57)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
ERROR 15:01:30,098 Exception in thread Thread[GossipStage:9,5,main]
java.lang.NumberFormatException: For input string: """"
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Long.parseLong(Long.java:453)
        at java.lang.Long.valueOf(Long.java:540)
        at org.apache.cassandra.dht.Murmur3Partitioner$1.fromString(Murmur3Partitioner.java:183)
        at org.apache.cassandra.service.StorageService.handleStateRelocating(StorageService.java:1490)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1180)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:956)
        at org.apache.cassandra.gms.Gossiper.applyNewStates(Gossiper.java:947)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:905)
        at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
",Amazon EC2,aledsage,lyubent,willymontaz,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6032,CASSANDRA-5874,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,lyubent,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,347760,,,Thu Apr 03 23:00:21 UTC 2014,,,,,,,,,,"0|i1nysf:",348059,,,,,,,,,,,,Normal,,1.2.0,,,,,,,,,,,,,,,,"10/Sep/13 22:26;aledsage;Looks similar to https://issues.apache.org/jira/browse/CASSANDRA-5874;;;","11/Sep/13 08:03;willymontaz;It seems that the same method is involved in that issue. In this particular case, I had no problem on startup and it is really the shuffle operation that led to the exception.;;;","13/Mar/14 22:00;jbellis;Can you have a look, Lyuben?;;;","14/Mar/14 00:19;lyubent;Will do.;;;","02/Apr/14 23:26;lyubent;[~willymontaz][~hsn] What exact version of Cassandra are you running and approximately how much load is the node that is being shuffled store? ;;;","03/Apr/14 22:35;lyubent;We [carry out a check|https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/service/StorageService.java#L3234] when moving tokens in {{StorageService#relocateTokens}} that removes the token to be moved if the source and destination addresses are the same. This means that the token is never sent to other nodes when carrying out {{shuffle enable}} leading to the empty string error in the coordinator and the NumberFormattingException in other involved nodes. I'm closing as not a problem because the only way I could reproduce is by shuffling twice, feel free to reopen if this can be reproduced on the first shuffle.  ;;;","03/Apr/14 22:40;brandon.williams;Can't we just put an else clause after the tokens.size check and fix it by making it a no-op?;;;","03/Apr/14 23:00;brandon.williams;Fixed this in 514ce33bc;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted Handoff: java.lang.ArithmeticException: / by zero,CASSANDRA-5990,12667683,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,kmueller,kmueller,09/Sep/13 22:09,16/Apr/19 09:32,14/Jul/23 05:53,12/Sep/13 11:22,1.2.10,2.0.1,,,,,0,,,,,"This node was down for a few hours. When bringing it back up, I saw this error in the logs. I'm not sure if it's receiving or sending hinted hand-offs.

 INFO [HintedHandoff:1] 2013-09-09 14:41:04,020 HintedHandOffManager.java (line 292) Started hinted handoff for host: 42bba02f-3088-4be1-8cb2-748a6f15e15d with IP: /10.93.12.14
ERROR [HintedHandoff:1] 2013-09-09 14:41:04,024 CassandraDaemon.java (line 192) Exception in thread Thread[HintedHandoff:1,1,main]
java.lang.ArithmeticException: / by zero
        at org.apache.cassandra.db.HintedHandOffManager.calculatePageSize(HintedHandOffManager.java:441)
        at org.apache.cassandra.db.HintedHandOffManager.doDeliverHintsToEndpoint(HintedHandOffManager.java:299)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:278)
        at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:90)
        at org.apache.cassandra.db.HintedHandOffManager$4.run(HintedHandOffManager.java:497)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)","cassandra 1.2.8
Oracle Java 1.7.0_25-b15
RHEL6",kmueller,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,347620,,,Thu Sep 12 11:22:34 UTC 2013,,,,,,,,,,"0|i1nxxb:",347919,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"12/Sep/13 11:22;jbellis;fixed in 8cc28a1477e19545bd0f6dca9180c937f9c85c8d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make hint TTL customizable,CASSANDRA-5988,12667676,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vkasar,okibirev,okibirev,09/Sep/13 21:45,16/Apr/19 09:32,14/Jul/23 05:53,28/Oct/13 17:02,1.2.12,2.0.3,,,,,0,patch,,,,"Currently time to live for stored hints is hardcoded to be gc_grace_seconds. This causes problems for applications using backdated deletes as a form of optimistic locking. Hints for updates made to the same data on which delete was attempted can persist for days, making it impossible to determine if delete succeeded by doing read(ALL) after a reasonable delay. We need a way to explicitly configure hint TTL, either through schema parameter or through a yaml file.

",,aleksey,jay.zhuang,kohlisankalp,okibirev,rha,vkasar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Oct/13 21:32;vkasar;5988.txt;https://issues.apache.org/jira/secure/attachment/12609531/5988.txt",,,,,,,,,,,,,,,,,,,,1.0,vkasar,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,347613,,,Thu Oct 13 17:19:59 UTC 2016,,,,,,,,,,"0|i1nxvr:",347912,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,brandon.williams,,,"09/Sep/13 22:45;kohlisankalp;We also want to make sure that hint TTL is less than gc grace period. ;;;","12/Sep/13 18:11;brandon.williams;So this would be per-cf? That's how hints are TTL'd currently.;;;","12/Sep/13 18:15;okibirev;That would be ideal, yes. ;;;","12/Sep/13 18:27;jbellis;Hint TTL will be exactly gc grace for the reasons documented in CASSANDRA-5314.

""using backdated deletes as a form of optimistic locking"" sounds like a bad idea, would you care to defend it? :);;;","12/Sep/13 18:44;okibirev;Referencing the above JIRA, there is no harm in having smaller hint TTL.

As for optimistic locking, the only other form (CAS) is not available until cassandra 2.0 and even then does not provide performance for very high volume operations.

If the objective is to do a user facing operation and a background operation simultaneously and without external locking, and to have background operation lose if there is an intervening user operation, backdating the background operation by a certain amount is a good compromise between consistency and performance. This feature will safeguard against user operation failing, being only stored as a hint and then confusing the background process as to ultimate success or failure of the backdated operation.;;;","21/Oct/13 20:35;vkasar;Index: branches/release/apache-cassandra-1.1.12/src/java/org/apache/cassandra/db/RowMutation.java
===================================================================
--- branches/release/apache-cassandra-1.1.12/src/java/org/apache/cassandra/db/RowMutation.java	(revision 105733)
+++ branches/release/apache-cassandra-1.1.12/src/java/org/apache/cassandra/db/RowMutation.java	(revision 105734)
@@ -123,6 +123,9 @@
      * }
      *
      */
+
+    static final int maxHintTTL = Integer.parseInt(System.getProperty(""ciedb.maxHintTTL"", String.valueOf(Integer.MAX_VALUE)));
+
     public static RowMutation hintFor(RowMutation mutation, ByteBuffer token) throws IOException
     {
         RowMutation rm = new RowMutation(Table.SYSTEM_TABLE, token);
@@ -131,7 +134,7 @@
         // determine the TTL for the RowMutation
         // this is set at the smallest GCGraceSeconds for any of the CFs in the RM
         // this ensures that deletes aren't ""undone"" by delivery of an old hint
-        int ttl = Integer.MAX_VALUE;
+        int ttl = maxHintTTL; //Integer.MAX_VALUE;
         for (ColumnFamily cf : mutation.getColumnFamilies())
             ttl = Math.min(ttl, cf.metadata().getGcGraceSeconds());
 
;;;","21/Oct/13 21:33;vkasar;Attached the diff as a file 5988.txt;;;","28/Oct/13 16:54;brandon.williams;I couldn't get this patch to apply, but it was simple enough to just recreate, so I committed it to 1.2.  It looks like we need a separate patch for 2.0/trunk, however.;;;","28/Oct/13 17:02;brandon.williams;Committed to 2.0/trunk.;;;","28/Oct/13 17:09;brandon.williams;I should note that I also changed the property name to ""cassandra.maxHintTTL"";;;","12/Oct/16 04:12;kohlisankalp;[~iamaleksey] I could not search ""cassandra.maxHintTTL"" in 3.0.9. With new hints in 3.0, how can we change this?;;;","12/Oct/16 10:36;aleksey;[~kohlisankalp] Will need to either modify {{HintsDispatcher}} logic to take 'maxhintttl' into account (compared to current time - hint's creationTime), or do the same even earlier, in {{HintsReader}}. The former is probably cleaner; the latter can be done a bit more efficiently - skipping hint body entirely if gcgs/creationTime/maxhinttl combination says the hint is basically dead.

Don't have time atm to do it, but [~bdeggleston] should be pretty familiar with that code, as he added compression logic - I can review.;;;","13/Oct/16 00:07;kohlisankalp;Without hintTTL, if we replay data older than GC grace, that will bring back data right? If it is not there in 3.0, it should be fixed as Major if not blocker? ;;;","13/Oct/16 15:59;aleksey;Nope, we are all good. We store creationTime and gcgs (at the time of hint's write), and check against current time (and current gcgs) before replaying. Essentially following the old behaviour, except even stricter (we use the min of gcgs at the time of writing the hint and the gcgs at the time of replay).

What changed is that you cannot *override* the *max* hintttl in 3.0 anymore to make it lower - or larger - than the calculated value.;;;","13/Oct/16 17:19;kohlisankalp;Thanks [~iamaleksey]. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paxos replay of in progress update is incorrect,CASSANDRA-5985,12667496,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,08/Sep/13 23:07,25/Oct/19 13:11,14/Jul/23 05:53,09/Sep/13 22:11,2.0.1,,,Feature/Lightweight Transactions,,,0,LWT,,,,"When we replay {{inProgress}}, we need to refresh it with the newly prepared ballot, or it will be (correctly) rejected.",,aleksey,jjordan,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6029,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,347433,,,Mon Sep 09 22:11:34 UTC 2013,,,,,,,,,,"0|i1nwrr:",347732,2.0.0,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"08/Sep/13 23:47;jbellis;Fix pushed to https://github.com/jbellis/cassandra/tree/5985, along with a patch to clean up the trace messages.  Also added a sleep to the collision-in-replay branch to match the collision-in-prepare.;;;","09/Sep/13 13:20;slebresne;One ""nit"": I'm not sure sleeping in the case where the propose work (and we commit) is really useful, the commit itself probably play a good enough sleep if we're contending (which, for what its worth, is confirmed by my unscientific test: moving the sleep only if the propose fails perform slightly better without making threads retry more often).

But overall, +1 on the fix (a quick test (that I'll commit to dtests) does confirm the current CAS timeout as soon a 2 inserts contend a bit too much).

;;;","09/Sep/13 22:11;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogDescriptor missing version 21,CASSANDRA-5984,12667450,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,cmolter,cmolter,07/Sep/13 14:29,16/Apr/19 09:32,14/Jul/23 05:53,08/Sep/13 17:24,,,,,,,0,,,,,"when running `ant clean test` I get:

    [junit] Testcase: testVersions(org.apache.cassandra.db.CommitLogTest):      FAILED
    [junit] expected:<8> but was:<7>
    [junit] junit.framework.AssertionFailedError: expected:<8> but was:<7>
    [junit]     at org.apache.cassandra.db.CommitLogTest.testVersions(CommitLogTest.java:224)

this seems to be caused by revision 14da6bca (CASSANDRA-5887) which introduced a new version for MessagingService without updating the versioning in CommitLogDescription
","MAC OSX 10.8
java 1.7.0_25
Trunk",cmolter,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Sep/13 14:45;cmolter;CASSANDRA-5984.patch;https://issues.apache.org/jira/secure/attachment/12601980/CASSANDRA-5984.patch",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,347387,,,Sun Sep 08 17:24:04 UTC 2013,,,,,,,,,,"0|i1nwhj:",347686,,,,,,,,,marcuse,,marcuse,Normal,,,,,,,,,,,,,,,,,,"07/Sep/13 14:45;cmolter;Adding VERSION_21 to CommitLogDescriptor fix this issue.;;;","08/Sep/13 17:24;marcuse;committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OutOfMemoryError when writing text blobs to a very large number of tables,CASSANDRA-5982,12667358,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,enigmacurry,enigmacurry,06/Sep/13 17:22,16/Apr/19 09:32,14/Jul/23 05:53,17/Sep/13 17:16,1.2.10,2.0.1,,,,,0,,,,,"This test goes outside the norm for Cassandra, creating ~2000 column families, and writing large text blobs to them. 

The process goes like this:

Bring up a 6 node m2.2xlarge cluster on EC2. This instance type has enough memory (34.2GB) so that Cassandra will allocate a full 8GB heap without tuning cassandra-env.sh. However, this instance type only has a single drive, so data and commitlog are comingled. (This test has also been run m1.xlarge instances which have four drives (but lower memory) and has exhibited similar results when assigning one to commitlog and 3 to datafile_directories.)

Use the 'memtable_allocator: HeapAllocator' setting from CASSANDRA-5935.

Create 2000 CFs:
{code}
CREATE KEYSPACE cf_stress WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 3}
CREATE COLUMNFAMILY cf_stress.tbl_00000 (id timeuuid PRIMARY KEY, val1 text, val2 text, val3 text ) ;
# repeat for tbl_00001, tbl_00002 ... tbl_02000
{code}

This process of creating tables takes a long time, about 5 hours, but for anyone wanting to create that many tables, presumably they only need to do this once, so this may be acceptable.

Write data:

The test dataset consists of writing 100K, 1M, and 10M documents to these tables:

{code}
INSERT INTO {table_name} (id, val1, val2, val3) VALUES (?, ?, ?, ?)
{code}

With 5 threads doing these inserts across the cluster, indefinitely, randomly choosing a table number 1-2000, the cluster eventually topples over with 'OutOfMemoryError: Java heap space'.

A heap dump analysis indicates that it's mostly memtables:

!2000CF_memtable_mem_usage.png!

Best current theory is that this is commitlog bound and that the memtables cannot flush fast enough due to locking issues. But I'll let [~jbellis] comment more on that.
",,cburroughs,chengren311,enigmacurry,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Sep/13 17:22;enigmacurry;2000CF_memtable_mem_usage.png;https://issues.apache.org/jira/secure/attachment/12601852/2000CF_memtable_mem_usage.png","06/Sep/13 17:23;enigmacurry;system.log.gz;https://issues.apache.org/jira/secure/attachment/12601853/system.log.gz",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,347295,,,Tue Sep 17 17:16:35 UTC 2013,,,,,,,,,,"0|i1nvxb:",347594,1.2.10,,,,,,,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,,"11/Sep/13 14:26;enigmacurry;This seems to be the solution:

* Use [~jbellis]' [cfs10k patch|https://github.com/jbellis/cassandra/tree/cfs10k]
* set concurrent_write:8 and memtable_total_space_in_mb:1024 in the yaml.
* Requires 8GB heap.

With these additional settings, I no longer see any OOM errors on any EC2 instance I've tested (m1.xlarge, m2.2xlarge, hs1.8xlarge);;;","13/Sep/13 22:31;jbellis;This workload presents several challenges to Cassandra:

- MemoryMeter is slow, and we [correctly] limit it to one CFS at a time.  If MemoryMeter is busy measuring a large memtable, we can easily write a lot of data to other CFs (or even the same one in a new memtable).  So we need to pick a more realistic liveRatio default to start with until we can get the first measurements.  (This is not a problem for Ryan's workload here; extremely large blobs have a liveRatio close to 1 anyway.  But it could be a problem for a workload with smaller cells.)
- The MemoryMeter queue is unbounded, and can keep memtables on-heap long after they've been flushed.  We should keep a reference to CFS instead and measure the currently active memtable when the task runs.
- forceFlush acquires Table.switchLock.writeLock temporarily, which means that it will block as long as readLock is held.  In particular, this means it will block for CommitLog.add.  This can actually be worse for PCLE than BCLE, since the former will allow up to 1K unwritten entries on its queue.  If the queue is full of large blobs like the ones in this workload, clearing up space in that queue can take a while.  (The queue itself can also be a significant source of memory consumption!)  We should move forceFlush to a separate executor so it can be truely nonblocking (which will allow MeteredFlusher to impose a moratorium on writes to the other CFs that should be flushed that much faster), and also reduce the CL queue size.  There is no reasonable way to adjust the queue size at runtime, so a configuration setting should be introduced.

IMO, all but the new pre-flush executor changes are reasonable to make in 1.2.10; the last I'd rather keep 2.0-only.;;;","13/Sep/13 22:31;jbellis;NB: even with these changes, testing shows that thousands of CFs fragments writes to the point that compaction and flush are effectively performing random i/o.  Rather than introduce a ""global flush log"" that imposes a moratorium on all CFs when the memtable memory budget is exeeded for this corner case, it is recommended to reduce concurrent_writes instead until flushing is able to keep up with the ingest.;;;","13/Sep/13 22:36;jbellis;Pushed branches to https://github.com/jbellis/cassandra/tree/5982-1.2 and https://github.com/jbellis/cassandra/tree/5982-2.0;;;","17/Sep/13 15:32;jjordan;We should add https://github.com/jbellis/cassandra/commit/2e22cf23ec4e18cd99b69bb3d419931c55e3ba93 to tpstats also.;;;","17/Sep/13 16:00;yukim;5982-1.2 is looking good to me (nit: unused import sun.security.provider.Sun in Memtable).

Approach in 5982-2.0 works, though changes to Future<Future> breaks some of the places like CommitLogReplayer that waits on only the first Future. So I think we need to fix those too.
Also, ColumnFamilyStore#reload needs to acquire write lock, so we have to switch switchMemtable to switchMemtableInternal there.;;;","17/Sep/13 16:25;yukim;In relation to CASSANDRA-5605, it may be better to change preExecutor -> flushwriter -> postExecuter chain with something different that correctly handles errors thrown.;;;","17/Sep/13 16:58;jbellis;bq. We should add https://github.com/jbellis/cassandra/commit/2e22cf23ec4e18cd99b69bb3d419931c55e3ba93 to tpstats also.

I'll turn Memtable.meterExecutor into a JMXEnabledTPE, which is what shows up in tpstats, but there isn't really a good way to shoehorn the commitlog executor in there.;;;","17/Sep/13 17:16;jbellis;bq. 5982-1.2 is looking good to me (nit: unused import sun.security.provider.Sun in Memtable).

Committed w/ nit fixed.;;;","17/Sep/13 17:16;jbellis;bq. it may be better to change preExecutor -> flushwriter -> postExecuter chain with something different that correctly handles errors thrown.

I'll just drop this for now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Netty frame length exception when storing data to Cassandra using binary protocol,CASSANDRA-5981,12667327,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,jsweeney,,06/Sep/13 13:52,08/Mar/23 21:19,14/Jul/23 05:53,04/Nov/13 15:13,2.0.3,,,,,,0,,,,,"Using Cassandra 1.2.8, I am running into an issue where when I send a large amount of data using the binary protocol, I get the following netty exception in the Cassandra log file:

{quote}
ERROR 09:08:35,845 Unexpected exception during request
org.jboss.netty.handler.codec.frame.TooLongFrameException: Adjusted frame length exceeds 268435456: 292413714 - discarded
        at org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder.fail(LengthFieldBasedFrameDecoder.java:441)
        at org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder.failIfNecessary(LengthFieldBasedFrameDecoder.java:412)
        at org.jboss.netty.handler.codec.frame.LengthFieldBasedFrameDecoder.decode(LengthFieldBasedFrameDecoder.java:372)
        at org.apache.cassandra.transport.Frame$Decoder.decode(Frame.java:181)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.callDecode(FrameDecoder.java:422)
        at org.jboss.netty.handler.codec.frame.FrameDecoder.messageReceived(FrameDecoder.java:303)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:268)
        at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:255)
        at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:84)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.processSelectedKeys(AbstractNioWorker.java:472)
        at org.jboss.netty.channel.socket.nio.AbstractNioWorker.run(AbstractNioWorker.java:333)
        at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:35)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
{quote}

I am using the Datastax driver and using CQL to execute insert queries. The query that is failing is using atomic batching executing a large number of statements (~55).

Looking into the code a bit, I saw that in the org.apache.cassandra.transport.Frame$Decoder class, the MAX_FRAME_LENGTH is hard coded to 256 mb.

Is this something that should be configurable or is this a hard limit that will prevent batch statements of this size from executing for some reason?","Linux, Java 7",danielnorberg,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Sep/13 14:17;slebresne;0001-Correctly-catch-frame-too-long-exceptions.txt;https://issues.apache.org/jira/secure/attachment/12603333/0001-Correctly-catch-frame-too-long-exceptions.txt","16/Sep/13 14:17;slebresne;0002-Allow-to-configure-the-max-frame-length.txt;https://issues.apache.org/jira/secure/attachment/12603334/0002-Allow-to-configure-the-max-frame-length.txt","02/Oct/13 09:34;slebresne;5981-v2.txt;https://issues.apache.org/jira/secure/attachment/12606272/5981-v2.txt","28/Oct/13 14:12;slebresne;5981-v3.txt;https://issues.apache.org/jira/secure/attachment/12610561/5981-v3.txt",,,,,,,,,,,,,,,,,4.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,347264,,,Mon Nov 04 15:13:45 UTC 2013,,,,,,,,,,"0|i1nvqf:",347563,,,,,,,,,danielnorberg,,danielnorberg,Low,,1.2.8,,,,,,,,,,,,,,,,"06/Sep/13 14:46;jbellis;CQL is not meant to be a bulk load protocol.  Use sstableloader / AbstractSSTableSimpleWriter for that.

That said, we should turn this into an InvalidRequestException instead of erroring out internally.;;;","06/Sep/13 14:58;slebresne;bq. Is this something that should be configurable or is this a hard limit that will prevent batch statements of this size from executing for some reason?

Sending a message of more than 256mb is a fairly bad idea tbh. Cassandra is just not optimized for that kind of huge request and so you will run into problems and get bad performance if you try to do even if we were lifting the hard coded limit. In fact, the only reason for this limit is to protect the server from OOM if the client sends something clearly wrong.

That being said, I'm not totally opposed to making the limit configurable. But more because I think most people may want to make it lower rather than higher.;;;","06/Sep/13 17:42;jsweeney;Thank you for taking a look into this, I agree 256mb is certainly too large, just something I ran into with using atomic batches. I'll be ensuring we aren't sending data this big going forward, but I think making the limit configurable would be useful. It seems to me to be similar to the thrift max message limit, which can be configured in the cassandra.yaml. Regardless, I appreciate the quick response.;;;","16/Sep/13 14:17;slebresne;bq. That said, we should turn this into an InvalidRequestException instead of erroring out internally

I agree and that's why there is a 'catch (TooLongFrameException)' in the Frame decoding code. But it appears that instead of throwing an exception the normal way (like it does for corrupted frames for instance), Netty instead fires the exceptionCaught callback directly so that catch was bypassed. Anyway, attaching patch that fixes that. The initial code was throwing a ProtocolException but I do agree an InvalidRequestException is probably more appropriate: after all a ProtocolException also close the connection which is not necessary here.  However, this required a few minor changes in Frame as we were not using Netty's LengthFieldBasedFrameDecoder.

As for making the max frame length configurable, I'm not necessarily against the idea. But as said above, more so than people can set it lower than the default if they want a more strict protection against badly behaving clients.  Attaching patch for that too.
;;;","17/Sep/13 22:32;jbellis;[~dln] can you review?;;;","19/Sep/13 16:14;jbellis;Oops, wrong Daniel.  Meant [~danielnorberg].;;;","26/Sep/13 09:05;danielnorberg;When handling the TooLongFrameException and sending an ErrorMessage reply with the InvalidRequestException without closing the connection, where is the stream id set on the ErrorMessage? I assume that without the stream id set and the connection still open, the client will be unable to infer that the request failed.
;;;","26/Sep/13 10:35;slebresne;Right, you're correct, the patch doesn't preserve the stream id correctly.

However, I have to say that I'm not too sure what's the easiest way to make that work correctly with Netty currently. To be able to use the stream id we've need to be able to start decoding the frame header before LengthFieldBasedFrameDecoder triggers the TooLongFrameException, but I don't know how to do that without knowing if LFBFD is in it's ""discardingTooLongFrame"" mode, and that's not exposed currently. Meaning that the only solutions I see so far are:
# push some feature request to netty so that LengthFieldBasedFrameDecoder exposes it's currently private discardingTooLongFrame field. Don't know if they'll be up for it and how quickly that'd get released.
# recode LengthFieldBasedFramedDecoder ourselves instead of using the netty one. Not the end of the world, it's not like it's a lot of code, but still a bit annoying in principle.

[~danielnorberg] Seeing any other simple solution that I would have missed?;;;","26/Sep/13 12:26;danielnorberg;Right, that's annoying.

I'd be tempted to actually close the connection immediately. It doesn't seem very attractive to read and discard that huge frame, potentially using up a lot of bandwidth doing only that. IMO better to prioritize well behaved clients and let the offending client reconnect.

If you still want to keep the connection open and fail the request nicely I'd probably go for implementing a custom frame decoder.

;;;","26/Sep/13 12:51;slebresne;bq. I'd be tempted to actually close the connection immediately.

That was the initial intent, but now I feel closing the connection in that case is too harsh. If we do allows to configure the max frame length (reasonable if only because some may want to lower it from the relatively high default) then client libraries can't valid frame size on their side and this become a end-user error. And closing the connection on a end-user error feels wrong (especially because it potentially cuts other unrelated streams on that connection).

bq. I'd probably go for implementing a custom frame decoder

Agreed, that's probably the simpler. I'll work that out.
;;;","02/Oct/13 09:34;slebresne;Alright, attaching a v2 (that includes making the ""max frame length configurable"" patch) that rewrite the Frame decoder to handle the frame slightly more manually to allow us to do what we want. This mostly mimick the code of Netty LengthFieldBasedFrameDecoder, though a bit simplified since adapted to just what we need. I'll note that this patch is against the 2.0 branch: I've been able to run the java driver tests with that patch so we should be good but this still is not entirely trivial a change so I'm starting to wonder if it's worth pushing it in 1.2, especially given that the current behavior (having the error logged server side) is not really a big deal.;;;","02/Oct/13 11:52;jbellis;Agreed on the 2.0 call.;;;","08/Oct/13 02:39;jbellis;How does v2 look [~danielnorberg]?;;;","21/Oct/13 15:49;jbellis;Hmm, timeout on that request.  [~norman], could you review v2?;;;","26/Oct/13 17:17;danielnorberg;Looks to me like it might discard too much data if buffer.readableBytes() > MAX_FRAME_LENGTH. Unless I'm mistaken this problem is also present in the original LengthFieldBasedFrameDecoder though. [~norman], what do you say? Admittedly it's a corner case that's unlikely to be encountered in production.

Are there any tests for the dropping of too large requests?

Apart from this it looks good to me.;;;","28/Oct/13 14:12;slebresne;I believe you're right. I suppose there is no reason to ever get into that case if you don't pick and unreasonably low max frame size, but there's no hurt in being careful so attaching v3 that make sure we don't discard too much.

And no, there isn't really a test in Cassandra for dropping large message because well, we don't really have any test for the native protocol so far. That being said, I do have a test for it in the java driver tests (though i'll need to commit it).;;;","03/Nov/13 20:06;danielnorberg;Looks good, thumbs up.;;;","04/Nov/13 15:13;slebresne;Alright, committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stressd broken by ClientEncriptionOptions,CASSANDRA-5978,12667116,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,benedict,jjordan,jjordan,05/Sep/13 14:28,16/Apr/19 09:32,14/Jul/23 05:53,12/Mar/14 09:52,2.1 rc2,,,Legacy/Tools,,,1,,,,,"The ClientEncryptionOptions object added to org.apache.cassandra.stress.Session is not Serializable.  So if you try to use stress with stressd, the Session can't be serialized to be passed over to stressd:

{noformat}
Exception in thread ""main"" java.io.NotSerializableException: org.apache.cassandra.config.EncryptionOptions$ClientEncryptionOptions
at java.io.ObjectOutputStream.writeObject0(Unknown Source)
at java.io.ObjectOutputStream.defaultWriteFields(Unknown Source)
at java.io.ObjectOutputStream.writeSerialData(Unknown Source)
at java.io.ObjectOutputStream.writeOrdinaryObject(Unknown Source)
at java.io.ObjectOutputStream.writeObject0(Unknown Source)
at java.io.ObjectOutputStream.writeObject(Unknown Source)
at org.apache.cassandra.stress.Stress.main(Unknown Source)
{noformat}",,bcantoni,djatnieks,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6199,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,347053,,,Tue Nov 26 19:54:50 UTC 2013,,,,,,,,,,"0|i1nufj:",347352,1.2.6,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"26/Nov/13 17:43;djatnieks;Ran into this today ... my stack has line numbers:

{noformat}
./dse-3.2.1/resources/cassandra/tools/bin/cassandra-stress -K 100 -t 50 -R org.apache.cassandra.locator.NetworkTopologyStrategy  --num-keys=10000000 --columns=50 -D nodelist -O Cassandra:3 --operation=INSERT --send-to 127.0.0.1

Exception in thread ""main"" java.io.NotSerializableException: org.apache.cassandra.config.EncryptionOptions$ClientEncryptionOptions
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1181)
	at java.io.ObjectOutputStream.defaultWriteFields(ObjectOutputStream.java:1541)
	at java.io.ObjectOutputStream.writeSerialData(ObjectOutputStream.java:1506)
	at java.io.ObjectOutputStream.writeOrdinaryObject(ObjectOutputStream.java:1429)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1175)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:347)
	at org.apache.cassandra.stress.Stress.main(Unknown Source)
Control-C caught. Canceling running action and shutting down...
{noformat}
;;;","26/Nov/13 18:29;jbellis;Fine with ""use stress-ng"" if that already supports daemon mode.  Otherwise this should be a quick fix.;;;","26/Nov/13 19:54;benedict;Download the latest snapshot from [6199|https://github.com/belliottsmith/cassandra/tree/iss-6199-stress] and run your command as

./dse-3.2.1/resources/cassandra/tools/bin/cassandra-stress *legacy* -K 100 -t 50 -R org.apache.cassandra.locator.NetworkTopologyStrategy  --num-keys=10000000 --columns=50 -D nodelist -O Cassandra:3 --operation=INSERT --send-to 127.0.0.1

I've confirmed this command and stressd launcher etc work with the latest version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix typo in HintedHandoffMetrics.java,CASSANDRA-5976,12666782,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,rcoli,rcoli,rcoli,03/Sep/13 23:01,16/Apr/19 09:32,14/Jul/23 05:53,03/Sep/13 23:15,,,,,,,0,,,,,"Summary : There's a typo in this file, ""diffrence"" instead of ""difference"". Attached patch changes all instances of ""diffrence"" to ""difference"".

",,aleksey,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/13 23:01;rcoli;HintedHandoffMetrics.diffrence.to.difference.patch;https://issues.apache.org/jira/secure/attachment/12601262/HintedHandoffMetrics.diffrence.to.difference.patch",,,,,,,,,,,,,,,,,,,,1.0,rcoli,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,346720,,,Tue Sep 03 23:15:05 UTC 2013,,,,,,,,,,"0|i1nsdz:",347021,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"03/Sep/13 23:15;aleksey;Ninja-committed. Shouldn't to into CHANGES.txt.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Filtering on Secondary Index Takes a Long Time Even with Limit 1, Trace Log Filled with Looping Messages",CASSANDRA-5975,12666780,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,rspitzer,rspitzer,03/Sep/13 22:47,16/Apr/19 09:32,14/Jul/23 05:53,10/Sep/13 14:59,1.2.10,2.0.1,,Feature/2i Index,,,0,,,,,"After creating a table with 300,000 keys. Attempting to filter on a column with a secondary index causes an rpc timeout. Using a limit statement does not alleviate the problem. The tracing log appears to be filled with the same set of messages repeated over and over until the query times out. 

The data was created with the attached script and the command
{code}
python create_data.py --num-keys 300000 --num-columns 50 --keyspace 'ks' --columnfamily cf_300000_keys_50_cols --create-index y -v 3
{code}

The query causing the delay is
{code}
select * from cf_300000_keys_50_cols where color = 'green' limit 1;
{code}

An excerpt of the trace log
{code}
Tracing session: cedbead0-14d7-11e3-915e-999f6c86239a

 activity                                                                          | timestamp    | source       | source_elapsed
-----------------------------------------------------------------------------------+--------------+--------------+----------------
                                                                execute_cql3_query | 20:31:27,230 | 10.196.1.106 |              0
       Parsing select * from cf_300000_keys_50_cols where color = 'green' limit 1; | 20:31:27,230 | 10.196.1.106 |             31
                                                                Peparing statement | 20:31:27,230 | 10.196.1.106 |            219
                                                     Determining replicas to query | 20:31:27,230 | 10.196.1.106 |            563
 Executing indexed scan for [min(-9223372036854775808), min(-9223372036854775808)] | 20:31:27,232 | 10.196.1.106 |           1816
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,232 | 10.196.1.106 |           2036
                                                      Acquiring sstable references | 20:31:27,232 | 10.196.1.106 |           2201
                                                       Merging memtable tombstones | 20:31:27,232 | 10.196.1.106 |           2345
                                                       Key cache hit for sstable 3 | 20:31:27,232 | 10.196.1.106 |           2493
                                       Seeking to partition beginning in data file | 20:31:27,232 | 10.196.1.106 |           2555
                                                       Key cache hit for sstable 1 | 20:31:27,234 | 10.196.1.106 |           3742
                                       Seeking to partition beginning in data file | 20:31:27,234 | 10.196.1.106 |           3806
                                        Merging data from memtables and 2 sstables | 20:31:27,236 | 10.196.1.106 |           5805
                                                Read 3 live and 0 tombstoned cells | 20:31:27,236 | 10.196.1.106 |           5977
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,236 | 10.196.1.106 |           6166
                                                      Acquiring sstable references | 20:31:27,236 | 10.196.1.106 |           6319
                                                       Merging memtable tombstones | 20:31:27,236 | 10.196.1.106 |           6382
                                                       Key cache hit for sstable 3 | 20:31:27,236 | 10.196.1.106 |           6421
                                       Seeking to partition beginning in data file | 20:31:27,236 | 10.196.1.106 |           6423
                                            Bloom filter allows skipping sstable 2 | 20:31:27,237 | 10.196.1.106 |           7060
                                            Bloom filter allows skipping sstable 1 | 20:31:27,237 | 10.196.1.106 |           7218
                                        Merging data from memtables and 1 sstables | 20:31:27,237 | 10.196.1.106 |           7358
                                                Read 1 live and 0 tombstoned cells | 20:31:27,238 | 10.196.1.106 |           7644
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,238 | 10.196.1.106 |           7855
                                                      Acquiring sstable references | 20:31:27,238 | 10.196.1.106 |           8008
                                                       Merging memtable tombstones | 20:31:27,238 | 10.196.1.106 |           8072
                                            Bloom filter allows skipping sstable 3 | 20:31:27,238 | 10.196.1.106 |           8225
                                            Bloom filter allows skipping sstable 2 | 20:31:27,238 | 10.196.1.106 |           8284
                                                       Key cache hit for sstable 1 | 20:31:27,238 | 10.196.1.106 |           8367
                                       Seeking to partition beginning in data file | 20:31:27,238 | 10.196.1.106 |           8468
                                        Merging data from memtables and 1 sstables | 20:31:27,239 | 10.196.1.106 |           8968
                                                Read 1 live and 0 tombstoned cells | 20:31:27,239 | 10.196.1.106 |           9234
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,239 | 10.196.1.106 |           9405
                                                      Acquiring sstable references | 20:31:27,239 | 10.196.1.106 |           9547
                                                       Merging memtable tombstones | 20:31:27,240 | 10.196.1.106 |           9608
                                                       Key cache hit for sstable 3 | 20:31:27,240 | 10.196.1.106 |           9700
                                 Seeking to partition indexed section in data file | 20:31:27,240 | 10.196.1.106 |           9884
                                                       Key cache hit for sstable 1 | 20:31:27,240 | 10.196.1.106 |          10005
                                 Seeking to partition indexed section in data file | 20:31:27,240 | 10.196.1.106 |          10175
                                        Merging data from memtables and 2 sstables | 20:31:27,240 | 10.196.1.106 |          10323
                                                Read 3 live and 0 tombstoned cells | 20:31:27,249 | 10.196.1.106 |          19358
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,249 | 10.196.1.106 |          19516
                                                      Acquiring sstable references | 20:31:27,249 | 10.196.1.106 |          19580
                                                       Merging memtable tombstones | 20:31:27,250 | 10.196.1.106 |          19670
                                                       Key cache hit for sstable 3 | 20:31:27,250 | 10.196.1.106 |          19765
                                       Seeking to partition beginning in data file | 20:31:27,250 | 10.196.1.106 |          19884
                                            Bloom filter allows skipping sstable 2 | 20:31:27,250 | 10.196.1.106 |          20357
                                            Bloom filter allows skipping sstable 1 | 20:31:27,250 | 10.196.1.106 |          20514
                                        Merging data from memtables and 1 sstables | 20:31:27,250 | 10.196.1.106 |          20576
                                                Read 1 live and 0 tombstoned cells | 20:31:27,251 | 10.196.1.106 |          20864
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,251 | 10.196.1.106 |          21072
                                                      Acquiring sstable references | 20:31:27,251 | 10.196.1.106 |          21137
                                                       Merging memtable tombstones | 20:31:27,251 | 10.196.1.106 |          21315
                                                       Key cache hit for sstable 3 | 20:31:27,251 | 10.196.1.106 |          21461
                                 Seeking to partition indexed section in data file | 20:31:27,252 | 10.196.1.106 |          21599
                                                       Key cache hit for sstable 1 | 20:31:27,252 | 10.196.1.106 |          21761
                                 Seeking to partition indexed section in data file | 20:31:27,252 | 10.196.1.106 |          21909
                                        Merging data from memtables and 2 sstables | 20:31:27,252 | 10.196.1.106 |          21977
                                                Read 3 live and 0 tombstoned cells | 20:31:27,261 | 10.196.1.106 |          30678
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,261 | 10.196.1.106 |          31153
                                                      Acquiring sstable references | 20:31:27,261 | 10.196.1.106 |          31156
                                                       Merging memtable tombstones | 20:31:27,261 | 10.196.1.106 |          31221
                                                       Key cache hit for sstable 3 | 20:31:27,261 | 10.196.1.106 |          31259
                                       Seeking to partition beginning in data file | 20:31:27,261 | 10.196.1.106 |          31261
                                            Bloom filter allows skipping sstable 2 | 20:31:27,264 | 10.196.1.106 |          33808
                                            Bloom filter allows skipping sstable 1 | 20:31:27,264 | 10.196.1.106 |          33875
                                        Merging data from memtables and 1 sstables | 20:31:27,264 | 10.196.1.106 |          33877
                                                Read 1 live and 0 tombstoned cells | 20:31:27,264 | 10.196.1.106 |          34313
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,264 | 10.196.1.106 |          34488
                                                      Acquiring sstable references | 20:31:27,264 | 10.196.1.106 |          34552
                                                       Merging memtable tombstones | 20:31:27,265 | 10.196.1.106 |          34642
                                                       Key cache hit for sstable 3 | 20:31:27,265 | 10.196.1.106 |          34792
                                 Seeking to partition indexed section in data file | 20:31:27,265 | 10.196.1.106 |          34851
                                                       Key cache hit for sstable 1 | 20:31:27,265 | 10.196.1.106 |          35007
                                 Seeking to partition indexed section in data file | 20:31:27,265 | 10.196.1.106 |          35066
                                        Merging data from memtables and 2 sstables | 20:31:27,265 | 10.196.1.106 |          35272
                                                Read 3 live and 0 tombstoned cells | 20:31:27,274 | 10.196.1.106 |          44333
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,274 | 10.196.1.106 |          44529
                                                      Acquiring sstable references | 20:31:27,275 | 10.196.1.106 |          44686
                                                       Merging memtable tombstones | 20:31:27,275 | 10.196.1.106 |          44752
                                            Bloom filter allows skipping sstable 3 | 20:31:27,275 | 10.196.1.106 |          44766
                                            Bloom filter allows skipping sstable 2 | 20:31:27,275 | 10.196.1.106 |          45021
                                                       Key cache hit for sstable 1 | 20:31:27,275 | 10.196.1.106 |          45163
                                       Seeking to partition beginning in data file | 20:31:27,275 | 10.196.1.106 |          45241
                                        Merging data from memtables and 1 sstables | 20:31:27,276 | 10.196.1.106 |          45719
                                                Read 1 live and 0 tombstoned cells | 20:31:27,276 | 10.196.1.106 |          45985
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,276 | 10.196.1.106 |          46171
                                                      Acquiring sstable references | 20:31:27,276 | 10.196.1.106 |          46235
                                                       Merging memtable tombstones | 20:31:27,276 | 10.196.1.106 |          46325
                                                       Key cache hit for sstable 3 | 20:31:27,276 | 10.196.1.106 |          46501
                                 Seeking to partition indexed section in data file | 20:31:27,276 | 10.196.1.106 |          46561
                                                       Key cache hit for sstable 1 | 20:31:27,277 | 10.196.1.106 |          46652
                                 Seeking to partition indexed section in data file | 20:31:27,277 | 10.196.1.106 |          46853
                                        Merging data from memtables and 2 sstables | 20:31:27,277 | 10.196.1.106 |          46922
                                                Read 3 live and 0 tombstoned cells | 20:31:27,286 | 10.196.1.106 |          56025
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,286 | 10.196.1.106 |          56198
                                                      Acquiring sstable references | 20:31:27,286 | 10.196.1.106 |          56264
                                                       Merging memtable tombstones | 20:31:27,286 | 10.196.1.106 |          56352
                                            Bloom filter allows skipping sstable 3 | 20:31:27,286 | 10.196.1.106 |          56439
                                            Bloom filter allows skipping sstable 2 | 20:31:27,286 | 10.196.1.106 |          56543
                                                       Key cache hit for sstable 1 | 20:31:27,287 | 10.196.1.106 |          56631
                                       Seeking to partition beginning in data file | 20:31:27,287 | 10.196.1.106 |          56634
                                        Merging data from memtables and 1 sstables | 20:31:27,287 | 10.196.1.106 |          57194
                                                Read 1 live and 0 tombstoned cells | 20:31:27,287 | 10.196.1.106 |          57494
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,288 | 10.196.1.106 |          57672
                                                      Acquiring sstable references | 20:31:27,288 | 10.196.1.106 |          57736
                                                       Merging memtable tombstones | 20:31:27,288 | 10.196.1.106 |          57922
                                                       Key cache hit for sstable 3 | 20:31:27,288 | 10.196.1.106 |          58047
                                 Seeking to partition indexed section in data file | 20:31:27,288 | 10.196.1.106 |          58200
                                                       Key cache hit for sstable 1 | 20:31:27,288 | 10.196.1.106 |          58351
                                 Seeking to partition indexed section in data file | 20:31:27,289 | 10.196.1.106 |          58783
                                        Merging data from memtables and 2 sstables | 20:31:27,289 | 10.196.1.106 |          58790
                                                Read 3 live and 0 tombstoned cells | 20:31:27,299 | 10.196.1.106 |          69455
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,300 | 10.196.1.106 |          69641
                                                      Acquiring sstable references | 20:31:27,300 | 10.196.1.106 |          69707
                                                       Merging memtable tombstones | 20:31:27,300 | 10.196.1.106 |          69713
                                            Bloom filter allows skipping sstable 3 | 20:31:27,300 | 10.196.1.106 |          69806
                                            Bloom filter allows skipping sstable 2 | 20:31:27,300 | 10.196.1.106 |          69904
                                                       Key cache hit for sstable 1 | 20:31:27,300 | 10.196.1.106 |          69994
                                       Seeking to partition beginning in data file | 20:31:27,300 | 10.196.1.106 |          70297
                                        Merging data from memtables and 1 sstables | 20:31:27,301 | 10.196.1.106 |          70778
                                                Read 1 live and 0 tombstoned cells | 20:31:27,301 | 10.196.1.106 |          71044
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,301 | 10.196.1.106 |          71271
                                                      Acquiring sstable references | 20:31:27,301 | 10.196.1.106 |          71335
                                                       Merging memtable tombstones | 20:31:27,301 | 10.196.1.106 |          71424
                                                       Key cache hit for sstable 3 | 20:31:27,301 | 10.196.1.106 |          71583
                                 Seeking to partition indexed section in data file | 20:31:27,302 | 10.196.1.106 |          71645
                                                       Key cache hit for sstable 1 | 20:31:27,302 | 10.196.1.106 |          71882
                                 Seeking to partition indexed section in data file | 20:31:27,302 | 10.196.1.106 |          71940
                                        Merging data from memtables and 2 sstables | 20:31:27,302 | 10.196.1.106 |          72030
                                                Read 3 live and 0 tombstoned cells | 20:31:27,311 | 10.196.1.106 |          81217
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,311 | 10.196.1.106 |          81369
                                                      Acquiring sstable references | 20:31:27,311 | 10.196.1.106 |          81515
                                                       Merging memtable tombstones | 20:31:27,312 | 10.196.1.106 |          81661
                                                       Key cache hit for sstable 3 | 20:31:27,312 | 10.196.1.106 |          81812
                                       Seeking to partition beginning in data file | 20:31:27,312 | 10.196.1.106 |          81873
                                            Bloom filter allows skipping sstable 2 | 20:31:27,312 | 10.196.1.106 |          82409
                                            Bloom filter allows skipping sstable 1 | 20:31:27,312 | 10.196.1.106 |          82479
                                        Merging data from memtables and 1 sstables | 20:31:27,312 | 10.196.1.106 |          82481
                                                Read 1 live and 0 tombstoned cells | 20:31:27,313 | 10.196.1.106 |          82860
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,313 | 10.196.1.106 |          83035
                                                      Acquiring sstable references | 20:31:27,313 | 10.196.1.106 |          83099
                                                       Merging memtable tombstones | 20:31:27,313 | 10.196.1.106 |          83217
                                                       Key cache hit for sstable 3 | 20:31:27,313 | 10.196.1.106 |          83307
                                 Seeking to partition indexed section in data file | 20:31:27,313 | 10.196.1.106 |          83410
                                                       Key cache hit for sstable 1 | 20:31:27,314 | 10.196.1.106 |          83588
                                 Seeking to partition indexed section in data file | 20:31:27,314 | 10.196.1.106 |          83652
                                        Merging data from memtables and 2 sstables | 20:31:27,314 | 10.196.1.106 |          83742
                                                Read 3 live and 0 tombstoned cells | 20:31:27,338 | 10.196.1.106 |         108372
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,345 | 10.196.1.106 |         115180
                                                      Acquiring sstable references | 20:31:27,345 | 10.196.1.106 |         115183
                                                       Merging memtable tombstones | 20:31:27,345 | 10.196.1.106 |         115188
                                            Bloom filter allows skipping sstable 3 | 20:31:27,345 | 10.196.1.106 |         115194
                                            Bloom filter allows skipping sstable 2 | 20:31:27,345 | 10.196.1.106 |         115197
                                                       Key cache hit for sstable 1 | 20:31:27,345 | 10.196.1.106 |         115203
                                       Seeking to partition beginning in data file | 20:31:27,345 | 10.196.1.106 |         115205
                                        Merging data from memtables and 1 sstables | 20:31:27,347 | 10.196.1.106 |         116821
                                                Read 1 live and 0 tombstoned cells | 20:31:27,348 | 10.196.1.106 |         117631
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,348 | 10.196.1.106 |         117997
                                                      Acquiring sstable references | 20:31:27,348 | 10.196.1.106 |         118000
                                                       Merging memtable tombstones | 20:31:27,348 | 10.196.1.106 |         118007
                                                       Key cache hit for sstable 3 | 20:31:27,348 | 10.196.1.106 |         118014
                                 Seeking to partition indexed section in data file | 20:31:27,348 | 10.196.1.106 |         118017
                                                       Key cache hit for sstable 1 | 20:31:27,348 | 10.196.1.106 |         118023
                                 Seeking to partition indexed section in data file | 20:31:27,348 | 10.196.1.106 |         118025
                                        Merging data from memtables and 2 sstables | 20:31:27,348 | 10.196.1.106 |         118030
                                                Read 3 live and 0 tombstoned cells | 20:31:27,356 | 10.196.1.106 |         126375
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,356 | 10.196.1.106 |         126418
                                                      Acquiring sstable references | 20:31:27,356 | 10.196.1.106 |         126421
                                                       Merging memtable tombstones | 20:31:27,356 | 10.196.1.106 |         126427
                                            Bloom filter allows skipping sstable 3 | 20:31:27,357 | 10.196.1.106 |         126695
                                            Bloom filter allows skipping sstable 2 | 20:31:27,357 | 10.196.1.106 |         126699
                                                       Key cache hit for sstable 1 | 20:31:27,357 | 10.196.1.106 |         126705
                                       Seeking to partition beginning in data file | 20:31:27,357 | 10.196.1.106 |         126708
                                        Merging data from memtables and 1 sstables | 20:31:27,357 | 10.196.1.106 |         127265
                                                Read 1 live and 0 tombstoned cells | 20:31:27,357 | 10.196.1.106 |         127519
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,358 | 10.196.1.106 |         127686
                                                      Acquiring sstable references | 20:31:27,358 | 10.196.1.106 |         127694
                                                       Merging memtable tombstones | 20:31:27,358 | 10.196.1.106 |         127700
                                                       Key cache hit for sstable 3 | 20:31:27,358 | 10.196.1.106 |         127708
                                 Seeking to partition indexed section in data file | 20:31:27,358 | 10.196.1.106 |         127710
                                                       Key cache hit for sstable 1 | 20:31:27,358 | 10.196.1.106 |         127716
                                 Seeking to partition indexed section in data file | 20:31:27,358 | 10.196.1.106 |         127718
                                        Merging data from memtables and 2 sstables | 20:31:27,358 | 10.196.1.106 |         127722
                                                Read 3 live and 0 tombstoned cells | 20:31:27,366 | 10.196.1.106 |         135976
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,366 | 10.196.1.106 |         136018
                                                      Acquiring sstable references | 20:31:27,366 | 10.196.1.106 |         136020
                                                       Merging memtable tombstones | 20:31:27,366 | 10.196.1.106 |         136026
                                            Bloom filter allows skipping sstable 3 | 20:31:27,366 | 10.196.1.106 |         136032
                                            Bloom filter allows skipping sstable 2 | 20:31:27,366 | 10.196.1.106 |         136035
                                                       Key cache hit for sstable 1 | 20:31:27,366 | 10.196.1.106 |         136040
                                       Seeking to partition beginning in data file | 20:31:27,366 | 10.196.1.106 |         136043
                                        Merging data from memtables and 1 sstables | 20:31:27,367 | 10.196.1.106 |         136852
                                                Read 1 live and 0 tombstoned cells | 20:31:27,367 | 10.196.1.106 |         137046
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,367 | 10.196.1.106 |         137154
                                                      Acquiring sstable references | 20:31:27,367 | 10.196.1.106 |         137159
                                                       Merging memtable tombstones | 20:31:27,367 | 10.196.1.106 |         137165
                                                       Key cache hit for sstable 3 | 20:31:27,367 | 10.196.1.106 |         137172
                                 Seeking to partition indexed section in data file | 20:31:27,367 | 10.196.1.106 |         137175
                                                       Key cache hit for sstable 1 | 20:31:27,367 | 10.196.1.106 |         137181
                                 Seeking to partition indexed section in data file | 20:31:27,367 | 10.196.1.106 |         137183
                                        Merging data from memtables and 2 sstables | 20:31:27,367 | 10.196.1.106 |         137187
                                                Read 3 live and 0 tombstoned cells | 20:31:27,375 | 10.196.1.106 |         145161
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,375 | 10.196.1.106 |         145195
                                                      Acquiring sstable references | 20:31:27,375 | 10.196.1.106 |         145198
                                                       Merging memtable tombstones | 20:31:27,375 | 10.196.1.106 |         145203
                                                       Key cache hit for sstable 3 | 20:31:27,375 | 10.196.1.106 |         145212
                                       Seeking to partition beginning in data file | 20:31:27,375 | 10.196.1.106 |         145214
                                            Bloom filter allows skipping sstable 2 | 20:31:27,376 | 10.196.1.106 |         146059
                                            Bloom filter allows skipping sstable 1 | 20:31:27,376 | 10.196.1.106 |         146063
                                        Merging data from memtables and 1 sstables | 20:31:27,376 | 10.196.1.106 |         146066
                                                Read 1 live and 0 tombstoned cells | 20:31:27,376 | 10.196.1.106 |         146295
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,376 | 10.196.1.106 |         146416
                                                      Acquiring sstable references | 20:31:27,376 | 10.196.1.106 |         146419
                                                       Merging memtable tombstones | 20:31:27,376 | 10.196.1.106 |         146425
                                                       Key cache hit for sstable 3 | 20:31:27,376 | 10.196.1.106 |         146433
                                 Seeking to partition indexed section in data file | 20:31:27,376 | 10.196.1.106 |         146435
                                                       Key cache hit for sstable 1 | 20:31:27,376 | 10.196.1.106 |         146441
                                 Seeking to partition indexed section in data file | 20:31:27,376 | 10.196.1.106 |         146443
                                        Merging data from memtables and 2 sstables | 20:31:27,376 | 10.196.1.106 |         146447
                                                Read 3 live and 0 tombstoned cells | 20:31:27,384 | 10.196.1.106 |         153664
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,384 | 10.196.1.106 |         153708
                                                      Acquiring sstable references | 20:31:27,384 | 10.196.1.106 |         153711
                                                       Merging memtable tombstones | 20:31:27,384 | 10.196.1.106 |         153717
                                                       Key cache hit for sstable 3 | 20:31:27,384 | 10.196.1.106 |         153899
                                       Seeking to partition beginning in data file | 20:31:27,384 | 10.196.1.106 |         153902
                                            Bloom filter allows skipping sstable 2 | 20:31:27,384 | 10.196.1.106 |         154556
                                            Bloom filter allows skipping sstable 1 | 20:31:27,384 | 10.196.1.106 |         154566
                                        Merging data from memtables and 1 sstables | 20:31:27,384 | 10.196.1.106 |         154568
                                                Read 1 live and 0 tombstoned cells | 20:31:27,385 | 10.196.1.106 |         155041
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,385 | 10.196.1.106 |         155112
                                                      Acquiring sstable references | 20:31:27,385 | 10.196.1.106 |         155156
                                                       Merging memtable tombstones | 20:31:27,385 | 10.196.1.106 |         155163
                                                       Key cache hit for sstable 3 | 20:31:27,385 | 10.196.1.106 |         155171
                                 Seeking to partition indexed section in data file | 20:31:27,385 | 10.196.1.106 |         155173
                                                       Key cache hit for sstable 1 | 20:31:27,385 | 10.196.1.106 |         155179
                                 Seeking to partition indexed section in data file | 20:31:27,385 | 10.196.1.106 |         155181
                                        Merging data from memtables and 2 sstables | 20:31:27,385 | 10.196.1.106 |         155185
                                                Read 3 live and 0 tombstoned cells | 20:31:27,393 | 10.196.1.106 |         163321
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,393 | 10.196.1.106 |         163365
                                                      Acquiring sstable references | 20:31:27,393 | 10.196.1.106 |         163367
                                                       Merging memtable tombstones | 20:31:27,393 | 10.196.1.106 |         163373
                                            Bloom filter allows skipping sstable 3 | 20:31:27,393 | 10.196.1.106 |         163379
                                            Bloom filter allows skipping sstable 2 | 20:31:27,393 | 10.196.1.106 |         163382
                                                       Key cache hit for sstable 1 | 20:31:27,393 | 10.196.1.106 |         163387
                                       Seeking to partition beginning in data file | 20:31:27,393 | 10.196.1.106 |         163389
                                        Merging data from memtables and 1 sstables | 20:31:27,394 | 10.196.1.106 |         164206
                                                Read 1 live and 0 tombstoned cells | 20:31:27,394 | 10.196.1.106 |         164397
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,394 | 10.196.1.106 |         164562
                                                      Acquiring sstable references | 20:31:27,394 | 10.196.1.106 |         164570
                                                       Merging memtable tombstones | 20:31:27,394 | 10.196.1.106 |         164576
                                                       Key cache hit for sstable 3 | 20:31:27,394 | 10.196.1.106 |         164584
                                 Seeking to partition indexed section in data file | 20:31:27,394 | 10.196.1.106 |         164587
                                                       Key cache hit for sstable 1 | 20:31:27,395 | 10.196.1.106 |         164593
                                 Seeking to partition indexed section in data file | 20:31:27,395 | 10.196.1.106 |         164595
                                        Merging data from memtables and 2 sstables | 20:31:27,395 | 10.196.1.106 |         164599
                                                Read 3 live and 0 tombstoned cells | 20:31:27,403 | 10.196.1.106 |         172818
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,403 | 10.196.1.106 |         172860
                                                      Acquiring sstable references | 20:31:27,403 | 10.196.1.106 |         172862
                                                       Merging memtable tombstones | 20:31:27,403 | 10.196.1.106 |         172868
                                            Bloom filter allows skipping sstable 3 | 20:31:27,403 | 10.196.1.106 |         172967
                                            Bloom filter allows skipping sstable 2 | 20:31:27,403 | 10.196.1.106 |         172971
                                                       Key cache hit for sstable 1 | 20:31:27,403 | 10.196.1.106 |         172979
                                       Seeking to partition beginning in data file | 20:31:27,403 | 10.196.1.106 |         172982
                                        Merging data from memtables and 1 sstables | 20:31:27,404 | 10.196.1.106 |         173735
                                                Read 1 live and 0 tombstoned cells | 20:31:27,404 | 10.196.1.106 |         173923
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,404 | 10.196.1.106 |         174088
                                                      Acquiring sstable references | 20:31:27,404 | 10.196.1.106 |         174096
                                                       Merging memtable tombstones | 20:31:27,404 | 10.196.1.106 |         174102
                                                       Key cache hit for sstable 3 | 20:31:27,404 | 10.196.1.106 |         174110
                                 Seeking to partition indexed section in data file | 20:31:27,404 | 10.196.1.106 |         174112
                                                       Key cache hit for sstable 1 | 20:31:27,404 | 10.196.1.106 |         174119
                                 Seeking to partition indexed section in data file | 20:31:27,404 | 10.196.1.106 |         174121
                                        Merging data from memtables and 2 sstables | 20:31:27,404 | 10.196.1.106 |         174124
                                                Read 3 live and 0 tombstoned cells | 20:31:27,414 | 10.196.1.106 |         183917
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,414 | 10.196.1.106 |         183959
                                                      Acquiring sstable references | 20:31:27,414 | 10.196.1.106 |         183961
                                                       Merging memtable tombstones | 20:31:27,414 | 10.196.1.106 |         183967
                                            Bloom filter allows skipping sstable 3 | 20:31:27,414 | 10.196.1.106 |         183973
                                            Bloom filter allows skipping sstable 2 | 20:31:27,414 | 10.196.1.106 |         183976
                                                       Key cache hit for sstable 1 | 20:31:27,414 | 10.196.1.106 |         183982
                                       Seeking to partition beginning in data file | 20:31:27,414 | 10.196.1.106 |         183984
                                        Merging data from memtables and 1 sstables | 20:31:27,415 | 10.196.1.106 |         184807
                                                Read 1 live and 0 tombstoned cells | 20:31:27,415 | 10.196.1.106 |         184994
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,415 | 10.196.1.106 |         185105
                                                      Acquiring sstable references | 20:31:27,415 | 10.196.1.106 |         185108
                                                       Merging memtable tombstones | 20:31:27,415 | 10.196.1.106 |         185114
                                                       Key cache hit for sstable 3 | 20:31:27,415 | 10.196.1.106 |         185134
                                 Seeking to partition indexed section in data file | 20:31:27,415 | 10.196.1.106 |         185136
                                                       Key cache hit for sstable 1 | 20:31:27,415 | 10.196.1.106 |         185142
                                 Seeking to partition indexed section in data file | 20:31:27,415 | 10.196.1.106 |         185144
                                        Merging data from memtables and 2 sstables | 20:31:27,415 | 10.196.1.106 |         185148
                                                Read 3 live and 0 tombstoned cells | 20:31:27,423 | 10.196.1.106 |         193586
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,424 | 10.196.1.106 |         193635
                                                      Acquiring sstable references | 20:31:27,424 | 10.196.1.106 |         193638
                                                       Merging memtable tombstones | 20:31:27,424 | 10.196.1.106 |         193643
                                            Bloom filter allows skipping sstable 3 | 20:31:27,424 | 10.196.1.106 |         193649
                                            Bloom filter allows skipping sstable 2 | 20:31:27,424 | 10.196.1.106 |         193652
                                                       Key cache hit for sstable 1 | 20:31:27,424 | 10.196.1.106 |         193657
                                       Seeking to partition beginning in data file | 20:31:27,424 | 10.196.1.106 |         193660
                                        Merging data from memtables and 1 sstables | 20:31:27,424 | 10.196.1.106 |         194274
                                                Read 1 live and 0 tombstoned cells | 20:31:27,424 | 10.196.1.106 |         194483
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,425 | 10.196.1.106 |         194597
                                                      Acquiring sstable references | 20:31:27,425 | 10.196.1.106 |         194605
                                                       Merging memtable tombstones | 20:31:27,425 | 10.196.1.106 |         194611
                                                       Key cache hit for sstable 3 | 20:31:27,425 | 10.196.1.106 |         194619
                                 Seeking to partition indexed section in data file | 20:31:27,425 | 10.196.1.106 |         194621
                                                       Key cache hit for sstable 1 | 20:31:27,425 | 10.196.1.106 |         194627
                                 Seeking to partition indexed section in data file | 20:31:27,425 | 10.196.1.106 |         194629
                                        Merging data from memtables and 2 sstables | 20:31:27,425 | 10.196.1.106 |         194633
                                                Read 3 live and 0 tombstoned cells | 20:31:27,433 | 10.196.1.106 |         203042
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,433 | 10.196.1.106 |         203084
                                                      Acquiring sstable references | 20:31:27,433 | 10.196.1.106 |         203086
                                                       Merging memtable tombstones | 20:31:27,433 | 10.196.1.106 |         203092
                                            Bloom filter allows skipping sstable 3 | 20:31:27,433 | 10.196.1.106 |         203097
                                            Bloom filter allows skipping sstable 2 | 20:31:27,433 | 10.196.1.106 |         203100
                                                       Key cache hit for sstable 1 | 20:31:27,433 | 10.196.1.106 |         203106
                                       Seeking to partition beginning in data file | 20:31:27,433 | 10.196.1.106 |         203108
                                        Merging data from memtables and 1 sstables | 20:31:27,434 | 10.196.1.106 |         203598
                                                Read 1 live and 0 tombstoned cells | 20:31:27,434 | 10.196.1.106 |         203942
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,434 | 10.196.1.106 |         204112
                                                      Acquiring sstable references | 20:31:27,434 | 10.196.1.106 |         204120
                                                       Merging memtable tombstones | 20:31:27,434 | 10.196.1.106 |         204126
                                                       Key cache hit for sstable 3 | 20:31:27,434 | 10.196.1.106 |         204134
                                 Seeking to partition indexed section in data file | 20:31:27,434 | 10.196.1.106 |         204136
                                                       Key cache hit for sstable 1 | 20:31:27,434 | 10.196.1.106 |         204142
                                 Seeking to partition indexed section in data file | 20:31:27,434 | 10.196.1.106 |         204144
                                        Merging data from memtables and 2 sstables | 20:31:27,434 | 10.196.1.106 |         204148
                                                Read 3 live and 0 tombstoned cells | 20:31:27,441 | 10.196.1.106 |         211397
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,441 | 10.196.1.106 |         211439
                                                      Acquiring sstable references | 20:31:27,441 | 10.196.1.106 |         211441
                                                       Merging memtable tombstones | 20:31:27,441 | 10.196.1.106 |         211447
                                            Bloom filter allows skipping sstable 3 | 20:31:27,442 | 10.196.1.106 |         211546
                                            Bloom filter allows skipping sstable 2 | 20:31:27,442 | 10.196.1.106 |         211732
                                                       Key cache hit for sstable 1 | 20:31:27,442 | 10.196.1.106 |         211737
                                       Seeking to partition beginning in data file | 20:31:27,442 | 10.196.1.106 |         211739
                                        Merging data from memtables and 1 sstables | 20:31:27,442 | 10.196.1.106 |         212484
                                                Read 1 live and 0 tombstoned cells | 20:31:27,443 | 10.196.1.106 |         212809
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,443 | 10.196.1.106 |         212918
                                                      Acquiring sstable references | 20:31:27,443 | 10.196.1.106 |         212921
                                                       Merging memtable tombstones | 20:31:27,443 | 10.196.1.106 |         212927
                                                       Key cache hit for sstable 3 | 20:31:27,443 | 10.196.1.106 |         212934
                                 Seeking to partition indexed section in data file | 20:31:27,443 | 10.196.1.106 |         212936
                                                       Key cache hit for sstable 1 | 20:31:27,443 | 10.196.1.106 |         212943
                                 Seeking to partition indexed section in data file | 20:31:27,443 | 10.196.1.106 |         212945
                                        Merging data from memtables and 2 sstables | 20:31:27,443 | 10.196.1.106 |         212949
                                                Read 3 live and 0 tombstoned cells | 20:31:27,451 | 10.196.1.106 |         220911
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,451 | 10.196.1.106 |         220954
                                                      Acquiring sstable references | 20:31:27,451 | 10.196.1.106 |         220956
                                                       Merging memtable tombstones | 20:31:27,451 | 10.196.1.106 |         220962
                                            Bloom filter allows skipping sstable 3 | 20:31:27,451 | 10.196.1.106 |         220968
                                            Bloom filter allows skipping sstable 2 | 20:31:27,451 | 10.196.1.106 |         220971
                                                       Key cache hit for sstable 1 | 20:31:27,451 | 10.196.1.106 |         220976
                                       Seeking to partition beginning in data file | 20:31:27,451 | 10.196.1.106 |         220979
                                        Merging data from memtables and 1 sstables | 20:31:27,452 | 10.196.1.106 |         221774
                                                Read 1 live and 0 tombstoned cells | 20:31:27,452 | 10.196.1.106 |         221965
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,452 | 10.196.1.106 |         222093
                                                      Acquiring sstable references | 20:31:27,452 | 10.196.1.106 |         222101
                                                       Merging memtable tombstones | 20:31:27,452 | 10.196.1.106 |         222108
                                                       Key cache hit for sstable 3 | 20:31:27,452 | 10.196.1.106 |         222115
                                 Seeking to partition indexed section in data file | 20:31:27,452 | 10.196.1.106 |         222118
                                                       Key cache hit for sstable 1 | 20:31:27,452 | 10.196.1.106 |         222124
                                 Seeking to partition indexed section in data file | 20:31:27,452 | 10.196.1.106 |         222126
                                        Merging data from memtables and 2 sstables | 20:31:27,452 | 10.196.1.106 |         222130
                                                Read 3 live and 0 tombstoned cells | 20:31:27,460 | 10.196.1.106 |         230378
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,460 | 10.196.1.106 |         230420
                                                      Acquiring sstable references | 20:31:27,460 | 10.196.1.106 |         230422
                                                       Merging memtable tombstones | 20:31:27,460 | 10.196.1.106 |         230428
                                            Bloom filter allows skipping sstable 3 | 20:31:27,461 | 10.196.1.106 |         230527
                                            Bloom filter allows skipping sstable 2 | 20:31:27,461 | 10.196.1.106 |         230619
                                                       Key cache hit for sstable 1 | 20:31:27,461 | 10.196.1.106 |         230624
                                       Seeking to partition beginning in data file | 20:31:27,461 | 10.196.1.106 |         230627
                                        Merging data from memtables and 1 sstables | 20:31:27,461 | 10.196.1.106 |         231223
                                                Read 1 live and 0 tombstoned cells | 20:31:27,461 | 10.196.1.106 |         231419
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,461 | 10.196.1.106 |         231531
                                                      Acquiring sstable references | 20:31:27,461 | 10.196.1.106 |         231540
                                                       Merging memtable tombstones | 20:31:27,461 | 10.196.1.106 |         231545
                                                       Key cache hit for sstable 3 | 20:31:27,461 | 10.196.1.106 |         231553
                                 Seeking to partition indexed section in data file | 20:31:27,461 | 10.196.1.106 |         231555
                                                       Key cache hit for sstable 1 | 20:31:27,461 | 10.196.1.106 |         231562
                                 Seeking to partition indexed section in data file | 20:31:27,461 | 10.196.1.106 |         231564
                                        Merging data from memtables and 2 sstables | 20:31:27,461 | 10.196.1.106 |         231568
                                                Read 3 live and 0 tombstoned cells | 20:31:27,470 | 10.196.1.106 |         239976
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,470 | 10.196.1.106 |         240020
                                                      Acquiring sstable references | 20:31:27,470 | 10.196.1.106 |         240022
                                                       Merging memtable tombstones | 20:31:27,470 | 10.196.1.106 |         240028
                                            Bloom filter allows skipping sstable 3 | 20:31:27,470 | 10.196.1.106 |         240034
                                            Bloom filter allows skipping sstable 2 | 20:31:27,470 | 10.196.1.106 |         240037
                                                       Key cache hit for sstable 1 | 20:31:27,470 | 10.196.1.106 |         240054
                                       Seeking to partition beginning in data file | 20:31:27,470 | 10.196.1.106 |         240093
                                        Merging data from memtables and 1 sstables | 20:31:27,470 | 10.196.1.106 |         240585
                                                Read 1 live and 0 tombstoned cells | 20:31:27,471 | 10.196.1.106 |         240885
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,471 | 10.196.1.106 |         240993
                                                      Acquiring sstable references | 20:31:27,471 | 10.196.1.106 |         240997
                                                       Merging memtable tombstones | 20:31:27,471 | 10.196.1.106 |         241003
                                                       Key cache hit for sstable 3 | 20:31:27,471 | 10.196.1.106 |         241011
                                 Seeking to partition indexed section in data file | 20:31:27,471 | 10.196.1.106 |         241013
                                                       Key cache hit for sstable 1 | 20:31:27,471 | 10.196.1.106 |         241019
                                 Seeking to partition indexed section in data file | 20:31:27,471 | 10.196.1.106 |         241020
                                        Merging data from memtables and 2 sstables | 20:31:27,471 | 10.196.1.106 |         241028
                                                Read 3 live and 0 tombstoned cells | 20:31:27,479 | 10.196.1.106 |         249225
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,479 | 10.196.1.106 |         249265
                                                      Acquiring sstable references | 20:31:27,479 | 10.196.1.106 |         249267
                                                       Merging memtable tombstones | 20:31:27,479 | 10.196.1.106 |         249273
                                            Bloom filter allows skipping sstable 3 | 20:31:27,479 | 10.196.1.106 |         249279
                                            Bloom filter allows skipping sstable 2 | 20:31:27,479 | 10.196.1.106 |         249282
                                                       Key cache hit for sstable 1 | 20:31:27,479 | 10.196.1.106 |         249287
                                       Seeking to partition beginning in data file | 20:31:27,479 | 10.196.1.106 |         249289
                                        Merging data from memtables and 1 sstables | 20:31:27,479 | 10.196.1.106 |         249296
                                                Read 1 live and 0 tombstoned cells | 20:31:27,479 | 10.196.1.106 |         249558
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,480 | 10.196.1.106 |         249655
                                                      Acquiring sstable references | 20:31:27,480 | 10.196.1.106 |         249658
                                                       Merging memtable tombstones | 20:31:27,480 | 10.196.1.106 |         249755
                                                       Key cache hit for sstable 3 | 20:31:27,480 | 10.196.1.106 |         249763
                                 Seeking to partition indexed section in data file | 20:31:27,480 | 10.196.1.106 |         249765
                                                       Key cache hit for sstable 1 | 20:31:27,480 | 10.196.1.106 |         249772
                                 Seeking to partition indexed section in data file | 20:31:27,480 | 10.196.1.106 |         249774
                                        Merging data from memtables and 2 sstables | 20:31:27,480 | 10.196.1.106 |         249778
                                                Read 3 live and 0 tombstoned cells | 20:31:27,487 | 10.196.1.106 |         257542
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,487 | 10.196.1.106 |         257575
                                                      Acquiring sstable references | 20:31:27,488 | 10.196.1.106 |         257610
                                                       Merging memtable tombstones | 20:31:27,488 | 10.196.1.106 |         257616
                                            Bloom filter allows skipping sstable 3 | 20:31:27,488 | 10.196.1.106 |         257622
                                            Bloom filter allows skipping sstable 2 | 20:31:27,488 | 10.196.1.106 |         257625
                                                       Key cache hit for sstable 1 | 20:31:27,488 | 10.196.1.106 |         257631
                                       Seeking to partition beginning in data file | 20:31:27,488 | 10.196.1.106 |         257633
                                        Merging data from memtables and 1 sstables | 20:31:27,488 | 10.196.1.106 |         258473
                                                Read 1 live and 0 tombstoned cells | 20:31:27,489 | 10.196.1.106 |         259067
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,489 | 10.196.1.106 |         259197
                                                      Acquiring sstable references | 20:31:27,489 | 10.196.1.106 |         259200
                                                       Merging memtable tombstones | 20:31:27,489 | 10.196.1.106 |         259206
                                                       Key cache hit for sstable 3 | 20:31:27,489 | 10.196.1.106 |         259214
                                 Seeking to partition indexed section in data file | 20:31:27,489 | 10.196.1.106 |         259216
                                                       Key cache hit for sstable 1 | 20:31:27,489 | 10.196.1.106 |         259222
                                 Seeking to partition indexed section in data file | 20:31:27,489 | 10.196.1.106 |         259224
                                        Merging data from memtables and 2 sstables | 20:31:27,489 | 10.196.1.106 |         259319
                                                Read 3 live and 0 tombstoned cells | 20:31:27,497 | 10.196.1.106 |         267071
                        Executing single-partition query on cf_300000_keys_50_cols | 20:31:27,497 | 10.196.1.106 |         267135
                                                      Acquiring sstable references | 20:31:27,497 | 10.196.1.106 |         267137
                                                       Merging memtable tombstones | 20:31:27,497 | 10.196.1.106 |         267143
                                                       Key cache hit for sstable 3 | 20:31:27,497 | 10.196.1.106 |         267151
                                       Seeking to partition beginning in data file | 20:31:27,497 | 10.196.1.106 |         267154
                                            Bloom filter allows skipping sstable 2 | 20:31:27,498 | 10.196.1.106 |         267943
                                            Bloom filter allows skipping sstable 1 | 20:31:27,498 | 10.196.1.106 |         267957
                                        Merging data from memtables and 1 sstables | 20:31:27,498 | 10.196.1.106 |         267960
                                                Read 1 live and 0 tombstoned cells | 20:31:27,498 | 10.196.1.106 |         268176
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:31:27,498 | 10.196.1.106 |         268271
                                                      Acquiring sstable references | 20:31:27,498 | 10.196.1.106 |         268274
                                                       Merging memtable tombstones | 20:31:27,498 | 10.196.1.106 |         268280
                                                       Key cache hit for sstable 3 | 20:31:27,498 | 
...
{code}","Ubuntu, Single Node",aleksey,alexliu68,colinkuo,jjordan,petter,rcoli,rspitzer,slebresne,srrepaka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Sep/13 22:05;alexliu68;5975-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12601701/5975-1.2-branch.txt","09/Sep/13 18:22;aleksey;5975-v2-extra-tracing.txt;https://issues.apache.org/jira/secure/attachment/12602178/5975-v2-extra-tracing.txt","06/Sep/13 23:57;aleksey;5975-v2.txt;https://issues.apache.org/jira/secure/attachment/12601927/5975-v2.txt","06/Sep/13 21:29;aleksey;5975-v2.txt;https://issues.apache.org/jira/secure/attachment/12601910/5975-v2.txt","03/Sep/13 22:54;rspitzer;create_data.py;https://issues.apache.org/jira/secure/attachment/12601259/create_data.py","08/Sep/13 00:36;alexliu68;patch.txt;https://issues.apache.org/jira/secure/attachment/12602024/patch.txt","03/Sep/13 22:54;rspitzer;trace.log;https://issues.apache.org/jira/secure/attachment/12601260/trace.log",,,,,,,,,,,,,,7.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,346718,,,Wed Jan 15 17:44:31 UTC 2014,,,,,,,,,,"0|i1nsdj:",347019,1.2.6,1.2.9,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"05/Sep/13 18:54;alexliu68;It does return the result if we increase the timeout to big enough.;;;","05/Sep/13 20:34;alexliu68;Paging need be done for secondary index search with Limit clause. It retrieves all matched rows under the hood. If there are large number of matched rows (e.g. 30,000), it's time out.;;;","05/Sep/13 22:01;alexliu68;5975-1.2-branch.txt patch is attached;;;","06/Sep/13 15:24;jjordan;Don't forget about:
{quote}
 // We shouldn't fetch only 1 row as this provides buggy paging in case the first row doesn't satisfy all clauses
{quote}

You don't want to strictly enforce the limit.;;;","06/Sep/13 17:07;alexliu68;Can you provide a sample testing case to reproduce the issue? I can't reproduce it.;;;","06/Sep/13 17:18;jjordan;I think it would be anything that needs ""allow filtering"", such that results from the 2i lookup might be thrown out later...;;;","06/Sep/13 17:38;alexliu68;Right, it needs ""allow filtering"" if there are more than one indexed columns in the query. But do you have any testing data to support your case. I tried a few ""allow filtering"" queries and all worked for me.;;;","06/Sep/13 17:47;jjordan;I was just reading the comments in the code, which make sense.  If you do a limit X on something that then check another column through filters, if the index check only returns X rows, and the filter throws them all away, you will get 0 rows.;;;","06/Sep/13 18:12;alexliu68;Based on tracing, I see it checks the first indexed column then do a scanning even though there is another indexed column until it hits the limit. The limit is on the number of returning Cql rows instead of a column. I am not sure the case you mentioned above can be reproduced or not, as along as my testing I can't reproduce it. ;;;","06/Sep/13 21:27;aleksey;There are two issues here.

1. https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L1530 will never actually do anything, because filter.lastCounted(data) will always return 0, since CompositesSearcher does not use the original filter at all (except to get limit, which never gets updated), so the condition at https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L1500 is equivalent to {monospaced}while (rowIterator.hasNext()){monospaced}.

2. https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/index/composites/CompositesSearcher.java#L198 always resets columnsCount to 0, so the checks like https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/index/composites/CompositesSearcher.java#L205 and https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/index/composites/CompositesSearcher.java#L250 are basically useless, too.

with 1 and 2 there is nothing to stop the iteration, so it goes on until it exhausts all the matching indexed entries. The easiest fix seems to be to move limit and count one level up and treat the limit as immutable, and deal with it entirely in CompositesSearcher. Attaching a v2, with some unrelated minor cleanups that get rid of all the warnings, while at it.;;;","06/Sep/13 22:27;alexliu68;I tested the version 2 patch, it doesn't fix the issue. It still does the full scan even limit 1

{code}
qlsh:ks> select * from cf_300000_keys_50_cols  where color = 'red' limit 1;                                    

 key      | color | column_0 | qty | size  | time
----------+-------+----------+-----+-------+--------------------------
 key_0002 |   red | column_0 | 124 | large | 2000-01-04 14:19:59-0500


Tracing session: 5182ef70-1743-11e3-a0e2-fb4a940b08ec

 activity                                                                          | timestamp    | source    | source_elapsed
-----------------------------------------------------------------------------------+--------------+-----------+----------------
                                                                execute_cql3_query | 18:26:04,904 | 127.0.0.1 |              0
        Parsing select * from cf_300000_keys_50_cols  where color = 'red' limit 1; | 18:26:04,904 | 127.0.0.1 |            101
                                                                Peparing statement | 18:26:04,904 | 127.0.0.1 |            500
                                                     Determining replicas to query | 18:26:04,905 | 127.0.0.1 |            966
 Executing indexed scan for [min(-9223372036854775808), min(-9223372036854775808)] | 18:26:04,906 | 127.0.0.1 |           1714
                  Executing single-partition query on cf_300000_keys_50_cols.color | 18:26:04,906 | 127.0.0.1 |           1962
                                                      Acquiring sstable references | 18:26:04,906 | 127.0.0.1 |           1987
                                                       Merging memtable tombstones | 18:26:04,906 | 127.0.0.1 |           2058
                                                       Key cache hit for sstable 5 | 18:26:04,906 | 127.0.0.1 |           2194
                                       Seeking to partition beginning in data file | 18:26:04,906 | 127.0.0.1 |           2232
                                        Merging data from memtables and 1 sstables | 18:26:04,906 | 127.0.0.1 |           2320
                                                Read 3 live and 0 tombstoned cells | 18:26:04,906 | 127.0.0.1 |           2466
                        Executing single-partition query on cf_300000_keys_50_cols | 18:26:04,907 | 127.0.0.1 |           2719
                                                      Acquiring sstable references | 18:26:04,907 | 127.0.0.1 |           2741
                                                       Merging memtable tombstones | 18:26:04,907 | 127.0.0.1 |           2819
                                                       Key cache hit for sstable 5 | 18:26:04,907 | 127.0.0.1 |           2950
                                       Seeking to partition beginning in data file | 18:26:04,907 | 127.0.0.1 |           2972
                                        Merging data from memtables and 1 sstables | 18:26:04,907 | 127.0.0.1 |           3040
                                                Read 1 live and 0 tombstoned cells | 18:26:04,907 | 127.0.0.1 |           3218
                        Executing single-partition query on cf_300000_keys_50_cols | 18:26:04,907 | 127.0.0.1 |           3545
                                                      Acquiring sstable references | 18:26:04,907 | 127.0.0.1 |           3563
                                                       Merging memtable tombstones | 18:26:04,908 | 127.0.0.1 |           3613
                                                       Key cache hit for sstable 5 | 18:26:04,908 | 127.0.0.1 |           3675
                                       Seeking to partition beginning in data file | 18:26:04,908 | 127.0.0.1 |           3688
                                        Merging data from memtables and 1 sstables | 18:26:04,908 | 127.0.0.1 |           3735
                                                Read 1 live and 0 tombstoned cells | 18:26:04,908 | 127.0.0.1 |           3874
                  Executing single-partition query on cf_300000_keys_50_cols.color | 18:26:04,908 | 127.0.0.1 |           4080
                                                      Acquiring sstable references | 18:26:04,908 | 127.0.0.1 |           4098
                                                       Merging memtable tombstones | 18:26:04,908 | 127.0.0.1 |           4143
                                                       Key cache hit for sstable 5 | 18:26:04,908 | 127.0.0.1 |           4211
                                 Seeking to partition indexed section in data file | 18:26:04,908 | 127.0.0.1 |           4234
                                        Merging data from memtables and 1 sstables | 18:26:04,908 | 127.0.0.1 |           4344
                                                Read 2 live and 0 tombstoned cells | 18:26:04,908 | 127.0.0.1 |           4406
                        Executing single-partition query on cf_300000_keys_50_cols | 18:26:04,909 | 127.0.0.1 |           4677
                                                      Acquiring sstable references | 18:26:04,909 | 127.0.0.1 |           4695
                                                       Merging memtable tombstones | 18:26:04,909 | 127.0.0.1 |           4751
                                                       Key cache hit for sstable 5 | 18:26:04,909 | 127.0.0.1 |           4816
                                       Seeking to partition beginning in data file | 18:26:04,909 | 127.0.0.1 |           4833
                                        Merging data from memtables and 1 sstables | 18:26:04,909 | 127.0.0.1 |           4869
                                                Read 1 live and 0 tombstoned cells | 18:26:04,909 | 127.0.0.1 |           5089
                  Executing single-partition query on cf_300000_keys_50_cols.color | 18:26:04,909 | 127.0.0.1 |           5276
                                                      Acquiring sstable references | 18:26:04,909 | 127.0.0.1 |           5290
                                                       Merging memtable tombstones | 18:26:04,909 | 127.0.0.1 |           5329
                                                       Key cache hit for sstable 5 | 18:26:04,909 | 127.0.0.1 |           5396
                                 Seeking to partition indexed section in data file | 18:26:04,909 | 127.0.0.1 |           5415
                                        Merging data from memtables and 1 sstables | 18:26:04,909 | 127.0.0.1 |           5541
                                                Read 1 live and 0 tombstoned cells | 18:26:04,910 | 127.0.0.1 |           5612
                                                      Scanned 3 rows and matched 3 | 18:26:04,910 | 127.0.0.1 |           5738
                                                                  Request complete | 18:26:04,910 | 127.0.0.1 |           6157

{code};;;","06/Sep/13 22:29;alexliu68;Cql query is a little different from thrift query, so the version 2 fix doesn't deal with the special case for cql query;;;","06/Sep/13 23:18;aleksey;How is that a full scan? Are you sure you've pasted the right trace? (there is no way the query would complete in 6ms if this were a full scan).;;;","06/Sep/13 23:40;alexliu68;My testing only has three records with red color, and you see the tracing check three times the index even though we limit it to 1;;;","06/Sep/13 23:43;alexliu68;You can also test it out with the attached data script.;;;","06/Sep/13 23:49;aleksey;Sorry, but 3 is not sufficient. Try with at least tens.;;;","06/Sep/13 23:59;aleksey;Updated the v2 slightly to use >= instead of > in ""columnsCount > limit"", to make one redundant query less.;;;","07/Sep/13 00:06;alexliu68;It's still the same. you can test your patch following the attached data script.;;;","07/Sep/13 00:18;aleksey;bq. It's still the same. you can test your patch following the attached data script.

Did so, obviously, with the original {noformat}python create_data.py --num-keys 300000 --num-columns 50 --keyspace 'ks' --columnfamily cf_300000_keys_50_cols --create-index y -v 3{noformat} and {noformat}select * from cf_300000_keys_50_cols where color = 'green' limit 1;{noformat}

Make sure you've got the patch applied and try that with tracing, too.;;;","07/Sep/13 00:36;alexliu68;It's still the same for me. I am not sure how you set it up and test it. If it passes your testing, you can commit it. ;;;","07/Sep/13 00:37;alexliu68;Can you post your tracing log?;;;","07/Sep/13 00:39;alexliu68;you need check how many times it 

{code}
  Executing single-partition query on cf_300000_keys_50_cols.color
{code}

if you understand the issue.;;;","07/Sep/13 00:59;alexliu68;The correct behavior is that there is only one  ""Executing single-partition query on cf_300000_keys_50_cols.color"" in the tracing log for your test case.;;;","07/Sep/13 08:56;aleksey;{noformat}
 activity                                                                          | timestamp    | source    | source_elapsed
-----------------------------------------------------------------------------------+--------------+-----------+----------------
                                                                execute_cql3_query | 11:55:57,938 | 127.0.0.1 |              0
       Parsing select * from cf_300000_keys_50_cols where color = 'green' limit 1; | 11:55:57,938 | 127.0.0.1 |             48
                                                                Peparing statement | 11:55:57,938 | 127.0.0.1 |            325
                                                     Determining replicas to query | 11:55:57,939 | 127.0.0.1 |            610
 Executing indexed scan for [min(-9223372036854775808), min(-9223372036854775808)] | 11:55:57,939 | 127.0.0.1 |           1276
                  Executing single-partition query on cf_300000_keys_50_cols.color | 11:55:57,940 | 127.0.0.1 |           1625
                                                      Acquiring sstable references | 11:55:57,940 | 127.0.0.1 |           1640
                                                       Merging memtable tombstones | 11:55:57,940 | 127.0.0.1 |           1696
                                                       Key cache hit for sstable 7 | 11:55:57,940 | 127.0.0.1 |           1785
                                       Seeking to partition beginning in data file | 11:55:57,940 | 127.0.0.1 |           1799
                                            Bloom filter allows skipping sstable 6 | 11:55:57,940 | 127.0.0.1 |           1855
                                                       Key cache hit for sstable 5 | 11:55:57,940 | 127.0.0.1 |           1901
                                       Seeking to partition beginning in data file | 11:55:57,940 | 127.0.0.1 |           1914
                                        Merging data from memtables and 2 sstables | 11:55:57,940 | 127.0.0.1 |           1954
                                                Read 3 live and 0 tombstoned cells | 11:55:57,940 | 127.0.0.1 |           2148
                        Executing single-partition query on cf_300000_keys_50_cols | 11:55:57,940 | 127.0.0.1 |           2295
                                                      Acquiring sstable references | 11:55:57,940 | 127.0.0.1 |           2308
                                                       Merging memtable tombstones | 11:55:57,940 | 127.0.0.1 |           2352
                                                       Key cache hit for sstable 7 | 11:55:57,940 | 127.0.0.1 |           2421
                                       Seeking to partition beginning in data file | 11:55:57,940 | 127.0.0.1 |           2434
                                            Bloom filter allows skipping sstable 6 | 11:55:57,940 | 127.0.0.1 |           2480
                                            Bloom filter allows skipping sstable 1 | 11:55:57,940 | 127.0.0.1 |           2511
                                        Merging data from memtables and 1 sstables | 11:55:57,940 | 127.0.0.1 |           2531
                                                Read 1 live and 0 tombstoned cells | 11:55:57,942 | 127.0.0.1 |           4260
                                                      Scanned 1 rows and matched 1 | 11:55:57,942 | 127.0.0.1 |           4492
                                                                  Request complete | 11:55:57,944 | 127.0.0.1 |           6000
{noformat};;;","08/Sep/13 00:39;alexliu68;I attach the patch I applied as patch.txt which is same as 5975-v2.txt.

The following is testing on my testing data

{code}
cqlsh:ks> select * from cf_300000_keys_50_cols where color='red' limit 1;

 key      | color | column_0 | qty | size  | time
----------+-------+----------+-----+-------+--------------------------
 key_0002 |   red | column_0 | 124 | large | 2000-01-04 14:19:59-0500


Tracing session: cef56100-181e-11e3-91e4-fb4a940b08ec

 activity                                                                          | timestamp    | source    | source_elapsed
-----------------------------------------------------------------------------------+--------------+-----------+----------------
                                                                execute_cql3_query | 20:37:15,154 | 127.0.0.1 |              0
           Parsing select * from cf_300000_keys_50_cols where color='red' limit 1; | 20:37:15,155 | 127.0.0.1 |            918
                                                                Peparing statement | 20:37:15,156 | 127.0.0.1 |           2176
                                                     Determining replicas to query | 20:37:15,159 | 127.0.0.1 |           5555
 Executing indexed scan for [min(-9223372036854775808), min(-9223372036854775808)] | 20:37:15,161 | 127.0.0.1 |           7353
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:37:15,162 | 127.0.0.1 |           7914
                                                      Acquiring sstable references | 20:37:15,162 | 127.0.0.1 |           7925
                                                       Merging memtable tombstones | 20:37:15,162 | 127.0.0.1 |           7973
                                                       Key cache hit for sstable 5 | 20:37:15,162 | 127.0.0.1 |           8055
                                       Seeking to partition beginning in data file | 20:37:15,162 | 127.0.0.1 |           8068
                                        Merging data from memtables and 1 sstables | 20:37:15,162 | 127.0.0.1 |           8334
                                                Read 3 live and 0 tombstoned cells | 20:37:15,162 | 127.0.0.1 |           8429
                        Executing single-partition query on cf_300000_keys_50_cols | 20:37:15,163 | 127.0.0.1 |           8734
                                                      Acquiring sstable references | 20:37:15,163 | 127.0.0.1 |           8745
                                                       Merging memtable tombstones | 20:37:15,163 | 127.0.0.1 |           8800
                                                       Key cache hit for sstable 5 | 20:37:15,163 | 127.0.0.1 |           8851
                                       Seeking to partition beginning in data file | 20:37:15,163 | 127.0.0.1 |           8860
                                        Merging data from memtables and 1 sstables | 20:37:15,163 | 127.0.0.1 |           9042
                                                Read 1 live and 0 tombstoned cells | 20:37:15,163 | 127.0.0.1 |           9124
                        Executing single-partition query on cf_300000_keys_50_cols | 20:37:15,163 | 127.0.0.1 |           9406
                                                      Acquiring sstable references | 20:37:15,163 | 127.0.0.1 |           9416
                                                       Merging memtable tombstones | 20:37:15,163 | 127.0.0.1 |           9443
                                                       Key cache hit for sstable 5 | 20:37:15,163 | 127.0.0.1 |           9487
                                       Seeking to partition beginning in data file | 20:37:15,163 | 127.0.0.1 |           9496
                                        Merging data from memtables and 1 sstables | 20:37:15,163 | 127.0.0.1 |           9522
                                                Read 1 live and 0 tombstoned cells | 20:37:15,163 | 127.0.0.1 |           9591
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:37:15,164 | 127.0.0.1 |           9689
                                                      Acquiring sstable references | 20:37:15,164 | 127.0.0.1 |           9705
                                                       Merging memtable tombstones | 20:37:15,164 | 127.0.0.1 |           9733
                                                       Key cache hit for sstable 5 | 20:37:15,164 | 127.0.0.1 |           9776
                                 Seeking to partition indexed section in data file | 20:37:15,164 | 127.0.0.1 |           9790
                                        Merging data from memtables and 1 sstables | 20:37:15,164 | 127.0.0.1 |           9868
                                                Read 2 live and 0 tombstoned cells | 20:37:15,164 | 127.0.0.1 |           9950
                        Executing single-partition query on cf_300000_keys_50_cols | 20:37:15,164 | 127.0.0.1 |          10095
                                                      Acquiring sstable references | 20:37:15,164 | 127.0.0.1 |          10104
                                                       Merging memtable tombstones | 20:37:15,164 | 127.0.0.1 |          10138
                                                       Key cache hit for sstable 5 | 20:37:15,164 | 127.0.0.1 |          10200
                                       Seeking to partition beginning in data file | 20:37:15,164 | 127.0.0.1 |          10209
                                        Merging data from memtables and 1 sstables | 20:37:15,164 | 127.0.0.1 |          10231
                                                Read 1 live and 0 tombstoned cells | 20:37:15,164 | 127.0.0.1 |          10305
                  Executing single-partition query on cf_300000_keys_50_cols.color | 20:37:15,164 | 127.0.0.1 |          10395
                                                      Acquiring sstable references | 20:37:15,164 | 127.0.0.1 |          10404
                                                       Merging memtable tombstones | 20:37:15,164 | 127.0.0.1 |          10429
                                                       Key cache hit for sstable 5 | 20:37:15,164 | 127.0.0.1 |          10471
                                 Seeking to partition indexed section in data file | 20:37:15,164 | 127.0.0.1 |          10479
                                        Merging data from memtables and 1 sstables | 20:37:15,164 | 127.0.0.1 |          10546
                                                Read 1 live and 0 tombstoned cells | 20:37:15,164 | 127.0.0.1 |          10594
                                                      Scanned 3 rows and matched 3 | 20:37:15,165 | 127.0.0.1 |          10713
                                                                  Request complete | 20:37:15,165 | 127.0.0.1 |          11030

cqlsh:ks> select * from cf_300000_keys_50_cols;                          

 key      | color | column_0  | qty | size  | time
----------+-------+-----------+-----+-------+--------------------------
 key_0002 |   red |  column_0 | 124 | large | 2000-01-04 14:19:59-0500
 key_0003 |   red |  column_0 | 125 | small |                     null
 key_0001 |   red | column__0 | 123 | small | 2000-01-04 14:19:59-0500
 key_0006 |  blue |  column_1 | 111 | small |                     null
 key_0005 |  blue |  column_1 | 111 | small |                     null


Tracing session: d35538b0-181e-11e3-91e4-fb4a940b08ec

 activity                                                                                        | timestamp    | source    | source_elapsed
-------------------------------------------------------------------------------------------------+--------------+-----------+----------------
                                                                              execute_cql3_query | 20:37:22,492 | 127.0.0.1 |              0
                                       Parsing select * from cf_300000_keys_50_cols LIMIT 10000; | 20:37:22,492 | 127.0.0.1 |             81
                                                                              Peparing statement | 20:37:22,492 | 127.0.0.1 |            322
                                                                   Determining replicas to query | 20:37:22,492 | 127.0.0.1 |            551
 Executing seq scan across 2 sstables for [min(-9223372036854775808), min(-9223372036854775808)] | 20:37:22,493 | 127.0.0.1 |            950
                                                                    Scanned 5 rows and matched 5 | 20:37:22,518 | 127.0.0.1 |          26549
                                                                                Request complete | 20:37:22,519 | 127.0.0.1 |          27123
{code}

It's broken.;;;","08/Sep/13 00:39;alexliu68;It's easy to reproduce it using a smaller set of data.;;;","08/Sep/13 00:45;alexliu68;the testing table is 
{code}
qlsh:ks> describe table cf_300000_keys_50_cols;

CREATE TABLE cf_300000_keys_50_cols (
  key text PRIMARY KEY,
  color text,
  column_0 text,
  qty int,
  size text,
  time timestamp
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

CREATE INDEX color ON cf_300000_keys_50_cols (color);

CREATE INDEX qty ON cf_300000_keys_50_cols (qty);

CREATE INDEX size ON cf_300000_keys_50_cols (size);

CREATE INDEX time ON cf_300000_keys_50_cols (time);
{code};;;","08/Sep/13 00:52;aleksey;I believe that that's what you are seeing, and at this point I'm really curious how it's possible that you see those results.

For the record, using a similar dataset to your last one:

{noformat}
cqlsh:ks> select * from cf_300000_keys_50_cols ;

 key   | color | column_0 | qty | size | time
-------+-------+----------+-----+------+--------------------------
 key_1 |   red |  value_0 |   0 |    P | 2000-01-01 00:00:01+0300
 key_3 |   red |  value_0 |   2 |    P | 2000-01-01 00:00:03+0300
 key_4 |   red |  value_0 |   3 |    P | 2000-01-01 00:00:04+0300
 key_2 |  blue |  value_0 |   1 |    P | 2000-01-01 00:00:02+0300
 key_5 |  blue |  value_0 |   4 |    P | 2000-01-01 00:00:05+0300

cqlsh:ks> select * FROM cf_300000_keys_50_cols where color = 'red' limit 1;

 key   | color | column_0 | qty | size | time
-------+-------+----------+-----+------+--------------------------
 key_1 |   red |  value_0 |   0 |    P | 2000-01-01 00:00:01+0300


Tracing session: 7913aec0-1820-11e3-a85f-070c076eda0a

 activity                                                                          | timestamp    | source    | source_elapsed
-----------------------------------------------------------------------------------+--------------+-----------+----------------
                                                                execute_cql3_query | 03:49:10,060 | 127.0.0.1 |              0
         Parsing select * FROM cf_300000_keys_50_cols where color = 'red' limit 1; | 03:49:10,060 | 127.0.0.1 |             58
                                                                Peparing statement | 03:49:10,061 | 127.0.0.1 |            289
                                                     Determining replicas to query | 03:49:10,061 | 127.0.0.1 |            485
 Executing indexed scan for [min(-9223372036854775808), min(-9223372036854775808)] | 03:49:10,061 | 127.0.0.1 |           1077
                  Executing single-partition query on cf_300000_keys_50_cols.color | 03:49:10,062 | 127.0.0.1 |           1175
                                                      Acquiring sstable references | 03:49:10,062 | 127.0.0.1 |           1190
                                                       Merging memtable tombstones | 03:49:10,062 | 127.0.0.1 |           1221
                                        Merging data from memtables and 0 sstables | 03:49:10,062 | 127.0.0.1 |           1273
                                                Read 3 live and 0 tombstoned cells | 03:49:10,062 | 127.0.0.1 |           1431
                        Executing single-partition query on cf_300000_keys_50_cols | 03:49:10,062 | 127.0.0.1 |           1577
                                                      Acquiring sstable references | 03:49:10,062 | 127.0.0.1 |           1587
                                                       Merging memtable tombstones | 03:49:10,062 | 127.0.0.1 |           1610
                                        Merging data from memtables and 0 sstables | 03:49:10,062 | 127.0.0.1 |           1636
                                                Read 1 live and 0 tombstoned cells | 03:49:10,062 | 127.0.0.1 |           1697
                                                      Scanned 1 rows and matched 1 | 03:49:10,062 | 127.0.0.1 |           1812
                                                                  Request complete | 03:49:10,062 | 127.0.0.1 |           2031
{noformat};;;","08/Sep/13 02:04;alexliu68;We need a third person to do the final testing as we get different results for the same patch.;;;","08/Sep/13 03:14;aleksey;It's just performing the cleanup of stale data. See CASSANDRA-2897. What you see in your tracing output is that CompositesSearcher is paging through the 2i partition, fetching rows from the base table, seeing stale entries, and moving on to the next index cell, until it finds enough non-stale base rows to satisfy the limit or exhausts all the cells in the 2i partition.;;;","08/Sep/13 10:15;jbellis;So if Alex forces a major compaction he should see what you're seeing unless there's a bug in our indexing code, right?;;;","08/Sep/13 10:17;jbellis;(Keeping in mind that CASSANDRA-5614 has only been fixed in the unreleased 2.0.1.);;;","08/Sep/13 13:32;aleksey;Or if he runs the same query one more time (with the same or lower limit). Stale entries will be cleaned up during the first execution. (Assuming no updates/deletes between the two SELECTs).;;;","09/Sep/13 17:59;alexliu68;I run the same query with limit 1 for a few times, and I get the same results.;;;","09/Sep/13 19:00;alexliu68;The patch is good on applied to Cassandra-1.2 branch. I had applied it to 1.2.6 which tells the difference between my testing and [~iamaleksey]'s

Some patches between 1.2.6 and 1.2.9 fix the issues that countCQL3Rows is somehow not set to true in the ExtendedFilter;;;","10/Sep/13 11:59;slebresne;The most recently attached 5975-v2.txt lgtm, +1.;;;","10/Sep/13 14:59;aleksey;Committed, thanks.;;;","16/Sep/13 13:44;petter;I volunteer to test this fix in my environment/my design prior to 2.0.1 if you merge it.;;;","16/Sep/13 13:46;aleksey;[~petter] It's already in 1.2.10 and 2.0.1;;;","17/Sep/13 08:08;petter;[~iamaleksey] Oh. Sorry. Shouldn't 2.0.1 be added to this issues Fix Version/s field then and be part of the release notes? Maybe I'm jumping ahead of things here. I'll check out the code and have a go.;;;","17/Sep/13 12:19;jbellis;Tagged 2.0.1.;;;","19/Sep/13 14:04;petter;The problem I was seeing (using secondary indexes on fields part of a compound primary key) seems to be solved with this commit. Good work!;;;","15/Jan/14 16:36;srrepaka;I think we have the similar issue on 1.2.5. Can a patch be applied to 1.2.5? Appreciate any response. thanks.;;;","15/Jan/14 17:44;jbellis;You can try to apply the patch as easily as we can.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool info throws NPE when connected to a booting instance,CASSANDRA-5968,12666573,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,jalkanen,jalkanen,02/Sep/13 12:21,16/Apr/19 09:32,14/Jul/23 05:53,13/Sep/13 17:20,1.2.10,2.0.1,,,,,0,,,,,"When an instance is newly added to the cluster and it's still streaming stuff, trying to call nodetool info on it throws NPE. Stack trace below.

To replicate: add a new node to the cluster, run nodetool info before bootstrap is complete.

Expected behaviour: is nice and just says RPC server is not running.

{noformat}
$ nodetool info
Token            : (invoke with -T/--tokens to see all 0 tokens)
ID               : cc7bcf48-4a54-48af-97f6-99c82bce76f2
Gossip active    : true
Exception in thread ""main"" java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.isRPCServerRunning(StorageService.java:330)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.Trampoline.invoke(MethodUtil.java:75)
	at sun.reflect.GeneratedMethodAccessor7.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.reflect.misc.MethodUtil.invoke(MethodUtil.java:279)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:112)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:46)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:237)
	at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
	at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:206)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:647)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:678)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1464)
	at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:97)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1328)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1420)
	at javax.management.remote.rmi.RMIConnectionImpl.getAttribute(RMIConnectionImpl.java:657)
	at sun.reflect.GeneratedMethodAccessor12.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:606)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
	at sun.rmi.transport.Transport$1.run(Transport.java:177)
	at sun.rmi.transport.Transport$1.run(Transport.java:174)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:173)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:553)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:808)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:667)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:724)
{noformat}
","Cassandra 1.2.9, Ubuntu 12.04 LTS, Oracle JVM 7u25",jalkanen,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Sep/13 06:29;mishail;cassandra-1.2-5968.patch;https://issues.apache.org/jira/secure/attachment/12601118/cassandra-1.2-5968.patch",,,,,,,,,,,,,,,,,,,,1.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,346511,,,Fri Sep 13 17:20:22 UTC 2013,,,,,,,,,,"0|i1nr3r:",346812,1.2.9,,,,,,,,brandon.williams,,brandon.williams,Low,,1.2.9,,,,,,,,,,,,,,,,"03/Sep/13 06:29;mishail;Patch: thriftServer and nativeServer can be null;;;","13/Sep/13 17:20;brandon.williams;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow local batchlog writes for CL.ANY,CASSANDRA-5967,12666490,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,01/Sep/13 13:42,16/Apr/19 09:32,14/Jul/23 05:53,01/Sep/13 19:35,1.2.10,2.0.1,,,,,0,,,,,"We tell people that ANY means you can write if even a single node is reachable.  We should apply that to batchlog writes as well.

Note: we already allow local batchlog writes for any single-node datacenter; batchlog writes are synchronous, so cross-dc latency would be slow enough to be unusable.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Sep/13 13:42;jbellis;5967.txt;https://issues.apache.org/jira/secure/attachment/12600980/5967.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,346429,,,Sun Sep 01 19:35:04 UTC 2013,,,,,,,,,,"0|i1nqlj:",346730,,,,,,,,,aleksey,,aleksey,Low,,1.2.0,,,,,,,,,,,,,,,,"01/Sep/13 15:15;aleksey;+1;;;","01/Sep/13 19:35;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexSummary load fails when empty key exists in summary,CASSANDRA-5965,12666441,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,yukim,yukim,31/Aug/13 02:20,16/Apr/19 09:32,14/Jul/23 05:53,03/Sep/13 21:18,1.2.10,,,,,,0,,,,,"IndexSummary load fails with the following error when empty key is added to summary:

{code}
ERROR [SSTableBatchOpen:1] 2013-08-30 20:17:41,210 CassandraDaemon.java (line 192) Exception in thread Thread[SSTableBatchOpen:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.utils.ByteBufferUtil.readBytes(ByteBufferUtil.java:401)
        at org.apache.cassandra.io.sstable.IndexSummary$IndexSummarySerializer.deserialize(IndexSummary.java:124)
        at org.apache.cassandra.io.sstable.SSTableReader.loadSummary(SSTableReader.java:491)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:388)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:198)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:157)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:262)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
{code}

I think this is typically caused by indexing empty column, and then the key in index columnfamily is added to its IndexSummary.",,dmeyer,jeromatron,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Aug/13 02:20;yukim;0001-Add-IndexSummary-test.patch;https://issues.apache.org/jira/secure/attachment/12600913/0001-Add-IndexSummary-test.patch","31/Aug/13 02:49;yukim;5965-1.2.txt;https://issues.apache.org/jira/secure/attachment/12600914/5965-1.2.txt",,,,,,,,,,,,,,,,,,,2.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,346380,,,Tue Sep 03 21:18:53 UTC 2013,,,,,,,,,,"0|i1nqan:",346681,1.2.9,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,dmeyer,,,"31/Aug/13 02:20;yukim;Attaching unit test case to reproduce.;;;","31/Aug/13 02:49;yukim;Patch attached to handle key of length 0.;;;","31/Aug/13 03:01;jbellis;+1

Also suggest adding a {{: length}} to the assert, which will make diagnosing such oversights easier in the future.;;;","03/Sep/13 19:19;dmeyer;Patch is good.  To repro the issue it is important to insert an empty string into an indexed column.  Leaving it null, will not cause a repro.

To repro:

set index_interval to 1

CREATE TABLE cf (
  name text PRIMARY KEY,
  val1 text,
  val2 text
) 

CREATE INDEX cf_val2_idx ON cf (val2);
INSERT INTO cf (name, val1, val2) VALUES ('dmeyer', 'testval', '');

Then run:
nodetool rebuild index ks cf cf.cf_val2_idx
flush
restart node
Observe the ERROR in system.log
After applying the patch and building and reproducing the above procedure the error did not occur.;;;","03/Sep/13 21:18;yukim;Committed with new assert comment.
Note that 2.0.0 and above do not have this bug, so I just merged unit test and comment to cassandra-2.0 branch and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh raises a ValueError when connecting to Cassandra running in Eclipse,CASSANDRA-5964,12666433,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,gdeangel,gdeangel,31/Aug/13 00:14,16/Apr/19 09:32,14/Jul/23 05:53,01/Sep/13 05:24,2.0.1,,,,,,0,,,,,"The release_version is set to 'Unknown' in system.local so the version parsing logic fails.

Traceback (most recent call last):
  File ""./cqlsh"", line 2027, in <module>
    main(*read_options(sys.argv[1:], os.environ))
  File ""./cqlsh"", line 2013, in main
    display_float_precision=options.float_precision)
  File ""./cqlsh"", line 486, in __init__
    self.get_connection_versions()
  File ""./cqlsh"", line 580, in get_connection_versions
    self.cass_ver_tuple = tuple(map(int, vers['build'].split('-', 1)[0].split('.')[:3]))
ValueError: invalid literal for int() with base 10: 'Unknown'",,alf239,dbrosius,eric@apache.org,gdeangel,mishail,odpeer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/May/14 09:26;odpeer;5964-v2.patch;https://issues.apache.org/jira/secure/attachment/12647090/5964-v2.patch","31/Aug/13 03:57;dbrosius;5964.txt;https://issues.apache.org/jira/secure/attachment/12600920/5964.txt",,,,,,,,,,,,,,,,,,,2.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,346372,,,Wed May 28 10:48:12 UTC 2014,,,,,,,,,,"0|i1nq8v:",346673,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"31/Aug/13 03:57;dbrosius;generate the version.properties file into ${basedir}/src/resources, and allow it to be copied into classes by the regular build process. Doing this allows eclipse/idea to see it as a resource that needs copying during build as well, and so they don't just delete the file on scrub-build. The IDE should just have src/resources as a src directory.  (Against trunk);;;","01/Sep/13 04:38;jbellis;+1, works on my machine!;;;","01/Sep/13 05:24;dbrosius;committed to 2.0.1 as commit d4884c76c7b6112e3cd2fa6ecf8bc6e0c8ed67f7;;;","25/Feb/14 14:49;eric@apache.org;[~dbrosius@apache.org] Works also for me (adding the src/resources).

I'm not sure if 5964.txt is a correct fix to this, as i still encountered the issue on my env.

Shouldn't we fix this in the ant targets? For eclipse, this is simply adding a <classpathentry kind=""src"" path=""src/resources""/>
Tell me if you like me to open a new JIRA and provide a patch for eclipse / idea?
;;;","01/May/14 21:33;alf239;That's interesting, the HEAD at the moment of writing ([c2579b92|https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=commit;h=c2579b92bf2e721c099720a27b5d9e56be66e49c]) seems to fail exactly as described in the ticket, with the only difference that the line numbering was changed:
{code}
$ ./cqlsh
Traceback (most recent call last):
  File ""./cqlsh"", line 1855, in <module>
    main(*read_options(sys.argv[1:], os.environ))
  File ""./cqlsh"", line 1841, in main
    ssl=options.ssl)
  File ""./cqlsh"", line 490, in __init__
    self.get_connection_versions()
  File ""./cqlsh"", line 578, in get_connection_versions
    self.cass_ver_tuple = tuple(map(int, vers['build'].split('-', 1)[0].split('.')[:3]))
ValueError: invalid literal for int() with base 10: 'Unknown'
{code}

So it does not appear to be fixed in HEAD or, it seems, 2.1.;;;","01/May/14 22:40;mishail;[~alf239] just regenerate {{version.properties}} file and you'll be fine. {{ant generate-eclipse-files}} will do that.;;;","28/May/14 09:12;odpeer;[~mishail] Alexey Filippov is right. Regenerating {{version.properties}} creates the {{version.properties}} file in {{$&#123;build.src.resources&#125;/org/apache/cassandra/config}}, however, {{$&#123;build.src.resources&#125;}} is not configured as an Eclipse src directory causing {{version.properties}} not to be copied to {{$&#123;build.classes.main&#125;}}.

;;;","28/May/14 09:26;odpeer;This is a proposed fix to the issue - adding the resources directory as an Eclipse source directory;;;","28/May/14 10:48;dbrosius;thanks (didn't realize we generated an eclipse .project file :)

 committed cassandra-2.0 as commit 0c96f99e481c4dc70dc8bbe326db10fdbc7fd213;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ant test should only show WARN+ on stdout,CASSANDRA-5961,12666387,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,jbellis,jbellis,30/Aug/13 20:37,16/Apr/19 09:32,14/Jul/23 05:53,31/Aug/13 00:41,2.1 beta1,,,Legacy/Testing,,,0,,,,,We had log4j configured to only output WARN or ERROR messages on stdout during ant test.  Would be nice to get the same behavior on trunk.,,dbrosius,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,346326,,,Sat Aug 31 00:41:38 UTC 2013,,,,,,,,,,"0|i1npyn:",346627,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"31/Aug/13 00:41;dbrosius;add threshold filter on stderr for tests to WARN, committed to trunk as commit 0ec481bd64874e8e03d046c5177dcf0f4a64e500 ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Unable to find property"" errors from snakeyaml are confusing",CASSANDRA-5958,12666334,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,jblangston@datastax.com,jblangston@datastax.com,30/Aug/13 14:39,16/Apr/19 09:32,14/Jul/23 05:53,03/Sep/13 17:12,2.0.1,,,,,,0,,,,,"When an unexpected property is present in cassandra.yaml (e.g. after upgrading), snakeyaml outputs the following message:

{code}Unable to find property 'some_property' on class: org.apache.cassandra.config.Config{code}

The error message is kind of counterintuitive because at first glance it seems to suggest the property is missing from the yaml file, when in fact the error is caused by the *presence* of an unrecognized property.  I know if you read it carefully it says it can't find the property on the class, but this has confused more than one user.

I think we should catch this exception and wrap it in another exception that says something like this:

{code}Please remove 'some_property' from your cassandra.yaml. It is not recognized by this version of Cassandra.{code}

Also, it might make sense to make this a warning instead of a fatal error, and just ignore the unwanted property.",,cburroughs,jblangston@datastax.com,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Sep/13 00:17;mishail;trunk-5958-skip-missing-properties.patch;https://issues.apache.org/jira/secure/attachment/12601017/trunk-5958-skip-missing-properties.patch","02/Sep/13 18:01;mishail;trunk-5958-v2-print-all-invalid-properties.patch;https://issues.apache.org/jira/secure/attachment/12601081/trunk-5958-v2-print-all-invalid-properties.patch",,,,,,,,,,,,,,,,,,,2.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,346273,,,Tue Sep 03 17:12:31 UTC 2013,,,,,,,,,,"0|i1npn3:",346574,2.0 rc2,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"30/Aug/13 15:08;cburroughs;Which version is this with?  We upgraded snakyaml in 2.0.x and it might have improved error messages.;;;","30/Aug/13 16:10;jblangston@datastax.com;1.2 and prior;;;","30/Aug/13 17:26;jblangston@datastax.com;I just tested with 2.0.0-rc2 and the message is the same as before.;;;","02/Sep/13 00:17;mishail;Instruct SnakeYaml to skip missing properties;;;","02/Sep/13 02:22;jbellis;# Skipping missing properties looks like the inverse of the problem here, which is dealing with properties that don't exist in the Config class
# We don't want to skip over errors, just turn the stacktrace into a friendlier message;;;","02/Sep/13 18:01;mishail;A new patch.
Don't skip missing properties, print them all out and terminate.;;;","03/Sep/13 03:57;jbellis;does HashSet.toString actually give us a human-readable error?;;;","03/Sep/13 04:10;mishail;For example I have the following in my cassandra.yaml

{code:title=cassandra.yaml}
oh: my
bla: bla
{code}

Then the stacktrace will be
{code}
ERROR 04:06:57 Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Invalid yaml. Please remove properties [bla, oh] from your cassandra.yaml
        at org.apache.cassandra.config.YamlConfigurationLoader$MissingPropertiesChecker.check(YamlConfigurationLoader.java:131) ~[main/:na]
        at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:94) ~[main/:na]
        at org.apache.cassandra.config.DatabaseDescriptor.loadConfig(DatabaseDescriptor.java:128) ~[main/:na]
        at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:104) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:153) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:391) ~[main/:na]
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:434) ~[main/:na]
Invalid yaml. Please remove properties [bla, oh] from your cassandra.yaml
Fatal configuration error; unable to start. See log for stacktrace.
{code};;;","03/Sep/13 17:12;jbellis;LGTM, committed!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cannot drop keyspace Keyspace1 after running cassandra-stress,CASSANDRA-5957,12666319,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,pkolaczk,pkolaczk,30/Aug/13 13:26,16/Apr/19 09:32,14/Jul/23 05:53,14/Oct/13 09:38,1.2.11,2.0.2,,,,,0,,,,,"Steps to reproduce:
# Set MAX_HEAP=""2G"", HEAP_NEWSIZE=""400M""
# Run ./cassandra-stress -n 50000 -c 400 -S 256
# The test should complete despite several warnings about low heap memory.
# Try to drop keyspace:
{noformat}
cqlsh> drop keyspace Keyspace1;
TSocket read 0 bytes
{noformat}

system.log:
{noformat}
 INFO 15:10:46,516 Enqueuing flush of Memtable-schema_columnfamilies@2127258371(0/0 serialized/live bytes, 1 ops)
 INFO 15:10:46,516 Writing Memtable-schema_columnfamilies@2127258371(0/0 serialized/live bytes, 1 ops)
 INFO 15:10:46,690 Completed flushing /var/lib/cassandra/data/system/schema_columnfamilies/system-schema_columnfamilies-ic-6-Data.db (38 bytes) for commitlog position ReplayPosition(segmentId=1377867520699, position=19794574)
 INFO 15:10:46,692 Enqueuing flush of Memtable-schema_columns@1997964959(0/0 serialized/live bytes, 1 ops)
 INFO 15:10:46,693 Writing Memtable-schema_columns@1997964959(0/0 serialized/live bytes, 1 ops)
 INFO 15:10:46,857 Completed flushing /var/lib/cassandra/data/system/schema_columns/system-schema_columns-ic-6-Data.db (38 bytes) for commitlog position ReplayPosition(segmentId=1377867520699, position=19794574)
 INFO 15:10:46,897 Enqueuing flush of Memtable-local@1366216652(98/98 serialized/live bytes, 3 ops)
 INFO 15:10:46,898 Writing Memtable-local@1366216652(98/98 serialized/live bytes, 3 ops)
 INFO 15:10:47,064 Completed flushing /var/lib/cassandra/data/system/local/system-local-ic-12-Data.db (139 bytes) for commitlog position ReplayPosition(segmentId=1377867520699, position=19794845)
 INFO 15:10:48,956 Enqueuing flush of Memtable-local@432522279(46/46 serialized/live bytes, 1 ops)
 INFO 15:10:48,957 Writing Memtable-local@432522279(46/46 serialized/live bytes, 1 ops)
 INFO 15:10:49,132 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 400882073/1094043713)bytes
 INFO 15:10:49,175 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 147514075/645675954)bytes
 INFO 15:10:49,185 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 223249644/609072261)bytes
 INFO 15:10:49,202 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 346471085/990388210)bytes
 INFO 15:10:49,215 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 294748503/2092376617)bytes
 INFO 15:10:49,257 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 692722235/739328646)bytes
 INFO 15:10:49,285 Completed flushing /var/lib/cassandra/data/system/local/system-local-ic-13-Data.db (82 bytes) for commitlog position ReplayPosition(segmentId=1377867520699, position=19794974)
 INFO 15:10:49,286 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ic-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ic-13-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ic-12-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ic-11-Data.db')]
ERROR 15:10:49,287 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.AssertionError: SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db') was already marked compacted
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:378)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:281)
	at org.apache.cassandra.service.MigrationManager.announceKeyspaceDrop(MigrationManager.java:262)
	at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:718)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:775)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1668)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:4048)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:4036)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError: SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db') was already marked compacted
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:374)
	... 13 more
Caused by: java.lang.AssertionError: SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db') was already marked compacted
	at org.apache.cassandra.db.DataTracker.removeOldSSTablesSize(DataTracker.java:354)
	at org.apache.cassandra.db.DataTracker.postReplace(DataTracker.java:325)
	at org.apache.cassandra.db.DataTracker.unreferenceSSTables(DataTracker.java:264)
	at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:302)
	at org.apache.cassandra.db.Table.unloadCf(Table.java:314)
	at org.apache.cassandra.db.Table.dropCf(Table.java:296)
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:607)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:469)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:355)
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:299)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
ERROR 15:10:49,287 Exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError: SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db') was already marked compacted
	at org.apache.cassandra.db.DataTracker.removeOldSSTablesSize(DataTracker.java:354)
	at org.apache.cassandra.db.DataTracker.postReplace(DataTracker.java:325)
	at org.apache.cassandra.db.DataTracker.unreferenceSSTables(DataTracker.java:264)
	at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:302)
	at org.apache.cassandra.db.Table.unloadCf(Table.java:314)
	at org.apache.cassandra.db.Table.dropCf(Table.java:296)
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:607)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:469)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:355)
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:299)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
 INFO 15:10:49,471 Compacted 4 sstables to [/var/lib/cassandra/data/system/local/system-local-ic-14,].  829 bytes to 501 (~60% of original) in 184ms = 0,002597MB/s.  4 total rows, 1 unique.  Row merge counts were {1:0, 2:0, 3:0, 4:1, }
{noformat}

",Cassandra 1.2.9 freshly built from cassandra-1.2 branch (f5b224cf9aa0f319d51078ef4b78d55e36613963),aecobley,cburroughs,jjordan,pkolaczk,rcoli,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Oct/13 22:10;thobbs;5957-1.2-v1.patch;https://issues.apache.org/jira/secure/attachment/12607668/5957-1.2-v1.patch","11/Oct/13 20:33;thobbs;5957-1.2-v2.patch;https://issues.apache.org/jira/secure/attachment/12608067/5957-1.2-v2.patch","31/Aug/13 19:23;pkolaczk;system.log;https://issues.apache.org/jira/secure/attachment/12600956/system.log",,,,,,,,,,,,,,,,,,3.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,346258,,,Mon Oct 14 09:39:05 UTC 2013,,,,,,,,,,"0|i1npjr:",346559,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"30/Aug/13 13:51;aecobley;IS this because stress test is creating the keyspace via thrift rather than CQL ?  try running the test with the --enable-cql option and then droping the keyspace ?
;;;","30/Aug/13 15:32;pkolaczk;No. I checked with --enable-cql (after starting from fresh empty /var/lib/cassandra) and the problem still persists.;;;","30/Aug/13 17:00;aecobley;Did you try deleting the keyspace with cassandra-cli  before running the test with --enable-cql ?
;;;","31/Aug/13 05:58;pkolaczk;Nope. But I can try it for you. However, my intuition tells me it is unrelated ;)

So far I tried:
- running cassandra-stress without cql and then dropping from cqlsh / cqlsh -3
- running cassandra-stress with --enable-cql and then dropping from cqlsh / cqlsh -3

;;;","31/Aug/13 06:02;pkolaczk;If I let cassandra-stress write only *one* row, deleting keyspace from cqlsh works fine.
I guess the problem might be compaction that is running long after cassandra-stress finished its work. And dropping keyspace during compaction breaks it. ;;;","31/Aug/13 06:29;pkolaczk;Ok, I tried from cassandra-cli and it worked fine. Not sure if it was a one-time luck so I'll try once again.
I tried once again with cqlsh and this time I got a different error:

{noformat}
INFO 08:26:10,314 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 107063147/625780253)bytes
 INFO 08:26:10,326 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 256225944/1056670573)bytes
 INFO 08:26:10,340 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 256445786/533336692)bytes
 INFO 08:26:10,350 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 369114811/593903163)bytes
ERROR 08:26:10,415 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-34-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,415 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-13-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,415 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-36-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,416 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-23-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,416 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-22-Data.db (it will be removed on server restart; we'll also retry after GC)
 INFO 08:26:10,416 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 794619126/1234742593)bytes
 INFO 08:26:10,548 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 186316188/523663644)bytes
 INFO 08:26:10,604 Compaction interrupted: Compaction@4d331c44-f018-302b-91c2-2dcf94c4bfad(Keyspace1, Standard1, 495853724/605444868)bytes
ERROR 08:26:10,618 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-30-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,627 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-15-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,627 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-14-Data.db (it will be removed on server restart; we'll also retry after GC)
ERROR 08:26:10,639 Unable to delete /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-29-Data.db (it will be removed on server restart; we'll also retry after GC)
 INFO 08:26:10,793 Completed flushing /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-92-Data.db (24952067 bytes) for commitlog position ReplayPosition(segmentId=1377929629788, position=10887214)
 INFO 08:26:10,802 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-57-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-83-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-92-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-55-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-91-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-74-Data.db')]
 INFO 08:26:10,805 Compacted 4 sstables to [/var/lib/cassandra/data/system/schema_keyspaces/system-schema_keyspaces-ic-5,].  529 bytes to 249 (~47% of original) in 872ms = 0,000272MB/s.  6 total rows, 3 unique.  Row merge counts were {1:2, 2:0, 3:0, 4:1, }
 INFO 08:26:10,810 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-48-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-52-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-62-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-68-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-87-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-65-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-33-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-81-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-45-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-66-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-69-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-25-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-19-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-21-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-88-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-58-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-84-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-61-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-72-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-73-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-75-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-64-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-59-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-24-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-46-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-76-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-60-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-80-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-79-Data.db')]
 INFO 08:26:10,813 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-40-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-56-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-8-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-50-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-86-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-67-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-49-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-44-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-85-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-26-Data.db')]
ERROR 08:26:10,815 Exception in thread Thread[CompactionExecutor:4,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:53)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1212)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:54)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1032)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1044)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:157)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:163)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:117)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.ThrottledReader.<init>(ThrottledReader.java:35)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:49)
	... 18 more
ERROR 08:26:10,815 Exception in thread Thread[CompactionExecutor:5,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:53)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1212)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:54)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1032)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1044)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:157)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:163)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:117)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.ThrottledReader.<init>(ThrottledReader.java:35)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:49)
	... 18 more
 INFO 08:26:10,821 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-48-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-52-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-62-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-68-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-87-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-65-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-33-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-81-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-45-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-66-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-69-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-25-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-19-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-21-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-88-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-58-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-84-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-61-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-72-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-73-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-75-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-64-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-59-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-24-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-46-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-76-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-60-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-80-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-79-Data.db')]
ERROR 08:26:10,822 Exception in thread Thread[CompactionExecutor:8,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:53)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1212)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:54)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1032)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1044)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:157)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:163)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:117)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.ThrottledReader.<init>(ThrottledReader.java:35)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:49)
	... 18 more
ERROR 08:26:10,862 Exception in thread Thread[MigrationStage:1,5,main]
java.lang.RuntimeException: Tried to hard link to file that does not exist /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Index.db
	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1081)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1567)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1612)
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:608)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:471)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:356)
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:304)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
ERROR 08:26:10,863 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Tried to hard link to file that does not exist /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Index.db
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:379)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:286)
	at org.apache.cassandra.service.MigrationManager.announceKeyspaceDrop(MigrationManager.java:267)
	at org.apache.cassandra.cql.QueryProcessor.processStatement(QueryProcessor.java:718)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:775)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1668)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:4048)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.getResult(Cassandra.java:4036)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Tried to hard link to file that does not exist /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Index.db
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:375)
	... 13 more
Caused by: java.lang.RuntimeException: Tried to hard link to file that does not exist /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Index.db
	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1081)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1567)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1612)
	at org.apache.cassandra.db.DefsTable.dropColumnFamily(DefsTable.java:608)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:471)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:356)
	at org.apache.cassandra.service.MigrationManager$2.runMayThrow(MigrationManager.java:304)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
 INFO 08:26:19,760 Compacted 6 sstables to [/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-93,].  137 181 408 bytes to 137 181 408 (~100% of original) in 8 958ms = 14,604419MB/s.  1 248 total rows, 1 248 unique.  Row merge counts were {1:1248, 2:0, 3:0, 4:0, 5:0, 6:0, }
 INFO 08:26:19,765 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-40-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-56-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-8-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-50-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-86-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-67-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-49-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-44-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-85-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-26-Data.db')]
ERROR 08:26:19,765 Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:53)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1212)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:54)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1032)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1044)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:157)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:163)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:117)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.ThrottledReader.<init>(ThrottledReader.java:35)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:49)
	... 18 more
 INFO 08:26:19,766 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-48-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-52-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-62-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-68-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-87-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-65-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-33-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-81-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-45-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-28-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-66-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-69-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-25-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-19-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-21-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-88-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-58-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-84-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-61-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-78-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-72-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-73-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-75-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-64-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-59-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-24-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-46-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-76-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-60-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-80-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-79-Data.db')]
 INFO 08:26:19,769 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-40-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-56-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-8-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-50-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-86-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-67-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-49-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-44-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-85-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-26-Data.db')]
ERROR 08:26:19,769 Exception in thread Thread[CompactionExecutor:3,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:53)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1212)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:54)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1032)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1044)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:157)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:163)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:117)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-31-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.ThrottledReader.<init>(ThrottledReader.java:35)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:49)
	... 18 more
ERROR 08:26:19,771 Exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db (No such file or directory)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:53)
	at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1212)
	at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:54)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1032)
	at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:1044)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:157)
	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:163)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:117)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:208)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-10-Data.db (No such file or directory)
	at java.io.RandomAccessFile.open(Native Method)
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:216)
	at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
	at org.apache.cassandra.io.util.ThrottledReader.<init>(ThrottledReader.java:35)
	at org.apache.cassandra.io.util.ThrottledReader.open(ThrottledReader.java:49)
	... 18 more

{noformat};;;","31/Aug/13 06:30;pkolaczk;And on the cqlsh side:

{noformat}
pkolaczk@m4600:~/Projekty/datastax/cassandra$ bin/cqlsh 
Connected to Test Cluster at 127.0.0.1:9160.
[cqlsh 3.1.7 | Cassandra 0.0.0 | CQL spec 2.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> drop keyspace Keyspace1;
TSocket read 0 bytes
cqlsh> drop keyspace Keyspace1;
TSocket read 0 bytes
cqlsh> drop keyspace Keyspace1;
Traceback (most recent call last):
  File ""bin/cqlsh"", line 1038, in perform_statement_untraced
    self.cursor.execute(statement, decoder=decoder)
  File ""bin/../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/cursor.py"", line 80, in execute
    response = self.get_response(prepared_q, cl)
  File ""bin/../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/thrifteries.py"", line 80, in get_response
    return self.handle_cql_execution_errors(doquery, compressed_q, compress)
  File ""bin/../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/thrifteries.py"", line 96, in handle_cql_execution_errors
    return executor(*args, **kwargs)
  File ""bin/../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/cassandra/Cassandra.py"", line 1741, in execute_cql_query
    self.send_execute_cql_query(query, compression)
  File ""bin/../lib/cql-internal-only-1.4.0.zip/cql-1.4.0/cql/cassandra/Cassandra.py"", line 1751, in send_execute_cql_query
    self._oprot.trans.flush()
  File ""bin/../lib/thrift-python-internal-only-0.7.0.zip/thrift/transport/TTransport.py"", line 293, in flush
    self.__trans.write(buf)
  File ""bin/../lib/thrift-python-internal-only-0.7.0.zip/thrift/transport/TSocket.py"", line 117, in write
    plus = self.handle.send(buff)
error: [Errno 32] Broken pipe


{noformat};;;","31/Aug/13 06:32;pkolaczk;After this I cannot also drop it from cassandra-cli:

{noformat}

pkolaczk@m4600:~/Projekty/datastax/cassandra$ bin/cassandra-cli
Column Family assumptions read from /home/pkolaczk/.cassandra/assumptions.json
Connected to: ""Test Cluster"" on 127.0.0.1/9160
Welcome to Cassandra CLI version 1.2.9-SNAPSHOT

Type 'help;' or '?' for help.
Type 'quit;' or 'exit;' to quit.

[default@unknown] drop keyspace Keyspace1;
null
org.apache.thrift.transport.TTransportException
	at org.apache.thrift.transport.TIOStreamTransport.read(TIOStreamTransport.java:132)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.transport.TFramedTransport.readFrame(TFramedTransport.java:129)
	at org.apache.thrift.transport.TFramedTransport.read(TFramedTransport.java:101)
	at org.apache.thrift.transport.TTransport.readAll(TTransport.java:84)
	at org.apache.thrift.protocol.TBinaryProtocol.readAll(TBinaryProtocol.java:378)
	at org.apache.thrift.protocol.TBinaryProtocol.readI32(TBinaryProtocol.java:297)
	at org.apache.thrift.protocol.TBinaryProtocol.readMessageBegin(TBinaryProtocol.java:204)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:69)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_system_drop_keyspace(Cassandra.java:1437)
	at org.apache.cassandra.thrift.Cassandra$Client.system_drop_keyspace(Cassandra.java:1424)
	at org.apache.cassandra.cli.CliClient.executeDelKeySpace(CliClient.java:1364)
	at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:249)
	at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:213)
	at org.apache.cassandra.cli.CliMain.main(CliMain.java:339)

{noformat};;;","31/Aug/13 17:01;aecobley;I'm having trouble reproducing this myself.  Can you post your cluster size, OS and java version ?
;;;","31/Aug/13 19:17;pkolaczk;Ubuntu Linux 13.04, single node
{noformat}
java version ""1.6.0_45""
Java(TM) SE Runtime Environment (build 1.6.0_45-b06)
Java HotSpot(TM) 64-Bit Server VM (build 20.45-b01, mixed mode)
{noformat};;;","31/Aug/13 19:23;pkolaczk;Attaching system.log from the whole last run.;;;","02/Sep/13 14:20;pkolaczk;Today I got this on C* 1.2.6 dse 3.1.2. Slightly different exception, but maybe will be helpful:

{noformat}
ERROR 16:16:37,702 Error occurred during processing of message.
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Tried to hard link to file that does not exist /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-64-Statistics.db
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:378)
	at org.apache.cassandra.service.MigrationManager.announce(MigrationManager.java:281)
	at org.apache.cassandra.service.MigrationManager.announceKeyspaceDrop(MigrationManager.java:262)
	at org.apache.cassandra.cql3.statements.DropKeyspaceStatement.announceMigration(DropKeyspaceStatement.java:60)
	at org.apache.cassandra.cql3.statements.SchemaAlteringStatement.execute(SchemaAlteringStatement.java:73)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:145)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:162)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1714)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:201)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.util.concurrent.ExecutionException: java.lang.RuntimeException: Tried to hard link to file that does not exist /var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ic-64-Statistics.db
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:374)
	... 15 more
{noformat};;;","09/Oct/13 22:10;thobbs;Quick summary of the problem: when the table is dropped, there are multiple ongoing compactions.  When handling the table drop, DataTracker first builds a view of non-compacting sstables.  Meanwhile, the compaction tasks get cancelled, and as part of the cleanup, it marks the sstables as compacted (because the column family is no longer valid).  Then the DataTracker finally tries to mark the sstables in its view as compacted, hitting the failing assertion.  Additionally, when a compaction task is cancelled, it doesn't unreference the sstables involved, and since the DataTracker ignored compacting sstables, they never get unreferenced (even if the failing assertion is disabled).

5957-1.2-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-5957]) uses the simplest fix I could think of, which is to block the table drop operation until all compactions on that table have stopped (either normally, because of an error, or because they saw the stop signal).  This way, when the DataTracker cleans up sstables as part of the drop operation, there are no compacting sstables.;;;","10/Oct/13 02:45;jbellis;I think we have a problem still.  We could have a compaction started by another thread after we stop the existing ones.  This requires the schema to be fetched before the purge, then the thread to be suspended until after we purge and stop compactions.  Rare, but possible.

Turns out it was truncate I was thinking of that changed in 2.0.  What we did there was introduce CFS.runWithCompactionsDisabled to avoid this race.

So you might want to backport that (d72e9381 for CASSANDRA-3430 -- I don't remember if this introduced bugs that we fixed in later commits.  Quite possible) but it's a bit involved.

I think there's a similar problem with flushing.  With truncate we don't care (if new writes arrive after the truncate starts, we're totally fine with having them survive; in fact, that's desirable) but we might care here.

All things considered I suspect it will be simpler to fix the cleanup race rather than try to lock down everything for the drop.;;;","11/Oct/13 20:33;thobbs;5957-1.2-v2.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-5957-v2]) disables the ""was not marked compacted"" assertion when cleaning up sstables for a dropped table and unreferences sstables when compaction tasks finish (or are interrupted) if the table has been dropped.

Regarding flushing, a blocking flush is forced after unregistering the CF but prior to the CFS being invalidated (which is when sstables get unreferenced), so we shouldn't see a similar problem there (and I haven't seen one while testing this).;;;","14/Oct/13 09:39;jbellis;LGTM; committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The native protocol server can trigger a Netty bug,CASSANDRA-5955,12666304,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,30/Aug/13 11:30,16/Apr/19 09:32,14/Jul/23 05:53,02/Sep/13 16:17,1.2.10,,,,,,0,,,,,"The patch from CASSANDRA-5926 did fix the original deadlock, but unfortunately we can now run into a netty bug (with MemoryAwareThreadPoolExecutor): https://github.com/netty/netty/issues/1310.

That bug has been fixed in netty 3.6.6 but we're currently using an older version (3.5.9). So we should just upgrade our dependency to 3.6.6. ",,cburroughs,jjordan,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,346243,,,Mon Sep 02 16:17:53 UTC 2013,,,,,,,,,,"0|i1npgf:",346544,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"30/Aug/13 14:42;jbellis;+1 on principle;;;","02/Sep/13 16:17;slebresne;Alright, dependency updated.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make nodetool ring print an error message and suggest nodetool status when vnodes are enabled,CASSANDRA-5954,12666245,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lyubent,jbellis,jbellis,30/Aug/13 02:53,16/Apr/19 09:32,14/Jul/23 05:53,24/Sep/13 15:31,1.2.11,2.0.2,,Tool/nodetool,,,0,,,,,,,aecobley,jjordan,lyubent,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Sep/13 14:15;lyubent;5954.patch;https://issues.apache.org/jira/secure/attachment/12604038/5954.patch",,,,,,,,,,,,,,,,,,,,1.0,lyubent,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,346184,,,Tue Sep 24 15:31:01 UTC 2013,,,,,,,,,,"0|i1np3r:",346485,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"30/Aug/13 03:04;brandon.williams;Well, do need *some* way to list vnode tokens.  Maybe add something like a -YES flag when using ring w/vnodes.;;;","30/Aug/13 03:18;jbellis;We can get it from a system table right?;;;","30/Aug/13 07:56;aecobley;Can we be careful with this change ?  I'm thinking there may be scripts out there that rely on the current behavior that may break if you change it (does opcenter use nodetool for its operations ?).  It seems to me the original suggestion is because nodetool ring is not the best way to monitor vnodes and user education is needed on the role of nodetool status.

With that in mind, would it be better to simply add a message at the bottom of the output of nodetool ring with vnodes suggesting nodetool status ?  That should minimise any script problems as they should just ignore the last line ?;;;","30/Aug/13 12:22;jjordan;I agree, put a message at the bottom of the output which says ""Now that you watched 5000 tokens scroll by, you probably want nodetool status"".;;;","30/Aug/13 19:11;brandon.williams;bq. We can get it from a system table right?

Yes, but then you'd have to calculate ownership yourself.;;;","19/Sep/13 14:16;lyubent;Added a warning message with a suggestion to use ""nodetool status"" next time.;;;","24/Sep/13 15:31;brandon.williams;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make snapshot/sequential repair the default,CASSANDRA-5950,12666169,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lyubent,jbellis,jbellis,29/Aug/13 17:40,16/Apr/19 09:32,14/Jul/23 05:53,28/Sep/13 00:47,2.0.2,,,Legacy/Tools,,,0,,,,,,,dbrosius,jjordan,lyubent,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6283,,,,,,,,,,,"19/Sep/13 10:18;lyubent;5950.patch;https://issues.apache.org/jira/secure/attachment/12604020/5950.patch","25/Sep/13 16:28;lyubent;5950_v2.patch;https://issues.apache.org/jira/secure/attachment/12605032/5950_v2.patch","26/Sep/13 17:36;lyubent;5950_v3.patch;https://issues.apache.org/jira/secure/attachment/12605286/5950_v3.patch",,,,,,,,,,,,,,,,,,3.0,lyubent,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,346108,,,Sat Sep 28 00:47:55 UTC 2013,,,,,,,,,,"0|i1nomv:",346409,,,,,,,,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,,"19/Sep/13 10:17;lyubent;Changed the function of the SNAPSHOT_REPAIR_OPT parameter to represent a parallel repair.;;;","25/Sep/13 03:31;dbrosius;The patch itself is fine, but the existing setting name isn't very good imo.

the setting is snapshot... true/false? 

to me that would mean create snapshots or not.

not parallel, serial.

given that we are changing the sense of the parameter, i think it would make sense to change the name so that folks aren't confused by the flip. (and to make it more clear)

;;;","25/Sep/13 16:28;lyubent;Changed the param from snapshot to *parallel* The command is one of the below:

{code}
./nodetool repair -parallel
./nodetool repair -par
./nodetool repair
{code};;;","26/Sep/13 00:39;dbrosius;probably should rename 

boolean snapshot, to boolean sequential

also please fix up NodeToolHelp.yaml;;;","26/Sep/13 17:36;lyubent;Renamed variable to *sequential* and updated NodeToolHelp.yaml;;;","28/Sep/13 00:47;dbrosius;+1 thanks, committed to cassandra-2.0 as commit 3a7093356ca032d9ce8767b2c47980aebe4bce60;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CqlRecordWriter sends empty composite partition-key components to thrift,CASSANDRA-5949,12666030,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mccraig,mccraigmccraig,mccraigmccraig,28/Aug/13 23:12,16/Apr/19 09:32,14/Jul/23 05:53,28/Aug/13 23:27,1.2.10,,,,,,0,,,,,"when there is a composite partition-key, CqlRecordWriter.getPartitionKey() consumes the content of the key-component ByteBuffers, leaving empty key-components to be written to thrift",,mccraigmccraig,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,mccraig,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345969,,,Wed Aug 28 23:27:38 UTC 2013,,,,,,,,,,"0|i1nnrz:",346270,1.2.8,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"28/Aug/13 23:23;mccraigmccraig;patch in https://github.com/apache/cassandra/pull/19 builds the composite partition key with duplicate components;;;","28/Aug/13 23:27;jbellis;Committed.  (To 1.2 only; CompositeType.build in 2.0 already builds in duplicate()).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StreamOut doesn't correctly handle wrapped ranges,CASSANDRA-5948,12665973,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sbtourist,sbtourist,sbtourist,28/Aug/13 18:19,16/Apr/19 09:32,14/Jul/23 05:53,29/Aug/13 15:21,1.2.10,2.0.1,,,,,0,,,,,"StreamOut doesn't normalize ranges, causing AbstractViewSSTableFinder to miss sstables when the requested range is wrapped, and hence breaking node bootstrapping/unbootstrapping on such ranges.",,jjordan,rbranson,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Aug/13 18:21;sbtourist;5948-0001.patch;https://issues.apache.org/jira/secure/attachment/12600426/5948-0001.patch",,,,,,,,,,,,,,,,,,,,1.0,sbtourist,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345912,,,Wed Sep 04 22:54:39 UTC 2013,,,,,,,,,,"0|i1nnfb:",346213,1.2.8,,,,,,,,yukim,,yukim,Normal,,1.2.6,,,,,,,,,,,,,,,,"28/Aug/13 18:22;sbtourist;Attached patch with StreamOut fix and a related test case.;;;","28/Aug/13 19:28;yukim;+1.
This is regression caused by CASSANDRA-5569.;;;","29/Aug/13 15:21;yukim;Committed, thanks!;;;","04/Sep/13 19:28;rbranson;Curious what the impact of this would have been? Has streaming effectively been broken since 1.2.6, since every cluster has wrapping ranges?;;;","04/Sep/13 22:54;yukim;[~rbranson] It only affects bootstrap/decommission/move of wrapping range, since only those use transferRanges. If you do repair after bootstrap or move, then it will sync unfetched data.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Sampling bug in metrics-core-2.0.3.jar used by Cassandra,CASSANDRA-5947,12665816,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,jblangston@datastax.com,jblangston@datastax.com,27/Aug/13 22:59,16/Apr/19 09:32,14/Jul/23 05:53,19/Sep/13 05:59,1.2.10,2.0.1,,Legacy/Tools,,,0,,,,,"There is a sampling bug in the version of the metrics library we're using in Cassandra. See https://github.com/codahale/metrics/issues/421. ExponentiallyDecayingSample is used by the Timer's histogram that is used in stress tool, and according to [~brandon.williams] it is also in a few other places like the dynamic snitch. The statistical theory involved in this bug goes over my head so i'm not sure if this would bug would meaningfully affect its usage by Cassandra.  One of the comments on the bug mentions that it affects slow sampling rates (10 samples/min was the example given).  We're currently distributing metrics-core-2.0.3.jar and according to the release nodes, this bug is fixed in 2.1.3: http://metrics.codahale.com/about/release-notes/#v2-1-3-aug-06-2012",,cburroughs,jblangston@datastax.com,jjordan,ravilr,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/13 20:05;yukim;5947-1.2.txt;https://issues.apache.org/jira/secure/attachment/12603904/5947-1.2.txt","10/Sep/13 21:32;ravilr;Screen Shot 2013-09-10 at 2.23.11 PM.png;https://issues.apache.org/jira/secure/attachment/12602426/Screen+Shot+2013-09-10+at+2.23.11+PM.png",,,,,,,,,,,,,,,,,,,2.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345755,,,Thu Sep 19 05:59:46 UTC 2013,,,,,,,,,,"0|i1nmgf:",346056,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"30/Aug/13 18:58;jbellis;Upgraded metrics-core to 2.2.0.;;;","05/Sep/13 15:42;cburroughs;Obligatory ""you forget to update build.xml"".

One thing that might be worth noting is that this introduces annoying ""quoting"" in jconsole https://github.com/codahale/metrics/commit/e2336060816ad339676c0e162359c17381ded99f  so it's

{noformat}
""org.apache.cassandra.metrics""
{noformat}

instead of 

{noformat}
org.apache.cassandra.metrics
{noformat}

But unless there is some JMX voodoo I'm missing I think that's just a cosmetic issue.  cc [~yukim];;;","05/Sep/13 16:05;jbellis;bq. you forget to update build.xml

fixed.;;;","10/Sep/13 21:29;ravilr;Is it just me or anyone else seeing this issue:  all mbean objectNames of org.apache.cassandra.metrics* have double quotes surrounding them after upgrading to metrics-core-2.2.0.jar.  like ""org.apache.cassandra.metrics"":type=""DroppedMessage"",scope=""READ"",name=""Dropped""  ?
;;;","10/Sep/13 21:32;ravilr;Screenshot of jconsole showing double quoted object names only for org.apache.cassandra.metrics yammer metrics in cassandra-1.2.10(latest cassandra-1.2 branch) with metrics-core-2.2.0.jar;;;","11/Sep/13 00:28;ravilr;Sorry, missed Chris's comment above. If this is going to be the case, can we have the http://wiki.apache.org/cassandra/Metrics and NEWS.txt updated on this change.;;;","12/Sep/13 14:49;jjordan;What jconsole are you running?  The 1.7u21 jconsole doesn't show the quoting.  As [~cburroughs] said earlier, I think it is just cosmetic either way.

Edit: Never mind, I see it now, it sorts the quoted one to the top.;;;","12/Sep/13 15:37;yukim;With this update, I see o.a.c.metrics.ColumnFamily and ThreadPools are unquoted and all others are quoted.
This is because above those metrics build their own JMX object name instead of auto generating from coda metrics name.
We can force JMX name as before, and I think we should to avoid confusion.;;;","18/Sep/13 20:05;yukim;Patch attached to change JMX ObjectName as the same (unquoted) as when we had metrics-core-2.0.3.jar.;;;","18/Sep/13 23:15;jbellis;+1;;;","19/Sep/13 05:59;yukim;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit Logs referencing deleted CFIDs not handled properly,CASSANDRA-5946,12665804,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,rbranson,rbranson,27/Aug/13 22:04,16/Apr/19 09:32,14/Jul/23 05:53,15/Oct/13 22:43,1.2.11,2.0.2,,,,,0,,,,,"ERROR 19:44:38,377 Exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.commitlog.CommitLogAllocator.flushOldestTables(CommitLogAllocator.java:299)
        at org.apache.cassandra.db.commitlog.CommitLogAllocator.fetchSegment(CommitLogAllocator.java:135)
        at org.apache.cassandra.db.commitlog.CommitLog.activateNextSegment(CommitLog.java:333)
        at org.apache.cassandra.db.commitlog.CommitLog.access$100(CommitLog.java:44)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:377)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:46)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.lang.Thread.run(Thread.java:679)

Working up a test case / patch for this.",,rbranson,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Oct/13 09:15;jbellis;5946-v2.txt;https://issues.apache.org/jira/secure/attachment/12608257/5946-v2.txt","09/Oct/13 14:58;jbellis;5946.txt;https://issues.apache.org/jira/secure/attachment/12607569/5946.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345743,,,Tue Oct 15 22:43:44 UTC 2013,,,,,,,,,,"0|i1nmdr:",346044,1.2.8,,,,,,,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,,"02/Oct/13 17:28;jbellis;Still working on this [~rbranson]?;;;","09/Oct/13 14:57;jbellis;I don't see a clean way to fix the drop/append race, so here's my proposed workaround.;;;","14/Oct/13 09:15;jbellis;v2 adds a debug line;;;","14/Oct/13 09:16;jbellis;(Rick emailed that he won't be available to review for a while, so switching to Tyler.);;;","15/Oct/13 16:01;thobbs;+1;;;","15/Oct/13 22:43;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CAS transactions permitting multiple updates,CASSANDRA-5945,12665793,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,ppersad,ppersad,27/Aug/13 21:26,25/Oct/19 13:10,14/Jul/23 05:53,20/Nov/13 00:58,,,,Feature/Lightweight Transactions,,,1,LWT,,,,"This bug is spawned off CASSANDRA-5925 to track an underlying issue not related to TTLs.  To reproduce:

Step 1:

CREATE TABLE IF NOT EXISTS tkns (tkn blob, consumed boolean, PRIMARY KEY (tkn));

Step 2:

INSERT INTO tkns (tkn, consumed) VALUES (?,FALSE);

Step 3:

UPDATE tkns SET consumed = TRUE WHERE tkn = ? IF consumed = FALSE;

Step 4:

UPDATE tkns SET consumed = TRUE WHERE tkn = ? IF consumed = FALSE;

Repeat steps 2-4 about 100,000 times.

Expectation:

For the '[applied]' column in the result sets for steps 3 and 4, exactly one should be true and one should be false.

Bug:

In a small number of cases (varying from 0.002% to 1%) both updates will report success.  See attached unit test.","3 node Cassandra 2.0.0-rc2 cluster
Java driver 1.0.2
Replication factor 3
Quorum consistency",ash2k,ppersad,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5925,,,,,,,,,,,"27/Aug/13 21:27;ppersad;TokenConsumptionTest.java;https://issues.apache.org/jira/secure/attachment/12600258/TokenConsumptionTest.java",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345732,,,Wed Nov 20 00:58:12 UTC 2013,,,,,,,,,,"0|i1nmbb:",346033,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"20/Nov/13 00:58;jbellis;fixed in 2.0.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Bloom filter will be loaded when SSTable is opened for batch,CASSANDRA-5938,12665535,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,26/Aug/13 16:04,16/Apr/19 09:32,14/Jul/23 05:53,26/Aug/13 16:44,2.0.0,,,,,,0,,,,,"We are setting bf and then load bf again.

{code}
sstable.bf = FilterFactory.ALWAYS_PRESENT;
sstable.load(true, false); // should be false for 1st arg
{code}
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Aug/13 16:05;yukim;5983-2.0.0.txt;https://issues.apache.org/jira/secure/attachment/12599963/5983-2.0.0.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345475,,,Mon Aug 26 16:44:34 UTC 2013,,,,,,,,,,"0|i1nkq7:",345776,,,,,,,,,jbellis,,jbellis,Low,,2.0 beta 2,,,,,,,,,,,,,,,,"26/Aug/13 16:05;yukim;Also, I think we don't need to create instance for every AlwaysPresentFilter.;;;","26/Aug/13 16:22;jbellis;+1

Nit: prefer to keep ALL_CAPS for primitives; camel case fine for static final instances;;;","26/Aug/13 16:44;yukim;Committed with nit fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
2.0 read performance is slower than 1.2,CASSANDRA-5933,12665355,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,enigmacurry,enigmacurry,24/Aug/13 00:56,16/Apr/19 09:32,14/Jul/23 05:53,14/Nov/13 05:28,,,,,,,0,,,,,"Over the course of several tests I have observed that 2.0 read performance is noticeably slower than 1.2

Example:

Blue line is 1.2, the rest are various forms of 2.0 rc1 (I've also seen this on rc2, just don't have a good graph handy)

!1.2-faster-than-2.0.png!
!1.2-faster-than-2.0-stats.png!

[See test data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.eager_retry.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1]",,cburroughs,enigmacurry,jjordan,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Aug/13 00:59;enigmacurry;1.2-faster-than-2.0-stats.png;https://issues.apache.org/jira/secure/attachment/12599767/1.2-faster-than-2.0-stats.png","24/Aug/13 00:56;enigmacurry;1.2-faster-than-2.0.png;https://issues.apache.org/jira/secure/attachment/12599766/1.2-faster-than-2.0.png","25/Sep/13 13:50;enigmacurry;5933-new-hardware.png;https://issues.apache.org/jira/secure/attachment/12605008/5933-new-hardware.png",,,,,,,,,,,,,,,,,,3.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345295,,,Thu Nov 14 05:28:47 UTC 2013,,,,,,,,,,"0|i1njmf:",345596,2.0 rc1,2.0 rc2,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"03/Sep/13 02:46;vijay2win@yahoo.com;Ryan, Do you mind testing the custom with 5 to 10 ms... 
I am thinking, we might need enough sample for Percentiles to make more sense (if conformed we might want to wait till the samples arrive etc).;;;","03/Sep/13 03:02;enigmacurry;Hi [~vijay2win@yahoo.com], I'm not sure what you meant by 'custom with 5 to 10 ms'. Can you please clarify the test scenario you'd like me to run?
;;;","03/Sep/13 03:15;vijay2win@yahoo.com;Hi Ryan, You can set a custom speculative execution like the below...
{code}
update column family Standard1 with speculative_retry=10ms;
{code};;;","03/Sep/13 03:21;enigmacurry;Ah, OK, I can run that test, that also applies to CASSANDRA-5932. However, in this case, I don't believe speculative retry can account for all the difference. The red line has none enabled.;;;","25/Sep/13 13:49;enigmacurry;I'm seeing very contradictory results on different hardware.

!5933-new-hardware.png!

[data|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.20_read_regression_4.json&metric=interval_op_rate&operation=stress-read&smoothing=1];;;","14/Nov/13 05:10;jbellis;So did we bottom out at this not actually being a thing?;;;","14/Nov/13 05:28;enigmacurry;Yea, I haven't seen this behavior since I switched machines.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Speculative read performance data show unexpected results,CASSANDRA-5932,12665353,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,enigmacurry,enigmacurry,24/Aug/13 00:51,16/Apr/19 09:32,14/Jul/23 05:53,26/Sep/13 20:55,2.0.2,,,,,,1,,,,,"I've done a series of stress tests with eager retries enabled that show undesirable behavior. I'm grouping these behaviours into one ticket as they are most likely related.

1) Killing off a node in a 4 node cluster actually increases performance.
2) Compactions make nodes slow, even after the compaction is done.
3) Eager Reads tend to lessen the *immediate* performance impact of a node going down, but not consistently.

My Environment:
1 stress machine: node0
4 C* nodes: node4, node5, node6, node7

My script:
node0 writes some data: stress -d node4 -F 30000000 -n 30000000 -i 5 -l 2 -K 20
node0 reads some data: stress -d node4 -n 30000000 -o read -i 5 -K 20

h3. Examples:

h5. A node going down increases performance:

!node-down-increase-performance.png!

[Data for this test here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.eager_retry.node_killed.just_20.json&metric=interval_op_rate&operation=stress-read&smoothing=1]

At 450s, I kill -9 one of the nodes. There is a brief decrease in performance as the snitch adapts, but then it recovers... to even higher performance than before.

h5. Compactions make nodes permanently slow:

!compaction-makes-slow.png!
!compaction-makes-slow-stats.png!

The green and orange lines represent trials with eager retry enabled, they never recover their op-rate from before the compaction as the red and blue lines do.

[Data for this test here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.eager_retry.compaction.2.json&metric=interval_op_rate&operation=stress-read&smoothing=1]

h5. Speculative Read tends to lessen the *immediate* impact:

!eager-read-looks-promising.png!
!eager-read-looks-promising-stats.png!

This graph looked the most promising to me, the two trials with eager retry, the green and orange line, at 450s showed the smallest dip in performance. 

[Data for this test here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.eager_retry.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1]

h5. But not always:

!eager-read-not-consistent.png!
!eager-read-not-consistent-stats.png!

This is a retrial with the same settings as above, yet the 95percentile eager retry (red line) did poorly this time at 450s.

[Data for this test here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.eager_retry.node_killed.just_20.rc1.try2.json&metric=interval_op_rate&operation=stress-read&smoothing=1]",,aleksey,cburroughs,colinkuo,dbrosius,enigmacurry,hubez,jeromatron,jjordan,kohlisankalp,lizou,mstump,rcoli,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4705,,,,,,,,,,,,,,,"29/Sep/13 21:27;enigmacurry;5932-6692c50412ef7d.png;https://issues.apache.org/jira/secure/attachment/12605828/5932-6692c50412ef7d.png","01/Oct/13 15:14;enigmacurry;5932.6692c50412ef7d.compaction.png;https://issues.apache.org/jira/secure/attachment/12606117/5932.6692c50412ef7d.compaction.png","01/Oct/13 20:15;enigmacurry;5932.6692c50412ef7d.rr0.png;https://issues.apache.org/jira/secure/attachment/12606179/5932.6692c50412ef7d.rr0.png","02/Oct/13 15:49;enigmacurry;5932.6692c50412ef7d.rr1.png;https://issues.apache.org/jira/secure/attachment/12606380/5932.6692c50412ef7d.rr1.png","29/Sep/13 16:16;enigmacurry;5932.ded39c7e1c2fa.logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12605805/5932.ded39c7e1c2fa.logs.tar.gz","25/Sep/13 01:36;aleksey;5932.txt;https://issues.apache.org/jira/secure/attachment/12604942/5932.txt","27/Sep/13 14:55;enigmacurry;5933-128_and_200rc1.png;https://issues.apache.org/jira/secure/attachment/12605460/5933-128_and_200rc1.png","27/Sep/13 14:55;enigmacurry;5933-7a87fc11.png;https://issues.apache.org/jira/secure/attachment/12605459/5933-7a87fc11.png","27/Sep/13 14:55;enigmacurry;5933-logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12605461/5933-logs.tar.gz","27/Sep/13 18:42;enigmacurry;5933-randomized-dsnitch-replica.2.png;https://issues.apache.org/jira/secure/attachment/12605518/5933-randomized-dsnitch-replica.2.png","27/Sep/13 21:13;enigmacurry;5933-randomized-dsnitch-replica.3.png;https://issues.apache.org/jira/secure/attachment/12605563/5933-randomized-dsnitch-replica.3.png","27/Sep/13 17:14;enigmacurry;5933-randomized-dsnitch-replica.png;https://issues.apache.org/jira/secure/attachment/12605480/5933-randomized-dsnitch-replica.png","24/Aug/13 01:04;enigmacurry;compaction-makes-slow-stats.png;https://issues.apache.org/jira/secure/attachment/12599771/compaction-makes-slow-stats.png","24/Aug/13 00:51;enigmacurry;compaction-makes-slow.png;https://issues.apache.org/jira/secure/attachment/12599762/compaction-makes-slow.png","24/Aug/13 01:04;enigmacurry;eager-read-looks-promising-stats.png;https://issues.apache.org/jira/secure/attachment/12599770/eager-read-looks-promising-stats.png","24/Aug/13 00:51;enigmacurry;eager-read-looks-promising.png;https://issues.apache.org/jira/secure/attachment/12599761/eager-read-looks-promising.png","24/Aug/13 01:04;enigmacurry;eager-read-not-consistent-stats.png;https://issues.apache.org/jira/secure/attachment/12599769/eager-read-not-consistent-stats.png","24/Aug/13 00:51;enigmacurry;eager-read-not-consistent.png;https://issues.apache.org/jira/secure/attachment/12599760/eager-read-not-consistent.png","24/Aug/13 00:51;enigmacurry;node-down-increase-performance.png;https://issues.apache.org/jira/secure/attachment/12599763/node-down-increase-performance.png",,19.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345293,,,Mon Oct 07 23:01:59 UTC 2013,,,,,,,,,,"0|i1njlz:",345594,2.0 rc1,2.0 rc2,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"16/Sep/13 16:03;hubez;[~enigmacurry], what was your collection interval for the metrics you've collected?

I ask because I'm observing results similar to yours for ""3) Eager Reads tend to lessen the immediate performance impact of a node going down, but not consistently."". However, I've polled metrics @ a 1 second granularity to see that it's actually a multi-second stress-client outage - not just poor and inconsistent performance.

Polling metrics @ a 1 second interval, has observations that a ~20 second read operations starvation outage occurs for the stress client for all data in the cluster (even with the lowest phi_convict_threshold=6).

Analysis so far indicates that high-operations reads starve out all the C* client threads/connections, because they get stuck on awaiting for a server response whenever the key-space hits the node that is down (and by probability + high-operation reads, within 1 second each stress client thread will all hit the downed-node's key-space).



So I'm confirming that I'm also seeing this bug that Speculative reads (even with an ALWAYS setting). It isn't solving this outage for clients during high-operation reads, and based on what I understand of the feature, it should.

Thanks, guys!

;;;","16/Sep/13 17:41;enigmacurry;I'm using 5 second intervals in these charts. 'multi-second stress-client outage' is a good way to put it, for both the case of speculative retry and not, the drop in performance after a node goes down is a duration of complete non-responsiveness (not degraded performance.) The addition of speculative retry consistently shortens this duration (it's always better), but this duration itself is inconsistent. ;;;","17/Sep/13 23:39;kohlisankalp;SpeculateAlwaysExecutor - Here we are not reading from more endpoints than normal. We are only reading data from two endpoints. We should be reading from one more endpoint if possible. ;;;","18/Sep/13 19:28;hubez;Thanks, [~enigmacurry]. Sounds like you are seeing the same thing as us, so it's great to see it's getting attention!;;;","24/Sep/13 21:53;lizou;Hello [~iamaleksey],

Thanks for the link to this jira and for your very detailed testing results. It confirms what we have seen in our lab testing for the Cassandra 2.0.0-rc2 ""Speculative Execution for Reads"".

We have a very simple data center setup consisting of four Cassandra nodes running on four server machines. A testing application (Cassandra client) is interacting with Cassandra nodes 1, 2 and 3. That is, the testing app does not directly connected to the Cassandra node 4.

The keyspace Replication Factor is set to 3 and the client requested Consistency Level is set to CL_TWO.

I have tested all of three configurations of the Speculative Execution for Reads ('ALWAYS', '85 PERCENTILE', '50 MS' / '100 MS'). It seems that none of them works as expected. From the test app log file point of view, they all give a 20-second window of outage immediately after the 4th node was killed. This behavior is consistent to Cassandra 1.2.4.

I have done a quick code reading of the Cassandra Server implementation (Cassandra 2.0.0 tarball) and I have noticed some design issues. I would like to discuss them with you.

*Issue 1* - StorageProxy.fetchRows() may still block for as long as conf.read_request_timeout_in_ms, though the speculative retry did fire correctly after the Cassandra node 4 was killed.

Take the speculative configuration of 'PERCENTILE' / 'CUSTOM' as example, after the Cassandra node 4 was killed, SpeculativeReadExecutor.speculate() would block for responses. If timed out, it would send out one more read request to an alternative node (from {{unfiltered}}) and increment the speculativeRetry counter. This part should work.

However, killing the 4th node would very likely cause inconsistency in the database and this will trigger the DigestMismatchException. In the fetchRows(), when handling DigestMismatchException, it uses handler.endpoints to send out digest mismatch retries and then block for responses. As we know that one of the endpoints was already killed, the handler.get() will block until it is timed out, which is 10 seconds.


{noformat}
                catch (DigestMismatchException ex)
                {
                    Tracing.trace(""Digest mismatch: {}"", ex);

                    ...

                    MessageOut<ReadCommand> message = exec.command.createMessage();
                    for (InetAddress endpoint : exec.handler.endpoints)
                    {
                        Tracing.trace(""Enqueuing full data read to {}"", endpoint);
                        MessagingService.instance().sendRR(message, endpoint, repairHandler);
                    }
                }
            }

            ...

            // read the results for the digest mismatch retries
            if (repairResponseHandlers != null)
            {
                for (int i = 0; i < repairCommands.size(); i++)
                {
                    ReadCommand command = repairCommands.get(i);
                    ReadCallback<ReadResponse, Row> handler = repairResponseHandlers.get(i);

                    Row row;
                    try
                    {
                        row = handler.get();
                    }
{noformat}



*Issue 2* - The speculative 'ALWAYS' does NOT send out any more read requests. Thus, in face of the failure of node 4, it will not help at all.

The SpeculateAlwaysExecutor.executeAsync() only sends out handler.endpoints.size() number of read requests and it blocks for the responses to come back. If one of the nodes is killed, say node 4, this speculative retry 'ALWAYS' will work the same way as Cassandra 1.2.4, i.e. it will block until timed out, which is 10 seconds.

??My understanding of this speculative retry 'ALWAYS' should ALWAYS send out ""handler.endpoints.size() + 1"" number of read requests and block for handler.endpoints.size() number of responses??.

*Issue 3* - Since the ReadRepairDecison is determined by a Random() number, this speculative retry may not work as the ReadRepairDecision may be ??ReadRepairDecision.GLOBAL??

*Issue 4* - For the ReadExecutor(s), the {{this.unfiltered}} and {{this.endpoints}} may not consistent. Thus, using {{this.unfiltered}} and {{this.endpoints}} for speculative retry may cause unexpected results. This is especially true when the Consistency Level is {{LOCAL_QUARUM}} and the ReadRepairDecision is {{DC_LOCAL}}.



;;;","24/Sep/13 21:58;aleksey;Hey [~lizou]. Yeah, I've fixed most of these already (rewritten most of the ARE code, actually). Specifically issues 2,3,4. Will look into 1 too.

Thanks.;;;","25/Sep/13 01:51;aleksey;Attaching 5932.txt that will hopefully fix this ([~enigmacurry] could you run the tests again, please, with the patch applied?)

1. As noted by [~lizou] and [~kohlisankalp], ALWAYS wasn't making an extra request, it was making an extra data request at the expense of one digest request. Fixed.

2. SpecRetry wasn't working correctly with RRD.DC_LOCAL, as noted by @lizou, because the two lists will be in different order, and a retry might be sent to a node that already had a request sent to it. (Please note that LOCAL_QUORUM here does not affect anything - CL.filterForQuery() sorts in place, so the two lists would be in the same order, everything was working correct). RRD.DC_LOCAL handling was a legit issue though. Fixed.

3. SpecRetry w/ RRD.GLOBAL is a noop, you can't speculate if you contact all the replicas in the first place. This is normal.

4. The DME issue is semi-legit. Killing a node shouldn't trigger DME or increase the likelihood of DME happening. *HOWEVER* when shooting requests for repair, we were not considering the case where one of the replies satisfying the original CL came from a SpecRetry attempt. The patch includes the extra replica in repair commands if SpecRetry had been triggered by the original request.


5. SP.getRangeSlice() is not SpectRetry-aware as of now. I don't know if this is an omission or by design, but for now, please don't include that in the benchmarks, since it would only be misleading. ;;;","25/Sep/13 16:07;jbellis;Pushed some OCD of my own to https://github.com/jbellis/cassandra/commits/5932 on top of this.

bq. ALWAYS wasn't making an extra request, it was making an extra data request at the expense of one digest request

I'm not sure what the distinction is here.  Do you mean that if we weren't read-repairing, there would be no extra data request at all?

bq. SpecRetry w/ RRD.GLOBAL is a noop, you can't speculate if you contact all the replicas in the first place.

I dunno, I think we should turn a digest into a data for redundancy the way ALWAYS used to.;;;","25/Sep/13 17:02;aleksey;Pushed even more OCD to https://github.com/iamaleksey/cassandra/commits/5932 on top of yours.

bq. I'm not sure what the distinction is here. Do you mean that if we weren't read-repairing, there would be no extra data request at all?

Instead of making, for example 1 data request + 2 digest requests, ALWAYS was making 2 data requests + 1 digest request, instead of making 2 data requests + 2 digest requests, not really helping to satisfy the CL in case of node's failure.

bq. I dunno, I think we should turn a digest into a data for redundancy the way ALWAYS used to.

Maybe.;;;","25/Sep/13 23:03;aleksey;Force-pushed the 'final' version to https://github.com/iamaleksey/cassandra/commits/5932.

Among other things, properly handles RRD.GLOBAL and RRD.DC_LOCAL in 1-DC scenario.;;;","26/Sep/13 17:54;jbellis;Pushed one more set of changes to mine, not forced: https://github.com/jbellis/cassandra/commits/5932.  Goal is to make SRE less fragile when doing RR.;;;","26/Sep/13 20:30;aleksey;+1, I'm out of OCD juice.;;;","26/Sep/13 20:43;lizou;Hello [~iamaleksey] and [~jbellis],

I took a quick look at the code changes. The new code looks very good to me. But I saw one potential issue in {{AlwaysSpeculatingReadExecutor.executeAsync()}}, in which it makes at least *two* data / digest requests. This will cause problems for a data center with only one Cassandra server node (e.g. bring up an embedded Cassandra node in JVM for JUnit test) or a deployed production data center of two Cassandra server nodes with one node shut down for maintenance. In the above mentioned two cases, {{AbstractReadExecutor.getReadExecutor()}} will return the {{AlwaysSpeculatingReadExecutor}} as condition {{(targetReplicas.size() == allReplicas.size())}} is met, though the tables may / may not be configured with ??Speculative ALWAYS??.

It is true for our legacy products we are considering to deploy each data center with only two Cassandra server nodes with RF = 2 and CL = 1.
;;;","26/Sep/13 20:55;jbellis;The logic looks like this:

# Figure out how many replicas we need to contact to satisfy the desired consistencyLevel + Read Repair settings
# If that ends up being all the replicas, then use ASRE to get some redundancy on the data reads.  This will allow the read to succeed even if a digest for RR times out.  Of course if you are reading at CL.ALL and a replica times out there's nothing we can do.
# Otherwise, use SRE and make an ""extra"" request later, if it looks like one of the minimal set isn't going to respond in time

Note that performing extra data requests does not affect handler.blockfor -- just makes it possible for the request to proceed if it gets enough responses back, no matter which replicas they come from.;;;","26/Sep/13 20:55;jbellis;(Committed after Aleksey's +1, incidentally.);;;","26/Sep/13 21:04;lizou;The logic for {{AlwaysSpeculatingReadExecutor}} is good. What I meant in my previous comment is that when {{targetReplicas.size() == allReplicas.size()}} and {{targetReplicas.size() == 1}}, then {{AlwaysSpeculatingReadExecutor.executeAsync()}} will throw an exception as there is only one endpoint in {{targetReplicas}}, but it tries to access two endpoints in {{targetReplicas}}.;;;","26/Sep/13 21:10;jbellis;I see what you mean.  Fixed in 7a87fc1186f39678382cf9b3e1dd224d9c71aead.;;;","27/Sep/13 14:54;enigmacurry;The good news is that speculative read has improved across the board.

However, this new batch of testing introduces some new mysteries.

Here is all of the runs from 7a87fc1186f39678382cf9b3e1dd224d9c71aead:

!5933-7a87fc11.png!

All of the speculative retry runs are better than with 2.0.0-rc1. However, I can't explain why sr=NONE did better than ALWAYS and 95percentile. There is no visible indication that a node went down for sr=NONE. I have double checked the logs, and it did, in fact, go down. 

Compare this to the baseline of 1.2.8 and 2.0.0-rc1 (redone last night on same hardware as above):

!5933-128_and_200rc1.png!

All of these have clear indications of the node going down.

You can [see all the data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1] - you can double click the colored squares to toggle the visibility of the lines, as they do overlap.

I've uploaded logs from all these runs as 5933-logs.tar.gz.;;;","27/Sep/13 15:00;jjordan;Yeah. The graphs for ALWAYS and NONE look swapped from what I would expect.;;;","27/Sep/13 15:02;enigmacurry;The other thing that I note, is that all of these runs are better than 1.2.8 :D (further evidence that CASSANDRA-5933 may be invalid);;;","27/Sep/13 15:07;jbellis;Hmm.

I wonder if it's just luck of the draw as to which replica dsnitch is preferring.  Here's a branch to randomize that, per-operation:

https://github.com/jbellis/cassandra/commits/5932-randomized;;;","27/Sep/13 16:28;brandon.williams;Couldn't we just do a run with the dsnitch disabled?;;;","27/Sep/13 16:31;jbellis;That still gives you luck-of-the-draw as to which replica it prefers.  (Unlikely to be evenly distributed.);;;","27/Sep/13 17:14;enigmacurry;This looks exactly like what I was expecting:

!5933-randomized-dsnitch-replica.png!

[data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.randomized-dsnitch.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1];;;","27/Sep/13 17:29;jbellis;That's awesome.

Can you test 90th and 75th percentile too?;;;","27/Sep/13 18:44;enigmacurry;Seems like it's quite tunable, but not a lot of difference under 90%:

!5933-randomized-dsnitch-replica.2.png!

[data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.randomized-dsnitch.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1];;;","27/Sep/13 19:43;jbellis;Yeah, that makes sense.  70th..95th are all pretty damn close to median still.

What I'd like to do is get close to the 10ms performance hit (~none) as a percentile, and make that default in 2.1.  Try 99th and 99.9th?;;;","27/Sep/13 19:54;jbellis;Also, did that stress patch work to get you failed request counts?  Would be good to get that too if we can show that even 10ms keeps requests from failing entirely.;;;","27/Sep/13 21:14;enigmacurry;Here's 99th and 99.9th percentiles:

!5933-randomized-dsnitch-replica.3.png!

I'll look at that stress patch again, I seem to recall it not making a lot of sense to me when I last tried it, but will give it another go.;;;","27/Sep/13 21:37;jbellis;Starting to think we still have a bug.  99.9 should be doing less retries than 10ms but the graph shows it doing more.;;;","27/Sep/13 21:47;lizou;I did some more code reading and noticed some potential issues and possible improvement. I've got to run now. I will get back to you guys Monday morning.

My guess is that the ??Speculative NONE?? is hit by the initial request reading path which is successfully resolved by the *Speculative Retry*. The observed throughput performance hit when Speculative Retry is enabled is caused by the ReadRepair path which has some coding / design issues. I will talk to you next Monday.;;;","27/Sep/13 21:49;jbellis;Maybe the problem is that we're using CF-level latency instead of StorageProxy.

What does cfhistograms give for read latency?;;;","27/Sep/13 22:40;jbellis;Pretty sure that's our smoking gun.  Pushed a commit to the -randomized branch that adds coordinator-level, per-cf latency tracking and uses that instead.

Can you repeat the last test with that?  (Maybe throw in ALL as well if you're feeling optimistic that we'll have a measurable difference between ALL and 90%. :);;;","29/Sep/13 16:15;enigmacurry;[~jbellis] Here's your two runs:

[dea27f84f40|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.dea27f84f40.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1]
[ded39c7e1c2fa|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.ded39c7e1c2fa.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1]

Logs for the second run are attached as 5932.ded39c7e1c2fa.logs.tar.gz;;;","29/Sep/13 17:48;jbellis;The code to convert 99 into 0.99 was buggy and was actually converting to 0.0099.  Fix pushed, can you try it again?;;;","29/Sep/13 21:26;enigmacurry;BINGO!

!5932-6692c50412ef7d.png!

[data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.6692c50412ef7d.node_killed.json&metric=interval_op_rate&operation=stress-read&smoothing=1];;;","30/Sep/13 01:25;hubez;Definitely the best results seen so far! Nice work!

In my mind, during the transition period right after the killing of the node, I expected ""ALWAYS"" to have negligible impact, or at least the smallest impact of all of the other values. However, red (90%), and purple (75%) is having a smaller impact. Seems fishy. Do I misunderstand the intention of the ""ALWAYS"" setting?

(Edited to clarify the period I'm talking about.);;;","30/Sep/13 03:10;jbellis;It looks to me like 75/90/Always are about the same, with Always dropping from a lower baseline.  Which makes sense; it's still doing a lot of unnecessary work compared to the others.;;;","30/Sep/13 13:49;jbellis;[~enigmacurry], can you also re-test the uncapped compaction scenario with the same set of retry settings?;;;","30/Sep/13 17:24;lizou;Hello [~iamaleksey] and [~jbellis],

It appears to me that the testing results have suggested that the ""_data read + speculative retry_"" path work as expected. This ""_data read + speculative retry_"" path has greatly minimized the throughput impact caused by the failure of one of Cassandra server nodes.

The observed small degradation of throughput performance when _speculative retry_ is enabled is very likely to be caused by the ""*_read repair_*"" path. I did the code reading of this path last Friday and noticed some design / coding issues. I would like to discuss them with you.

Please note that my code base is still the Cassandra 2.0.0 tarball, not updated with the latest code changes.

*Issue 1* -- When handling {{DigestMismatchException}} in {{StorageProxy.fetchRows()}}, all _data read requests_ are sent out using {{sendRR}} without distinguishing remote nodes from the local node.

Will this cause an issue, as {{MessagingService.instance().sendRR()}} will send out enqueued messages for a specified remote node via its pre-established TCP socket connection. For local node, this should be done via {{LocalReadRunnable}}, i.e. {{StageManager.getStage(Stage.READ).execute(new LocalReadRunnable(command, handler))}}.

If this may cause an issue, the following wait may block.

{noformat}
            // read the results for the digest mismatch retries
            if (repairResponseHandlers != null)
            {
                for (int i = 0; i < repairCommands.size(); i++)
                {
                    ReadCommand command = repairCommands.get(i);
                    ReadCallback<ReadResponse, Row> handler = repairResponseHandlers.get(i);

                    Row row;
                    try
                    {
                        row = handler.get();
                    }
{noformat}

For two reasons.
* The data read request for local node may never sent out
* As one of the nodes is down (which triggered the Speculative Retry) will cause one missing response.

*If missing two responses, this will block for 10 seconds*. 

*Issue 2* -- For _data repair_, {{RowDataResolver.resolve()}} has a similar issue as it calls  {{scheduleRepairs()}} to send out  messages using sendRR() without distinguishing remote nodes from the local node.

*Issue 3* -- When handling _data repair_, {{StorageProxy.fetchRows()}} blocks waiting for acks to all of {{data repair}} requests sent out using sendRR(). This may cause the thread to block.

For _data repair_ path, *data requests* are sent out and then compare / merge the received responses; send out the merged / diff version and then block for acks.

How do we handle the case for _local node_? Does the sendRR() and the corresponding receive part can handle the case for local node? If not, then this may block for 10 seconds.

{noformat}
            if (repairResponseHandlers != null)
            {
                for (int i = 0; i < repairCommands.size(); i++)
                {
                    ReadCommand command = repairCommands.get(i);
                    ReadCallback<ReadResponse, Row> handler = repairResponseHandlers.get(i);

                    Row row;
                    try
                    {
                        row = handler.get();
                    }
                    catch (DigestMismatchException e)
                    ...
                    RowDataResolver resolver = (RowDataResolver)handler.resolver;
                    try
                    {
                        // wait for the repair writes to be acknowledged, to minimize impact on any replica that's
                        // behind on writes in case the out-of-sync row is read multiple times in quick succession
                        FBUtilities.waitOnFutures(resolver.repairResults, DatabaseDescriptor.getWriteRpcTimeout());
                    }
                    catch (TimeoutException e)
                    {
                        Tracing.trace(""Timed out on digest mismatch retries"");
                        int blockFor = consistency_level.blockFor(Keyspace.open(command.getKeyspace()));
                        throw new ReadTimeoutException(consistency_level, blockFor, blockFor, true);
                    }
{noformat}

*Question for waiting for the ack* -- Do we really need to wait for the ack?

We should assume the best effort approach, i.e. do the data repair and then return. No need to block waiting for the acks for confirmation.

*Question for the Randomized approach* -- Since the end points are randomized, the first node in the list is no likely the local node. This may cause a higher possibility of data repair.

In the *Randomized Approach*, the end points are reshuffled. Then, the first node in the list used for _data read request_ is not likely the local node. If this node happens to be the *DOWN* node, then, we end with all digest responses without the data, which will block and eventually timed out.

;;;","30/Sep/13 20:46;aleksey;First, let me thank you for your continued digging. Some of it helped. That said, you should probably look at the current cassandra-2.0 branch, and not the 2.0.0 tarball/branches here in the comments.

bq. Issue 1 – When handling DigestMismatchException in StorageProxy.fetchRows(), all data read requests are sent out using sendRR without distinguishing remote nodes from the local node.

This is not an issue, and it's not spec retry related. Using LRR for local read requests is merely an optimisation - there is nothing wrong with sendRR (not that it isn't worth optimising here - just noting that it's not an issue). This is also the answer to ""How do we handle the case for local node? Does the sendRR() and the corresponding receive part can handle the case for local node? If not, then this may block for 10 seconds."" Same goes for Issue 2 and Issue 3.

bq. The data read request for local node may never sent out. As one of the nodes is down (which triggered the Speculative Retry) will cause one missing response.

The former is not true, the latter won't, since the current cassandra-2.0 code will send requests to all the contacted replicas. So if a node triggered spec retry, that extra speculated replica will get the request as well, and we can still satisfy the CL.

{noformat}
                    for (InetAddress endpoint : exec.getContactedReplicas())
                    {
                        Tracing.trace(""Enqueuing full data read to {}"", endpoint);
                        MessagingService.instance().sendRR(message, endpoint, repairHandler);
                    }
{noformat}


bq. Question for the Randomized approach – Since the end points are randomized, the first node in the list is no likely the local node. This may cause a higher possibility of data repair.

I don't see how the possibility of data repair is correlated with the locality of a target node, but, it doesn't matter. The 'randomised approach' was an experiment, it wasn't committed as part of the fix. See the latest cassandra-2.0 branch code.

bq. In the Randomized Approach, the end points are reshuffled. Then, the first node in the list used for data read request is not likely the local node. If this node happens to be the DOWN node, then, we end with all digest responses without the data, which will block and eventually timed out.

See the above reply.

TLDR: None of these seem to be issues, but we could optimise RR to use LRR for local reads to get slightly better performance for local requests (and to be consistent with the regular reads code).;;;","30/Sep/13 21:32;lizou;Thanks for the clarification of the sendRR issue.

Since the Randomized approach is not checked in, let us skip over it.

For the _data repair_, do we need to block waiting for the acks?;;;","30/Sep/13 21:53;aleksey;bq. For the data repair, do we need to block waiting for the acks?

The reasons are listed in the comments, as you've seen:
{noformat}
// wait for the repair writes to be acknowledged, to minimize impact on any replica that's
// behind on writes in case the out-of-sync row is read multiple times in quick succession
{noformat}

To reach that goal - yes, it's necessary. Is that scenario worth optimizing for or should we reconsider? Dunno. We are only writing to the replicas that we got the result from, though, so a known down replica wouldn't affect it.

;;;","30/Sep/13 22:00;aleksey;[~lizou] see CASSANDRA-4792 (TLDR: yes);;;","01/Oct/13 15:15;enigmacurry;I had to double the test length to get a good compaction graph. I'm not sure why it took so long, it didn't take as long in the original test.

!5932.6692c50412ef7d.compaction.png!

[data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.6692c50412ef7d.compaction.2.json&metric=interval_op_rate&operation=stress-read&smoothing=4]

([~iamaleksey] your read_repair 0 / 1 tests are in progress...);;;","01/Oct/13 20:15;enigmacurry;Node killed while read_repair_chance=0. I accidentally left the test run to be 60M rows, so I chopped off the uninteresting bit.

!5932.6692c50412ef7d.rr0.png!

[data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.6692c50412ef7d.node_killed.rr0.json&metric=interval_op_rate&operation=stress-read&smoothing=1]

(rr=1 is next..);;;","02/Oct/13 03:39;jbellis;The throughput is a wash in the compaction scenario, but the 99.9% latency looks a lot better with the retries.

Any theories on why the percentile settings are posting better latency numbers than ALWAYS though?;;;","02/Oct/13 15:46;lizou;This testing result is reasonable and what is expected.

For PERCENTILE / CUSTOM configuration, the larger the {{cfs.sampleLatencyNanos}} the smaller the throughput impact for normal operations before the outage. However, during the outage period, the situation is reversed, i.e. the smaller {{cfs.sampleLatencyNanos}}, the smaller the throughput impact will be, as it times out quicker and triggers the speculative retries.

For the ALWAYS configuration, as it always sends out one speculative in addition to the usual read requests, the throughput performance should be lower than those of PERCENTILE / CUSTOM for normal operations before the outage. Since it always sends out the speculative retries, the throughput impact during the outage period should be the smallest. The testing result indicates that this is true.
;;;","02/Oct/13 15:50;enigmacurry;With read_repair_chance = 1

!5932.6692c50412ef7d.rr1.png!

[data here|http://ryanmcguire.info/ds/graph/graph.html?stats=stats.5933.6692c50412ef7d.node_killed.rr1.json&metric=interval_op_rate&operation=stress-read&smoothing=1];;;","04/Oct/13 20:34;lizou;Have done some testing using today's trunk. Have observed following issues.

*Issue 1* -- The first method {{MessagingService.addCallback()}} (i.e. without the ConsistencyLevel argument) asserts.

Commenting out the assert statement seems to work. But the Cassandra servers themselves will produce 10-second outage (i.e. zero transactions from the client point of view) periodically.

*Issue 2* -- The Speculative Retry seems stop retrying during the outage window.

During the outage window triggered either by killing one of Cassandra nodes or produced by Cassandra servers themselves, the JConsole shows that the JMX stats, SpeculativeRetry counter stops incrementing until the gossip figures out the outage issue.

What is the reason for this? The Speculative Retry is meant to help during the outage period. This observed behavior is consistent with Cassandra 2.0.0-rc2.
;;;","04/Oct/13 20:47;jbellis;bq. MessagingService.addCallback

Are you sure you have the latest code?  The only invocations of addCallback in 2.0/trunk include the consistencylevel argument as of late last night.;;;","04/Oct/13 20:58;lizou;The trunk load I used for testing was pulled this noon. It has two addCallback() methods. One of them (i.e. without the ConsistencyLevel) asserts. 

I checked the MessagingService.java, there are two addCallback() methods.
* The one without ConsistencyLevel is called by sendRR()
* The one with ConsistencyLevel is called by sendMessageToNonLocalDC()
;;;","04/Oct/13 21:06;lizou;As for yesterday's trunk load, there were two addCallback() methods. But the one with ConsistencyLevel was not called by anyone. The one without ConsistencyLevel asserts.;;;","04/Oct/13 21:27;jbellis;Are you doing counter updates?  That's the only use of sendRR for updates I see.

Can you post the stack trace of the assertion error you're getting?;;;","04/Oct/13 21:37;jbellis;(Pushed fix for mutateCounter in 3da10f469d6a328bad209d723a5997c932284344.);;;","07/Oct/13 18:56;lizou;[~jbellis], this morning's trunk load has a slightly different symptom, and is even more serious than last Friday's load, as this time just commenting out the assert statement in the {{MessagingService.addCallback()}} will not help.

I copy the {{/var/log/cassandra/system.log}} exception errors below.

{noformat}
ERROR [Thrift:12] 2013-10-07 14:42:39,396 Caller+0       at org.apache.cassandra.service.CassandraDaemon$2.uncaughtException(CassandraDaemon.java:134)
 - Exception in thread Thread[Thrift:12,5,main]
java.lang.AssertionError: null
        at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:543) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:591) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:571) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.service.StorageProxy.sendToHintedEndpoints(StorageProxy.java:869) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.service.StorageProxy$2.apply(StorageProxy.java:123) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:739) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:511) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:581) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:379) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:363) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:126) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:267) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.thrift.CassandraServer.execute_prepared_cql3_query(CassandraServer.java:2061) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query.getResult(Cassandra.java:4502) ~[apache-cassandra-thrift-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query.getResult(Cassandra.java:4486) ~[apache-cassandra-thrift-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39) ~[libthrift-0.9.1.jar:0.9.1]
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39) ~[libthrift-0.9.1.jar:0.9.1]
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:194) ~[apache-cassandra-2.1-SNAPSHOT.jar:2.1-SNAPSHOT]
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145) ~[na:1.7.0_25]
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615) ~[na:1.7.0_25]
        at java.lang.Thread.run(Thread.java:724) ~[na:1.7.0_25]

{noformat}

;;;","07/Oct/13 19:35;jbellis;There's a ticket open for trunk over at CASSANDRA-6154.;;;","07/Oct/13 21:40;lizou;Hello,

As this ticket is already fixed in 2.0.2, where can I get the 2.0.2 source code?

Currently, my ""git tag"" only shows up to 2.0.1.
;;;","07/Oct/13 21:48;jbellis;the cassandra-2.0 branch is what will become 2.0.2;;;","07/Oct/13 22:34;lizou;I even cannot see the cassandra-2.0 branch.
My ""git tag"" gives a list including following branches.

{noformat}
$ git tag
1.2.8
1.2.8-tentative
cassandra-0.3.0-final
cassandra-0.3.0-rc1
cassandra-0.3.0-rc2
...
cassandra-1.2.4
cassandra-1.2.5
cassandra-1.2.6
cassandra-1.2.7
cassandra-1.2.8
cassandra-1.2.9
cassandra-2.0.0
cassandra-2.0.0-beta1
cassandra-2.0.0-beta2
cassandra-2.0.0-rc1
cassandra-2.0.0-rc2
cassandra-2.0.1
drivers
list
{noformat}

There is no cassandra-2.0 branch. Where can I find it?
;;;","07/Oct/13 23:01;jbellis;Under {{git branch}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix periodic flushing when encountering an empty memtable,CASSANDRA-5931,12665335,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,23/Aug/13 21:59,16/Apr/19 09:32,14/Jul/23 05:53,23/Aug/13 23:20,2.0.0,,,,,,0,,,,,"CASSANDRA-5241 broke it by making forceFlush() always return a valid future, never a null, and CASSANDRA-4237 was relying on that null check to determine cleanliness.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/13 22:00;aleksey;5931.txt;https://issues.apache.org/jira/secure/attachment/12599721/5931.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345275,,,Fri Aug 23 23:20:28 UTC 2013,,,,,,,,,,"0|i1njhz:",345576,2.0 rc2,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"23/Aug/13 22:24;jbellis;+1

Nit: it's probably time to add javadoc with @returns to forceFlush to help prevent this confusion;;;","23/Aug/13 23:20;aleksey;Committed with the nit addressed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offline scrubs can choke on broken files,CASSANDRA-5930,12665333,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,jjordan,jjordan,23/Aug/13 21:54,16/Apr/19 09:32,14/Jul/23 05:53,03/Feb/14 20:33,2.0.5,,,,,,1,,,,,"There are cases where offline scrub can hit an exception and die, like:

{noformat}
WARNING: Non-fatal error reading row (stacktrace follows)
Exception in thread ""main"" java.io.IOError: java.io.IOError: java.io.EOFException
	at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:242)
	at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:121)
Caused by: java.io.IOError: java.io.EOFException
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:116)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:99)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:176)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:182)
	at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:171)
	... 1 more
Caused by: java.io.EOFException
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:399)
	at java.io.RandomAccessFile.readFully(RandomAccessFile.java:377)
	at org.apache.cassandra.utils.BytesReadTracker.readFully(BytesReadTracker.java:95)
	at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:401)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:363)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:120)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:37)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:144)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:234)
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:112)
	... 5 more
{noformat}

Since the purpose of offline scrub is to fix broken stuff, it should be more resilient to broken stuff...",,aleksey,jeffpotter,jjordan,jpotter,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/14 22:39;thobbs;5930-v1.patch;https://issues.apache.org/jira/secure/attachment/12621396/5930-v1.patch","07/Jan/14 21:40;thobbs;5930-v2.patch;https://issues.apache.org/jira/secure/attachment/12621862/5930-v2.patch",,,,,,,,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345273,,,Mon Feb 03 20:33:07 UTC 2014,,,,,,,,,,"0|i1njhj:",345574,1.1.7,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"23/Aug/13 21:57;jbellis;Can you have a look, Jason?;;;","02/Sep/13 16:49;jeffpotter;We're seeing this too -- slightly different stack trace, which I'll include here in case it's of use.


WARNING: Non-fatal error reading row (stacktrace follows)
Exception in thread ""main"" java.io.IOError: java.lang.IllegalArgumentException
at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:244)
at org.apache.cassandra.tools.StandaloneScrubber.main(StandaloneScrubber.java:125)
Caused by: java.lang.IllegalArgumentException 
at java.nio.Buffer.limit(Buffer.java:247)
at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:51)
at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:60)
at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:78) 
at org.apache.cassandra.db.marshal.AbstractCompositeType.compare(AbstractCompositeType.java:31)
at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:128)
at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:114)
at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:109) 
at org.apache.cassandra.db.ColumnFamily.addAtom(ColumnFamily.java:219)
at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumnsFromSSTable(ColumnFamilySerializer.java:149)
at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:234)
at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:114) 
at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:98)
at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:160)
at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:166)
at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:173) 
... 1 more
;;;","02/Jan/14 18:50;thobbs;[~jeffpotter] what version of Cassandra were you running when you hit the above error?

As far as the original stacktrace for this ticket goes, it's unfortunately necessary for counter CFs.  CASSANDRA-2759 explains the reasoning.  I suppose I could make the error message mention that and point to the ticket.

The scrub code looks reasonably robust in general, so I think it's better to wait for individual bugs to get reported than to try to improve the code without any failure examples.;;;","02/Jan/14 23:02;jpotter;Hi Tyler -- based on my notes, it should have been Cassandra 1.2.6.1 (DSE 3.1), at least, that's what other tickets we have filed at this same time suggest.;;;","03/Jan/14 22:39;thobbs;Thanks, [~jeffpotter].  It looks like you also had a Counter table, in this case.

5930-v1.patch (and [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-5930-2.0]) clarifies the error message for counter tables.;;;","03/Jan/14 23:52;jbellis;It would be nice to be able to tell people how to fix it (realistically: what their options are) rather than just ""sorry, scrub can't help you.""  But I'm not sure what those options are. :)  /cc [~slebresne] [~iamaleksey];;;","04/Jan/14 00:00;aleksey;[~jbellis] There are no options. That said, we should probably allow users to override this behavior, if they prefer losing some of the counters history to not scrubbing at all.

Also, with CASSANDRA-6504 in this becomes a non-issue (for the newly written 'global' 2.1 shards, at least - we *can* repair those after the scrub).;;;","07/Jan/14 21:40;thobbs;You're right, it would be nice to give the user an option to skip the corrupted rows anyway.

5930-v2.patch (the [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-5930-2.0] is still good) adds a {{--skip-corrupted}} option and a unit test to exercise it.;;;","03/Feb/14 20:33;aleksey;LGTM, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dateOf() in 2.0 won't work with timestamp columns created in 1.2-,CASSANDRA-5928,12665231,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,23/Aug/13 14:55,16/Apr/19 09:32,14/Jul/23 05:53,26/Aug/13 15:54,2.0.0,,,,,,0,cql3,,,,"dateof() return type is TimestampType now, so it won't work with previously created DateType columns
(Type error: cannot assign result of function dateof (type timestamp) to value (type 'org.apache.cassandra.db.marshal.DateType')).",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/13 15:22;aleksey;5928.txt;https://issues.apache.org/jira/secure/attachment/12599635/5928.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345172,,,Mon Aug 26 15:54:41 UTC 2013,,,,,,,,,,"0|i1niv3:",345473,2.0 rc1,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"23/Aug/13 15:24;aleksey;Actually, I think the fix is as simple as making DateType.asCQL3Type() return TIMESTAMP (since the assignability check is basically comparing the cql3 types).;;;","26/Aug/13 15:33;slebresne;You're right, that fixes dateOf (and to be clear, I don't see a clearly much better fix). The one small concern I have is that because we use asCQL3Type() when printing error message, we will show ""timestamp"" when DateType is used even though ""timestamp"" is really TimestampType in practice. That being say, I ""think"" this has no real practical consequence so +1. ;;;","26/Aug/13 15:54;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress skipKeys option does not appear to do anything,CASSANDRA-5927,12665223,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,benedict,cburroughs,cburroughs,23/Aug/13 13:19,16/Apr/19 09:32,14/Jul/23 05:53,13/Mar/14 22:06,2.1 rc2,,,Legacy/Tools,,,0,,,,,"I don't see it used anywhere in the code.  FWIW I *thought* when it worked the option was ""start at key X"" instead of ""Fraction of keys to skip initially"".

{noformat}
$ grep -rni skipkeys ./tools/stress/src/org/
./tools/stress/src/org/apache/cassandra/stress/Session.java:122:    private float skipKeys       = 0;
./tools/stress/src/org/apache/cassandra/stress/Session.java:199:                skipKeys = Float.parseFloat(cmd.getOptionValue(""N""));
./tools/stress/src/org/apache/cassandra/stress/Session.java:513:    public float getSkipKeys()
./tools/stress/src/org/apache/cassandra/stress/Session.java:515:        return skipKeys;
{noformat}",1.2.8,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,benedict,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345164,,,Thu Mar 13 22:05:55 UTC 2014,,,,,,,,,,"0|i1nitb:",345465,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"13/Mar/14 22:01;jbellis;Is this still a thing in stress-ng?;;;","13/Mar/14 22:05;benedict;No. I'm not sure what it's even referring to (I don't support it in Legacy either, but maybe that's because it wasn't wired up when I was migrating, which is exactly what this ticket refers to I guess). 

In stress-ng you can specify an explicit range and distribution for your keys.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The native protocol server can deadlock,CASSANDRA-5926,12665214,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,23/Aug/13 11:57,16/Apr/19 09:32,14/Jul/23 05:53,30/Aug/13 11:32,1.2.9,,,,,,0,,,,,"Until CASSANDRA-5239 (i.e. since StorageProxy is blocking), the native protocol server needs to use a thread per request being processed. For that, it currently use a DebuggableThreadPoolExecutor, but with a limited queue. The rational being that we don't want to OOM if a client overwhelm the server. Rather, we prefer blocking (which DTPE gives us) on the submission of new request by the netty worker threads when all threads are busy.

However, as it happens, when netty sends back a response to a query, there is cases where some events (technically, InterestChanged and WriteComplete events) are send up the pipeline. And those event are submitted on the request executor as other requests. Long story short, a request thread can end blocking on the submission to its own executor, hence deadlocking.

The simplest solution is probably to reuse MemoryAwareThreadPoolExecutor from netty rather that our own DTPE as it also allow to block task submission when all threads are busy but knows not to block it's own internal events.
",,mkjellman,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Aug/13 11:58;slebresne;5926.txt;https://issues.apache.org/jira/secure/attachment/12599611/5926.txt","28/Aug/13 18:45;mkjellman;stack;https://issues.apache.org/jira/secure/attachment/12600437/stack",,,,,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345155,,,Fri Aug 30 11:32:34 UTC 2013,,,,,,,,,,"0|i1nirb:",345456,1.2.9,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"23/Aug/13 11:58;slebresne;Attaching patch that switches to MemoryAwareThreadPoolExecutor. One interesting detail is that MATPE always forces corePoolSize == maxPoolSize, so doing that pretty much deprecates the native_transport_min_threads in the yaml. I'd say this is probably fine in practice though.
;;;","23/Aug/13 14:09;jbellis;""serie"" typo, otherwise +1;;;","26/Aug/13 15:43;slebresne;Committed, thanks;;;","28/Aug/13 18:45;mkjellman;Ironically, I hit this deadlock after the proposed change. Thread dump attached.

IO Worker #11 is most interesting.;;;","29/Aug/13 01:12;mkjellman;https://github.com/netty/netty/issues/1310;;;","29/Aug/13 17:55;slebresne;Yep, pretty sure that's what you've run into. We'll have to update your netty dependency.;;;","30/Aug/13 11:32;slebresne;Let me re-close this since the committed did fix the original deadlock. It's obviously now unfortunate that we're running into a netty bug, but since 1.2.9 has shipped, I've opened a separate issue (CASSANDRA-5955) to upgrade our dependency.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in update lightweight transaction,CASSANDRA-5925,12665142,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,ppersad,ppersad,22/Aug/13 23:17,25/Oct/19 13:11,14/Jul/23 05:53,28/Aug/13 10:33,2.0.0,,,Feature/Lightweight Transactions,,,0,LWT,,,,"I'm building some tests for a Cassandra PoC.  One scenario I need to test is consumption of 1 time tokens.  These tokens must be consumed exactly once.  The cluster involved is a 3 node cluster.  All queries are run with ConsistencyLevel.QUORUM. I'm using the following queries:

CREATE KEYSPACE IF NOT EXISTS test WITH replication = { 'class' : 'SimpleStrategy', 'replication_factor' : 3 };

CREATE TABLE IF NOT EXISTS tkns (tkn blob, consumed boolean, PRIMARY KEY (tkn));

INSERT INTO tkns (tkn, consumed) VALUES (?,FALSE) USING TTL 30;

UPDATE tkns USING TTL 1 SET consumed = TRUE WHERE tkn = ? IF consumed = FALSE;

I use the '[applied]' column in the result set of the update statement to determine whether the token has been successfully consumed or if the token is being replayed.

My test involves concurrently executing many sets of 1 insert and 2 update statements (using Session#execute on BoundStatemnts) then checking to make sure that only one of the updates was applied.

When I run this test with relatively few iterations (~100) my results are  what I expect (exactly 1 update succeeds).  At ~1000 iterations, I start seeing both updates reporting success in 1-2% of cases.  While my test is running, I see corresponding error entries in the Cassandra log:

ERROR 15:34:53,583 Exception in thread Thread[MutationStage:522,5,main]
java.lang.NullPointerException
ERROR 15:34:53,584 Exception in thread Thread[MutationStage:474,5,main]
java.lang.NullPointerException
ERROR 15:34:53,584 Exception in thread Thread[MutationStage:536,5,main]
java.lang.NullPointerException
ERROR 15:34:53,729 Exception in thread Thread[MutationStage:480,5,main]
java.lang.NullPointerException
ERROR 15:34:53,729 Exception in thread Thread[MutationStage:534,5,main]
java.lang.NullPointerException


Thanks.

Update:

I'm not sure what's going on with the logging the the dev release.  I grabbed the rc2 source and built that.  The resultant log is a bit more informative:

ERROR 11:53:38,967 Exception in thread Thread[MutationStage:114,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.serializers.UUIDSerializer.deserialize(UUIDSerializer.java:32)
	at org.apache.cassandra.serializers.UUIDSerializer.deserialize(UUIDSerializer.java:26)
	at org.apache.cassandra.db.marshal.AbstractType.compose(AbstractType.java:142)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getUUID(UntypedResultSet.java:131)
	at org.apache.cassandra.db.SystemKeyspace.loadPaxosState(SystemKeyspace.java:785)
	at org.apache.cassandra.service.paxos.PaxosState.commit(PaxosState.java:118)
	at org.apache.cassandra.service.paxos.CommitVerbHandler.doVerb(CommitVerbHandler.java:34)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
",3 node Cassandra 2.0.0-rc2 cluster. Java driver 1.0.2.,ppersad,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5945,,,,,,,,,,,,,,,"27/Aug/13 13:53;slebresne;5925.txt;https://issues.apache.org/jira/secure/attachment/12600165/5925.txt","26/Aug/13 18:04;ppersad;TokenConsumptionTest.java;https://issues.apache.org/jira/secure/attachment/12599991/TokenConsumptionTest.java",,,,,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,345083,,,Wed Aug 28 10:33:54 UTC 2013,,,,,,,,,,"0|i1nibj:",345384,2.0 rc2,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"23/Aug/13 07:43;slebresne;Don't you have more stack trace in the log? To have a NPE in the MutationStage is definitively a bug, but it would be immensely easier to find what's the problem with a stack trace. And there should be a trace. ;;;","23/Aug/13 16:49;ppersad;I'm a little puzzled by that myself.  What I've posted is exactly what I see in the log output.  The exception seems to be being logged by the UncaughtExceptionHandler set at CassandraDaemon:129.  That's the only place in the code base where I can find a matching exception message.  That call looks like it should have a full stack trace, but I'm not seeing it in the log.;;;","23/Aug/13 18:59;ppersad;I've updated the ticket with a more complete stack trace.;;;","26/Aug/13 09:36;slebresne;Thanks for the full stack.

Not completely sure what triggers that NPE however. I do see one scenario where the paxos state on-disk could not be empty but the there's no ""in_progress_ballot"" column (hence triggering that NPE): since savePaxosPromise only write the ""proposal"" and not the ""in_progress_ballot"", and since we set TTL on inserts, it sounds possible to end up in a case where a paxos row only has the ""proposal"" column but everything else has expired. I do am attaching a simple patch to always write ""in_progress_ballot"" to avoid that, but given that paxos TTL is at least 3 hours, I doubt that's the scenario you are running into in your test. So not sure what's going on ([~jbellis], if you have a brilliant idea...).

Phil, if you could check if the attached patch fixes it by any chance, that could be helpful. If it doesn't (likely), would you be able to provide a simple test script that reproduce this?
;;;","26/Aug/13 18:04;ppersad;JUnit test to demonstrate the issue.;;;","26/Aug/13 18:07;ppersad;I applied the patch and it did indeed make the NPEs go away.  However the double-consumption of tokens persists.  See the attached JUnit test class (the CassandraClient mentioned in the test is just a simple wrapper that creates a Cluster and Session and sets a default consistency of QUORUM on every query).;;;","27/Aug/13 13:53;slebresne;Thanks for the test.

There is indeed 2 problems:
# the NPE while loading the paxos state. Contrarily to my first reading, the fact that savePaxosProposal doesn't save the inProgress ballot is not only a problem due to expiration. We may call that method if our own inProgress is older than the proposal, so given a node received a proposal without having seen the prepare first (and without having a previous state), we'd end up with a state where just the 'proposal' column is set. Note that technically, I don't think it breaks Paxos not to update the ballot when saving a proposal so we could just fix loadPaxosState to not NPE in that case, but it feels saner/simpler to me to write the proposal ballot when we save a proposal value.
# the reason Phil test fails is different however. The problem is that when we were building the 'expected' ColumnFamily for the cas() call in CQL3, we were using the full parameters of the statement, including (which was the problem) the TTL. In that test, the TTL is 1 second, so it's possible (and even not that unlikely since we only have up to 1 second accuracy internally) that when we were comparing 'expected' to 'current' the expected column was considered deleted. So, if when the 2nd update to a row was processed the first one had expired (again, not unlikely given the 1 second ttl), the 2nd CAS update was comparing both a deleted 'expected' and a deleted 'current', thus succeeding.  So anyway, the fix is to not use the TTL for the conditions in ModificationStatement.

So attaching patch that fix both issue.
;;;","27/Aug/13 18:26;ppersad;I applied the patch and it does seem to improve the situation.  Unfortunately, I'm still seeing double consumptions.  However, the occurrence has dropped from 1-2% to 0.002%-0.04%.  That rate is low enough that it may not show up with the sample size of 40,000 in the test I posted.  Increasing the iterations by 10-20 times should serve to demonstrate the issue.

For the sake of exploration, after I patched I tried running the test both with and without the TTL in the update statement an saw no appreciable difference in the number of failures. It looks like there may be yet a third problem.;;;","27/Aug/13 18:32;jbellis;+1;;;","27/Aug/13 18:38;slebresne;bq. I applied the patch and it does seem to improve the situation. Unfortunately, I'm still seeing double consumptions. However, the occurrence has dropped from 1-2% to 0.002%-0.04%

Forgot to say, your test is actually broken in theory because it picks random keys. It is thus possible for an insert of one of the jobs to interleave with the updates of another one, thus making both update apply (and presumably both update of the other job fail, but the test don't check for that). So I'd suggest first re-running with guaranteed unique keys. If it still fails, I'm happy to look at it more deeply, though we did fixed 2 problem so let's commit those and open a separate ticket if a 3rd problem there is indeed (and if there is 3rd that take more than 40K iterations to manifest itself, chances are it will be tricky to track down, so it's not worth holding on the initial fixes). ;;;","27/Aug/13 20:02;ppersad;I take your point and I'll tweak my test to check for duplicate keys.  That being said, even without the use of SecureRandom, I seriously doubt that I'm getting collisions on a 64 Byte key.

I'll create a new ticket once I've had time to update my test.;;;","27/Aug/13 21:29;ppersad;I've updated my test to ensure uniqueness of the tokens and am still experiencing failures.  I've created CASSANDRA-5945 to track the issue so that the fixes here can be committed.;;;","28/Aug/13 10:33;slebresne;Alright, committed, thanks, we'll followup on CASSANDRA-5945.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NoSuchMethodError when calling YamlConfigurationLoader.loadConfig(),CASSANDRA-5917,12664887,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,pookieman,pookieman,21/Aug/13 17:59,16/Apr/19 09:32,14/Jul/23 05:53,22/Aug/13 00:48,2.0.0,,,Legacy/CQL,,,0,,,,,"Hi, when this method is called, I see this:

java.lang.NoSuchMethodError: org.yaml.snakeyaml.Yaml.<init>(Lorg/yaml/snakeyaml/constructor/BaseConstructor;)V
	at org.apache.cassandra.config.YamlConfigurationLoader.loadConfig(YamlConfigurationLoader.java:86)
	at org.apache.cassandra.config.DatabaseDescriptor.loadConfig(DatabaseDescriptor.java:125)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:101)
......

I believe it's because of an enhancement made in ticket CASSANDRA-5606. The version of snakeyaml that 2.0.0-rc1 depends on is 1.6, but this constructor doesn't exist in that version but it does in version 1.12.

Coincidentally CASSANDRA-5317 speaks of upgrading the snakeyaml dependency, but I'm not sure what was upgraded. ",Linux,dbrosius,pookieman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,344830,,,Thu Aug 22 00:48:19 UTC 2013,,,,,,,,,,"0|i1ngr3:",345130,,,,,,,,,,,,Low,,2.0 rc1,,,,,,,,,,,,,,,,"21/Aug/13 18:06;jbellis;We're shipping 1.11 in lib/.  Is this some kind of maven-only problem?;;;","21/Aug/13 18:11;pookieman;The build.xml:

https://github.com/apache/cassandra/blob/cassandra-2.0.0-rc1/build.xml

seems to point to version 1.6 of snakeyaml;;;","21/Aug/13 18:18;jbellis;Translation: yes.;;;","21/Aug/13 18:53;dbrosius;The problem is that the committed version of snakeyaml is 1.11 (in lib), but the ant(maven) specification is 1.6, which is why you don't see it in dev. Need to change maven version to 1.11 to match what we've all been using. Why do we have jars committed to git again?
;;;","21/Aug/13 19:40;jbellis;Because Apache requires that we ship a license for every dependency and the only solution we've found is to hand-maintain that.;;;","21/Aug/13 19:57;dbrosius;i see..  btw, there is no metrics-core license in 1.2;;;","21/Aug/13 20:01;jbellis;We should add one. /cc [~yukim];;;","21/Aug/13 20:44;yukim;[~jbellis] done in 0db3413548552ea044890b7d7ffd0dbed0078be1;;;","22/Aug/13 00:48;dbrosius;fix ninja'd to cassandra-2.0.0 as commit 8551ff93e9de972bc604d1804b86a10ecb2d3382;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
gossip and tokenMetadata get hostId out of sync on failed replace_node with the same IP address,CASSANDRA-5916,12664877,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,21/Aug/13 17:12,16/Apr/19 09:32,14/Jul/23 05:53,18/Oct/13 02:08,1.2.11,2.0.2,,,,,0,,,,,"If you try to replace_node an existing, live hostId, it will error out.  However if you're using an existing IP to do this (as in, you chose the wrong uuid to replace on accident) then the newly generated hostId wipes out the old one in TMD, and when you do try to replace it replace_node will complain it does not exist.  Examination of gossipinfo still shows the old hostId, however now you can't replace it either.",,andrew@addthis.com,cburroughs,jasobrown,ravilr,rcoli,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5571,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/13 15:25;brandon.williams;5916-v2.txt;https://issues.apache.org/jira/secure/attachment/12607367/5916-v2.txt","16/Oct/13 20:26;brandon.williams;5916-v3.txt;https://issues.apache.org/jira/secure/attachment/12608795/5916-v3.txt","17/Oct/13 21:12;brandon.williams;5916-v4.txt;https://issues.apache.org/jira/secure/attachment/12609025/5916-v4.txt","24/Sep/13 23:40;brandon.williams;5916.txt;https://issues.apache.org/jira/secure/attachment/12604915/5916.txt",,,,,,,,,,,,,,,,,4.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,344820,,,Fri Oct 18 02:08:38 UTC 2013,,,,,,,,,,"0|i1ngov:",345120,1.2.8,,,,,,,,thobbs,,thobbs,Normal,,1.2.0,,,,,,,,,,,,,,,,"21/Aug/13 17:25;brandon.williams;The problem runs a little deeper, too: even if you specify the right uuid, and the replace fails for whatever reason, now they're out of sync again and you can't do the replace at all.;;;","21/Aug/13 19:13;brandon.williams;This same behavior also occurs with replace_token.;;;","22/Aug/13 20:21;brandon.williams;This isn't so much a problem with retrying the replace, as it is with the same IP address (which won't work at all currently.) The reason for this is that by using the same IP address, the replacing node itself changes the HOST_ID, and then can't find the old one.  It's not just as simple as not advertising a new HOST_ID either, since by not having one but modifying STATUS we wipe out any existing HOST_ID as well.;;;","24/Sep/13 23:40;brandon.williams;Here's my first (working) attempt at solving this.  This patch disables replace_[token,node] and adds a new replace_address.  In some ways replace_address seems more intuitive, but really we have to do it this way because we're going to pull everything else we need out of gossip, and endpoints are keyed by address.

We use a special gossip operation I'm calling 'shadow gossip' where we use a generation of zero and only do a single, half-round.  This means we send an empty SYN with our own blank digest to a seed, accept one ACK and then stop the gossip round there, so as not to perturb any existing state.

From there we extract the original HOST_ID and tokens, and use those for the replacement process.  A catch here though is once our gossiper actually starts, we'll knock both the TOKENS state and the existing STATUS state (for single token replacements) out with our newer, real generation, so if the replace fails past this point, we can't retry.  It may be possible to stay in shadow gossip mode through all of the process to get around that (and just remove the hibernate state), but I haven't tried this.;;;","29/Sep/13 22:45;ravilr;Tested the patch applied against 1.2.10 and it works. Hints replay also works now after replace/bootstrap.  Regarding the corner case, where replace fails to finish after gossiper started with new generation, hence knocking out the TOKENS state,  does it make sense to allow the operator to specify replace_token with the token(s) along with the replace_address to recover from such scenario. the token list is logged during the first attempt already.
I think remaining in shadow mode may not work optimally well for cases where the node being replaced was down for more than hint window. So, all the nodes would have stopped hinting, and after replace, it would require repair to be ran to get the new data fed during the replace.
;;;","07/Oct/13 20:22;brandon.williams;First, thanks for testing, [~ravilr]!

bq. does it make sense to allow the operator to specify replace_token with the token(s) along with the replace_address to recover

That could work, but I find it a bit ugly and confusing, especially since replace_token alone is supposed to work right now, but does not.

bq. I think remaining in shadow mode may not work optimally well for cases where the node being replaced was down for more than hint window. So, all the nodes would have stopped hinting, and after replace, it would require repair to be ran to get the new data fed during the replace.

That is true regardless of shadow mode though, since hibernate is a dead state and the node doesn't go live to reset the hint timer until the replace has completed.;;;","07/Oct/13 21:03;ravilr;bq. That is true regardless of shadow mode though, since hibernate is a dead state and the node doesn't go live to reset the hint timer until the replace has completed.

my understanding is, due to the generation change of the replacing node, gossiper.handleMajorStateChange marks the node as dead, as hibernate is one of the DEAD_STATES. So, the other nodes marks the replacing node as dead before the token bootstrap starts, hence should be storing hints to the replacing node from that point.  Am i reading it wrong? ;;;","07/Oct/13 21:53;brandon.williams;You're right, it will change the endpoint's expire time and reset the window.  That said, once the bootstrap has started the node should be receiving any incoming writes for the range it owns, so 'new' hints shouldn't matter in the common case where it succeeds.;;;","07/Oct/13 23:36;ravilr;bq. once the bootstrap has started the node should be receiving any incoming writes for the range it owns, so 'new' hints shouldn't matter in the common case where it succeeds.

Is this true for node bootstrapping in hibernate state? From what i have observed, writes to hibernate'd node during its bootstrap are not sent to it, as gossip marks that node down right. 

;;;","08/Oct/13 15:25;brandon.williams;It's not true for replacing, not only because we're down but also because we don't do any pending range announcement since there's no point.

I'd be fine with telling people they need to have a large enough hint window to complete the replace to avoid needing to repair, but we have to spin up 'real' gossip to get the schema anyway, so staying in shadow mode the entire time won't work.

However, there is a relatively simple way to have our cake (automatically extended hint window) and eat it too (be able to retry on failure and not have to specify anything new.)  As soon as we receive the tokens via shadow gossip, we can set them ourselves along with the hibernate state.  When we spin up the full gossip mode to get the schema, we'll be using the same HOST_ID and TOKENS that we grabbed, so if anything goes wrong at that point we can just grab them again next time.

This just leaves the issue of checking that the host is really dead, but this doesn't make any sense when replacing with the same IP anyway, so we can skip it when the addresses match.

v2 does all of this and includes a few other minor cleanups.;;;","08/Oct/13 21:42;thobbs;I'm testing this out with a three-node ccm cluster.  If I do the following:
# (optional) stop node3
# add a blank node4
# start node4 with replace_address=127.0.0.3

I'll get the following:
{noformat}
ERROR 16:29:02,689 Exception encountered during startup
java.lang.RuntimeException: Cannot replace_address /127.0.0.3because it doesn't exist in gossip
    at org.apache.cassandra.service.StorageService.prepareReplacementInfo(StorageService.java:421)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:623)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:604)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:501)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
java.lang.RuntimeException: Cannot replace_address /127.0.0.3because it doesn't exist in gossip
    at org.apache.cassandra.service.StorageService.prepareReplacementInfo(StorageService.java:421)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:623)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:604)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:501)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Exception encountered during startup: Cannot replace_address /127.0.0.3because it doesn't exist in gossip
ERROR 16:29:02,692 Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
    at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:321)
    at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:370)
    at org.apache.cassandra.service.StorageService.access$000(StorageService.java:88)
    at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:569)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    at java.lang.Thread.run(Thread.java:724)
{noformat}

This happens whether node3 is up or down.  It seems like this problem occurs any time replace_address doesn't match the broadcast address.;;;","16/Oct/13 20:26;brandon.williams;There are two distinct cases here: we replace 'ourself' with the same IP, or we replace a dead node with a new IP (ala ec2.)  We can't know which one we're doing _a priori_, so we shadow gossip.  If we're replacing the same IP, our shadow SYN will contain it, and the remote node will ACK with what we need.

If we're _not_ replacing with the same IP, there's a problem: an ACK will only contain what was present in the SYN digest list.  One could argue this is the sender being naive, since it obviously knows the node that sent the SYN doesn't have some states that it does, but I think at scale this makes sense since it's possible a third node has begun gossiping with the SYN sender, too.  In any case, I don't want to change that behavior at this point.

The other problem is, we can't just sit around and wait for someone to send us a populated SYN either, since we're not a part of gossip and we're new.  But we don't know we're new yet, and can't insert ourselves into gossip either, or we'll break the case of using the same IP.

So, we'll create a special case for shadow gossip, and redefine it a bit.  Instead of sending a SYN with our own endpoint and a generation of zero, we'll send a completely empty SYN (digest-wise, we still populate the cluster name and partioner, since those checks still make sense.)  This won't ever normally occur in gossip, because a node always knows about and adds itself.  When we see an empty SYN, we can know that the node that sent it is asking for everything we've got, and we can ACK with just that, allowing the replacement node to have whatever it needs for either the same or different IP cases.

v3 does this.;;;","17/Oct/13 19:14;thobbs;That strategy sounds good to me in principle.

I'm seeing a few problems when testing, though.

If I start node4 with replace_address=node3 (while node3 is either up or down), I get an NPE:

{noformat}
DEBUG 14:01:33,359 Node /127.0.0.4 state normal, token [6564349027099416762]
 INFO 14:01:33,362 Node /127.0.0.4 state jump to normal
ERROR 14:01:33,363 Exception encountered during startup
java.lang.NullPointerException
	at org.apache.cassandra.gms.Gossiper.usesHostId(Gossiper.java:682)
	at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:694)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1382)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1250)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:973)
	at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1187)
	at org.apache.cassandra.service.StorageService.setTokens(StorageService.java:214)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:824)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:584)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:481)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
java.lang.NullPointerException
	at org.apache.cassandra.gms.Gossiper.usesHostId(Gossiper.java:682)
	at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:694)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1382)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1250)
	at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:973)
	at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1187)
	at org.apache.cassandra.service.StorageService.setTokens(StorageService.java:214)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:824)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:584)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:481)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Exception encountered during startup: null
ERROR 14:01:33,368 Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:321)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:370)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:88)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:549)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:724)
{noformat}

If I do replace_address with a non-existent node, after the ring delay sleep, I'll see:
{noformat}
java.lang.RuntimeException: Unable to gossip with any seeds
{noformat}
which is misleading, as that's not the actual problem.  Perhaps we should explicitly check for presence of the address to replace?

I've also seen that the node to replace can be the seed selected to gossip with, which results in this:
{noformat}
 INFO 14:12:58,298 Gathering node replacement information for /127.0.0.3
 INFO 14:12:58,302 Starting Messaging Service on port 7000
DEBUG 14:12:58,316 attempting to connect to /127.0.0.3
ERROR 14:13:29,320 Exception encountered during startup
java.lang.RuntimeException: Unable to gossip with any seeds
	at org.apache.cassandra.gms.Gossiper.doShadowRound(Gossiper.java:1123)
	at org.apache.cassandra.service.StorageService.prepareReplacementInfo(StorageService.java:396)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:603)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:584)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:481)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
{noformat};;;","17/Oct/13 21:12;brandon.williams;v4 fixes the NPE and throws when autobootstrap is disabled.  The second issue wasn't because of the replace_address, but because of checks in sendGossip.  v4 just manually sends the message to all seeds.  Depending on how many seeds you had, that may also fix the last issue (if the node being replaced is the only seed, obviously that can't work.);;;","17/Oct/13 21:38;thobbs;Minor nitpick: you're missing a space before ""because"" in:
{noformat}
throw new RuntimeException(""Cannot replace_address "" + DatabaseDescriptor.getReplaceAddress() + ""because it doesn't exist in gossip"");
{noformat}

Other than that, +1;;;","18/Oct/13 02:08;brandon.williams;Committed.  I will note for ops folks, you can use replace_address in a mixed minor version 1.2 cluster, as long as one seed is also upgraded.  If no seeds are upgraded there will be no harm, the replace will simply fail.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Most CQL3 functions should handle null gracefully,CASSANDRA-5910,12664835,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,21/Aug/13 13:21,16/Apr/19 09:32,14/Jul/23 05:53,11/Sep/13 06:32,1.2.10,,,,,,0,,,,,"Currently, we don't allow null parameters for functions. So
{noformat}
UPDATE test SET d=dateOf(null) WHERE k=0
{noformat}
is basically an invalid query. Unfortunately, there's at least one case where we don't validate correctly, namely if we do:
{noformat}
SELECT k, dateOf(t) FROM test
{noformat}
In that case, if for any of the row {{t}} is null, we end up with a server side NPE. But more importantly, throwing an InvalidException in that case would be pretty inconvenient and actually somewhat wrong since the query is not invalid in itself. So, at least in that latter case, we want {{dateOf(t) == null}} when {{t == null}}. And if we do that, I suggest making it always the case (i.e. make the first query valid but assigning {{null}} to {{d}}).
",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/13 13:10;slebresne;5910-v2.txt;https://issues.apache.org/jira/secure/attachment/12602331/5910-v2.txt","21/Aug/13 13:23;slebresne;5910.txt;https://issues.apache.org/jira/secure/attachment/12599187/5910.txt",,,,,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,344778,,,Wed Sep 11 06:32:30 UTC 2013,,,,,,,,,,"0|i1ngfj:",345078,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"21/Aug/13 13:29;slebresne;Attaching simple patch for this. I'll note that because of multi-parameters functions (and because a future function may not want to return null when passed null as argument), I think it should be each function job to handle null the way it makes the most sense. That being said, outside of token() that refuse null competely, the patch makes all our currently existing function return null on null.
;;;","21/Aug/13 13:46;jbellis;I don't think turning RTE into IRE is correct -- if it passes validation, it should execute, and if it doesn't, it's a bug and not an invalid request.

Where does this leave varcharasblob?  (Wish we'd gone with textasblob, FTR.);;;","21/Aug/13 14:43;slebresne;bq. I don't think turning RTE into IRE is correct

I don't really disagree and those catch all could probably be removed. The motivation was that if the execution of a function triggers an unexpected exception, then we send it back to the user instead of ""crashing"" server side, but as long as we don't have user custom function, an unexpected exception in a function is a bug so it's probably fine to throw server side. And I agree IRE is a bad exception anyway, we just don't have anything better so far.

But the bigger problem is that we have functions (in the selection of a SELECT) that take their input at execution time. So I don't fully agree with ""if it passes validation, it should execute, and if it doesn't, it's a bug"" as that would mean we limit ourselves to functions that can never error out but that's going to be pretty limiting. We already have the token() method for which it's very unclear how to deal with null. We could return null on a null parameter, but if we have a composite partition key (in which case token() is a multi-parameter function), what to do when only one argument is null and not the other? Of course we could still return null in that case, but it feels wrong to silent what is essentially an error. Or to take another example, what do we do about division by 0 if we add a division function tomorrow?

My hunch is that we need a new ExecutionException, though unfortunately it's not that easy to add new exceptions (rather, there is backward compatibility concerns).

To be clear, we can probably find a simple hack for now, making token() return null as soon as any of its parameter is null is, while not perfect, probably good enough for now. But it's probably worth having a longer term plan.

bq. Where does this leave varcharasblob?

varcharasblob (and all the asBlob functions for that matter) execute method just return it's argument without any processing (it's really just a way to make the CQL type system happy, since internally everything is already bytes anyway), so if said argument is null, it will return null.

bq. Wish we'd gone with textasblob

We have it too, we have both. We have one XasBlob and one blobAsX for every CQL3 type. The only reason varchar is a special case in ByteConversionFcts is that the makeToBlobFunction method wouldn't generate it.;;;","26/Aug/13 22:05;jbellis;So how about we drop the RTE->IRE conversion for now and we can look at adding an ExecutionException around when we add more problematic functions/operators and/or UDF?;;;","10/Sep/13 13:10;slebresne;Sounds good to me. Attaching v2 that just makes all existing function handle null correctly (token() will return null as soon as any one of its argument is null). We can see later for what we do when returning null doesn't fly anymore.;;;","10/Sep/13 15:49;aleksey;+1;;;","11/Sep/13 06:32;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogReplayer date time issue,CASSANDRA-5909,12664833,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,artur.kronenberg,artur.kronenberg,21/Aug/13 13:12,16/Apr/19 09:32,14/Jul/23 05:53,15/Sep/13 00:59,1.2.10,,,,,,0,,,,,"Hi,

First off I am sorry if the component is not right for this. 

I am trying to get the point-in-time backup to work. And I ran into the following issues: 

1. The documentation in the commitlog_archiving.properties seems to be out of date, as the example date format is no more valid and can't be parsed. 

2. 

The restore_point_in_time property seems to differ from the actual maxTimeStamp. I added additional logging to the codebase in the class CommitLogReplayer like that: 

protected boolean pointInTimeExceeded(RowMutation frm)
    {
        long restoreTarget = CommitLog.instance.archiver.restorePointInTime;
        logger.info(String.valueOf(restoreTarget));
        for (ColumnFamily families : frm.getColumnFamilies())
        {
            logger.info(String.valueOf(families.maxTimestamp()));
        	if (families.maxTimestamp() > restoreTarget)
                return true;
        }
        return false;
    }

The following output can be seen: 

The restoreTarget timestamp is: 1377015783000
This has been correctly parsed as I added this date to the properties: 
2013:08:20 17:23:03

the value for families.maxTimestamp() is: 1377009021033000
This date corresponds to: Mon 45605-09-05 10:50:33 BST (44 millennia from now)

It seems like the timestamp has 3 additional zeros. This also means that the code can never return false on the call, as the restoreTarget will always be smaller then the maxTimestamp(). Therefore the Replayer can never replay any of my commitlog files. 
The timestamp minus the 3 zeros corresponds to ""Tue 2013-08-20 15:30:21 BST (23 hours ago)"" which makes more sense and would allow for the replay to work. 

My config: 

Cassandra-1.2.4
Java 1.6
Ubuntu 12.04 64bit 

If you need any more information let me know and I'll be happy to suply whatever info I can. 

-- artur 


",,artur.kronenberg,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Aug/13 07:35;vijay2win@yahoo.com;0001-CASSANDRA-5909.patch;https://issues.apache.org/jira/secure/attachment/12600113/0001-CASSANDRA-5909.patch",,,,,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,344776,,,Sun Sep 15 00:59:59 UTC 2013,,,,,,,,,,"0|i1ngf3:",345076,1.2.4,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"27/Aug/13 01:52;vijay2win@yahoo.com;Ahaaa looks like we need a configuration for Milli/Micro second precisions, 
Users should not mix those in a cluster to have a reliable delete and updates, so it should be fine. The other option is to write additional long field while storing the RM in the commit log.  ;;;","27/Aug/13 07:35;vijay2win@yahoo.com;Attached patch and test case as a fix to add precision. Thanks!;;;","13/Sep/13 08:22;jbellis;Comments:

- Default in the config file is millis, but default in the code is micros.  For 1.2, should both be whatever it was before, for compatibility [think this is millis]; for 2.0 should be micros (add NEWS of change)
- Extra no-op eol semicolon in the test code; extra unused imports as well
- Formatting of ?: in archiver is nonstandard

Otherwise, LGTM.;;;","15/Sep/13 00:59;vijay2win@yahoo.com;Fixed the comments and Committed to 1.2, 2.0 and trunk, Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add existing sstables to leveled manifest on startup,CASSANDRA-5908,12664817,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,21/Aug/13 12:14,16/Apr/19 09:32,14/Jul/23 05:53,21/Aug/13 12:55,2.0.0,,,,,,0,lcs,,,,"we need to add all sstables to the leveled manifest on startup, looks like this was introduced in 6968f68cd7c",,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/13 12:15;marcuse;0001-on-startup-add-all-sstables-to-the-leveled-manifest.patch;https://issues.apache.org/jira/secure/attachment/12599179/0001-on-startup-add-all-sstables-to-the-leveled-manifest.patch",,,,,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,344760,,,Wed Aug 21 12:55:43 UTC 2013,,,,,,,,,,"0|i1ngbj:",345060,,,,,,,,,,,,Normal,,2.0 beta 1,,,,,,,,,,,,,,,,"21/Aug/13 12:43;jbellis;+1;;;","21/Aug/13 12:55;marcuse;cool, committed as 8f367fdf92c03ee4bbcb0daa2272bd5155cf4174;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Leveled compaction may cause overlap in L1 when L0 compaction get behind,CASSANDRA-5907,12664648,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,20/Aug/13 17:27,16/Apr/19 09:32,14/Jul/23 05:53,21/Aug/13 20:37,1.2.9,,,,,,0,lcs,,,,"1.2 makes LCS run parallel, though if L0 compaction get far behind and concurrent compactions at L0 where each compaction holds maximum number of SSTable to compact at L0(32), it will likely cause overlap in L1. There will be ERROR log as follows:

{code}
ERROR [CompactionExecutor:30] 2013-08-19 17:54:29,648 LeveledManifest.java (line 244) At level 1, SSTableReader(path='xxx-Data.db') [DecoratedKey(204853724659241194183955214890519, 30303132343830), DecoratedKey(69227335985728660912035125310473966323, 30393537373332)] overlaps SSTableReader(path='xxx-Data.db') [DecoratedKey(217896711032704014921095870827202, 30333635363932), DecoratedKey(71430242198281555888954138354238066233, 30333035343132)].  This is caused by a bug in Cassandra 1.1.0 .. 1.1.3.  Sending back to L0.  If you have not yet run scrub, you should do so since you may also have rows out-of-order within an sstable
{code}

We should send back compacted SSTables to L0 when compacting max SSTables at L0. Also, the above error message is confusing, at version 1.2, we can reduce to WARNing level without mentioning scrub.

C* 2.0 performs Size-Tiered compaction on L0 when it has max SSTables and sends back compacted SSTable to L0, so I think we don't need to fix this on 2.0.",,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/13 16:42;yukim;5907-1.2.txt;https://issues.apache.org/jira/secure/attachment/12599220/5907-1.2.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,344591,,,Wed Aug 21 20:37:20 UTC 2013,,,,,,,,,,"0|i1nf9z:",344891,,,,,,,,,jbellis,,jbellis,Low,,1.2.0,,,,,,,,,,,,,,,,"20/Aug/13 17:36;jbellis;bq. We should send back compacted SSTables to L0 when compacting max SSTables at L0

I'm not sure if that's the right solution.  Isn't it possible-but-unlikely to generate the same kind of overlap with non-maxed-out compactions if we happen to flush the right data at the right time after the first compaction starts?;;;","20/Aug/13 18:36;jjordan;Is this an issue any time L0 compactions happen in parallel?  Maybe we should not allow that?;;;","20/Aug/13 19:12;yukim;No.
We check overlap with compacting L0 to prevent L1 overlap if L0 is not maxed out.
(https://github.com/apache/cassandra/blob/cassandra-1.2.8/src/java/org/apache/cassandra/db/compaction/LeveledManifest.java#L534)

So this only likely happen when compaction task with 32 L0 sstables run concurrently and those sstables have overlap.;;;","20/Aug/13 23:07;jbellis;It looks like the problem then is a race when we are running getCandidatesFor before a parallel compaction has marked its sources compacting.  We synchronize getCompactionCandidates (caller of getCandidatesFor) but that is not adequate.

(Edit: which would mean that it can indeed happen with any number of candidates, in theory.);;;","21/Aug/13 16:42;yukim;I think simply moving check overlap with compacting L0 out to the end of getCandidateFor will fix the problem.
Patch attached.;;;","21/Aug/13 17:00;jjordan;Reading the patch 10 times, I think that logic is right to skip doing parallel L0 compactions if there is any overlap.;;;","21/Aug/13 18:16;jbellis;+1

NB: I think {{!Sets.intersection(candidates, compacting).isEmpty()}} is redundant since markCompacting will fail later.  Take that out in trunk?;;;","21/Aug/13 20:37;yukim;Committed, with nit change in trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't announce schema version until we've loaded the changes locally,CASSANDRA-5904,12664589,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,20/Aug/13 10:21,16/Apr/19 09:32,14/Jul/23 05:53,20/Aug/13 14:49,1.2.9,,,,,,0,,,,,"Currently, we call updateSchemaAndAnnounce (which sets the schema version in the system table and announce it on gossip) in DefsTable.mergeSchema as soon as we've persisted the schema mutation but *before* we've actually loaded new/updated KS/CF locally. This makes it impossible to reliably check for schema agreement in a cluster as even if all nodes have the same version set in the system tables, you could still get an insert on a new table rejected because the table hasn't been loaded yet.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Aug/13 10:23;slebresne;5904.txt;https://issues.apache.org/jira/secure/attachment/12598929/5904.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,344532,,,Tue Aug 20 14:49:11 UTC 2013,,,,,,,,,,"0|i1nex3:",344832,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"20/Aug/13 10:23;slebresne;Attaching trivial patch that moves the call to updateVersionAndAnnounce at the end of mergeSchema.;;;","20/Aug/13 14:01;aleksey;+1;;;","20/Aug/13 14:49;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Integer overflow in OffHeapBitSet when bloomfilter > 2GB,CASSANDRA-5903,12664583,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,tdevelioglu,tdevelioglu,20/Aug/13 08:29,16/Apr/19 09:32,14/Jul/23 05:53,21/Aug/13 17:32,1.2.9,,,,,,0,patch,,,,"In org.apache.cassandra.utils.obs.OffHeapBitSet.

byteCount overflows and causes an IllegalArgument exception in Memory.allocate when bloomfilter is > 2GB.

Suggest changing byteCount to long.

{code:title=OffHeapBitSet.java}
    public OffHeapBitSet(long numBits)
    {
        // OpenBitSet.bits2words calculation is there for backward compatibility.
        int byteCount = OpenBitSet.bits2words(numBits) * 8;
        bytes = RefCountedMemory.allocate(byteCount);
        // flush/clear the existing memory.
        clear();
    }

{code}",,tdevelioglu,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/13 20:58;vijay2win@yahoo.com;0001-CASSANDRA-5903-check.patch;https://issues.apache.org/jira/secure/attachment/12599276/0001-CASSANDRA-5903-check.patch","20/Aug/13 20:33;vijay2win@yahoo.com;0001-CASSANDRA-5903.patch;https://issues.apache.org/jira/secure/attachment/12599022/0001-CASSANDRA-5903.patch","21/Aug/13 16:04;tdevelioglu;0002-CASSANDRA-5903.patch;https://issues.apache.org/jira/secure/attachment/12599214/0002-CASSANDRA-5903.patch",,,,,,,,,,,,,,,,,,3.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,344526,,,Thu Aug 22 02:26:50 UTC 2013,,,,,,,,,,"0|i1nevz:",344826,1.2.8,,,,,,,,tdevelioglu,,tdevelioglu,Normal,,,,,,,,,,,,,,,,,,"20/Aug/13 11:03;tdevelioglu;Added relevant stacktrace

DEBUG [CompactionExecutor:116] 2013-08-19 17:06:07,543 CompactionTask.java (line 115) Expected bloom filter size : 1830440832
ERROR [CompactionExecutor:116] 2013-08-19 17:06:07,584 CassandraDaemon.java (line 192) Exception in thread Thread[CompactionExecutor:116,1,main]
java.lang.IllegalArgumentException
        at org.apache.cassandra.io.util.Memory.allocate(Memory.java:58)
        at org.apache.cassandra.utils.obs.OffHeapBitSet.<init>(OffHeapBitSet.java:40)
        at org.apache.cassandra.utils.FilterFactory.createFilter(FilterFactory.java:143)
        at org.apache.cassandra.utils.FilterFactory.getFilter(FilterFactory.java:137)
        at org.apache.cassandra.utils.FilterFactory.getFilter(FilterFactory.java:126)
        at org.apache.cassandra.io.sstable.SSTableWriter$IndexWriter.<init>(SSTableWriter.java:446)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:92)
        at org.apache.cassandra.db.ColumnFamilyStore.createCompactionWriter(ColumnFamilyStore.java:1983)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:143)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:211)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
;;;","20/Aug/13 18:29;vijay2win@yahoo.com;I can change the byte count to long, 

As a side note, i am not sure if we are addressing the right issue. From the stack trace the byteCount should be 228805104 which is 228 MB (OpenBitSet.bits2words(1830440832L) * 8L) / ((1830440832L/64) * 8) which should fit in a integer.;;;","20/Aug/13 19:14;jbellis;Hmm.  Is bits2words overflowing somehow?;;;","20/Aug/13 19:44;vijay2win@yahoo.com;Not sure yet, still trying to figure it out (Since i am more curious)... A simple test shows it might run out after 17B to 18B keys in a single SSTable (thats a giant SST) :)

{code}
        for (int i = 0; i < 30; i++) {
            long items = (i * 1000000000L);
            System.out.println(""Items: "" + items + "" byteCount: "" + (OpenBitSet.bits2words(items) * 8));
        }
{code}

{noformat}
Items: 0 byteCount: 0
Items: 1000000000 byteCount: 125000000
Items: 2000000000 byteCount: 250000000
Items: 3000000000 byteCount: 375000000
Items: 4000000000 byteCount: 500000000
Items: 5000000000 byteCount: 625000000
Items: 6000000000 byteCount: 750000000
Items: 7000000000 byteCount: 875000000
Items: 8000000000 byteCount: 1000000000
Items: 9000000000 byteCount: 1125000000
Items: 10000000000 byteCount: 1250000000
Items: 11000000000 byteCount: 1375000000
Items: 12000000000 byteCount: 1500000000
Items: 13000000000 byteCount: 1625000000
Items: 14000000000 byteCount: 1750000000
Items: 15000000000 byteCount: 1875000000
Items: 16000000000 byteCount: 2000000000
Items: 17000000000 byteCount: 2125000000
Items: 18000000000 byteCount: -2044967296
Items: 19000000000 byteCount: -1919967296
Items: 20000000000 byteCount: -1794967296
...
{noformat};;;","20/Aug/13 19:54;vijay2win@yahoo.com;Actually my calculations where wrong it does use 2 GB for 1830440832

long numElements = 1830440832L;
FilterFactory.getFilter(numElements, 0.01d, true);

fixing it.;;;","20/Aug/13 20:33;vijay2win@yahoo.com;Simple fix for 1.2, it also catches for native OOM (I am neutral, i can also remove it so we fail fast) and throws a RTE to pause the compaction etc.;;;","20/Aug/13 20:45;jbellis;+1;;;","20/Aug/13 22:55;vijay2win@yahoo.com;Committed to 1.2 and merged into 2.0.0 -> 2.0 -> trunk. Thanks!;;;","20/Aug/13 22:59;jbellis;Can you also add a CHANGES entry?;;;","20/Aug/13 23:33;vijay2win@yahoo.com;Done! Thanks.;;;","21/Aug/13 15:07;tdevelioglu;Sadly that wasn't sufficient, there's another overflow in OffHeapBitSet.deserialize:

{code}
    public static OffHeapBitSet deserialize(DataInput dis) throws IOException
    {
        int byteCount = dis.readInt() * 8;
        Memory memory = RefCountedMemory.allocate(byteCount);
        for (int i = 0; i < byteCount;)
        {
            long v = dis.readLong();
            memory.setByte(i++, (byte) (v >>> 0));
            memory.setByte(i++, (byte) (v >>> 8));
            memory.setByte(i++, (byte) (v >>> 16));
            memory.setByte(i++, (byte) (v >>> 24));
            memory.setByte(i++, (byte) (v >>> 32));
            memory.setByte(i++, (byte) (v >>> 40));
            memory.setByte(i++, (byte) (v >>> 48));
            memory.setByte(i++, (byte) (v >>> 56));
        }
        return new OffHeapBitSet(memory);
    }
{code};;;","21/Aug/13 15:09;tdevelioglu;ERROR [SSTableBatchOpen:6] 2013-08-21 15:29:51,799 CassandraDaemon.java (line 192) Exception in thread Thread[SSTableBatchOpen:6,5,main]
java.lang.IllegalArgumentException
        at org.apache.cassandra.io.util.Memory.allocate(Memory.java:58)
        at org.apache.cassandra.utils.obs.OffHeapBitSet.deserialize(OffHeapBitSet.java:123)
        at org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:46)
        at org.apache.cassandra.utils.Murmur2BloomFilter$Murmur2BloomFilterSerializer.deserialize(Murmur2BloomFilter.java:40)
        at org.apache.cassandra.utils.FilterFactory.deserialize(FilterFactory.java:71)
        at org.apache.cassandra.io.sstable.SSTableReader.loadBloomFilter(SSTableReader.java:365)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:195)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:153)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:258)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
;;;","21/Aug/13 16:06;tdevelioglu;Patch against 1.2, that fixes overflow in OffHeapBitSet.deserialize.

With both patches I was succesfully able to load a 2.3GB bloomfilter.;;;","21/Aug/13 17:27;vijay2win@yahoo.com;Thanks Taylan, I will writeup a test case for it... The patch on 1.2 (0002) should handle up to 2GB * 8 over which we might want to serialize and deserialize into long for 2.1.;;;","21/Aug/13 17:32;jbellis;LGTM, committed;;;","21/Aug/13 17:33;jbellis;Oops, comment race condition w/ Vijay. :)  Test would still be nice.

16GB filter?  Well, maybe that's where we decide that just making bigger and bigger sstables is a bad idea...;;;","21/Aug/13 20:55;vijay2win@yahoo.com;Not sure if we still need this patch, attaching it just in case :) Ignored the test since we need 4 GB to test it function.;;;","21/Aug/13 21:36;jbellis;+1;;;","22/Aug/13 02:26;vijay2win@yahoo.com;Done, Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Setting bloom filter fp chance to 1.0 causes ClassCastExceptions,CASSANDRA-5900,12664366,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jblangston@datastax.com,jblangston@datastax.com,19/Aug/13 14:56,16/Apr/19 09:32,14/Jul/23 05:53,19/Aug/13 23:24,1.2.9,,,Legacy/Tools,,,0,,,,,"In 1.2, we introduced the ability to turn bloom filters off completely by setting fp chance to 1.0.  It looks like there is a bug with this though. When it's set to 1.0 the following errors occur because AlwaysPresentFilter is not present in the switch statement here at https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/utils/FilterFactory.java#L91, and we default to Murmur3BloomFilter for an unknown type.

Exception in thread ""main"" java.lang.ClassCastException: org.apache.cassandra.utils.AlwaysPresentFilter cannot be cast to org.apache.cassandra.utils.Murmur3BloomFilter
at org.apache.cassandra.utils.FilterFactory.serializedSize(FilterFactory.java:91)
at org.apache.cassandra.io.sstable.SSTableReader.getBloomFilterSerializedSize(SSTableReader.java:531)
at org.apache.cassandra.metrics.ColumnFamilyMetrics$15.value(ColumnFamilyMetrics.java:273)
at org.apache.cassandra.metrics.ColumnFamilyMetrics$15.value(ColumnFamilyMetrics.java:268)
at org.apache.cassandra.db.ColumnFamilyStore.getBloomFilterDiskSpaceUsed(ColumnFamilyStore.java:1825)

",,jblangston@datastax.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/13 19:18;jbellis;5900.txt;https://issues.apache.org/jira/secure/attachment/12598808/5900.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,344367,,,Mon Aug 19 23:24:19 UTC 2013,,,,,,,,,,"0|i1ndwn:",344667,,,,,,,,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,,"19/Aug/13 19:18;jbellis;Patch attached.

Note that this only affects ""nodetool cfstats"" as well as calling the affected method directly; no internal code calls this method.;;;","19/Aug/13 20:57;yukim;+1;;;","19/Aug/13 23:24;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
GossipingPropertyFileSnitch does not auto-reload local rack/dc,CASSANDRA-5897,12664029,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,daniels,jeromatron,jeromatron,16/Aug/13 09:38,16/Apr/19 09:32,14/Jul/23 05:53,06/Mar/14 17:29,2.1 rc2,,,,,,0,,,,,"With the property file snitch, I can change the rack and dc of a node in cassandra-topology.properties from DC1/RAC1 to DC2/RAC2 while the server is running and that configuration is reloaded a short time later.

With the gossiping property file snitch, I change the cassandra-rackdc.properties when the server is running from DC1/RAC1 to DC2/RAC2 while the server is running and it never reloads.  It requires a restart.",,cburroughs,daniels,jeromatron,kohlisankalp,thobbs,yeshvanthni,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Feb/14 07:37;daniels;trunk-5897-v2.txt;https://issues.apache.org/jira/secure/attachment/12631475/trunk-5897-v2.txt","27/Feb/14 07:41;daniels;trunk-5897-v3.txt;https://issues.apache.org/jira/secure/attachment/12631476/trunk-5897-v3.txt","03/Mar/14 00:10;daniels;trunk-5897-v4.txt;https://issues.apache.org/jira/secure/attachment/12632166/trunk-5897-v4.txt","26/Feb/14 05:42;daniels;trunk-5897.txt;https://issues.apache.org/jira/secure/attachment/12631156/trunk-5897.txt",,,,,,,,,,,,,,,,,4.0,daniels,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,344030,,,Thu Mar 06 17:29:36 UTC 2014,,,,,,,,,,"0|i1nbuf:",344332,1.2.6,2.1 rc3,,,,,,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,,"25/Feb/14 04:13;daniels;I'll see if I can make this work.  Should be relatively straightforward.;;;","26/Feb/14 05:44;daniels;Here is my stab at a fix for this issue.  Pretty much just following update listening logic that is already implemented in PropertyFileSnitch.

;;;","26/Feb/14 20:47;thobbs;Thanks, Daniel!

Overall this looks pretty good.  I would still check if the dc, rack, or preferLocal have changed before invalidating cached rings and gossiping.  Although ResourceWatch only triggers a reload if the mod time on the file goes up, that doesn't necessarily mean the dc or rack have changed.

And a couple of nitpicks:
* You don't need curly braces for single-line ""if"" statements
* Name the constant {{DEFAULT_REFRESH_PERIOD_IN_SECONDS}} instead of {{DEFAULT_REFRESH_PERIOD_IN_S}}
* Although this case should be rare, I would log at error instead of debug if there's a ConfigurationException setting up the watcher.;;;","27/Feb/14 07:37;daniels;Addressed CR feedback.  Note that I have previously omitted a couple of new test files from the patch (looks like git instruction on http://wiki.apache.org/cassandra/HowToContribute might be slightly off).;;;","27/Feb/14 07:41;daniels;One more amendment:  elevating log level on configuration exception;;;","28/Feb/14 17:59;thobbs;bq. looks like git instruction on http://wiki.apache.org/cassandra/HowToContribute might be slightly off

You're right.  I'll get that fixed, thanks.

v3 is almost good to go, it just needs some minor cleanup on the test file:
* Remove unused imports
* The docstring for testBasic isn't really useful.  I would just delete it and name the method ""testAutoReloadConfig"" or something similar.
* You can shorten the reload period and sleep to 1 and 1.5 seconds, respectively.

Thanks!;;;","03/Mar/14 00:10;daniels;addressed test issues;;;","04/Mar/14 21:17;thobbs;+1;;;","06/Mar/14 17:29;thobbs;Comitted;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stress reports invalid latencies,CASSANDRA-5896,12663892,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,dbrosius,enigmacurry,enigmacurry,15/Aug/13 17:44,16/Apr/19 09:32,14/Jul/23 05:53,16/Aug/13 12:40,2.1 beta1,,,Legacy/Tools,,,0,,,,,"I get this output from trunk:

{code}
$ ccm node1 stress
total,interval_op_rate,interval_key_rate,latency,95th,99th,elapsed_time
176678,17667,17667,153510.0,1322730.2,181065524.2,10
335566,15888,15888,145371.0,1220290.5,181065524.2,20
498488,16292,16292,145535.0,1227675.0,46032966.9,30
642012,14352,14352,143999.0,1130215.4,46004055.9,40
776083,13407,13407,145573.5,1737871.2,153211818.5,50
1000000,22391,22391,145128.0,1336373.6,148773096.0,60


Averages from the middle 80% of values:
interval_op_rate          : 15521
interval_key_rate         : 15521
latency median            : 146797.7
latency 95th percentile   : 1327756.5
latency 99.9th percentile : 121475978.0
Total operation time      : 00:01:00
END
{code}

Notice the wild latency values.

Whereas this is the output that I've come to expect (from cassandra-2.0):

{code}
$ ccm node1 stress
total,interval_op_rate,interval_key_rate,latency,95th,99th,elapsed_time
157972,15797,15797,0.1,1.2,119.1,10
346627,18865,18865,0.1,1.2,118.8,20
493937,14731,14731,0.1,1.2,119.2,30
663086,16914,16914,0.2,1.3,119.8,40
893083,22999,22999,0.1,1.1,65.6,50
1000000,10691,10691,0.1,1.1,65.6,55


Averages from the middle 80% of values:
interval_op_rate          : 17861
interval_key_rate         : 17861
latency median            : 0.1
latency 95th percentile   : 1.2
latency 99.9th percentile : 108.5
Total operation time      : 00:00:55
END
{code}",,dbrosius,enigmacurry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Aug/13 05:26;dbrosius;5896.txt;https://issues.apache.org/jira/secure/attachment/12598363/5896.txt",,,,,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,343893,,,Fri Aug 16 12:40:39 UTC 2013,,,,,,,,,,"0|i1nazz:",344195,,,,,,,,,jbellis,,jbellis,Normal,,2.1 rc3,,,,,,,,,,,,,,,,"15/Aug/13 17:56;jbellis;Could this be caused by the new metrics library [~dbrosius]?;;;","15/Aug/13 19:48;dbrosius;likely,.. i'll look;;;","16/Aug/13 05:26;dbrosius;timer is now returning values as nanoseconds, so convert to milliseconds as was expected before.;;;","16/Aug/13 06:47;jbellis;+1;;;","16/Aug/13 06:50;jbellis;Nit: could use TimeUnit.NANOSECONDS.toMillis instead of rolling our own.;;;","16/Aug/13 06:59;dbrosius;Yeah i was going to use that, but that is only integer based, and the existing output was decimal.;;;","16/Aug/13 07:16;jbellis;Ah, right.;;;","16/Aug/13 12:40;dbrosius;committed to trunk (2.1) as 5dabd1cc0c65b329ff518d7ad3f09e4c11494f18;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CqlParser throws StackOverflowError on bigger batch operation,CASSANDRA-5893,12663873,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,vmallet,vmallet,15/Aug/13 15:36,16/Apr/19 09:32,14/Jul/23 05:53,21/Aug/13 16:00,1.2.9,,,,,,0,,,,,"We are seeing a problem with CQL3/Cassandra 1.2.8 where a large batch operation causes the CqlParser to throw a StackOverflowError (-Xss180k initially, then -Xss325k).

Shouldn't a batch be processed iteratively to avoid having to bump stack sizes to unreasonably large values?

Here is more info from the original problem description:


<<<
It looks like the CqlParser in 1.2.8 (probably 1.2.x, but i didn't look) is implemented recursively in such a way that large batch statements blow up the stack. We, of course on a Friday night, have a particular piece of code that's hitting a degenerate case that creates a batch of inserts with a VERY large number of collection items, and it manifests as a StackOverflow coming out the cass servers:

java.lang.StackOverflowError
       at org.apache.cassandra.cql3.CqlParser.value(CqlParser.java:5266)
       at org.apache.cassandra.cql3.CqlParser.term(CqlParser.java:5627)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4807)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
       at org.apache.cassandra.cql3.CqlParser.set_tail(CqlParser.java:4813)
	...
	
I think in the short term I can give up the atomicity of a batch in this code and kind of suck it up, but obviously I'd prefer not to. I'm also not sure if I kept a single batch, but split this into smaller pieces in each statement, whether that would still fail. I'm guessing I could also crank the hell out of the stack size on the servers, but that feels pretty dirty.

It seems like the CqlParser should probably be implemented in a way that isn't quite so vulnerable to this, though I fully accept that this batch is koo-koo-bananas.
>>>

Thanks!

 ",,aleksey,jeromatron,jjordan,slebresne,vmallet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Aug/13 14:44;aleksey;5893.txt;https://issues.apache.org/jira/secure/attachment/12598664/5893.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,343874,,,Mon Aug 26 19:05:58 UTC 2013,,,,,,,,,,"0|i1navr:",344176,1.2.8,,,,,,,,slebresne,,slebresne,Normal,,1.2.0,,,,,,,,,,,,,,,,"15/Aug/13 15:49;jbellis;bq. Shouldn't a batch be processed iteratively

Probably, although batch is not intended for making ""bulk load"" atomic or other Really Large Sets of Rows.  Specifically, it's quite possible that this batch would time out even if it were parsed without issue.;;;","17/Aug/13 04:49;jbellis;Hmm.

Can we at least catch the overflow and return InvalidRequest?;;;","18/Aug/13 00:18;aleksey;Never mind what I said earlier, sorry. A misunderstanding on my part. The size of the batch doesn't matter, only the size of any set/map literal.

Stack overflow happens when parsing huge set and map literals (list literals are not affected). Starting at around 24k elements with the default 1.2 -Xss. Now, while you probably shouldn't be using literals this big, this *fits* within the 64k limit and should be supported.

And yeah, it can be done non-recursively. Will fix.;;;","21/Aug/13 15:06;slebresne;+1;;;","21/Aug/13 16:00;aleksey;Committed, thanks.;;;","26/Aug/13 19:05;vmallet;Great, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrub disk footprint needs to be reduced,CASSANDRA-5891,12663745,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lyubent,kohlisankalp,kohlisankalp,14/Aug/13 22:42,16/Apr/19 09:32,14/Jul/23 05:53,20/Aug/13 21:41,1.2.9,,,Legacy/Tools,,,0,,,,,"Currently scrub creates a snapshot at the beginning of the scrub. This causes the disk used to be doubled after the scrub. 

If the disk utilization is more than 50%, scrub wont work. It would be nice to have an overriding option to disable snapshot. Something like --no-snapshot.",,dbrosius,kohlisankalp,lyubent,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Aug/13 14:09;lyubent;5891.patch;https://issues.apache.org/jira/secure/attachment/12598762/5891.patch","20/Aug/13 19:46;lyubent;5891_cassandra-1.2.patch;https://issues.apache.org/jira/secure/attachment/12599016/5891_cassandra-1.2.patch",,,,,,,,,,,,,,,,,,,2.0,lyubent,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,343746,,,Tue Aug 20 21:41:43 UTC 2013,,,,,,,,,,"0|i1na3b:",344048,,,,,,,,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,,"19/Aug/13 14:09;lyubent;Added a -no-snapshot parameter to the ""nodetool scrub"" command. When supplied, the parameter tells scrub to skip snapshot creation and saves disk space:

The below is for the size of the var/lib/cassandra directory
./nodetool scrub with -no-snapshot: cassandra dir size before scrub: 4.1G, after scrub 4.1G
./nodetool scrub (without -no-snapshot): cassandra dir size before scrub: 4.1G, after scrub 7.1G;;;","20/Aug/13 04:25;dbrosius;patch seems fine, but does not apply cleanly to the cassandra-1.2 branch, can you please rebase?


nit: it seems awkward that the disableSnapshot parm is between keyspace and columnfamilies, (understood that varargs gets in the way) perhaps it should be first?;;;","20/Aug/13 19:46;lyubent;Rebased to cassandra-1.2 branch, switched order of params for StorageService#scrub so that disableSnapshot is first.;;;","20/Aug/13 21:41;dbrosius;thanks,

committed to cassandra-1.2 as commit dbb55ebd685ca36dc962e07d1a33b3354a1ce433;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
add tombstone metrics to cfstats or cfhistograms,CASSANDRA-5889,12663700,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mishail,jbellis,jbellis,14/Aug/13 17:45,16/Apr/19 09:32,14/Jul/23 05:53,08/Oct/13 21:28,1.2.11,2.0.2,,Legacy/Tools,,,0,,,,,/cc [~pmcfadin],,mishail,pmcfadin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Oct/13 19:36;mishail;cassandra-1.2-5889.patch;https://issues.apache.org/jira/secure/attachment/12607416/cassandra-1.2-5889.patch","08/Oct/13 05:32;mishail;cassandra-2.0-5889.patch;https://issues.apache.org/jira/secure/attachment/12607315/cassandra-2.0-5889.patch",,,,,,,,,,,,,,,,,,,2.0,mishail,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,343701,,,Wed Oct 09 17:04:19 UTC 2013,,,,,,,,,,"0|i1n9tr:",344005,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"07/Oct/13 03:09;jbellis;Want to take a stab at this, [~mishail]?;;;","07/Oct/13 03:21;mishail;[~jbellis] will do my best. ;;;","08/Oct/13 05:32;mishail;Add tombstones/live cells count and ratio to nodetool output;;;","08/Oct/13 18:53;jbellis;Sorry, realized we committed the histogram to 1.2 as well.  Can you post a patch for that branch too?;;;","08/Oct/13 19:36;mishail;Patch for 1.2;;;","08/Oct/13 21:28;mishail;https://github.com/apache/cassandra/commit/f31e399abe01c0e78f3b8289c562c2f279f04c2c;;;","09/Oct/13 17:04;jbellis;Yes. :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing column type from int to bigint or vice versa causes decoding errors.,CASSANDRA-5882,12663433,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jblangston@datastax.com,jblangston@datastax.com,13/Aug/13 14:22,16/Apr/19 09:32,14/Jul/23 05:53,26/Aug/13 16:01,1.2.9,,,,,,0,,,,,"cqlsh:dbsite> create table testint (id uuid, bestof bigint, primary key (id) );
cqlsh:dbsite> insert into testint (id, bestof ) values (49d30f84-a409-4433-ad60-eb9c1a06b7bb, 1376399966);
cqlsh:dbsite> insert into testint (id, bestof ) values (6cab4798-ad29-4419-bd59-308f9ec3bc44, 1376389800);
cqlsh:dbsite> insert into testint (id, bestof ) values (685bb9ff-a4fe-4e47-95eb-f6a353d9e179, 1376390400);
cqlsh:dbsite> insert into testint (id, bestof ) values (a848f832-5ded-4ef7-bf4b-7db561564c57, 1376391000);
cqlsh:dbsite> select * from testint ;
 id                                   | bestof
--------------------------------------+------------
 a848f832-5ded-4ef7-bf4b-7db561564c57 | 1376391000
 49d30f84-a409-4433-ad60-eb9c1a06b7bb | 1376399966
 6cab4798-ad29-4419-bd59-308f9ec3bc44 | 1376389800
 685bb9ff-a4fe-4e47-95eb-f6a353d9e179 | 1376390400

cqlsh:dbsite> alter table testint alter bestof TYPE int;
cqlsh:dbsite> select * from testint ;
 id                                   | bestof
--------------------------------------+-----------------------------
 a848f832-5ded-4ef7-bf4b-7db561564c57 |  '\x00\x00\x00\x00R\n\x0fX'
 49d30f84-a409-4433-ad60-eb9c1a06b7bb |     '\x00\x00\x00\x00R\n2^'
 6cab4798-ad29-4419-bd59-308f9ec3bc44 | '\x00\x00\x00\x00R\n\n\xa8'
 685bb9ff-a4fe-4e47-95eb-f6a353d9e179 | '\x00\x00\x00\x00R\n\r\x00'

Failed to decode value '\x00\x00\x00\x00R\n\x0fX' (for column 'bestof') as int: unpack requires a string argument of length 4
Failed to decode value '\x00\x00\x00\x00R\n2^' (for column 'bestof') as int: unpack requires a string argument of length 4
2 more decoding errors suppressed.


I realize that going from BIGINT to INT would cause overflow if a column contained a number larger than 2^31-1, it is at least technically possible to go in the other direction.  I also understand that rewriting all the data in the correct format would be a very expensive operation on a large column family, but if that's not something we want to allow we should explicitly disallow changing data types if the table has any rows.",,aleksey,carlyeks,jblangston@datastax.com,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Aug/13 14:11;slebresne;5882.txt;https://issues.apache.org/jira/secure/attachment/12599197/5882.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,343434,,,Mon Aug 26 16:01:12 UTC 2013,,,,,,,,,,"0|i1n86f:",343738,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"21/Aug/13 14:11;slebresne;The reason is that we currently don't validate anything when a non-PK column type is altered, which is probably a mistake (at least imo). There's an historical reason for that: thrift doesn't validate such change either. So in a way, that's not a new thing: if you change a validator for an incompatible one (which is the case here, int (Int32Type) is not able to decode most value of bigint (IntegerType)), you'll likely break your client code (which is the case btw, it's a cqlsh error here, not a server side one). 
 
So I would personally be in favor of just refusing that kind of change pure and simple. Attaching a simple patch to do that (for CQL3 only since Thou Shalt Not Modify Thrift).;;;","21/Aug/13 14:17;carlyeks;a couple of misspelled comments: s/let is fly/let it fly/; s/changet/change/

;;;","26/Aug/13 15:56;aleksey;+1;;;","26/Aug/13 16:01;slebresne;Alright, committed (with typo fixed). Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The Pig CqlStorage/AbstractCassandraStorage classes don't handle collection types,CASSANDRA-5867,12662857,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,jeromatron,jeromatron,09/Aug/13 14:04,16/Apr/19 09:32,14/Jul/23 05:53,10/Sep/13 18:47,1.2.10,2.0.1,,,,,2,pig,,,,"The CqlStorage class gets the Pig data type for values from the AbstractCassandraStorage class, in the getPigType method.  If it isn't a known data type, it makes the value into a ByteArray.  Currently there aren't any cases there for lists, maps, and sets.
https://github.com/apache/cassandra/blob/cassandra-1.2.8/src/java/org/apache/cassandra/hadoop/pig/AbstractCassandraStorage.java#L336

See this describe output from the grunt shell:

{code}
grunt> describe listdata ;                                        
listdata: {id: (name: chararray,value: int),alist: (name: chararray,value: bytearray),amap: (name: chararray,value: bytearray),aset: (name: chararray,value: bytearray)}
{code}

where the cql data structures had this schema:

{code}
CREATE TABLE alltypes (
  id int PRIMARY KEY,
  alist list<text>,
  amap map<text, text>,
  aset set<text>
{code}

It turns out that if you cast the map in grunt to a pig map, then it sort of works, but I don't think we should probably use a pig map.  Lists don't appear to work at all, as there is no Pig analogue.  I *think* you could probably just do a UDF to cast these things, but we already have all of the type information, so we just need to change them to tuples or bags or whatever.",,8forty@gmail.com,ahattrell,alex.holmansky,alexliu68,jeromatron,jjordan,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6073,,,,,"15/Aug/13 02:08;alexliu68;5867-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12598137/5867-1.2-branch.txt","15/Aug/13 16:51;alexliu68;5867-2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12598238/5867-2-1.2-branch.txt","19/Aug/13 20:14;alexliu68;5867-3-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12598812/5867-3-1.2-branch.txt","27/Aug/13 04:15;alexliu68;5867-4-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12600091/5867-4-1.2-branch.txt","28/Aug/13 23:56;alexliu68;5867-5-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12600509/5867-5-1.2-branch.txt","27/Aug/13 04:15;alexliu68;5867-bug-fix-filter-push-down-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12600090/5867-bug-fix-filter-push-down-1.2-branch.txt",,,,,,,,,,,,,,,6.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,342859,,,Tue Sep 10 18:47:28 UTC 2013,,,,,,,,,,"0|i1n4mn:",343163,,,,,,,,,brandon.williams,,brandon.williams,Normal,,1.2.7,,,,,,,,,,,,,,,,"09/Aug/13 14:26;brandon.williams;A tuple is the obvious choice, since a bag is cumbersome and there's no need to spill to disk.;;;","09/Aug/13 14:31;jeromatron;In the same vein as data types, as of PIG-2764 Pig has a BigInteger.  So once 0.12 is out and mainstream, we could look at using that instead of the Pig integer type to avoid overflow.  Just as a heads up.;;;","15/Aug/13 02:08;alexliu68;Patch on 1.2 branch is attached, which map ListType and SetType to tuple, MapType to map;;;","15/Aug/13 12:31;jeromatron;We may not want to use the MapType for maps.

In Pig, you can only have text based keys for maps.  From Programming Pig: ""A map in Pig is a chararray to data element mapping, where that element can be any Pig type, including a complex type.""

In Cassandra, you can have map keys of any type, like timestamp in the maps example on http://www.datastax.com/dev/blog/cql3_collections

So we may want to do a bag or tuple of tuples.;;;","15/Aug/13 16:50;alexliu68;The key of Cassandra map has been converted to string into Pig map. We need to decide whether use tuple of tuples vs map for Cassandra map type. Tuple of tuples is more general than map, and map is more specific. HBase uses map to map its row. So which one to use for map? Map vs Tuple of tuples?;;;","16/Aug/13 22:50;alexliu68;To store data to CQL3 table, we supports the following prepared statements

{code}
List
   e.g.
   UPDATE users SET top_places = ?
   UPDATE users SET top_places = [ 'rivendell', 'rohan' ] WHERE user_id = 'frodo';

   UPDATE users SET top_places = ? + top_places
   UPDATE users SET top_places = [ 'the shire' ] + top_places WHERE user_id = 'frodo';

   UPDATE users SET top_places = top_places - ?;
   UPDATE users SET top_places = top_places - ['riddermark'] WHERE user_id = 'frodo';

Set statements are similar to List

Map

  UPDATE users SET todo = ?
  UPDATE users
       SET todo = { '2012-9-24' : 'enter mordor',
                    '2012-10-2 12:00' : 'throw ring into mount doom' }
       WHERE user_id = 'frodo';


The following queries are handled as a regular value instead of tuples
   UPDATE users SET top_places[2] = ?
   UPDATE users SET top_places[2] = 'riddermark' WHERE user_id = 'frodo';

   DELETE top_places[3] FROM users;
   DELETE top_places[3] FROM users WHERE user_id = 'frodo';

   UPDATE users SET todo[?] = ?
   UPDATE users SET todo['2012-10-2 12:10'] = 'die' WHERE user_id = 'frodo';
{code}

The output schema for collections is as following

{code}
 (((name, value), (name, value)), (value ... value), (value...value)
 If a value of tuple (value...value) is a tuple of (inner_value ...inner_value) 
 and the first inner_value is the collection type. 
 it is either ""set"", ""list"" or ""map"".

 e.g. (value ... value) as (value1, value2, (set, riddermark, tom))
 map  (value1, value2, (map, (sfo, 12), (ny, 34))
{code}
;;;","19/Aug/13 20:14;alexliu68;5867-3-1.2-branch.txt is attached to support storing collections to Cassandra;;;","22/Aug/13 20:13;alex.holmansky;If the key of C* map is converted to string (chararray) in a Pig map on read, how will the keys be handled on write?  Will the strings be auto-converted to appropriate C* types?  In the past, I've seen issues with that - especially with timestamps and UUID values.;;;","22/Aug/13 20:49;alexliu68;The following is the data which will be auto converted to C* type, anything else will be bytes.

{code}
        if (o == null)
            return (ByteBuffer)o;
        if (o instanceof java.lang.String)
            return ByteBuffer.wrap(new DataByteArray((String)o).get());
        if (o instanceof Integer)
            return Int32Type.instance.decompose((Integer)o);
        if (o instanceof Long)
            return LongType.instance.decompose((Long)o);
        if (o instanceof Float)
            return FloatType.instance.decompose((Float)o);
        if (o instanceof Double)
            return DoubleType.instance.decompose((Double)o);
        if (o instanceof UUID)
            return ByteBuffer.wrap(UUIDGen.decompose((UUID) o));

        return ByteBuffer.wrap(((DataByteArray) o).get());
{code}

you need prepare the data for write different from read result.;;;","26/Aug/13 22:16;jbellis;Alex, is this ready for review?;;;","26/Aug/13 23:04;alexliu68;Yes, But I map the C* MapType to Pig map, we may need map it to tuples of tuples.;;;","26/Aug/13 23:05;alexliu68;I also has a few bug fixes related to CqlStorage, so if it's possible, I can add those fix to this patch as well.;;;","26/Aug/13 23:14;brandon.williams;Since pig's maps are limited to string keys, I think we'll need to use tuples. I'm ok with fixing bugs here, provided that's in a separate patch.;;;","27/Aug/13 04:22;alexliu68;First apply 5867-bug-fix-filter-push-down-1.2-branch.txt to fix issue with filter push down. (schema changes), then apply 5867-4-1.2-branch.txt for collection supports in CqlStorage which map SetType/ListType to tuple, MapType to tuple of tuples.

;;;","28/Aug/13 23:56;alexliu68;5867-5-1.2-branch.txt is attached to fix a bug related to writing map collection to C*. It should be applied after 5867-bug-fix-filter-push-down-1.2-branch.txt;;;","29/Aug/13 00:07;alexliu68;e.g.
{code}
   CREATE TABLE test(
     m text PRIMARY KEY,
     n map<text, text>
   ) 
   data format for insertion (((m,kk)),((map,(m,mm),(n,nn))))
   store recs into 'cql://test/test77?output_query=update+test.test+set+n+%3D+%3F' using CqlStorage();
   where output_query is url encoded
{code};;;","10/Sep/13 18:47;brandon.williams;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when you mistakenly set listen_address to 0.0.0.0,CASSANDRA-5865,12662741,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,mfiguiere,mfiguiere,08/Aug/13 23:41,16/Apr/19 09:32,14/Jul/23 05:53,25/Sep/13 15:10,1.2.11,,,,,,0,,,,,"It's clearly stated that setting {{listen_address}} to {{0.0.0.0}} is always wrong. But if you mistakenly do it anyway you end up with an NPE on 1.2.8 while it's not the case on 2.0.0-rc1. See bellow:

{code}
 INFO 16:34:43,598 JOINING: waiting for ring information
 INFO 16:34:44,505 Handshaking version with /127.0.0.1
 INFO 16:34:44,533 Handshaking version with /0.0.0.0
 INFO 16:35:13,626 JOINING: schema complete, ready to bootstrap
 INFO 16:35:13,631 JOINING: getting bootstrap token
ERROR 16:35:13,633 Exception encountered during startup
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:666)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:154)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:135)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapTokens(BootStrapper.java:115)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:666)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:554)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:348)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
Exception encountered during startup: No other nodes seen!  Unable to bootstrap.If you intended to start a single-node cluster, you should make sure your broadcast_address (or listen_address) is listed as a seed.  Otherwise, you need to determine why the seed being contacted has no knowledge of the rest of the cluster.  Usually, this can be solved by giving all nodes the same seed list.
ERROR 16:35:13,668 Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:321)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:370)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:88)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:519)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:724)
{code}",Cassandra 1.2.8,dbrosius,mfiguiere,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Sep/13 19:02;brandon.williams;5865.txt;https://issues.apache.org/jira/secure/attachment/12603079/5865.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,342743,,,Wed Sep 25 15:10:05 UTC 2013,,,,,,,,,,"0|i1n3wv:",343047,,,,,,,,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,,"09/Aug/13 14:48;brandon.williams;You basically can never set listen_address to an unspecified IP address, because gossip needs a way to communicate with the node.;;;","09/Aug/13 15:01;jbellis;Shouldn't we make it reject invalid addresses instead of starting up and NPEing later?;;;","09/Aug/13 20:05;brandon.williams;We can do that, though the only invalid IP is 0.0.0.0;;;","11/Aug/13 06:32;dbrosius;255.*.*.* and multicast addresses are bad as well, altho probably unlikely.;;;","13/Sep/13 19:02;brandon.williams;Trivial patch to throw when listen_address is 0.0.0.0.;;;","24/Sep/13 13:37;jbellis;Any reason not to check the cases Dave mentioned?;;;","24/Sep/13 15:16;brandon.williams;There are just too many possibilities, if someone screws up their subnetting.  192.168.0.255 would be invalid in a /24 due to being the broadcast address, but so would 192.168.0.3 in a /30.  255.* is just a special case of this problem.  I don't think anyone is really going to do this, or use multicast, so it doesn't seem worth bothering with.;;;","25/Sep/13 03:20;dbrosius;+1;;;","25/Sep/13 15:10;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't return internal StreamState objects from streaming mbeans,CASSANDRA-5859,12662499,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,nickmbailey,nickmbailey,07/Aug/13 21:37,16/Apr/19 09:32,14/Jul/23 05:53,12/Aug/13 16:15,2.0.0,,,,,,0,streaming,,,,The stream manager mbean returns StreamState objects. We want to avoid returning internal C* objects over jmx. We should switch to a map or something similar that can represent streaming state.,,jeromatron,jjordan,nickmbailey,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/13 19:40;yukim;5859.txt;https://issues.apache.org/jira/secure/attachment/12596919/5859.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,342502,,,Mon Aug 12 16:15:05 UTC 2013,,,,,,,,,,"0|i1n2fz:",342807,,,,,,,,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,,"08/Aug/13 19:40;yukim;Initial patch attached. (Also on github: https://github.com/yukim/cassandra/commits/5859)

Since StreamState is complex object, I used JMX's [CompositeData|http://docs.oracle.com/javase/7/docs/api/javax/management/openmbean/CompositeData.html] to convert StreamState to be transferred over JMX.
Patch also adds support for exposing JMX notification support for stream events to StreamManagerMBean, so one can just monitor start/progress/completion of streaming.;;;","09/Aug/13 19:43;thobbs;In {{ProgressInfoCompositeData}}, the direction is being reported as a single byte, 0 or 1, which can't be interpreted without the internal enum.  Just using a string ""in"" or ""out"" or a boolean with name ""outbound"" would be fine.

There are several alignment issues around one-arg-per-line lists in all of the new streaming.management classes.  For example:
{noformat}
            COMPOSITE_TYPE = new CompositeType(ProgressInfo.class.getName(),
                                                      ""ProgressInfo"",
                                                      ITEM_NAMES,
                                                      ITEM_DESCS,
                                                      ITEM_TYPES);
{noformat}

Other than those two minor issues, +1.;;;","12/Aug/13 16:15;yukim;Committed with above fix(string instead of byte/coding style).
Thanks for review!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NumberFormatException during decommission,CASSANDRA-5857,12662442,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,07/Aug/13 17:36,16/Apr/19 09:32,14/Jul/23 05:53,08/Aug/13 18:05,1.2.9,,,,,,0,,,,,"We half-fixed this in CASSANDRA-5696, but unfortunately StorageService is still looking at the token to get the expiretime in some cases.

{noformat}
java.lang.NumberFormatException: For input string: ""113427455640312821154458202477256070484""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:65)
        at java.lang.Long.parseLong(Long.java:444)
        at java.lang.Long.parseLong(Long.java:483)
        at org.apache.cassandra.service.StorageService.extractExpireTime(StorageService.java:1660)
        at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:1515)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1234)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:953)
        at org.apache.cassandra.gms.Gossiper.applyNewStates(Gossiper.java:944)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:902)
        at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:50)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{noformat}",,enigmacurry,jasobrown,jjordan,mheffner,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/13 17:30;brandon.williams;5857.txt;https://issues.apache.org/jira/secure/attachment/12596885/5857.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,342446,,,Fri Aug 23 21:22:18 UTC 2013,,,,,,,,,,"0|i1n23j:",342751,1.2.8,,,,,,,,jasobrown,,jasobrown,Low,,1.2.0,,,,,,,,,,,,,,,,"08/Aug/13 14:25;enigmacurry;-OK, so this is upgrade related? How do I reproduce this? I'm wondering what scenarios we need to add to the upgrade_through_versions dtest to avoid this error again.-

Nevermind, [~jjordan] set me straight that this is purely decomission.;;;","08/Aug/13 17:30;brandon.williams;Patch to have SS look at the correct piece for the expire time.;;;","08/Aug/13 17:56;jasobrown;lgtm;;;","08/Aug/13 18:05;brandon.williams;Committed.;;;","23/Aug/13 21:22;mheffner;Should this be a higher priority issue for release planning purposes? We ran into this issue when decommissioning a node from a 1.2.8 ring. Actually getting that node to finally leave was quite a task:

1) Ran decommision, failed with above error after streaming all data.
2) Tried to use nodetool removenode, node was in 'UL' state so wouldn't run.
3) Shutdown cassandra to try and move node to a DOWN state, node was still stuck in 'UL' state.
4) Tried a unsafeAssassinateEndpoint operation. That failed with same error as above and node was not removed. It did however move the node from UL -> DL.
5) Reran removenode as node was now in a DOWN state. After restreaming, node was finally removed.

;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE in ArrayBackedSortedColumns,CASSANDRA-5856,12662410,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,aleksey,brandon.williams,brandon.williams,07/Aug/13 15:47,16/Apr/19 09:32,14/Jul/23 05:53,17/Aug/13 15:53,1.2.9,,,,,,1,,,,,"{noformat}
ERROR [ReadStage:3] 2013-08-07 06:58:21,485 CassandraDaemon.java (line 192) Exception in thread Thread[ReadStage:3,5,main]
java.lang.AssertionError: Added column does not sort as the last column
    at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:131)
    at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:119)
    at org.apache.cassandra.db.AbstractColumnContainer.addColumn(AbstractColumnContainer.java:114)
    at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:171)
    at org.apache.cassandra.db.filter.QueryFilter.collateColumns(QueryFilter.java:136)
    at org.apache.cassandra.db.filter.QueryFilter.collateOnDiskAtom(QueryFilter.java:84)
    at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:291)
    at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
    at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
    at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1213)
    at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1125)
    at org.apache.cassandra.db.Table.getRow(Table.java:347)
    at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:70)
    at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1047)
    at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1593)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
{noformat}

test_column_index_stress in wide_rows_test will reproduce this within ~20 runs and bisect strongly points to a regression in CASSANDRA-5762",,aleksey,dimetrio,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Aug/13 05:55;jbellis;5856-assert.txt;https://issues.apache.org/jira/secure/attachment/12597413/5856-assert.txt","16/Aug/13 00:47;aleksey;5856.txt;https://issues.apache.org/jira/secure/attachment/12598335/5856.txt",,,,,,,,,,,,,,,,,,,2.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,342414,,,Sat Aug 17 15:53:35 UTC 2013,,,,,,,,,,"0|i1n1wf:",342719,,,,,,,,,jbellis,,jbellis,Critical,,1.2.7,,,,,,,,,,,,,,,,"12/Aug/13 04:46;jbellis;First guess: something's getting confused about reversed-ness.  Is that part of the test in question?;;;","12/Aug/13 05:07;jbellis;CASSANDRA-5762 is probably causing this indirectly by forcing more code to go through the slice path, rather than introducing a bug in collation directly.;;;","12/Aug/13 05:34;jbellis;bq. First guess: something's getting confused about reversed-ness

Second guess: there's a bug in ISR's code for reversed fetches (CASSANDRA-5712).

Might need to make it print out the cells in collectReducedColumns to see...;;;","12/Aug/13 05:49;jbellis;If there's a bug in ISR it's probably older than 5712.  {{prefetched}} makes my head hurt.;;;","12/Aug/13 05:55;jbellis;Patch to get more information from the assert.

(NB: the existing error message indicates that this is NOT a reversed slice.  So, beats the hell out of me how this could be erroring out.  Hence, the need for more information.);;;","12/Aug/13 10:32;aleksey;I'll get to it today/tomorrow. Yeah. 5762 is very unlikely to be the cause of it.;;;","13/Aug/13 16:00;jbellis;Two of the patched assertion failures:

{noformat}
java.lang.AssertionError: Added cell val31254: does not sort as the last; contents are val2960::false:0@1376407099059000,val2960:value:false:4@1376407099059000,val31254::false:0@1376407110826001,val31254:value:false:4@1376407110826001, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)
{noformat}

{noformat}
java.lang.AssertionError: Added cell val54806: does not sort as the last; contents are val22917::false:0@1376408295872003,val22917:value:false:4@1376408295872003,val54806::false:0@1376408305568001,val54806:value:false:4@1376408305568001, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type
{noformat}
;;;","13/Aug/13 16:05;brandon.williams;Here's the most concise one I've seen:

{noformat}
java.lang.AssertionError: Added cell val11599: does not sort as the last; contents are val11599::false:0@1376409359730001,val11599:value:false:4@1376409359730001, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)
{noformat};;;","15/Aug/13 18:10;brandon.williams;So, this is a lot simpler than the test makes it look.  It's caused simple by asking for the same column by name twice:

{noformat}
cqlsh> create keyspace foo WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh> use foo;
cqlsh:foo> create table bar (row varchar, name varchar, value int, PRIMARY KEY (row, name));
cqlsh:foo> update bar set value = 1 WHERE row = 'baz' AND name = 'qux';
cqlsh:foo> select value from bar where row='baz' AND name in ('qux', 'qux');
Request did not complete within rpc_timeout.
{noformat}

Results in:
{noformat}
java.lang.AssertionError: Added cell qux: does not sort as the last; contents are qux::false:0@1376590034567000,qux:value:false:4@1376590034567000, with comparator org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)
{noformat};;;","17/Aug/13 15:23;jbellis;+1

Nit: can we make buildBound return Collection to avoid the extra copy?;;;","17/Aug/13 15:53;aleksey;Committed, thanks.

bq. Nit: can we make buildBound return Collection to avoid the extra copy?

No, unfortunately we can't. Well, we kinda can, but we'd then have to create an extra iterator for getKeyBound (won't be able to just .get(0)) and two extra iterators in makeFilter (again, won't be able to just .get(i) - and we need to iterate over startBounds and endBounds simultaneously to build the ColumnSlices).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Scrub does not understand compound primary key created in CQL 3 beta,CASSANDRA-5855,12662401,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,jblangston@datastax.com,jblangston@datastax.com,07/Aug/13 15:02,16/Apr/19 09:32,14/Jul/23 05:53,08/Aug/13 22:10,1.2.9,2.0.0,,Legacy/Tools,,,0,,,,,"We have a customer who was using the beta version of CQL 3 in DSE 3.0 which includes Cassandra 1.1.9 plus patches backported from later versions.  They've now upgraded to DSE 3.1, which includes Cassandra 1.2.6 plus patches.

When restarting for the first time after running upgradesstables, they noticed the following error in the log:

{noformat}
Thread[SSTableBatchOpen:2,5,main]
java.lang.AssertionError
        at org.apache.cassandra.utils.ByteBufferUtil.readBytes(ByteBufferUtil.java:401)
        at org.apache.cassandra.io.sstable.IndexSummary$IndexSummarySerializer.deserialize(IndexSummary.java:124)
        at org.apache.cassandra.io.sstable.SSTableReader.loadSummary(SSTableReader.java:426)
        at org.apache.cassandra.io.sstable.SSTableReader.load(SSTableReader.java:360)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:201)
        at org.apache.cassandra.io.sstable.SSTableReader.open(SSTableReader.java:154)
        at org.apache.cassandra.io.sstable.SSTableReader$1.run(SSTableReader.java:241)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

This error was also reported on CASSANDRA-5703.  The comments suggested it was caused by an empty row key, so I had them run scrub on it.  When they did, scrub reported the following warning almost 4 million times:

{noformat}
 WARN [CompactionExecutor:27] 2013-08-02 10:13:13,041 OutputHandler.java (line 52) Row at 530332255 is unreadable; skipping to next
 WARN [CompactionExecutor:27] 2013-08-02 10:13:13,041 OutputHandler.java (line 57) Non-fatal error reading row (stacktrace follows)
java.lang.RuntimeException: Error validating row DecoratedKey(139154446688383793922009760478335751546, 735fc9da503b11e2844b123140ff209f)
 at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:243)
 at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:114)
 at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:98)
 at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:160)
 at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:166)
 at org.apache.cassandra.db.compaction.Scrubber.scrub(Scrubber.java:173)
 at org.apache.cassandra.db.compaction.CompactionManager.scrubOne(CompactionManager.java:529)
 at org.apache.cassandra.db.compaction.CompactionManager.doScrub(CompactionManager.java:518)
 at org.apache.cassandra.db.compaction.CompactionManager.access$400(CompactionManager.java:73)
 at org.apache.cassandra.db.compaction.CompactionManager$3.perform(CompactionManager.java:283)
 at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:253)
 at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
 at java.util.concurrent.FutureTask.run(FutureTask.java:138)
 at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
 at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
 at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.db.marshal.MarshalException: String didn't validate.
 at org.apache.cassandra.db.marshal.UTF8Type.validate(UTF8Type.java:66)
 at org.apache.cassandra.db.Column.validateFields(Column.java:292)
 at org.apache.cassandra.db.ColumnFamily.validateColumnFields(ColumnFamily.java:382)
 at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:239)
 ... 15 more
{noformat}

The customer did some testing and they've determined that the issue only exists when taking a Cassandra 1.1 sstable with compound primary keys and running scrub on it in either Cassandra 1.1 or 1.2.

It appears that the scrub does not understand the 1.1 compound primary key so is invalidating the row.

The customer provided a cassandra data directory that's from DSE 3.0. Running ""nodetool scrub"" in either DSE 3.0 or 3.1 generates all sorts of exceptions.

If you fire up cassandra before running the scrub, running this query:

{noformat}
select count(*) from b_projectsubscription_project;
{noformat}

will return 88.

After the scrub, it returns 0.

When I discussed this with Alexey Yeschenko, he said that if he recalls correctly, the beta didn't have row markers, and did not use a composite comparator for simple primary keys. Whereas CQL3 final used CompositeType(UTF8Type), the beta would just use UTF8Type. I asked him if this could cause these errors, and he said he didn't think so because after upgrading your schema created under the beta would still have UTF8Type, so Cassandra would know how to handle it correctly. 

Based on the customer's investigation, it sounds like this may be true of the normal read/write path but not for scrub. However, given the error that occurred at startup, this may be causing some issues aside from just scrub.  My theory is that scrub is looking at what's just a UTF8 string and trying to interpret the first few bytes as the sentinels for a composite type.  When it then tries to interpret the remaining bytes, if only part of a multi-byte UTF8 character was in the remaining byte array, it might cause the UTF8 validation errors above.",,jblangston@datastax.com,rcoli,thobbs,xcbsmith,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Aug/13 19:39;thobbs;0001-Correctly-validate-sparse-composite-columns.patch;https://issues.apache.org/jira/secure/attachment/12596918/0001-Correctly-validate-sparse-composite-columns.patch","08/Aug/13 21:16;jbellis;5855-followup.txt;https://issues.apache.org/jira/secure/attachment/12596943/5855-followup.txt",,,,,,,,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,342405,,,Thu Aug 08 22:10:55 UTC 2013,,,,,,,,,,"0|i1n1uf:",342710,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"07/Aug/13 17:02;jbellis;Can you get us instructions to reproduce along these lines?

# Start C* 1.1
# CREATE TABLE ...
# INSERT ...
# Start C* 1.2
# Scrub and watch the error;;;","07/Aug/13 21:30;jblangston@datastax.com;The error will occur whether scrub is run under 1.1 or 1.2.

1) Start Cassandra 1.1.9

2) Run the following in cqlsh -3

CREATE TABLE b_projectsubscription_project (
  project uuid,
  uuid uuid,
  created_on bigint,
  deleted boolean,
  hsn_deleted boolean,
  send_notifications boolean,
  user uuid,
  PRIMARY KEY (project, uuid)
);

insert into b_projectsubscription_project (project,uuid,created_on,deleted,hsn_deleted,send_notifications,user) values (d7ea9931-3e70-4ec3-b7a8-a7a7535473f2,00388f1e-5b59-41d8-92c7-16b6cedf25a5,1375831887947529,'False','False','True',d3151d4d-873c-4c8e-9445-dabd7f0660ef);

4) run nodetool flush
5) optional: upgrade to Cassandra 1.2.6
6) run nodetool scrub
7) check system.log for ""RuntimeException: Error validating row""
8) in cqlsh -3 run 'select * from b_projectsubscription_project;' and notice the row is gone.;;;","07/Aug/13 21:52;jblangston@datastax.com;One more observation: the bug may only happen if the pk fields are uuid. I tried to reproduce it before with pk fields that are text and wasn't able to.;;;","07/Aug/13 23:07;thobbs;At least with a 1.1.9 scrub, it's trying to look up the column value validator with the full composite column name, but CfMetadata is expecting to only see the last component (e.g. ""created_on"").  So, it ends up using the default validator, which is UTF8Type, to try to validate all of the column values.;;;","07/Aug/13 23:34;xcbsmith;Tyler, that makes sense... does that mean if you kept the composite key fields all the same type, it would work properly?

It's kind of weird that you'd still see the problem with the table after it had been upgraded to 1.2. Really, after an upgrade, I would expect scrubs of a table to behave *exactly the same* as if the table had been originally created in 1.2. Is the upgrade process perhaps less complete than I imagined?;;;","08/Aug/13 02:43;jbellis;Usually upgrades only fix bugs that we knew we had. :);;;","08/Aug/13 06:53;xcbsmith;Hehe.

I figured the idea would be to make it the _same_ as the same column family created with the new tool. So for example, if the problem was the missing markers, while a bug might exist with the 1.1.9 scrub tool with 1.1.9 column family, it'd be a different story after upgrade. Since the bug shows up you upgrade the column family, but not if you always created it in 1.2, that would imply the markers still weren't there after the upgrade, which you'd think would be noticed by someone just checking to make sure upgrade actually upgraded, even if nobody realized this tickled an actual bug in scrub.;;;","08/Aug/13 12:36;jbellis;The problem is that it can't read the existing data file.  If it could, it would indeed write out a 1.2-format file.;;;","08/Aug/13 16:20;thobbs;bq. Tyler, that makes sense... does that mean if you kept the composite key fields all the same type, it would work properly?

If all of the column values were UTF8Type, it would work properly, but that's definitely not a feasible workaround.  I'm looking into a proper fix, which would allow scrub to lookup the correct column validators.;;;","08/Aug/13 19:39;thobbs;Patch 0001 should apply to 1.1 or 1.2.  It's not the most general technique, so I welcome any suggestions, but it does fix this particular issue.  If a sparse composite schema is being used, only the last composite component is used to try to look up the column validator.

I'll also note that this code path is currently only used by scrub, so it should be safe to test.;;;","08/Aug/13 20:20;jbellis;It looks to me like we actually need to handle 3 cases:

# classic one-cell-per-column, no composites; the existing code
# CQL-style one-cell-per-column; the code you added
# COMPACT style, multiple columns in one cell; need to split them up and check each value

Nit: prefer assigning variables only once, e.g. we could write the validationName code [without addressing case 3 above] as

{code}
        ByteBuffer validationName;
        if (cfdef.isComposite && !cfdef.isCompact)
        {
            AbstractCompositeType comparator = (AbstractCompositeType) metadata.comparator;
            List<AbstractCompositeType.CompositeComponent> components = comparator.deconstruct(name);
            validationName = components.get(components.size() - 1).value;
        }
        else
        {
            validationName = name;
        }
{code}
;;;","08/Aug/13 20:21;jbellis;Actually, COMPACT case is handled by validateName so we're good.  I'll fix the nit and commit.;;;","08/Aug/13 21:16;jbellis;Attached followup patch to make it collection-aware.;;;","08/Aug/13 21:31;thobbs;+1 on 5585-followup.txt;;;","08/Aug/13 22:10;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
json2sstable breaks on data exported from sstable2json.,CASSANDRA-5852,12662160,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lyubent,enigmacurry,enigmacurry,06/Aug/13 16:58,16/Apr/19 09:32,14/Jul/23 05:53,19/Sep/13 10:00,1.2.10,2.0.1,,Legacy/Tools,,,0,,,,,"Attached is a JSON formatted sstable generated by sstable2json.

This file cannot be loaded back into Cassandra via json2sstable; it outputs this error:

{code}
Counting keys to import, please wait... (NOTE: to skip this use -n <num_keys>)
Importing 16 keys...
java.lang.NumberFormatException: Non-hex characters in value6
	at org.apache.cassandra.utils.Hex.hexToBytes(Hex.java:60)
	at org.apache.cassandra.utils.ByteBufferUtil.hexToBytes(ByteBufferUtil.java:503)
	at org.apache.cassandra.tools.SSTableImport.stringAsType(SSTableImport.java:578)
	at org.apache.cassandra.tools.SSTableImport.access$000(SSTableImport.java:59)
	at org.apache.cassandra.tools.SSTableImport$JsonColumn.<init>(SSTableImport.java:154)
	at org.apache.cassandra.tools.SSTableImport.addColumnsToCF(SSTableImport.java:231)
	at org.apache.cassandra.tools.SSTableImport.addToStandardCF(SSTableImport.java:214)
	at org.apache.cassandra.tools.SSTableImport.importSorted(SSTableImport.java:432)
	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:319)
	at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:543)
ERROR: Non-hex characters in value6
{code}

Steps to reproduce:

{code}
$ ccm create -v git:trunk test-json-import
Fetching Cassandra updates...
Current cluster is now: test-json-import
$ ccm populate -n 1
$ ccm start
$ ccm node1 cqlsh
Connected to test-json-import at 127.0.0.1:9160.
[cqlsh 4.0.0 | Cassandra 2.0.0-rc1-SNAPSHOT | CQL spec 3.1.0 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh> CREATE TABLE test.test (key varchar PRIMARY KEY, value varchar);
cqlsh> INSERT INTO test.test (key, value) VALUES ('ryan', 'ryan');
cqlsh> 
$ ccm node1 flush
$ ccm stop
$ ~/.ccm/test-json-import/node1/bin/json2sstable -s -K test -c test ~/Downloads/import_error/r.json ~/.ccm/test-json-import/node1/data/test/test/test-test-ja-1-Data.db 
{code}",,enigmacurry,lyubent,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5853,,,,,,,,,,,,,,,,,,,,,,,,"18/Sep/13 10:27;lyubent;5852_cassandra-1.2.patch;https://issues.apache.org/jira/secure/attachment/12603805/5852_cassandra-1.2.patch","18/Sep/13 15:09;lyubent;5852_cassandra-1.2_v2.patch;https://issues.apache.org/jira/secure/attachment/12603842/5852_cassandra-1.2_v2.patch","18/Sep/13 16:34;lyubent;5852_cassandra-1.2_v3.patch;https://issues.apache.org/jira/secure/attachment/12603858/5852_cassandra-1.2_v3.patch","18/Sep/13 10:45;lyubent;5852_cassandra-2.0.patch;https://issues.apache.org/jira/secure/attachment/12603806/5852_cassandra-2.0.patch","18/Sep/13 14:10;lyubent;5852_cassandra-2.0_v2.patch;https://issues.apache.org/jira/secure/attachment/12603830/5852_cassandra-2.0_v2.patch","06/Aug/13 16:59;enigmacurry;r.json;https://issues.apache.org/jira/secure/attachment/12596369/r.json",,,,,,,,,,,,,,,6.0,lyubent,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,342164,,,Thu Sep 19 10:00:26 UTC 2013,,,,,,,,,,"0|i1n0d3:",342469,2.1 rc3,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,,"01/Sep/13 12:58;lyubent;There are two problems here.
* First the json has an empty column that causes empty values to be added to the sstable [gist|https://gist.github.com/lyubent/6404249] 
* Second, the key is stored as hex which is correct, but the values are stored as strings, should we change this in sstable2json (when we export the data) or should the import tool (json2sstable) just allow an option to import from string values and hex values?;;;","01/Sep/13 13:05;jbellis;sstable2json should convert values to be human-readable.  Blob type should be hex but others should not.;;;","17/Sep/13 21:52;lyubent;Changed the lookup for comparator to use CFMetaData#getColumnDefinition when it isn't null, otherwise the function falls back to the previously used CFMetaData#getValueValidator. Updated AbstractCompositeType#fromString to interpret empty cell names """" as cql3 row markers.;;;","17/Sep/13 22:30;jbellis;I assume this is a problem in 1.2 as well?  If so we should commit there too.;;;","18/Sep/13 10:45;lyubent;+1 Added patch for C* 1.2 ;;;","18/Sep/13 11:37;slebresne;Looking closer at this, I don't think we can change AbstractCompositeType.fromString() like that. The problem is that ACT.getString() will return an empty string in 2 different cases: if its argument is one component, but an empty one, and if it's argument is empty. Which means ACT.fromString() is basically stuck at picking one interpretation over the other and it currently picks ""completely empty"" currently.

Now, the problem is that there is cases where we do want ACT.fromString("""") to return an empty byte buffer and that's when you do a select query with CQL2. In that case, an empty byte buffer is ok (basically empty cell name are not ok, but empty is allowed as the bound of slices) and probably almost always what you want.

In sstable2json, that's the contrary, you'd want fromString("""") to always return a one empty component buffer, because a stored on disk cell name cannot be empty.

So anyway, just changing ACT.fromString("""") will break CQL2 and I'd rather avoid that. One possible fix would be to change getString() so it generates something different than """" when given a one empty component name and make fromString understand that. But maybe it's just not worth the trouble and we should just special case then handling of """" in SSTableImport directly, before passing it to ACT.fromString.

Other than that, on the patch itself, the corret way to get the type for the value would be to use: meta.getValueValidator(meta.getColumnDefinitionFromColumnName(...))
;;;","18/Sep/13 14:10;lyubent;I agree with special casing being simpler. Switched to using CFMetaData#getValueValidator#getColumnDefinitionFromColumnName for retrieving column types and tweaked CFMetaData#getColumnDefinitionFromColumnName to avoid an [ArrayIndexOutOfBoundsException ex|https://gist.github.com/lyubent/6609670];;;","18/Sep/13 16:15;slebresne;You're right that getColumnDefinitionFromColumnName doesn't always work, but in fact to be more reliable it shouldn't assume anything on the number of components after split. So I'd changed it to something like:
{noformat}
ByteBuffer[] components = composite.split(columnName);
for (ColumnDefinition def : column_metadata.values())
{
    ByteBuffer toCompare;
    if (def.componentIndex == null)
    {
        toCompare = columnName;
    }
    else
    {
        if (def.componentIndex >= components.length)
            break;

        toCompare = components[def.componentIndex];
    }
    if (def.name.equals(toCompare))
        return def;
}
{noformat}

Other nits:
* the patch adds a println, not sure that was the intent.
* getValueValidator actually knows how to handle null, so we don't need to check the result of getColumnDefinitionFromColumnName first.

As a side note, it's ok to only provide a 1.2 patch (unless the patch for 2.0 has important differences that are related to the ticket, which is not the case here), I'll fix the 2.0 specificities during merge.
;;;","18/Sep/13 16:34;lyubent;Added 5852_cassandra-1.2_v3.patch;;;","19/Sep/13 10:00;slebresne;+1, committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix 2i on composite components omissions,CASSANDRA-5851,12662147,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,06/Aug/13 16:03,16/Apr/19 09:32,14/Jul/23 05:53,12/Aug/13 10:58,2.0.0,,,,,,0,,,,,"There some edge-cases, not covered by CASSANDRA-5125, the attached patch fixes those:

(Assuming CREATE TABLE test (pk0 int, pk1 int, ck0 int, ck1 int, val int, PRIMARY KEY ((pk0, pk1), ck0, ck1)))

- could not create a 2i on the first part of a composite partition key (pk0)
- if created, it couldn't work because of getKeyBounds() returning non-empty bounds
- could create an index on the first clustering key column (ck0), but it would never actually be triggered on reads
- queries like SELECT * FROM test WHERE pk0 = x AND pk1 = y AND ck1 = z would throw an exception because COCK.makeIndexColumnNameBuilder() couldn't handle empty provided columnName
- cqlsh could not describe any of these indexes because it was taking column aliases and key aliases from schema_columnfamilies and not reading them directly from schema_columns (had to do the related refactoring).",,aleksey,duyleekun,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5854,,,,,,,,,,,,,,,,,,,,"07/Aug/13 21:43;aleksey;5851-extra.txt;https://issues.apache.org/jira/secure/attachment/12596720/5851-extra.txt","06/Aug/13 16:04;aleksey;5851.txt;https://issues.apache.org/jira/secure/attachment/12596362/5851.txt",,,,,,,,,,,,,,,,,,,2.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,342151,,,Mon Aug 12 10:58:32 UTC 2013,,,,,,,,,,"0|i1n0a7:",342456,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"07/Aug/13 05:27;duyleekun;I got this when I query on 2 indexed column. Is this expected?

SELECT * FROM gurugara.outgoing_edge WHERE source_id = 3 AND dest_id = 1 AND type = 'meh'  ALLOW FILTERING;
`dest_id` and `type` are secondary indexed column


ERROR 07:23:13,702 Exception in thread Thread[ReadStage:4,5,main]
java.lang.RuntimeException: java.lang.RuntimeException: Unable to search across multiple secondary index types
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1867)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.RuntimeException: Unable to search across multiple secondary index types
	at org.apache.cassandra.db.index.SecondaryIndexManager.search(SecondaryIndexManager.java:523)
	at org.apache.cassandra.db.ColumnFamilyStore.search(ColumnFamilyStore.java:1627)
	at org.apache.cassandra.db.RangeSliceCommand.executeLocally(RangeSliceCommand.java:135)
	at org.apache.cassandra.service.StorageProxy$LocalRangeSliceRunnable.runMayThrow(StorageProxy.java:1358)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1863)
	... 3 more
;;;","07/Aug/13 05:31;aleksey;bq. I got this when I query on 2 indexed column. Is this expected?

Unfortunately, yes.;;;","07/Aug/13 21:44;aleksey;Attached 5851-extra.txt that handles non-complete column names in CIOCK.makeIndexColumnNameBuilder() when used for searches.;;;","08/Aug/13 03:07;jbellis;Can you add a unit test?;;;","08/Aug/13 23:49;aleksey;bq. Can you add a unit test?

https://github.com/riptano/cassandra-dtest/commit/26f9cc6ff5e6d95d44784f034a3d8ed974f415b3;;;","11/Aug/13 05:11;jbellis;+1;;;","12/Aug/13 10:58;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support thrift tables in Pig CqlStorage,CASSANDRA-5847,12661940,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,05/Aug/13 19:03,16/Apr/19 09:32,14/Jul/23 05:53,11/Sep/13 15:20,1.2.10,2.0.1,,,,,0,,,,,This is fix for Pig side of CASSANDRA-5752,,alexliu68,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Sep/13 21:13;alexliu68;001_5847_patch.txt;https://issues.apache.org/jira/secure/attachment/12602422/001_5847_patch.txt","10/Sep/13 21:13;alexliu68;002_5847_patch.txt;https://issues.apache.org/jira/secure/attachment/12602423/002_5847_patch.txt",,,,,,,,,,,,,,,,,,,2.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,341944,,,Wed Sep 11 15:20:00 UTC 2013,,,,,,,,,,"0|i1mz0f:",342250,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"05/Aug/13 19:07;alexliu68;001 patch fixes counter issue.
002 patch to support thrift tables in CqlStorage;;;","11/Sep/13 15:20;brandon.williams;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updating primary key only fails,CASSANDRA-5846,12661904,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,brandon.williams,brandon.williams,05/Aug/13 15:27,16/Apr/19 09:32,14/Jul/23 05:53,05/Aug/13 16:18,,,,,,,0,,,,,"""UPDATE test SET PRIMARY KEY WHERE k = 0"" is now invalid syntax, but shouldn't be.  This bisected to CASSANDRA-5125.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,341908,,,Mon Aug 05 16:18:57 UTC 2013,,,,,,,,,,"0|i1mysf:",342214,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"05/Aug/13 16:18;aleksey;Removed the dtest in question.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicate classes in Cassandra-all package.,CASSANDRA-5833,12660810,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,dbrosius,samschumer,samschumer,30/Jul/13 23:08,16/Apr/19 09:32,14/Jul/23 05:53,14/Mar/14 04:45,2.0.7,,,Legacy/CQL,Packaging,,0,maven,,,,"As of Cassandra-All version 1.1.6 the classes org.apache.cassandra.thrift.ITransportFactory and org.apache.cassandra.thrift.TFramedTransportFactory are located in both the cassandra-thrift and the cassandra-all Maven JARS, and caasandra-thrift is imported by cassandra-all POM. This makes the cassandra-all package unbuildable when using the duplicate-finder Maven extension. The files were originally copied over due to [CASSANDRA-4668|https://issues.apache.org/jira/browse/CASSANDRA-4668]. All versions since have failed to build when using this maven extension.",,__tango,dbrosius,samschumer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Mar/14 00:36;dbrosius;5833.txt;https://issues.apache.org/jira/secure/attachment/12634597/5833.txt",,,,,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340999,,,Fri Mar 14 04:45:53 UTC 2014,,,,,,,,,,"0|i1mt93:",341317,1.2.7,,,,,,,,jbellis,,jbellis,Normal,,1.1.6,,,,,,,,,,,,,,,,"31/Jul/13 13:01;brandon.williams;It seems like the easiest thing to do would be not use the extension, or configure it to ignore cassandra-thrift.;;;","31/Jul/13 13:10;jbellis;bq. cassandra-thrift is imported by cassandra-all POM

that also sounds like a problem to me.;;;","31/Jul/13 17:20;__tango;Yes, the fact that cassandra-thrift imports cassandra-thrift means that you can't get away from this problem. Disabling the duplicate-finder extension is a sub-optimal way to fix the problem as having the exact same class in two separate jar files can lead to very odd bugs later in life if the files ever diverge.  ;;;","13/Mar/14 22:03;jbellis;Is this still a problem, Dave?;;;","14/Mar/14 00:23;dbrosius;The problem still exists on trunk. It seems we should remove those two classes from cassandra-all. I believe those are the only classes in conflict.;;;","14/Mar/14 00:36;dbrosius;patch is against trunk.

I can redo and apply to whatever branch you like assuming it is ok.;;;","14/Mar/14 01:23;jbellis;LGTM; I'm okay with either 2.0+ or 2.1+.;;;","14/Mar/14 04:45;dbrosius;committed to cassandra-2.0 as c49d33633aa07551af52e40277e284eb78bb73d4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Running sstableupgrade on C* 1.0 data dir, before starting C* 1.2 for the first time breaks stuff",CASSANDRA-5831,12660765,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,jjordan,jjordan,30/Jul/13 19:55,16/Apr/19 09:32,14/Jul/23 05:53,07/Aug/13 23:05,1.2.9,,,Legacy/Tools,,,0,,,,,"If you try to upgrade from C* 1.0.X to 1.2.X and run offline sstableupgrade to try and migrate the sstables before starting 1.2.X for the first time, it messes up the system folder, because it doesn't migrate it right, and then C* 1.2 can't start.
sstableupgrade should either refuse to run against a C* 1.0 data folder, or migrate stuff the right way.",,colinkuo,jjordan,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Aug/13 00:07;thobbs;0001-Handle-pre-1.1-data-directory-layout.patch;https://issues.apache.org/jira/secure/attachment/12595521/0001-Handle-pre-1.1-data-directory-layout.patch","07/Aug/13 18:03;thobbs;0002-Handle-old-system-data-in-health-check.patch;https://issues.apache.org/jira/secure/attachment/12596668/0002-Handle-old-system-data-in-health-check.patch",,,,,,,,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340954,,,Wed Aug 07 23:05:43 UTC 2013,,,,,,,,,,"0|i1msz3:",341272,,,,,,,,,jjordan,,jjordan,Low,,,,,,,,,,,,,,,,,,"30/Jul/13 20:23;jbellis;I think all we need to do here is ""don't run upgradesstables if the ks/cf/ heirarchy doesn't exist already for the system tables.""

In particular, upgradesstables against a 1.1 install should be fine.;;;","31/Jul/13 18:10;thobbs;bq. I think all we need to do here is ""don't run upgradesstables if the ks/cf/ heirarchy doesn't exist already for the system tables.""

If I'm interpreting you correctly, we'll just want upgradesstables to error out in that case and mention something about starting Cassandra 1.1+ before running upgradesstables again, is that correct?;;;","31/Jul/13 18:12;jbellis;Right.;;;","31/Jul/13 20:45;thobbs;Attached patch 0001 does exactly that.;;;","31/Jul/13 23:07;jbellis;I think we probably want to decouble sstableNeedsMigration from its caller in CassandraDaemon, it throws a bunch of exceptions that are probably not expected.

However, I also note that StandaloneScrubber calls sNM, so maybe making upgradesstables able to perform the migration automagically isn't as painful as I thought.;;;","01/Aug/13 18:19;thobbs;bq. I think we probably want to decouble sstableNeedsMigration from its caller in CassandraDaemon, it throws a bunch of exceptions that are probably not expected.

Are you just referring to the RuntimeExceptions around the total path length on Windows?  If not, can you clarify?

bq. However, I also note that StandaloneScrubber calls sNM, so maybe making upgradesstables able to perform the migration automagically isn't as painful as I thought.

Yeah, I noticed that with only a few minor changes to {{migrateSSTables()}}, the upgrader should be able to do this pretty easily.  Do you want me to add that in?;;;","01/Aug/13 18:49;jbellis;bq. Are you just referring to the RuntimeExceptions

Yes. 

bq. with only a few minor changes to migrateSSTables(), the upgrader should be able to do this pretty easily

Let's go ahead and do that, then.;;;","01/Aug/13 22:40;thobbs;Hmm, there's one complication: {{sstableupgrade}} requires a keyspace and table to be specified, whereas the data directory migration is normally done all at once.  Changing the directory layout for only a single CF feels strange, but changing the layout for other keyspaces and CFs that you didn't specify also feels wrong.;;;","01/Aug/13 23:14;jbellis;How does scrub deal with this?;;;","01/Aug/13 23:27;thobbs;bq. How does scrub deal with this?

Good point. It just runs the full migration.  That still seems less-than-perfect, but at least the behavior would be consistent.;;;","01/Aug/13 23:29;jbellis;Seems like if they're running upgrade then they've signaled their intent to be on the new version, so maybe just add a notice that we're moving *everything* to the new directory structure and call it good.;;;","01/Aug/13 23:31;jjordan;Agreed.  Also, if you want to add a ""migrate all keyspaces"" option, I don't think anyone would complain.;;;","02/Aug/13 00:07;thobbs;The new 0001 patch adds a {{\-\-migrate}} option to {{sstableupgrade}} and {{sstablescrub}}.  If that option is not used and a pre-1.1 layout is detected, both tools will error out and mention the {{\-\-migrate}} option.  If the option is used, all keyspaces and column families will be migrated.;;;","07/Aug/13 03:17;jjordan;Something more needs to happen on migration for 1.2.  Both sstableupgrade and sstablescrub create a broken set of system tables.

{noformat}
ERROR 22:11:40,896 Fatal exception during initialization
org.apache.cassandra.exceptions.ConfigurationException: Found system table files, but they couldn't be loaded!
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:440)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:243)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:447)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:490)
{noformat}

Works fine if I just run ./cassandra to do the upgrade.;;;","07/Aug/13 18:03;thobbs;Patch 0002 alters the failing health check to also look for a saved cluster name in {{System.LocationInfo}}, which indicates that the system data has not been migrated yet (it happens at the end of the startup process, whereas the health check is early in the startup process).

I also have a [branch|https://github.com/thobbs/cassandra/tree/CASSANDRA-5831] that contains the two patches.;;;","07/Aug/13 21:58;jjordan;Still not sure what is really going on here.  ""Directories.migrateSSTables();"" is called before ""SystemTable.checkHealth()"" on a regular C* startup.  All sstablescrub and sstableupgrade do is call Directories.migrateSSTables();, then call DatabaseDescriptor.loadSchemas();, so why does ""SystemTable.checkHealth"" fail after that?  SystemTable.finishStartup(); which does the table upgrade doesn't get called until well after checkHealth.  Is calling DatabaseDescriptor.loadSchemas(); in the sstable* scripts doing something that breaks checkHealth?  I don't know that the answer should be ""change check health"".  Should we call upgrade tables from the migration stuff in sstable* scripts?

Regular C* startup does:
{noformat}
Directories.migrateSSTables();
SystemTable.checkHealth();
DatabaseDescriptor.loadSchemas();
{noformat}

sstable* do:
{noformat}
Directories.migrateSSTables();
DatabaseDescriptor.loadSchemas();
{noformat}

So if you run sstable* then start c* the order is:

{noformat}
Directories.migrateSSTables();
DatabaseDescriptor.loadSchemas();
SystemTable.checkHealth();
DatabaseDescriptor.loadSchemas();
{noformat}
;;;","07/Aug/13 22:38;jjordan;So after going through it some more I think the health check change is fine.  Looks like the issue is that loadSchemas is causing empty new tables to be made, so those get checked for stuff.

Everything looks good now.  +1 from me.;;;","07/Aug/13 23:05;jbellis;Committed (to 1.2 only), thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Paxos loops endlessly due to faulty condition check,CASSANDRA-5830,12660728,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,soumava,soumava,soumava,30/Jul/13 17:52,25/Oct/19 13:11,14/Jul/23 05:53,05/Aug/13 21:44,2.0.0,,,Feature/Lightweight Transactions,,,0,LWT,paxos,,,"Following is the code segment (StorageProxy.java:361) which causes the issue: 

Start is the start time of the paxos, is always less than the current system time, and therefore the negative difference is always less than the timeout. 

{code:title=StorageProxy.java|borderStyle=solid}
private static UUID beginAndRepairPaxos(long start, ByteBuffer key, CFMetaData metadata, List<InetAddress> liveEndpoints, int requiredParticipants, ConsistencyLevel consistencyForPaxos)
    throws WriteTimeoutException
    {
        long timeout = TimeUnit.MILLISECONDS.toNanos(DatabaseDescriptor.getCasContentionTimeout());

        PrepareCallback summary = null;
        while (start - System.nanoTime() < timeout)
        {
            long ballotMillis = summary == null
                              ? System.currentTimeMillis()
                              : Math.max(System.currentTimeMillis(), 1 + UUIDGen.unixTimestamp(summary.inProgressCommit.ballot));
            UUID ballot = UUIDGen.getTimeUUID(ballotMillis);
{code}

Here, the paxos gets stuck when PREPARE returns 'true' but with inProgressCommit. The code in StorageProxy.java:beginAndRepairPaxos() then tries to issue a PROPOSE and COMMIT for the inProgressCommit, and if it repeatedly receives 'false' as a PREPARE_RESPONSE it gets stuck in an endless loop until PREPARE_RESPONSE is true. ",,blair,jjordan,rcoli,soumava,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,soumava,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340917,,,Mon Aug 05 21:44:40 UTC 2013,,,,,,,,,,"0|i1msqv:",341235,,,,,,,,,jbellis,,jbellis,Normal,,2.0 beta 1,,,,,,,,,,,,,,,,"05/Aug/13 21:44;jbellis;Fixed condition check in 4b4ccc3ccfcc7be8fad0b25bde9a180f0016d520, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Expose setters for consistency level in Hadoop config helper,CASSANDRA-5827,12660553,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,mkmainali,mkmainali,mkmainali,30/Jul/13 04:50,16/Apr/19 09:32,14/Jul/23 05:53,30/Jul/13 14:24,1.2.9,2.0 rc1,,,,,0,,,,,"ConfigHelper exposes the getters for read and write consistency, which defaults to the consistency level of ""ONE"" if one is not defined. However, setters are missing.",,mkmainali,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/13 04:51;mkmainali;trunk-CASSANDRA-5827.patch;https://issues.apache.org/jira/secure/attachment/12594868/trunk-CASSANDRA-5827.patch",,,,,,,,,,,,,,,,,,,,1.0,mkmainali,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340744,,,Tue Jul 30 14:24:59 UTC 2013,,,,,,,,,,"0|i1mrof:",341062,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"30/Jul/13 04:51;mkmainali;Attaching the patch;;;","30/Jul/13 14:24;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix trigger directory detection code,CASSANDRA-5826,12660523,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,aleksey,aleksey,29/Jul/13 23:33,16/Apr/19 09:32,14/Jul/23 05:53,06/Aug/13 02:55,2.0 rc1,,,,,,0,triggers,,,,"At least when building from source, Cassandra determines the trigger directory wrong. C* calculates the trigger directory as 'build/triggers' instead of 'triggers'.

FBUtilities.cassandraHomeDir() is to blame, and should be replaced with something more robust.",OS X,aleksey,jjordan,vijay2win@yahoo.com,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jul/13 19:04;vijay2win@yahoo.com;0001-5826.patch;https://issues.apache.org/jira/secure/attachment/12595243/0001-5826.patch","05/Aug/13 23:39;vijay2win@yahoo.com;0001-handle-trigger-non-existance-v2.patch;https://issues.apache.org/jira/secure/attachment/12596240/0001-handle-trigger-non-existance-v2.patch","05/Aug/13 20:18;vijay2win@yahoo.com;0001-handle-trigger-non-existance.patch;https://issues.apache.org/jira/secure/attachment/12596207/0001-handle-trigger-non-existance.patch",,,,,,,,,,,,,,,,,,3.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340714,,,Tue Aug 06 03:20:44 UTC 2013,,,,,,,,,,"0|i1mrhr:",341032,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"30/Jul/13 18:41;vijay2win@yahoo.com;Probably have to change the build.xml to copy the trigger directory to build like what we do with conf directory?
I will add the above and also add it to Debian package may be (in addition adding a property to override the trigger absolute path).;;;","31/Jul/13 19:04;vijay2win@yahoo.com;Attached a small patch moves the trigger directory into conf directory, hope it is fine. that way we can just search for the triggers directory in the class path (which is Conf). Thanks!;;;","02/Aug/13 17:53;jbellis;What is the Right Thing to do here?

I kind of like the original idea of locating lib/ and making triggers/ be a sibling of that by convention.

""Any directory named triggers on the classpath"" makes me nervous.

What would a Real Java Programmer do here [~zznate]?;;;","02/Aug/13 18:30;zznate;bq. What would a Real Java Programmer do here Nate McCall?

Awww shucks. Capitalized and everything!

I would expect that it is on me to put my own dependencies in the existing ""lib"" directory. As a principle of least surprise: web containers, such as Tomcat, have a top level lib directory where you can put shared dependencies (http://tomcat.apache.org/tomcat-7.0-doc/appdev/deployment.html#Shared_Library_Files). 

This has the benefit of not requiring modification of cassandra.in.sh or similar init scripts already in the wild. 

As long as we are not trying to isolate classloaders or anything (not a bad idea for triggers in the future) there is no need technically or conventionally to have user binaries somewhere else. If folks want something different, they can hack the scripts to noodle the classpath to their hearts content. ;;;","02/Aug/13 18:57;vijay2win@yahoo.com;{quote}
As long as we are not trying to isolate classloaders or anything 
{quote}

Actually we do it with triggers, similar to what solr does for Tokenizer code etc (but not the same). 

For the record: You can place all of your dependencies in the trigger directory except everything which Cassandra depends on.
If the user uses maven for building, all he needs to do is, and place the jars in the trigger directory.

{code}
    <dependency>
      <groupId>org.apache.cassandra</groupId>
      <artifactId>cassandra-all</artifactId>
      <version>2.0.0-beta2</version>
      <scope>provided</scope>
    </dependency>
{code}

My understanding is that, Java doesn't do nested class path scanning on sub directories, hence conf file was ok to do. 
But understand it is kind of scary if someone places in conf instead.;;;","02/Aug/13 20:13;zznate;Ok, if the classloader is already isolated, a separate top-level ""triggers"" dir(s) would be fine. 

As a point of reference, and it's significantly more sophisticated than our current needs here, but Vert.x has a nice scanning and loading system for this.
Details:
http://vertx.io/mods_manual.html#module-classpath
Impl:
https://github.com/eclipse/vert.x/blob/master/vertx-platform/src/main/java/org/vertx/java/platform/impl/ModuleClassLoader.java

This approach has the benefit of being able load directly from dirs and bintray or maven repos (local, remote or central). This flexibility is way outside scope here, but just food for thought. 


;;;","03/Aug/13 02:04;jbellis;What the hell, we're already labeling this Experimental.

Ship it!;;;","03/Aug/13 18:32;vijay2win@yahoo.com;Committed to trunk, Thanks!

Nate: All we wanted to do was to separate/sandbox the ITriggers from already loaded Cassandra's classes, https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/triggers/CustomClassLoader.java.;;;","05/Aug/13 18:33;brandon.williams;Reopening because first of all the directory detection code doesn't work when the directory does not exist:

{noformat}
ERROR 18:32:00,800 Exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.ExceptionInInitializerError
        at org.apache.cassandra.service.StorageProxy.mutateWithTriggers(StorageProxy.java:535)
        at org.apache.cassandra.cql3.statements.ModificationStatement.executeWithoutCondition(ModificationStatement.java:358)
        at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:342)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:101)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:117)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:130)
        at org.apache.cassandra.auth.Auth.setupDefaultSuperuser(Auth.java:209)
        at org.apache.cassandra.auth.Auth.access$000(Auth.java:44)
        at org.apache.cassandra.auth.Auth$1.run(Auth.java:144)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:292)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.utils.FBUtilities.cassandraTriggerDir(FBUtilities.java:351)
        at org.apache.cassandra.triggers.TriggerExecutor.<init>(TriggerExecutor.java:45)
        at org.apache.cassandra.triggers.TriggerExecutor.<clinit>(TriggerExecutor.java:41)
        ... 17 more
{noformat}

And secondly, I don't think we should throw if the directory doesn't exist, instead just move along without triggers (perhaps log a warning);;;","05/Aug/13 18:42;vijay2win@yahoo.com;Hi Brandon, Oooops... Isn't the directory found in conf? i can remove the RTE and make it log, if not found.;;;","05/Aug/13 18:46;brandon.williams;The config is unmodified, but the conf/triggers dir itself is missing.;;;","05/Aug/13 20:18;vijay2win@yahoo.com;Hi Brandon, Attached, handles un reachable trigger directory.;;;","05/Aug/13 20:35;brandon.williams;This actually doesn't help since the NPE is occurring slightly sooner:

{noformat}
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.utils.FBUtilities.cassandraTriggerDir(FBUtilities.java:350)
        at org.apache.cassandra.triggers.TriggerExecutor.reloadClasses(TriggerExecutor.java:58)
        at org.apache.cassandra.triggers.TriggerExecutor.<init>(TriggerExecutor.java:49)
        at org.apache.cassandra.triggers.TriggerExecutor.<clinit>(TriggerExecutor.java:41)
        ... 17 more
{noformat};;;","05/Aug/13 20:42;vijay2win@yahoo.com;Hi Brandon did the patch apply clean? 

{code}
        File tiggerDirectory = FBUtilities.cassandraTriggerDir();
        if (tiggerDirectory == null)
            return;
{code}

should save a NPE, i did test it and worked fine for me.;;;","05/Aug/13 21:39;brandon.williams;The NPE is on 350 (with the patch applied) in FBU, which is this:

{noformat}
            triggerDir = new File(FBUtilities.class.getClassLoader().getResource(DEFAULT_TRIGGER_DIR).getFile());
{noformat};;;","05/Aug/13 23:39;vijay2win@yahoo.com;Hi Brandon, fixed in v2 ;;;","06/Aug/13 01:51;brandon.williams;+1, minor nit: s/Directory doesnt/directory doesn't/ in the warning.;;;","06/Aug/13 02:55;vijay2win@yahoo.com;Committed, with nit Thanks!;;;","06/Aug/13 03:11;jbellis;It looks like you only committed to trunk.  You should cherry-pick to 2.0.0, then merge to 2.0 and then to trunk again.;;;","06/Aug/13 03:20;vijay2win@yahoo.com;Done, sorry for all the mess on a simple patch.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix quoting in CqlPagingRecordReader and CqlRecordWriter,CASSANDRA-5824,12660496,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,29/Jul/13 21:16,16/Apr/19 09:32,14/Jul/23 05:53,30/Jul/13 19:32,1.2.9,2.0 rc1,,,,,0,,,,,"To support case sensitive in CQL, we need add double quotes to the name of columns and table.",,alexliu68,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/13 22:18;alexliu68;5824-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12594806/5824-1.2-branch.txt",,,,,,,,,,,,,,,,,,,,1.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340688,,,Tue Jul 30 19:30:56 UTC 2013,,,,,,,,,,"0|i1mrbz:",341006,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"29/Jul/13 21:18;jbellis;done in CASSANDRA-5763;;;","29/Jul/13 22:18;alexliu68;It fixes the CqlRecordWriter;;;","30/Jul/13 19:30;jbellis;To clarify: this fixes redundant quoting in CqlPRR of what keyString has already quoted for us, and adds quoting to CqlRW.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstableloader broken in 1.2.7/1.2.8,CASSANDRA-5820,12660414,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,nickmbailey,nickmbailey,29/Jul/13 16:41,16/Apr/19 09:32,14/Jul/23 05:53,30/Jul/13 20:08,1.2.9,,,Legacy/Tools,,,0,,,,,"I don't see this happen on 1.2.6.

To reproduce (on a fresh single node cluster):

{noformat}
[Nicks-MacBook-Pro:11:33:06 (cassandra-1.2.7)*] cassandra$ bin/cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.1.4 | Cassandra 1.2.7-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.36.0]
cqlsh> CREATE KEYSPACE test_backup_restore WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh> use test_backup_restore;
cqlsh:test_backup_restore> CREATE TABLE cf0 (
                       ...   a text PRIMARY KEY,
                       ...   b text,
                       ...   c text
                       ... );
cqlsh:test_backup_restore> INSERT INTO cf0 (a, b, c) VALUES ( 'a', 'b', 'c');
cqlsh:test_backup_restore> select * from cf0;

 a | b | c
---+---+---
 a | b | c

cqlsh:test_backup_restore> ^D
[Nicks-MacBook-Pro:11:34:22 (cassandra-1.2.7)*] cassandra$ bin/nodetool snapshot
Requested creating snapshot for: all keyspaces
Snapshot directory: 1375115668449
[Nicks-MacBook-Pro:11:34:40 (cassandra-1.2.7)*] cassandra$ mkdir -p test_backup_restore/snapshots
[Nicks-MacBook-Pro:11:34:48 (cassandra-1.2.7)*] cassandra$ cp /var/lib/cassandra/data/test_backup_restore/cf0/snapshots/1375115668449/* test_backup_restore/snapshots/
[Nicks-MacBook-Pro:11:35:14 (cassandra-1.2.7)*] cassandra$ bin/sstableloader --debug -v -d 127.0.0.1 test_backup_restore/snapshots
Streaming revelant part of test_backup_restore/snapshots/test_backup_restore-cf0-ic-1-Data.db  to [/127.0.0.1]
org.apache.cassandra.io.util.CompressedSegmentedFile cannot be cast to org.apache.cassandra.io.util.CompressedPoolingSegmentedFile
java.lang.ClassCastException: org.apache.cassandra.io.util.CompressedSegmentedFile cannot be cast to org.apache.cassandra.io.util.CompressedPoolingSegmentedFile
	at org.apache.cassandra.io.sstable.SSTableReader.getCompressionMetadata(SSTableReader.java:574)
	at org.apache.cassandra.streaming.StreamOut.createPendingFiles(StreamOut.java:179)
	at org.apache.cassandra.streaming.StreamOut.transferSSTables(StreamOut.java:154)
	at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:145)
	at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:67)
{noformat}",,mbulman,nickmbailey,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/13 15:52;yukim;0001-Add-SSTableLoader-unit-test.patch;https://issues.apache.org/jira/secure/attachment/12594975/0001-Add-SSTableLoader-unit-test.patch","30/Jul/13 18:21;thobbs;0002-Create-CompressedFile-common-interface.patch;https://issues.apache.org/jira/secure/attachment/12595007/0002-Create-CompressedFile-common-interface.patch",,,,,,,,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340606,,,Tue Jul 30 20:08:34 UTC 2013,,,,,,,,,,"0|i1mqtr:",340924,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"30/Jul/13 15:52;yukim;Looks like this is a regression from CASSANDRA-5555.
(I think workaround is to use sstableloader from 1.2.6).

Attached unit test for SSTableLoader. if run with 'ant test-compression -Dtest.name=SSTableLoaderTest', it fails as described above.;;;","30/Jul/13 18:21;thobbs;Patch 0002 creates a common Interface for the two Compressed*File classes to allow access to the compression metadata without casting.;;;","30/Jul/13 20:08;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Duplicated error messages on directory creation error at startup,CASSANDRA-5818,12660297,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lyubent,mfiguiere,mfiguiere,28/Jul/13 20:47,16/Apr/19 09:32,14/Jul/23 05:53,16/May/14 03:28,2.1 rc2,,,,,,0,,,,,"When I start Cassandra without the appropriate OS access rights to the default Cassandra directories, I get a flood of {{ERROR}} messages at startup, whereas one per directory would be more appropriate. See bellow:

{code}
ERROR 13:37:39,792 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 13:37:39,797 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 13:37:39,798 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 13:37:39,798 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 13:37:39,799 Failed to create /var/lib/cassandra/data/system/schema_triggers directory
ERROR 13:37:39,800 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 13:37:39,801 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 13:37:39,801 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 13:37:39,802 Failed to create /var/lib/cassandra/data/system/batchlog directory
ERROR 13:37:39,802 Failed to create /var/lib/cassandra/data/system/peer_events directory
ERROR 13:37:39,803 Failed to create /var/lib/cassandra/data/system/peer_events directory
ERROR 13:37:39,803 Failed to create /var/lib/cassandra/data/system/peer_events directory
ERROR 13:37:39,804 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 13:37:39,805 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 13:37:39,805 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 13:37:39,806 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 13:37:39,807 Failed to create /var/lib/cassandra/data/system/compactions_in_progress directory
ERROR 13:37:39,808 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 13:37:39,809 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 13:37:39,809 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 13:37:39,811 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 13:37:39,811 Failed to create /var/lib/cassandra/data/system/hints directory
ERROR 13:37:39,812 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 13:37:39,812 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 13:37:39,813 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 13:37:39,814 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 13:37:39,814 Failed to create /var/lib/cassandra/data/system/schema_keyspaces directory
ERROR 13:37:39,815 Failed to create /var/lib/cassandra/data/system/range_xfers directory
ERROR 13:37:39,816 Failed to create /var/lib/cassandra/data/system/range_xfers directory
ERROR 13:37:39,817 Failed to create /var/lib/cassandra/data/system/range_xfers directory
ERROR 13:37:39,817 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,818 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,818 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,820 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,821 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,821 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,822 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,822 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,823 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,824 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,824 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,825 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,825 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,827 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,828 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,828 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,829 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,830 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,831 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,831 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,832 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,833 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,834 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,834 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,835 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,836 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,836 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,838 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,838 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,839 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,840 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,840 Failed to create /var/lib/cassandra/data/system/schema_columnfamilies directory
ERROR 13:37:39,841 Failed to create /var/lib/cassandra/data/system/NodeIdInfo directory
ERROR 13:37:39,849 Failed to create /var/lib/cassandra/data/system/NodeIdInfo directory
ERROR 13:37:39,850 Failed to create /var/lib/cassandra/data/system/NodeIdInfo directory
ERROR 13:37:39,851 Failed to create /var/lib/cassandra/data/system/NodeIdInfo directory
ERROR 13:37:39,854 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,855 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,857 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,859 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,859 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,860 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,865 Failed to create /var/lib/cassandra/data/system/paxos directory
ERROR 13:37:39,866 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,867 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,867 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,868 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,869 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,869 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,870 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,870 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,871 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,872 Failed to create /var/lib/cassandra/data/system/schema_columns directory
ERROR 13:37:39,872 Failed to create /var/lib/cassandra/data/system/IndexInfo directory
ERROR 13:37:39,873 Failed to create /var/lib/cassandra/data/system/IndexInfo directory
ERROR 13:37:39,873 Failed to create /var/lib/cassandra/data/system/IndexInfo directory
ERROR 13:37:39,874 Failed to create /var/lib/cassandra/data/system/IndexInfo directory
ERROR 13:37:39,874 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,875 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,876 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,876 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,877 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,877 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,878 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,879 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,879 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,880 Failed to create /var/lib/cassandra/data/system/peers directory
ERROR 13:37:39,880 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,881 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,881 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,882 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,883 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,883 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,884 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,885 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,885 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,886 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,887 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,888 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,889 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,889 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,890 Failed to create /var/lib/cassandra/data/system/local directory
ERROR 13:37:39,894 Fatal error: java.io.IOException: Failed to mkdirs /var/lib/cassandra/data
Failed to mkdirs /var/lib/cassandra/data; unable to start server
{code}",,aleksey,dbrosius,lyubent,mfiguiere,mishail,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-7135,,,,,,,,,,,,,,,,,,,,"27/Nov/13 13:02;lyubent;5818_v2.patch;https://issues.apache.org/jira/secure/attachment/12616046/5818_v2.patch","13/May/14 09:55;lyubent;cassandra-2.0-5818_v2.diff;https://issues.apache.org/jira/secure/attachment/12644594/cassandra-2.0-5818_v2.diff","13/Oct/13 14:02;ksaritek;patch.diff;https://issues.apache.org/jira/secure/attachment/12608206/patch.diff","06/Oct/13 04:23;mishail;trunk-5818.patch;https://issues.apache.org/jira/secure/attachment/12607052/trunk-5818.patch",,,,,,,,,,,,,,,,,4.0,lyubent,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340489,,,Fri May 16 03:28:40 UTC 2014,,,,,,,,,,"0|i1mq3r:",340807,2.0.1,,,,,,,,mishail,,mishail,Low,,,,,,,,,,,,,,,,,,"29/Jul/13 00:21;dbrosius;c* should consider switching to logback

http://logback.qos.ch/apidocs/ch/qos/logback/classic/turbo/DuplicateMessageFilter.html;;;","21/Aug/13 23:29;aleksey;Moving this to 2.1 (to be able to use logback);;;","27/Aug/13 05:24;dbrosius;the issue is, you can look for duplicates in two ways:

1) look at duplicates in the format string -> logbacks DuplicateMessageFilter class
2) look at duplicates in the total message -> catch only the pathological runaway logger case (c* potentially supplied filter)

obviously for the this case, you would want 1.

However there are lots of message that are like this, like 

INFO  05:09:36 Initializing system.IndexInfo
INFO  05:09:36 Initializing system.peers
INFO  05:09:36 Initializing system.local
....

etc.

So, i'm not sure c* wants to set anything in logback.xml by default. Certainly the Op can add it, (perhaps we just comment it into the lockback.xml class so they can uncomment if they so desire).;;;","27/Aug/13 07:33;slebresne;Honestly, I think we've been looking at that issue the wrong way. I don't think we should do anything with the general logging here (and I'm -1 on DuplicateMessageFilter given how it works).

What we should do is, in the code that creates the directories at startup, add a check for permissions on the data directory (the same could be done for the commitlog directory btw) that stops startup right away with just one message (which would be more friendly as it would tell what's the problem). On top of that, if we really want to be thorough, we could try creating the data directory first (if it doesn't exist already) and check for errors, and then create the CF directories (instead of trying to create all the CF directories upfront and logging a message for each one when the problem is actually on the top-level data directory).;;;","06/Oct/13 04:23;mishail;There is already a logic which checks _existing_ dirs on start-up. We can extend it to try to create non existing directories and check the result. Attaching the patch just in case;;;","06/Oct/13 07:03;dbrosius;Not sure how this addresses the original issue? Is this to fix something else?

BTW, this bug report seems to require the removal of assertion exceptions... ie, remove -ea from cassandra-env.sh

;;;","06/Oct/13 17:45;mishail;{quote}
 if we really want to be thorough, we could try creating the data directory first (if it doesn't exist already) and check for errors, and then create the CF directories (instead of trying to create all the CF directories upfront and logging a message for each one when the problem is actually on the top-level data directory)
{quote}

The patch address the issue in the way proposed by [~slebresne]. It tries to create data/commit log/saved caches directories if they don't exist and stops the startup in case of errors.;;;","07/Oct/13 20:11;ksaritek;+1;;;","08/Oct/13 07:12;slebresne;We should not use asserts here because some people disallow them. The fact that we currently check the directory permissions in an assert is a bug, we should use this ticket to fix it. Nit: while we're at it, could make sense to move that directory checking code in a static method of Directories (like is done with Directories.migrateSSTables()).;;;","08/Oct/13 17:20;ksaritek;attached a patch. Check for  filepermission then trigger exit to not to go whole proccess.;;;","09/Oct/13 01:02;dbrosius;please fix formatting -- curlies on their own lines;;;","09/Oct/13 03:03;mishail;On Windows 7 (JDK 7u40) with [^patch.diff] applied I get 
{{ERROR 02:59:36 Has no permission for /var/lib/cassandra/data directory}} 
even though dir exists and writable/readable.

I made a quick test and I see that {{AccessController.checkPermission(perm);}} always throws an exception;;;","10/Oct/13 19:59;ksaritek;Mikhail, that was my fault, i traced and see that just the directory that classes are loaded are readonly permitted. Just put an enum and a simple method at Directories. If that approach is ok, will go on it. 

And one more thing, attached patch name is also same as before : patch.diff;;;","11/Oct/13 00:50;dbrosius;I don't understand why hasPrivilege takes a varargs of actions, considering, you are only considering the last one passed in, the way the code is now. Where they supposed to be privilege &= .... ?;;;","11/Oct/13 06:46;ksaritek;You are right, sorry for crappy code.

Could we approach like chmod staff instead of varargs? 
then we will check that have permission like:
7 = 4+2+1 (read/write/execute)
6 = 4+2 (read/write)
5 = 4+1 (read/execute)
4 = 4 (read)
3 = 2+1 (write/execute)
2 = 2 (write)
1 = 1 (execute)


;;;","13/Oct/13 14:03;ksaritek;attached a new path under patch.diff, am I at wrong way?;;;","20/Oct/13 01:10;jbellis;How does that look to you, Mikhail?;;;","20/Oct/13 21:04;mishail;Few comments 
* Permissions in a 1-7 format don't seem very informative for me. I would rather stick with R. X, W, RW, RWX etc combination.
* For existing file objects the patch does not check whether the file can be read and executed, and whether it's directory at all. It only checks if file is writable ({{Directories.FileAction._2}}).
* There is no need to create a new {{File}} object in {{!Directories.FileAction.hasPrivilege(new File(dataDir), Directories.FileAction._2)}} since we already {{dir}} object.;;;","18/Nov/13 23:33;jbellis;Are you still working on this, [~ksaritek]?;;;","27/Nov/13 13:01;lyubent;all the nits pointed out by [~mishail] are corrected in v2. Also changed the logic to:
{code}
check if dir exists
    if not try to create it
if dir does exist
    check permissions
{code}

Moved the directory checking function to o.a.c.db.Directores#hasFullPermissions

Also patch is for cassandra-2.0;;;","27/Nov/13 18:30;mishail;+1;;;","27/Nov/13 18:37;jbellis;committed;;;","02/May/14 15:49;aleksey;Should back port to 2.0.;;;","13/May/14 09:55;lyubent;Patch for cassandra-2.0 on 453a07430c3ebce938047f9d5d0339ff90c6bfcc
;;;","13/May/14 15:22;aleksey;[~mishail] can you review/commit the 2.0 backport patch?;;;","16/May/14 03:28;mishail;Commited into 2.0;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE from migration manager,CASSANDRA-5815,12660139,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,vkasar,vkasar,26/Jul/13 21:51,16/Apr/19 09:32,14/Jul/23 05:53,22/Oct/13 20:34,1.2.12,2.0.2,,,,,0,,,,,"In one of our production clusters we see this error often. Looking through the source, Gossiper.instance.getEndpointStateForEndpoint(endpoint) is returning null for some end point. De we need any config change on our end to resolve this? In any case, cassandra should be updated to protect against this NPE.

{noformat}
ERROR [OptionalTasks:1] 2013-07-24 13:40:38,972 AbstractCassandraDaemon.java (line 132) Exception in thread Thread[OptionalTasks:1,5,main] 
java.lang.NullPointerException 
at org.apache.cassandra.service.MigrationManager$1.run(MigrationManager.java:134) 
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) 
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) 
at java.util.concurrent.FutureTask.run(FutureTask.java:138) 
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98) 
at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:662)
{noformat}

It turned out that the reason for NPE was we bootstrapped a node with the same token as another node. Cassandra should not throw an NPE here but log a meaningful error message. ",,cburroughs,jjordan,thobbs,vkasar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Oct/13 18:05;brandon.williams;5185.txt;https://issues.apache.org/jira/secure/attachment/12609180/5185.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340331,,,Tue Oct 22 20:34:21 UTC 2013,,,,,,,,,,"0|i1mp53:",340649,,,,,,,,,thobbs,,thobbs,Low,,,,,,,,,,,,,,,,,,"07/Oct/13 15:37;cburroughs;I'm seeing an NPE in migration manager in 1.2.9 and what I think is the same spot (line numbers changed slightly since July).  This occurs on at least one node every time (about 10 attempts) I try to bootstrap with a 2 dc production cluster using the GPFS w/ reconnecting.

{noformat}
ERROR [OptionalTasks:1] 2013-10-07 08:06:05,658 CassandraDaemon.java (line 194) Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.MigrationManager$1.run(MigrationManager.java:130)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

I added a log message to confirm that Gossiper really really thinks it's not there (off of the 1.2.10 tag if that matters).  I'm suspicious of this being a timing problem the reconnect dance, but I'm not sure how to prove or disprove that.

{noformat}
                    logger.warn(""[csb] Trying to get endpoint state for {} ; exists {}"", new Object[] {endpoint, Gossiper.instance.isKnownEndpoint(endpoint)});

 INFO [GossipTasks:1] 2013-10-07 11:19:10,565 Gossiper.java (line 803) InetAddress /208.49.103.36 is now DOWN
 INFO [GossipTasks:1] 2013-10-07 11:19:13,572 Gossiper.java (line 608) FatClient /208.49.103.36 has been silent for 30000ms, removing from gossip
 INFO [HANDSHAKE-/208.49.103.36] 2013-10-07 11:19:13,863 OutboundTcpConnection.java (line 399) Handshaking version with /208.49.103.36
 INFO [HANDSHAKE-/208.49.103.36] 2013-10-07 11:19:15,275 OutboundTcpConnection.java (line 399) Handshaking version with /208.49.103.36
 WARN [OptionalTasks:1] 2013-10-07 11:19:36,696 MigrationManager.java (line 130) [csb] Trying to get endpoint state for /208.49.103.36 ; exists false
ERROR [OptionalTasks:1] 2013-10-07 11:19:36,696 CassandraDaemon.java (line 193) Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.MigrationManager$1.run(MigrationManager.java:131)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat};;;","07/Oct/13 15:41;brandon.williams;It looks the same to me.  The good news is the error is purely cosmetic at this point, there's nothing left to do if the gossiper has removed the node (not to mention it's a fat client);;;","07/Oct/13 16:28;cburroughs;Whoops, missed the important part for the case I am seeing but might not be part of the original (bootstrapping with the same token would presumably fail anyway).  The situation I am seeing post NPE is:
 * Bootstrapping node expects steams from NPE-node
 * NPE-node says it has no outstanding streams

And thus bootstrap never completes.;;;","07/Oct/13 16:39;brandon.williams;[~cburroughs] I think your problem is something else, since the bootstrapping node has not only been marked down, but it's been down long enough to get removed (which is the race between the gossiper and MM causing this NPE)  I will note for myself though that the fat client removal should also wait until the node has been marked down before beginning the 30s countdown to removal.

If the node has connected but the gossiper doesn't know about it, they haven't gossiped yet, so there's really nothing for MM to do yet anyway.;;;","18/Oct/13 18:02;brandon.williams;Mostly this problem is cosmetic, but the crux of it is that in a couple of places we assume that since we're in a call regarding an endpoint, the gossiper will always know about the endpoint while we're in the call.  This isn't the case with fat clients though, which the gossiper could expire at any time.  Patch to check that the gossiper actually still knows about the endpoint to avoid the NPE.;;;","22/Oct/13 20:27;thobbs;+1;;;","22/Oct/13 20:34;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RowIndexEntry.deletionTime raises UnsupportedOperationException when upgrading to 1.2.7,CASSANDRA-5814,12660134,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,bretthoerner,bretthoerner,26/Jul/13 21:19,16/Apr/19 09:32,14/Jul/23 05:53,27/Jul/13 19:20,1.2.8,2.0 rc1,,,,,0,,,,,"Upgrading from 1.2.5 to 1.2.7 immediately caused the following exception. We stopped the node and reverted to 1.2.5.

I'll note that we also run a 1.2.6 cluster that is doing fine, so I think this is caused by a change in 1.2.7.

{code}
ERROR [MutationStage:59] 2013-07-26 19:35:10,460 CassandraDaemon.java (line 192) Exception in thread Thread[MutationStage:59,5,main]
java.lang.ExceptionInInitializerError
	at org.apache.cassandra.utils.CounterId.localIds(CounterId.java:49)
	at org.apache.cassandra.utils.CounterId.getLocalId(CounterId.java:54)
	at org.apache.cassandra.db.context.CounterContext.create(CounterContext.java:105)
	at org.apache.cassandra.db.CounterUpdateColumn.localCopy(CounterUpdateColumn.java:84)
	at org.apache.cassandra.db.CounterUpdateColumn.localCopy(CounterUpdateColumn.java:34)
	at org.apache.cassandra.db.CounterMutation.apply(CounterMutation.java:133)
	at org.apache.cassandra.service.StorageProxy$7.runMayThrow(StorageProxy.java:758)
	at org.apache.cassandra.service.StorageProxy$LocalMutationRunnable.run(StorageProxy.java:1630)
	at org.apache.cassandra.service.StorageProxy$3.apply(StorageProxy.java:142)
	at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:385)
	at org.apache.cassandra.service.StorageProxy.applyCounterMutationOnLeader(StorageProxy.java:733)
	at org.apache.cassandra.db.CounterMutationVerbHandler.doVerb(CounterMutationVerbHandler.java:53)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.lang.UnsupportedOperationException
	at org.apache.cassandra.db.RowIndexEntry.deletionTime(RowIndexEntry.java:81)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader.<init>(IndexedSliceReader.java:109)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.createReader(SSTableSliceIterator.java:68)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:44)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:101)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:272)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1213)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1125)
	at org.apache.cassandra.db.SystemTable.getCurrentLocalCounterId(SystemTable.java:595)
	at org.apache.cassandra.utils.CounterId$LocalCounterIdHistory.<init>(CounterId.java:194)
	at org.apache.cassandra.utils.CounterId$LocalIds.<clinit>(CounterId.java:42)
	... 16 more
{code} 

The code causing it seems to have changed in 1.2.7 via https://issues.apache.org/jira/browse/CASSANDRA-5677",,aleksey,bretthoerner,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jul/13 21:56;jbellis;5814.txt;https://issues.apache.org/jira/secure/attachment/12594458/5814.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340326,,,Sat Jul 27 19:20:44 UTC 2013,,,,,,,,,,"0|i1mp3z:",340644,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"26/Jul/13 21:56;jbellis;Patch to read the deletion time from the row header when dealing with old sstable formats.;;;","27/Jul/13 15:58;aleksey;+1;;;","27/Jul/13 19:20;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQLSH Windows: TypeError: argument of type 'NoneType' is not iterable,CASSANDRA-5812,12660109,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,mickdelaney,mickdelaney,26/Jul/13 18:05,16/Apr/19 09:32,14/Jul/23 05:53,27/Jul/13 01:46,1.2.8,2.0 rc1,,Legacy/Tools,,,0,cqlsh,windows,,,"I downloaded Cassandra Beta 2. 
I've tried to use CQLSH against Cassandra. 

My python version is: 
Enthought Canopy Python 2.7.3 | 32-bit | (default, Mar 25 2013, 15:38:39) [MSC v.1500 32 bit (Intel)] on win32

I get the following exception when I run the utility:

c:\Servers\apache-cassandra\bin>python cqlsh 127.0.0.1 9160
Traceback (most recent call last):
  File ""cqlsh"", line 131, in <module>
    if readline is not None and 'libedit' in readline.__doc__:
TypeError: argument of type 'NoneType' is not iterable",Windows 8 64 Bit,aleksey,mickdelaney,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Jul/13 01:32;aleksey;5812.txt;https://issues.apache.org/jira/secure/attachment/12594499/5812.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340301,,,Sat Jul 27 01:46:35 UTC 2013,,,,,,,,,,"0|i1moyn:",340619,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"27/Jul/13 01:33;brandon.williams;+1;;;","27/Jul/13 01:46;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE during repair,CASSANDRA-5806,12659929,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,hsn,hsn,25/Jul/13 23:33,16/Apr/19 09:32,14/Jul/23 05:53,07/Aug/13 18:18,2.0 rc1,,,,,,0,,,,," INFO 01:06:00,656 Connecting to /10.0.0.3 for streaming
 INFO 01:06:00,656 Connecting to /10.0.0.3 for streaming
ERROR 01:06:05,828 Streaming error occurred
java.lang.NullPointerException
        at org.apache.cassandra.streaming.ConnectionHandler.sendMessage(Connecti
onHandler.java:175)
        at org.apache.cassandra.streaming.StreamSession.maybeCompleted(StreamSes
sion.java:600)
        at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.ja
va:446)
        at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSe
ssion.java:357)
        at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandl
er.run(ConnectionHandler.java:294)",,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340121,,,Wed Aug 07 18:18:18 UTC 2013,,,,,,,,,,"0|i1mnun:",340439,,,,,,,,,,,,Low,,2.0 beta 2,,,,,,,,,,,,,,,,"07/Aug/13 18:18;yukim;NPE was due to the wrong ordering of messaging and was fixed in commit 931be4803b1f21dc1605612364df128dce74a6ef.
If you see the error again in upcoming RC1 release, feel free to reopen the ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL 'set' returns incorrect value,CASSANDRA-5805,12659851,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,yukim,yukim,25/Jul/13 17:40,16/Apr/19 09:32,14/Jul/23 05:53,25/Jul/13 20:35,2.0 rc1,,,,,,0,,,,,"CQL 'set' returns incorrect value after flush.
Create the following table:

{code}
CREATE KEYSPACE ks WITH replication = {'class': 'SimpleStrategy', 'replication_factor': 1};
USE ks;
CREATE TABLE cf ( k int PRIMARY KEY , s set<int> );
{code}

Insert data:

{code}
INSERT INTO cf (k, s) VALUES (1, {1});
INSERT INTO cf (k, s) VALUES (1, {2});
{code}

This should return:

{code}
cqlsh:ks> SELECT * FROM cf;

 k | s
---+--------
 1 | {2}
{code}

and it does when no flush has happened.

But when I do flush after each insert, it starts returning:

{code}
cqlsh:ks> SELECT * FROM cf;

 k | s
---+--------
 1 | {1, 2}
{code}

'system.local' table flushes every time it inserts(updates) tokens, and this behavior is causing 'nodetool move' to act weirdly.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/13 19:00;slebresne;5805.txt;https://issues.apache.org/jira/secure/attachment/12594235/5805.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340043,,,Thu Jul 25 20:35:30 UTC 2013,,,,,,,,,,"0|i1mndb:",340361,,,,,,,,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,,"25/Jul/13 17:57;yukim;I tested with 1.2 branch with the same direction above, and I got the correct value.;;;","25/Jul/13 18:17;aleksey;Maps and Lists are also affected, but I'm not certain that the issue is necessarily collection-related.;;;","25/Jul/13 19:00;slebresne;That's a bug in the part specific to trunk from CASSANDRA-5677, my bad. Attaching patch to fix. I've also pushed a dtest that capture this bug.;;;","25/Jul/13 19:13;jbellis;+1;;;","25/Jul/13 20:35;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AntiEntropySession fails when OutboundTcpConnection receives IOException,CASSANDRA-5804,12659845,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,awinters,awinters,awinters,25/Jul/13 17:15,16/Apr/19 09:32,14/Jul/23 05:53,26/Jul/13 15:59,1.2.8,2.0 rc1,,,,,0,,,,,"When requesting merkle trees for a repair, if the OutboundTcpConnection grabbed from the connection pool is reset (java.io.IOException: Connection reset by peer), the target node is not marked as dead, the TREE_REQUEST is not retried, and the repair does not fail. Instead, the repair stalls waiting for the merkle tree response which will never arrive.",CentOS 6.3 x86_64,awinters,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/13 19:00;awinters;5804-v1.patch;https://issues.apache.org/jira/secure/attachment/12594234/5804-v1.patch","25/Jul/13 18:08;awinters;ioexception.txt;https://issues.apache.org/jira/secure/attachment/12594217/ioexception.txt",,,,,,,,,,,,,,,,,,,2.0,awinters,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,340037,,,Fri Jul 26 15:59:26 UTC 2013,,,,,,,,,,"0|i1mnbz:",340355,,,,,,,,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,,"25/Jul/13 18:08;awinters;TRACE level log showing the IOException thrown when sending TREE_REQUEST via MessagingService;;;","25/Jul/13 18:11;awinters;The dead connections are cross-datacenter, and I believe I have some network equipment _(naughty firewall)_ resetting idle connections. That'd be why gossip says the node is up, even though a pooled {{OutboundTcpConnection}} would be reset due to idleness.;;;","25/Jul/13 18:52;awinters;// if the message was important, such as a repair acknowledgement, put it back on the queue
// to retry after re-connecting.  See CASSANDRA-5393
if (e instanceof SocketException && qm.shouldRetry())

CASSANDRA-5393 was insufficient to completely fix the issue, because {{e instanceof IOException}} should have been used, instead. Looks like a simple fix.;;;","25/Jul/13 18:59;awinters;One-line fix to possibly retry for any IOException, not just SocketException.;;;","25/Jul/13 19:00;awinters;Patch against cassandra-1.2 git;;;","26/Jul/13 15:59;yukim;+1 and committed.
Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in HH metrics,CASSANDRA-5802,12659645,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,brandon.williams,brandon.williams,24/Jul/13 19:25,16/Apr/19 09:32,14/Jul/23 05:53,30/Jul/13 17:46,2.0 rc1,,,,,,0,,,,,"{noformat}
    [junit] Testcase: testCompactionOfHintsCF(org.apache.cassandra.db.HintedHandOffTest):	Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit] 	at com.google.common.base.Preconditions.checkNotNull(Preconditions.java:191)
    [junit] 	at com.google.common.cache.LocalCache.get(LocalCache.java:3989)
    [junit] 	at com.google.common.cache.LocalCache.getOrLoad(LocalCache.java:3994)
    [junit] 	at com.google.common.cache.LocalCache$LocalLoadingCache.get(LocalCache.java:4878)
    [junit] 	at com.google.common.cache.LocalCache$LocalLoadingCache.getUnchecked(LocalCache.java:4884)
    [junit] 	at org.apache.cassandra.metrics.HintedHandoffMetrics.incrCreatedHints(HintedHandoffMetrics.java:67)
    [junit] 	at org.apache.cassandra.db.HintedHandOffManager.hintFor(HintedHandOffManager.java:125)
    [junit] 	at org.apache.cassandra.db.HintedHandOffTest.testCompactionOfHintsCF(HintedHandOffTest.java:68)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.HintedHandOffTest FAILED
{noformat}",,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/13 18:16;thobbs;0001-Handle-no-matching-endpoint-for-hint-target.patch;https://issues.apache.org/jira/secure/attachment/12594751/0001-Handle-no-matching-endpoint-for-hint-target.patch",,,,,,,,,,,,,,,,,,,,1.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339837,,,Tue Jul 30 17:46:55 UTC 2013,,,,,,,,,,"0|i1mm3r:",340156,,,,,,,,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,,"29/Jul/13 18:16;thobbs;Patch 0001 gracefully handles the case where a matching endpoint cannot be found for the hint's targetID.;;;","30/Jul/13 17:46;yukim;+1 and committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE during validation compaction,CASSANDRA-5801,12659635,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,24/Jul/13 18:59,16/Apr/19 09:32,14/Jul/23 05:53,25/Jul/13 15:14,2.0 rc1,,,,,,0,,,,,"While repairing with vnodes enabled:

{noformat}
ERROR [ValidationExecutor:1] 2013-07-24 12:09:36,326 Validator.java (line 197) Failed creating a merkle tree for [repair #d13fd210-f483-11e2-b6fb-f1fe0a5dda64 on ks/cf, (9214460999857687863,-9209369219500956981]], /127.0.0.1 (see log for details)
ERROR [ValidationExecutor:1] 2013-07-24 12:09:36,328 CassandraDaemon.java (line 196) Exception in thread Thread[ValidationExecutor:1,1,main]
java.lang.AssertionError: -9191651187195735134 is not contained in (9214460999857687863,-9209369219500956981]
    at org.apache.cassandra.repair.Validator.add(Validator.java:136)
    at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:669)
    at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:64)
    at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:395)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
    at java.util.concurrent.FutureTask.run(FutureTask.java:166)
    at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
    at java.lang.Thread.run(Thread.java:722)
{noformat}",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/13 14:32;slebresne;5801.txt;https://issues.apache.org/jira/secure/attachment/12594184/5801.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339828,,,Thu Jul 25 15:14:22 UTC 2013,,,,,,,,,,"0|i1mm1r:",340147,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"25/Jul/13 14:32;slebresne;Seems the SSTableScanner implementation was still not right for wrapping ranges. Basically the patch from CASSANDRA-5757 was correctly seeking at the end of the first part of a wrapping range, but it was still returning one wrong key before starting reading from where it had seek to. Attaching patch to fix.;;;","25/Jul/13 15:04;brandon.williams;+1;;;","25/Jul/13 15:14;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support pre-1.2 release CQL3 tables in CqlPagingRecordReader,CASSANDRA-5800,12659618,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,alexliu68,alexliu68,24/Jul/13 16:54,16/Apr/19 09:32,14/Jul/23 05:53,13/Aug/13 21:30,1.2.9,,,,,,0,,,,,Pre-1.2 release CQL3 table stores the key in system.schema_columnfamilies key_alias column which is different from 1.2 release. We should support it in CqlPagingRecordReader as well.,,aleksey,alexliu68,bcoverston,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5803,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/13 17:06;alexliu68;5800-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12593980/5800-1.2-branch.txt","29/Jul/13 19:29;alexliu68;5800-2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12594770/5800-2-1.2-branch.txt","12/Aug/13 10:45;aleksey;5800-v3-1.2.txt;https://issues.apache.org/jira/secure/attachment/12597448/5800-v3-1.2.txt","12/Aug/13 10:47;aleksey;5800-v3-2.0.txt;https://issues.apache.org/jira/secure/attachment/12597449/5800-v3-2.0.txt",,,,,,,,,,,,,,,,,4.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339811,,,Tue Aug 13 21:30:49 UTC 2013,,,,,,,,,,"0|i1mlxz:",340130,,,,,,,,,thobbs,,thobbs,Low,,,,,,,,,,,,,,,,,,"24/Jul/13 17:06;alexliu68;Patch on 1.2 branch is attached;;;","24/Jul/13 23:52;jbellis;I don't understand what the goal is here.  CPRR is only part of 1.2.;;;","25/Jul/13 00:26;alexliu68;If the user upgrades from pre-1.2 release to 1.2 release, any CQL3 table created from pre-1.2 release doesn't work for CqlPagingRecordReader because it stores the key name in key_alias column instead of key_aliases column. The fix helps the upgrading.
;;;","25/Jul/13 01:14;jbellis;Sounds like the right fix is to propagate the column rename e.g. in SystemTable.upgradeSystemData. Other parts of the system need that b/s the RR.;;;","29/Jul/13 19:29;alexliu68;5800-2-1.2-branch.txt is attached to update the system.schema_columnfamilies settings for pre-1.2 CQL3 tables;;;","09/Aug/13 20:23;thobbs;The patch looks good except for a minor nit: I would rename {{upgradeCQL3tables()}} to {{migrateKeyAlias()}}, or something along those lines.;;;","09/Aug/13 20:25;jbellis;I'll tweak that on commit.;;;","09/Aug/13 20:57;jbellis;Committed (to 1.2 only; 2.0 requires upgrading from 1.2.9 already);;;","09/Aug/13 22:22;aleksey;Huh, let me bikeshed a little as well:
- catching NPE there is bad, idiomatically we check row.has(String column) first instead
- (key_alias != null && key_aliases == null) is not comprehensive enough. if key_alias is null, we still want to set key_aliases (to '[]'). so it should be if (key_aliases == null) instead.
- with the above in place, all the special-casing for key_aliases being null and all the references to key_alias should be dropped from the 2.0 codebase entirely, assuming that we already mandate 1.2.9 for upgraders;;;","09/Aug/13 22:33;aleksey;Also, if you detect key_alias, you should probably ditch it after the transformation (do a DELETE key_alias etc.);;;","09/Aug/13 23:21;aleksey;Will attach a v3.;;;","12/Aug/13 16:37;thobbs;Except for the cql3handling.py portion of the 2.0 patch (which you're already aware of), +1.;;;","12/Aug/13 17:23;aleksey;Thanks, committed.;;;","13/Aug/13 15:28;aleksey;The migration, currently, only runs on startup - and that doesn't cover the case of, for example, 1.2.8- -> 1.2.9+ -> 2.0 migration without a 1.2.9+ -> 1.2.9+ restart (you are going to have the 1.2.8- schema rows carried verbatim into 2.0).

We should perform this migration after each schema pull instead.
;;;","13/Aug/13 21:30;aleksey;Fixed by https://github.com/apache/cassandra/commit/82081c7b4b2ad2d602374f0817abc4cfc2c86c98;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Column can expire while lazy compacting it...,CASSANDRA-5799,12659571,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,frousseau,frousseau,24/Jul/13 13:37,16/Apr/19 09:32,14/Jul/23 05:53,26/Jul/13 18:15,1.2.7,,,,,,0,,,,,"Using TTL + range tombstones can lead to failure while lazy compacting rows.

Scenario to reproduce :
 - create an SSTable with one row and some columns and a TTL of 8 seconds
 - wait one second
 - create a second SSTable with the same rowkey as above, and add a range tombstone
 - start the first pass of the lazy compaction before the columns with TTL are expired
 - wait 10 seconds (enough for columns with TTL to expire)
 - continue lazy expiration
 - the following assertion will fail :
    [junit] junit.framework.AssertionFailedError: originally calculated column size of 1379 but now it is 1082
    [junit] 	at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:150)



",,christianmovi,efalcao,frousseau,ralph@massrelevance.com,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5720,,,,,,,,,,,"24/Jul/13 14:28;slebresne;5799.txt;https://issues.apache.org/jira/secure/attachment/12593942/5799.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339764,,,Thu Aug 22 07:58:06 UTC 2013,,,,,,,,,,"0|i1mlnj:",340083,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"24/Jul/13 13:45;frousseau;Note : it was initially reported here http://www.mail-archive.com/user@cassandra.apache.org/msg31277.html

The row size is around 153Mb, thus, compacting at 16Mb/s => around 9s to compact, which is an open window for columns to expire;;;","24/Jul/13 14:20;jbellis;If it requires compaction to take longer than TTL to be a problem, I'm inclined to say ""don't do that in 1.2.""  2.0 has single-pass compaction so should not matter there.;;;","24/Jul/13 14:28;slebresne;bq. If it requires compaction to take longer than TTL to be a problem

No, it only requires that a TTL expire between the first and second phase, which can happen whatever the TTL and compaction time is. For some reason, DeletionTime.isDeleted(), which is just supposed to check if the deletion time shadows a given column is completely broken (it checks if the columns is deleted which shouldn't even matter for that method) and depends on the current time. Attaching simple patch to fix.
;;;","24/Jul/13 14:35;jbellis;+1;;;","24/Jul/13 14:45;slebresne;Alright, committed, thanks;;;","27/Jul/13 20:34;efalcao;I was excited to see this fix in 1.2.7 but I fear it's still an issue. Here is a stack I just saw in my freshly upgraded 1.2.7 cluster:

{code}
ERROR [CompactionExecutor:17] 2013-07-27 17:24:31,132 CassandraDaemon.java (line 192) Exception in thread Thread[CompactionExecutor:17,1,RMI Runtime]
java.lang.AssertionError: originally calculated column size of 516898177 but now it is 516898234
	at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:355)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
{code}

Does it matter that my SSTables are mostly on version ic?

FWIW, I've been unable to compact this CF since we went to C* 1.2. I'll attach other JIRAs that may be related.
;;;","27/Jul/13 20:36;efalcao;Here's my earlier JIRA that might be related. Any thoughts, Sylvain?;;;","22/Aug/13 07:58;slebresne;bq. Does it matter that my SSTables are mostly on version ic?

I shouldn't matter, no.

bq. but I fear it's still an issue. Here is a stack I just saw in my freshly upgraded 1.2.7 cluster

It could be that there is still a problem. But I'm confident that we did fixed one issue with this ticket, so that would be a separate problem. Let's track that new problem in CASSANDRA-5720 then maybe. Can you indicate in that latter issue that you can reproduce on 1.2.7 btw (so we don't close it as a duplicate of something fixed).

bq. I've been unable to compact this CF since we went to C* 1.2.

If you have some sstables that always fail to compact with that error on 1.2.7, then would you be allowed to provide that set of sstables privately so I can check what's going on on my side?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DC-local CAS,CASSANDRA-5797,12659484,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,jbellis,jbellis,24/Jul/13 03:02,25/Oct/19 13:11,14/Jul/23 05:53,26/Jul/13 15:31,2.0 rc1,,,Feature/Lightweight Transactions,Legacy/CQL,,1,LWT,,,,"For two-datacenter deployments where the second DC is strictly for disaster failover, it would be useful to restrict CAS to a single DC to avoid cross-DC round trips.

(This would require manually truncating {{system.paxos}} when failing over.)",,aleksey,blair,pmcfadin,relgames,rschildmeijer,sayap,slebresne,soumava,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jul/13 12:51;slebresne;0001-Thrift-generated-files.txt;https://issues.apache.org/jira/secure/attachment/12594174/0001-Thrift-generated-files.txt","25/Jul/13 12:51;slebresne;0002-Add-LOCAL_SERIAL-CL.txt;https://issues.apache.org/jira/secure/attachment/12594175/0002-Add-LOCAL_SERIAL-CL.txt","25/Jul/13 12:51;slebresne;0003-CQL-and-native-protocol-changes.txt;https://issues.apache.org/jira/secure/attachment/12594176/0003-CQL-and-native-protocol-changes.txt",,,,,,,,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339677,,,Fri Jul 26 15:31:27 UTC 2013,,,,,,,,,,"0|i1ml4n:",339997,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"24/Jul/13 03:03;jbellis;Not sure what CQL syntax for this is.  Is it protocol level the way CL is?;;;","24/Jul/13 11:49;slebresne;bq. Not sure what CQL syntax for this is. Is it protocol level the way CL is?

That's a good question and I'm not really sure what's the right answer.

I think it may make the most sense to make it protocol level because of reads.  For CAS writes, we do have a CQL syntax for it, so we could extends it with say:
{noformat}
UPDATE foo SET v1 = 2, v2 = 3 WHERE k = 1 IF v1 = 1 AND v2 = 1 IN LOCAL DC
{noformat}
But for reads, we don't have any syntax, the consistency level (SERIAL) is the only thing that makes a read go through paxos, so I'm afraid adding some CQL syntax in that case would be confusing.

But even making it protocol level is not that easy. For thrift, on the read side, the only way I can see us supporting this DC-local CAS would be add a LOCAL_SERIAL consistency level (short of duplicating all read methods for CAS reads that is). But that doesn't really work for writes since the consistency level for writes is really the consistency of the paxos learn/commit phase.

One option (the best I can come up with so far) would be to add the LOCAL_SERIAL consistency level, and then to change CAS write to take 2 CL: the first one would be for the commit (learn) phase (as we have now, but we would refuse CL.SERIAL and CL.LOCAL_SERIAL in that case) and a 2nd CL that would control the ""Paxos consistency"" (and for that one, only CL.SERIAL or CL.LOCAL_SERIAL would be valid). It's not perfect however because the one thing you can't properly express is the ability to do CL.SERIAL for paxos but don't wait on any node for the learn phase. Unless we make CL.ANY for the ""commit consistency"" mean that, but that's a slight stretch.

In any case, we should probably make it sure to shove that in 2.0.0 because I don't want to change the thrift API nor break the native protocol in 2.0.1.

Any better idea?
;;;","24/Jul/13 15:14;jbellis;Sounds reasonable, although I think it would be better to come up w/ a different enum for the Paxos phases than re-use CL, most of whose options are not appropriate.

I actually think CL.ANY on commit is fine.;;;","24/Jul/13 15:23;slebresne;bq. although I think it would be better to come up w/ a different enum for the Paxos phases than re-use CL

The thing is that for reads, we must have SERIAL and LOCAL_SERIAL in CL if we want thrift to support it. So once we have them in CL, is it really worth adding a separate enum for the write case? (honest question, I'm fine doing it, just wonder if it's worth bothering since things will be mixed up for reads anyway).;;;","24/Jul/13 15:27;pmcfadin;I like LOCAL_SERIAL over ANY. It makes a closer match to LOCAL_QUORUM in that it's not meant to cross datacenter boundaries. There is enough confusion about ANY as it is and I think this would simplify things.;;;","24/Jul/13 16:33;slebresne;bq. I like LOCAL_SERIAL over ANY

I think there is some confusion. The suggestion of CL.ANY was for the commit part of Paxos. That part is basically a standard write (that happens after the paxos algorithm has unfolded but does impact the visibility of the CAS write by non-serial reads). For that, LOCAL_SERIAL don't really make sense imo (it's ""wrong"" even). ANY does is what match the most what happens, because you are guaranteed the write is replicated somewhere (paxos ensures that) but you may not be able to see your write right away with normal reads, even at CL.ALL (which is also something that CL.ANY).  ;;;","24/Jul/13 23:51;jbellis;bq. The thing is that for reads, we must have SERIAL and LOCAL_SERIAL in CL if we want thrift to support it. So once we have them in CL, is it really worth adding a separate enum for the write case?

The problem is that none of {ANY, ONE, TWO, THREE, LOCAL_QUORUM, EACH_QUORUM} are valid on writes, which isn't very clear if we reuse CL for everything.

Then again we ANY is not a valid CL for read, and EACH_QUORUM is not valid for writes.  I dunno.;;;","25/Jul/13 12:51;slebresne;Attaching patch for this. I've currently stayed with the idea of 2 CL for writes. One small reason for which it is somewhat convenient is that when we throw a write timeout exception, we need to ship the consistency level. And when that timeout happens during the paxos prepare/propose phases, returning CL.SERIAL or CL.LOCAL_SERIAL make the most sense, so using CL as argument of the method in the first place is somewhat consistent. Anyway, it's certainly possible to add a new enum for that instead, but I don't think that' too horrible as is.

There's 3 patches: the first one is just the update to the thrift generated files and can be largely ignored. The 2nd one does the main change but does not change CQL. The 3rd patch is the CQL and native protocol change. That latter patch is a tad big because while adding the new ""serial consistency level"", I realized this was a pain with the current code and that in the protocol v2, QUERY and EXECUTE had basically the same parameters but they were set out in different order (in the protocol) which was killing all possibility of code reuse for no good reason. So I decided to go ahead and refactor that more cleanly since there's no point in making the life of client implementors harder for no good reason.

Anyway, moving this issue to 2.0.0 because it changes the native protocol and thrift, so we really should have it in 2.0.
;;;","26/Jul/13 14:14;jbellis;+1 (mostly looked at patch 2)

Nit: rename isSameDCThan to isSameDCAs, or maybe better sameDCPredicateFor (since ""is"" implies it's doing a boolean evaluation);;;","26/Jul/13 15:30;aleksey;QueryOptions.Codec.encode() writes flags before writing the CL - should be reversed. Otherwise LGTM.;;;","26/Jul/13 15:31;slebresne;Alright, committed (with nit fixed). Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
now() is being rejected in INSERTs when inside collections,CASSANDRA-5795,12659472,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,aleksey,aleksey,24/Jul/13 00:52,16/Apr/19 09:32,14/Jul/23 05:53,24/Jul/13 15:49,1.2.7,,,,,,0,,,,,"Lists, Sets and Maps reject NonTerminal terms in prepare:

{code}
    if (t instanceof Term.NonTerminal)
                    throw new InvalidRequestException(String.format(""Invalid list literal for %s: bind variables are not supported inside collection literals"", receiver));
{code}

and now() is instanceof NonTerminal since CASSANDRA-5616, hence

{noformat}
cqlsh:test> insert into demo (id, timeuuids) values (0, [now()]);
Bad Request: Invalid list literal for tus: bind variables are not supported inside collection literals
{noformat}",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jul/13 07:58;slebresne;5795.txt;https://issues.apache.org/jira/secure/attachment/12593881/5795.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339665,,,Wed Jul 24 15:49:24 UTC 2013,,,,,,,,,,"0|i1ml1z:",339985,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"24/Jul/13 07:58;slebresne;CASSANDRA-5616 was indeed slightly more work that I though for collections, my bad. Attaching patch to fix (on the plus side, if we ever want to support bind markers inside collections (not that I think it's useful), the code will be mostly there). I've pushed a dtest too.;;;","24/Jul/13 15:28;aleksey;+1

nits:
- The copy-pasted DelayedValue code from Lists in Sets still references 'List' in bind (""List value too long"")
;;;","24/Jul/13 15:49;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OPP seems completely unsupported,CASSANDRA-5793,12659295,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,varakumar,varakumar,varakumar,23/Jul/13 09:42,16/Apr/19 09:32,14/Jul/23 05:53,30/Jul/13 20:40,1.2.9,,,,,,0,,,,,"We were using 0.7.6 version. And upgraded to 1.2.5 today. We were using OPP (OrderPreservingPartitioner).

OPP throws error when any node join the cluster. Cluster can not be brought up due to this error. After digging little deep, We realized that ""peers"" column family is defined with key as type ""inet"". Looks like many other column families in system keyspace has same issue.

Exception trace:
java.lang.RuntimeException: The provided key was not UTF8 encoded.
	at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:172)
	at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
	at org.apache.cassandra.db.Table.apply(Table.java:379)
	at org.apache.cassandra.db.Table.apply(Table.java:353)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:258)
	at org.apache.cassandra.cql3.statements.ModificationStatement.executeInternal(ModificationStatement.java:117)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:172)
	at org.apache.cassandra.db.SystemTable.updatePeerInfo(SystemTable.java:258)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1231)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1948)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:823)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:901)
	at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:50)


Possibilities:
- Changing partitioner to BOP (or something else) fails while loading schema_keyspaces. So, it does not look like an option.
- One possibility is that getToken of OPP can return hex value if it fails to encode bytes to UTF-8 instead of throwing error. By this system tables seem to be working fine with OPP.
- Or Completely remove OPP from code base & configuration files. Mark clearly that OPP is no longer supported in upgrade instructions.",Cassandra on Ubuntu,jeromatron,varakumar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,varakumar,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339488,,,Tue Jul 30 20:40:14 UTC 2013,,,,,,,,,,"0|i1mjyn:",339808,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"23/Jul/13 10:32;jeromatron;See https://github.com/apache/cassandra/blob/cassandra-1.2.6/NEWS.txt#L184-L186

You can't change partitioner once the data has been written.  Rows of data are ordered in sstables according to the partitioner.  You can upgrade to 1.1.x or stay at 0.7, then using Hadoop or a batch job, you can read from your existing cluster and write to a different cluster running 1.2.6 with your new partitioner.;;;","23/Jul/13 12:59;jeromatron;I suppose I was assuming that you were using the now removed COPP, but since you're using just the OPP, that *should* work.  Sorry for the confusion.;;;","23/Jul/13 13:31;jbellis;bq. One possibility is that getToken of OPP can return hex value if it fails to encode bytes to UTF-8 instead of throwing error.

I can't think of how it would break anything to accept keys we previously rejected.;;;","30/Jul/13 08:21;varakumar;Should we handle in this way (return hex value if OPP fails to encode bytes to UTF-8 instead of throwing error) & mark OPP as unsupported in relevant documentation?
;;;","30/Jul/13 20:40;jbellis;Made the change to OPP in 2ccbe3c6f547511e79454b79cbef682ef8a6973a.

cassandra.yaml has noted that OPP is deprecated in favor of BOP for... years.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Buffer Underflow during streaming,CASSANDRA-5792,12659202,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,brandon.williams,brandon.williams,22/Jul/13 21:33,16/Apr/19 09:32,14/Jul/23 05:53,30/Jul/13 22:59,2.0 rc1,,,,,,0,,,,,"{noformat}
ERROR [STREAM-IN-/127.0.0.3] 2013-07-22 16:19:50,597 StreamSession.java (line 414) Streaming error occurred
java.nio.BufferUnderflowException
    at java.nio.Buffer.nextGetIndex(Buffer.java:492)
    at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:135)
    at org.apache.cassandra.streaming.messages.StreamMessage.deserialize(StreamMessage.java:52)
    at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:288)
    at java.lang.Thread.run(Thread.java:722)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/13 15:41;yukim;5792.txt;https://issues.apache.org/jira/secure/attachment/12593712/5792.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339395,,,Tue Jul 30 23:00:06 UTC 2013,,,,,,,,,,"0|i1mjdz:",339715,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"23/Jul/13 15:41;yukim;There is a chance we get nothing from socket,  like when socket gets closed.
Patch attached to check we actually read something from socket.;;;","30/Jul/13 21:32;jbellis;+1;;;","30/Jul/13 23:00;yukim;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
StorageProxy#cas() doesn't order columns names correctly when querying,CASSANDRA-5788,12659037,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,22/Jul/13 08:32,25/Oct/19 13:11,14/Jul/23 05:53,22/Jul/13 13:50,2.0 beta 2,,,Feature/Lightweight Transactions,,,0,LWT,,,,"When querying columns for CAS, we build the SortedSet with:
{noformat}
new NamesQueryFilter(ImmutableSortedSet.copyOf(expected.getColumnNames())
{noformat}
but ImmutableSortedSet.copyOf() uses the natural order of keys unless a comparator is given, which is not what we want.",,appodictic,slebresne,soumava,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/13 08:43;slebresne;5788.txt;https://issues.apache.org/jira/secure/attachment/12593471/5788.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339230,,,Tue Jul 23 16:07:34 UTC 2013,,,,,,,,,,"0|i1midb:",339550,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"22/Jul/13 08:43;slebresne;Trivial patch attached.;;;","22/Jul/13 13:41;jbellis;+1;;;","22/Jul/13 13:50;slebresne;Committed, thanks;;;","23/Jul/13 15:51;appodictic;Can we re-open so I can put a unit test around this? I see we said this is trivial but I think a test could help prevent this from happening again.;;;","23/Jul/13 16:07;jbellis;We've rolled a release w/ this now so probably best to make a new ticket, sorry.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Commit#updatesWithPaxosTime should also update the row and range tombstones,CASSANDRA-5787,12659031,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,22/Jul/13 07:27,16/Apr/19 09:32,14/Jul/23 05:53,22/Jul/13 13:48,2.0 beta 2,,,,,,0,,,,,Rows and range tombstones should both also respect the paxos timestamp otherwise an update may contradict the serialization order decided by paxos.,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/13 07:28;slebresne;5787.txt;https://issues.apache.org/jira/secure/attachment/12593465/5787.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339224,,,Mon Jul 22 13:48:02 UTC 2013,,,,,,,,,,"0|i1mibz:",339544,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"22/Jul/13 13:39;jbellis;+1;;;","22/Jul/13 13:48;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift cas() method crashes if input columns are not sorted.,CASSANDRA-5786,12659025,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,22/Jul/13 06:55,25/Oct/19 13:11,14/Jul/23 05:53,22/Jul/13 13:47,2.0 beta 2,,,Feature/Lightweight Transactions,,,0,LWT,,,,"CassandraServer#cas() use UnsortedColumns for the ""updates"", which might result later to a
{noformat}
java.lang.AssertionError: Added column does not sort as the last column
        at org.apache.cassandra.db.ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:115)
        at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:117)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:119)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:96)
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:91)
        at org.apache.cassandra.service.paxos.Commit$CommitSerializer.deserialize(Commit.java:139)
        at org.apache.cassandra.service.paxos.Commit$CommitSerializer.deserialize(Commit.java:128)
        at org.apache.cassandra.net.MessageIn.read(MessageIn.java:99)
        at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:175)
        at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:135)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:82)
{noformat}",,slebresne,soumava,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/13 08:02;slebresne;5786.txt;https://issues.apache.org/jira/secure/attachment/12593467/5786.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339218,,,Mon Jul 22 13:47:49 UTC 2013,,,,,,,,,,"0|i1mian:",339538,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"22/Jul/13 07:10;slebresne;Attaching simple patch to switch to TreeMap. I'm also including a minor optimization that switch to ArraySorted when we clone the updates in Commit#updatesWithPaxosTime.;;;","22/Jul/13 08:02;slebresne;Disregard the comment above. For normal updates we keep columns unsorted until they are applied to the memtable and we should probably do that here too for consistency. So attaching patch that makes sure we don't assume the columns are sorted when deserializing for paxos.;;;","22/Jul/13 13:40;jbellis;+1;;;","22/Jul/13 13:47;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificationException during streaming,CASSANDRA-5782,12658861,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,20/Jul/13 00:56,16/Apr/19 09:32,14/Jul/23 05:53,22/Jul/13 13:48,2.0 beta 2,,,,,,0,,,,,"During repair:

{noformat}
 INFO [STREAM-IN-/127.0.0.1] 2013-07-19 19:42:28,270 StreamSession.java (line 602) Flushing memtables for [CFS(Keyspace='ks', ColumnFamily='cf')]...
 INFO [STREAM-IN-/127.0.0.2] 2013-07-19 19:42:28,270 StreamSession.java (line 602) Flushing memtables for [CFS(Keyspace='ks', ColumnFamily='cf')]...
ERROR [STREAM-IN-/127.0.0.2] 2013-07-19 19:42:28,292 StreamSession.java (line 410) Streaming error occurred
java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:894)
    at java.util.HashMap$ValueIterator.next(HashMap.java:922)
    at org.apache.cassandra.streaming.ConnectionHandler.sendMessages(ConnectionHandler.java:169)
    at org.apache.cassandra.streaming.StreamSession.startStreamingFiles(StreamSession.java:624)
    at org.apache.cassandra.streaming.StreamSession.prepare(StreamSession.java:445)
    at org.apache.cassandra.streaming.StreamSession.messageReceived(StreamSession.java:358)
    at org.apache.cassandra.streaming.ConnectionHandler$IncomingMessageHandler.run(ConnectionHandler.java:294)
    at java.lang.Thread.run(Thread.java:722)
{noformat}",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jul/13 06:45;slebresne;5782.txt;https://issues.apache.org/jira/secure/attachment/12593457/5782.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339054,,,Mon Jul 22 13:48:17 UTC 2013,,,,,,,,,,"0|i1mha7:",339374,,,,,,,,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,,"22/Jul/13 06:45;slebresne;I think racing between the qeuing of the messages and the completion of the first ones is what might lead to that. Attaching a trivial patch to fix that.;;;","22/Jul/13 13:32;yukim;+1;;;","22/Jul/13 13:48;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Providing multiple keys to sstable2json results in invalid json,CASSANDRA-5781,12658853,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cnlwsu,cnlwsu,cnlwsu,19/Jul/13 23:13,16/Apr/19 09:32,14/Jul/23 05:53,20/Jul/13 19:18,1.2.7,,,Legacy/Tools,,,0,,,,,"if you pass multiple keys via the -k parameter to sstable for json the 2nd row will be appended to the end of the first without a comma.  It would look like so:
{code}
sstable2json foo-Data.db -k key1 -k key2 -k key3 -k key4
{
key1 : [[]...]key2: [[]...],
key3 : [[]...],
key4 : [[]...]
}
{code}


",,cnlwsu,dbrosius,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/13 23:14;cnlwsu;patch;https://issues.apache.org/jira/secure/attachment/12593294/patch",,,,,,,,,,,,,,,,,,,,1.0,cnlwsu,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,339046,,,Sat Jul 20 19:18:19 UTC 2013,,,,,,,,,,"0|i1mh8f:",339366,,,,,,,,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,,"19/Jul/13 23:15;cnlwsu;patch to print comma before instead of after row
https://github.com/clohfink/cassandra/commit/b7307a618090cd802c42f34482723a78f720f79a;;;","20/Jul/13 19:18;dbrosius;committed to cassandra-1.2 as 089f92b1a04cbcc7d391ea7e61d90df56174ad73;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sudden NPE while accessing Cassandra with CQL driver,CASSANDRA-5779,12658773,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,irengrig,irengrig,19/Jul/13 15:43,16/Apr/19 09:32,14/Jul/23 05:53,22/Jul/13 09:25,1.2.7,,,,,,0,,,,,"java.lang.NullPointerException at org.apache.cassandra.db.RowMutation.addOrGet(RowMutation.java:153)
 at org.apache.cassandra.cql3.statements.UpdateStatement.mutationForKey(UpdateStatement.java:216)
 at org.apache.cassandra.cql3.statements.UpdateStatement.getMutations(UpdateStatement.java:133)
 at org.apache.cassandra.cql3.statements.BatchStatement.getMutations(BatchStatement.java:99)
at org.apache.cassandra.cql3.statements.ModificationStatement.execute(ModificationStatement.java:88)
at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:118)
at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:128)
at org.apache.cassandra.transport.messages.QueryMessage.execute(QueryMessage.java:87)
at org.apache.cassandra.transport.Message$Dispatcher.messageReceived(Message.java:287)
at org.jboss.netty.channel.SimpleChannelUpstreamHandler.handleUpstream(SimpleChannelUpstreamHandler.java:75)
at org.jboss.netty.channel.DefaultChannelPipeline.sendUpstream(DefaultChannelPipeline.java:565)
at org.jboss.netty.channel.DefaultChannelPipeline$DefaultChannelHandlerContext.sendUpstream(DefaultChannelPipeline.java:793)
at org.jboss.netty.handler.execution.ChannelUpstreamEventRunnable.doRun(ChannelUpstreamEventRunnable.java:45)
at org.jboss.netty.handler.execution.ChannelEventRunnable.run(ChannelEventRunnable.java:69)
at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
at java.lang.Thread.run(Thread.java:724)
",,irengrig,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,338967,,,Mon Jul 22 09:25:29 UTC 2013,,,,,,,,,,"0|i1mgqv:",339287,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"19/Jul/13 16:36;slebresne;Is that possible that you've dropped a table on which you were writing?;;;","21/Jul/13 13:35;irengrig;I rechecked logs, and see that indeed, we have been dropping keyspace asynchronously (that was in test),
maybe it's our error to not synchronize flushing buffers vs finishing execution...

However, I suppose that NPE is not good anyway, I'd rather prefer to get ""table doesn't exist"" message;;;","22/Jul/13 09:25;slebresne;bq. However, I suppose that NPE is not good anyway, I'd rather prefer to get ""table doesn't exist"" message

I agree with you in theory. Ideally, we'd want to throw a meaningful error message for the query. However, the problem here is that you dropped the keyspace concurrently of inserting in that keyspace. And making sure we catch such race properly in all cases would, while probably possible, be prohibitive performance wise for queries (and distribution makes this even worst). It's just not worse it.

So at the end of the day, concurrently dropping a table/keyspace while requesting it (reads or writes) is just not supported you should synchonize client side to avoid it.

For what it's worth, I've committed a small change (commit 86a077a190a1cec89112b9f1fd991f0fe06d2423) so that this case shouldn't happen anymore. Instead, in that same situation, an assertion error would get triggered that should at least have a more meaningful error message. And so I'll consider this ""fixed"".  But:
# this will still not be an error returned to the client. So really, you should synchronize client side.
# I do not pretend that a race between a keyspace drop and inserts can't yield a NPE anywhere else in the code.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Native protocol event don't respect the protocol version,CASSANDRA-5778,12658754,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,19/Jul/13 14:25,16/Apr/19 09:32,14/Jul/23 05:53,19/Jul/13 15:23,2.0 beta 2,,,,,,0,,,,,"Currently, the protocol version is on a per-message basis only. When we get a request, we respond with a message on the same protocol version. This is however broken for server events, that are responses to no request and currently always default to version 2 (on trunk), even if the client is on version 1.

So instead, we need to force a version per connection, and event messages should use that.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/13 14:27;slebresne;5578.txt;https://issues.apache.org/jira/secure/attachment/12593201/5578.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,338948,,,Fri Jul 19 15:23:33 UTC 2013,,,,,,,,,,"0|i1mgmn:",339268,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"19/Jul/13 15:11;aleksey;+1;;;","19/Jul/13 15:23;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix broken streaming retry,CASSANDRA-5775,12658570,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,yukim,yukim,18/Jul/13 17:40,16/Apr/19 09:32,14/Jul/23 05:53,22/Jul/13 13:52,2.0 beta 2,,,,,,0,streaming,,,,Current streaming 2.0 retry is broken since the receiver would keep on reading file even when something bad happens and continue to throw exceptions.,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jul/13 19:06;yukim;0001-fix-retry-streaming.patch;https://issues.apache.org/jira/secure/attachment/12593237/0001-fix-retry-streaming.patch",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,338764,,,Mon Jul 22 13:52:26 UTC 2013,,,,,,,,,,"0|i1mfhr:",339084,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"18/Jul/13 17:43;yukim;Attaching patch for fix.
I introduced new message to ACK upon file receiving. Sender will keep the file reference until receiving ACK, and when it receives RETRY instead, it then sends the file again.;;;","19/Jul/13 08:26;slebresne;When we do retry, shouldn't we move back the file from waitingForAck to files?  Feels wrong not to do it, if only for reporting sakes. And is there a reason why fileSent is guarded by 'if (files.containsKey(sequenceNumber))'? I mean, intuitively it seems that this should always be true, so we should assert it, not silently ignore it if it's not true.

Also, I think expecting that the file is in waitingForAck in STT.createMessageForRetry() is slightly racy. In theory, we could get the retry messge before the code has gone through fileSent() (it's very unlikely, but not totally impossible).

Nit: In (Compressed)StreamReader, I think we can remove the (duplicate) line before the 'while (toSkip > 0)'.;;;","19/Jul/13 19:06;yukim;Updated patch attached.
I think I can just simplify the patch without waitForAck.
Also, StreamReader is refactored to remove duplicated codes.;;;","22/Jul/13 06:37;slebresne;+1;;;","22/Jul/13 13:52;yukim;Committed. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix system.schema_triggers-related trigger/schema issues,CASSANDRA-5774,12658342,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,17/Jul/13 19:34,16/Apr/19 09:32,14/Jul/23 05:53,22/Jul/13 23:03,2.0.0,,,,,,0,triggers,,,,"Among other things, the patch does the following:
- adds missing schema_triggers to MigrationManager.resetLocalSchema()
- adds missing schema_triggers to SystemKeyspace.serializeSchema() - so that triggers would be part of schema version calculation
- adds missing schema_triggers to DefsTables.flushSchemaCFs()
- adds missing triggers to CFMetaData.toSchema(), so that CFs created via thrift with triggers from the beginning would serialize triggers
- removes triggers from CFMetaData.newIndexMetadata(), so that 2i CFs wouldn't inherit the triggers from the parent CF

There are other minor and not so minor changes, but these were the most critical ones. The patch also (unnecessarily) cleans up ColumnDefinition, but that was done to make it consistent with the new TriggerDefinition class.

The bulk of the patch is the updated thrift-gen files.

",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/13 19:35;aleksey;5774.txt;https://issues.apache.org/jira/secure/attachment/12592836/5774.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,338536,,,Mon Jul 22 23:03:28 UTC 2013,,,,,,,,,,"0|i1me33:",338856,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"22/Jul/13 15:40;jbellis;Do we need a mutable map in the CFDefinition copies?

Should probably move the to/from/deleteFrom Schema code into SystemTable for consistency.

Would prefer to split the ColumnDefinition cleanup (others?) to a separate commit, ninja is fine.

Otherwise LGTM.;;;","22/Jul/13 20:41;aleksey;bq. Do we need a mutable map in the CFDefinition copies?

Right now we do.

bq. Should probably move the to/from/deleteFrom Schema code into SystemTable for consistency.

it's currently in TriggerDefinition for consistency with ColumnDefinition, which follows very similar structure. If we move it to SystemTables, then we should move both. So I'm gonna keep it as is in this commit, and maybe move all of schema read/write code from both ColumnDefinition and TriggerDefinition in that ninja commit.

bq. Would prefer to split the ColumnDefinition cleanup (others?) to a separate commit, ninja is fine.

will do;;;","22/Jul/13 21:25;jbellis;Ship it!;;;","22/Jul/13 23:03;aleksey;Pushed in separate commits, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Small bug in Range.intersects(Bound),CASSANDRA-5771,12658301,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,17/Jul/13 17:06,16/Apr/19 09:32,14/Jul/23 05:53,18/Jul/13 06:57,1.2.7,,,,,,0,,,,,Range.intersects(Bound) doesn't correctly handle the case where the bound in argument has the same start and end (i.e. is a Bound of 1 value). It basically always return true in that case.,,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5757,,"17/Jul/13 17:07;slebresne;5771.txt;https://issues.apache.org/jira/secure/attachment/12592798/5771.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,338495,,,Thu Jul 18 06:57:06 UTC 2013,,,,,,,,,,"0|i1mdtz:",338815,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"17/Jul/13 17:07;slebresne;Attaching patch to fix.;;;","17/Jul/13 18:07;jbellis;+1, although I'm not sure what a punition is.;;;","18/Jul/13 06:57;slebresne;Aren't you supposed to speak french !? :)

Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Minor bugs in the native protocol v2 on 2.0.0-beta1,CASSANDRA-5770,12658292,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,17/Jul/13 16:26,16/Apr/19 09:32,14/Jul/23 05:53,19/Jul/13 06:50,2.0 beta 2,,,,,,0,,,,,There is a few minor bugs in the new features of the native protocol. Attaching patch to fix those.,,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/13 16:27;slebresne;5770.txt;https://issues.apache.org/jira/secure/attachment/12592792/5770.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,338486,,,Fri Jul 19 06:50:33 UTC 2013,,,,,,,,,,"0|i1mdrz:",338806,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"18/Jul/13 20:16;aleksey;+1;;;","19/Jul/13 06:50;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Not all STATUS_CHANGE UP events reported via the native protocol,CASSANDRA-5769,12658251,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,baldrick,baldrick,17/Jul/13 13:06,16/Apr/19 09:32,14/Jul/23 05:53,19/Jul/13 06:54,1.2.7,,,Legacy/CQL,,,0,,,,,"Not all gossip UP events are pushed to native protocol users who have registered for them.  This seems to be a native protocol issue because nodes themselves get the UP event (as seen in their logs).  I can consistently reproduce this issue as follows:

1) connect a client to a cluster node (""node1"") using the native protocol, register for TOPOLOGY_CHANGE and STATUS_CHANGE events.  (Probably you only need to register for STATUS_CHANGE to see this, however my client registers for both).
2) on another node (""node2""), send SIGSTOP to the Cassandra process.
3) after about 30 seconds the client gets pushed a STATUS_CHANGE DOWN event for the stopped node.
4) on node2, send SIGCONT to the the Cassandra process.
5) wait forever to get a STATUS_CHANGE UP event.  This is failure: no event is ever received.

Observe that node1 does know that node2 is back up: in its system log I see for example
  INFO [GossipStage:1] 2013-07-17 14:27:41,238 Gossiper.java (line 771) InetAddress /172.18.34.169 is now UP
shortly after sending SIGCONT to the stopped process.

To eliminate the possibility that my client is at fault, I performed the following sanity check:

2') on node2, stopped Cassandra nicely using: sudo service cassandra stop
4') on node2, restarted Cassandra using: sudo service cassandra start

In this case the client soon after gets a STATUS_CHANGE DOWN event followed by a STATUS_CHANGE UP event for node2.","Uubuntu 12.04, x86, 64 bit",baldrick,jasobrown,slebresne,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/13 12:27;slebresne;5769.txt;https://issues.apache.org/jira/secure/attachment/12592963/5769.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,338445,,,Fri Jul 19 06:54:08 UTC 2013,,,,,,,,,,"0|i1mdiv:",338765,,,,,,,,,jasobrown,,jasobrown,Low,,,,,,,,,,,,,,,,,,"17/Jul/13 18:06;jbellis;Tyler, can you reproduce?;;;","17/Jul/13 18:40;thobbs;Yes, I can reliably reproduce the issue (through the python driver) with those steps using ccm.;;;","18/Jul/13 12:27;slebresne;We are currently calling the native protocol notification in SS.handleStateNormal(), but that's only called on major state changes, while in the this case there is no generation change.

I wouldn't claim the gossiper code is always easy to follow but it seems that moving the notification code from SS.handleStateNormal() to SS.onAlive() ensures we'll always notify (without over-notifying) so attaching a patch to do that. I've checked it does fix the notification in the case above.
;;;","18/Jul/13 22:06;jasobrown;patch lgtm, although I might debate your claim about the gossiper code - it's not that bad :);;;","19/Jul/13 06:54;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"If a Seed can't be contacted, a new node comes up as a cluster of 1",CASSANDRA-5768,12658249,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,aecobley,aecobley,17/Jul/13 12:51,16/Apr/19 09:32,14/Jul/23 05:53,18/Jul/13 21:18,1.2.7,2.0 beta 2,,,,,0,,,,,"Setting up a new test cluster using  2.0.0-beta1 and I noticed the following behaviour with vnodes turned on.  

I bring up one node all well and good.  however if I bring up a second node, that can't contact the first (the first being the seed for the second) after a short period of time, the second goes ahead and assumes it's the only node and bootstraps with all tokens.  

NOTE also this email from Robert Coli 
To: user@cassandra.apache.org
Obviously if you have defined a seed and cannot contact it, the node should not start as a cluster of one. I have a to-do list item to file a JIRA on the subject, but if you wanted to file and link us, that'd be super. :)


Startup trace (from the can't contact the seed messages below).

http://aep.appspot.com/display/ABcWltCES1srzPrj5CkS69-GB8o/",,aecobley,jasobrown,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/13 17:13;brandon.williams;5768.txt;https://issues.apache.org/jira/secure/attachment/12592799/5768.txt","18/Jul/13 18:32;aecobley;cassandra.yaml;https://issues.apache.org/jira/secure/attachment/12593026/cassandra.yaml",,,,,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,338443,,,Wed Jul 24 22:21:40 UTC 2013,,,,,,,,,,"0|i1mdif:",338763,,,,,,,,,jasobrown,,jasobrown,Low,,,,,,,,,,,,,,,,,,"17/Jul/13 17:13;brandon.williams;It seems reasonable to me that we should ensure we've contacted a seed during bootstrap when one has been set. Patch to check for this and refuse if it fails.;;;","18/Jul/13 05:33;jasobrown;lgtm. One (incredibly minor) optimization in Gossiper.checkSeedContact() would be to check the boolean before checking the map for the seed ep, like this:

{code}protected void checkSeedContact(InetAddress ep)
{
    if (!seedContacted && seeds.contains(ep))
        seedContacted = true;
}
{code}

It could be that the effort of me writing this comment is greater than the grand sum of all processing time saved by this optimization, but what the hell :).

UPDATE: Thought about this some more, but what if you only have one seed in the cluster, and that node is starting up, I'm not sure it will ever set the Gossiper.seedContacted to true as it's only in the GossipDigest*VerbHandlers where we call checkSeedContact(). Thus, the seed won't start up (unless I've missed something).

Perhaps in Gossiper.buildSeedsList() we can do this:

{code}private void buildSeedsList()
{
    List<InetAddress> seeds = DatabaseDescriptor.getSeeds(); 
    for (InetAddress seed : seeds)
    {
        if (seed.equals(FBUtilities.getBroadcastAddress()) && seeds.size == 1)
            seedContacted = true;
        else
            seeds.add(seed);
    }
}{code}

(or something similar to this)

This way if there's only one seed (and this node is it), it can still come up. Otherwise, if there are other seeds besides this one, maybe wait to set the seedContacted? (not sure about that, though).


;;;","18/Jul/13 05:35;jasobrown;Also, is there any harm in adding this to 1.2? Seems rather innocuous, I think.;;;","18/Jul/13 08:33;aecobley;I tried 5768.txt on cassandra-trunk.  It does halt cassandra if the seed can't be connected (with  java.lang.IllegalStateException: Unable to contact any seeds!)

However it also halts with the same exception if seeds is set to 127.0.0.1 (the default in cassandra.yaml):

seed_provider:
    # Addresses of hosts that are deemed contact points.
    # Cassandra nodes use this list of hosts to find each other and learn
    # the topology of the ring.  You must change this if you are running
    # multiple nodes!
    - class_name: org.apache.cassandra.locator.SimpleSeedProvider
      parameters:
          # seeds is actually a comma-delimited list of addresses.
          # Ex: ""<ip1>,<ip2>,<ip3>""
          - seeds: ""127.0.0.1""

So thats not correct I guess.;;;","18/Jul/13 09:03;aecobley;Jason,

You code for build list should read:

{code}

private void buildSeedsList()
    {
        Set<InetAddress> seeds = DatabaseDescriptor.getSeeds();
        for (InetAddress seed : seeds)
            {
                if (seed.equals(FBUtilities.getBroadcastAddress()) && seeds.size() == 1)
                    seedContacted = true;
                else
                    seeds.add(seed);
            }
    }
{code}
Sadly that throws an exception at startup:

{noformat}

ERROR 09:58:35,896 Exception encountered during startup
java.lang.UnsupportedOperationException
	at com.google.common.collect.ImmutableCollection.add(ImmutableCollection.java:92)
	at org.apache.cassandra.gms.Gossiper.buildSeedsList(Gossiper.java:1072)
	at org.apache.cassandra.gms.Gossiper.start(Gossiper.java:1046)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:555)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:527)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:426)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
java.lang.UnsupportedOperationException
	at com.google.common.collect.ImmutableCollection.add(ImmutableCollection.java:92)
	at org.apache.cassandra.gms.Gossiper.buildSeedsList(Gossiper.java:1072)
	at org.apache.cassandra.gms.Gossiper.start(Gossiper.java:1046)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:555)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:527)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:426)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Exception encountered during startup: null
ERROR 09:58:35,933 Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:310)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:359)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:95)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:492)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:724)
{noformat};;;","18/Jul/13 09:53;aecobley;Jason,

this code seems to work:

{code}

private void buildSeedsList()
    {
	Set<InetAddress> tempseeds = DatabaseDescriptor.getSeeds();
       
	for (InetAddress seed : tempseeds)
	    {
		
		if (seed.equals(FBUtilities.getBroadcastAddress()) && seeds.size() == 1){

		    seedContacted = true;
		}else{
		    seeds.add(seed);
		}
	    }
    }
{code}

This boots if localhost ip = seed ip
It throws an exception if localhost ip != seed ip and seed ip can not be contacted

However if seed ip is the default 127.0.0.1 the code will throw a ""java.lang.IllegalStateException: Unable to contact any seeds!"" exception. I'm not sure thats correct;;;","18/Jul/13 16:23;brandon.williams;bq. This way if there's only one seed (and this node is it), it can still come up.

A seed node can't bootstrap as it is, so this shouldn't be a problem.

bq. Also, is there any harm in adding this to 1.2? Seems rather innocuous, I think.

When I started I thought it was going to be a bit riskier, but now that it's done I think 1.2 is fine.

[~aecobley] bq. However it also halts with the same exception if seeds is set to 127.0.0.1

I can't reproduce this, and it doesn't make any sense to me (since seeds don't bootstrap);;;","18/Jul/13 18:32;aecobley;Yaml file at 127.0.0.1;;;","18/Jul/13 18:34;aecobley;Brandon,
For info:

With the code from 5678.txt, the code above and the attached yaml file and no other nodes in the cluster I'm getting the following trace:


sudo ./cassandra
xss =  -ea -javaagent:./../lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1024M -Xmx1024M -Xmn256M -XX:+HeapDumpOnOutOfMemoryError
LifeintheAirAge:bin Administrator$ objc[2779]: Class JavaLaunchHelper is implemented in both /Library/Java/JavaVirtualMachines/jdk1.8.0.jdk/Contents/Home/bin/java and /Library/Java/JavaVirtualMachines/jdk1.8.0.jdk/Contents/Home/jre/lib/libinstrument.dylib. One of the two will be used. Which one is undefined.
 INFO 10:50:38,698 Logging initialized
 INFO 10:50:38,719 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.8.0-ea
 INFO 10:50:38,719 Heap size: 1046937600/1046937600
 INFO 10:50:38,720 Classpath: ./../conf:./../build/classes/main:./../build/classes/thrift:./../lib/antlr-3.2.jar:./../lib/commons-cli-1.1.jar:./../lib/commons-codec-1.2.jar:./../lib/commons-lang-2.6.jar:./../lib/compress-lzf-0.8.4.jar:./../lib/concurrentlinkedhashmap-lru-1.3.jar:./../lib/disruptor-3.0.1.jar:./../lib/guava-13.0.1.jar:./../lib/high-scale-lib-1.1.2.jar:./../lib/jackson-core-asl-1.9.2.jar:./../lib/jackson-mapper-asl-1.9.2.jar:./../lib/jamm-0.2.5.jar:./../lib/jbcrypt-0.3m.jar:./../lib/jline-1.0.jar:./../lib/json-simple-1.1.jar:./../lib/libthrift-0.9.0.jar:./../lib/log4j-1.2.16.jar:./../lib/lz4-1.1.0.jar:./../lib/metrics-core-2.0.3.jar:./../lib/netty-3.5.9.Final.jar:./../lib/servlet-api-2.5-20081211.jar:./../lib/slf4j-api-1.7.2.jar:./../lib/slf4j-log4j12-1.7.2.jar:./../lib/snakeyaml-1.11.jar:./../lib/snappy-java-1.0.5.jar:./../lib/snaptree-0.1.jar:./../lib/thrift-server-0.2.jar:./../lib/jamm-0.2.5.jar
 INFO 10:50:38,721 JNA not found. Native methods will be disabled.
 INFO 10:50:38,739 Loading settings from file:/Users/Administrator/Documents/raspberry/cassandra2/test/cassandra-trunk/conf/cassandra.yaml
 INFO 10:50:39,159 Data files directories: [/var/lib/cassandra/data]
 INFO 10:50:39,159 Commit log directory: /var/lib/cassandra/commitlog
 INFO 10:50:39,160 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 10:50:39,160 disk_failure_policy is stop
 INFO 10:50:39,165 Global memtable threshold is enabled at 332MB
 INFO 10:50:39,298 Not using multi-threaded compaction
 INFO 10:50:39,547 Initializing key cache with capacity of 49 MBs.
 INFO 10:50:39,560 Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO 10:50:39,562 Initializing row cache with capacity of 0 MBs
 INFO 10:50:39,570 Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO 10:50:39,977 Couldn't detect any schema definitions in local storage.
 INFO 10:50:39,977 To create keyspaces and column families, see 'help create keyspace' in cqlsh.
 INFO 10:50:40,064 Enqueuing flush of Memtable-local@1164440413(149/149 serialized/live bytes, 6 ops)
 INFO 10:50:40,066 Writing Memtable-local@1164440413(149/149 serialized/live bytes, 6 ops)
 INFO 10:50:40,109 Completed flushing /var/lib/cassandra/data/system/local/system-local-ja-1-Data.db (178 bytes) for commitlog position ReplayPosition(segmentId=1374141039946, position=417)
 INFO 10:50:40,129 No commitlog files found; skipping replay
 INFO 10:50:40,466 Cassandra version: 2.0.0-beta1-SNAPSHOT
 INFO 10:50:40,467 Thrift API version: 19.37.0
 INFO 10:50:40,473 CQL supported versions: 2.0.0,3.1.0 (default: 3.1.0)
 INFO 10:50:40,509 Loading persisted ring state
 INFO 10:50:40,512 Starting up server gossip
 WARN 10:50:40,534 No host ID found, created 8e0fac5a-b8c6-4a6b-b476-de794fb4b85c (Note: This should happen exactly once per node).
 INFO 10:50:40,539 Enqueuing flush of Memtable-local@366873404(300/300 serialized/live bytes, 11 ops)
 INFO 10:50:40,540 Writing Memtable-local@366873404(300/300 serialized/live bytes, 11 ops)
 INFO 10:50:40,552 Completed flushing /var/lib/cassandra/data/system/local/system-local-ja-2-Data.db (293 bytes) for commitlog position ReplayPosition(segmentId=1374141039946, position=81742)
Broadcast: /134.36.9.8
Size of seeds0
Seed : 127.0.0.1
 INFO 10:50:40,605 Starting Messaging Service on port 7000
 INFO 10:50:40,647 Enqueuing flush of Memtable-local@210156003(86/86 serialized/live bytes, 4 ops)
 INFO 10:50:40,647 Writing Memtable-local@210156003(86/86 serialized/live bytes, 4 ops)
 INFO 10:50:40,657 Completed flushing /var/lib/cassandra/data/system/local/system-local-ja-3-Data.db (118 bytes) for commitlog position ReplayPosition(segmentId=1374141039946, position=82003)
 INFO 10:50:40,659 JOINING: waiting for ring information
 INFO 10:50:41,582 Handshaking version with /127.0.0.1
 INFO 10:50:46,585 Handshaking version with /127.0.0.1
 INFO 10:50:46,585 Cannot handshake version with /127.0.0.1
 INFO 10:50:51,586 Cannot handshake version with /127.0.0.1
 INFO 10:50:51,587 Handshaking version with /127.0.0.1
 INFO 10:50:56,589 Cannot handshake version with /127.0.0.1
 INFO 10:50:56,590 Handshaking version with /127.0.0.1
 INFO 10:51:01,591 Cannot handshake version with /127.0.0.1
 INFO 10:51:01,593 Handshaking version with /127.0.0.1
 INFO 10:51:06,594 Cannot handshake version with /127.0.0.1
 INFO 10:51:06,595 Handshaking version with /127.0.0.1
 INFO 10:51:10,713 JOINING: schema complete, ready to bootstrap
 INFO 10:51:10,716 JOINING: getting bootstrap token
 INFO 10:51:10,766 Enqueuing flush of Memtable-local@1819776360(10114/10114 serialized/live bytes, 257 ops)
 INFO 10:51:10,767 Writing Memtable-local@1819776360(10114/10114 serialized/live bytes, 257 ops)
 INFO 10:51:10,797 Completed flushing /var/lib/cassandra/data/system/local/system-local-ja-4-Data.db (5290 bytes) for commitlog position ReplayPosition(segmentId=1374141039946, position=94018)
 INFO 10:51:10,813 Compacting [SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ja-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ja-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ja-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/local/system-local-ja-4-Data.db')]
 INFO 10:51:10,814 JOINING: sleeping 30000 ms for pending range setup
 INFO 10:51:10,864 Compacted 4 sstables to [/var/lib/cassandra/data/system/local/system-local-ja-5,].  5,879 bytes to 5,718 (~97% of original) in 46ms = 0.118546MB/s.  4 total rows, 1 unique.  Row merge counts were {1:0, 2:0, 3:0, 4:1, }
 INFO 10:51:11,597 Cannot handshake version with /127.0.0.1
 INFO 10:51:11,598 Handshaking version with /127.0.0.1
 INFO 10:51:16,598 Cannot handshake version with /127.0.0.1
 INFO 10:51:16,599 Handshaking version with /127.0.0.1
 INFO 10:51:21,600 Cannot handshake version with /127.0.0.1
 INFO 10:51:21,601 Handshaking version with /127.0.0.1
 INFO 10:51:26,602 Cannot handshake version with /127.0.0.1
 INFO 10:51:26,604 Handshaking version with /127.0.0.1
 INFO 10:51:31,605 Cannot handshake version with /127.0.0.1
 INFO 10:51:31,606 Handshaking version with /127.0.0.1
 INFO 10:51:36,607 Cannot handshake version with /127.0.0.1
 INFO 10:51:36,609 Handshaking version with /127.0.0.1
ERROR 10:51:40,816 Exception encountered during startup
java.lang.IllegalStateException: Unable to contact any seeds!
	at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:897)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:668)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:527)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:426)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
java.lang.IllegalStateException: Unable to contact any seeds!
	at org.apache.cassandra.service.StorageService.bootstrap(StorageService.java:897)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:668)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:527)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:426)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Exception encountered during startup: Unable to contact any seeds!
ERROR 10:51:40,822 Exception in thread Thread[StorageServiceShutdownHook,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:310)
	at org.apache.cassandra.service.StorageService.shutdownClientServers(StorageService.java:359)
	at org.apache.cassandra.service.StorageService.access$000(StorageService.java:95)
	at org.apache.cassandra.service.StorageService$1.runMayThrow(StorageService.java:492)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.lang.Thread.run(Thread.java:724)
;;;","18/Jul/13 18:35;brandon.williams;Your seed is 127.0.0.1, but your listen_address is something else.  So the behavior is actually correct, since it can't contact 127.0.0.1 since you're not listening there.;;;","18/Jul/13 18:41;aecobley;Brandon,  you're right of course !  Apologies
;;;","18/Jul/13 21:08;jasobrown;bq. A seed node can't bootstrap as it is, so this shouldn't be a problem

Ahh, crap, forgot about that. Then on the whole, the original patch lgtm. You might want to consider setting seedContacted to true in buildSeedsList() to optimize the lookup in checkSeedContact(), but that's a minor optimization and shouldn't stop us from moving forward on this.;;;","18/Jul/13 21:18;brandon.williams;Committed with the !seedContacted check, but skipped the buildSeedsList() optimization since I couldn't figure out how if we're a seed we'd bootstrap (and if we do, as you say, it should be minor.);;;","24/Jul/13 22:21;rcoli;This comment is not particularly substantive, I just want to express my enthusiasm for this behavior being fixed. YAY! This formerly very confusing-to-noobs behavior will now be much less potentially confusing. Thanks to the reporter and to the contributors of the patch! :D;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The output of the describe command does not necessarily give you the right DDL to re-create the CF,CASSANDRA-5766,12658129,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,slowenthal,slowenthal,16/Jul/13 22:08,16/Apr/19 09:32,14/Jul/23 05:53,18/Jul/13 20:44,1.2.7,,,Legacy/Tools,,,0,cqlsh,describe,,,"If compression is not set for a CF, cqlsh omits the compression attribute.  When you replay that very same DDL, you get a CF with Snappy compression.  This may occur with other parameters.  Perhaps describe should always show every parameter in full.  The absence of a setting is a setting.  (Think of the arrow in the FedEx logo).

Create a CF with cassandra-stress.  cassandra-stress defaults to NO compression.

 ~/dse/resources/cassandra/tools/bin/cassandra-stress -S 100 -c 1 --num-keys 1

describe it
CREATE TABLE ""Standard1"" (
  key blob PRIMARY KEY,
  ""C0"" blob
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'};

replay it - I changed the cf name to standard2

describe the new CF:

CREATE TABLE standard2 (
  key blob PRIMARY KEY,
  ""C0"" blob
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};


",,aleksey,slowenthal,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jul/13 20:37;aleksey;5766.txt;https://issues.apache.org/jira/secure/attachment/12593049/5766.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,338323,,,Thu Jul 18 20:44:50 UTC 2013,,,,,,,,,,"0|i1mcrz:",338643,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"18/Jul/13 20:39;brandon.williams;+1;;;","18/Jul/13 20:44;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CommitLogReplayer should calculate checksums differently for < 2.0,CASSANDRA-5764,12657926,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius,dbrosius,16/Jul/13 04:42,16/Apr/19 09:32,14/Jul/23 05:53,17/Jul/13 01:09,2.0 beta 2,,,,,,0,,,,,code uses the wrong version to check whether to use old style or new style checksumming.,,dbrosius,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/13 04:43;dbrosius;5764.txt;https://issues.apache.org/jira/secure/attachment/12592487/5764.txt",,,,,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,338120,,,Wed Jul 17 01:09:12 UTC 2013,,,,,,,,,,"0|i1mbj3:",338441,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"16/Jul/13 21:54;jbellis;Good catch, +1;;;","17/Jul/13 01:09;dbrosius;committed to trunk as faa9d7ebf8c2376ab23d11fd68e2850ef3966288;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CqlPagingRecordReader should quote table and column identifiers,CASSANDRA-5763,12657767,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,pkolaczk,pkolaczk,pkolaczk,15/Jul/13 16:24,16/Apr/19 09:32,14/Jul/23 05:53,15/Jul/13 22:37,1.2.7,,,,,,0,,,,,"Using CPIF on table with uppercase name or with uppercase column names doesn't work (""unconfigured table"" message).",,alexliu68,pkolaczk,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Jul/13 21:33;alexliu68;5763-2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12594797/5763-2-1.2-branch.txt","15/Jul/13 18:27;pkolaczk;DSP-2292.patch;https://issues.apache.org/jira/secure/attachment/12592368/DSP-2292.patch",,,,,,,,,,,,,,,,,,,2.0,pkolaczk,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,337987,,,Mon Jul 29 21:42:11 UTC 2013,,,,,,,,,,"0|i1mapr:",338309,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"15/Jul/13 16:40;pkolaczk;Related DSE issue: https://datastax.jira.com/browse/DSP-2292;;;","15/Jul/13 22:37;jbellis;LGTM.  Rebased for you and committed.;;;","29/Jul/13 21:33;alexliu68;5763-2-1.2-branch.txt patch is to fix a small bug at CqlPagingRecordReader and add case sensitive to CqlRecordWriter as well.;;;","29/Jul/13 21:42;jbellis;Ah, I see.  Go ahead and reopen the other issue for this then, since it will be committed to 1.2.9.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Lost row marker after TTL expires,CASSANDRA-5762,12657720,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,tnr,tnr,15/Jul/13 13:18,16/Apr/19 09:32,14/Jul/23 05:53,22/Jul/13 12:19,1.2.7,,,,,,0,,,,,"I have the following table

cqlsh:loginproject> DESCRIBE TABLE gameservers;
 
CREATE TABLE gameservers (
  address inet PRIMARY KEY,
  last_update timestamp,
  regions blob,
  server_status boolean
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};



after inserting a row and executing the following command:
UPDATE gameservers USING TTL 10 SET server_status = true WHERE address = '192.168.0.100'

after waiting for the ttl to expire, the row will lose its rowmarker making ""select address from gameservers"" returning 0 results although there are some.

in cassandra-cli the table looks like this:
[default@loginproject] list gameservers;
Using default limit of 100
Using default cell limit of 100
-------------------
RowKey: 192.168.0.100
=> (name=last_update, value=0000000000000017, timestamp=1373884433543000)
=> (name=regions, value=<truncated>, timestamp=1373883701652000)

1 Row Returned.
Elapsed time: 345 msec(s).
[default@loginproject]


",Ubuntu 12.04,aleksey,blair,jjordan,rcoli,sayap,slebresne,tnr,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jul/13 12:24;slebresne;0001-Always-do-slice-queries-for-CQL3-tables.txt;https://issues.apache.org/jira/secure/attachment/12592529/0001-Always-do-slice-queries-for-CQL3-tables.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,337940,,,Wed Jan 15 12:11:16 UTC 2014,,,,,,,,,,"0|i1mafb:",338262,,,,,,,,,aleksey,,aleksey,Critical,,,,,,,,,,,,,,,,,,"15/Jul/13 14:16;slebresne;This is a rather annoying problem. To sum it up, if you TTL one column, the row marker is ttled too, because we have no way to know that this is not the last live column in that CQL3 row, but then if there was other non-ttled columns, they will survive the death of the row marker which breaks the invariant that a live row always has a row marker.

Tbh, I have a hard time coming with a way to fix this. And so the only workaround that comes to mind would be disallow setting a TTL on an individual columns. That is, you'd have to update all columns of the row to be able to use a TTL, which I hate because 1) it'll look random syntax wise and 2) I'm convinced that being able to TTL individual columns in a row is useful.;;;","16/Jul/13 12:24;slebresne;As much as this pains me, I don't see any easy way to make this work outside of doing a read-before-write (which is not acceptable).

It ""might"" be possible to make it work (without read-before-write) by specializing the row marker in the storage so that it tracks TTL to provide the desired behavior but at best that wouldn't be trivial and would probably make the row marker prohibitive in term of storage (though something like CASSANDRA-4175 might help make it more reasonable). In any case, it's _at best_ a solution for 2.1 but not before that, and that's leaving aside the debate of whether the feature is worth the complexity.

In the meantime, the best workaround I can come with would be to force SELECT queries to slice the whole CQL3 row even when only some columns are selected.  That is, we would revert to what we did for selects before CASSANDRA-4361. Tbh, this probably wouldn't have much impact on performance since 1) CQL3 rows are bound to be relatively small and 2) we now optimize slice queries relatively well for that kind of case (partly in 1.2 with promoted index and even more in 2.0 with CASSANDRA-5514) so that queries by names probably don't have that much benefits anymore.

Doing that would fix the problem is most cases, including the one of the description since it'll basically relegate the row marker to only mark rows where only the PK is set. This does not fix it fully though, since if you do
{noformat}
CREATE TABLE test (k int PRIMARY KEY, a int, b int);
INSERT INTO test (k, a, b) VALUES (0, 1, 2);
UPDATE test USING TTL=1 SET b=3 WHERE k=0;
// wait 2 seconds
DELETE a FROM test WHERE k=0;
SELECT * FROM test WHERE k=0;
{noformat}
then the last select will return no results, even though it kind of should return one result (with {{a == null}} and {{b == null}}) since we haven't done a full row deletion. But then we could accept that as a whacky known special situation (don't get me wrong, I don't like it, it's just that ""we have a problem and I don't have a better solution""). And to be fair, you would really have to try fairly hard to get bitten by this.

Attaching the patch that do what's above for info (IN queries on the last clustering column, which we support. make that slightly more annoying that one would hope, but it's not too much of a big deal either).

As mentionned above, another workaround could be to not let user get into that state by forcing all the (CQL3) columns of the (CQL3) row to be set int the statement if a TTL is used.

The (imho big) problem is that this is a breaking change. If someone is using different TTL in the same CQL3 row (and his application do depends on it), it basically cannot upgrade (short of migrating data that have differents TTL into their own separate table, which is extremely painful). Part of me is also pretty convinced that the convenience of being able to set TTL to individual columns outweight the ""not exactly right"" behavior of the special case above (especially since only people that *needs* per-columns TTL will ever run into that special case).
;;;","18/Jul/13 19:17;aleksey;+1;;;","22/Jul/13 12:19;slebresne;Alright, committed, thanks;;;","10/Jan/14 22:39;jjordan;Could we keep a ""something in here has a TTL"" flag and do the optimized way unless it is set?  I think we even already keep that kind of data to do the sstable tombstone compaction?  That way if you don't use TTL's, and you have big columns, you don't get the heap penalty (CASSANDRA-6569).;;;","11/Jan/14 11:18;slebresne;bq. Could we keep a ""something in here has a TTL"" flag and do the optimized way unless it is set?

I don't see how we could maintain such flag. We'd need to know no TTL is involved whatsoever at the coordinator level (so before we're reached the replicas), which I don't think is possible to do reliably. What we could probably do is add a per-table ""disable ttl"" option that would make inserts with TTL rejected. And we could optimize when this option is set. That being said, if you want to store both small and large values in the same row and strongly rely on only being able to query only the small ones often, you might be better off using 2 tables, one for the small values, one for the large ones.;;;","15/Jan/14 12:11;slebresne;Actually, CASSANDRA-6588 is probably a better idea than my ""disable ttl option"" above.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh DESCRIBE should properly describe CUSTOM secondary indexes,CASSANDRA-5760,12657651,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,14/Jul/13 23:19,16/Apr/19 09:32,14/Jul/23 05:53,15/Jul/13 15:39,1.2.7,,,Feature/2i Index,,,0,cqlsh,describe,,,"CASSANDRA-5484 and then CASSANDRA-5639 added CREATE CUSTOM INDEX support to CQL3, but cqlsh hasn't been updated to describe such indexes properly.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jul/13 00:12;aleksey;5760.txt;https://issues.apache.org/jira/secure/attachment/12592221/5760.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,337871,,,Mon Jul 15 15:39:13 UTC 2013,,,,,,,,,,"0|i1m9zz:",338193,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"15/Jul/13 15:28;jbellis;+1;;;","15/Jul/13 15:39;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Any exception hangs repair,CASSANDRA-5758,12657585,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,hsn,hsn,13/Jul/13 11:50,16/Apr/19 09:32,14/Jul/23 05:53,26/Jul/13 15:42,2.0 rc1,,,,,,0,,,,,"If there is any exception during repair, it hangs and never complete. This is far away from optimal error handling.

CASSANDRA-5646 and CASSANDRA-5757 and probably much other issues can be used to reproduce this problem for testing purposes.",,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,337806,,,Fri Jul 26 15:42:18 UTC 2013,,,,,,,,,,"0|i1m9lj:",338128,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"13/Jul/13 21:18;jbellis;Repair was updated for 2.0b1 in CASSANDRA-5426.  Please provide more details about what you see there since it is no longer similar to the 1.2 design.;;;","13/Jul/13 22:59;hsn;Actually any time I see exception during repair then repair hangs and you can not run other repair until you restart node.

if you want example for 2.0 then CASSANDRA-5757 hangs repair.;;;","25/Jul/13 23:30;hsn;still not fixed in 2.0b2. 

But i guess i am just wasting time with this for 1 year. Lets scrap this ticket, you will never agree that exception handling during repair is broken and fix that exception instead of exception handling, right?

I am not saying that fixing exception which caused this hang is bad thing.;;;","26/Jul/13 15:42;brandon.williams;I'm pretty sure if nothing else 0cc0d8ded34051bf94e936dc4564b634a68ea864 fixed this.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
assertionError in repair,CASSANDRA-5757,12657584,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,hsn,hsn,13/Jul/13 11:45,16/Apr/19 09:32,14/Jul/23 05:53,18/Jul/13 12:30,2.0 beta 2,,,,,,0,,,,,"i increased replication factor and run repair, some token ranges were repaired okay, but one failed with:

 INFO 13:03:52,234 [repair #dd7937a0-ebab-11e2-ba07-c38e7fba9d51] session completed successfully
ERROR 13:03:52,343 Exception in thread Thread[ValidationExecutor:2,1,main]
java.lang.AssertionError: (max(9099058114996150811),max(-5486100704702537010)]
        at org.apache.cassandra.db.DataRange.<init>(DataRange.java:50)
        at org.apache.cassandra.db.DataRange.forKeyRange(DataRange.java:74)
        at org.apache.cassandra.io.sstable.SSTableReader.getScanner(SSTableReade
r.java:1033)
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScan
ners(AbstractCompactionStrategy.java:214)
        at org.apache.cassandra.db.compaction.CompactionManager$ValidationCompac
tionIterable.<init>(CompactionManager.java:751)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationComp
action(CompactionManager.java:657)",,hsn,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5771,"15/Jul/13 17:06;slebresne;5757.txt;https://issues.apache.org/jira/secure/attachment/12592354/5757.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,337805,,,Thu Jul 18 12:30:54 UTC 2013,,,,,,,,,,"0|i1m9lb:",338127,,,,,,,,,yukim,,yukim,Normal,,,,,,,,,,,,,,,,,,"15/Jul/13 17:06;slebresne;The actual AssertionError is due to basically a misplaced assertion.  DataRange.Paging don't really support wrapping ranges but is only used in getRangeSlice which unwraps ranges first anyway.  However repair does use wrapping range so DataRange itself should allow wrapping ranges.

So fixing that is trivial, but this uncover the fact that SSTableScanner is broken on trunk if the range is wrapping. For the records, it has initially been broken by CASSANDRA-4180 (after this patch, for a wrapping range, only the part between the start of the range and then end of ring/file was returned by the scanner). CASSANDRA-4415 introduced DataRange but didn't really changed the logic there so the bug persisted.

Attaching a patch that move the assert and fix SSTableScanner to handle wrapping ranges correctly.
;;;","17/Jul/13 15:14;yukim;Hmm, the patch broke SSTableReaderTest#testGetScannerForNoIntersectionRanges.

{code}
java.util.NoSuchElementException
	at com.google.common.collect.AbstractIterator.next(AbstractIterator.java:154)
	at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:169)
	at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:42)
	at org.apache.cassandra.io.sstable.SSTableReaderTest.testGetScannerForNoIntersectingRanges(SSTableReaderTest.java:302)
{code};;;","17/Jul/13 17:08;slebresne;Turns out that this is a bug in Range.intersects(Bound) that this test happens to run into. And since that bug actually affect 1.2 too, I've created CASSANDRA-5771 for that specific problem. With the patch from CASSANDRA-5771, the test passes correctly.;;;","18/Jul/13 11:57;yukim;Confirmed it's passing. +1.;;;","18/Jul/13 12:30;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shuffle disable subcommand not recognised,CASSANDRA-5756,12657559,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,dbrosius,hsn,hsn,13/Jul/13 01:52,16/Apr/19 09:32,14/Jul/23 05:53,13/Jul/13 17:58,2.0 beta 2,,,Legacy/Tools,,,0,,,,,"command disable listed in help but not recognized

C:\cassandra2\bin>shuffle.bat disable
Unknown subcommand: disable
Usage: shuffle [options] <sub-command>

Sub-commands:
 create           Initialize a new shuffle operation
 ls               List pending relocations
 clear            Clear pending relocations
 en[able]         Enable shuffling
 dis[able]        Disable shuffling
",,dbrosius,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/13 07:13;dbrosius;5756.txt;https://issues.apache.org/jira/secure/attachment/12592130/5756.txt",,,,,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,337780,,,Sat Jul 13 17:58:27 UTC 2013,,,,,,,,,,"0|i1m9fr:",338102,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"13/Jul/13 07:13;dbrosius;fix simple typo in 2.0;;;","13/Jul/13 15:11;jbellis;+1;;;","13/Jul/13 17:58;dbrosius;committed to trunk as commit c5bca30a0f8d21c3c3b59fde0b8e1bd0581c8905;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
shuffle clear fails,CASSANDRA-5755,12657558,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,hsn,hsn,13/Jul/13 01:51,16/Apr/19 09:32,14/Jul/23 05:53,13/Jul/13 18:17,2.0.1,,,Legacy/Tools,,,0,,,,,"C:\cassandra2\bin>shuffle.bat clear
Exception in thread ""main"" java.lang.RuntimeException: InvalidRequestException(why:Invalid STRING constant (ee7f3e119820c96d) for token_bytes of type blob)
        at org.apache.cassandra.tools.Shuffle.executeCqlQuery(Shuffle.java:516)
        at org.apache.cassandra.tools.Shuffle.clear(Shuffle.java:439)
        at org.apache.cassandra.tools.Shuffle.main(Shuffle.java:689)
Caused by: InvalidRequestException(why:Invalid STRING constant (ee7f3e119820c96d
) for token_bytes of type blob)
        at org.apache.cassandra.thrift.Cassandra$execute_cql3_query_result$execu
te_cql3_query_resultStandardScheme.read(Cassandra.java:45153)
        at org.apache.cassandra.thrift.Cassandra$execute_cql3_query_result$execu
te_cql3_query_resultStandardScheme.read(Cassandra.java:45130)
        at org.apache.cassandra.thrift.Cassandra$execute_cql3_query_result.read(",,dbrosius,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jul/13 06:53;dbrosius;5755.txt;https://issues.apache.org/jira/secure/attachment/12592128/5755.txt",,,,,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,337779,,,Sat Jul 13 18:17:29 UTC 2013,,,,,,,,,,"0|i1m9fj:",338101,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"13/Jul/13 06:54;dbrosius;patch switches to new blob format;;;","13/Jul/13 15:10;jbellis;+1

(Note, changing this in 1.2.7 is fine, but it's not broken until 2.0 where we don't support the string syntax anymore.);;;","13/Jul/13 18:17;dbrosius;ah, you're right... must have confused myself switching between branches.

committed to trunk as commit 0ed859a5d0ece694f240b64e3212af37e0def1ac;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-shuffle is not available for windows,CASSANDRA-5753,12657550,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,hsn,hsn,13/Jul/13 00:35,16/Apr/19 09:32,14/Jul/23 05:53,13/Jul/13 21:15,1.2.7,2.0 beta 2,,Legacy/Tools,,,0,,,,,windows version (.BAT file) of cassandra-shuffle utility is missing,,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,337771,,,Sat Jul 13 21:15:38 UTC 2013,,,,,,,,,,"0|i1m9dr:",338093,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"13/Jul/13 11:40;hsn;easy to fix. copy cassandra-cli.bat and change name of main class;;;","13/Jul/13 21:15;jbellis;Done in fa327aa699d4c6a72ea0a194c129d40e48d43024;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift tables are not supported from CqlPagingInputFormat,CASSANDRA-5752,12657535,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,jbellis,jbellis,12/Jul/13 22:07,16/Apr/19 09:32,14/Jul/23 05:53,09/Oct/13 14:32,1.2.11,,,,,,0,qa-resolved,,,,"CqlPagingInputFormat inspects the system schema to generate the WHERE clauses needed to page ""wide rows,"" but for a classic Thrift table there are no entries for the ""default"" column names of key, column1, column2, ..., value so CPIF breaks.",,alexliu68,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/13 21:58;alexliu68;5752-1-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12592870/5752-1-1.2-branch.txt","16/Jul/13 20:36;alexliu68;5752-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12592616/5752-1.2-branch.txt","31/Jul/13 21:22;alexliu68;5752-2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12595275/5752-2-1.2-branch.txt",,,,,,,,,,,,,,,,,,3.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,337756,,,Wed Jul 31 23:21:10 UTC 2013,,,,,,,,,,"0|i1m9af:",338078,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,enigmacurry,,,"12/Jul/13 22:10;jbellis;This was fixed Cassandra-side for 2.0 in CASSANDRA-5702 but the fix was pretty heavyweight and backporting to 1.2.x is not an option.

IMO the right fix for 1.2 is to call CFDefinition.get[Key,Column,Value]Id methods the way the CQL parser does.

/cc [~iamaleksey] [~pkolaczk];;;","16/Jul/13 20:36;alexliu68;Patch for 1.2 branch is attached.;;;","16/Jul/13 22:59;jbellis;-1 on using the exception for control flow; looks like the thrift logic should be merged w/ retrieveKeys.

Also, please use modern logging API instead of string concatenation; see b2daab711cd8dc03afb39a6464f21325ba0193ee;;;","17/Jul/13 21:58;alexliu68;5752-1-1.2-branch.txt is attached. It doesn't use exception any more;;;","22/Jul/13 18:28;jbellis;When do we expect partitionBoundColumns to be empty after a non-empty schema_columnfamilies resultset?;;;","29/Jul/13 21:57;alexliu68;If the key_aliases return as string [], the partitionBoundColumns size is zero. ;;;","30/Jul/13 19:41;jbellis;But you're checking for key_aliases being null earlier.  Which is it going to be?  Both checks should not be necessary.

Also, I think the {{rows.size()==0}} check is bogus; there should always be an entry in schema_columnfamilies.  If there isn't, falling back to describe_columnfamilies isn't going to help.;;;","31/Jul/13 21:22;alexliu68;5752-2-1.2-branch.txt is attached to clean up the code as suggested. It also includes the fix for CqlRecordWriter.;;;","31/Jul/13 23:21;jbellis;Committed to 1.2 only. (In 2.0 the system schema include the required information for Thrift tables, as mentioned above.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CLI can show bad DESCRIBE for CQL3 CF if given the CF explicitly,CASSANDRA-5750,12657488,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,enigmacurry,enigmacurry,12/Jul/13 16:33,16/Apr/19 09:32,14/Jul/23 05:53,21/Nov/13 08:59,1.2.13,,,,,,0,,,,,"The CLI omits CQL3 tables if you do a regular describe command. It also emits a nice warning about it. However, if you do a describe with an explicit CF name, it does something a bit unintuitive:

{code}
[default@ryan] describe r1;

WARNING: CQL3 tables are intentionally omitted from 'describe' output.
See https://issues.apache.org/jira/browse/CASSANDRA-4377 for details.

    ColumnFamily: r1
      Key Validation Class: org.apache.cassandra.db.marshal.Int32Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Cells sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type)
      GC grace seconds: 0
      Compaction min/max thresholds: 0/0
      Read repair chance: 0.0
      DC Local Read repair chance: 0.0
      Populate IO Cache on flush: false
      Replicate on write: false
      Caching: keys_only
      Bloom Filter FP chance: default
      Built indexes: []
      Compaction Strategy: null
{code}

In this case it emitted the WARNING message, but it still showed the table anyway, and many of the CF settings are incorrect because of this. Better to show nothing than incorrect values.",,aleksey,enigmacurry,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Nov/13 15:33;slebresne;5750.txt;https://issues.apache.org/jira/secure/attachment/12614906/5750.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,337709,,,Thu Nov 21 08:59:14 UTC 2013,,,,,,,,,,"0|i1m907:",338032,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"20/Nov/13 15:33;slebresne;Attaching a patch that omits CQL3 even if you use their name explicitly with a message that tells you to use cqlsh instead in that case. ;;;","20/Nov/13 21:40;aleksey;+1;;;","21/Nov/13 08:59;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DESC TABLE omits some column family settings,CASSANDRA-5749,12657484,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,enigmacurry,enigmacurry,12/Jul/13 16:00,16/Apr/19 09:32,14/Jul/23 05:53,15/Jul/13 10:49,2.0 beta 2,,,,,,0,cqlsh,,,,"In CQL I can create a table with settings introduced in 2.0:

{code}
cqlsh:Keyspace1> CREATE TABLE r1 ( key int PRIMARY KEY, value varchar) WITH speculative_retry='ALWAYS';
{code}

But the settings don't show up when I DESC TABLE:
{code}
cqlsh:Keyspace1> DESC TABLE r1;

CREATE TABLE r1 (
  key int PRIMARY KEY,
  value text
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  index_interval=128 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{code}

For comparison, here is the same table viewed from cassandra-cli:

{code}
[default@Keyspace1] describe r1;

WARNING: CQL3 tables are intentionally omitted from 'describe' output.
See https://issues.apache.org/jira/browse/CASSANDRA-4377 for details.

WARNING: Could not connect to the JMX on 127.0.0.1:7199 - some information won't be shown.

    ColumnFamily: r1
      Key Validation Class: org.apache.cassandra.db.marshal.Int32Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Cells sorted by: org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type)
      GC grace seconds: 0
      Compaction min/max thresholds: 0/0
      Read repair chance: 0.0
      DC Local Read repair chance: 0.0
      Populate IO Cache on flush: false
      Replicate on write: false
      Caching: keys_only
      Default time to live: 0
      Bloom Filter FP chance: default
      Index interval: default
      Speculative Retry: NONE
      Compaction Strategy: null
{code}

Ideally, all of these values that cli shows would be shown by cqlsh.
",,aleksey,enigmacurry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jul/13 22:51;aleksey;5749.txt;https://issues.apache.org/jira/secure/attachment/12592219/5749.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,337705,,,Mon Jul 15 10:49:34 UTC 2013,,,,,,,,,,"0|i1m8zb:",338028,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"12/Jul/13 16:03;enigmacurry;In the example above, the speculative_retry value didn't actually get set. I'll open a separate issue for this.

EDIT: Actually, I think this is just that the CLI doesn't understand cql3 tables. I opened CASSANDRA-5750 to save some else this same confusion.;;;","14/Jul/13 22:53;aleksey;The patch adds the missing default_time_to_live, speculative_retry, and memtable_flush_period_in_ms to DESCRIBE output.;;;","15/Jul/13 03:19;brandon.williams;+1;;;","15/Jul/13 10:49;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
HHOM.countPendingHints is a trap for the unwary,CASSANDRA-5746,12657166,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,thobbs,jbellis,jbellis,10/Jul/13 22:54,16/Apr/19 09:32,14/Jul/23 05:53,18/Jul/13 20:27,2.0 beta 2,,,Legacy/Tools,,,0,,,,,"countPendingHints can OOM the server fairly easily since it does a per-target seq scan without paging.

More generally, countPendingHints is far too slow to be useful for routine monitoring.",,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/13 21:20;thobbs;0001-Remove-HHOM.countPendingHints.patch;https://issues.apache.org/jira/secure/attachment/12592857/0001-Remove-HHOM.countPendingHints.patch","17/Jul/13 21:20;thobbs;0002-Report-created-hint-count-by-endpoint.patch;https://issues.apache.org/jira/secure/attachment/12592858/0002-Report-created-hint-count-by-endpoint.patch",,,,,,,,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,337389,,,Thu Jul 18 20:27:57 UTC 2013,,,,,,,,,,"0|i1m713:",337712,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"10/Jul/13 22:58;jbellis;If we want it to be fast enough for monitoring use, we need to denormalize the count.  So the question is, do we use a Counters table, a normal int table with manual lock-and-read-before-write, or just one-off it with AtomicInteger and sync to disk occasionally? 

Personally I'd be inclined towards the last; it's okay if under- or over-count (because of periodic CL sync for instance), as long as we reliably distinguish between zero and non-zero hints for a given target.  We could sanity check that with an approach like listEndpointsPendingHints on startup, which would be fairly low-overhead.;;;","16/Jul/13 19:00;thobbs;Doesn't the TTL on hints cells complicate all of those strategies?  I can't think of a cheap way to schedule all of those future decrements to a counter.

Instead, I suppose we could recount on demand if the time since the last count is greater than the smallest TTL we've seen, but without throttling of some sort recounts would still happen frequently under some circumstances.

Alternatively, it seems like a fair amount of work, but perhaps a get_range_counts() implementation with internal auto-paging (like get_count) is a decent option?;;;","16/Jul/13 19:34;jbellis;What if we redefine the problem to be ""how many hints have I generated for node X?"" and get rid of countPendingHints entirely?

ISTM that's a more important indicator of cluster health than how many hints X expects to get when he comes back up.  I can live without that.;;;","16/Jul/13 20:07;thobbs;> What if we redefine the problem to be ""how many hints have I generated for node X?"" and get rid of countPendingHints entirely?

That seems like a reasonable replacement metric for most purposes, but I can see where countPendingHints() might still be useful.  However, in its current state, it seems too dangerous to leave in.  Maybe pull it out in this ticket and open a new ticket for a better implementation if there's interest?

I'm assuming we don't want to persist this one to disk. (Most RRD-style metric systems handle normally monotonically increasing metrics resetting to 0 occasionally, so just counting hints created since the node has been up should be fine.);;;","16/Jul/13 20:33;jbellis;Sounds good to me.  Let's try to get this into b2 so we don't rip it out of a stable release.;;;","17/Jul/13 21:20;thobbs;0001 removes countPendingHints().

0002 adds a per-endpoint count through the new metrics system. I made HHOM.hintFor() non-static, as I couldn't see why the singleton couldn't be used.  Let me know if there was some motivation for that.;;;","18/Jul/13 20:27;jbellis;LGTM, committed.  (Tweaked to use LoadingCache.getUnchecked instead of manually throwing RTE ourselves, also in the existing incrPastWindow.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not query secondary index,CASSANDRA-5732,12656702,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,adanecito,adanecito,08/Jul/13 21:35,16/Apr/19 09:32,14/Jul/23 05:53,13/Oct/13 16:14,1.2.11,2.0.2,,Feature/2i Index,,,2,,,,,"Noticed after taking a column family that already existed and assigning to an IntegerType index_type:KEYS and the caching was already set to 'ALL' that the prepared statement do not return rows neither did it throw an exception. Here is the sequence.
1. Starting state query running with caching off for a Column Family with the query using the secondary index for te WHERE clause.
2, Set Column Family caching to ALL using Cassandra-CLI and update CQL. Cassandra-cli Describe shows column family caching set to ALL
3. Rerun query and it works.
4. Restart Cassandra and run query and no rows returned. Cassandra-cli Describe shows column family caching set to ALL
5. Set Column Family caching to NONE using Cassandra-cli and update CQL. Rerun query and no rows returned. Cassandra-cli Describe for column family shows caching set to NONE.
6. Restart Cassandra. Rerun query and it is working again. We are now back to the starting state.

Best Regards,
-Tony","Windows 8, Jre 1.6.0_45 32-bit",adanecito,colinkuo,cscetbon,davidstrauss,dmeyer,jalkanen,jeromatron,jjordan,mishail,rcoli,samt,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4785,,,,,,,,,,,,,,,"10/Oct/13 14:02;samt;5732-1.2;https://issues.apache.org/jira/secure/attachment/12607811/5732-1.2","10/Oct/13 18:24;jbellis;5732-v2.txt;https://issues.apache.org/jira/secure/attachment/12607852/5732-v2.txt","10/Oct/13 19:49;jbellis;5732-v3.txt;https://issues.apache.org/jira/secure/attachment/12607870/5732-v3.txt",,,,,,,,,,,,,,,,,,3.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,336925,,,Mon Feb 10 10:09:57 UTC 2014,,,,,,,,,,"0|i1m46f:",337248,1.1.9,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"09/Jul/13 04:52;adanecito;Hi,

I tried the same set of tables, data, java code in Windows 7 jre 1.7.0_05 64-bit and it seems to not have the issue so far. Will try other environment using 1.7.0_x 32 then 64-bit to see if that solves the issue.

Regards,
-Tony;;;","10/Jul/13 04:54;adanecito;Ok. I figured out it has to do with a config setting in cassandra.yaml file. If you set row_cache_size_in_mb to 200 instead of 0 and you are setting caching = ""ALL"" for a column family and using and secondary index for a query this issue occurs. If you set row_cache_size_in_mb this problem goes away.
Let me know why this is. It would be nice to use row caching and column family caching.;;;","10/Jul/13 15:04;jalkanen;Is this the same as CASSANDRA-4785 and CASSANDRA-4973?;;;","10/Jul/13 15:19;adanecito;Hi Janne,
 
There are simularities. Mine though is a solid failure and I narrowed it down to what I said so the Cassandra team should be able to solve the issue.
 
Best Regards,
-Tony

From: Janne Jalkanen (JIRA) <jira@apache.org>
To: adanecito@yahoo.com 
Sent: Wednesday, July 10, 2013 9:05 AM
Subject: [jira] [Commented] (CASSANDRA-5732) Can not query secondary index



    [ https://issues.apache.org/jira/browse/CASSANDRA-5732?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13704633#comment-13704633 ] 

Janne Jalkanen commented on CASSANDRA-5732:
-------------------------------------------

Is this the same as CASSANDRA-4785?
                

--
This message is automatically generated by JIRA.
If you think it was sent incorrectly, please contact your JIRA administrators
For more information on JIRA, see: http://www.atlassian.com/software/jira
;;;","19/Sep/13 16:54;dmeyer;This is definitly a bug in latest 1.2.  Just reproduced in 1.2.10

Repro is easy.  Create single node cluster off of latest 1.2 branch and do the following:

cqlsh:ks> CREATE TABLE test ( row text, name text, PRIMARY KEY (row) );
cqlsh:ks> ALTER TABLE test WITH caching='all';
cqlsh:ks> INSERT INTO test (row, name) VALUES ( 'row1', 'daniel' );
cqlsh:ks> INSERT INTO test (row, name) VALUES ( 'row2', 'ryan' );
cqlsh:ks> CREATE INDEX on test (name);
cqlsh:ks> select * from test where name='daniel';

 row  | name
------+--------
 row1 | daniel
#Notice how row is returned

Stop node and set:
row_cache_size_in_mb: 200
Start node again and follow this procedure:

cqlsh> use ks;
cqlsh:ks> select * from test where name='daniel';
#Nothing is returned from this query


Now stop the node and set:
row_cache_size_in_mb: 0

start the node and do the following:

cqlsh> use ks;
cqlsh:ks> select * from test where name='daniel';

 row  | name
------+--------
 row1 | daniel

#Notice how the row is returned.;;;","19/Sep/13 16:57;jbellis;Suspect this might be up your alley, [~beobal].;;;","10/Oct/13 14:02;samt;The reason for the missing results is that in CFS.getColumnFamily() we look up the cfs id from Schema to calculate the cache key. However, 2i CFSes are never loaded into the Schema, so Schema.instance.getId always returns null. Simply fixing this by calling Schema.instance.load() with the 2i CFMD when the index is initialized uncovers another issue. The cfid is now retrievable, but the deserialization of a cached 2i row fails as it depends on the 2i CFMD being present in the enclosing KSMD for the eventual call to Schema.getCFMD(). Once we start adding index CFs to Schema they then become involved in schema migrations which makes everything very messy. So rather than adding them directly to KSMD like regular CFs, I added a separate cfId->CFMD map to Schema, so as far as most things are concerned nothing has changed, just we have one further place to look when retrieving CFMD for a given cfId.

The attached patch is against the 1.2 branch, CASSANDRA-4875 is a duplicate of this, but has a fixver of 1.1 [~jbellis], do you want me to submit a patch against 1.1 also?

I wrote a dtest for this, pull request for that here: https://github.com/riptano/cassandra-dtest/pull/22

Looking at this, I also uncovered what I think is an issue with the setup of the 2i cache config. In AbstractSimplePerColumnSecondaryIndex (in 1.2, the same code is in KeysIndex in 1.1), the estimated key and mean column counts are used to gauge the index's cardinality then use that to decide whether or not to enable row caching. This calculation is first performed prior to the index actually being built, so there are no SSTables to provide the estimates, which results in row caching always being disabled until the next time the index is initialized when C* is restarted (this appears to be why the repro steps require a restart). If this is a genuine problem, I'll create a separate JIRA to address it. 
;;;","10/Oct/13 18:24;jbellis;Hmm, it's not actually necessary to look up your id through the schema here.  v2 is a simpler solution if that's the only problem.;;;","10/Oct/13 18:50;samt;Yeah, unfortunately that failed lookup was only masking the other problem I mentioned. Even with the v2 fix, without the index cfm in Schema, you fall foul (silently, except for debug) of https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/ColumnFamilySerializer.java#L183 and so the row cache is effectively bypassed for 2i cfs.;;;","10/Oct/13 19:49;jbellis;I see.

I think your patch is probably fine, but I'm super nervous about messing with Schema internals this late in 1.2.x (let alone 1.1).

v3 just disables row cache entirely for 2i CFs.;;;","10/Oct/13 19:59;samt;Sure, that's reasonable and pretty much expected. I'll attach a patch for trunk/2.0;;;","13/Oct/13 16:14;jbellis;Committed v3 then.

I don't think it's worth doing extra work for 2.0 since we're removing row cache for 2.1.;;;","10/Feb/14 10:09;jeromatron;FWIW, appears to also be a problem in Cassandra 1.1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Should use index_interval from already loaded index summary,CASSANDRA-5731,12656637,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,08/Jul/13 17:35,16/Apr/19 09:32,14/Jul/23 05:53,08/Jul/13 18:53,2.0 beta 1,,,,,,0,,,,,"Since index_interval can be changed anytime in 2.0, SSTableReader should use index interval from loaded summary.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/13 17:36;yukim;5731.txt;https://issues.apache.org/jira/secure/attachment/12591234/5731.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,336860,,,Mon Jul 08 18:53:37 UTC 2013,,,,,,,,,,"0|i1m3rz:",337183,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"08/Jul/13 17:36;yukim;Fix attached.;;;","08/Jul/13 17:45;jbellis;+1;;;","08/Jul/13 18:53;yukim;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add new TimestampType to cqlsh,CASSANDRA-5729,12656631,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,slebresne,slebresne,08/Jul/13 16:58,16/Apr/19 09:32,14/Jul/23 05:53,08/Jul/13 20:09,2.0.0,,,,,,0,,,,,"Since cqlsh used CQL-over-thrift currently, we'd need to add support for the new TimestampType introduced in CASSANDRA-5723.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jul/13 19:46;aleksey;5729.txt;https://issues.apache.org/jira/secure/attachment/12591260/5729.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,336854,,,Mon Jul 08 20:09:46 UTC 2013,,,,,,,,,,"0|i1m3qn:",337177,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"08/Jul/13 19:48;aleksey;Adding the type locally in cqlsh. Will add to the next cassandra-dbapi2 release, too, but it's not ready yet.;;;","08/Jul/13 19:53;brandon.williams;+1;;;","08/Jul/13 20:09;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Silently failing messages in case of schema not fully propagated,CASSANDRA-5725,12656254,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sureshsajja,sbtourist,sbtourist,04/Jul/13 13:28,16/Apr/19 09:32,14/Jul/23 05:53,09/Oct/13 12:36,1.2.11,2.0.2,,,,,0,,,,,"When a new keyspace and/or column family is created on a multi nodes cluster (at least three), and then a mutation is executed on such new column family, the operations sometimes silently fails by timing out.

I tracked this down to the schema not being fully propagated to all nodes. Here's what happens:
1) Node 1 receives the create keyspace/column family request.
2) The same node receives a mutation request at CL.QUORUM and sends to other nodes too.
3) Upon receiving the mutation request, other nodes try to deserialize it and fail in doing so if the schema is not fully propagated, i.e. because they don't find the mutated column family.
4) The connection between node 1 and the failed node is dropped, and the request on the former hangs until timing out.

Here is the underlying exception, I had to tweak several log levels to get it: 
{noformat}
INFO 13:11:39,441 IOException reading from socket; closing
org.apache.cassandra.db.UnknownColumnFamilyException: Couldn't find cfId=a31c7604-0e40-393b-82d7-ba3d910ad50a
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeCfId(ColumnFamilySerializer.java:184)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:94)
	at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:397)
	at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:407)
	at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:367)
	at org.apache.cassandra.net.MessageIn.read(MessageIn.java:94)
	at org.apache.cassandra.net.IncomingTcpConnection.receiveMessage(IncomingTcpConnection.java:207)
	at org.apache.cassandra.net.IncomingTcpConnection.handleModernVersion(IncomingTcpConnection.java:139)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:82)
{noformat}

Finally, there's probably a correlated failure happening during repairs of newly created/mutated column family, causing the repair process to hang forever as follows:
{noformat}
""AntiEntropySessions:1"" daemon prio=5 tid=7fe981148000 nid=0x11abea000 in Object.wait() [11abe9000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7c6200840> (a org.apache.cassandra.utils.SimpleCondition)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.cassandra.utils.SimpleCondition.await(SimpleCondition.java:34)
	- locked <7c6200840> (a org.apache.cassandra.utils.SimpleCondition)
	at org.apache.cassandra.service.AntiEntropyService$RepairSession.runMayThrow(AntiEntropyService.java:695)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)

""http-8983-1"" daemon prio=5 tid=7fe97d24d000 nid=0x11a5c8000 in Object.wait() [11a5c6000]
   java.lang.Thread.State: WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <7c620db58> (a org.apache.cassandra.utils.SimpleCondition)
	at java.lang.Object.wait(Object.java:485)
	at org.apache.cassandra.utils.SimpleCondition.await(SimpleCondition.java:34)
	- locked <7c620db58> (a org.apache.cassandra.utils.SimpleCondition)
	at org.apache.cassandra.service.StorageService$4.runMayThrow(StorageService.java:2442)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at org.apache.cassandra.service.StorageService.forceTableRepairRange(StorageService.java:2409)
	at org.apache.cassandra.service.StorageService.forceTableRepair(StorageService.java:2387)
	at com.datastax.bdp.cassandra.index.solr.SolrCoreResourceManager.repairResources(SolrCoreResourceManager.java:693)
	at com.datastax.bdp.cassandra.index.solr.SolrCoreResourceManager.createCore(SolrCoreResourceManager.java:255)
	at com.datastax.bdp.cassandra.index.solr.CassandraCoreAdminHandler.handleCreateAction(CassandraCoreAdminHandler.java:121)
	at org.apache.solr.handler.admin.CoreAdminHandler.handleRequestBody(CoreAdminHandler.java:144)
	at org.apache.solr.handler.RequestHandlerBase.handleRequest(RequestHandlerBase.java:135)
	at org.apache.solr.servlet.SolrDispatchFilter.handleAdminRequest(SolrDispatchFilter.java:615)
	at org.apache.solr.servlet.SolrDispatchFilter.doFilter(SolrDispatchFilter.java:206)
{noformat}

I wasn't able to track any exception as I can't reproduce it reliably enough, but I believe it's correlated to schema propagation as based on log messages the merkle tree request on node 1 happens concurrently to schema installation on other nodes.",,rcoli,sbtourist,slebresne,sureshsajja,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/13 09:47;sbtourist;5725-0001.patch;https://issues.apache.org/jira/secure/attachment/12591120/5725-0001.patch","09/Oct/13 05:35;sureshsajja;5725_V2.patch;https://issues.apache.org/jira/secure/attachment/12607512/5725_V2.patch",,,,,,,,,,,,,,,,,,,2.0,sureshsajja,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,336477,,,Wed Oct 09 12:36:50 UTC 2013,,,,,,,,,,"0|i1m1f3:",336801,,,,,,,,,sbtourist,,sbtourist,Normal,,,,,,,,,,,,,,,,,,"04/Jul/13 16:13;jbellis;This is working as designed.  What do you think should happen instead?;;;","04/Jul/13 16:33;sbtourist;Well, in an ideal world, given C* has the notion of schema, mutations should be validated with the schema of the coordinator node and associated to such schema version, which should be unique and monotonic (we are the former, not the latter): this way, replica nodes could understand if they're missing a schema update and request it (which would solve this bug), as well as recognize if a partition is ongoing and react accordingly.
By the way, this probably translates in using vector clocks for schema updates, and I understand C* has not been designed this way, so let's forget about the ideal world.

A more pragmatic solution may be to implement a consistency level for schema updates too: right now we only wait for the schema to be applied on the local node, while supporting all consistency levels would allow subsequent updates to succeed under the same CL specification: i.e., applying a schema update at CL.QUORUM would allow subsequent updates at the same CL to succeed too.

Finally, a trivial one may just be to make the schema problem explicit with a specific exception.

Certainly, in my opinion, masking a schema problem with a timeout exception is pretty much confusing, and may lead to several hours spent in debugging/testing or (if the user isn't that smart to do that) increasing the timeouts, which is a bad solution to a wrong problem.

Unless I'm missing something in the current design/implementation, which may well be :);;;","04/Jul/13 16:56;jbellis;bq. Finally, a trivial one may just be to make the schema problem explicit with a specific exception.

This is not trivial since replicas only ack writes on success.

Here's how it's supposed to work: you perform your schema change, then you check for schema agreement before starting to write to the new table.;;;","04/Jul/13 17:05;sbtourist;bq. Here's how it's supposed to work: you perform your schema change, then you check for schema agreement before starting to write to the new table.

Sure, you can do that, but doesn't look like a great solution to me :)

By the way, if any change to fix this is too big at the moment, or really not worth, feel free to close this as won't fix, we'll live with this.;;;","04/Jul/13 17:40;jbellis;IMO the fix here is to special case UnknownColumnFamilyException so that it gets logged at INFO or WARN instead of being swallowed by the default IOException handler, which it currently subclasses.;;;","05/Jul/13 09:31;slebresne;bq. the fix here is to special case UnknownColumnFamilyException so that it gets logged at INFO or WARN instead of being swallowed by the default IOException handler

Agreed, at least it's by far the simplest fix and it's probably good enough in practice.

I don't think getting more fancy is worth the complexity that it would add. ""you perform your schema change, then you check for schema agreement before starting to write to the new table"" is not that hard a rule to follow, and a good client driver will do that for you under the hood anyway :) ;;;","07/Jul/13 09:46;sbtourist;Attaching patch.

I think it's better to catch a generic Throwable in IncomingTcpConnection, as special casing to UnknownColumnFamilyException means we may find ourselves in the same situation if some other unexpected exception is thrown.;;;","07/Jul/13 19:46;jbellis;This doesn't work on two levels:

# UCFE will still be caught and logged at debug by the IOE clause
# There's no need to add a logger.warn for ""everything else"" because fatal exceptions are logged by the default uncaught exception handler (that is set up in CassandraDaemon);;;","07/Jul/13 20:15;sbtourist;bq. UCFE will still be caught and logged at debug by the IOE clause

Oh, UCFE actually being an IOE is a bit unexpected. By the way, silly ranting, my fault I didn't check for it.

bq. There's no need to add a logger.warn for ""everything else"" because fatal exceptions are logged by the default uncaught exception handler

Got it, but my point still stands: by special casing for UCFE, we possibly miss other IOE-derived exceptions and make the IncomingTcpConnection catch block pretty ugly (and wrong actually, as an implementation detail about an exception thrown by an opaque message leaks through).

At this point, I'd rather log IOEs at info level: what do you think? Or are there too many IOEs during normal C* operations, which would mess up the logs?;;;","07/Jul/13 20:47;jbellis;bq. are there too many IOEs during normal C* operations, which would mess up the logs?

Exactly (every time you bounce a node, for instance); that's why we special cased it.;;;","07/Jul/13 21:10;sbtourist;Then I guess there's only one thing left ... I'll update the patch.;;;","02/Aug/13 03:26;jbellis;Still working on this, [~sbtourist]?;;;","02/Aug/13 18:05;sbtourist;I'm sorry I didn't have the time to get back to this: if anyone else wants to pick it up, feel free, otherwise I'll try to update the PR in the next few days.;;;","09/Oct/13 05:35;sureshsajja;Based on above comments, UnknownColumnFamilyException is handled separately and logged as WARN.

Patch is attached here for 1.2 branch;;;","09/Oct/13 07:53;sbtourist;Apologize for not getting back to this one myself.
Patch looks good to me.;;;","09/Oct/13 12:36;jbellis;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DateType (timestamp type in CQL3) does not sort pre-'unix epoch' dates correctly,CASSANDRA-5723,12656161,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,zhouhero,zhouhero,04/Jul/13 08:19,16/Apr/19 09:32,14/Jul/23 05:53,08/Jul/13 16:59,2.0 beta 1,,,,,,0,,,,,"- this bug can be confirmed by fellow:


1.create table like fellow:

create table test2 (
id varchar,
c varchar,
create_date timestamp,
primary key(id)
);

create index idx_test2_c on test2 (c);
create index idx_test2_create_date on test2 (create_date);


2.insert data like fellow;

cqlsh:pgl> update test2 set create_date='1950-01-01', c='1' where id='111';
cqlsh:pgl> update test2 set create_date='1917-01-01', c='1' where id='111';
cqlsh:pgl> update test2 set create_date='2013-01-01', c='1' where id='111';

3.select data :
cqlsh:pgl> select * from test2 where c='1' and create_date>'2011-01-01 12:00:01' ALLOW FILTERING ;

id | c | create_date
-----+---+--------------------------
111 | 1 | 2012-12-31 15:00:00+0000

4. add data:
update test2 set create_date='1917-05-01', c='1' where id='111';

5.select data:
cqlsh:pgl> select * from test2 where c='1' and create_date>'2011-01-01 12:00:01' ALLOW FILTERING ;

id | c | create_date
-----+---+--------------------------
111 | 1 | 1917-04-30 15:00:00+0000
↑
the search result is not right!
it should be fellow:

id | c | create_date
-----+---+--------------------------
111 | 1 | 2012-12-31 15:00:00+0000",,slebresne,zhouhero,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-9609,,,,,,,,,,,,,,,,,,,,"08/Jul/13 14:02;slebresne;5723.txt;https://issues.apache.org/jira/secure/attachment/12591217/5723.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,336436,,,Mon Jul 08 16:59:38 UTC 2013,,,,,,,,,,"0|i1m15z:",336760,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"04/Jul/13 15:51;slebresne;bq. it should be fellow

No, it should be empty, because you're overwriting the same row all the time, so after that update in 4, the row '111' should contain {{create_date='1917-05-01'}} and you could rightfully expect to not get that row back with your query.

The reason it's actually returned is that for some reason, DateType.compare() (the comparator used for the timestamp CQL3 type) use an unsigned comparison, and since 1917 is before the unix epoch, it's timestamp is negative and wrongfully sort after any post-epoch date. This is *not* a CQL3 specific bug in particular.

The simple fix would be to change the comparison to be signed, but there is obviously backward compatibility concerns (since DateType has done that for years). In any case, in the meantime, avoid pre-epoch dates.;;;","05/Jul/13 13:28;slebresne;I'm slightly surprised that nobody ran into this with DateType and I would argue that sorting pre-epoch dates after post-epoch ones is definitively a bug, but at the same time changing DateType now is too risky imo as it could screw current DateType rather badly.

Instead, I would suggest adding a new, fixed, DateType (TimestampType or DateTimeType maybe?). And for CQL3, we would switch to that for the 'timestamp' type (which would not switch existing table however, since they would still be DateType internally). We can also make it so that switching (manually) from/to DateType to that new type is allowed (with maybe a warning in the log).

Opinions?
;;;","06/Jul/13 01:09;jbellis;bq. Instead, I would suggest adding a new, fixed, DateType (TimestampType or DateTimeType maybe?). And for CQL3, we would switch to that for the 'timestamp' type (which would not switch existing table however, since they would still be DateType internally). We can also make it so that switching (manually) from/to DateType to that new type is allowed (with maybe a warning in the log)

Sounds reasonable.  What would we show for the old tables on the CQL side?  ""oldtimestamp?""  

(I would vote for TimestampType.);;;","06/Jul/13 12:44;slebresne;bq. What would we show for the old tables on the CQL side? ""oldtimestamp?

I was thinking of not having be any CQL3 specific type. I.e. it would be displayed as a custom type, or ""org.apache.cassandra.db.marshal.DateType"".;;;","06/Jul/13 15:46;jbellis;That's reasonable.;;;","08/Jul/13 14:02;slebresne;Attaching a patch with that new TimestampType. The patch is against 1.2 right now, but I'm starting to wonder if 2.0 is not a more reasonable goal.

The basics of the patch is that the CQL3 timestamp type now default to that new type. For the native protocol however, we make both DateType and TimestampType be returned as 'timestamp'. Otherwise, if we were returning it as a custom type, this would likely break users since client driver wouldn't recognize it anymore. Besides, the actual sorting of a type in a ResultSet shouldn't matter for a CQL3 driver.

On the CQL-over-thrift side however, we return the full ""thrift"" comparator name, so in practice we'd have to update cqlsh so it continues to work with dates, but the patch doesn't do it.

The patch let user switch between DateType and TimestampType, but log a warning when you do so.;;;","08/Jul/13 16:20;jbellis;+1, but I agree that this should be for 2.0.;;;","08/Jul/13 16:59;slebresne;Alright, committed to trunk only then.

I've also created CASSANDRA-5719 to followup on the cqlsh changes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError in CompactionExecutor,CASSANDRA-5720,12656062,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,efalcao,efalcao,03/Jul/13 16:34,16/Apr/19 09:32,14/Jul/23 05:53,18/Mar/14 14:36,,,,,,,2,compaction,ttl,,,"Seeing this on all 4 of my nodes during a major compaction.

{code}
ERROR [CompactionExecutor:57927] 2013-07-03 13:41:27,591 CassandraDaemon.java (line 175) Exception in thread Thread[CompactionExecutor:57927,1,RMI Runtime]
java.lang.AssertionError: originally calculated column size of 443395646 but now it is 443395712
        at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:135)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:162)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
        at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
        at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:355)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
{code}

A little bit about this CF. It stores data with a TTL of up to 30 days. These rows are wide. We run major compactions to remove expired data. This has been our setup for almost 2 years and this issue only started cropping up after upgrading to 1.2.5 (from 1.1.5)

I've been running scrub in the meantime to remove expired data. Now, I'm ending up with lots of similarly sized SSTables that C* is trying constantly to compact. These minor compactions of the bigger SSTables are failing also.","4 nodes
RF 2",christianmovi,colinkuo,cscetbon,efalcao,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5799,,,,CASSANDRA-5359,CASSANDRA-4206,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,336337,,,Tue Mar 18 14:36:05 UTC 2014,,,,,,,,,,"0|i1m0jz:",336661,1.2.7,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"03/Jul/13 16:37;efalcao;Although it may be the same root issue, I created a new ticket because this is not exclusively in the realm of HintedHandoff (as in the other linked issues).;;;","18/Mar/14 14:36;jbellis;As with the other errors from 2-pass compaction, the fix is to upgrade to 2.0.x.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cql3 reader returns duplicate rows if the cluster column is reversed,CASSANDRA-5718,12655805,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,alexliu68,alexliu68,02/Jul/13 16:57,16/Apr/19 09:32,14/Jul/23 05:53,05/Aug/13 22:00,1.2.9,,,,,,0,,,,,"To reproduce it,

cqlsh:test>  select * from wordfreq;

 title   | occurances | word
---------+------------+-------
 alex123 |          4 |  liu3
   alex1 |      23456 |  liu2
  alex10 |         10 | liu10
  alex12 |         34 |  liu3
    alex |     123456 |  liu1
    alex |       1000 |   liu


CREATE TABLE wordfreq ( title text, word text, occurances int, PRIMARY KEY (title,occurances)) WITH CLUSTERING ORDER by (occurances DESC);

The hadoop job returns 7 rows instead of 6 rows. 

I will post a patch soon.
",,alexliu68,jjordan,rohitbrai,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-7100,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/13 20:23;alexliu68;5718-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12590500/5718-1.2-branch.txt","01/Aug/13 18:04;alexliu68;5718-2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12595459/5718-2-1.2-branch.txt","02/Aug/13 17:28;alexliu68;5718-3-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12595633/5718-3-1.2-branch.txt",,,,,,,,,,,,,,,,,,3.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,336080,,,Sat Apr 26 04:40:33 UTC 2014,,,,,,,,,,"0|i1lyz3:",336404,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"02/Jul/13 20:23;alexliu68;Patch for 1.2 branch is attached;;;","11/Jul/13 15:13;jbellis;Looks to me like this doesn't handle reversed for non-composite cells.;;;","15/Jul/13 23:01;alexliu68;create a compact table

{code}
 CREATE TABLE wordfreq ( title text, word text, occurances int, PRIMARY KEY (title,occurances)) 
 WITH COMPACT STORAGE and CLUSTERING ORDER by (occurances DESC);
{code}

show the schema

{code}
  cqlsh:test> select key_aliases, column_aliases, key_validator, comparator from system.schema_columnfamilies where   keyspace_name='test';

 key_aliases | column_aliases | key_validator                            | comparator
-------------+----------------+------------------------------------------+-----------------------------------------------------------------------------------------
   [""title""] | [""occurances""] | org.apache.cassandra.db.marshal.UTF8Type | org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.Int32Type)
{code}

It has the clustering column and reversed type defined for the table in system.schema_columnfamilies table.

The patch covers the compact storage type tables. Is there any other type of thrift table I am missing? ;;;","29/Jul/13 21:25;jbellis;Right; I'm saying that the only place this sets reversed away from its default of false is here:

{code}
+        if (comparatorValidator instanceof CompositeType)
+        {
+            for (int i = 0; i < clusterColumns.size(); i++)
+                clusterColumns.get(i).reversed = (((CompositeType) comparatorValidator).types.get(i) instanceof ReversedType);
{code};;;","01/Aug/13 18:04;alexliu68;Oh, I get it. 5718-2-1.2-branch.txt is attached to fix the none-composite reversed type of comparator;;;","02/Aug/13 17:28;alexliu68;5718-3-1.2-branch.txt is attached as the latest patch.;;;","05/Aug/13 22:00;jbellis;LGTM, committed;;;","26/Apr/14 04:40;rohitbrai;[~jbellis] [~alexliu68]

There is a regression in 2.0.x builds of Cassandra and the trunk.

The changes done for this issue were inadvertently overwritten in a merge from 1.2 to 2.0
https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blobdiff;f=src/java/org/apache/cassandra/hadoop/cql3/CqlPagingRecordReader.java;h=d9b9a39cebe0ec50e5e80c6f8057d4cce0646b6a;hp=b6e793c7e46adf3694c71e365833960c650c8152;hb=3a4d6beb;hpb=8e7d7285cdeac4f2527c933280d595bbddd26935

Should I submit a patch against 2.0 and trunk?
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remark on cassandra-5273 : Hanging system after OutOfMemory. Server cannot die due to uncaughtException handling,CASSANDRA-5716,12655574,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,ignaced,ignaced,01/Jul/13 10:28,16/Apr/19 09:32,14/Jul/23 05:53,02/Aug/13 03:30,1.2.7,,,,,,0,,,,,"Possible incorrect handling of an OOM as a result of modifications made for issue cassandra-5273.
I could reproduce the OOM, with the patch of Cassandra-5273 applied.
The good news is that, at least in my case, it works fine : the system did die !
 
However, due to multiple uncaughtException handling, multiple threads are calling the exitThread.start() routine, causing an IllegalStateException. There are some other exceptions also, but that seems logical. Also, after calling the start() function, the thread(s) will continue to run, and that could not be wanted.
 
Below I pasted the stack trace.
Just for your information, after all this works, and I could restart the Cassandra server and redo the OOM

[stack trace moved to http://aep.appspot.com/display/mQFNFHUh1VvQJYGcxRK0lQSM2j8/ ]",linux,ignaced,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,335849,,,Wed Jul 03 08:29:07 UTC 2013,,,,,,,,,,"0|i1lxjr:",336173,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"01/Jul/13 16:12;jbellis;fixed in 40f0bdce069db14e912f28d7351c4b602389c6a5;;;","02/Jul/13 16:49;ignaced;I did try your latest patch, applied on 1.2.5. Looks good, no more exceptions due to the start() calls. Question/remark : Also that implementation to shutdown allows the uncaught exceptions to finish without problems. Should these be blocked instead?;;;","02/Jul/13 17:00;jbellis;I'm not sure what you mean.;;;","03/Jul/13 08:29;ignaced;Sorry, I forgot the fact that the thread is dead when the uncaught exception handling is called and thus was thinking that thread would continue to run until the system.exit would stop all the threads.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reverse slice queries can skip range tombstones,CASSANDRA-5712,12655335,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,28/Jun/13 13:45,16/Apr/19 09:32,14/Jul/23 05:53,01/Jul/13 07:38,1.2.7,,,,,,0,,,,,"On disk, we represent range tombstones by a marker at the beginning of the range covered. Since we repeat such markers when they overlap an index block and since an index block is always read in forward order (even in reverse queries), we are guaranteed to see a range tombstone before any column it covers. However, IndexedSliceReader returns the columns of an index block in reverse order and thus can return a range tombstone *after* columns it covers.

It follows that some range tombstone can be skipped during a reversed range slice. We need to fix IndexedSliceReader to always return range tombstone first (or at least before the first column covered by each range tombstone).",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jun/13 13:48;slebresne;5712.txt;https://issues.apache.org/jira/secure/attachment/12590037/5712.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,335612,,,Mon Jul 01 07:38:23 UTC 2013,,,,,,,,,,"0|i1lw33:",335936,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"28/Jun/13 13:48;slebresne;Attaching patch with a unit test to demonstrate the bug as well as a fix. The fix simply make sure that for an index block we always return all the range tombstone first before returning any column. In theory this means we might sometimes return a few range tombstones the caller won't really need, but it's harmless and probably not worth bothering with.;;;","28/Jun/13 15:54;jbellis;Do we really need rangeTombstonesReversed?  ISTM that we could just special case tombstones in addColumn (rename to addAtom) to always addFirst regardless of reversed-ness.;;;","28/Jun/13 16:31;slebresne;bq. ISTM that we could just special case tombstones in addColumn (rename to addAtom) to always addFirst regardless of reversed-ness.

I don't think that works :).

We get t(0), c(1), c(2), t(3), c(4) from the reader and we want to ultimately return them as t(3), t(0), c(4), c(2), c(1).

If we do addFirst for columns and addLast for tombstone, we'll get c(4), c(2), c(1), t(0), t(3), which gives us all tombstone last (and if we pollLast instead of pollFirst, the columns won't be reversed). If we do addLast for columns and addFirst for tombstone, we have the exact equivalent of pollLast in the previous case.

We can probably do some peekLast first and do pollLast as long as it's a tombstone and pollFirst if it's not, but is that really much of an improvement compared to the patch?
;;;","29/Jun/13 05:34;jbellis;I'm not a huge fan of having two Collections of atoms involved, but you're right, it's not as simple as I thought.  +1 on the patch.;;;","01/Jul/13 07:38;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compression chunk_length shouldn't be mandatory,CASSANDRA-5707,12654959,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,26/Jun/13 16:20,16/Apr/19 09:32,14/Jul/23 05:53,27/Jun/13 07:57,2.0 beta 1,,,,,,0,,,,,CASSANDRA-5693 introduced a minor regression on trunk in that we refuse compression parameters that don't have the chunk_length option. This is not the case in 1.2 where we just use the default.,,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/13 16:21;slebresne;5707.txt;https://issues.apache.org/jira/secure/attachment/12589770/5707.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,335236,,,Thu Jun 27 07:57:08 UTC 2013,,,,,,,,,,"0|i1ltrr:",335560,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"26/Jun/13 16:21;slebresne;Trivial patch attached.;;;","26/Jun/13 18:04;aleksey;+1;;;","27/Jun/13 07:57;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
OOM while loading key cache at startup,CASSANDRA-5706,12654953,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,frousseau,frousseau,frousseau,26/Jun/13 15:40,16/Apr/19 09:32,14/Jul/23 05:53,28/Jun/13 15:31,1.2.7,,,,,,0,,,,,"Steps to be able to reproduce it :
 - have a heap of 1Gb
 - have a saved key cache without the SSTables

When looking at KeyCacheSerializer.serialize : it always writes a Boolean
When looking at KeyCacheSerializer.deserialize : no Boolean is read if SSTable is missing...

In case of a promoted index, RowIndexEntry.serializer.skip(...) should be called rather than RowIndexEntry.serializer.skipPromotedIndex(...) (again for symmetry between serialization/deserialization)

Attached is a proposed patch",,frousseau,rcoli,tomvandenberge,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/13 15:41;frousseau;5706-OOM-while-loading-key-cache-at-startup.patch;https://issues.apache.org/jira/secure/attachment/12589764/5706-OOM-while-loading-key-cache-at-startup.patch","26/Jun/13 22:09;jbellis;5706-v2-txt;https://issues.apache.org/jira/secure/attachment/12589803/5706-v2-txt",,,,,,,,,,,,,,,,,,,2.0,frousseau,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,335230,,,Fri Oct 18 08:47:42 UTC 2013,,,,,,,,,,"0|i1ltqf:",335554,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"26/Jun/13 22:00;jbellis;(setting affects- to 1.2.5 b/c suspect was introduced by CASSANDRA-5492);;;","26/Jun/13 22:06;jbellis;(actually, annotate says this dates back to CASSANDRA-3762);;;","26/Jun/13 22:09;jbellis;Your analysis is correct; attached is slightly revised patch to emphasize that we already read exactly one boolean;;;","27/Jun/13 23:58;jbellis;LGTY Fabien?;;;","28/Jun/13 07:56;frousseau;Sorry for the delay.

Yep, it looks good to me.
;;;","28/Jun/13 15:31;jbellis;committed;;;","18/Oct/13 08:47;tomvandenberge;I'm having this problem again since I've upgraded to 1.2.10.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow renaming an indexed column,CASSANDRA-5705,12654945,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,26/Jun/13 15:17,16/Apr/19 09:32,14/Jul/23 05:53,26/Jun/13 18:01,2.0 beta 1,,,,,,0,,,,,"We can now index CQL3 primary key parts (CASSANDRA-5125) but also allow to rename such columns. Unfortunately, since 2ndary index CF names currently depend on the name of the column they index, renaming an indexed column basically ends up dropping the index.

For now, the simplest solution is probably to just forbid renaming when the column is 2ndary indexed. If you really want to rename, you can always drop the index and recreate it after the rename.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/13 15:18;slebresne;5705.txt;https://issues.apache.org/jira/secure/attachment/12589762/5705.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,335222,,,Wed Jun 26 18:01:10 UTC 2013,,,,,,,,,,"0|i1lton:",335546,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"26/Jun/13 15:18;slebresne;Attaching trivial patch.;;;","26/Jun/13 17:56;aleksey;+1;;;","26/Jun/13 18:01;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"add cassandra.unsafesystem property (Truncate flushes to disk again in 1.2, even with durable_writes=false)",CASSANDRA-5704,12654927,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,christianmovi,christianmovi,26/Jun/13 13:39,16/Apr/19 09:32,14/Jul/23 05:53,17/Jul/13 19:05,1.2.7,2.0 beta 2,,,,,0,,,,,"I just upgraded my dev-environment to C* 1.2. Unfortunetaly 1.2 makes my JUnit tests slow again, due to a blocking-flush in saveTruncationRecord().

With Cassandra 1.1 truncate was very fast due to: CASSANDRA-4153


My proposal is to make saveTruncationRecord() only flush when durableWrites are enabled.

My assumption is that if somebody turn off durable writes then he does not mind if truncate is not guaranteed to be durable either.


I successfully tested the following patch with my testsuite. Its as fast as it was with 1.1 (maybe even faster!):
{code}
@@ -186,5 +186,8 @@ public class SystemTable
         String req = ""UPDATE system.%s SET truncated_at = truncated_at + %s WHERE key = '%s'"";
         processInternal(String.format(req, LOCAL_CF, truncationAsMapEntry(cfs, truncatedAt, position), LOCAL_KEY));
-        forceBlockingFlush(LOCAL_CF);
+        
+        KSMetaData ksm = Schema.instance.getKSMetaData(cfs.table.name);
+        if (ksm.durableWrites) // flush only when durable_writes are enabled
+            forceBlockingFlush(LOCAL_CF);
     }
{code}
",,christianmovi,rcoli,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-7511,,,,CASSANDRA-4153,,,,,,,,,,,"17/Jul/13 17:50;jbellis;5704-v2.txt;https://issues.apache.org/jira/secure/attachment/12592809/5704-v2.txt","17/Jul/13 23:19;christianmovi;5704_unsafeSystem.txt;https://issues.apache.org/jira/secure/attachment/12592887/5704_unsafeSystem.txt","26/Jun/13 13:45;christianmovi;CASSANDRA_5704_V1_1_2.patch;https://issues.apache.org/jira/secure/attachment/12589747/CASSANDRA_5704_V1_1_2.patch","26/Jun/13 13:45;christianmovi;CASSANDRA_5704_V1_trunk.patch;https://issues.apache.org/jira/secure/attachment/12589748/CASSANDRA_5704_V1_trunk.patch",,,,,,,,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,335204,,,Thu Jul 18 19:00:09 UTC 2013,,,,,,,,,,"0|i1ltkn:",335528,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"26/Jun/13 13:46;christianmovi;I attached patchfiles for 1.2 and trunk;;;","26/Jun/13 14:29;jbellis;Hmm.  I'm not sure I'm convinced that we should not save the truncation record when durable writes are disabled.  Durable writes means ""I'm okay if you lose the data I'm about to write in case of power failure,"" but not flushing the truncate means ""Data that was supposed to be gone, could reappear"" which feels semantically different to me.

WDYT [~brandon.williams] [~tjake]?

(I note that for production use, CASSANDRA-4906 made the biggest difference since now we only need to flush the table being truncated instead of everyone.);;;","26/Jun/13 14:36;brandon.williams;bq. ""Data that was supposed to be gone, could reappear"" which feels semantically different to me.

I tend to agree.  Certainly if the biggest problem here is tests are slow, it's not worth the risk just to speed them up.  We could do something like add a flag if we need this.;;;","26/Jun/13 14:55;tjake;I agree, perhaps add sone test.mode property that skips it for unit tests?;;;","26/Jun/13 15:44;christianmovi;{quote}""Data that was supposed to be gone, could reappear"" which feels semantically different to me.{quote}
I can understand that. My thought was that durable_writes already applies to deletes. So I thought why not also apply that pattern to truncate?


I could live with a new ""test.mode"" property though (in cassandra.yaml, right?). I would even go as far to say that this one should also deactivate all fsyncs in Cassandra :-)

Nevertheless, for practical reasons I like the idea of using the durable_writes flag, because it allows me to use have different behaviour on my dev-laptop for...
- ... one keyspace for my junits. These require constant truncation.
- ... my local installation of our application. Here I like a bit more data safety, because I not want to set up my testdata all the time.
;;;","26/Jun/13 15:47;christianmovi;{quote}I'm not sure I'm convinced that we should not save the truncation record when durable writes are disabled.{quote}
My understanding was that my patch only stops it from flushing. The record is still being written to a memtable, right?
;;;","17/Jul/13 17:49;jbellis;Yes, it's a memtable write ... to the system.local table, with information about the truncated table.

No doubt you would reply, ""then let's skip the flush when durable writes are disabled on the system keyspace,"" but you'll note that we've already special cased this flush even though the memtable writes is commitlogged -- we want it to persist immediately even with periodic commitlog mode.  Truncated data reappearing is very high on the list of Behaviors That Surprise People.

Therefore I propose the attached patch along the lines of what Jake and Brandon suggest, that adds a {{cassandra.unsafetruncate}} property.  (Normally I am not a fan of using a 'negative' property name like 'unsafe' but I don't want to encourage people to use it, who shouldn't, which I think we risk if we use {{quicktruncate}} or similar.);;;","17/Jul/13 18:08;brandon.williams;+1;;;","17/Jul/13 19:05;jbellis;Committed;;;","17/Jul/13 23:18;christianmovi;[~jbellis], [~brandon.williams]: I'm fine with a property. Thanks!

Is there any chance we can make it a ""cassandra.unsafesystem""-property and apply the same behaviour to all system table operations? This would then also speed up schema changes, which are are also a pain for tests.

FWIW: I attached a patch which works nicely for me.

Update: Perhaps as a separate 2.0 ticket?
;;;","18/Jul/13 16:45;jbellis;WFM.  Committed w/ light modifications.;;;","18/Jul/13 19:00;christianmovi;Awesome, thanks!

For documentation purposes: The property is: -Dcassandra.unsafesystem=true;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALTER RENAME is broken in trunk,CASSANDRA-5702,12654893,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,26/Jun/13 09:02,16/Apr/19 09:32,14/Jul/23 05:53,04/Jul/13 13:33,2.0 beta 1,,,,,,0,,,,,"CASSANDRA-5125 has broken {{ALTER RENAME}} when the column is a default alias (for thrift column families where the PK columns haven't been renamed yet).

The problem is basically that while we assign default aliases to PK columns when they don't have one, we currently ""fake"" those default aliases and do not persist them. Concretely, CFDefinition is aware of them, but CFMetaData is not, which break renaming post CASSANDRA-5125.

We could fix rename punctually, but there is another related problem: for the same reason, if you try to create an index on a column that is a non-renamed default alias, this doesn't work with the arguably confusing message ""No column definition found for column X"". Here again, we could fix it punctually, but it starts to sound like we need a more general fix.

So I suggest stopping to ""fake"" those default aliases, but instead to just create ""real"" aliases (that are known of CFMetaData and persisted in the schema) when there is none. After all, from a user point of view, why should a default column name be any special. And on top of fixing the issues above, this also:
# fix CASSANDRA-5489 in a somewhat simpler way
# makes it easier for clients reading the schema CFs. They won't to infer the default aliases anymore.

The only theoretical downside is that we lose the information that a given CQL3 column name is one assigned by default versus one set up by the user, but given the user can rename those column names anyway, not sure this matters in any way.
",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/13 13:22;slebresne;5702.txt;https://issues.apache.org/jira/secure/attachment/12589744/5702.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,335170,,,Thu Jul 04 13:33:44 UTC 2013,,,,,,,,,,"0|i1ltd3:",335494,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"26/Jun/13 10:44;aleksey;Thought about this as well (in the context of CASSANDRA-5489) and came to the same conclusion.;;;","26/Jun/13 13:22;slebresne;Attaching patch for this. Note that persisting those default columns implied to only rebuild CQL3 metadata once the CFMetaData is basically complete, hence the patch moves the calls to said rebuild a bit (it's a good thing anyway, we were calling that rebuild way too often). Also:
* the patch fixes a small bug in CFMetaData.isThriftCompatible(). It was excluding way too much stuff post CASSANDRA-5125.
* this happen to lift the limitation introduced by CASSANDRA-5531 (not a big deal but not a bad thing either).
* I've also rewritten CFMetaData.isDense(). I don't think that was strictly necessary, but while trying to check it wasn't broken by the patch it felt it could be simplified/clarified.
;;;","26/Jun/13 17:25;aleksey;+1;;;","26/Jun/13 17:37;slebresne;Committed, thanks;;;","03/Jul/13 17:36;brandon.williams;This broke the wide_slice_test dtest (which admittedly is using cql2, but we shouldn't regress there);;;","04/Jul/13 13:33;slebresne;Had forgot a toUppercase() call when checking the CQL2 key alias. I've committed the trivial fix as b7e49b3.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Apache.Cassandra.Cassandra.get_count will disconnect but not throw InvalidRequestException when column family is not exist.,CASSANDRA-5701,12654890,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,lyubent,yuemaoxing,yuemaoxing,26/Jun/13 08:41,16/Apr/19 09:32,14/Jul/23 05:53,20/Dec/13 20:40,2.0.5,,,Legacy/CQL,,,0,,,,,"When I use get_count interface which is defined in Cassandra.thrift, the Cassandra Server(1.2.4) close the connection from Client when column family is not exist in that keyspace but not throw InvalidRequestException. 

It seemed the get_count method in cassandra.thrift.CassandraServer.java did not validate parameters(ThriftValidation.validateColumnFamily) in this method.


system.log:
ERROR [RPC-Thread:3373] 2013-06-26 14:23:09,264 TNonblockingServer.java (line 638) Unexpected exception while invoking!
java.lang.IllegalArgumentException: Unknown table/cf pair (Keyspace1.Standard)
	at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:165)
	at org.apache.cassandra.thrift.CassandraServer.get_count(CassandraServer.java:471)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_count.getResult(Cassandra.java:3381)
	at org.apache.cassandra.thrift.Cassandra$Processor$get_count.getResult(Cassandra.java:3369)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.thrift.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:632)
	at org.apache.cassandra.thrift.CustomTHsHaServer$Invocation.run(CustomTHsHaServer.java:109)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)


Please check this bug, thanks!","cassandra server : 1.2.4
and cassandra1.2.5 has same bug.
client : C# client",lyubent,yuemaoxing,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,86400,86400,,0%,86400,86400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Nov/13 15:42;lyubent;5701_cassandra-2.0.patch;https://issues.apache.org/jira/secure/attachment/12616062/5701_cassandra-2.0.patch",,,,,,,,,,,,,,,,,,,,1.0,lyubent,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,335167,,,Fri Dec 20 20:40:06 UTC 2013,,,,,,,,,,"0|i1ltcf:",335491,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"02/Aug/13 02:39;yuemaoxing;Uh...;;;","14/Aug/13 02:21;yuemaoxing;Is it not important?;;;","27/Nov/13 15:41;lyubent;Added a catch clause to handle IllegalArgumentException and throw a InvalidRequestException which will be picked up by the client.;;;","27/Nov/13 15:44;lyubent;This is also reproducible in 2.0.x;;;","27/Nov/13 18:18;jbellis;patch is on the doInsert method; still need fix for get_count?.  also, build fails b/c you're mixing thrift and internal IRE classes.;;;","19/Dec/13 23:36;jbellis;Where were we on this?  ISTR having some issues applying the patch.  You were going to post a branch?;;;","20/Dec/13 11:13;lyubent;Here's the [branch|https://github.com/lyubent/cassandra/tree/5701] I think the problem was that the patch was added to trunk, but this branch is on cassandra-2.0 and should build properly. ;;;","20/Dec/13 20:40;jbellis;LGTM, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Streaming (2.0) can deadlock,CASSANDRA-5699,12654745,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,25/Jun/13 14:57,16/Apr/19 09:32,14/Jul/23 05:53,03/Jul/13 17:56,2.0 beta 1,,,,,,0,,,,,"The new streaming implementation (CASSANDRA-5286) creates 2 threads per host for streaming, one for the incoming stream and one for the outgoing one. However, both currently share the same socket, but since we use synchronous I/O, a read can block a write, which can result in a deadlock if 2 nodes are both blocking on a read a the same time, thus blocking their respective writes (this is actually fairly easy to reproduce with a simple repair).

So instead attaching a patch that uses one socket per thread.

The patch also correct the stream throughput throttling calculation that was 8000 times lower than what it should be.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jul/13 09:50;slebresne;5699.txt;https://issues.apache.org/jira/secure/attachment/12590400/5699.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,335022,,,Wed Jul 03 17:56:00 UTC 2013,,,,,,,,,,"0|i1lsgf:",335346,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"29/Jun/13 18:21;jbellis;Is the ""stream lifecycle"" documented anywhere the way we did in 1.2 StreamOut?

NB: this patches StreamingRepairTask, which does not exist in current trunk.;;;","01/Jul/13 11:05;slebresne;bq. Is the ""stream lifecycle"" documented anywhere

Yuki had written https://gist.github.com/yukim/5672508. The one thing this patch changes compared to that ""design document"" is that the initialization phase is slightly more complex since we need to create 2 connection. So the first node sends a StreamInit to the other end to create the first connection (as was done previously), but then the remote side creates a connection back sending a StreamInit message of it's own. Then and only then do we go to the prepare phase.

In any case, we probably want that lifecycle to be documented in the javadoc or we'll lose track of it, so I've described this in relative detail at the head of StreamSession.

So updated the patch with those added comments and the modification to StreamRepairTask cleaned up.
;;;","01/Jul/13 23:48;jbellis;It's somewhat confusing that we use StreamInit both as leader and as follower, to mean different things, but I get how the MessagingService architecture makes it difficult to do otherwise.  As a minor improvement, suggest renaming isInitiator to sentByInitiator.

What is going on with the switch from {{Set<UUID> ongoingSessions}} to {{Map<InetAddress, StreamSession> ongoingSessions}} in SRF?

Nit: onConnect could mean ""leader connects to follower"" or [what it actually means] ""stream is fully connected.""  Suggest renaming, e.g. onSessionEstablished.

Other comments on new Streaming:

Somewhat confused by logic in complete() -- ""I received a Complete message.  If I'm already waiting for a complete message, close the session.  Otherwise, wait for [another] Complete message"" ?

It looks like there may be synchronization issues with ""state;"" some accesses are synchronized and some are not.

Why is init broken out from construction?  Makes some things awkward, e.g. streamResult which is final-post-init but we have to null-check until then.

Why do we include a String description in SIM?

When does follower immediately have something to stream to leader, is this a Repair optimization?

;;;","02/Jul/13 09:50;slebresne;bq. It's somewhat confusing that we use StreamInit both as leader and as follower, to mean different things

I guess, though if you see StreamInit as a way for both node to open/initialize their outgoing connection to the other node (which is what this is doing), it's not really that different. And it's somewhat consistent with the rest of the protocol, where each side will sent a Prepare and then finally a Complete.  Anyway, updated the patch with the sentByInitiator renaming.

bq. What is going on with the switch from Set<UUID> ongoingSessions to Map<InetAddress, StreamSession> ongoingSessions in SRF?

ongoingSession only role initialy was to track how many sessions for a StreamResultFuture were live to know when we're done (SRF.maybeComplete()). In the original patch of CASSANDRA-5286, it wasn't even there, and there was just an AtomicInteger decremented each time a session completed. I was slightly afraid that a race could have us counting the same session done twice, so I changed it to a Set<UUID> and added a simple UUID per session that was only used for that purpose (it was never send to the other side or anything). Anyway, that UUID addition was stupid in the first place since for a SRF, there is only one StreamSession per remote host, so it should have been a Set<InetAddress> to start with and we have no use for a StreamSession UUID.

But on top of that, this patch add the need to be able to find back a StreamSession in a SRF given the remote endpoint (in IncomingStreamConnection, when the initiator gets the other side StreamInit), so we really need a map now.

bq. Suggest renaming, e.g. onSessionEstablished

Agreed, though I've renamed to onInitializationComplete to be consistent with the wording of the javadoc.

bq. Somewhat confused by logic in complete() – ""I received a Complete message. If I'm already waiting for a complete message, close the session. Otherwise, wait for [another] Complete message"" ?

Each side will send a CompleteMessage to say ""I'm done on my side"", and the session will be considered really complete (and closed) when we got the Complete for both side. So WAIT_COMPLETE means ""I've seen only one complete message so far"". Hence the logic of complete() is ""I just received a complete from the other side, if I had already completed my side, close the session, otherwise move to the ""I'm waiting for the other complete"" state"".

bq. It looks like there may be synchronization issues with ""state;""

I think we're good, because the only case where we can race to set a state is during the completion phase, for WAIT_COMPLETE and COMPLETE, and those are protected. For other states, there are set sequentially on each node by construction.

bq. Why is init broken out from construction?

SRF takes his StreamSession in his ctor so we can't have StreamSessions take their SRF without some other refactor.  Now don't get me wrong, it bugs me too. And I do think there is a few things we can do to make that whole new streaming API simpler/cleaner (including probably renaming StreamRepairFuture). And I'm volonteering to give to a shot to that. But in another follow ticket because:
# I really think that kind of code cleanup shouldn't block beta1 (but this ticket, the fact that streaming deadlock, do is a blocker)
# I've already rewrote quite a bit of Yuki's code without him having the change to chime in. Would feel fair to wait to him to be back before refactoring parts that are not really crucial for beta1.

bq. Why do we include a String description in SIM?

This the description of what the operation is doing (""Repair"", ""Bootstrap"", ""Restore replica count"", ...)

bq. When does follower immediately have something to stream to leader

A StreamSession in the initiator handle both incoming and outgoing streams. Sometimes we only have outgoing ones (Unbootstrap/Restore replica count/Bulkloading), sometimes only
incoming ones (Bootstrap) and sometimes both (Repair indeed but also the SS.RangeRelocator for moves and vnodes tokens relocation). Does that answer your question?
;;;","03/Jul/13 15:47;jbellis;Ship it!;;;","03/Jul/13 17:56;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh should support collections in COPY FROM,CASSANDRA-5698,12654636,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,kisehireoshi,kisehireoshi,25/Jun/13 01:12,16/Apr/19 09:32,14/Jul/23 05:53,25/Jul/13 18:48,1.2.8,2.0 rc1,,Legacy/Tools,,,0,collections,cqlsh,,,"Concrete operation is as follows.
---------*---------*---------*---------*---------*---------*---------*---------*
(1)map type's export/import
<export>
[root@castor bin]# ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> create keyspace maptestks with replication  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
cqlsh> use maptestks;
cqlsh:maptestks> create table maptestcf (rowkey varchar PRIMARY KEY, targetmap map<varchar,varchar>);
cqlsh:maptestks> insert into maptestcf (rowkey, targetmap) values ('rowkey',{'mapkey':'mapvalue'});
cqlsh:maptestks> select * from maptestcf;

 rowkey | targetmap
--------+--------------------
 rowkey | {mapkey: mapvalue}
cqlsh:maptestks>  copy maptestcf to 'maptestcf-20130619.txt';
1 rows exported in 0.008 seconds.
cqlsh:maptestks> exit;

[root@castor bin]# cat maptestcf-20130619.txt
rowkey,{mapkey: mapvalue}
　　　　　　　　　　　　　　　　　　　　　　　　　　　<------------------------(a)
<import>
[root@castor bin]# ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> create keyspace mapimptestks with replication  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
cqlsh> use mapimptestks;
cqlsh:mapimptestks> create table mapimptestcf (rowkey varchar PRIMARY KEY, targetmap map<varchar,varchar>);

cqlsh:mapimptestks> copy mapimptestcf from ' maptestcf-20130619.txt ';
Bad Request: line 1:83 no viable alternative at input '}'
Aborting import at record #0 (line 1). Previously-inserted values still present.
0 rows imported in 0.025 seconds.
---------*---------*---------*---------*---------*---------*---------*---------*
(2)list type's export/import
<export>
[root@castor bin]#./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> create keyspace listtestks with replication  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
cqlsh> use listtestks;
cqlsh:listtestks> create table listtestcf (rowkey varchar PRIMARY KEY, value list<varchar>);
cqlsh:listtestks> insert into listtestcf (rowkey,value) values ('rowkey',['value1','value2']);
cqlsh:listtestks> select * from listtestcf;

 rowkey | value
--------+------------------
 rowkey | [value1, value2]

cqlsh:listtestks> copy listtestcf to 'listtestcf-20130619.txt';
1 rows exported in 0.014 seconds.
cqlsh:listtestks> exit;

[root@castor bin]# cat listtestcf-20130619.txt
rowkey,""[value1, value2]""
　　　　　　　　　　　　　　　　　　　　　　　　　　　<------------------------(b)
<export>
[root@castor bin]# ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> create keyspace listimptestks with replication  = { 'class' : 'SimpleStrategy', 'replication_factor' : '1' };
cqlsh> use listimptestks;
cqlsh:listimptestks> create table listimptestcf (rowkey varchar PRIMARY KEY, value list<varchar>);
cqlsh:listimptestks> copy listimptestcf from ' listtestcf-20130619.txt ';
Bad Request: line 1:79 no viable alternative at input ']'
Aborting import at record #0 (line 1). Previously-inserted values still present.
0 rows imported in 0.030 seconds.
---------*---------*---------*---------*---------*---------*---------*---------*
Reference: (correct, or error, in another dimension)

Manually, I have rewritten the export file.
[root@castor bin]# cat nlisttestcf-20130619.txt
rowkey,""['value1',' value2']""

....
cqlsh:listimptestks> copy listimptestcf from 'nlisttestcf-20130619.txt';
1 rows imported in 0.035 seconds.

cqlsh:listimptestks> select * from implisttestcf;
 rowkey | value
--------+------------------
 rowkey | [value1, value2]
cqlsh:implisttestks> exit;

[root@castor bin]# cat nmaptestcf-20130619.txt
rowkey,”{'mapkey': 'mapvalue'}”

[root@castor bin]# ./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 1.2.5 | CQL spec 3.0.0 | Thrift protocol 19.36.0]
Use HELP for help.
cqlsh> use  mapimptestks;
cqlsh:mapimptestks> copy mapimptestcf from 'nmaptestcf-20130619.txt';
1 rows imported in 0.023 seconds.
cqlsh:mapimptestks> select * from mapimptestcf;

 rowkey | targetmap
--------+--------------------
 rowkey | {mapkey: mapvalue}

(It appears to be as normal processing.)
---------*---------*---------*---------*---------*---------*---------*---------*
Please confirm from the operation described above.
Comparing the above (a) and (b), in the data format of export file, 
only the presence or absence of (""), 
it suggests a lack of consistency of the treatment(?).
","Using the Copy of cqlsh, the data included the “{“ and “[“ (= CollectionType) case,
I think in the export / import process, data integrity is compromised.
",aleksey,kisehireoshi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5710,,,,,,,,,,,,,,,,,,,,"21/Jul/13 17:49;aleksey;5698.txt;https://issues.apache.org/jira/secure/attachment/12593424/5698.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334913,,,Thu Jul 25 18:48:13 UTC 2013,,,,,,,,,,"0|i1lrs7:",335237,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"25/Jun/13 06:47;aleksey;Nothing is compromised, but collections import isn't supported by cqlsh, yet, if either keys or values require quoting (ascii, text(varchar), timestamp, and inet types).;;;","25/Jun/13 07:07;jbellis;Should we tag it 2.0.1 instead of closing?  Seems reasonably important for COPY to be feature-complete to me.;;;","25/Jun/13 07:12;aleksey;As you wish;;;","25/Jul/13 16:22;brandon.williams;+1;;;","25/Jul/13 18:48;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cqlsh doesn't allow semicolons in BATCH statements,CASSANDRA-5697,12654635,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,rspitzer,rspitzer,25/Jun/13 00:59,16/Apr/19 09:32,14/Jul/23 05:53,08/Jul/13 20:36,1.2.7,2.0 beta 1,,Legacy/Tools,,,0,cqlsh,,,,"The documentation for BATCH statements declares that semicolons are required between update operations. Currently including them results in an error 'expecting K_APPLY'. To match the design specifications, semi-colons should be allowed or optional. 

","Mac OSX, cqlsh 3.0.2",aleksey,rspitzer,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jul/13 19:27;aleksey;5697.txt;https://issues.apache.org/jira/secure/attachment/12590933/5697.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334912,,,Mon Jul 08 20:36:07 UTC 2013,,,,,,,,,,"0|i1lrrz:",335236,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"25/Jun/13 05:46;jbellis;The grammar suggests that it was intended for semicolon to be optional.  I guess some kind of antlr nondeterminism makes it not work as designed?;;;","25/Jun/13 07:19;aleksey;The semicolon is optional. You are getting this error because cqlsh splits its statements internally. Using python-cql directly (or any other client), the semicolons will be accepted.

It's questionable whether or not messing with cqlsh internals just to allow the semicolons in BATCH is worth it, but I'll have a look later today.;;;","08/Jul/13 19:53;brandon.williams;+1;;;","08/Jul/13 20:36;aleksey;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Upgrading to cassandra-1.2 with a dead LEFT state from 1.1 causes problems,CASSANDRA-5696,12654598,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,24/Jun/13 21:34,16/Apr/19 09:32,14/Jul/23 05:53,26/Jun/13 21:16,1.2.7,,,,,,0,qa-resolved,,,,"In 1.1, we wrote LEFT states as LEFT,token,expiretime in gossip.  However in 1.2, VersionValue serializes this to LEFT,expiretime,tokens and causes the upgrade 1.2 to try and parse it this way as well, causing it to try to parse the token as an expiretime.

Another wrinkle to this is assassinate still writes it the old way: LEFT,tokens,expiretime.",,christianmovi,jasobrown,jeromatron,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/13 19:19;brandon.williams;5696.txt;https://issues.apache.org/jira/secure/attachment/12589783/5696.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334875,,,Wed Jun 26 21:16:07 UTC 2013,,,,,,,,,,"0|i1lrjr:",335199,,,,,,,,,jasobrown,,jasobrown,Normal,,,,,,,,,,,,,,,enigmacurry,,,"26/Jun/13 19:19;brandon.williams;There are a few different ways this can be solved, but it's obvious that the most correct thing to do is correct the order in VV.  This too turns out to be workable though, as long as you either a) do a full ring restart (to wipe the dead states) or b) do not upgrade within 72 hours of decommissioning a node.  Almost everyone will likely fall into group b) but if they don't, and don't read the warning in NEWS.txt the possible things that can happen are:

* they receive a harmless NumberFormatException everywhere
* they carry a useless dead state around for a token that doesn't exist for an indeterminate amount of time (which assassinate could fix)
* the timestamp used for the decom expiration matches a node's token and it will need to be  shutdown and rebootstrapped

None of these seem world-ending, with the last one being the worst and also least likely to occur, so here's a patch where we just change the VV order and update NEWS.;;;","26/Jun/13 20:37;jasobrown;After we make this change, won't the same problems apply to 1.2.\{0..6\} -> 1.2.7+/2.0 upgrades? The VV fields will again be inverted, although arguably in a more internally consistent manner (with 1.1)?
;;;","26/Jun/13 20:45;jasobrown;Thought a little bit more, and I think this patch is fine. +1;;;","26/Jun/13 21:16;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Race condition in detecting version on a mixed 1.1/1.2 cluster,CASSANDRA-5692,12654375,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,sbtourist,sbtourist,sbtourist,23/Jun/13 12:57,16/Apr/19 09:32,14/Jul/23 05:53,27/Jun/13 20:40,1.2.7,2.0 beta 1,,,,,0,,,,,"On a mixed 1.1 / 1.2 cluster, starting 1.2 nodes fires sometimes a race condition in version detection, where the 1.2 node wrongly detects version 6 for a 1.1 node.

It works as follows:
1) The just started 1.2 node quickly opens an OutboundTcpConnection toward a 1.1 node before receiving any messages from the latter.
2) Given the version is correctly detected only when the first message is received, the version is momentarily set at 6.
3) This opens an OutboundTcpConnection from 1.2 to 1.1 at version 6, which gets stuck in the connect() method.

Later, the version is correctly fixed, but all outbound connections from 1.2 to 1.1 are stuck at this point.

Evidence from 1.2 logs:
TRACE 13:48:31,133 Assuming current protocol version for /127.0.0.2
DEBUG 13:48:37,837 Setting version 5 for /127.0.0.2",,cdaw,christianmovi,jasobrown,jjordan,rcoli,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jun/13 12:52;sbtourist;5692-0005.patch;https://issues.apache.org/jira/secure/attachment/12589397/5692-0005.patch","25/Jun/13 17:23;sbtourist;5692-0006.patch;https://issues.apache.org/jira/secure/attachment/12589627/5692-0006.patch",,,,,,,,,,,,,,,,,,,2.0,sbtourist,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334652,,,Tue Oct 15 16:53:38 UTC 2013,,,,,,,,,,"0|i1lq6v:",334978,,,,,,,,,jasobrown,,jasobrown,Low,,,,,,,,,,,,,,,,,,"23/Jun/13 13:01;sbtourist;I believe the correct thing to do is to _never_ assume a version when opening an outbound connection: rather, retrying a few times waiting for the version to be detected, and eventually failing fast.;;;","23/Jun/13 13:33;sbtourist;Attaching patch with fail fast behaviour if no version is known yet after 30 seconds.;;;","23/Jun/13 15:23;sbtourist;Attaching a second patch, cleaner and more correct imho.;;;","23/Jun/13 16:15;sbtourist;Third patch, even better :);;;","23/Jun/13 16:34;sbtourist;Cleaned up things, left only the two relevant versions.;;;","23/Jun/13 22:46;cdaw;[~sbtourist]
i can apply patch 001 or 004 successfully, but not able to apply 001 + 004.   Do I need both?

{noformat}
# via git apply
$ git reset --hard HEAD
HEAD is now at 0f7c7dc Merge pull request #15 from riptano/CASSANDRA-5476

$ git apply --whitespace=fix 5692-0001.patch 5692-0004.patch
5692-0001.patch:37: trailing whitespace.

5692-0004.patch:89: trailing whitespace.

5692-0004.patch:133: trailing whitespace.

5692-0004.patch:138: trailing whitespace.

5692-0004.patch:143: trailing whitespace.

error: patch failed: src/java/org/apache/cassandra/net/OutboundTcpConnection.java:278
error: src/java/org/apache/cassandra/net/OutboundTcpConnection.java: patch does not apply

# via unix patch
$ git reset --hard HEAD
HEAD is now at 0f7c7dc Merge pull request #15 from riptano/CASSANDRA-5476

$ patch -l -p1 < 5692-0001.patch
patching file src/java/org/apache/cassandra/net/OutboundTcpConnection.java

$ patch -l -p1 < 5692-0004.patch
patching file src/java/org/apache/cassandra/net/MessagingService.java
patching file src/java/org/apache/cassandra/net/OutboundTcpConnection.java
Hunk #1 FAILED at 278.
1 out of 1 hunk FAILED -- saving rejects to file src/java/org/apache/cassandra/net/OutboundTcpConnection.java.rej

$ cat src/java/org/apache/cassandra/net/OutboundTcpConnection.java.rej
***************
*** 278,284 ****
          if (logger.isDebugEnabled())
              logger.debug(""attempting to connect to "" + poolReference.endPoint());

-         targetVersion = MessagingService.instance().getVersion(poolReference.endPoint());

          long start = System.currentTimeMillis();
          while (System.currentTimeMillis() < start + DatabaseDescriptor.getRpcTimeout())
--- 278,284 ----
          if (logger.isDebugEnabled())
              logger.debug(""attempting to connect to "" + poolReference.endPoint());

+         targetVersion = MessagingService.instance().getVersion(poolReference.endPoint(), true);

          long start = System.currentTimeMillis();
          while (System.currentTimeMillis() < start + DatabaseDescriptor.getRpcTimeout())
{noformat} ;;;","23/Jun/13 22:56;sbtourist;[~cdaw], those are two different versions of the same patch, you don't need both: it's either one or the other, up to the reviewer for discussion, I second the latter.;;;","24/Jun/13 12:52;sbtourist;Given the first message which should setup the version is sent along the same connection, this patch doesn't actually work, causing two 1.2 nodes to block each other during bootstrap.

So I'm attaching a different patch (0005), which implements a simple handshake by assuming version 6 and trying to read the actual version on a different thread, so that it can be interrupted (disconnected) and can retry the handshake until one of the following happens:
1) The version is confirmed to be >= 6, and the handshake succeeds.
2) The version is an old one, hence it is expected to be found among the tracked versions when the first gossip message is received.

Sorry for all the different patches, but the implementation details of all the version exchange machinery turned out to be quite subtle.;;;","25/Jun/13 13:49;jasobrown;[~tourist], can you rebase against 1.2? I can't apply the patch to either 1.2 nor trunk. thanks.;;;","25/Jun/13 17:23;sbtourist;Rebased patch on top of cassandra-1.2 branch.;;;","27/Jun/13 19:47;jasobrown;On the whole, lgtm, but I'm not sure about the InterruptedException part. From my reading of the CDL javadoc, the IE is thrown ""if the current thread is interrupted while waiting"". This is indicates that the waiting thread, the one calling versionLatch.await in your code, is being interrupted, not the Handshake thread. Everywhere else in OTC where we catch IE, we throw an AssertionError - basically stop execution of the method and bail. I'm not sure we would we do something different in handshakeVersion(), as if we got an interrupt I think we'd want to bail out, as well, and try to to finish connecting to the remote node.

Also, as a minor nit, I moved the versionLatch.countDown() into a finally block, as we want to unblock the waiting thread regardless of success or failure to read the version from the socket.;;;","27/Jun/13 20:06;sbtourist;bq. From my reading of the CDL javadoc, the IE is thrown ""if the current thread is interrupted while waiting"". This is indicates that the waiting thread, the one calling versionLatch.await in your code, is being interrupted, not the Handshake thread.

Correct. That was to account for spurious interrupts, but I'm fine with adhering to conventions and throwing AE.

bq. Also, as a minor nit, I moved the versionLatch.countDown() into a finally block, as we want to unblock the waiting thread regardless of success or failure to read the version from the socket.

It would have go unblocked after the timeout, but actually better to fail/unblock as fast as possible :);;;","27/Jun/13 20:39;jasobrown;Cool, thanks for the confirmation.

Committed to 1.2 and trunk. Thanks!;;;","15/Oct/13 06:26;jjordan;This does not seem to be fixed (or there is another race condition) as of 1.2.10.  Just saw this happen that during an upgrade 10 node cluster, 5 in each DC.  There were 6 nodes, 3 in each DC, seeing 4 nodes, 2 in each DC as the wrong version.  This was causing timeout failures, and describe cluster failures (only from the nodes seen as being on the wrong version).  Restarting the ""wrong version"" nodes didn't fix anything.  We had to restart the 6 nodes to get them to re-detect version, and then things started working.;;;","15/Oct/13 07:30;sbtourist;[~jjordan], do we have thread dumps from the timeout failures (prior the timeout)? If that didn't involve the connect method, we're probably seeing a different race.

Anyways, I'll have a look.;;;","15/Oct/13 14:53;jjordan;[~sbtourist] no thread dumps.  given nodes e1-e5.  Queries to e1, and ""describe cluster"" on e1 showed all nodes.  Queries to e5 would usually timeout, and ""describe cluster;"" on e5 would show e1-e3 as ""UNAVAILABLE"".  netstat -tn | grep <e1 ip> on e5 showed:

{noformat}
e5:7000 <-> e1:<high port>
e5:<high port> <-> e1:7000
e5:<high port 2> <-> e1:7000
{noformat}

Where e5 to a node which responded to the ""describe cluster;"" showed:

{noformat}
e5:7000 <-> e4:<high port>
e5:7000 <-> e4:<high port 2>
e5:<high port> <-> e4:7000
e5:<high port 2> <-> e4:7000
{noformat}

And in the TRACE level logs for IncomingTcpConnection.java when restarting e5 I only see one connection come in from e1, but there are two that come in from e4.  When I enabled TRACE level logs for e1, it was just spitting out something about ""version 5"" over and over really fast.  At that point we restarted e1, and it came up and everything was happy from e1->e5, so then we did a rolling restart on the cluster and went to bed.
;;;","15/Oct/13 16:53;jjordan;Actually, I didn't read this ticket right, this is something different.  The nodes are all on 1.2 already, but being seen as version 5 (1.1).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove SimpleCondition,CASSANDRA-5691,12654372,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ash2k,ash2k,ash2k,23/Jun/13 10:20,16/Apr/19 09:32,14/Jul/23 05:53,26/Jun/13 14:56,,,,,,,0,,,,,"Problematic scenario:
1. two threads get blocked in SimpleCondition.await();
2. some thread calls SimpleCondition.signal();
3. one of blocked threads wakes up and runs;
4. spurious wakeup happens in the second thread and it wakes up too and runs even though nobody signaled it.

Thus this is a broken implementation of Condition interface.

Anyway, looking at how code uses it, SimpleCondition can just be replaced with CountDownLatch.",,ash2k,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/13 16:33;ash2k;trunk-5691-v2.patch;https://issues.apache.org/jira/secure/attachment/12589622/trunk-5691-v2.patch","23/Jun/13 10:21;ash2k;trunk-5691.patch;https://issues.apache.org/jira/secure/attachment/12589307/trunk-5691.patch",,,,,,,,,,,,,,,,,,,2.0,ash2k,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334649,,,Wed Jun 26 14:56:33 UTC 2013,,,,,,,,,,"0|i1lq67:",334975,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"24/Jun/13 13:39;jbellis;Yes, it's pretty obvious that signal==signalAll in SimpleCondition.  (Signaling just a single waiter is fairly useless and not something we care about.)

Unless CDL offers a performance benefit I don't see any point in churning this.;;;","25/Jun/13 03:18;ash2k;I see following reasons to make this change:
1. It removes code that have a bug;
2. It removes code that is not tested;
3. Why invent a wheel? CountDownLatch(1) is a common way to do this kind of signaling and is widely used and known. So it's easier to understand code.

If the patch is not applied then at least singnal() invocations should be replaced with signalAll() and signal() should throw UnsupportedOperationException.

p.s. I don't think CDL offers any better or worse performance.;;;","25/Jun/13 05:48;jbellis;bq. If the patch is not applied then at least singnal() invocations should be replaced with signalAll() and signal() should throw UnsupportedOperationException.

Agreed.;;;","25/Jun/13 16:33;ash2k;Attaching v2 patch.

Can you please explain your point of view? Why do you prefer SimpleCondition that is not really a Condition to CountDownLatch?;;;","26/Jun/13 14:56;jbellis;Because ""I like CountdownLatch better"" isn't a very compelling reason to change a core construct?

Committed v2, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix AsyncOneResponse,CASSANDRA-5690,12654369,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ash2k,ash2k,ash2k,23/Jun/13 07:54,16/Apr/19 09:32,14/Jul/23 05:53,22/Jul/13 17:12,2.0.0,,,,,,0,,,,,"Current implementation of AsyncOneResponse suffers from two problems:
1. Spurious wakeup will lead to TimeoutException being thrown because awaiting for condition is not done in loop;
2. condition.signal() is used where .signalAll() should be used - this leads to only one thread blocked on .get() to be unblocked. Other threads will stay blocked forever.",,ash2k,christianmovi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5623,,,"23/Jun/13 07:55;ash2k;trunk-5690.patch;https://issues.apache.org/jira/secure/attachment/12589300/trunk-5690.patch",,,,,,,,,,,,,,,,,,,,1.0,ash2k,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334646,,,Mon Jul 22 17:12:22 UTC 2013,,,,,,,,,,"0|i1lq5j:",334972,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"18/Jul/13 18:48;jbellis;This doesn't work; if one thread is blocked in get(), the synchronization prevents responses from being accepted.;;;","19/Jul/13 02:30;ash2k;No, that is not true.

get() obtains the lock, then do TimeUnit.NANOSECONDS.timedWait() [1] that uses Object.wait() [2] internally.
From it's javadocs:
{quote}
 The current thread must own this object's monitor. The thread releases ownership of this monitor and waits until either of the following two conditions has occurred:

    Another thread notifies threads waiting on this object's monitor to wake up either through a call to the notify method or the notifyAll method.
    The timeout period, specified by timeout milliseconds plus nanos nanoseconds arguments, has elapsed. 

The thread then waits until it can re-obtain ownership of the monitor and resumes execution. 
{quote}

So, any number of threads blocked in get() are not preventing response() from setting the result.

p.s. Current implementation uses the same pattern but just doing it wrong.

[1]: https://docs.oracle.com/javase/6/docs/api/java/util/concurrent/TimeUnit.html#timedWait%28java.lang.Object,%20long%29
[2]: https://docs.oracle.com/javase/6/docs/api/java/lang/Object.html#wait%28long,%20int%29;;;","22/Jul/13 14:22;jbellis;I see.

Why not just use {{this}} instead of a separate Object?  Are you sure that {{done}} doesn't need to be volatile?;;;","22/Jul/13 15:47;ash2k;Pros for using ""this"":
- smaller object's memory footprint;

Cons:
- exposes synchronization i.e. breaks incapsulation of synchronization mechanics. This can possibly result in unexpected problems/interference if some other code tries to use this instance as it's own synchronization object - synchronized (AsyncOneResponse instance). See [1] for a bit more details.

IMO in this case it's better to hide synchronization details for safety reasons. But I can change patch if you think otherwise.

""done"" doesn't need to be volatile because all access to this field is synchronized using lock. So this lock guarantees both mutual exclusion of concurrent response() method calls (prevents race on setting ""result"" and ""done"") and visibility by establishing happens-before between lock release (finish of response() method) and subsequent lock acquire (start of get() method and exit from corresponding Object.wait()). From [2]:

{quote}
Synchronization is built around an internal entity known as the intrinsic lock or monitor lock. (The API specification often refers to this entity simply as a ""monitor."") Intrinsic locks play a role in both aspects of synchronization: enforcing exclusive access to an object's state and establishing happens-before relationships that are essential to visibility.

Every object has an intrinsic lock associated with it. By convention, a thread that needs exclusive and consistent access to an object's fields has to acquire the object's intrinsic lock before accessing them, and then release the intrinsic lock when it's done with them. A thread is said to own the intrinsic lock between the time it has acquired the lock and released the lock. As long as a thread owns an intrinsic lock, no other thread can acquire the same lock. The other thread will block when it attempts to acquire the lock.

When a thread releases an intrinsic lock, a happens-before relationship is established between that action and any subsequent acquistion of the same lock.
{quote}

For a similar code and more details see [3]. You may also want to read the famous great book Java Concurrency in Practice [4].

[1]: http://stackoverflow.com/questions/442564/avoid-synchronizedthis-in-java
[2]: http://docs.oracle.com/javase/tutorial/essential/concurrency/locksync.html
[3]: http://docs.oracle.com/javase/tutorial/essential/concurrency/guardmeth.html
[4]: http://www.amazon.com/exec/obidos/ASIN/0321349601;;;","22/Jul/13 17:12;jbellis;Sounds good.  Committed a synchronized (this) version; we don't need to be super defensive about things since we ourselves are the only use case we care about.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update debian packaging for 2.0,CASSANDRA-5688,12654306,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,blair,blair,blair,22/Jun/13 05:22,16/Apr/19 09:32,14/Jul/23 05:53,24/Jun/13 16:48,2.0 beta 1,,,Packaging,,,0,build,,,,"Building trunk on an Ubuntu Precise VM fails with the following output:

{code}
$ git describe 
cassandra-1.2.5-983-g96a1bb0
$ dpkg-buildpackage
...
...
gen-cql3-grammar:
     [echo] Building Grammar /home/blair/Code/Cassandra/cassandra-0.2.0.0.1.2.5.982/src/java/org/apache/cassandra/cql3/Cql.g  ...

build-project:
     [echo] apache-cassandra: /home/blair/Code/Cassandra/cassandra-0.2.0.0.1.2.5.982/build.xml
    [javac] Compiling 41 source files to /home/blair/Code/Cassandra/cassandra-0.2.0.0.1.2.5.982/build/classes/thrift
    [javac] javac: invalid target release: 1.7
    [javac] Usage: javac <options> <source files>
    [javac] use -help for a list of possible options

BUILD FAILED
{code}

I'm working on changes to the files in debian/ to support this.",Ubuntu precise,blair,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/13 06:10;blair;trunk-5688.txt;https://issues.apache.org/jira/secure/attachment/12589235/trunk-5688.txt",,,,,,,,,,,,,,,,,,,,1.0,blair,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334583,,,Mon Jun 24 16:48:32 UTC 2013,,,,,,,,,,"0|i1lpsn:",334909,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"24/Jun/13 16:48;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"cqlsh shouldn't display ""null"" for empty values",CASSANDRA-5675,12653906,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,slebresne,slebresne,20/Jun/13 10:36,16/Apr/19 09:32,14/Jul/23 05:53,26/Jun/13 22:29,1.2.7,2.0 beta 1,,,,,0,cqlsh,,,,"For historical reason (and compatibility with thrift), all type support an empty value, even type like int for which it doesn't really make sense (see CASSANDRA-5674 too on that subject).

If you input such an empty value for a type like int, cqlsh will display it as null:
{noformat}
cqlsh:ks> CREATE TABLE test (k text PRIMARY KEY, v int);
cqlsh:ks> INSERT INTO test(k, v) VALUES ('someKey', blobAsInt(0x));
cqlsh:ks> SELECT * FROM test;

 k       | v
---------+------
 someKey | null

{noformat} 

But that's not correct, it suggests {{v}} has no value but that's not true, it has a value, it's just an empty one.

Now, one may argue support empty values for a type like int is broken, and I would agree with that. But thrift allows it so we probably need to preserve that behavior for compatibility sake. And I guess the need to use blobAsInt at least make it clear that it's kind of a hack.

That being said, cqlsh should not display null as this is confusing. Instead I'd suggest either displaying nothing (that's how an empty string is displayed after all), or to just go with some random explicit syntax like say ""[empty value]""",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/13 11:58;aleksey;5675.txt;https://issues.apache.org/jira/secure/attachment/12589247/5675.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334183,,,Wed Jun 26 22:29:08 UTC 2013,,,,,,,,,,"0|i1lnbr:",334509,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"26/Jun/13 20:21;brandon.williams;+1;;;","26/Jun/13 22:29;aleksey;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NullPointerException on running instances,CASSANDRA-5673,12653871,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,analyst,analyst,20/Jun/13 06:39,16/Apr/19 09:32,14/Jul/23 05:53,12/Sep/13 07:34,1.2.6,,,,,,0,,,,,"Hallo,
We are having sporadic NullPointerException in some of the cassandra nodes in cluster (See stacktrace). 
We are having two Datacenter, each having 15 nodes with RF = 2, OS is SLES with java-1_6_0-ibm-1.6.0_sr12.0-0.5.1. 
At present only  workaround is to stop the application running on same node and run repair tool on cassandra. We are unable to identify the cause of error.

1)
INFO|ScheduledTasks:1|org.apache.cassandra.service.GCInspector|GC for MarkSweepCompact: 347 ms for 1 collections, 138398568 used; ma
x is 1051721728
2013-06-19T16:25:50:843|ERROR|ReplicateOnWriteStage:115|org.apache.cassandra.service.CassandraDaemon|Exception in thread Thread[ReplicateOnWriteStage:115,5,m
ain]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1582)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:897)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:919)
        at java.lang.Thread.run(Thread.java:738)
Caused by: java.lang.NullPointerException
        at java.util.TreeSet.iterator(TreeSet.java:230)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:163)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:64)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:81)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
        at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:274)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1357)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1126)
        at org.apache.cassandra.db.Table.getRow(Table.java:347)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:64)
        at org.apache.cassandra.db.CounterMutation.makeReplicationMutation(CounterMutation.java:90)
        at org.apache.cassandra.service.StorageProxy$7$1.runMayThrow(StorageProxy.java:796)
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1578)
        ... 3 more
2013-06-19T16:26:01:001|ERROR|ReadStage:4833|org.apache.cassandra.service.CassandraDaemon|Exception in thread Thread[ReadStage:4833,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1582)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:897)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:919)
        at java.lang.Thread.run(Thread.java:738)
Caused by: java.lang.NullPointerException
        at java.util.TreeSet.iterator(TreeSet.java:230)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:163)
        at org.

2)
2013-06-19T08:38:23:436| INFO|Thread-2447|org.apache.cassandra.service.StorageService|Starting repair command #2, repairing 1 ranges for keyspace system_auth
2013-06-19T08:58:25:685|ERROR|ReadStage:9270|org.apache.cassandra.service.CassandraDaemon|Exception in thread Thread[ReadStage:9270,5,main]
java.lang.NullPointerException
        at java.util.TreeSet.iterator(TreeSet.java:230)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:163)
        at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:64)
        at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:81)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
        at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:133)
        at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1357)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1214)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1126)
        at org.apache.cassandra.db.Table.getRow(Table.java:347)
        at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:64)
        at org.apache.cassandra.db.ReadVerbHandler.doVerb(ReadVerbHandler.java:44)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:908)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:931)
        at java.lang.Thread.run(Thread.java:738)

Best Regards
Sanjay",,analyst,christianmovi,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334148,,,Thu Sep 12 07:34:23 UTC 2013,,,,,,,,,,"0|i1ln3z:",334474,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"20/Jun/13 16:27;jbellis;Please test 1.2.5, SSTableNamesIterator has changed substantially due to CASSANDRA-5492.;;;","12/Sep/13 07:34;analyst;After Upgrading the problem seems to be fixed. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
running compact on an index did not compact two index files into one,CASSANDRA-5670,12653833,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,cdaw,cdaw,20/Jun/13 01:14,16/Apr/19 09:32,14/Jul/23 05:53,30/Jul/13 20:54,1.2.9,2.0 rc1,,Legacy/Tools,,,0,nodetool,secondary_index,,,"With a data directory containing secondary index files ending in -1 and -2, I expected that when I ran compact against the index that they would compact down to a set of -3 files.  This column family uses SizeTieredCompactionStrategy.

Using our standard CQL example, the compact command used was: 
$ ./nodetool compact test1 test1-playlists.playlists_artist_idx

Please note: reproducing this test on 1.1.12 (using a single primary key), you will see that running compact on the keyspace also does not compact the index file.  There is no option to compact the index, so I could not compare that.

{noformat}
CREATE KEYSPACE test1 WITH replication = {'class':'SimpleStrategy', 'replication_factor':1};

use test1;

CREATE TABLE playlists (
  id uuid,
  song_order int,
  song_id uuid,
  title text,
  album text,
  artist text,
  PRIMARY KEY  (id, song_order ) );

INSERT INTO playlists (id, song_order, song_id, title, artist, album)
  VALUES (62c36092-82a1-3a00-93d1-46196ee77204, 1,
  a3e64f8f-bd44-4f28-b8d9-6938726e34d4, 'La Grange', 'ZZ Top', 'Tres Hombres');

select * from playlists;

=====================================
./nodetool flush test1

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-1-CompressionInfo.db				
test1-playlists-ic-1-Data.db	
test1-playlists-ic-1-Filter.db					
test1-playlists-ic-1-Index.db					
test1-playlists-ic-1-Statistics.db				
test1-playlists-ic-1-Summary.db					
test1-playlists-ic-1-TOC.txt					

=====================================

CREATE INDEX ON playlists(artist );
select * from playlists;
select * from playlists where artist = 'ZZ Top';

=====================================
$ ./nodetool flush test1

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-1-CompressionInfo.db			
test1-playlists-ic-1-Data.db					
test1-playlists-ic-1-Filter.db					
test1-playlists-ic-1-Index.db					
test1-playlists-ic-1-Statistics.db				
test1-playlists-ic-1-Summary.db					
test1-playlists-ic-1-TOC.txt					
	
test1-playlists.playlists_artist_idx-ic-1-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-1-Data.db
test1-playlists.playlists_artist_idx-ic-1-Filter.db
test1-playlists.playlists_artist_idx-ic-1-Index.db
test1-playlists.playlists_artist_idx-ic-1-Statistics.db
test1-playlists.playlists_artist_idx-ic-1-Summary.db
test1-playlists.playlists_artist_idx-ic-1-TOC.txt

=====================================

delete artist from playlists where id = 62c36092-82a1-3a00-93d1-46196ee77204 and song_order = 1;
select * from playlists;
select * from playlists where artist = 'ZZ Top';

=====================================
$ ./nodetool flush test1

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-1-CompressionInfo.db	
test1-playlists-ic-1-Data.db					
test1-playlists-ic-1-Filter.db					
test1-playlists-ic-1-Index.db					
test1-playlists-ic-1-Statistics.db				
test1-playlists-ic-1-Summary.db					
test1-playlists-ic-1-TOC.txt					
test1-playlists-ic-2-CompressionInfo.db				
test1-playlists-ic-2-Data.db					
test1-playlists-ic-2-Filter.db					
test1-playlists-ic-2-Index.db					
test1-playlists-ic-2-Statistics.db				
test1-playlists-ic-2-Summary.db					
test1-playlists-ic-2-TOC.txt
			
test1-playlists.playlists_artist_idx-ic-1-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-1-Data.db
test1-playlists.playlists_artist_idx-ic-1-Filter.db
test1-playlists.playlists_artist_idx-ic-1-Index.db
test1-playlists.playlists_artist_idx-ic-1-Statistics.db
test1-playlists.playlists_artist_idx-ic-1-Summary.db
test1-playlists.playlists_artist_idx-ic-1-TOC.txt
test1-playlists.playlists_artist_idx-ic-2-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-2-Data.db
test1-playlists.playlists_artist_idx-ic-2-Filter.db
test1-playlists.playlists_artist_idx-ic-2-Index.db
test1-playlists.playlists_artist_idx-ic-2-Statistics.db
test1-playlists.playlists_artist_idx-ic-2-Summary.db
test1-playlists.playlists_artist_idx-ic-2-TOC.txt

=====================================

./nodetool compact test1

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-3-CompressionInfo.db
test1-playlists-ic-3-Data.db
test1-playlists-ic-3-Filter.db
test1-playlists-ic-3-Index.db
test1-playlists-ic-3-Statistics.db
test1-playlists-ic-3-Summary.db
test1-playlists-ic-3-TOC.txt
test1-playlists.playlists_artist_idx-ic-1-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-1-Data.db
test1-playlists.playlists_artist_idx-ic-1-Filter.db
test1-playlists.playlists_artist_idx-ic-1-Index.db
test1-playlists.playlists_artist_idx-ic-1-Statistics.db
test1-playlists.playlists_artist_idx-ic-1-Summary.db
test1-playlists.playlists_artist_idx-ic-1-TOC.txt
test1-playlists.playlists_artist_idx-ic-2-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-2-Data.db
test1-playlists.playlists_artist_idx-ic-2-Filter.db
test1-playlists.playlists_artist_idx-ic-2-Index.db
test1-playlists.playlists_artist_idx-ic-2-Statistics.db
test1-playlists.playlists_artist_idx-ic-2-Summary.db
test1-playlists.playlists_artist_idx-ic-2-TOC.txt

=====================================

$ ./nodetool compact test1 test1-playlists.playlists_artist_idx

$ ls /var/lib/cassandra/data/test1/playlists
test1-playlists-ic-3-CompressionInfo.db
test1-playlists-ic-3-Data.db
test1-playlists-ic-3-Filter.db
test1-playlists-ic-3-Index.db
test1-playlists-ic-3-Statistics.db
test1-playlists-ic-3-Summary.db
test1-playlists-ic-3-TOC.txt
test1-playlists.playlists_artist_idx-ic-1-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-1-Data.db
test1-playlists.playlists_artist_idx-ic-1-Filter.db
test1-playlists.playlists_artist_idx-ic-1-Index.db
test1-playlists.playlists_artist_idx-ic-1-Statistics.db
test1-playlists.playlists_artist_idx-ic-1-Summary.db
test1-playlists.playlists_artist_idx-ic-1-TOC.txt
test1-playlists.playlists_artist_idx-ic-2-CompressionInfo.db
test1-playlists.playlists_artist_idx-ic-2-Data.db
test1-playlists.playlists_artist_idx-ic-2-Filter.db
test1-playlists.playlists_artist_idx-ic-2-Index.db
test1-playlists.playlists_artist_idx-ic-2-Statistics.db
test1-playlists.playlists_artist_idx-ic-2-Summary.db
test1-playlists.playlists_artist_idx-ic-2-TOC.txt


=====================================
cqlsh:test1> describe keyspace test1;

CREATE KEYSPACE test1 WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '1'
};

USE test1;

CREATE TABLE playlists (
  id uuid,
  song_order int,
  album text,
  artist text,
  song_id uuid,
  title text,
  PRIMARY KEY (id, song_order)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

CREATE INDEX playlists_artist_idx ON playlists (artist);

{noformat}
",,cdaw,colinkuo,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jul/13 19:54;jasobrown;5670-v1.diff;https://issues.apache.org/jira/secure/attachment/12595031/5670-v1.diff",,,,,,,,,,,,,,,,,,,,1.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334110,,,Tue Jul 30 20:53:48 UTC 2013,,,,,,,,,,"0|i1lmvj:",334436,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"22/Jun/13 06:25;michalm;I think it's impossible to compact secondary index CF using nodetool - I remember that I tried it once with 1.2.1 when having a problem with indexes and as far as I remember I was getting an exception (or just notting happened, I'm not sure now). I had to use JMX to compact it. Anyway, I'll check it.
;;;","22/Jun/13 06:38;michalm;Yes, as I supposed, found in log:

{noformat} WARN 08:09:53,952 Operation not allowed on secondary Index column family (test1-
playlists.playlists_artist_idx){noformat}

So ""it's not a bug, it's a feature"" ;-)

JMX's forceMajorCompaction call works fine.;;;","22/Jun/13 08:18;michalm;BTW. method that decides what to compact accepts two boolean flags that tell (a) if index CFs should be allowed and (b) if index CFs should be automatically included in compaction, so adding a ""--with-indexes"" to ""nodetool compact"" command and/or adding a separate ""nodetool compactindex"" command will not be a problem.;;;","23/Jun/13 05:59;jbellis;bq. it's not a bug, it's a feature

I thought we are allowing it in CASSANDRA-4464.

What is nodetool doing if not calling forceMajorCompaction?;;;","23/Jun/13 07:24;michalm;bq. I thought we are allowing it in CASSANDRA-4464.

My bad, didn't know about the task you mentioned. 
I thought that it maybe was overwritten by other patch, but it wasn't - it seems it was intended (or it's just a mistake):

{noformat}$ git show cef8eb0
(...)
     public void forceTableCompaction(String tableName, String... columnFamilies) throws IOException, ExecutionException, InterruptedExcepti
     {
-        for (ColumnFamilyStore cfStore : getValidColumnFamilies(tableName, columnFamilies))
+        for (ColumnFamilyStore cfStore : getValidColumnFamilies(false, false, tableName, columnFamilies))
         {
             cfStore.forceMajorCompaction();
         }{noformat}

BTW. I can see your comment in that task ( https://issues.apache.org/jira/browse/CASSANDRA-4464?focusedCommentId=13461974&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13461974 ) didn't mention ""compact"" in any of the categories, so I'd guess it was just ""forgotten"".

Anyway, I can't see a good reason to disallow compacting indexes via nodetool.
;;;","30/Jul/13 19:41;jasobrown;(Getting back to this one now). I worked from the list of tasks that Jonathan provided in the comment linked to above - and, yeah, looks like compact was left out or forgotten. Will work on adding compacting 2is today.;;;","30/Jul/13 19:54;jasobrown;v1 simply enables compaction on 2Is (changes a false argument to true);;;","30/Jul/13 20:14;jbellis;+1;;;","30/Jul/13 20:53;jasobrown;commmitted to 1.2 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Connection thrashing in multi-region ec2 during upgrade, due to messaging version",CASSANDRA-5669,12653820,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,19/Jun/13 22:51,16/Apr/19 09:32,14/Jul/23 05:53,20/Jun/13 19:24,1.2.6,2.0 beta 1,,,,,0,ec2,ec2multiregionsnitch,gossip,,"While debugging the upgrading scenario described in CASSANDRA-5660, I discovered the ITC.close() will reset the message protocol version of a peer node that disconnects. CASSANDRA-5660 has a full description of the upgrade path, but basically the Ec2MultiRegionSnitch will close connections on the publicIP addr to reconnect on the privateIp, and this causes ITC to drop the message protocol version of previously known nodes. I think we want to hang onto that version so that when the newer node (re-)connects to the lower node version, it passes the correct protocol version rather than the current version (too high for the older node),the connection attempt getting dropped, and going through the dance again.

To clarify, the 'thrashing' is at a rather low volume, from what I observed. Anecdotaly, perhaps one connection per second gets turned over.",,jasobrown,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Jun/13 22:53;jasobrown;5669-v1.diff;https://issues.apache.org/jira/secure/attachment/12588719/5669-v1.diff","20/Jun/13 18:22;jasobrown;5669-v2.diff;https://issues.apache.org/jira/secure/attachment/12588882/5669-v2.diff",,,,,,,,,,,,,,,,,,,2.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334097,,,Tue Jul 09 17:43:01 UTC 2013,,,,,,,,,,"0|i1lmsn:",334423,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"19/Jun/13 22:53;jasobrown;Attached patch simply deletes the call to Gossiper.resetVersion() from ITC.close().;;;","19/Jun/13 23:27;jbellis;The reason that's there is so that if the other node got upgraded while he was disconnected, we'll now negotiate the new version instead of continuing to use the old.;;;","19/Jun/13 23:55;jasobrown;It looks like we'll (re)set the version on any new connection from a given node, so I'm not sure we need to explicitly throw away the version on close(). 

Just to clarify (even if just for my own benefit): The problem I'm trying to solve here is the upgraded node trying to contact the older node, and things getting wonky (data race) when the Ec2MultiRegionSnitch chooses to close the publicIP connection in favor of the localIP. When the older node closes the connection (after we've established the first round of gossip and we've discovered we're in the same DC), the new node triggers ICP.close() and forgets the older's version number (even though it had just negotiated a moments previously). Thus, when new node attempts to connect on localIP, older node sees the newer protocol version, and refuses the connection.;;;","20/Jun/13 16:23;jbellis;bq. It looks like we'll (re)set the version on any new connection from a given node, so I'm not sure we need to explicitly throw away the version on close()

Here's the scenario.  A is 1.2.  B is 1.1.

B is restarted for upgrade.  A reconnects to B before B connects to A -- maybe it had an ""undroppable"" command to retry, or maybe it's just luck of the draw that A gossips or sends a command to B.

If we don't reset the version on close, A will connect to B as 1.1, and then B will think, ""Oh, A is a 1.1 node, I'd better connect to him that way too.""

bq. The problem I'm trying to solve here is the upgraded node trying to contact the older node, and things getting wonky (data race) when the Ec2MultiRegionSnitch chooses to close the publicIP connection in favor of the localIP

So damned if you do, damned if you don't...

What if we add logic to EC2MRS to only reconnect if we're both on the current version?  1.1 -> 1.2 would reconnect then (because 1.2 drops down to 1.1 after initial negotiation) but that's okay since it would reconnect at 1.1 again.  1.2 -> 1.1 would not reconnect, so you'd have extra public traffic until everyone upgrades.  Acceptable?;;;","20/Jun/13 17:29;jasobrown;Ahh, I see your point. Our upgrades are never that short, time-wise, when we bounce a node for upgrade, A would usually mark B as dead and drop any messages.

Yes, I think your proposal will be fine, a little extra public traffic is better than thrashing (all) connections. This will work now that we keep the version with the OTC rather than in each individual message (as we did pre-1.2).;;;","20/Jun/13 18:22;jasobrown;v2 adds additional check in Ec2MRS.reConnect() to make sure peer node is at same MS.current_version before closing connection on publicIP (and reconnecting on privateIP).;;;","20/Jun/13 19:03;jbellis;+1, just fix your IDE alignment settings on the && clause :);;;","20/Jun/13 19:13;jasobrown;changed name of ticket to better reflect the problem (and the solution);;;","20/Jun/13 19:24;jasobrown;committed to 1.2 and trunk, with indentation alignment change. thanks!;;;","07/Jul/13 17:12;vijay2win@yahoo.com;This patch actually breaks the assumption while using EC2MRS, within the DC we always use private IP and public IP communication is only needed for seeds within a AZ (please see CASSANDRA-5432). This assumption was partly because of the older version and Priam.... 

Should we add this info to changes.txt or communicate to users?;;;","08/Jul/13 22:38;jasobrown;How does this patch break the 'use localIP addr when in the same DC (ec2 region)'? Yes, it temporarily bypasses it during upgrades (due to insanity described in CASSANDRA-5660), but otherwise I believe it behaves as before. Is there a bug or subtlety that I'm not seeing?;;;","08/Jul/13 22:45;vijay2win@yahoo.com;Yes because now reconnect to local ip is not happening during upgrades (you will try to connect in a Non SSL port within a DC).... 
Lets say Public IP address is not open between node A and B (which are in the local DC and they are not seeds) then node A cannot talk to B if you dont reconnect using private IP...

which is the case in https://issues.apache.org/jira/browse/CASSANDRA-5432?focusedCommentId=13637454&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13637454;;;","09/Jul/13 12:27;jasobrown;I spent a lot of time thinking about this :), and I think the situation in this ticket is subtly different from what happened in CASSANDRA-5171/CASSANDRA-5432. I commented on that ticket as to why I think it had a problem (short answer: connecting to publicIP on non-SSL port). This ticket does not get us into that situation as we will continue to connect to the publicIP/(SSL) port - we simply bypass reconnecting on the local port if we see the other node has a lower messaging version.

I did test out this upgrade scenario a few weeks ago when we concocted it (and it worked), and will be happy to try it out again. It'll take a few hours (including time for dropping kids of at camp), so I'll update this ticket later in the morning.;;;","09/Jul/13 17:43;vijay2win@yahoo.com;Makes sense, it is hard to conform the theory without testing it :), Sorry to pollute the ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE in net.OutputTcpConnection when tracing is enabled,CASSANDRA-5668,12653785,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,enigmacurry,enigmacurry,19/Jun/13 19:07,16/Apr/19 09:32,14/Jul/23 05:53,21/Jun/13 19:19,1.2.6,,,,,,0,pull-request-available,qa-resolved,,,"I get multiple NullPointerException when trying to trace INSERT statements.

To reproduce:
{code}
$ ccm create -v git:trunk
$ ccm populate -n 3
$ ccm start
$ ccm node1 cqlsh < 5668_npe_ddl.cql
$ ccm node1 cqlsh < 5668_npe_insert.cql
{code}

And see many exceptions like this in the logs of node1:
{code}
ERROR [WRITE-/127.0.0.3] 2013-06-19 14:54:35,885 OutboundTcpConnection.java (line 197) error writing to /127.0.0.3
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:182)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:144)
{code}


This is similar to CASSANDRA-5658 and is the reason that npe_ddl and npe_insert are separate files.",,enigmacurry,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"Github user dineshjoshi commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/253#discussion_r216105848
  
    --- Diff: src/java/org/apache/cassandra/net/MessageOut.java ---
    @@ -180,6 +199,73 @@ public String toString()
             return sbuf.toString();
         }
     
    +    /**
    +     * The main entry point for sending an internode message to a peer node in the cluster.
    +     */
    +    public void serialize(DataOutputPlus out, int messagingVersion, OutboundConnectionIdentifier destinationId, int id, long timestampNanos) throws IOException
    +    {
    +        captureTracingInfo(destinationId);
    +
    +        out.writeInt(MessagingService.PROTOCOL_MAGIC);
    +        out.writeInt(id);
    +
    +        // int cast cuts off the high-order half of the timestamp, which we can assume remains
    +        // the same between now and when the recipient reconstructs it.
    +        out.writeInt((int) NanoTimeToCurrentTimeMillis.convert(timestampNanos));
    +        serialize(out, messagingVersion);
    +    }
    +
    +    /**
    +     * Record any tracing data, if enabled on this message.
    +     */
    +    @VisibleForTesting
    +    void captureTracingInfo(OutboundConnectionIdentifier destinationId)
    +    {
    +        try
    +        {
    +            UUID sessionId =  (UUID)getParameter(ParameterType.TRACE_SESSION);
    +            if (sessionId != null)
    +            {
    +                TraceState state = Tracing.instance.get(sessionId);
    +                String logMessage = String.format(""Sending %s message to %s"", verb, destinationId.connectionAddress());
    +                // session may have already finished; see CASSANDRA-5668
    +                if (state == null)
    +                {
    +                    Tracing.TraceType traceType = (Tracing.TraceType)getParameter(ParameterType.TRACE_TYPE);
    +                    traceType = traceType == null ? Tracing.TraceType.QUERY : traceType;
    +                    Tracing.instance.trace(ByteBuffer.wrap(UUIDGen.decompose(sessionId)), logMessage, traceType.getTTL());
    +                }
    +                else
    +                {
    +                    state.trace(logMessage);
    +                    if (verb == MessagingService.Verb.REQUEST_RESPONSE)
    +                        Tracing.instance.doneWithNonLocalSession(state);
    +                }
    +            }
    +        }
    +        catch (Exception e)
    +        {
    +            logger.warn(""failed to capture the tracing info for an outbound message to {}, ignoring"", destinationId, e);
    +        }
    +    }
    +
    +    private Object getParameter(ParameterType type)
    +    {
    +        for (int ii = 0; ii < parameters.size(); ii += PARAMETER_TUPLE_SIZE)
    +        {
    +            if (((ParameterType)parameters.get(ii + PARAMETER_TUPLE_TYPE_OFFSET)).equals(type))
    --- End diff --
    
    Don't need the typecast to `ParameterType`
;07/Sep/18 23:09;githubbot;600","Github user jasobrown commented on a diff in the pull request:

    https://github.com/apache/cassandra/pull/253#discussion_r216157097
  
    --- Diff: src/java/org/apache/cassandra/net/MessageOut.java ---
    @@ -180,6 +199,73 @@ public String toString()
             return sbuf.toString();
         }
     
    +    /**
    +     * The main entry point for sending an internode message to a peer node in the cluster.
    +     */
    +    public void serialize(DataOutputPlus out, int messagingVersion, OutboundConnectionIdentifier destinationId, int id, long timestampNanos) throws IOException
    +    {
    +        captureTracingInfo(destinationId);
    +
    +        out.writeInt(MessagingService.PROTOCOL_MAGIC);
    +        out.writeInt(id);
    +
    +        // int cast cuts off the high-order half of the timestamp, which we can assume remains
    +        // the same between now and when the recipient reconstructs it.
    +        out.writeInt((int) NanoTimeToCurrentTimeMillis.convert(timestampNanos));
    +        serialize(out, messagingVersion);
    +    }
    +
    +    /**
    +     * Record any tracing data, if enabled on this message.
    +     */
    +    @VisibleForTesting
    +    void captureTracingInfo(OutboundConnectionIdentifier destinationId)
    +    {
    +        try
    +        {
    +            UUID sessionId =  (UUID)getParameter(ParameterType.TRACE_SESSION);
    +            if (sessionId != null)
    +            {
    +                TraceState state = Tracing.instance.get(sessionId);
    +                String logMessage = String.format(""Sending %s message to %s"", verb, destinationId.connectionAddress());
    +                // session may have already finished; see CASSANDRA-5668
    +                if (state == null)
    +                {
    +                    Tracing.TraceType traceType = (Tracing.TraceType)getParameter(ParameterType.TRACE_TYPE);
    +                    traceType = traceType == null ? Tracing.TraceType.QUERY : traceType;
    +                    Tracing.instance.trace(ByteBuffer.wrap(UUIDGen.decompose(sessionId)), logMessage, traceType.getTTL());
    +                }
    +                else
    +                {
    +                    state.trace(logMessage);
    +                    if (verb == MessagingService.Verb.REQUEST_RESPONSE)
    +                        Tracing.instance.doneWithNonLocalSession(state);
    +                }
    +            }
    +        }
    +        catch (Exception e)
    +        {
    +            logger.warn(""failed to capture the tracing info for an outbound message to {}, ignoring"", destinationId, e);
    +        }
    +    }
    +
    +    private Object getParameter(ParameterType type)
    +    {
    +        for (int ii = 0; ii < parameters.size(); ii += PARAMETER_TUPLE_SIZE)
    +        {
    +            if (((ParameterType)parameters.get(ii + PARAMETER_TUPLE_TYPE_OFFSET)).equals(type))
    --- End diff --
    
    done
;09/Sep/18 13:13;githubbot;600",,0,1200,,,0,1200,,,,,,,,,,,,,,,CASSANDRA-5658,,,,,,,,,,,,,,,"19/Jun/13 22:15;jbellis;5668-assert-2.txt;https://issues.apache.org/jira/secure/attachment/12588704/5668-assert-2.txt","19/Jun/13 20:46;jbellis;5668-assert.txt;https://issues.apache.org/jira/secure/attachment/12588684/5668-assert.txt","21/Jun/13 15:25;jbellis;5668-followup.txt;https://issues.apache.org/jira/secure/attachment/12589085/5668-followup.txt","19/Jun/13 19:11;enigmacurry;5668-logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12588666/5668-logs.tar.gz","20/Jun/13 15:25;jbellis;5668.txt;https://issues.apache.org/jira/secure/attachment/12588856/5668.txt","19/Jun/13 19:10;enigmacurry;5668_npe_ddl.cql;https://issues.apache.org/jira/secure/attachment/12588665/5668_npe_ddl.cql","19/Jun/13 19:10;enigmacurry;5668_npe_insert.cql;https://issues.apache.org/jira/secure/attachment/12588664/5668_npe_insert.cql","19/Jun/13 22:23;enigmacurry;system.log;https://issues.apache.org/jira/secure/attachment/12588710/system.log",,,,,,,,,,,,,8.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334062,,,Fri Jun 21 19:19:35 UTC 2013,,,,,,,,,,"0|i1lmkv:",334388,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,enigmacurry,,,"19/Jun/13 19:27;enigmacurry;I've tried to introduce sleeps in between INSERTs, even with 30s pauses between them I still see the same in the logs. I have also seen this fail on the very first INSERT.;;;","19/Jun/13 19:32;enigmacurry;This is what one the traces looks like that failed:

{code}
127.0.0.1	78	Parsing statement	Thrift:1
127.0.0.1	606	Peparing statement	Thrift:1
127.0.0.1	949	Determining replicas for mutation	Thrift:1
127.0.0.1	3577	Acquiring switchLock read lock	MutationStage:1
127.0.0.1	3622	Appending to commitlog	MutationStage:1
127.0.0.1	3754	Adding to test memtable	MutationStage:1
127.0.0.1	4936	Sending message to /127.0.0.2	WRITE-/127.0.0.2
127.0.0.2	37	Message received from /127.0.0.1	Thread-4
127.0.0.2	562	Acquiring switchLock read lock	MutationStage:1
127.0.0.2	583	Appending to commitlog	MutationStage:1
127.0.0.1	32	Message received from /127.0.0.2	Thread-7
127.0.0.2	634	Adding to test memtable	MutationStage:1
127.0.0.2	1033	Enqueuing response to /127.0.0.1	MutationStage:1
127.0.0.2	1217	Sending message to /127.0.0.1	WRITE-/127.0.0.1
127.0.0.1	132	Processing response from /127.0.0.2	RequestResponseStage:3
{code}

With a replication factor of 3 it should be writing to all three nodes (127.0.0.1, 127.0.0.2, and 127.0.0.3) - 127.0.0.3 is conspicuously missing from that trace.;;;","19/Jun/13 20:46;jbellis;attached additional assert to see where we are trying to trace without setting up the session first;;;","19/Jun/13 22:08;enigmacurry;I ran with the patch, but I did not see any assertion errors.;;;","19/Jun/13 22:15;jbellis;Hmm.  Take 2.;;;","19/Jun/13 22:23;enigmacurry;OK, that prodcued the assertion errors. See attached system.log

Also, I saw a lot of this in the cqlsh terminal:
{code}
<stdin>:9:Request did not complete within rpc_timeout.
<stdin>:10:Request did not complete within rpc_timeout.
<stdin>:11:'NoneType' object is not iterable
<stdin>:12:Request did not complete within rpc_timeout.
<stdin>:13:Request did not complete within rpc_timeout.
<stdin>:14:'NoneType' object is not iterable
<stdin>:15:'NoneType' object is not iterable
<stdin>:16:Request did not complete within rpc_timeout.
<stdin>:17:'NoneType' object is not iterable
<stdin>:18:Request did not complete within rpc_timeout.
<stdin>:19:Request did not complete within rpc_timeout.
<stdin>:20:'NoneType' object is not iterable
<stdin>:21:'NoneType' object is not iterable
<stdin>:22:'NoneType' object is not iterable
{code} ;;;","19/Jun/13 22:32;jbellis;So, .2 is sending two messages for this session, and the first is deleting the session when it's done.  (We know that the session is getting created correctly since the first patch, that checks at message send time, works fine.)

The part I don't understand is, why two messages from .2?  There should only be one (responding to .1).

(This could be problematic for cross-dc replication, although so far tracing seems to be working for CASSANDRA-5632.  But for same-dc, one message per replica should be straightforward.);;;","19/Jun/13 22:44;jbellis;Are we actually creating multiple sessions?

Edit: Yes.;;;","20/Jun/13 04:49;jbellis;Okay, here's what's happening.  I added logging of session cleanup:

{noformat}
 INFO [Thrift:1] 2013-06-19 23:36:51,719 Tracing.java (line 176) session 0702a620-d963-11e2-832d-53376523a4a2 is complete

java.lang.AssertionError: Asked to trace TYPE:MUTATION VERB:MUTATION for session 0702a620-d963-11e2-832d-53376523a4a2 but that state does not exist
{noformat}

cqlsh is requesting QUORUM CL (or ONE?) so once that's achieved the coordinator sends success to the client and closes the tracing session.

if other messages have not yet gone out, then we error.

But it gets worse...

Once the coordinator's state is discarded, any late-arriving replies will create a new, ""non-local"" session.  Since the coordinator will not send any messages again for this session -- which is the trigger we use on replicas to indicate ""we're done"" -- the nonlocal session will persist indefinitely, ""leaking"" memory.

I think we can solve both of these:
# Make a static TraceState method that only needs the sessionid to be passed in to log an event.  OTC can use this to avoid having to look up tracestate at all; if it's cleared out, not a problem.
# Make Tracing.sessions an expiring map so sessions we don't clean up manually still get removed

Alternatively we could just go with #2 by itself and not try to cleanup manually at all.  Average case memory used will be worse, but maybe that is okay since we assume only a tiny fraction of requests are traced at all.

What do you think [~slebresne]?;;;","20/Jun/13 08:38;slebresne;For what it's worth, I think that for the 2nd problem, another option might be to make Tracing.initializeMessage behave slightly differently depending on the message type. So if the state doesn't exist but the message type is a REQUEST_RESPONSE, we could create the state and set it in the threadLocal, but not save it in the global state map.

It's a bit of a hack though, but it slightly bother me to leave this to expiration either so .... ;;;","20/Jun/13 15:25;jbellis;Good idea to check for REQUEST_RESPONSE, although it's not quite as easy as it sounds since we still need to be able to inject the TraceState into the executor stage.  Patch attached.

(Note that once the session is closed we won't know elapsed time anymore.  I don't see a good way around this.);;;","21/Jun/13 08:46;slebresne;I don't think this patch can be committed as is: in both TracingExecutorService and ExpiredTracingState, the package declaration is before the license header, and I can't let that slide.

But with that fixed, +1.;;;","21/Jun/13 14:28;jbellis;Fixed and committed. :);;;","21/Jun/13 15:25;jbellis;followup patch to make session cleanup check for REQUEST_RESPONSE instead of local-ness, to keep from destroying session prematurely in RMVH forwarding case.;;;","21/Jun/13 15:31;slebresne;+1 on that follow up.;;;","21/Jun/13 15:42;jbellis;committed.;;;","21/Jun/13 15:44;enigmacurry;The NullPointerException is gone, however, I see an unexpected change in the trace after this commit.

Prior to the patch, I do this:
{code}
$ ccm create 5668
Current cluster is now: 5668
$ ccm populate -n 2:2
$ ccm start
$ ccm node1 cqlsh
Connected to 5668 at 127.0.0.1:9160.
[cqlsh 4.0.0 | Cassandra 2.0-SNAPSHOT | CQL spec 3.1.0 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> CREATE KEYSPACE test WITH replication = {'class': 'NetworkTopologyStrategy', 'dc1':2, 'dc2':2};
cqlsh> CREATE TABLE test.test (id int PRIMARY KEY, value text);
cqlsh> tracing on;
Now tracing requests.
cqlsh> INSERT INTO test.test (id, value) VALUES ( 5, 'asdf');

Tracing session: 5d3bdaa0-da87-11e2-9eaa-35db2404c433

 activity                                                       | timestamp    | source    | source_elapsed
----------------------------------------------------------------+--------------+-----------+----------------
                                             execute_cql3_query | 11:29:29,423 | 127.0.0.1 |              0
 Parsing INSERT INTO test.test (id, value) VALUES ( 5, 'asdf'); | 11:29:29,424 | 127.0.0.1 |           1082
                                             Peparing statement | 11:29:29,425 | 127.0.0.1 |           2241
                              Determining replicas for mutation | 11:29:29,425 | 127.0.0.1 |           2617
                                  Sending message to /127.0.0.2 | 11:29:29,432 | 127.0.0.1 |           9321
                                 Acquiring switchLock read lock | 11:29:29,433 | 127.0.0.1 |          10376
                                         Appending to commitlog | 11:29:29,433 | 127.0.0.1 |          10424
                                        Adding to test memtable | 11:29:29,433 | 127.0.0.1 |          10544
                                  Sending message to /127.0.0.3 | 11:29:29,435 | 127.0.0.1 |          12658
                               Message received from /127.0.0.1 | 11:29:29,436 | 127.0.0.2 |            102
                               Message received from /127.0.0.1 | 11:29:29,438 | 127.0.0.3 |            103
                        Enqueuing forwarded write to /127.0.0.4 | 11:29:29,441 | 127.0.0.3 |           3596
                                 Acquiring switchLock read lock | 11:29:29,441 | 127.0.0.3 |           3686
                                         Appending to commitlog | 11:29:29,441 | 127.0.0.3 |           3704
                                 Acquiring switchLock read lock | 11:29:29,442 | 127.0.0.2 |           5768
                                        Adding to test memtable | 11:29:29,442 | 127.0.0.3 |           3759
                                         Appending to commitlog | 11:29:29,442 | 127.0.0.2 |           5821
                                  Sending message to /127.0.0.4 | 11:29:29,442 | 127.0.0.3 |           3834
                                        Adding to test memtable | 11:29:29,442 | 127.0.0.2 |           5896
                               Enqueuing response to /127.0.0.1 | 11:29:29,442 | 127.0.0.3 |           4307
                               Enqueuing response to /127.0.0.1 | 11:29:29,443 | 127.0.0.2 |           6602
                               Message received from /127.0.0.3 | 11:29:29,444 | 127.0.0.4 |             92
                                  Sending message to /127.0.0.1 | 11:29:29,444 | 127.0.0.2 |           8037
                               Message received from /127.0.0.2 | 11:29:29,445 | 127.0.0.1 |             36
                            Processing response from /127.0.0.2 | 11:29:29,445 | 127.0.0.1 |            147
                                 Acquiring switchLock read lock | 11:29:29,452 | 127.0.0.4 |           8791
                                         Appending to commitlog | 11:29:29,452 | 127.0.0.4 |           8835
                                        Adding to test memtable | 11:29:29,452 | 127.0.0.4 |           8897
                               Enqueuing response to /127.0.0.1 | 11:29:29,453 | 127.0.0.4 |           9542
                                  Sending message to /127.0.0.1 | 11:29:29,454 | 127.0.0.4 |          10021
                               Message received from /127.0.0.4 | 11:29:29,454 | 127.0.0.1 |           9660
                            Processing response from /127.0.0.4 | 11:29:29,455 | 127.0.0.1 |           9804
                                               Request complete | 11:29:29,435 | 127.0.0.1 |          12668
{code}

That's a 2x2 multi-dc cluster with a RF of 2 in each dc. I write a single INSERT and I see four nodes append to their commit log. All is well.

After the patch, I see this trace:

{code}
Tracing session: c3da4d00-da87-11e2-9a95-35db2404c433

 activity                                                       | timestamp    | source    | source_elapsed
----------------------------------------------------------------+--------------+-----------+----------------
                                             execute_cql3_query | 11:32:21,587 | 127.0.0.1 |              0
 Parsing INSERT INTO test.test (id, value) VALUES ( 5, 'asdf'); | 11:32:21,587 | 127.0.0.1 |            495
                                             Peparing statement | 11:32:21,588 | 127.0.0.1 |            962
                              Determining replicas for mutation | 11:32:21,588 | 127.0.0.1 |           1108
                                  Sending message to /127.0.0.2 | 11:32:21,590 | 127.0.0.1 |           3341
                                  Sending message to /127.0.0.3 | 11:32:21,590 | 127.0.0.1 |           3920
                               Message received from /127.0.0.1 | 11:32:21,592 | 127.0.0.2 |             91
                               Message received from /127.0.0.1 | 11:32:21,593 | 127.0.0.3 |             93
                                 Acquiring switchLock read lock | 11:32:21,593 | 127.0.0.2 |           1398
                                         Appending to commitlog | 11:32:21,593 | 127.0.0.2 |           1447
                                        Adding to test memtable | 11:32:21,593 | 127.0.0.2 |           1517
                        Enqueuing forwarded write to /127.0.0.4 | 11:32:21,594 | 127.0.0.3 |           1266
                               Enqueuing response to /127.0.0.1 | 11:32:21,594 | 127.0.0.2 |           2573
                                 Acquiring switchLock read lock | 11:32:21,594 | 127.0.0.3 |           1343
                                         Appending to commitlog | 11:32:21,594 | 127.0.0.3 |           1360
                                        Adding to test memtable | 11:32:21,594 | 127.0.0.3 |           1412
                               Enqueuing response to /127.0.0.1 | 11:32:21,595 | 127.0.0.3 |           2060
                               Message received from /127.0.0.2 | 11:32:21,595 | 127.0.0.1 |           null
                                  Sending message to /127.0.0.1 | 11:32:21,595 | 127.0.0.2 |           2850
                                  Sending message to /127.0.0.1 | 11:32:21,595 | 127.0.0.3 |           2442
                            Processing response from /127.0.0.2 | 11:32:21,595 | 127.0.0.1 |           null
                                  Sending message to /127.0.0.4 | 11:32:21,595 | 127.0.0.3 |           2442
                               Message received from /127.0.0.3 | 11:32:21,595 | 127.0.0.1 |           null
                            Processing response from /127.0.0.3 | 11:32:21,595 | 127.0.0.1 |           null
                               Message received from /127.0.0.3 | 11:32:21,597 | 127.0.0.4 |             95
                                 Acquiring switchLock read lock | 11:32:21,598 | 127.0.0.4 |           1040
                                         Appending to commitlog | 11:32:21,598 | 127.0.0.4 |           1087
                                        Adding to test memtable | 11:32:21,598 | 127.0.0.4 |           1391
                               Enqueuing response to /127.0.0.1 | 11:32:21,599 | 127.0.0.4 |           1885
                               Message received from /127.0.0.4 | 11:32:21,600 | 127.0.0.1 |           null
                                  Sending message to /127.0.0.1 | 11:32:21,600 | 127.0.0.4 |           3290
                            Processing response from /127.0.0.4 | 11:32:21,600 | 127.0.0.1 |           null
                                               Request complete | 11:32:21,592 | 127.0.0.1 |           5564
{code}

127.0.0.1 (which also happens to be the coordinator) didn't write anything to it's commit log (according to the trace at least). Reproducible on cassandra-1.2 and trunk.

;;;","21/Jun/13 15:46;jbellis;Did you get the followup patch in that?  Should have addressed it.;;;","21/Jun/13 15:51;enigmacurry;I've retested with the second patch, it's the same.;;;","21/Jun/13 16:13;jbellis;fix pushed in 110d283afd780774a44368b17177b5e8e781e37f;;;","21/Jun/13 16:21;enigmacurry;That works in cassandra-1.2 - if you commit to trunk, I'll test there too.;;;","21/Jun/13 18:09;jbellis;Merged to trunk.;;;","21/Jun/13 19:19;enigmacurry;+1 - all my tests are passing now. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL3 should not allow ranges on the partition key without the token() method, even for byte ordered partitioner.",CASSANDRA-5666,12653773,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,19/Jun/13 18:05,16/Apr/19 09:32,14/Jul/23 05:53,20/Jun/13 17:12,1.2.6,,,,,,0,,,,,"When the partition is an ordered one, CQL3 currently allows non-equal conditions on the partition key directly. I.e. we allow
{noformat}
CREATE TABLE t (k timeuuid PRIMARY KEY);
SELECT * FROM t WHERE k > ... AND k < ...;
{noformat}
but this is a bug because even ordered partitioner don't order following the type of the partition key. They order by bytes, always.

So that type of query doesn't do in general what it is supposed to do and we should disallow it. Even for ordered partitioner, the token() function should be used. ",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/13 11:08;slebresne;5666.txt;https://issues.apache.org/jira/secure/attachment/12588825/5666.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,334050,,,Thu Jun 20 17:12:28 UTC 2013,,,,,,,,,,"0|i1lmi7:",334376,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"20/Jun/13 11:08;slebresne;Attached trivial patch for this (that include update of the documentation).

Just to illustrate the problem we currently have, consider (where BytesOrderingPartitioner is used):
{noformat}
cqlsh:ks> CREATE TABLE test ( k int PRIMARY KEY);
cqlsh:ks> INSERT INTO test(k) VALUES (1);
cqlsh:ks> INSERT INTO test(k) VALUES (0);
cqlsh:ks> INSERT INTO test(k) VALUES (-1);
cqlsh:ks> SELECT * FROM test;

 k
----
  0
  1
 -1

cqlsh:ks> SELECT * FROM test WHERE k >= -1 AND k < 1;
Bad Request: Start key must sort before (or equal to) finish key in your partitioner!

{noformat};;;","20/Jun/13 14:47;jbellis;+1;;;","20/Jun/13 17:12;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Discard pooled readers for cold data,CASSANDRA-5661,12653702,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,jbellis,jbellis,19/Jun/13 13:31,16/Apr/19 09:32,14/Jul/23 05:53,17/Sep/13 21:58,2.0.1,,,,,,0,,,,,"Reader pooling was introduced in CASSANDRA-4942 but pooled RandomAccessReaders are never cleaned up until the SSTableReader is closed.  So memory use is ""the worst case simultaneous RAR we had open for this file, forever.""

We should introduce a global limit on how much memory to use for RAR, and evict old ones.",,aleksey,ben.manes,cburroughs,christianmovi,jeromatron,jjordan,kohlisankalp,maheeg,pjulien,rcoli,tjake,vijay2win@yahoo.com,weideng,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6191,,,,,,,,,,,,,,,,,,,,"21/Jul/13 04:50;xedin;CASSANDRA-5661-global-multiway-cache.patch;https://issues.apache.org/jira/secure/attachment/12593385/CASSANDRA-5661-global-multiway-cache.patch","05/Jul/13 19:22;xedin;CASSANDRA-5661.patch;https://issues.apache.org/jira/secure/attachment/12591034/CASSANDRA-5661.patch","26/Jun/13 19:50;jjordan;DominatorTree.png;https://issues.apache.org/jira/secure/attachment/12589788/DominatorTree.png","26/Jun/13 19:50;jjordan;Histogram.png;https://issues.apache.org/jira/secure/attachment/12589787/Histogram.png",,,,,,,,,,,,,,,,,4.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,333979,,,Tue Sep 17 22:19:51 UTC 2013,,,,,,,,,,"0|i1lm2f:",334305,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,dmeyer,,,"19/Jun/13 13:32;jbellis;One possibility would be to have a {{CLHM<SSTableReader, Queue<RandomAccessReader>>}} that PooledSegmentedFile looks itself up in.

[~xedin], do you have time to take a stab at this?;;;","25/Jun/13 18:39;jbellis;This has bitten someone in production now...  LCS w/ 4000 sstables, using 2GB of heap (16k CRAR buffers).;;;","25/Jun/13 18:54;xedin;I'm starting to think that it would be just easier for everybody to just start using mmap buffers for compressed buffers all the time instead before we make caching too complex. I will start working in that direction to see if there is any promise in that.;;;","25/Jun/13 18:57;jjordan;Saw some issues caused by these never getting cleared out.  On a cluster using LCS CompressedRandomAccessReader objects are using 2 GB of heap space per node.;;;","25/Jun/13 21:46;jbellis;bq. I'm starting to think that it would be just easier for everybody to just start using mmap buffers for compressed buffers all the time

Good idea, but probably too big a change for 1.2.7?;;;","25/Jun/13 21:55;xedin;As a brain dump, I see 3 ways to resolve this:

1. Introduce queue total memory limit and evict based on that, but there is no guarantee that we won't be evicting incorrect instances.

2. Introduce liveness per instance (e.g. 20-30 seconds) before evictor considers it as ""old"", that solves the problem with #1 but are relying on eviction thread to be robust and run constantly, any delays in such manual GC could result in the same memory bloat as described in the issue.

3. Remove caching and go with mmap'ed segments instead, the problem with that is that we need to create direct byte buffer every time we decompress data which I'm not sure if could be GC'ed reliably, so for example, if particular JVM implementation only cleans up such buffers only on CMS or full GC process can effectively OOM because we are actually trying to avoid any significant GC activity as much as possible.

I like #2 the most. Thoughts?;;;","25/Jun/13 23:05;jbellis;bq. Introduce queue total memory limit and evict based on that, but there is no guarantee that we won't be evicting incorrect instances.

I don't follow -- surely LRU is a better heuristic than ""idle for N seconds"" as in #2?;;;","26/Jun/13 01:08;xedin;LRU is more of a replacement strategy when you have distinct objects and fixed upper limit (num items, memory), but in here we deal with mostly duplicate objects and the states where num of items hardly overgrows queue so the problem is not handling replacement but rather expiring items after some period of idling. I was thinking of doing something like ExpiringQueue with the one (or set of timers) similar to Guava Cache. Which solves the problem of [C]RAR instances being stuck in the cache for long periods of time even when read pattern have changed and they are not useful anymore.;;;","26/Jun/13 03:28;jbellis;It still looks like an LRU problem to me: I have room for N objects, when I try to allocate N+1 I throw away the LRU.

This means we will use more memory than a timer approach if N is actually more than we need, but it will do better than timers if we are crunched for space, which seems like the more important scenario to optimize.

It also means that we have a very clear upper bound on memory use, rather than depending on workload, which history shows is tough for users to tune.;;;","26/Jun/13 03:55;xedin;It seems like we are trying to address different problems of what is in description to the ticket and what Jeremia pointed out. Let me describe what I'm trying to solve: When reading from multiple SSTables for a while and then pattern changes and load is switched to the different subset of SSTables, previous [C]RAR instances are returned to the appropriate queues and stuck there until each SSTable is deallocated (by compaction) which creates memory pressure on stale workloads or when compaction is running behind.

LRU could solve that problem when we have limit on total amount of memory that we can use so it would start kicking in only after we reach that limit and create a jitter in the queue and processing latencies. 

What I propose adds minimal booking overhead per queue and expires items quicker than LRU and more precise, also I'm not really worried about max number of items in the queue per SSTable as it's organically limited to the number of concurrent readers.;;;","26/Jun/13 04:45;jbellis;LCS makes number of sstables x concurrent readers a problem.;;;","26/Jun/13 06:13;xedin;What is LCS?;;;","26/Jun/13 09:46;jjordan;[~xedin] LCS == LeveledCompactionStrategy.  While in theory I like the idea of the searchers in most cases expiring quicker with timers, in practice since these are on the heap, and LCS defaults to only 5 MB files per levels, you can have A LOT of sstables, 200 per GB...  Which as [~jbellis] mentioned makes # of sstables * concurrent readers a big problem, which was the problem hit above.  So we really need to bound memory usage with a hard cap, not the fuzzy cap of ""how many can I open in X seconds"".

1k reqs/sec hitting a 4 level deep LCS CF could mean 4k Reader's (~500 MB) created per second.  Now that I say that out loud, I almost think we should go back to not caching these at all so they always just get recycled in young gen and never have a chance to hit old gen.  I guess it could be a config parameter which defaults on for STCS and off for LCS.  Since STCS work loads have no where near as many sstables.;;;","26/Jun/13 14:20;jbellis;I'm not as pessimistic -- I think pooling will still be useful for most workloads -- but if we add a configurable ceiling on memory use, we could certainly add that zero = no pooling at all.

Would also be useful to track the pool ""hit rate."";;;","26/Jun/13 19:00;xedin;bq. While in theory I like the idea of the searchers in most cases expiring quicker with timers, in practice since these are on the heap, and LCS defaults to only 5 MB files per levels, you can have A LOT of sstables, 200 per GB... Which as Jonathan Ellis mentioned makes # of sstables * concurrent readers a big problem, which was the problem hit above. So we really need to bound memory usage with a hard cap, not the fuzzy cap of ""how many can I open in X seconds""

When I was talking about concurrent readers I actually meant the max number of threads in READ stage (concurrent_reads in yaml, default 32) that could be running in the same time, that is the worst case limit of cache queue per SSTable.
 
I'm not convinced that LRU on SSTable would work better then [C]RAR instance expiration for our use case because due to replacement mechanism it creates jitter in latencies grater than p75 once global threshold is reached, which in production systems create situation when people can't explain why latencies suddenly degraded without increasing amount of data.

I'm not sure that I follow what you mean by last part, we are not trying to limit number of open for period of time but rather expire items that been returned to the queue after time period. I agree that we can add upper limit on memory usage just like we did for key/row caches, which in combination with expiring would yield predictable behavior.

bq. 1k reqs/sec hitting a 4 level deep LCS CF could mean 4k Reader's (~500 MB) created per second. Now that I say that out loud, I almost think we should go back to not caching these at all so they always just get recycled in young gen and never have a chance to hit old gen. I guess it could be a config parameter which defaults on for STCS and off for LCS. Since STCS work loads have no where near as many sstables.

It sounds like memory overhead caused by compression for your use-case is too big with or without caching (it's around 79KB = 64KB default chunk + snappy overhead) per file, even if you drop caching your allocation rate would cause [C]RAR buffers to be promoted and cause fragmentation in the old gen which could cause long ""remark"" phrase of CMS (can be battled with CMSScavengeBeforeRemark option) and as CMS is not compacting you can suffer from FullGC more frequently. That is what we are trying to battle with caching [C]RAR instances (and their buffers) as if we allocate those close in time and reuse, it's more luckily that they are going to be laid out closely plus removes portion of work from young gen collector.;;;","26/Jun/13 19:56;jjordan;Screen shots attached to show the issue.
Histogram.png show my histogram.  Over 2 GB is being retained by CRAR objects on a node with only 80 GB of data, most of that in LCS CF's with 20 MB sstable size, and some ine LCS CF's with 10 MB sstable size.
!Histogram.png!

Here is the dominator tree for that same heap dump.
!DominatorTree.png!

The problem is that most of those sstables have 1 MB or more of CRAR buffers sitting there waiting to be used.  If we do a timer with an absolute cap, even better, but we need some kind of cap.

The compression overhead itself isn't really an issue, the issue is the cached CRAR objects/buffers.;;;","26/Jun/13 22:05;xedin;bq. The problem is that most of those sstables have 1 MB or more of CRAR buffers sitting there waiting to be used. If we do a timer with an absolute cap, even better, but we need some kind of cap.

1MB actually means that it's around 12-14 caches reads which is not even half of default concurrency limit, we can start with expiring for 1.2.7 as it doesn't require adding options (and maybe disable caching with LCS), then for 2.0 we can add cap limit per CF or even global into yaml.

bq. The compression overhead itself isn't really an issue, the issue is the cached CRAR objects/buffers.

It is for such a small files because it's not just buffers but also compression metadata per SSTable although you can't see it in histogram/tree as it was moved off heap, I guess it's an artifact of LCS that we can't do anything about.

This all got me wondering what is BF space overhead LCS vs. STCS, can you please check?;;;","26/Jun/13 22:12;jbellis;bq. it's not just buffers but also compression metadata per SSTable although you can't see it in histogram/tree as it was moved off heap

The on-heap part is bad enough :);;;","26/Jun/13 23:17;xedin;After thinking about this more as LCS produces similar (same) sized files there is actually no benefit of caching [C]RAR instances. I think we should do caching only with STCS + expiring as it introduces less memory overhead per file for [C]RAR.

I also want to share my worries about LCS using 5MB file size with mmap and no compression: default vm.max_map_count is 65536 (maximum number of memory mappings per process) which is around 322GB of data, we need to mention somewhere that if somebody wants to run with mmap + LCS they need to adjust that setting. Another thing that worries me is that performance of mmap, munmap and similar + minor/major page faults would degrade logarithmically as memory mappings are handled by red-black tree and with constant compaction we would be burning a lot more cpu on tree balancing as dataset grows.;;;","27/Jun/13 01:36;jbellis;I don't see what similar-sized files has to do with it.  CRAR buffer size is independent of file size, and that's what causes the fragmentation.  (Zeroing out large buffers isn't free, either.);;;","27/Jun/13 01:54;xedin;I'm not sure what you mean. I was trying to say that caching doesn't deliver good trade-off in terms of memory usage on small files. On the other hand caching makes more sense with STCS especially on higher tiers as number of blocks handled by one buffer would be few orders of magnitude higher.;;;","27/Jun/13 04:03;xedin;In other words, my initial idea didn't take into account leveled compaction, it was simple - by paying overall small memory price (one chunk size or less depending on size of file) per 1GB we can minimize GC work, memory allocation and number of syscalls per read. With leveled compaction that strategy doesn't work as price per 1 GB is pretty big as most of the files are very small which increases allocation rate to cache and GC activity by very frequent compactions.;;;","27/Jun/13 13:43;tjake;We use LCS and haven't seen any heap pressure here, though we set our files to 128M.  Would it make more sense to change this default from 5mb.  No one successfully using LCS has it set to 5mb.;;;","27/Jun/13 14:16;jbellis;Agreed that we should evaluate LCS defaults (and we have Daniel Meyer working on this now), but that just kicks the can down the road; if you're in trouble with 50GB of data and 5MB sstables, you'll be in equal trouble at 1TB of data and 100MB sstables.;;;","27/Jun/13 19:28;xedin;Well it depends on how do you define equal, having 1TB of data would definitely require bigger heap and physical memory configuration. 

Let's calculate (where each file have one buffer in memory at all times):

5MB   files (each 79KB decompression buffer) for 1GB of such files in memory would be: 204 (num files in 1GB) * 79KB = *16MB* buffers
128MB files (-//-) require 25.5 times less buffers per 1GB than 5MB files: 16MB (buffers per 1GB in case of 5MB files) / 25.5 = *643KB* buffers

So for 1TB with 5MB files we need 1024 * 16MB = *16GB* of heap and for 128MB files it's 25.5 times less = *643MB*, if each of the files is going to have at least 8 caches items in the same time with 128MB files we are going to have around 5GB of heap but I do think this scenario is a worst case, normal mode would be 2-3GB. If you go with 14-16GB heap and 1TB of data, 2GB of cache is the least of your problems as it's around 10% of total heap size which is still good trade-off to allocation rate if those buffers are allocated per call.;;;","29/Jun/13 17:32;jbellis;It occurs to me that we may be approaching the problem the wrong way.  If we just pool the buffers rather than the CRAR objects, we would only need (concurrent readers x max sstables for a given partition) which is going to be much lower than the total sstable count.;;;","29/Jun/13 20:18;xedin;I tried that before going with cached instances which is per CF map<int, queue<ByteBuffer>> as one CF could have different chunk sizes, it actually performs worse because of queue contention and we would still have to pay the price of ""open"" call on each read of file.;;;","29/Jun/13 20:31;jbellis;We don't need exact chunk size matches though -- i.e., we can use a larger buffer.  So if we just pool max chunk size buffers we'll probably come out ahead.;;;","29/Jun/13 20:58;xedin;It's waste of memory and doesn't solve contention problem.;;;","29/Jun/13 23:59;jbellis;It's a lot less memory used than the status quo.  I'd take a little contention over OOMing people.;;;","30/Jun/13 00:42;xedin;I think we are trying solve the consequence instead of actual problem of adjusting max sstable size as jake pointed out, I think [~vijay2win@yahoo.com] was also doing so in production. Caching in the current state does the job for STCS and LCS with bigger files, expiring would be a good addition tho.;;;","30/Jun/13 01:32;jbellis;As I explained, increasing sstable size helps but does not solve the problem; we're supposed to be supporting up to 5-10TB of data in 1.2.;;;","30/Jun/13 02:00;xedin;What I am just trying to say is that expiring with global limit as good enough even for LCS with bigger files, but useless with 5MB besides all other problems. And as I pointed in on of the comment for 5-10 terabytes even with 128MB files are too small and affect system performance without taking into account indexing/bf overhead.;;;","30/Jun/13 03:33;jbellis;bq. What I am just trying to say is that expiring with global limit as good enough even for LCS with bigger files

I don't see how that follows at all.  The expiring approach is broken at any dataset size where memory pressure is a problem, since in the worst case it will not evict quickly enough.;;;","30/Jun/13 04:44;xedin;Right, that's what max memory size cap is for, to make eviction more intelligent in times of memory pressure, and concurrency as limited so as dataset grows there wouldn't be a lot if items in each the queue anyway.;;;","05/Jul/13 19:22;xedin;Attached patch adds FileCacheService (+ metrics) which can expire instances after they are not accessed for time period and has a global memory usage limit (set to 512MB).;;;","07/Jul/13 02:25;jbellis;I talked this over a bit with [~ben.manes] (author of CLHM).  Here's his take:

{quote}
[This] sounds less like a multimap cache than a multi-way object pool. To me a multimap cache would be like any other cache where entries are read frequently, rarely explicitly invalidated, and evicted by a boundary condition. An object pool has instances checked in and out, so care needs be taken to make sure the transfer overhead is cheap. I think you want a more advanced pool with global boundary conditions and is multi-way, so more complex than a traditional database connection pool. For that, actually, a few years ago I advised the author of BoneCP to use a LinkedTransferQueue to leverage elimination to avoid contention which provided the performance improvements to make his library the fastest available.
{quote}

Ben put together an implementation at https://github.com/ben-manes/multiway-pool:

{quote}
 * A concurrent object pool that supports pooling multiple resources that are associated with a
* single key. A resource is borrowed from the pool, used exclusively, and released back for reuse
* by another caller. This implementation can optionally be bounded by a maximum size, time-to-live,
* or time-to-idle policies.
* 
* A traditional object pool is homogeneous; all of the resources are identical in the data and
* capabilities offered. For example a database connection pool to a shared database instance. A
* multiway object pool is heterogeneous; resources may differ in the data and capabilities offered.
* For example a flat file database may pool random access readers to the database table files. The
* relationship of a single-way to a multi-way object pool is similar to that of a map to a
* multimap.
* 
* When this pool is bounded any resource is eligible for eviction regardless of the key that it is
* associated with. A size based bound will evict resources by a best-effort LRU policy and a time
* based policy will evict by either a time-to-idle and/or time-to-live policy. The resource's life
* cycle can be instrumented, such as when cleaning up after eviction, by using the appropriate
* ResourceLifecycle method.
{quote};;;","07/Jul/13 03:15;xedin;The only point I disagree with is ""rarely explicitly invalidated"", this is not true especially with LCS and small files, that work already done for us by compaction, things we need to care are - expiry after access + total memory limit (as concurrency is also limited by read stage size so all of the queues are implicitly bounded). My implementation is fairly simple and takes advantage of all services provided by upper levels (compaction for eviction, read stage for concurrently limiting) as well as build-in guava cache compatibilities of expiring items and handling removals.;;;","07/Jul/13 03:35;ben.manes;""rarely explicitly invalidated"" is in regards to a cache, as Jonathan originally described the problem as a multimap cache instead of as an object pool. He also expressed concern with evicting a block of buffers at once when he conceived of the same model that you implemented.

I am intimately familiar with Guava's cache as I designed the algorithms, ported and wrote code for it, and advised on the api. Unfortunately I am not familiar with Cassandra's needs and its code, so the pool was implemented based on a brief description of the problem and ideal behavior.

It was a fun exercise for a long weekend. I'd recommend writing tests and benchmarks, which unfortunately appears to be missing with the patch in its current form. Of couse use whatever makes the most sense.;;;","07/Jul/13 03:46;xedin;It's more of object pool, where each key has limited number of ""equal"" objects, so if caller wants to read any portion of the file it would get an instance which represents whole file, seek to appropriate position and do reading, returning that instance to the pull when done. evicting block of buffers is still required when files are compacted out which would be more frequent than eviction by timer because use-cases usually don't drift in pattern. I also raised question about what I think is a major problem here - max size of a file being 5MB by default for CLS.;;;","07/Jul/13 04:21;jbellis;bq. It's more of object pool, where each key has limited number of ""equal"" objects

Yes.  This is what the javadoc means by ""supports pooling multiple resources that are associated with a single key.""

bq. evicting block of buffers is still required when files are compacted out

Sure, and this is supported, but this is not a case of contention which was the concern I raised with Ben.;;;","07/Jul/13 04:24;jbellis;bq. I also raised question about what I think is a major problem here - max size of a file being 5MB by default for CLS

I mentioned that Daniel Meyer is working on this, but I've formally opened CASSANDRA-5727 in the hopes that that helps restrict our scope of discussion here. :);;;","07/Jul/13 05:00;xedin;bq. Sure, and this is supported, but this is not a case of contention which was the concern I raised with Ben.

What contention are we talking? If that is memory pressure, then expiring items in bulk is good and leaves room for new files. In case of LCS instances mostly are going to be invalidated by compaction vs access time expiry, STCS observes the same effect but better in terms of memory usage as smaller files are going to be compacted into bigger one so each compaction reduces number of instances and buffers. 

I think the original problem in here was that with default settings LCS eats a lot of memory because files are too small and even if minor portion of dataset is read memory, overhead for caching is still unacceptable, this is why I was talking about adding global memory cap and as a good bonus - expiry of unused instances, because if file is expired by timer, it means that reads have turned away from it or it's read in bursts so deallocating all of the cached instances is the way to go, where with LRU we would only replace when new instances are returned to the pull (as we don't pre-allocate) which could create a problem when there is a burst of requests to the same file after long internal of inactivity. But even after so many comments that we had here and people ([~tjake]) reporting that even current setup works for them on bigger files, it still looks like the everybody is trying to solve different issues.;;;","07/Jul/13 05:39;jbellis;Perhaps it will help if I recap:

# CRAR pooling + LCS OOMs us at relatively small amounts of data (20GB in Jeremiah's example)
# worst case pool size proportional to sstables * concurrent readers means that memory usage increases linearly with disk size; increasing sstable size decreases it linearly, so we expect 100MB sstables [which is on the high side of what is reasonable IMO] to get us to ~4TB of space managed which is good but not ""problem solved"" territory. 
# thus, we need to bound CRAR pooling and not just say ""it's working as designed, go use larger sstables or STCS"" [although that is probably adequate for 1.2, so I am tagging this for 2.0]
# The most common operations in an object pool are the borrow/return.  Ben's multiway pool optimizes for this, with particular attention to possible contention from multiple reader threads.
# The multiway pool supports expiring objects after they have been idle for a given time, as well as a total pool size.
;;;","07/Jul/13 05:59;xedin;bq. worst case pool size proportional to sstables * concurrent readers means that memory usage increases linearly with disk size; increasing sstable size decreases it linearly, so we expect 100MB sstables [which is on the high side of what is reasonable IMO] to get us to ~4TB of space managed which is good but not ""problem solved"" territory.

Let's clarify that it's a really-really worse case which means that your reads touch whole dataset all the time and it's a good show case for side affect of having fixed size files of small size so even if you don't do caching it would create allocation rate proportional to the dataset size as well as increased syscall rate to open/seek/close those files so it's frequent GC/FullGC vs OOM in here (when run without memory cap).

bq. The most common operations in an object pool are the borrow/return. Ben's multiway pool optimizes for this, with particular attention to possible contention from multiple reader threads.

If I understand correctly by borrow you mean allocate object in the pool and give it way to the caller, this is not how I would like it to behave instead we can allocate object without assigning to the queue when it runs short (as all of the objects are short lived) and then decide if we want it back when caller it done with it, disadvantages of allocating to the pool I have already described in my previous comments.

Bottom line for me being, I'm tired of arguing about this so I will let mighty people to decide what they see fit as in all other cases (e.g. fadvic'ing whole file on reads, preheating page cache etc.). 

;;;","07/Jul/13 07:33;xedin;[~ben.manes] I briefly looked through the code and I think there is an important component missing - we need to weight size of the cache based on internal structure of each object allocated into it (borrowed) as the biggest part would be in memory buffer that each instance holds, so simple number of entries wouldn't do...

I'm worried that getResourceHandler has to go through few queues and allocation, I wanted to avoid polling + atomic CAS on read path, which is most critical, as much as possible because it adds additional undesired latency especially to high cardinality requests.

Also I don't think using just weak references to remove unused queues is a good idea (if I interpreted comment at the top correctly), we need something more agressive, because ParNew+CMS only processes those in FullGC phrase even when objects effectively die in young gen (sun/open jdk) and G1 doesn't even have any guarantees on when they are going to be processed, not mentioning that it requires double pass.;;;","07/Jul/13 08:38;ben.manes;Weights are trivial to add and I wanted to avoid adding non-critical features without more details. In your patch, it appears assumed that every queue as a single entry with the same size buffer and privately Jonathan's description of the problem stated 128KB per CRAR. If the weight is constant than they are merely a convenience mapping as it really is the number of entries.

Uncontended CAS is cheap, short lived allocations are trivial, and Doug Lea describes LTQ as lower overhead than CLQ (especially under load).

The use of weak references was an easy way to avoid race conditions when flushing out the primary structure. It could be replaced with lazy clean-up passes, which is what I originally started with. At this point it seemed unwise to complicate things without more information so I simplified it. The number of queues is probably going to be quite small, on the order of dozens, so the reference cost in this case is quite small.

You're trying to compare approaches, which is valid but better oriented towards discussing with Jonathan. The challenge presented to me is as described in the class's JavaDoc: an multiway object pool bounded by the total number of entries. I took a more general approach due to not knowing the trade-offs one could make with context to Cassandra's behavior.;;;","07/Jul/13 09:03;xedin;I understand that you didn't account for any specific nuances, I just wanted to point those things out for subscribers of that ticket...

bq. In your patch, it appears assumed that every queue as a single entry with the same size buffer and privately Jonathan's description of the problem stated 128KB per CRAR. If the weight is constant than they are merely a convenience mapping as it really is the number of entries.

I'm not sure if Jonathan wants to use it per CF or globally but different column families are going to have different buffer settings especially with CRAR, my patch accounts each queue buffer as being more or less a constant size but doesn't assume very key introduces the same overhead.

bq. The number of queues is probably going to be quite small, on the order of dozens, so the reference cost in this case is quite small.

It's actually going to be pretty high especially with LCS (default size of 5 MB), it creates 204 files for 1GB, so it's order of thousands without taking into account compaction process.

bq. Uncontended CAS is cheap

Well if 8-32 threads (default number of threads) are going to request the same row from the same number of files, there could be contention especially it we throw LRU in the mix. But I'm more concerned about 1 ms polling timeout in there tho... This is why I'm concerned with tries to make cache first class citizen instead of lucky shot especially when we don't really need that.
;;;","07/Jul/13 10:28;ben.manes;I solved the LRU problem years ago, which gave you CLHM and Guava's Cache. It scales very well without degrading due to LRU management under higher thread count, limited primarily by the hash table usage. Previous approaches didn't scale to 4-8 threads, but 32+ is limited by the chosen hash table design.

In neither approaches will there be significant contention or overhead. The difference is about the level of granularity to bound the resources by and how to evict them.

You seem to be focusing on tuning parameters, minute details, etc. for a class written in a few evenings as a favor, knowing that those things are trivial to change. There's not much of a point debating it with me as I don't care and have no stake or interest in what is decided. Especially when you're comparing it against a simplistic usage relying on another class I wrote much of, Guava's. In the end something I wrote will be used to solve this bug. ;)

;;;","07/Jul/13 19:17;xedin;You got me wrong, I'm not trying to debate anything with you and minute details are proven to be the hardest sometimes... I'm just trying so say, to Jonathan most of all, if we go with borrowing approach and put cache in front of each row read we would suffer additional latency on every read (even if it's 1-3 ms, that adds to every file read on each request) of the row where people already report order of magnitude worse latencies on >= p99, am I right [~tjake]?

P.S. am I sorry I forgot to bow before I darred to speak up to such master who solved all your problems and who's modesty doesn't know it's limits...;;;","07/Jul/13 19:36;jbellis;I'm confused.  We already have a borrowing approach, what part of the proposal do you see adding 1-3ms?;;;","07/Jul/13 19:55;xedin;We don't borrow if there is no items in the cache, we just allocate new instance and then decide if we want to add it on recycle. 1-3 ms I'm talking is in Ben's multiway pool in getResourceHandler has to poll few times (once of those from blocking queue with 1 ms timeout) + atomic CAS.;;;","09/Jul/13 08:28;xedin;I think this discussion already outgrown proportions of the original problem, I want to suggest we try currently implemented queueing approach with expiry and maximum memory size cap as fix for problem related to memory usage (with LCS in particular) without introducing any additional complexity on getSegment (e.g. borrowing) at least for next 1.2 release. And in the meantime I'm open for discussion of other ways of handling caching which could be borrowing and gradual control over each instance in the queue, like proposed multiway pool, without adding any significant overhead on such critical path as reading file segments.  ;;;","09/Jul/13 16:44;jbellis;As mentioned above (but apparently I didn't actually touch fixver, oops) I'd rather just tweak default sstable size for 1.2.x and do a deeper fix in 2.0.  1.2 is 6 months old at this point and it seems like we have a pretty good workaround available without making deep code changes.  WDYT?;;;","09/Jul/13 19:04;xedin;I'm fine with that, I can work on a patch that would use multiway pool, [~dmeyer] Will you be able to test and compare expiry + mem limit to multiway borrowing, once I'm done with second patch, to check performance/latenties of reads and memory usage?;;;","10/Jul/13 00:38;xedin;I have started working on integrating multiway pool and it looks like we have two problems:

#1. As each SegmentedFile has to return unique instance using ""createReader(String)"" LoadingCache won't do for us, as we need get(K, Callable) per SSTableReader.
#2. as MultiWay returns a handle I changed RAR to have a setHandle method instead of passing SegmentedFile into constructor, which seems a bit hacky to me as we need to be careful in maintaining that relationship...

I did some performance testing (with attached patch) where MultiwayPool allocated per instance because we can't specify loader in borrow(...) yet, which should be a best case for it, not in terms of memory usage but contention. I loaded 5,000,000 keys with following stress command (./tools/bin/cassandra-stress -n 5000000 -S 512 -C 20 -Z LeveledCompactionStrategy) for initial data and then I made it run in a loop and was doing reads in parallel.

With writes:

Average read performance for MultiwayPool: median 6.2, 95th 11.4, 99.9th 78.8 
Average read performance for FileCacheService: median: 5.3, 95th 9.6, 99.9th 73.1

No writes, no compaction:

Average read performance for MultiwayPool: median 2.3, 95th 3.2, 99.9th 21.3 
Average read performance for FileCacheService: median: 1.7, 95th 2.9, 99.9th 19.2

I tried doing range_slice but due to timeouts I couldn't really complete test on any of the implementations, median latenties on average different by 3-4 ms.

Edit: I forgot to mention that I hardcoded maxSize per MultiwayPool instance which was fine for that test, but we really need a way to weight items if we are going to use it globally.
;;;","10/Jul/13 01:29;ben.manes;I think part of the problem is that idle caching is not overly efficient in this version. That can be improved upon, but maximum size might be better to verify as a baseline with first. 

Weights are supported as of the 7th.;;;","10/Jul/13 01:51;xedin;Good to know that weight is supported now, i must have overlooked it... Anyhow since we can't make pool global yet maxSize was good baseline, as you mentioned. ;;;","12/Jul/13 21:50;jbellis;bq. As each SegmentedFile has to return unique instance using ""createReader(String)"" LoadingCache won't do for us, as we need get(K, Callable) per SSTableReader.

Ben has added a borrow(K, Callable) method.

bq. as MultiWay returns a handle I changed RAR to have a setHandle method instead of passing SegmentedFile into constructor, which seems a bit hacky to me as we need to be careful in maintaining that relationship

Can you elaborate?;;;","13/Jul/13 04:16;ben.manes;I rewrote the time-to-idle policy, so it should be faster when enabled. 

Details (if interested)
------------------------
For prototyping purposes, I previously used a secondary Guava Cache to track idle resources. Unlike a cache's time-to-idle, which is reset when an entry is read, an object pool's concept of idle time is when a resource resides unused and ready to be borrowed. The use of a secondary Guava Cache meant that the resource had to be added and removed frequently, resulting in locking on the hashtable segments and incurring other maintenance overhead.

In Guava's cache we observed that expiration policies mirrored maximum size policies, but time based. Thus time-to-live is a FIFO queue and time-to-idle is an LRU queue. That let us leverage the amortization technique in CLHM to be used for expiration with O(1) reorder costs.

The new implementation strips off the unnecessary work by maintaining a time ordered queue that only supports adds and removals. For our definition of idle there is no need to reorder so it is effectively a FIFO. A tryLock guards the policy operations, draining a queue of pending operations if acquired. I decided to allow this to be proactively drained whenever possible, though if we see a need then we can buffer the operations for longer like the caches do.;;;","13/Jul/13 05:27;xedin;bq. Can you elaborate?

Please that a look to v2 patch PoolingSegmentFile.getSegment() method, I have added comment about that.

v2 patch makes cache global and adds per borrow loading of missing items. The problem I encountered is that system is unable to start because of the following error:

I'm not sure if I'm doing something wrong or it's a bug in multiway pool

{noformat}
 INFO 22:19:07,254 Opening /var/lib/cassandra/data/system/local/system-local-ja-21 (520 bytes)
ERROR 22:19:07,298 Exception encountered during startup
com.google.common.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2258)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3990)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4793)
	at com.github.benmanes.multiway.TransferPool.newResourceHandle(TransferPool.java:206)
	at com.github.benmanes.multiway.TransferPool.tryToGetResourceHandle(TransferPool.java:186)
	at com.github.benmanes.multiway.TransferPool.getResourceHandle(TransferPool.java:167)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:152)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:143)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:64)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1040)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.createFileDataInput(SSTableNamesIterator.java:96)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:109)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:62)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:87)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:124)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1458)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1284)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:55)
	at org.apache.cassandra.cql3.statements.SelectStatement.readLocally(SelectStatement.java:227)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:245)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:154)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:456)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:237)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Caused by: java.lang.NullPointerException
	at com.github.benmanes.multiway.TransferPool$2.call(TransferPool.java:209)
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4796)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3589)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2374)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2337)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2252)
	... 28 more
com.google.common.util.concurrent.UncheckedExecutionException: java.lang.NullPointerException
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2258)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3990)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4793)
	at com.github.benmanes.multiway.TransferPool.newResourceHandle(TransferPool.java:206)
	at com.github.benmanes.multiway.TransferPool.tryToGetResourceHandle(TransferPool.java:186)
	at com.github.benmanes.multiway.TransferPool.getResourceHandle(TransferPool.java:167)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:152)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:143)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:64)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1040)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.createFileDataInput(SSTableNamesIterator.java:96)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:109)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:62)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:87)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:124)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1458)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1284)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:55)
	at org.apache.cassandra.cql3.statements.SelectStatement.readLocally(SelectStatement.java:227)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:245)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:154)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:456)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:237)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Caused by: java.lang.NullPointerException
	at com.github.benmanes.multiway.TransferPool$2.call(TransferPool.java:209)
	at com.google.common.cache.LocalCache$LocalManualCache$1.load(LocalCache.java:4796)
	at com.google.common.cache.LocalCache$LoadingValueReference.loadFuture(LocalCache.java:3589)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2374)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2337)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2252)
	... 28 more
Exception encountered during startup: java.lang.NullPointerException
{noformat};;;","13/Jul/13 05:33;xedin;v2 updated with a small thing - added recordStats() to the builder so we have statistics information available on demand. Exception listed above still accours.;;;","13/Jul/13 06:12;ben.manes;I think I just fixed this issue in my last push. Sorry I didn't check my email earlier, as I found it when writing more test cases. The problem is that I forgot to default the lifecycle to a discarding instance if not used, after I made it an optional setting.;;;","13/Jul/13 06:26;xedin;I have pulled/rebuild your code and now there is another error:

{noformat}
 INFO 23:24:33,367 Opening /var/lib/cassandra/data/system/local/system-local-ja-21 (520 bytes)
ERROR 23:24:33,412 Exception encountered during startup
java.lang.ClassCastException: com.github.benmanes.multiway.ResourceKey$LinkedResourceKey cannot be cast to java.lang.String
	at org.apache.cassandra.io.util.PoolingSegmentedFile$1.weigh(PoolingSegmentedFile.java:35)
	at com.google.common.cache.LocalCache$Segment.setValue(LocalCache.java:2219)
	at com.google.common.cache.LocalCache$Segment.storeLoadedValue(LocalCache.java:3196)
	at com.google.common.cache.LocalCache$Segment.getAndRecordStats(LocalCache.java:2410)
	at com.google.common.cache.LocalCache$Segment.loadSync(LocalCache.java:2375)
	at com.google.common.cache.LocalCache$Segment.lockedGetOrLoad(LocalCache.java:2337)
	at com.google.common.cache.LocalCache$Segment.get(LocalCache.java:2252)
	at com.google.common.cache.LocalCache.get(LocalCache.java:3990)
	at com.google.common.cache.LocalCache$LocalManualCache.get(LocalCache.java:4793)
	at com.github.benmanes.multiway.TransferPool.newResourceHandle(TransferPool.java:215)
	at com.github.benmanes.multiway.TransferPool.tryToGetResourceHandle(TransferPool.java:195)
	at com.github.benmanes.multiway.TransferPool.getResourceHandle(TransferPool.java:176)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:156)
	at com.github.benmanes.multiway.TransferPool.borrow(TransferPool.java:147)
	at org.apache.cassandra.io.util.PoolingSegmentedFile.getSegment(PoolingSegmentedFile.java:65)
	at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:1040)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.createFileDataInput(SSTableNamesIterator.java:96)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:109)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:62)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:87)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:62)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:124)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:53)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1458)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1284)
	at org.apache.cassandra.db.Keyspace.getRow(Keyspace.java:332)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:55)
	at org.apache.cassandra.cql3.statements.SelectStatement.readLocally(SelectStatement.java:227)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:245)
	at org.apache.cassandra.cql3.statements.SelectStatement.executeInternal(SelectStatement.java:56)
	at org.apache.cassandra.cql3.QueryProcessor.processInternal(QueryProcessor.java:154)
	at org.apache.cassandra.db.SystemKeyspace.checkHealth(SystemKeyspace.java:456)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:237)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
{noformat};;;","13/Jul/13 06:44;ben.manes;okay, fixed. thanks for catching this. The tests no longer use raw keys, which should catch this from occurring again.;;;","13/Jul/13 07:31;xedin;Thanks, it worked that time, I did a quick test on local machine with global cache backed by MultiwayPool.

MultiwayPool average run: median 3.1, 95th 4.7, 99.9th 21.9
FileCacheService (+ LinkedTranferQueue) average run: median 2.4, 95th 3.0, 99.9th 17.1

It's the same setup that I used in my previous test with no writes and no compaction.

I will try to experiment with ArrayBlockingQueue as we know upper bound on concurrency and update my FileCacheService patch with either of them (LTQ vs. ABQ) soon.;;;","13/Jul/13 07:49;ben.manes;LTQ is best when you allow there to be some spin between producers and consumers, as its optimized for message passing scenarios. In your usage you don't allow any delay, so the likelihood of a successful transfer is low. When transfers are common, the overhead is less due to fewer contented CAS operations.

If desired, I can make the pool parameterized to take a supplier of queues to produce so you can parameterize that as well.

The pool will always be slower than the FileCacheService patch, since it does more. The decision is whether the performance degradation is acceptable and if the rational for the pool is to provide a finer grained eviction policy is still desired.;;;","13/Jul/13 08:07;xedin;I understand what is ideal use-case for LTQ is, I wanted to try it out since it was mentioned couple of times to have better results than CLQ under load.

I expected FileCacheService to be faster, I was just trying to check how much latency it actually adds even on such synthetic scenario as stress with no writes nor compaction. I strongly think (and I explained why multiple times) that we can't allow any degradation more than 0.5 ms in percentile on such critical path and why expiring items in bulk is okey for us since, in steady state, eviction would be driven by compactions cleaning all of open file descriptors per compacted sstable, where timed expiry would be very infrequent as read pattern doesn't change frequently in production systems.;;;","14/Jul/13 06:58;ben.manes;In a simple single-threaded benchmark, LTQ is relatively on par within the object pool.

Currently I have a finalizer on the handle as a safety net, both to catch my bugs and usage mistakes. This includes a note on the performance impact, which appears to have add 2.5x overhead. I had intended to replace this with phantom references instead, though now I'm wondering if I should not put any safety net in whatsoever.

# Finalizer
queueType  ns linear runtime
      ABQ 489 =========================
      SAQ 545 ============================
      CLQ 535 ===========================
      LBQ 578 ==============================
      LTQ 555 ============================
      LBD 490 =========================

# No finalizer
queueType  ns linear runtime
      ABQ 176 =========================
      SAQ 159 ======================
      CLQ 166 =======================
      LBQ 210 ==============================
      LTQ 183 ==========================
      LBD 181 =========================
;;;","14/Jul/13 08:48;xedin;Finalizers are never a good idea, because it requires double pass though GC and secondary actual finalization processing is single threaded with global queue, all of which creates additional CPU overhead as well as memory pressure. Also as as far as I remember CMS/G1 both process all types of references on FullGC phase even if refs to underlying objects where lost in young gen they are still promoted to old gen.;;;","14/Jul/13 08:59;ben.manes;yes, I understand that and that was documented. It was correct to add it early on, due to prototyping to help catch my bugs if there were race conditions. When looking at performance then it became appropriate to remove it as tests have baked the code.

The only aspect I'm grudgingly punting on is that I prefer warning developers when they have resource leaks, when possible without overhead, instead of silently letting production environments crash. This can be done with phantom references, but I dislike having libraries spawn its own threads (e.g. MapMaker did) and prefer amortizing it (e.g. CacheBuilder). There's no free hook in my pool to tie into, so I'm not providing that warning given you don't need it atm.;;;","14/Jul/13 09:01;ben.manes;anyways, this is now removed so hopefully your performance tests will see a favorable impact like mine do.;;;","14/Jul/13 09:52;xedin;As default queue size is set to 512MB the tests I did weren't actually using expiry on replacement (as there were no compaction nor dataset was been enough) so I shouldn't have touched the finalization path. I re-run with updated MultiwayPool code and saw the same results as previously stated in the steady state (which is expected), but when I add compaction in the mix (by overwriting data), latencies become shaky with MultiwayPool most notably on 99.9th percentile where random spikes by 10-15 ms are observed, FileCacheService 99.9th stays almost constant.;;;","14/Jul/13 20:11;ben.manes;Can you test without time-to-idle? Most likely there are bursts of expirations and the penalty is now better spread out. ;;;","15/Jul/13 02:39;ben.manes;Profiled to reduce allocations, cutting out about 10ms in a caliper benchmark. The dominating factor in a profile are reads from the cache.;;;","20/Jul/13 09:00;ben.manes;I replaced the external handle with directly providing the resource and maintaining the association in a threadlocal. This should better match your usage and resolve your concern above.

The primary motivation was to reduce object churn, as a handle was created per borrow. This reduced the hot spot time from an average invocation time of 1001us to 704us, when summing up the worst offenders.

This may remove the random spiked that you observed if they were caused by garbage collection.

98% of the overhead is now due to usage of other collections (Guava's Cache, LTQ, CLQ).;;;","20/Jul/13 10:41;ben.manes;Switching from LTQ to a custom elimination backoff stack appears to have dropped the 98% to 179us. The single threaded benchmark improves by 30ns. A significant gain was also observed when using an EBS instead of an array of CLQs in the time-to-idle policy.

I'm surprised by how much of a gain occurs, so I'll have to experiment further to understand if its factual. LTQ/CLQ are hindered by having to honor FIFO with j.u.c. interfaces, and LIFO elimination is the ideal strategy for an object pool. The more frequently successful exchanges may reduce down to eden-space GC, resulting in major net wins. That, or I'm prematurely believing that its working correctly.;;;","20/Jul/13 20:51;xedin;attaching patch with supports the latest version of multiway pool (and latest trunk) and adds (C)RAR deallocation on object removal. Sorry I didn't run the test sooner, I was quiet busy with other things... So I see that with this version 95-99.9 have degraded comparing to previous by ~3 ms but they don't shake as they used too - it's around ~5 ms now (I'm testing with expireAfterAccess). ;;;","20/Jul/13 21:03;ben.manes;Thanks Pavel.

I'm not sure why it got worse recently, except that you did turn on recordStats() in the last few runs. That can incur significant overhead by maintaining multiple LongAdders. Since you did not turn it on in the FileCache patch, which would provide similar stats, it may be an unfair comparison.

I'll try to wrap up my EBS prototype and push those changes soon. Those aren't on github yet.;;;","20/Jul/13 22:14;xedin;Maybe that's side effect of theadlocals, i am not sure. We need recordStats() to expose some via JMX, FileCache uses separate metrics service as we track explicitly every get/release which are exposed via JMX the same way as the rest of Cassandra, i didn't do that for multiway yet but that would be a final step.

Edit: I have also added onRemoval listener with the latest patch which does syscall to close the file among other things, which could have affected borrow/release latencies somehow.;;;","21/Jul/13 02:31;ben.manes;I was able to reduce the EBS version down to 120us. I probably won't have it on github until tomorrow, though.;;;","21/Jul/13 04:42;xedin;Ok, let me know and I will try to test it tomorrow.;;;","21/Jul/13 04:50;xedin;this is updated patch which actually includes onRemoval callback.;;;","22/Jul/13 05:28;ben.manes;Since the EBS version is still in progress, the code is shared below. It uses a treiber stack with backoff to an elimination array, mixing in optimizations borrowed from j.u.c.Exchanger. It performs superior to the queues by not to honor FIFO ordering, making cancellation easy to achieve.

While all tests pass, I think that the time-to-idle policy is corrupted as it assumed fifo ordering. I can make it tolerant of running out-of-order. I may try writing an elimination queue (LTQ uses a dual queue design).

https://github.com/ben-manes/multiway-pool/tree/elimination;;;","08/Aug/13 15:17;jbellis;[~xedin], have you had a chance to get back to this?;;;","08/Aug/13 18:34;xedin;I was actually destructed with other things and kind of waiting until code is done so I can test the version of multipool with all of the changes.;;;","08/Aug/13 18:50;ben.manes;My benchmark had a bug and EBS may only be on par with LTQ performance wise. I need to investigate that again, though.

I shifted focus to fixing the performance bottleneck in Guava's cache. The way we tracked usage history (e.g. LRU) was focused on common usage, but is a bottleneck on synthetic benchmarks. I made the fixes to CLHM (v1.4) and offered them upstream (issue 1487). I'll experiment with using CLHM instead to see if that removes the hotspot.;;;","10/Aug/13 04:02;ben.manes;EBS is 28% faster than LTQ. There might be opportunities to make it slightly faster with some tuning. I could probably add blocking methods if there was interest in experimenting with it for CASSANDRA-4718.;;;","10/Aug/13 05:01;xedin;Sounds good, let me know when it's available so I can run stress tests again.;;;","29/Aug/13 08:27;ben.manes;Sorry that I haven't had the time to work on this for a while. I've been playing with writing a queue using a combining arena, similar to how the stack has an elimination arena, and how to incorporate thread locals to reduce contention. That made me think about the flat combining technique, so after a little digging I uncovered a conversation I had with Chris Vest. At the time he was starting on an object pool, which he's released as Stormpot (http://chrisvest.github.io/stormpot). The implementation has some excellent ideas that are worth borrowing and mixing in. While it is not multi-way, he might be inclined to add that capability after playing with my prototype.;;;","17/Sep/13 21:58;jbellis;We're now beginning 2.0's ""stabilization"" period, so even if Ben's multiway cache were available tomorrow I'd be uneasy about pushing it out to our ""stable"" branch.

So I've committed Pavel's original queue-based cache with light revisions (primarily adding a conf setting to allow overriding the size).

Longer term I think CASSANDRA-6045 may be a better approach overall.;;;","17/Sep/13 22:19;ben.manes;sounds good to me.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Gossiper incorrectly drops AppState for an upgrading node,CASSANDRA-5660,12653697,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,19/Jun/13 13:07,16/Apr/19 09:32,14/Jul/23 05:53,19/Jun/13 19:24,1.2.6,2.0 beta 1,,,,,0,gossip,upgrade,,,"When doing a major rev upgrade (from 1.1 to 1.2, for example), after a node is upgraded to the new version, the older nodes incorrectly drop ApplicationState about the new nodes when the new ones attempt to connect. The short story is the older node doesn't allow the new node to connect because it contains a higher message protocol version number, and will call Gossiper.addSavedEndpoint(), which overwrites any previous entry in the endpointStateMap. 

Here's a fuller description of the steps:

0) have a stable, multi-datacenter cluster running 1.1.x 
1) upgrade one node to 1.2, and bring it up 
2) 1.2 node starts to gossip 
3) 1.1 node gets request in IncomingTcpConnection. 1.1 checks the messaging version, sees that it is greater than it's own, and calls Gossiper.addSavedEnpoint, which overwrites any previous entry in endpointStateMap. 1.1 closes connection 
4) 1.1 node chooses to gossip with 1.2 (as it kept the InetAddr around), on 1.2's publicIp. As standard part of request header, it sends it's version along 
5) 1.2 node ACKs the gossip request, and sends along the message version of 1.1 in it's header 
6) 1.1 can successfully accept the ACK connection and Gossiper.handleMajorStateChange() is called and the 1.2 node is fully 'alive' to this 1.1 node 
6a) if you are using Ec2MultiRegionSnitch, it's onAlive/onJoin methods are called, and eventually onChange. If 1.2 node is in same region as 1.1, 
1.1 node will close the socket on the publicIP, and attempt to connect on localIp 
7) when 1.2 reads on publicIp socket it will find it has been closed (broken pipe). It then closes its end of socket and calls Gossiper.resetVersion(). 
This throws away the previously recorded messaging version for the 1.1 node (Gossiper.versions). Thus, when 1.2 goes to communicate back with 1.1, 
it sends it's message protocol version (instead of 1.1's), and we're back at step 3 again.

For most use cases, there is an very short term problem of the AppState getting dropped, but that will be quickly fixed when the lower version node connects to the newer node (and passes it's message protocol version). However, everything goes to hell in a handbasket when the Ec2MultiRegionSnitch gets involved (steps 6a/7 above) and the connections keep getting closed.",,jasobrown,sbtourist,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5498,,,,,,,,,,,,,,,"19/Jun/13 13:09;jasobrown;0001-5660.diff;https://issues.apache.org/jira/secure/attachment/12588586/0001-5660.diff",,,,,,,,,,,,,,,,,,,,1.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,333974,,,Wed Jun 19 19:24:06 UTC 2013,,,,,,,,,,"0|i1lm1b:",334300,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"19/Jun/13 13:09;jasobrown;Attached patch changes the logic in Gossiper.addSavedEndpoint() to check if there is already an existing entry in the endointStateMap, and if so, just reset it's heartbeat version. This will keep around any previous ApplicationState info.;;;","19/Jun/13 17:50;brandon.williams;+1;;;","19/Jun/13 19:24;jasobrown;committed to 1.2 and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Equals method in PermissionDetails causes StackOverflowException,CASSANDRA-5655,12653462,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,samt,samt,samt,18/Jun/13 14:48,16/Apr/19 09:32,14/Jul/23 05:53,18/Jun/13 15:20,1.2.6,,,,,,0,,,,,"It simply delegates to Guava's Objects.equal, which itself ends up calling back to the original caller's equals after performing some basic checks.
",,aleksey,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/13 14:50;samt;5655.txt;https://issues.apache.org/jira/secure/attachment/12588375/5655.txt",,,,,,,,,,,,,,,,,,,,1.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,333740,,,Tue Jun 18 15:20:20 UTC 2013,,,,,,,,,,"0|i1lklr:",334068,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"18/Jun/13 14:50;samt;patch against 1.2 branch attached;;;","18/Jun/13 15:20;aleksey;D-oh. It's correct in AuthenticatedUser and DataResource, but somehow slipped in PermissionDetails.
Committed with slight cosmetic modification for consistency with other equals() methods in auth classes.
Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Suppress custom exceptions thru jmx,CASSANDRA-5652,12653349,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius,dbrosius,18/Jun/13 00:03,16/Apr/19 09:32,14/Jul/23 05:53,18/Jun/13 12:48,1.2.6,,,Legacy/Tools,,,0,,,,,"startNativeTransport, can send back 

org.jboss.netty.channel.ChannelException

which causes jconsole to puke with a bad message such as

Problem invoking startNativeTransport: java.rmi.UnmarshalException: Error unmarshaling return header; nested exception is: java.io.EOFException


convert to RuntimeException so you get something like:

org.jboss.netty.channel.ChannelException: Failed to bind to: localhost/127.0.0.1:9042",,dbrosius,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/13 00:04;dbrosius;5652.txt;https://issues.apache.org/jira/secure/attachment/12588262/5652.txt",,,,,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,333627,,,Tue Jun 18 20:52:50 UTC 2013,,,,,,,,,,"0|i1ljwn:",333955,,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,,"18/Jun/13 11:35;slebresne;lgtm, though provided jconsole can handle it, I'd replace the exception by something like:
{noformat}
new RuntimeException(""Error starting native transport: "" + e.getMessage(), e);
{noformat}
so we don't lose the original stack trace (but if it's still too hard to handle for JMX, let's just ship it as in you patch).;;;","18/Jun/13 12:39;dbrosius;yea, unfortunately jmx will still CNFE if you pass e as the initcause, but i will change the error message.;;;","18/Jun/13 12:48;dbrosius;committed to cassandra-1.2 as f30015c862eb913d1f0cf8c10d201de5698a6dda;;;","18/Jun/13 20:52;dbrosius;really, imo mbeans should be seperate objects that impl the interface and just call into the real objects, so that these mbean objects can do the exception sanitization stuff (and other cleaning) outside of real code. but, probably just being pendantic.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Typo in error message in cqlsh (CQL3) when using more than one relation with a IN,CASSANDRA-5647,12653074,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,ruivieira,ruivieira,16/Jun/13 16:38,16/Apr/19 09:32,14/Jul/23 05:53,16/Jun/13 17:10,1.2.6,,,Legacy/Tools,,,0,,,,,"When performing a query such as

bq. select * from foo where bucket in (1) and id in ('O1', 'O2') and bucket in (2) and id in ('O3', 'O4') ;

the error message in cqlsh is:

bq. Bad Request: bucket cannot be restricted by more than one *reation* if it includes a IN

Expected:

bq. Bad Request: bucket cannot be restricted by more than one *relation* if it includes a IN
","Linux 3.8.0-25-generic #37-Ubuntu 13.04 SMP
[cqlsh 3.0.2 | Cassandra 1.2.5-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.36.0] (installed via ccm)",dbrosius,ruivieira,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,333397,,,Sun Jun 16 17:10:15 UTC 2013,,,,,,,,,,"0|i1lihj:",333725,,,,,,,,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,,"16/Jun/13 17:10;dbrosius;Thanks

committed as 155afa1b7f22dedad3fb9c0d7270a101e72be814 to cassandra-1.1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exception swallowing in ..net.MessagingService,CASSANDRA-5644,12653022,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,metskem,metskem,metskem,15/Jun/13 13:29,16/Apr/19 09:32,14/Jul/23 05:53,15/Jun/13 15:44,1.2.6,,,Local/Config,,,0,,,,,"While I was trying to setup internode encryption, I spent too much time finding out that the name of my keystore was wrong.
Main reason was the org/apache/cassandra/net/MessagingService.java swallowing the exception and just spitting out:
{noformat}
ERROR [main] 2013-06-15 12:49:43,758 CassandraDaemon.java (line 358) Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Unable to create ssl socket
	at org.apache.cassandra.net.MessagingService.getServerSocket(MessagingService.java:432)
	at org.apache.cassandra.net.MessagingService.listen(MessagingService.java:412)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:564)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:529)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:428)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Unable to create ssl socket
{noformat}

I will attach a minor patch, that shows just a bit more:

{noformat}
ERROR [main] 2013-06-15 12:58:44,979 CassandraDaemon.java (line 358) Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Unable to create ssl socket
	at org.apache.cassandra.net.MessagingService.getServerSocket(MessagingService.java:432)
	at org.apache.cassandra.net.MessagingService.listen(MessagingService.java:412)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:564)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:529)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:428)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:354)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:453)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:496)
Caused by: java.io.IOException: Error creating the initializing the SSL Context
	at org.apache.cassandra.security.SSLFactory.createSSLContext(SSLFactory.java:124)
	at org.apache.cassandra.security.SSLFactory.getServerSocket(SSLFactory.java:53)
	at org.apache.cassandra.net.MessagingService.getServerSocket(MessagingService.java:428)
	... 7 more
Caused by: java.io.FileNotFoundException: conf/oeps-wrong-truststore.jks (No such file or directory)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:138)
	at java.io.FileInputStream.<init>(FileInputStream.java:97)
	at org.apache.cassandra.security.SSLFactory.createSSLContext(SSLFactory.java:105)
	... 9 more
Unable to create ssl socket
{noformat}

kind regards,
Harry
","RedHat Linux, OpenJDK 7",dbrosius,metskem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/13 13:30;metskem;CASSANDRA-5644.patch;https://issues.apache.org/jira/secure/attachment/12587976/CASSANDRA-5644.patch",,,,,,,,,,,,,,,,,,,,1.0,metskem,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,333345,,,Sat Jun 15 15:44:49 UTC 2013,,,,,,,,,,"0|i1li5z:",333673,,,,,,,,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,,"15/Jun/13 15:44;dbrosius;Thanks!

committed to cassandra-1.2 as 082684dd806a0bf0964901e3749992e4b1a1d650;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
error in help text of cassandra-cli (--tspw option shows SSL: full path to truststore),CASSANDRA-5643,12653019,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,metskem,metskem,metskem,15/Jun/13 12:43,16/Apr/19 09:32,14/Jul/23 05:53,15/Jun/13 15:30,1.2.6,,,Legacy/Tools,,,0,,,,,"When you issue the cmdline ""cassandra-cli -?"" you get the following:

{noformat}
cssndra@ubuntu2:/opt/cassandra/conf$ cassandra-cli -?
usage: cassandra-cli
 -?,--help                                           usage help
 -alg,--ssl-alg <ALGORITHM>                          SSL: algorithm
                                                     (default: SunX509)
 -B,--batch                                          enabled batch mode
                                                     (suppress output; errors are fatal)
 -ciphers,--ssl-ciphers <CIPHER-SUITES>              SSL: comma-separated
                                                     list of encryption suites to use
    --debug                                          display stack-traces
                                                     (NOTE: We print strack-traces in the places where it makes sense even
                                                     without --debug)
 -f,--file <FILENAME>                                load statements from
                                                     the specific file
 -h,--host <HOSTNAME>                                cassandra server's
                                                     host name
    --jmxpassword <JMX-PASSWORD>                     JMX service password
    --jmxport <JMX-PORT>                             JMX service port
    --jmxusername <JMX-USERNAME>                     JMX service username
 -k,--keyspace <KEYSPACE>                            cassandra keyspace
                                                     user is authenticated against
 -p,--port <PORT>                                    cassandra server's
                                                     thrift port
 -prtcl,--ssl-protocol <PROTOCOL>                    SSL: connections
                                                     protocol to use (default: TLS)
 -pw,--password <PASSWORD>                           password for
                                                     cassandra authentication
    --schema-mwt <TIME>                              Schema migration wait
                                                     time (secs.), default is 10 secs
 -st,--store-type <STORE-TYPE>                       SSL: type of store
 -tf,--transport-factory <TRANSPORT-FACTORY>         Fully-qualified
                                                     TTransportFactory class name for creating a connection to cassandra
 -ts,--truststore <TRUSTSTORE>                       SSL: full path to
                                                     truststore
 -tspw,--truststore-password <TRUSTSTORE-PASSWORD>   SSL: full path to
                                                     truststore
 -u,--username <USERNAME>                            user name for
                                                     cassandra authentication
 -v,--verbose                                        verbose output when
                                                     using batch mode
{noformat}

As you see, the help text for the --tspw switch has the same as the --ts switch.
","N/A
(Ubuntu Linux, OpenJDK 7)",dbrosius,metskem,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jun/13 12:45;metskem;CASSANDRA-5634.patch;https://issues.apache.org/jira/secure/attachment/12587973/CASSANDRA-5634.patch",,,,,,,,,,,,,,,,,,,,1.0,metskem,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,333342,,,Sat Jun 15 15:30:15 UTC 2013,,,,,,,,,,"0|i1li5b:",333670,,,,,,,,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,,"15/Jun/13 12:45;metskem;proposed patch;;;","15/Jun/13 15:30;dbrosius;Thanks!

committed to cassandra-1.2 as d87ed95c0fbffc978cbcaec5d02cb65fdd4b0ea4;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ThriftServer.stop() hangs forever,CASSANDRA-5635,12652678,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,snazy,snazy,13/Jun/13 15:00,16/Apr/19 09:32,14/Jul/23 05:53,13/May/14 00:14,2.1 rc1,,,,,,0,,,,,"I've written a very small main() method just to start to test ""how to embed Cassandra"". But the code hangs while executing CassandraDaemon.stop()...
I've used a default {{cassandra.yaml}} file.

{noformat}
cassandraDaemon = new CassandraDaemon();
cassandraDaemon.init(null);
cassandraDaemon.start();
cassandraDaemon.stop();
{noformat}

{{CassandraDaemon.stop()}} calls {{ThriftServer.stop()}, which ends somehow in {{TCustomServerSocket.close()}}, which sets its field {{serverSocket=null}}. This causes {{CustomTThreadPoolServer.server()}} to loop forever, because it's {{stopped}} field is still {{false}} - {{TServerTransport.accept()}} immediatly throws a {{TTransportException}} because {{TCustomServerSocket}}'s {{serverSocket}} is {{null}}.
",,dbrosius,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/May/14 02:31;dbrosius;5635.txt;https://issues.apache.org/jira/secure/attachment/12644368/5635.txt",,,,,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,333002,,,Tue May 13 00:14:09 UTC 2014,,,,,,,,,,"0|i1lg1r:",333330,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"27/Apr/14 22:28;dbrosius;On trunk at least, what's keeping it alive is the threads

COMMIT-LOG-ALLOCATOR  (CommitLogSegmentManager.java)
PERIODIC-COMMIT-LOG-SYNCER (PeriodicCommitLogService.java)
ACCEPT-(ip) (MessagingService)

not of which are terminated by CassandraDaemon.stop()

might need a StorageService.instance.drain(); afterwards.;;;","12/May/14 02:31;dbrosius;make SlabPoolCleaner thread, a daemon.;;;","12/May/14 19:57;jbellis;+1;;;","13/May/14 00:14;dbrosius;committed to cassandra-2.1 as commit 81bf2b08895336b7f650624b7a581ccc5e2dbf26;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
relocatingTokens should be ConcurrentMap,CASSANDRA-5634,12652563,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,13/Jun/13 05:22,16/Apr/19 09:32,14/Jul/23 05:53,19/Jun/13 18:56,1.2.6,,,,,,0,vnodes,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Jun/13 05:23;jbellis;5634.txt;https://issues.apache.org/jira/secure/attachment/12587562/5634.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,332887,,,Wed Jun 19 18:56:22 UTC 2013,,,,,,,,,,"0|i1lfc7:",333215,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"13/Jun/13 05:23;jbellis;Trivial patch.;;;","19/Jun/13 17:49;brandon.williams;+1;;;","19/Jun/13 18:56;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cross-DC bandwidth-saving broken,CASSANDRA-5632,12652525,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,12/Jun/13 23:37,16/Apr/19 09:32,14/Jul/23 05:53,20/Jun/13 20:54,1.2.6,,,,,,0,,,,,"We group messages by destination as follows to avoid sending multiple messages to a remote datacenter:

{code}
        // Multimap that holds onto all the messages and addresses meant for a specific datacenter
        Map<String, Multimap<Message, InetAddress>> dcMessages
{code}

When we cleaned out the MessageProducer stuff for 2.0, this code

{code}
                    Multimap<Message, InetAddress> messages = dcMessages.get(dc);
...
                    messages.put(producer.getMessage(Gossiper.instance.getVersion(destination)), destination);
{code}

turned into

{code}
                    Multimap<MessageOut, InetAddress> messages = dcMessages.get(dc);
...
                    messages.put(rm.createMessage(), destination);
{code}

Thus, we weren't actually grouping anything anymore -- each destination replica was stored under a separate Message key, unlike under the old CachingMessageProducer.",,cburroughs,dbrosius,enigmacurry,hayato.shimizu,jeromatron,jjordan,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6314,,,,,,,,,,,,,,,,,,,,"18/Jun/13 17:27;jbellis;5632-v2.txt;https://issues.apache.org/jira/secure/attachment/12588416/5632-v2.txt","12/Jun/13 23:38;jbellis;5632.txt;https://issues.apache.org/jira/secure/attachment/12587523/5632.txt","18/Jun/13 15:02;hayato.shimizu;cassandra-topology.properties;https://issues.apache.org/jira/secure/attachment/12588382/cassandra-topology.properties","18/Jun/13 15:02;hayato.shimizu;fix_patch_bug.log;https://issues.apache.org/jira/secure/attachment/12588383/fix_patch_bug.log",,,,,,,,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,332849,,,Fri Sep 20 22:02:54 UTC 2013,,,,,,,,,,"0|i1lf3r:",333177,,,,,,,,,dbrosius,,dbrosius,Normal,,,,,,,,,,,,,,,hayato.shimizu,,,"12/Jun/13 23:38;jbellis;Backport of 62f429337caf0aa83b68720a5904e8527b840c80 from 2.0 to fix.;;;","13/Jun/13 22:28;hayato.shimizu;Tested and fixed the issue.;;;","18/Jun/13 15:04;hayato.shimizu;The patch fixes the issue of bandwidth-saving.

However, there seems to be two regressive issues being introduced.

1. DC2 coordinator selection by the DC1 coordinator is not equal across all available nodes in DC2. Some nodes in DC2 are unused as coordinators.
2. When using cqlsh, with EACH_QUORUM/ALL, with tracing on, on a row insert, RPC timeout occurs from a node that is not verifiable in the trace output.

Trace output has been attached for a 6 node cluster, DC1:3, DC2:3 replication factor configuration. network-topology configuration is also attached for clarity.;;;","18/Jun/13 15:18;jbellis;bq. Secondary DC coordinator node is always the same node. This introduces a bottleneck in the secondary DC.

It's the same node for a given token range.  When all token ranges are considered, it is evenly spread.

bq. RPC timeout occurs from a node that is not verifiable in the trace output.

Well.  That's not a very useful error message, is it. :);;;","18/Jun/13 15:29;jbellis;.55 is the forwarding node in DC2.  It logs that it applies the mutation and acks it:

{noformat}
    Enqueuing response to /192.168.56.50 | 05:57:33,825 | 192.168.56.55 |          14785
{noformat}

But there is no ""Processing response from /192.168.56.55"" line logged by .50.  Hmm.;;;","18/Jun/13 16:18;jbellis;You're not running with cross_node_timeout enabled, are you?  Because some of these clocks are minutes apart.

{noformat}
# Enable operation timeout information exchange between nodes to accurately
# measure request timeouts, If disabled cassandra will assuming the request
# was forwarded to the replica instantly by the coordinator
#
# Warning: before enabling this property make sure to ntp is installed
# and the times are synchronized between the nodes.
cross_node_timeout: false
{noformat};;;","18/Jun/13 17:25;jbellis;I note that .55 doesn't ever log ""Sending message"" to .50 either.  So the message is getting dropped somewhere inside .55's MessagingService.

cross_node_timeout is my best guess.  Next-best guess is that there's a reconnect somehow dropping the message a la CASSANDRA-5393.;;;","18/Jun/13 17:27;jbellis;v2 attached that rebases and does some further cleanup to improve trace messages.;;;","19/Jun/13 01:59;dbrosius;other than simple FF, +LGTM

-import org.apache.cassandra.tracing.Tracing;
+import org.apache.cassandra.tracing.4Tracing;;;;","19/Jun/13 03:42;jbellis;Committed.  That should give Hayato an easier way to test at least. :);;;","19/Jun/13 21:03;hayato.shimizu;It seems that issue 1. in my earlier comment was fixed with 1.2.5 by Yuki (CASSANDRA-5424), where in 1.2.4 NetworkTopologyStrategy.calculateNaturalEndpoints HashSet replicas was changed to LinkedHashSet, so please ignore.;;;","20/Jun/13 03:34;enigmacurry;I've [written a dtest|https://github.com/riptano/cassandra-dtest/pull/13/files] that automates the testing of this issue.

This test clearly shows that the coordinator was talking to more than one node in a different datacenter, and the patch resolves that issue. It also verifies that [~hayato.shimizu]'s comment about using the same forwarder is not happening now.

[~jbellis] - I noticed in your [blog post about tracing|http://www.datastax.com/dev/blog/advanced-request-tracing-in-cassandra-1-2] you said not to rely on the activity field, well, that's exactly what I'm doing here. So, +1 to the idea of making those enums so this doesn't break in the future.;;;","20/Jun/13 03:42;jbellis;Thanks, Ryan.  Go ahead and create a ticket for that and I'll put my next junior hire on it. :);;;","20/Sep/13 18:46;rcoli;Do you have an ""affects"" version for this issue? Description says it started when a re-write for 2.0 started, but it affects 1.2.x so I'm confused? :D;;;","20/Sep/13 22:02;jeromatron;I believe for the issue that was fixed here it originated in 1.2 and was present up through 1.2.5.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when creating column family shortly after multinode startup,CASSANDRA-5631,12652464,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,mserrano,mserrano,12/Jun/13 19:27,16/Apr/19 09:32,14/Jul/23 05:53,19/Feb/14 19:28,1.2.16,2.0.6,2.1 beta1,,,,2,,,,,"I'm testing a 2-node cluster and creating a column family right after the nodes startup.  I am using the Astyanax client.  Sometimes column family creation fails and I see NPEs on the cassandra server:

{noformat}
2013-06-12 14:55:31,773 ERROR CassandraDaemon [MigrationStage:1] - Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.db.DefsTable.addColumnFamily(DefsTable.java:510)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:444)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:354)
	at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:55)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)

{noformat}

{noformat}
2013-06-12 14:55:31,880 ERROR CassandraDaemon [MigrationStage:1] - Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:475)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:354)
	at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:55)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
	at java.util.concurrent.FutureTask.run(FutureTask.java:166)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
{noformat}
",,aleksey,colinkuo,jjordan,kohlisankalp,mishail,mserrano,Proggie,rlow,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/Feb/14 08:47;aleksey;5631.txt;https://issues.apache.org/jira/secure/attachment/12629732/5631.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,332788,,,Wed Feb 19 19:28:18 UTC 2014,,,,,,,,,,"0|i1leqf:",333117,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"12/Jun/13 19:29;jbellis;Please test 1.2.5;;;","12/Jun/13 21:21;mserrano;Unfortunately we are using the Astyanax client which only supports up to 1.2.2.  Is there anything I can do to detect this case and retry?  or work around it?;;;","13/Jun/13 19:46;mserrano;I was incorrect regarding Astyanax support.  I just had a classpath issue.  Anyway, I have tested in 1.2.5 and can no longer reproduce.  Thanks!;;;","13/Jun/13 19:49;jbellis;Great; thanks for the followup!;;;","24/Jan/14 14:10;rlow;I've seen this on Cassandra 1.2.11.  It happens if you create a keyspace, following quickly by creating a column family within that keyspace.  The NPE is thrown because Schema.instance.getTableDefinition returns null for the keyspace but it isn't checked.

In the case I saw, the node that threw the NPE had problems so it wasn't receiving many messages - it didn't get the create keyspace message but did get the create CF message.  Even if a node doesn't have any problems, the ordering of these messages is not guaranteed.  The node will get the create keyspace message some time later (probably about 60 seconds later when another node has noticed the schema version is wrong) but it won't attempt to recreate the CF unless there is a further CF change (create, update or delete) within that keyspace.  Only then is the current cached schema compared with the on disk schema (in DefsTable.mergeColumnFamilies).  It then notices the CF doesn't exist so creates it.  This could never happen, so the node won't ever create the CF (unless it is restarted).

I think a fix would be to catch the NPEs above, and then, on learning about a new keyspace, check to see if any CFs should have been created for that keyspace.

I haven't tried to repro this on 2.0 but the code looks almost identical so I would expect it to still be present.

Could someone reopen the ticket please?;;;","30/Jan/14 18:37;Proggie;I'm just seeing the exact same thing on my 2 node cluster (cassandra 2.0.4).;;;","06/Feb/14 02:18;aleksey;bq. I think a fix would be to catch the NPEs above, and then, on learning about a new keyspace, check to see if any CFs should have been created for that keyspace.

This sounds reasonable to me. Another way would be to send the keyspace mutation serialized along with any column families created/altered messages, so that there will never be an NPE there in the first place. This had actually come up before. Will have a look.;;;","18/Feb/14 20:26;jjordan;If the issue is about the node not having gotten the create KS yet.  Can you not just wait for schema agreement in your client before going on to the next create?  That is how I do things to avoid these kinds of issues.;;;","19/Feb/14 08:50;aleksey;The attached patch sends the serialized keyspace itself with any CF update/create/drop migration, making the NPE in question impossible - the keyspace will always be there now.;;;","19/Feb/14 13:18;slebresne;Lgtm (nit: I'd rename serializeKeyspace to say addSerializedKeyspace).

bq. Can you not just wait for schema agreement in your client before going on to the next create?

For the record, Jeremiah is right that clients are supposed to wait for schema agreement if they want to guarantee the table creation won't fail just after the keyspace one (or alternatively make sure both creation goes through the same coordinator node). Of course, we shouldn't NPE internally if a user don't respect that and that's just what this ticket is about.;;;","19/Feb/14 19:28;aleksey;Committed with a nit, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect handling of blob literals when the blob column is in reverse clustering order,CASSANDRA-5629,12652101,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,10/Jun/13 22:00,16/Apr/19 09:32,14/Jul/23 05:53,10/Jun/13 22:12,1.2.6,,,,,,0,cql3,,,,"Parsing goes through ReversedType.fromString() in this case, and that doesn't strip ""0x"" when calling BytesType.fromString().

The attached patch makes Constants.parsedValue() ReversedType-aware. ",,aleksey,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jun/13 22:01;aleksey;5629.txt;https://issues.apache.org/jira/secure/attachment/12587140/5629.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,332425,,,Mon Jun 10 22:12:27 UTC 2013,,,,,,,,,,"0|i1lchz:",332754,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"10/Jun/13 22:05;slebresne;+1;;;","10/Jun/13 22:12;aleksey;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BlacklistingCompactionTest missing Apache license,CASSANDRA-5627,12651946,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,carlyeks,carlyeks,carlyeks,09/Jun/13 20:16,16/Apr/19 09:32,14/Jul/23 05:53,12/Jun/13 05:16,2.0 beta 1,,,,,,0,,,,,BlacklistingCompactionsTest is missing the Apache license header,,carlyeks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jun/13 20:19;carlyeks;5627.patch;https://issues.apache.org/jira/secure/attachment/12586970/5627.patch",,,,,,,,,,,,,,,,,,,,1.0,carlyeks,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,332270,,,Wed Jun 12 05:16:06 UTC 2013,,,,,,,,,,"0|i1lbjr:",332599,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"12/Jun/13 05:16;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update CqlRecordWriter to conform to RecordWriter API,CASSANDRA-5622,12651522,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,jbellis,jbellis,06/Jun/13 22:09,16/Apr/19 09:32,14/Jul/23 05:53,17/Jun/13 22:43,1.2.6,,,,,,0,,,,,"{{RecordWriter<K, V>}} is supposed to write values V that can be uniquely identified by keys K.

Currently CqlRW requires the user to give it all the bind variables for a complete statement in V, and effectively ignores K.
",,alexliu68,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jun/13 21:08;alexliu68;5622-1-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12586799/5622-1-1.2-branch.txt","07/Jun/13 20:45;alexliu68;5622-1-trunk.txt;https://issues.apache.org/jira/secure/attachment/12586794/5622-1-trunk.txt","17/Jun/13 17:56;alexliu68;5622-2-1.2-branch.txt;https://issues.apache.org/jira/secure/attachment/12588169/5622-2-1.2-branch.txt","17/Jun/13 18:02;alexliu68;5622-2-trunk-branch.txt;https://issues.apache.org/jira/secure/attachment/12588172/5622-2-trunk-branch.txt",,,,,,,,,,,,,,,,,4.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,331848,,,Mon Jun 17 22:43:29 UTC 2013,,,,,,,,,,"0|i1l8yn:",332177,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"06/Jun/13 22:15;jbellis;Two solutions suggest themselves:
# Embrace ignoring of K and just pass new Object()
# Treat K as designed and accept responsibility for generating appropriate bind variables

I'm not a huge fan of #1 since it forces the user to do more work to generate bind variables for the PK values.  I'd rather automate that.

Let me give an example of what I mean in #2.  Suppose we have word counts as follows:
{code}
CREATE TABLE word_counts (
  file_name text,
  counted_at timestamp,
  word text,
  count int,
  PRIMARY KEY ((file_name, counted_at), word)
);
{code}

Then the user would configure CQL such as ""UPDATE word_counts SET count = ?"".  CqlRW would inspect the table definition and realize it needs to add "" WHERE file_name = ? AND counted_at = ? AND word = ?"".

Then when it gets a Map<String, BB> as K, it combines those with the List<BB> variables.  It knows the ordering of the pk columns since it generated it itself.

One downside: this doesn't lend itself to INSERT statements, since that does not separate SET/WHERE the way UPDATE does.  This is a limitation I can live with.;;;","07/Jun/13 00:13;alexliu68;Agreed, #2 is more concise and easy to use.;;;","07/Jun/13 20:45;alexliu68;patch for trunk is attached.;;;","07/Jun/13 21:08;alexliu68;Patch for 1.2 branch is attached;;;","12/Jun/13 22:19;jbellis;Is it worth supporting INSERT at all since it's semantically identical to UPDATE?  I think I'd rather just support UPDATE only and avoid the confusion of two different ways of working.;;;","13/Jun/13 21:23;alexliu68;I will disable INSERT query, only allow UPDATE or DELETE statement.;;;","17/Jun/13 14:09;jbellis;How is that coming, Alex?  Would like to ship this in 1.2.6.;;;","17/Jun/13 15:24;alexliu68;I was busy with other project, I will get it done today.;;;","17/Jun/13 18:04;alexliu68;[~jbellis] I attach the version 2 patches which throws 
{code}
   IOException(""INSERT statement is depreciated and not supported, please use UPDATE/DELETE statement.""); 
{code}
if the INSERT type query is used for writter.;;;","17/Jun/13 20:44;jbellis;Pushed a bunch of changes to https://github.com/jbellis/cassandra/commits/5622.;;;","17/Jun/13 21:24;alexliu68;+1 for the changes, [~jbellis] I can create the final patch.;;;","17/Jun/13 22:43;jbellis;Not necessary, I merge --squashed and committed.  Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Link for latest bleeding edge releases is broken,CASSANDRA-5621,12651470,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,anssssss,anssssss,06/Jun/13 21:03,16/Apr/19 09:32,14/Jul/23 05:53,13/Mar/14 23:08,,,,Legacy/Documentation and Website,,,0,,,,,"This issue is just like [CASSANDRA-2736|https://issues.apache.org/jira/browse/CASSANDRA-2736]. The builds link on this [page|https://cassandra.apache.org/download/] points to [Jenkins continuous integration|https://builds.apache.org/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/] which reports a 404 status.

",,anssssss,sureshsajja,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,331796,,,2013-06-06 21:03:46.0,,,,,,,,,,"0|i1l8n3:",332125,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect Syntax for Timestamp in Select query,CASSANDRA-5620,12651414,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,tariqrahiman,tariqrahiman,06/Jun/13 18:36,16/Apr/19 09:32,14/Jul/23 05:53,23/Aug/13 17:29,,,,Legacy/Documentation and Website,,,0,,,,,"In the page http://cassandra.apache.org/doc/cql3/CQL.html under the heading Queries --> SELECT --> <where-clause>  We have a table posts followed by a Query which doesnt seem to work against the Cassandra Server. 
Timestamp requires quotes in the WHERE clause.

CREATE TABLE posts (
    userid text,
    blog_title text,
    posted_at timestamp,
    entry_title text,
    content text,
    category int,
    PRIMARY KEY (userid, blog_title, posted_at)
)

Incorrect
SELECT entry_title, content FROM posts WHERE userid='john doe' AND blog_title='John's Blog' AND posted_at >= 2012-01-01 AND posted_at < 2012-01-31

I get the the error mismatched character '<EOF>' expecting '''

If I correc the error in Johns''Blog and proceed I get missing EOF at '-01'

Correct
SELECT entry_title, content FROM posts WHERE userid='john doe' AND blog_title='John''s Blog' AND posted_at >= '2012-01-01' AND posted_at < '2012-01-31'",,aleksey,tariqrahiman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,331740,,,Fri Aug 23 17:29:06 UTC 2013,,,,,,,,,,"0|i1l8b3:",332071,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"23/Aug/13 17:29;aleksey;Fixed on Aug 10 by 2a8379372e6cbcd2fa3008b203c70f57ad29f392;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL now() on prepared statements is evaluated at prepare time and not query execution time,CASSANDRA-5616,12651102,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,mpenet,mpenet,05/Jun/13 12:10,16/Apr/19 09:32,14/Jul/23 05:53,05/Jun/13 18:07,1.2.6,,,Legacy/CQL,,,0,cql3,,,,"insert into some_table (id,time) values (?,now())

On the example above now() will always have the same value, it should probably be evaluated at ""query"" time and not at prepare time. ",,aleksey,mpenet,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Jun/13 15:12;slebresne;5616.txt;https://issues.apache.org/jira/secure/attachment/12586332/5616.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,331428,,,Wed Jun 05 18:07:29 UTC 2013,,,,,,,,,,"0|i1l6dz:",331760,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"05/Jun/13 14:15;aleksey;Another option (that I'd prefer) is to reject now() in prepared statements altogether.;;;","05/Jun/13 15:12;slebresne;Being able to have now() (or any non-pure function really, it's just that now is only one we have so far) can be handy so I'm not in favor of rejecting it altogether.

Now I agree that the only user-visible semantic that make sense for functions is that they are evaluated at execution time. It just happens that if the function is pure and all it's argument are terminal it's more efficient to evaluate it at preparation time without failing that semantic (by definition of a pure function), and that's what the current implementation does. It's just an oversight that now() is not pure and so shouldn't be optimized that way.

Thus attaching a simple patch that correctly distinguishes pure and non pure functions.
;;;","05/Jun/13 17:05;aleksey;All right. Given how small a change this is, I agree.

+1;;;","05/Jun/13 18:07;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
W/O specified columns ASPCSI does not get notified of deletes,CASSANDRA-5614,12651026,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,samt,bcoverston,bcoverston,05/Jun/13 02:29,16/Apr/19 09:32,14/Jul/23 05:53,06/Aug/13 19:17,2.0.1,,,,,,0,,,,,"I'm working on a secondary index implementation based on the composite index type.

AbstractSimplePerColumnSecondaryIndex.java#delete is not called when CQL delete statements do not specify columns.

When I specify columns it is called. Pretty sure this is a bug.

Setup:
{code}
cqlsh> create KEYSPACE foo WITH replication = {'class': 'SimpleStrategy' , 'replication_factor': 1};
cqlsh> use foo;
cqlsh:foo> CREATE TABLE albums (artist text, album text, rating int, release int, PRIMARY KEY (artist, album));
cqlsh:foo> CREATE INDEX ON albums (rating);
{code}

{code}
cqlsh:foo> insert into albums (artist, album, rating, release) VALUES ('artist', 'album', 1, 2);
{code}

Does not get called here:
{code}
cqlsh:foo> DELETE FROM albums where artist='artist' and album='album';
{code}

{code}
cqlsh:foo> insert into albums (artist, album, rating, release) VALUES ('artist', 'album', 1, 2);
{code}

gets called here:
{code}
cqlsh:foo> DELETE rating FROM albums where artist='artist' and album='album';
{code}
",,bcoverston,jjordan,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/13 18:42;samt;0001-CASSANDRA-5614-PreCompactedRow-updates-2i-correctly.patch;https://issues.apache.org/jira/secure/attachment/12593757/0001-CASSANDRA-5614-PreCompactedRow-updates-2i-correctly.patch","23/Jul/13 18:42;samt;0002-CASSANDRA-5614-LazilyCompactedRow-outputs-SSTables-a.patch;https://issues.apache.org/jira/secure/attachment/12593758/0002-CASSANDRA-5614-LazilyCompactedRow-outputs-SSTables-a.patch","23/Jul/13 18:42;samt;0003-CASSANDRA-5614-Memtable-updates-with-RowTombstone-up.patch;https://issues.apache.org/jira/secure/attachment/12593759/0003-CASSANDRA-5614-Memtable-updates-with-RowTombstone-up.patch","24/Jul/13 08:11;samt;0004-CASSANDRA-5614-Consider-timestamps-when-checking-col.patch;https://issues.apache.org/jira/secure/attachment/12593884/0004-CASSANDRA-5614-Consider-timestamps-when-checking-col.patch",,,,,,,,,,,,,,,,,4.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,331352,,,Tue Aug 06 19:17:45 UTC 2013,,,,,,,,,,"0|i1l5xb:",331685,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"05/Jun/13 14:19;jbellis;We probably need to ""expand"" row deletes into column tombstones for the benefit of the index.;;;","05/Jun/13 15:53;bcoverston;Instead of expanding them, at least for the purposes of what I'm doing, I would be OK with just getting keybytes, and column if column had a special marker for a row tombstone. That way I know the whole row was deleted up front (and I can take one action) rather than getting a notification per column.;;;","05/Jun/13 17:42;jbellis;(Oops, wrong Sam.);;;","12/Jul/13 14:19;samt;I'm tempted to say that this is not a problem because it works as expected with both KeysIndex & CompositesIndex. Yes, the presence of range rather than individual column tombstones in the DeletionInfo means that the index isn't updated directly at update time, but the index repair-on-read compensates for that at query time. 

We could take the deletion info from the update & use that in some new deleteAll method on SIM.Updater which iterates through the indexed columns issuing deletes, but AFAICT that would require reads to aquire the existing indexed values.;;;","12/Jul/13 21:44;jbellis;I don't see how this is different from ""pretend the user had deleted the indexed columns by name"" as far as read-before-write goes.
;;;","12/Jul/13 22:56;samt;To delete from the 2i cf you need the old column value to derive the 2i cf key. When a column is deleted by name and that column is present in the memtable, we have the value and so can do that. When the named column isn't in the memtable, then we don't have the old value so we skip the delete and treat the operation as an insert. In SIM.StandardUpdater this is a no-op when isMarkedForDelete() == true, so even when the column is named if it's not in the memtable ASPCSI.delete is never called.;;;","16/Jul/13 22:07;jbellis;bq. When a column is deleted by name and that column is present in the memtable, we have the value and so can do that. When the named column isn't in the memtable, then we don't have the old value

Right, but we know the old value is in the sstable and will be cleaned out on compaction.

So I still don't see what would break here if we ""expand"" the tombstone to the indexed columns -- the same logic applies, we either get it from the memtable or at compaction time.

(I *think* we apply row tombstones to indexed data correctly in compaction already -- if so, all we need to do is handle the memtable case, and not actually create extra tombstones.);;;","23/Jul/13 18:43;samt;First, we aren't doing the right thing in compaction either. Neither PCR or LCR clean up the index in the face of range tombstones, relying on repairs at read time to purge obsolete entries.
Looking into this, it also seems that PCR & LCR actually generate different SSTables. Although they're functionally equivalent, those that come from PCR don't have any columns covered by a RangeTombstone. SSTables generated by LCR do contain these redundant extra columns.

eg: if we have two SSTables:
{code}
[{""key"": ""6e697276616e61"",""columns"": 
	[
		[""bleach:"","""",1374246211348000], 
		[""bleach:rating"",""4"",1374246211348000], 
		[""bleach:release"",""1"",1374246211348000], 
		[""nevermind:"","""",1374246211365000], 
		[""nevermind:rating"",""5"",1374246211365000], 
		[""nevermind:release"",""2"",1374246211365000]
	]
}]

[{""key"": ""6e697276616e61"",""columns"": 
	[
		[""nevermind"",""nevermind:!"",1374246212053000,""t"",1374246212]
	]
}]
{code}

When compacted with PCR we end up with in :
{code}
[{
	""key"": ""6e697276616e61"",
	""columns"": [
		[""bleach:"","""",1374246658577000], 
		[""bleach:rating"",""4"",1374246658577000], 
		[""bleach:release"",""1"",1374246658577000], 
		[""nevermind"",""nevermind:!"",1374246659274000,""t"",1374246659]
	]
}]
{code}

But with LCR we get:
{code}
[{
	""key"": ""6e697276616e61"",
	""columns"": [
		[""bleach:"","""",1374246211348000], 
		[""bleach:rating"",""4"",1374246211348000], 
		[""bleach:release"",""1"",1374246211348000], 
		[""nevermind"",""nevermind:!"",1374246212053000,""t"",1374246212], 
		[""nevermind:"","""",1374246211365000], 
		[""nevermind:rating"",""5"",1374246211365000], 
		[""nevermind:release"",""2"",1374246211365000]
	]
}]
{code}

The first patch fixes PCR to clean up indexes properly.  
The second fixes LCR so that it generates SSTables like PCR & fixes its index cleanup. 
The third adds the index cleanup for memtable updates with RT.;;;","24/Jul/13 02:52;jbellis;Thanks! Do you have a github branch for this?;;;","24/Jul/13 08:11;samt;I realised I'd missed the case where the timestamp on a column is greater than the RT's max, so attaching a fourth patch which handles that. 
My github branch is https://github.com/beobal/cassandra/tree/5614
;;;","06/Aug/13 19:17;jbellis;Rebased and committed to 2.0.1.  The most significant change was to include columns in the same mutation as the tombstone in the loop here:

{code}
                for (Column currentColumn : Iterables.concat(current.map.values(), cm))
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE when upgrading a mixed version 1.1/1.2 cluster fully to 1.2,CASSANDRA-5612,12650803,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,enigmacurry,enigmacurry,04/Jun/13 02:55,16/Apr/19 09:32,14/Jul/23 05:53,16/Aug/13 02:09,,,,,,,0,qa-resolved,,,,"See the attached upgrade_through_versions_test.py upgrade_test_mixed().

Conceptually this method does the following:

* Instantiates a 3 node 1.1.9 cluster
* Writes some data
* Shuts down node 1 and upgrades it to 1.2 (HEAD)
* Brings the node1 back up, making the cluster a mixed version 1.1/1.2
* Brings down node2 and node3 and does the same upgrade making it all the same version.
* At this point, I would run upgradesstables on each of the nodes, but there is already an error on node3 directly after it's upgrade:

{code}
INFO [FlushWriter:1] 2013-06-03 22:49:46,543 Memtable.java (line 461) Writing Memtable-peers@1023263314(237/237 serialized/live bytes, 14 op
s)
 INFO [FlushWriter:1] 2013-06-03 22:49:46,556 Memtable.java (line 495) Completed flushing /tmp/dtest-YqMtHN/test/node3/data/system/peers/syst
em-peers-ic-2-Data.db (291 bytes) for commitlog position ReplayPosition(segmentId=1370314185862, position=58616)
 INFO [GossipStage:1] 2013-06-03 22:49:46,568 StorageService.java (line 1330) Node /127.0.0.2 state jump to normal
ERROR [MigrationStage:1] 2013-06-03 22:49:46,655 CassandraDaemon.java (line 192) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.db.DefsTable.addColumnFamily(DefsTable.java:511)
        at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:445)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:355)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:55)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{code}

This error is repeatable, but inconsistent. Interestingly, it is always node3 with the error.",,aleksey,christianmovi,enigmacurry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5811,,,,,,,,,,,"04/Jun/13 02:56;enigmacurry;logs.tar.gz;https://issues.apache.org/jira/secure/attachment/12586032/logs.tar.gz","04/Jun/13 02:56;enigmacurry;upgrade_through_versions_test.py;https://issues.apache.org/jira/secure/attachment/12586033/upgrade_through_versions_test.py",,,,,,,,,,,,,,,,,,,2.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,331130,,,Fri Aug 16 02:09:11 UTC 2013,,,,,,,,,,"0|i1l4jz:",331463,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,enigmacurry,,,"15/Aug/13 21:46;aleksey;So, what's happening:

1. node2 comes up, slightly earlier than node3
2. node2 calls Auth.setupAuthKeyspace(); but node3 is not alive yet, so this change is *not* pushed to node3
3. node3 comes up and gets noticed by node2
4. node2 calls Auth.setupUsersTable(); the new CF is pushed to all the nodes, including node3. However, node3 does *not* have the keyspace itself, so NPE is thrown at https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/DefsTable.java#L514 - because ksm is null;;;","15/Aug/13 21:51;aleksey;This is entirely harmless - node3's own auth setup code will recreate both the keyspace and the table in the same second or so.
 
This is not even auth-specific. Can happen with any new ks/cf if some node becomes visible between the CREATE KEYSPACE and CREATE TABLE calls. And it's also harmless - the node would still get the changes when a different schema version is detected via gossip.;;;","15/Aug/13 22:06;aleksey;The general solution would be to push the seriazlied keyspace RM when announcing create/alter table migration. Or to do it just for auth, to make the test not cause that error in the logs.;;;","16/Aug/13 02:09;aleksey;Actually, even that wouldn't help with the dtest, since it would be replaced with another error - AE on the 1.1 side from the 1.2 nodes querying the default superuser presence.

Committed ea712bbbe8bb66ceb439dad0ddbb10da18a19c22 to simply skip setting up auth keyspace/users table when AllowAll is used.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ORDER BY desc breaks cqlsh COPY,CASSANDRA-5610,12650728,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,jjordan,jjordan,03/Jun/13 20:40,16/Apr/19 09:32,14/Jul/23 05:53,03/Jun/13 22:00,1.2.6,,,Legacy/Tools,,,0,cqlsh,,,,"If you have a reversed text field, COPY chokes on it because the type is
""'org.apache.cassandra.db.marshal.ReversedType'<text>"" not just 'text' so the strings don't get quoted in the generated CQL.

{noformat}
    def do_import_row(self, columns, nullval, layout, row):
        rowmap = {}
        for name, value in zip(columns, row):
            if value != nullval:
                type = layout.get_column(name).cqltype.cql_parameterized_type()
                if type in ('ascii', 'text', 'timestamp', 'inet'):
                    rowmap[name] = self.cql_protect_value(value)
                else:
                    rowmap[name] = value
            else:
                rowmap[name] = 'null'
        return self.do_import_insert(layout, rowmap)
{noformat}",,aleksey,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jun/13 21:54;aleksey;5610.txt;https://issues.apache.org/jira/secure/attachment/12585963/5610.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,331055,,,Mon Jun 03 22:00:38 UTC 2013,,,,,,,,,,"0|i1l43b:",331388,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"03/Jun/13 21:55;brandon.williams;+1;;;","03/Jun/13 22:00;aleksey;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Crash caused by insufficient disk space to flush,CASSANDRA-5605,12650385,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,dhendry,dhendry,31/May/13 19:08,16/Apr/19 09:32,14/Jul/23 05:53,18/Sep/13 17:45,1.2.10,2.0.1,,,,,2,,,,,"A few times now I have seen our Cassandra nodes crash by running themselves out of memory. It starts with the following exception:

{noformat}
ERROR [FlushWriter:13000] 2013-05-31 11:32:02,350 CassandraDaemon.java (line 164) Exception in thread Thread[FlushWriter:13000,5,main]
java.lang.RuntimeException: Insufficient disk space to write 8042730 bytes
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:42)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
{noformat} 

After which, it seems the MemtablePostFlusher stage gets stuck and no further memtables get flushed: 

{noformat} 
INFO [ScheduledTasks:1] 2013-05-31 11:59:12,467 StatusLogger.java (line 68) MemtablePostFlusher               1        32         0
INFO [ScheduledTasks:1] 2013-05-31 11:59:12,469 StatusLogger.java (line 73) CompactionManager                 1         2
{noformat} 

What makes this ridiculous is that, at the time, the data directory on this node had 981GB free disk space (as reported by du). We primarily use STCS and at the time the aforementioned exception occurred, at least one compaction task was executing which could have easily involved 981GB (or more) worth of input SSTables. Correct me if I am wrong but but Cassandra counts data currently being compacted against available disk space. In our case, this is a significant overestimation of the space required by compaction since a large portion of the data being compacted has expired or is an overwrite.

More to the point though, Cassandra should not crash because its out of disk space unless its really actually out of disk space (ie, dont consider 'phantom' compaction disk usage when flushing). I have seen one of our nodes die in this way before our alerts for disk space even went off.","java version ""1.7.0_15""",agundabattula,cburroughs,colinkuo,daubman,dhendry,jeromatron,jjordan,rcoli,thorkild,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,330712,,,Wed Sep 18 17:45:39 UTC 2013,,,,,,,,,,"0|i1l1zj:",331046,1.2.4,1.2.6,1.2.8,,,,,,yukim,,yukim,Low,,1.2.0 beta 1,,,,,,,,,,,,,,,,"31/May/13 19:27;brandon.williams;Do you have multiple data directories?  If you only have one, it may have been blacklisted and marked read-only by a previous issue, can you check the logs for anything like that?;;;","31/May/13 19:53;dhendry;We have only one data directory. There is nothing in the log about it being blacklisted.;;;","11/Jul/13 03:56;agundabattula;Am not sure if the following information helps but we too hit this issue in production today. We were running with cassandra 1.2.4 and two patches CASSANDRA-5554 & CASSANDRA-5418. 

We were running with RF=3 and LCS. 

We ran into this issue while using sstablelaoder to push data from  remote 1.2.4 cluster nodes to another cluster

We cross checked using JMX if blacklisting is the cause of this bug and it looks like it is definitely not the case. 

We however saw a pile up of pending compactions ~ 1800 pending compactions per node when node crashed. Surprising thing is that the ""Insufficient disk space to write xxxx bytes"" appears much before the node crashes. For us it started appearing aprrox 3 hours before the node crashed. 

The cluster which showed this behavior was having loads of writes occurring ( We were using multiple SSTableLoaders to stream data into this cluster. ). We pushed in almost 15 TB worth data ( including the RF =3 ) in a matter of 16 hours. We were not serving any reads from this cluster as we were still migrating data to it. 

Another interesting behavior observed that nodes were neighbors in most of the time. 

Am not sure if the above information helps but wanted to add it to the context of the ticket.  ;;;","07/Aug/13 15:27;daubman;Apologies if this isn't directly relevant, but I seem to be experiencing the same issue using 1.2.8 launched via ccm for integration testing. One differentiating feature here is that this happened the first time the node was ever brought up (0 data). The integration tests attempting to use the ccm cluster never completed due to this hang, but all they do is test creation of a fairly simple schema and then attempt to write and then read back a single row. There is plenty of free disk space available...

Here's what was in the log:
 INFO [main] 2013-08-07 14:56:46,763 CassandraDaemon.java (line 118) Logging initialized
 INFO [main] 2013-08-07 14:56:46,807 CassandraDaemon.java (line 145) JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.7.0_25
 INFO [main] 2013-08-07 14:56:46,808 CassandraDaemon.java (line 183) Heap size: 8248098816/8248098816
 INFO [main] 2013-08-07 14:56:46,808 CassandraDaemon.java (line 184) Classpath: /opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/conf:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/build/classes/main:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/build/classes/thrift:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/antlr-3.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/avro-1.4.0-fixes.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/avro-1.4.0-sources-fixes.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-cli-1.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-codec-1.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/commons-lang-2.6.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/compress-lzf-0.8.4.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/concurrentlinkedhashmap-lru-1.3.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/guava-13.0.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/high-scale-lib-1.1.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jackson-core-asl-1.9.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jackson-mapper-asl-1.9.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jamm-0.2.5.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jbcrypt-0.3m.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jline-1.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/json-simple-1.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/libthrift-0.7.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/log4j-1.2.16.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/lz4-1.1.0.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/metrics-core-2.0.3.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/netty-3.5.9.Final.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/servlet-api-2.5-20081211.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/slf4j-api-1.7.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/slf4j-log4j12-1.7.2.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/snakeyaml-1.6.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/snappy-java-1.0.5.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/snaptree-0.1.jar:/opt/ccmlib_cassandra/apache-cassandra-1.2.8-src/lib/jamm-0.2.5.jar
 INFO [main] 2013-08-07 14:56:46,822 CLibrary.java (line 65) JNA not found. Native methods will be disabled.
 INFO [main] 2013-08-07 14:56:46,891 DatabaseDescriptor.java (line 132) Loading settings from file:/opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/conf/cassandra.yaml
 INFO [main] 2013-08-07 14:56:47,821 DatabaseDescriptor.java (line 150) Data files directories: [/opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/data]
 INFO [main] 2013-08-07 14:56:47,822 DatabaseDescriptor.java (line 151) Commit log directory: /opt/ccmlib_cassandra/ccm/lwcdbng_test_cluster/node1/commitlogs
 INFO [main] 2013-08-07 14:56:47,822 DatabaseDescriptor.java (line 191) DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO [main] 2013-08-07 14:56:47,822 DatabaseDescriptor.java (line 205) disk_failure_policy is stop
 INFO [main] 2013-08-07 14:56:48,000 DatabaseDescriptor.java (line 273) Global memtable threshold is enabled at 2622MB
 INFO [main] 2013-08-07 14:56:49,142 DatabaseDescriptor.java (line 401) Not using multi-threaded compaction
 INFO [main] 2013-08-07 14:56:52,610 CacheService.java (line 111) Initializing key cache with capacity of 100 MBs.
 INFO [main] 2013-08-07 14:56:52,675 CacheService.java (line 140) Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO [main] 2013-08-07 14:56:52,678 CacheService.java (line 154) Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
 INFO [main] 2013-08-07 14:56:52,698 CacheService.java (line 166) Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO [main] 2013-08-07 14:56:57,163 DatabaseDescriptor.java (line 535) Couldn't detect any schema definitions in local storage.
 INFO [main] 2013-08-07 14:56:57,164 DatabaseDescriptor.java (line 540) To create keyspaces and column families, see 'help create' in cqlsh.
 INFO [main] 2013-08-07 14:56:57,258 CommitLog.java (line 120) No commitlog files found; skipping replay
 INFO [main] 2013-08-07 14:56:57,715 StorageService.java (line 456) Cassandra version: 1.2.8-SNAPSHOT
 INFO [main] 2013-08-07 14:56:57,715 StorageService.java (line 457) Thrift API version: 19.36.0
 INFO [main] 2013-08-07 14:56:57,716 StorageService.java (line 458) CQL supported versions: 2.0.0,3.0.5 (default: 3.0.5)
 INFO [main] 2013-08-07 14:56:57,795 StorageService.java (line 483) Loading persisted ring state
 INFO [main] 2013-08-07 14:56:57,805 StorageService.java (line 564) Starting up server gossip
 WARN [main] 2013-08-07 14:56:57,823 SystemTable.java (line 573) No host ID found, created 6abf69c6-8472-4607-96b7-4a532ed8e2a8 (Note: This should happen exactly once per node).
 INFO [main] 2013-08-07 14:56:57,852 ColumnFamilyStore.java (line 630) Enqueuing flush of Memtable-local@2038679449(367/367 serialized/live bytes, 15 ops)
ERROR [FlushWriter:1] 2013-08-07 14:56:57,860 CassandraDaemon.java (line 192) Exception in thread Thread[FlushWriter:1,5,main]
java.lang.RuntimeException: Insufficient disk space to write 452 bytes
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:42)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:724)
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,243 GCInspector.java (line 119) GC for ParNew: 2786 ms for 1 collections, 230275096 used; max is 8248098816
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,250 StatusLogger.java (line 53) Pool Name                    Active   Pending   Blocked
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,291 StatusLogger.java (line 68) MemtablePostFlusher               1         1         0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,292 StatusLogger.java (line 68) FlushWriter                       0         0         0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,293 StatusLogger.java (line 68) commitlog_archiver                0         0         0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,294 StatusLogger.java (line 73) CompactionManager                 0         0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,522 StatusLogger.java (line 85) MessagingService                n/a       0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,523 StatusLogger.java (line 95) Cache Type                     Size                 Capacity               KeysToSave                                                         Provider
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,524 StatusLogger.java (line 96) KeyCache                          0                104857600                      all                                                                 
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,525 StatusLogger.java (line 102) RowCache                          0                        0                      all              org.apache.cassandra.cache.SerializingCacheProvider
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,525 StatusLogger.java (line 109) ColumnFamily                Memtable ops,data
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,526 StatusLogger.java (line 112) system.local                              0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,526 StatusLogger.java (line 112) system.peers                              0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,527 StatusLogger.java (line 112) system.batchlog                           0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,527 StatusLogger.java (line 112) system.NodeIdInfo                         0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,528 StatusLogger.java (line 112) system.LocationInfo                       0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,528 StatusLogger.java (line 112) system.Schema                             0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,529 StatusLogger.java (line 112) system.Migrations                         0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,529 StatusLogger.java (line 112) system.schema_keyspaces                 8,251
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,530 StatusLogger.java (line 112) system.schema_columns               398,24717
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,530 StatusLogger.java (line 112) system.schema_columnfamilies           369,22187
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,530 StatusLogger.java (line 112) system.IndexInfo                          0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,531 StatusLogger.java (line 112) system.range_xfers                        0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,531 StatusLogger.java (line 112) system.peer_events                        0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,532 StatusLogger.java (line 112) system.hints                              0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,532 StatusLogger.java (line 112) system.HintsColumnFamily                  0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,532 StatusLogger.java (line 112) system_traces.sessions                    0,0
 INFO [ScheduledTasks:1] 2013-08-07 15:14:08,533 StatusLogger.java (line 112) system_traces.events                      0,0
;;;","16/Sep/13 14:46;jjordan;After CASSANDRA-4292 flushing checks the space reserved by compactions.  So check if you have a bunch of pending compactions with a huge size.;;;","16/Sep/13 16:33;jjordan;Have seen a lot of instances of people hitting this reported recently.  I think we probably shouldn't fail stuff if C* ""thinks"" it might not have enough free space, because stuff is reserved.  Probably a good idea to use that reserving information as a hint to which drive to pick for the JBOD case, but if no drive will fit the data when looking at reserved, pick the one with the most free space (like we used to).;;;","17/Sep/13 18:55;jbellis;Patchset to avoid prematurely declaring ourselves out of space at https://github.com/jbellis/cassandra/commits/5605;;;","18/Sep/13 15:05;yukim;+1 to the patch.

Though (maybe in separate ticket?) we still need to add error handler to FlushRunnable so postExecutor does not get blocked.;;;","18/Sep/13 15:17;jbellis;bq. we still need to add error handler to FlushRunnable so postExecutor does not get blocked

I'm not sure we want to unblock it -- if the flush errors out, then we definitely don't want commitlog segments getting cleaned up.  What did you have in mind?;;;","18/Sep/13 15:32;yukim;Ah, right.
Just wondered if we can do something about preventing filling up postExecutor queue.;;;","18/Sep/13 17:45;jbellis;Committed.  If you come up with a good idea there, let's open a new ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
YamlFileNetworkTopologySnitch doesn't call reconnectViaPreferredAddress in onChange,CASSANDRA-5603,12650198,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,jjordan,jjordan,30/May/13 19:32,16/Apr/19 09:32,14/Jul/23 05:53,31/May/13 15:32,2.0 beta 1,,,,,,0,,,,,"I was looking into how EC2 Multi Region did its magic with the private/public ips for the local network.  YamlFileNetworkTopologySnitch looks like it tries to do the same thing, but it doesn't do the reconnectViaPreferredAddress in onChange.  Don't know if it is technically needed, but EC2MRS reconnects in onJoin, onChange, and onAlive.  While YamlFileNetworkTopologySnitch only does it in onJoin and onAlive.",,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,330525,,,Fri May 31 15:32:31 UTC 2013,,,,,,,,,,"0|i1l0tz:",330859,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"31/May/13 15:32;brandon.williams;I can't think of a scenario where this is actually needed, so I think it's EC2MRS that's being gratuitous, but I went ahead and added it to onChange since the cost is nothing if I'm right, and the behavior is correct if I'm wrong.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SnitchProperties uses GossipingPropertyFileSnitch log factory,CASSANDRA-5602,12650161,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,30/May/13 15:26,16/Apr/19 09:32,14/Jul/23 05:53,02/Jun/13 04:00,1.2.6,,,,,,0,,,,,"{noformat} 
 $ git diff
diff --git a/src/java/org/apache/cassandra/locator/SnitchProperties.java b/src/java/org/apache/cassandra/locator/SnitchProperties.java
index ae27fbe..809a180 100644
--- a/src/java/org/apache/cassandra/locator/SnitchProperties.java
+++ b/src/java/org/apache/cassandra/locator/SnitchProperties.java
@@ -26,7 +26,7 @@ import org.slf4j.LoggerFactory;
 
 public class SnitchProperties
 {
-    private static final Logger logger = LoggerFactory.getLogger(GossipingPropertyFileSnitch.class);
+    private static final Logger logger = LoggerFactory.getLogger(SnitchProperties.class);
     public static final String RACKDC_PROPERTY_FILENAME = ""cassandra-rackdc.properties"";
     private static Properties properties = new Properties();
{noformat}  
",,cburroughs,dbrosius,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,cburroughs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,330488,,,Sun Jun 02 04:00:53 UTC 2013,,,,,,,,,,"0|i1l0lr:",330822,,,,,,,,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,,"02/Jun/13 04:00;dbrosius;committed as 456e91d182bf8d2cbcb82746cfff6c62af6f0150 to cassandra-1.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix intersection-checking in CompositeType,CASSANDRA-5600,12649995,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,29/May/13 20:09,16/Apr/19 09:32,14/Jul/23 05:53,31/May/13 14:26,,,,,,,0,,,,,"CASSANDRA-5514 introduced the ability to skip entire sstables based on max/min column names in the sstable. This ticket aims to fix a few issues with it:

* dont use ACT.deconstruct on the hot path
* remove assert that compares type count with collected columns",,christianmovi,marcuse,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/May/13 08:24;marcuse;0001-CASSANDRA-5600-v1.patch;https://issues.apache.org/jira/secure/attachment/12585380/0001-CASSANDRA-5600-v1.patch",,,,,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,330322,,,Fri May 31 14:26:49 UTC 2013,,,,,,,,,,"0|i1kzl3:",330656,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"30/May/13 20:16;slebresne;The changes look good. However, the methods in ColumnNameHelper basically assume that all column names (for a given CF) will have the same number of components. But that's not true (it's not true for CQL3 because of collections, but more importantly, since CompositeType doesn't force all components to be set, thrift users may have existing CF where the column names differ widely in number of components). So I believe we should have something along the lines of
{noformat}
List<ByteBuffer> components = Arrays.asList(ct.split(candidate));
int minSize = Math.min(maxSeen.size(), components.size());
for (int i = 0; i < minSize; i++)
    retList.add(ColumnNameHelper.max(maxSeen.get(i), components.get(i), ct.types.get(i)));

List<ByteBuffer> biggest = maxSeen().size() > components().size() ? maxSeen : components;
for (int i = minSize; i < biggest.size(); i++)
    retList.add(bigget.get(i));
{noformat}

Nit: In CompositeType.intersects, could be worth asserting is that minColumnNames.size() == maxColumnNames.size() (since the method does assume it). It's reasonable in that case since both are guaranteed to be the size of the longest column name in the sstable.
;;;","31/May/13 14:26;marcuse;thanks, pushed as 1ea5059fe3976ce8f660b46520859c31bb433fda with the comments fixed, and the addition to only at most collect typecount columns (typecount is the number CompositeType.types.size() or -1 if it is a collection);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Intermittently, CQL SELECT  with WHERE on secondary indexed field value returns null when there are rows",CASSANDRA-5599,12649952,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,rb732u,rb732u,29/May/13 16:17,16/Apr/19 09:32,14/Jul/23 05:53,20/Nov/13 03:11,,,,Feature/2i Index,,,0,,,,,"Intermittently, CQL SELECT  with WHERE on secondary indexed field value returns null when there are rows.

As it happens intermittently, it is difficult to replicate. To resolve we have had to recreate the index. Also using the nodetool to reindex did not help us either.

We would create a table, create a secondary index for that table on a field, import data then when we try to select rows from that table with where on said field which should return results, we get a null back ...intermittently.  Sometimes works, sometimes not.








",x86_64 / RedHat Enterprise Linux 4.x,rb732u,srrepaka,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,330279,,,Wed Nov 20 03:11:27 UTC 2013,,,,,,,,,,"0|i1kzbj:",330613,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"06/Jun/13 09:00;michalm;To make sure: does it work after recreating index or the problem still happens sometimes?
If it's still failing sometimes:
* Did you try running repair?
* What consistency level do you use for writes and reads?
* How many nodes do you have? 
* Does it happen when you SELECT with CL.CL.ALL?;;;","05/Jul/13 17:56;srrepaka;We have 2 dc and 2 nodes/dc. Heres the keyspace configuration:

CREATE KEYSPACE grd WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'HYWRCA02': '2',
  'CHRLNCUN': '2'
};

All these queries are executed on node1 on dc HYWRCA02. As you can see we only see the results when consistency level is set to ""ALL"". 

After repair on node1 (dc: HYWRCA02), its the same result.
After repair on node2 (dc: HYWRCA02), its the same result.
After repair on node1 (dc: CHRLNCUN), its the same result.	
After repair on node2 (dc: CHRLNCUN), its the same result.

Let me know if you need any more info.

cqlsh> consistency one;
Consistency level set to ONE.
cqlsh> select count(*) from grd.route where serviceidentifier='com.att.scld.GRMServerTestService'
   ... ;

 count
-------
     0

cqlsh> consistency local_quorum;
Consistency level set to LOCAL_QUORUM.
cqlsh> select count(*) from grd.route where serviceidentifier='com.att.scld.GRMServerTestService';

 count
-------
     0
	 
cqlsh> consistency all;
Consistency level set to ALL.
cqlsh> select count(*) from grd.route where serviceidentifier='com.att.scld.GRMServerTestService' ;

 count
-------
   158;;;","05/Jul/13 18:13;michalm;I had similar problem - repairing nodes and rebuilding index didn't help too (and I couldn't drop & recreate  it due to some other problems). I had to use JMX to compact index CF.
So I can confirm that such problem exists, however I didn't manage to reproduce it.;;;","05/Jul/13 18:22;srrepaka;I could not reproduce it as well. But my guess its occurring when we keep deleting the same content and recreate it over a period of time.;;;","20/Nov/13 03:11;jbellis;bq. my guess its occurring when we keep deleting the same content and recreate it over a period of time

Suspect this has been addressed by one of the index tickets recently (CASSANDRA-5614, CASSANDRA-5732).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ArrayIndexOutOfBoundsException in LeveledManifest,CASSANDRA-5589,12649106,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jeromatron,jeromatron,23/May/13 12:36,16/Apr/19 09:32,14/Jul/23 05:53,23/May/13 19:51,1.2.6,,,,,,0,compaction,,,,"The following stack trace was in the system.log:

{quote}
ERROR [CompactionExecutor:2] 2013-05-22 16:19:32,402 CassandraDaemon.java (line 174) Exception in thread Thread[CompactionExecutor:2,1,main]
 java.lang.ArrayIndexOutOfBoundsException: 5
	at org.apache.cassandra.db.compaction.LeveledManifest.skipLevels(LeveledManifest.java:176)
	at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:215)
	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:155)
	at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:410)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:223)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:991)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:230)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.executeInternal(CompactionTask.java:58)
	at org.apache.cassandra.db.compaction.AbstractCompactionTask.execute(AbstractCompactionTask.java:60)
	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:188)
{quote}",,jeffery.griffith,jeromatron,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/May/13 15:33;jbellis;5589.txt;https://issues.apache.org/jira/secure/attachment/12584513/5589.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,329433,,,Thu Sep 25 15:04:46 UTC 2014,,,,,,,,,,"0|i1ku3r:",329768,,,,,,,,,marcuse,,marcuse,Low,,,,,,,,,,,,,,,,,,"23/May/13 15:33;jbellis;The manifest is assuming that when the sstable size is increased, existing sstables are magically resized to that, which is not the case.

Fix attached, with an additional warning to not increase sstable size above 1GB (which is what the user has done here).;;;","23/May/13 16:31;marcuse;lgtm;;;","23/May/13 19:51;jbellis;committed;;;","25/Sep/14 15:04;jeffery.griffith;hi [~jbellis], it looks like this problem may not have been the total # of generations allocated because I can still see this in 1.2.19. The stack trace is the same as Jeremy's however the index out of bounds is 9 (log 10 of 1 billion=9 from your fix.) skipLevels does not check newLevel against generations.length however the fix obviously isn't that simple since it needs to return the newLevel... some other logic problem in the loop termination?

    private int skipLevels(int newLevel, Iterable<SSTableReader> added)
    {
        while (maxBytesForLevel(newLevel) < SSTableReader.getTotalBytes(added)
            && generations[(newLevel + 1)].isEmpty())
        {
            newLevel++;
        }
        return newLevel;
    }

Apparently, this seems to have begun when someone set the max sstable size to 400MB. Going back to 300MB it seems to have gone away.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add get commands to nodetool for things with set,CASSANDRA-5588,12648965,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,michalm,jjordan,jjordan,22/May/13 21:14,16/Apr/19 09:32,14/Jul/23 05:53,28/May/13 18:55,1.2.6,,,Tool/nodetool,,,1,lhf,,,,"Can we add:
nodetool getcompactionthroughput
nodetool getstreamthroughput

To go with the set commands?  You currently have to fire up a JMX client to know what the current values are.
",,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/May/13 08:30;michalm;5588-extended-info-v1.patch;https://issues.apache.org/jira/secure/attachment/12584660/5588-extended-info-v1.patch","24/May/13 08:30;michalm;5588-new-commands-v1.patch;https://issues.apache.org/jira/secure/attachment/12584661/5588-new-commands-v1.patch","25/May/13 08:09;michalm;5588-new-commands-v2.patch;https://issues.apache.org/jira/secure/attachment/12584826/5588-new-commands-v2.patch",,,,,,,,,,,,,,,,,,3.0,michalm,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,329294,,,Tue May 28 18:55:15 UTC 2013,,,,,,,,,,"0|i1kt9r:",329631,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"23/May/13 06:58;michalm;My first thought - do we really need a separate commands for both of them? Maybe we could include it in nodetool info's output, like it's done for cache? ;;;","23/May/13 16:22;brandon.williams;That's not a bad idea, but we actually already have getcompactionthroughput, so simply adding getstreamthroughput would be most consistent.;;;","23/May/13 16:33;michalm;Yes, but please note that getcompactionthreshold (I guess you meant it) is called with two additional params (KS and CF), so it wouldn't make sense to mix with node-wide ""info"" command. 
Anyway, I'm fine with both options, so I'll do it as stated in description if you prefer so :-);;;","23/May/13 16:37;jjordan;We don't have getcompactionthroughput, we have getcompactionthreshold.

We do have a call to getCompactionThroughput from the time estimation code, but the getcompactionthroughput command is not exposed to the end user.

Putting them in info works for me, we already have the global cache stuff there. ;;;","24/May/13 08:29;michalm;It wasn't too much work to do both, so I'm attaching two patches - pick whatever you like ;-)

Both patches do one more change for consistency's sake - move getCompactionThreshold() from NodeProbe to NodeCmd and rename it to printCompactionThreshold(), so almost all the printing (does not apply to RepairRunner) is done in NodeCmd.;;;","24/May/13 19:29;brandon.williams;Can you rebase new-commands-v1 against the 1.2 branch?  I think I like that one better at this point, since anytime someone sees a set<something> they're going to be expecting a get<something> and it more closely matches the underlying JMX model.;;;","25/May/13 08:09;michalm;Sure :-) Attaching v2 rebased onto 67e57a974f7fe5f7b950fa370b665ba828f6e5e1.;;;","28/May/13 18:55;brandon.williams;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkLoader fails with NoSuchElementException,CASSANDRA-5587,12648818,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,julien.ayme@gmail.com,julien.ayme@gmail.com,julien.ayme@gmail.com,22/May/13 05:49,16/Apr/19 09:32,14/Jul/23 05:53,02/Jun/13 03:46,1.2.6,,,Legacy/Tools,,,0,,,,,"When using BulkLoader tool (sstableloader command) to transfer data from a cluster to another, 
a java.util.NoSuchElementException is thrown whenever the directory contains a ""snapshot"" sub directory,
and the bulk load fails.

The fix should be quite simple:
Catch any NoSuchElementException thrown in {{SSTableLoader#openSSTables()}}

The directory structure:
{noformat}
user@cassandrasrv01:~$ ls /var/lib/cassandra/data/Keyspace1/CF1/
Keyspace1-CF1-ib-1872-CompressionInfo.db
Keyspace1-CF1-ib-1872-Data.db
Keyspace1-CF1-ib-1872-Filter.db
Keyspace1-CF1-ib-1872-Index.db
Keyspace1-CF1-ib-1872-Statistics.db
Keyspace1-CF1-ib-1872-Summary.db
Keyspace1-CF1-ib-1872-TOC.txt
Keyspace1-CF1-ib-2166-CompressionInfo.db
Keyspace1-CF1-ib-2166-Data.db
Keyspace1-CF1-ib-2166-Filter.db
Keyspace1-CF1-ib-2166-Index.db
Keyspace1-CF1-ib-2166-Statistics.db
Keyspace1-CF1-ib-2166-Summary.db
Keyspace1-CF1-ib-2166-TOC.txt
Keyspace1-CF1-ib-5-CompressionInfo.db
Keyspace1-CF1-ib-5-Data.db
Keyspace1-CF1-ib-5-Filter.db
Keyspace1-CF1-ib-5-Index.db
Keyspace1-CF1-ib-5-Statistics.db
Keyspace1-CF1-ib-5-Summary.db
Keyspace1-CF1-ib-5-TOC.txt
...
snapshots
{noformat}


The stacktrace: 
{noformat}
user@cassandrasrv01:~$ ./cassandra/bin/sstableloader -v --debug -d cassandrabck01 /var/lib/cassandra/data/Keyspace1/CF1/
null
java.util.NoSuchElementException
        at java.util.StringTokenizer.nextToken(StringTokenizer.java:349)
        at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:265)
        at org.apache.cassandra.io.sstable.Component.fromFilename(Component.java:122)
        at org.apache.cassandra.io.sstable.SSTable.tryComponentFromFilename(SSTable.java:194)
        at org.apache.cassandra.io.sstable.SSTableLoader$1.accept(SSTableLoader.java:71)
        at java.io.File.list(File.java:1087)
        at org.apache.cassandra.io.sstable.SSTableLoader.openSSTables(SSTableLoader.java:67)
        at org.apache.cassandra.io.sstable.SSTableLoader.stream(SSTableLoader.java:119)
        at org.apache.cassandra.tools.BulkLoader.main(BulkLoader.java:67)
{noformat}",,dbrosius,julien.ayme@gmail.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,14400,14400,,0%,14400,14400,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/May/13 07:02;julien.ayme@gmail.com;cassandra-1.2-5587.txt;https://issues.apache.org/jira/secure/attachment/12584257/cassandra-1.2-5587.txt",,,,,,,,,,,,,,,,,,,,1.0,julien.ayme@gmail.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,329148,,,Sun Jun 02 03:46:11 UTC 2013,,,,,,,,,,"0|i1ksdz:",329488,,,,,,,,,dbrosius,,dbrosius,Normal,,,,,,,,,,,,,,,,,,"22/May/13 06:16;julien.ayme@gmail.com;The proposed patch, made against branch cassandra-1.2;;;","22/May/13 06:53;dbrosius;the SSTableLoader.openSSTables filenameFilter could immediately ignore directories before even trying SSTable.tryComponentFromFilename. as well.

ie

if (new File(dir, name).isDirectory()) return false;;;;","22/May/13 06:59;julien.ayme@gmail.com;Yes, this is also a valid solution, but there is one use case I could think of which would fail with the same symptom: if a user deliberatly adds other files (like metainfo, ...) in the directory (yes, this is a bad thing to do).

And nothing prevents us from doing both: immediatly ignore directories, and keep the second safe guard catch NSEE.;;;","22/May/13 07:02;julien.ayme@gmail.com;Updated patch: both checks are done;;;","31/May/13 04:04;dbrosius;works as advertised, altho just my nitpick perhaps but not a big fan of catching RuntimeExceptions, i'd rather just prevent them, perhaps Descriptor.fromFilename could just use String.split and early exit if the number of components wasn't found rather than going thru the morass of using an old school StringTokenizer.;;;","02/Jun/13 03:46;dbrosius;Committed as c0c1492666e44612ec4f3ca5b47ffcd68e85e210 to cassandra-1.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Incorrect use of System.nanoTime(),CASSANDRA-5584,12648683,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ash2k,ash2k,ash2k,21/May/13 18:04,16/Apr/19 09:32,14/Jul/23 05:53,21/May/13 19:19,1.2.6,,,,,,0,,,,,"From System.nanoTime() JavaDoc:
{noformat}
For example, to measure how long some code takes to execute:
 long startTime = System.nanoTime();
 // ... the code being measured ...
 long estimatedTime = System.nanoTime() - startTime; 

To compare two nanoTime values
 long t0 = System.nanoTime();
 ...
 long t1 = System.nanoTime();
one should use t1 - t0 < 0, not t1 < t0, because of the possibility of numerical overflow.
{noformat}
I found one place with such incorrect use that can result in overflow and in incorrect timeout handling. See attached patch.",,ash2k,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/13 18:05;ash2k;trunk-5584.txt;https://issues.apache.org/jira/secure/attachment/12584046/trunk-5584.txt",,,,,,,,,,,,,,,,,,,,1.0,ash2k,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,329038,,,Tue May 21 19:19:18 UTC 2013,,,,,,,,,,"0|i1krpz:",329380,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"21/May/13 19:19;jbellis;LGTM; committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
InputStreams not closed,CASSANDRA-5580,12648337,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ash2k,ash2k,ash2k,19/May/13 06:32,16/Apr/19 09:32,14/Jul/23 05:53,20/May/13 22:11,2.0 beta 1,,,Local/Config,,,0,,,,,InputStreams are not closed properly in few places.,,ash2k,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"19/May/13 06:34;ash2k;trunk-5580.txt;https://issues.apache.org/jira/secure/attachment/12583762/trunk-5580.txt",,,,,,,,,,,,,,,,,,,,1.0,ash2k,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,328693,,,Mon May 20 22:11:20 UTC 2013,,,,,,,,,,"0|i1kplr:",329036,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"19/May/13 06:34;ash2k;Patch.;;;","20/May/13 22:11;jbellis;LGTM; committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid unnecessary second pass on name-based queries,CASSANDRA-5577,12648176,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,17/May/13 13:18,16/Apr/19 09:32,14/Jul/23 05:53,17/May/13 14:43,2.0 beta 1,,,,,,0,,,,,,,christianmovi,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/May/13 13:34;jbellis;5577.txt;https://issues.apache.org/jira/secure/attachment/12583651/5577.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,328532,,,Thu Sep 19 22:38:50 UTC 2013,,,,,,,,,,"0|i1kom7:",328876,,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,,"17/May/13 14:28;slebresne;The patch lgtm, though going a bit further I think we can always dispense ourselves of the collateOnDiskAtom call. Because returning gcable tombstones during read should never be a problem since we'll skip any tombstone before returning to the client.

Nit: might be worst adding as a comment than skipping collateOnDiskAtom is only ok because we have a NamesQueryFilter, so we don't inadvertently copy the idea when a slice filter is involved.
;;;","17/May/13 14:43;jbellis;I don't think we have any code in read repair to distinguish b/t gc-able tombstones and otherwise, so I'll leave the collate call in for now.

Committed w/ extra comment.;;;","19/Sep/13 22:38;jbellis;(Turns out this is invalid; see CASSANDRA-6025 for an example.  Reverted in 2.0.1.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CREATE/DROP TRIGGER in CQL,CASSANDRA-5576,12648170,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,jbellis,jbellis,17/May/13 12:24,16/Apr/19 09:32,14/Jul/23 05:53,16/Jun/13 23:00,2.0 beta 1,,,Legacy/CQL,,,0,,,,,,,aleksey,alexliu68,ardot,christianmovi,daubman,jjordan,liqusha,makhr,slebresne,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5578,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,328526,,,Mon Jun 17 02:13:20 UTC 2013,,,,,,,,,,"0|i1kokv:",328870,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"29/May/13 05:29;vijay2win@yahoo.com;Just to clarify is this what we are looking for?

CREATE TRIGGER <trigger_name> ON <table_name> FOR EACH MUTATION <trigger_class>;

or are we expecting something like this?

CREATE TRIGGER <trigger_name> FOR EACH MUTATION <trigger_class>;
ALTER TABLE ADD TRIGGER <trigger_name>;;;;","29/May/13 14:16;jbellis;Whichever is easier. :);;;","29/May/13 16:18;aleksey;I would drop 'FOR EACH MUTATION' for now - until we have other trigger types (if we ever do at all).

Also, we might (and probably will) need to store additional info - or at least add the ability to parametrize triggers. So a column in schema_columnfamilies won't do (even a map, to keep the names).

We should add another system schema cf, something like this:

{noformat}
CREATE TABLE schema_triggers (
  keyspace_name text,
  columnfamily_name text,
  trigger_name text,
  trigger_options map<text, text>,
  PRIMARY KEY (keyspace_name, columnfamily_name, trigger_name)
);
{noformat}

And for consistency with CREATE CUSTOM INDEX and other CREATE's, use the following syntax for CREATE TRIGGER:

{noformat}
CREATE TRIGGER <name> ON <cfname> WITH options = {'class': …, ..}
{noformat}

(Support only 'class' for now).

This makes CASSANDRA-5578 semi-irrelevant, since we are not going to keep triggers info in schema_columnfamilies anymore.;;;","02/Jun/13 22:20;vijay2win@yahoo.com;Alright i have pushed the changes to https://github.com/Vijay2win/cassandra/commits/5576

The trigger schematics is as follows:
{code}
cqlsh>CREATE TRIGGER index_logger ON ""Keyspace1"".""Standard1"" EXECUTE ('org.apache.cassandra.triggers.InvertedIndex', 'org.apache.cassandra.triggers.LogColumnUpdates'); 
cqlsh>DROP TRIGGER index_logger ON ""Keyspace1"".""Standard1"";
{code}

CLI
{code}
update column family Standard1 with triggers='{""test"":[""org.apache.cassandra.triggers.InvertedIndex"",""org.apache.cassandra.triggers.LogColumnUpdates""]}';
{code};;;","03/Jun/13 20:58;aleksey;Was there an issue with any of the suggestions in https://issues.apache.org/jira/browse/CASSANDRA-5576?focusedCommentId=13669381&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13669381?

CQL is part of public API, we *try* not to change it too often, and to keep it consistent.

One issue with
{noformat}
CREATE TRIGGER index_logger ON ""Keyspace1"".""Standard1"" EXECUTE ('org.apache.cassandra.triggers.InvertedIndex', 'org.apache.cassandra.triggers.LogColumnUpdates');
{noformat}
Is that it closes the door to future parametrization of trigger classes (which is not planned for 2.0, but will probably happen eventually). Another is that I just see no value in being able to bundle several trigger classes under one name - what exactly does it buy us?

So, ideally, I'd rather see
{noformat}
CREATE TRIGGER indexer ON ""Keyspace1"".""Standard1"" WITH options = {'class': 'org.apache.cassandra.triggers.InvertedIndex'}
{noformat}

(Use map syntax, even if we are only going to support one option - 'class', for now. You can look at CREATE CUSTOM INDEX implementation to see what exactly I mean).

We also *don't* want to serialize anything as JSON in schema columns anymore (this is what CASSANDRA-5578 was about). The original plan was to use a set<text> field for the triggers, but now that we can attach names to them, and potentially want to be able to parametrize them, neither a set<text> or a map<text, text> will do. So we need a separate schema-table - I listed one example schema in the comment before this one.;;;","04/Jun/13 03:26;vijay2win@yahoo.com;{quote}
Was there an issue with any of the suggestions
{quote}
Didn't we just drop the options (Map<String, String>) style configuration on CF in CASSANDRA-4795? Future extension can do simple match on the input parameter which i think is not that bad...

{quote}
what exactly does it buy us?
{quote}
It allows the user to group things so they can drop and manage a set of triggers than individual ones... I am fine dropping that and having one to one relationship (name to class names).

{quote}
We also don't want to serialize anything as JSON in schema columns anymore
{quote}

Well it was not just simple type specification in table definition the announce method has to change, let me spend more time troubleshooting. Hope everything else is alright.


;;;","04/Jun/13 13:47;aleksey;bq. Didn't we just drop the options (Map<String, String>) style configuration on CF in CASSANDRA-4795?

We didn't. We made the validation stricter, that's all.;;;","06/Jun/13 04:28;jbellis;bq. I just see no value in being able to bundle several trigger classes under one name 

+1

bq. CREATE TRIGGER indexer ON ""Keyspace1"".""Standard1"" WITH options = {'class': 'org.apache.cassandra.triggers.InvertedIndex'}

-0 -- I see the value of ""options"" when we introduce parameterization, but having the actual trigger class be part of that seems odd, since that's something every trigger needs (i.e. a requirement, not an option or parameter).

So I think I'd prefer to have the EXECUTE syntax, with a single target.  We can add {{WITH options}} later.  (But I agree that it's a good idea to split the triggers into a separate system table to make that easier.);;;","06/Jun/13 06:38;aleksey;{quote} 
-0 – I see the value of ""options"" when we introduce parameterization, but having the actual trigger class be part of that seems odd, since that's something every trigger needs (i.e. a requirement, not an option or parameter).

So I think I'd prefer to have the EXECUTE syntax, with a single target. We can add WITH options later. (But I agree that it's a good idea to split the triggers into a separate system table to make that easier.)
{quote}

I would agree with you if there were no precedent. But that's the syntax we already use for CREATE CUSTOM INDEX (CASSANDRA-5484), and it's worth reusing it just for consistency's sake, IMO.;;;","06/Jun/13 13:46;jbellis;I don't buy ""we made a mistake in 5484 so we should keep perpetuating the mistake in other places"" argument.  We should do it right here and update CREATE CUSTOM INDEX syntax for 2.0 to be consistent with this, instead of the other way around.;;;","06/Jun/13 15:52;aleksey;I don't think it was a mistake in case of 5484. Do you have better suggestions?

But, all right, I'm okay with your suggestion for CREATE TRIGGER.

[~slebresne] any ideas re: changing 5484 and syntax for CREATE TRIGGER?;;;","06/Jun/13 16:07;jbellis;bq. I don't think it was a mistake in case of 5484.

Hmm.  I see what you mean wrt compaction, compression configuration.  Damn it.  All right, I guess I'm sold on ""everything in options map."";;;","06/Jun/13 16:17;slebresne;I don't think I care too much which syntax we pick, though I'm definitely in favor of more consistency over less.

It does slightly bug me to change the syntax for custom indexes after it's been release. But, given that nobody is probably using the custom index syntax at this point (it's not really properly documented yet typically), I'm fine with it if you guys think it measurably improve the syntax (I have no real opinion for once on that question). But I'd *really* prefer doing it now rather than leaving the current syntax in 1.2 and changing it in 2.0.  

Though if we do change the custom index syntax, what are we talking about? Something like:
{noformat}
CREATE CUSTOM INDEX ON <table>(<column>) USING <classname>
{noformat}
with a trigger syntax that would be
{noformat}
CREATE TRIGGER <name> ON <table> EXECUTE <classname>
{noformat}
Knowing that for both we could add support for {{WITH options=\{...\} }} whenever we actually have options?

;;;","06/Jun/13 16:28;aleksey;btw, if we do go with USING for CREATE CUSTOM INDEX, a more consistent
{noformat}
CREATE TRIGGER <name> on <table> USING <class> WITH options
{noformat}
also sounds okay to me (and avoids adding an extra keyword, too).

But I'm equally fine with 'everything in options map'. As long as we are consistent, at least betweetn CREATE CUSTOM INDEX and CREATE TRIGGER.;;;","06/Jun/13 18:52;jbellis;If everyone is fine with both options then I move we leave the final decision to Vijay who is coding it up. :);;;","11/Jun/13 05:39;vijay2win@yahoo.com;It took longer than expected to troubleshoot CFMetaData, 
https://github.com/Vijay2win/cassandra/commits/5576-v2 has the following semantics...

{code}
cqlsh> create TRIGGER test2 ON ""Keyspace1"".""Standard1"" using 'org.apache.cassandra.triggers.InvertedIndex';
cqlsh> SELECT * FROM system.schema_triggers WHERE keyspace_name='Keyspace1' AND column_family='Standard1';

 keyspace_name | column_family | trigger_classes
---------------+---------------+------------------------------------------------------
     Keyspace1 |     Standard1 | {test2: org.apache.cassandra.triggers.InvertedIndex}

cqlsh> drop trigger test2 on ""Keyspace1"".""Standard1"";                           
{code}

The trigger data is now stored in schema_triggers and cli is as follows
{code}
update column family Standard1 with triggers= '{""test"":""org.apache.cassandra.triggers.InvertedIndex""}';
{code};;;","12/Jun/13 19:52;aleksey;Yep, that's the CQL3 syntax we want. I'll open a ticket for me to update CREATE CUSTOM INDEX to switch to USING as well.

I'm afraid that schema_triggers schema like this is not enough - won't allow options in the future (and changing this schema is gonna be *a lot* more painful than adding WITH to the CREATE statement).

Can't we use

{noformat}
CREATE TABLE schema_triggers (
  keyspace_name text,
  columnfamily_name text,
  trigger_name text,
  trigger_options map<text, text>,
  PRIMARY KEY (keyspace_name, columnfamily_name, trigger_name)
);
{noformat}
, as it was suggested in https://issues.apache.org/jira/browse/CASSANDRA-5576?focusedCommentId=13669381&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13669381 initially?

Your example would translate to
keyspace_name=Keyspace1, column_family=Standard1, trigger_name=test2, trigger_options={class:org.apache.cassandra.triggers.InvertedIndex}

Will also need to update the cli syntax to something column_metadata-like

{noformat}
update column family Standard1 with triggers = [
{trigger_name: test2, class: org.apache.cassandra.triggers.InvertedIndex}
];
{noformat};;;","14/Jun/13 06:23;vijay2win@yahoo.com;Done with all the above changes and pushed to the same branch. Let me know... Thanks!

{code}
          update column family Standard1 with triggers =
              '[{""name"":""test"",""class"":""org.apache.cassandra.triggers.InvertedIndex""}]';
{code};;;","14/Jun/13 16:02;jbellis;I don't see Aleksey's schema change; am I missing something?;;;","14/Jun/13 16:28;vijay2win@yahoo.com;Yep, We do have it in https://github.com/Vijay2win/cassandra/commit/ee36fe0fe99252f7ca7268863f33a9bc585a4c58#L0R165 i separated into 2 commits but will squash before merging to trunk.;;;","14/Jun/13 16:40;jjordan;[~vijay2win@yahoo.com] I think the change he is refering to is using:

{noformat}
  trigger_options map<text, text>,
{noformat}

instead of

{noformat}
trigger_class text,
{noformat}

So that adding more parameters later doesn't require a schema change.;;;","14/Jun/13 17:05;vijay2win@yahoo.com;[~jjordan] the options can be additional columns which can be defined later (same way we do SCHEMA_COLUMNFAMILIES_CF) when we add them we need to validate them anyways.... 
If they are dynamic then they can be additional columns like list<map<String, map<String, String>> i believe this schema provides a lot of flexibility for now.

I really dont have a lot of opinion on it, it just was more logical with the rest of the code (Specially with querying and dropping etc)... 
if you want to change it, i can change it. Then we just need to special case trigger name from the map etc..;;;","14/Jun/13 23:59;jbellis;bq. the options can be additional columns which can be defined later

I think compaction options is a better example -- valid options are defined by the implementing class, so we just want a Map<option, value> that we can pass to the constructor.;;;","15/Jun/13 04:52;aleksey;Yes, options should definitely be a map<text, text>. Question is, should class be a separate column or should it be simply one of the options in that map.

Cases where class is part of the options and not a separate column:
- custom 2i (schema_columns.index_options)
- compression (schema_columnfamilies.compression_parameters)

Cases where class is a separate column:
- replication (schema_keyspaces.strategy_class)
- compaction (schema_columnfamilies.compaction_strategy_class)

In the last two cases, even though they are stored separately, in CQL3 CREATE/ALTER the class is still conceptually part of the options map:
- CREATE KEYSPACE <name> WITH replication = {'class': 'path.to.strategy.class', 'option1': 'value1', 'optionX': ...}
- ALTER TABLE <name> WITH compaction = {'class': 'SizeTieredCompactionStrategy', 'option1': 'value1', 'optionX': ...}

Now, once we add parameters support to triggers, we could just add trigger_options map<text, text> and to keep all but class there. Adding columns to system tables is okay, dropping them isn't. But adding trigger_options map<text,text> right now and storing the class there (with 'class' key) will let us to leave schema_triggers schema like this forever, future-proofing it (this is what I suggested in https://issues.apache.org/jira/browse/CASSANDRA-5576?focusedCommentId=13681562&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13681562 and in https://issues.apache.org/jira/browse/CASSANDRA-5576?focusedCommentId=13669381&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13669381 before that).;;;","15/Jun/13 05:03;vijay2win@yahoo.com;{quote}
 dropping them isn't.
{quote}
With range tombstone we just delete all the Composite columns with a prefix ""where trigger_name='xyz'""... thats what the patch does :);;;","15/Jun/13 05:08;aleksey;I'm talking about removing columns from schema of system tables themselves, not the values (say, getting rid of schema_columnfamilies.compaction_strategy_class column altogether). Values, sure.;;;","15/Jun/13 05:17;aleksey;Mentioned it to highlight how important getting the schema of system tables right the first time is. Changing it is hard. Removing columns - almost impossible. If we can have one less from the start, I'd much rather prefer it. And we *will* add trigger parametrization in the future - so why not add trigger_options map<text,text> right now and keep the class there, instead of having a separate trigger_class column? Not having to touch schema_triggers schema in the future = win.;;;","15/Jun/13 06:10;vijay2win@yahoo.com;Pushed the schema change to the same branch. Thanks!;;;","15/Jun/13 07:29;aleksey;Thanks! Everything looks good except some 1-line-fixable things and thrift/cli issue:
- DropTriggerStatement.checkAccess() should check for ALTER - we are altering a table here, not dropping it
- DropTriggerStatement.changeType() should return UPDATED, for the same reason
- in Cql.g should probably add K_TRIGGER to unreserved_function_keyword to not break people's queries if they have a column named 'trigger' somewhere. The less reserved keywords we got, the better
- (nit) in CFMetaData, SchemaTriggerCf = compile(5, ...) - there is no need for oldCfId argument here (5), since the table has never existed in 1.1
- (nit) CFMetaData.getTriggerClass() should be renamed to CFMetaData.getTriggerClasses() - it returns a collection, after all

Thrift/cli issue:
{noformat}
43: optional list<map<string, string>> triggers,
{noformat}
Is not future-parametrization proof.

Also, cli syntax is using json, instead of using arrayConstruct from Cli.g (with nested hashConstruct). The former requires json where there shouldn't be, and quotes around key names (and quotes around the whole thing), which it shouldn't.

*HOWEVER* I suggest not fixing it at all and removing triggers entirely from CLI and thrift APIs. CLI is deprecated. And with Thrift you won't be able to add/remove triggers from CQL3 tables, anyway, because you can't even see them. With CQL3 CREATE TRIGGER/DROP TRIGGER, however, you can modify both CQL3 and old-style tables, easily. So let's just rely on them/cqlsh for modifying triggers.

I'm open to debate about thirft/cli here. But if we are going to support trigger modification from cli/thrift, it'll be done right, and I can't justify wasting any more of your time on that.;;;","16/Jun/13 18:24;vijay2win@yahoo.com;Removed cli support for it, but thrift i am not sure there is a lot of automation in testing which relies on it... 

As a user i use thrift for various reasons like performance and i am not sure if i would switch that soon, specially with the cross platforms we have to deal with.

v4 addresses the nits too. https://github.com/Vijay2win/cassandra/commits/5576-v4
;;;","16/Jun/13 18:56;aleksey;bq. As a user i use thrift for various reasons like performance and i am not sure if i would switch that soon, specially with the cross platforms we have to deal with.

You'd only have to use CQL3 for DROP and CREATE TRIGGER (via thfit.execute_cql3_query()) and ignore it otherwise. Speed not affected (unless you are talking about the speed of creating a trigger (: ). Though I can see some value of being able to modify triggers via thrift.system_update_column_family() if one has no CQL3 tables at all and doesn't want to bother with it, ever.

Can you make it map<string, map<string, string>> (index name -> index options, including class) then? So that once we add parametrization support, thrift wouldn't have to change at all?

Other than this, everything looks fine to me.

(tiny nit: FBUtilities.fromJsonListObject() is now unused and can be removed);;;","16/Jun/13 19:17;aleksey;Also, TriggerOptions should be moved to org.apache.cassandra.config (and ideally renamed to just ""Triggers"").

This is all.;;;","16/Jun/13 22:59;vijay2win@yahoo.com;{quote}
Speed not affected
{quote}
I am talking about libraries that are written on thrift which has to special case just for triggers if we dont support, though it is not required per say.

{quote}
renamed to just ""Triggers""
{quote}
Triggers class name will cause confusion and hence left it alone and moved it to config, though I am not sure if thats the right place for it.

Committed to trunk, Glad it is done... and has map<string, map<string, string> now :-), Thanks!;;;","16/Jun/13 23:01;aleksey;Good (:;;;","17/Jun/13 02:13;jbellis;\o/

On Sun, Jun 16, 2013 at 6:03 PM, Aleksey Yeschenko (JIRA)



-- 
Jonathan Ellis
Project Chair, Apache Cassandra
co-founder, http://www.datastax.com
@spyced
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Querying with an empty (impossible) range returns incorrect results,CASSANDRA-5573,12647879,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,mikeschrag,mikeschrag,16/May/13 02:50,16/Apr/19 09:32,14/Jul/23 05:53,12/Jul/13 15:17,1.2.7,2.0 beta 2,,,,,0,,,,,"SELECT * FROM cf WHERE token(key) > 2000 AND token(key) <= 2000 LIMIT 1000 ALLOW FILTERING;

This should return nothing, but instead appears to freak and return arbitrary token values.",,aleksey,alexliu68,mck,mikeschrag,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jul/13 14:18;slebresne;5573.txt;https://issues.apache.org/jira/secure/attachment/12591437/5573.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,328235,,,Fri Jul 12 15:17:50 UTC 2013,,,,,,,,,,"0|i1kms7:",328579,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"16/May/13 20:08;alexliu68; * The semantics of start keys and tokens are slightly different.
 * Keys are start-inclusive; tokens are start-exclusive.  Token
 * ranges may also wrap -- that is, the end token may be less
 * than the start one.  Thus, a range from keyX to keyX is a
 * one-element range, but a range from tokenY to tokenY is the
 * full ring.

So that query covers the whole ring. I think it's a bug in the hadoop code. I will fix it in CASSANDRA-4421;;;","17/May/13 06:17;slebresne;Hum, that's not an hadoop query, that's a CQL3, so this is a valid issue.;;;","17/May/13 06:53;slebresne;Let me maybe justify a bit more why I believe it's a valid issue.

It is true that in thrift, get_range_slide can take a range of tokens and this range is allowed to wrap, so the start token can be >= to the end token.
However, in CQL, the where clause expresses a condition that each returned record needs to fulfill to be returned. This is different. And it is impossible to have a key for which the token is both > 2000 and <= 2000, so such request must return nothing (or return an error thought that would be less convenient in practice).

Now, a related question could be, how do I return in CQL3 all the records whose tokens is in the wrapping (2000, 2000] token range? Well, you can't with just one request currently, you have to unwrap the range manually (and thus potentially do 2 queries since we don't have OR conditions). Is that a problem? I don't think so. If you have a not trivial amount of data, iterating over it all is going to take way more than one query anyway and if you care about performance, you'd better use hadoop for such things. Besides, if you do really want to iterate over the whole ring, just iterate from the min token to the max one and you don't even have to bother about wrapping.;;;","09/Jul/13 14:18;slebresne;Attaching patch for this.

Now, it turns out that the current behavior of CQL when given unsatisfiable conditions is to throw an exception, so it would be somewhat to do it in that case too. However, the attached takes the other direction, of always returning an empty result set in that case because:
# the current behavior is not consistent anyway. If you do one of:
{noformat}
SELECT v FROM test WHERE k=0 AND v > 1 AND v <= 1
SELECT v FROM test WHERE k=0 AND v > 1 AND v < 2
{noformat}
(where {{v}} is a clustering column), then you get an exception, but if you do
{noformat}
SELECT v FROM test WHERE k=0 AND v >= 2 AND v < 1
{noformat}
then you actually get an empty result set.
# I think returning an empty result set is really more user friendly (and I've meant to change that for a while). As far as I know this is also how every SQL DB works.
;;;","12/Jul/13 13:17;aleksey;LGTM, +1

nanonits:
- you can kill both ColumnSlice.validate() overloads (only used by QP.validateSLiceFilter) altogether if you ditch QueryProcessor.validateSliceFilter() (unused).
- SelectStatement.getKeyBounds() comment 'it will happilly return the whole' has a typo in 'happilly'.
;;;","12/Jul/13 15:17;slebresne;Committed with nits addressed. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Write row markers when serializing columnfamilies and columns schema,CASSANDRA-5572,12647866,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,16/May/13 00:20,16/Apr/19 09:32,14/Jul/23 05:53,16/May/13 13:51,1.2.6,,,,,,0,,,,,"ColumnDefinition.toSchema() and CFMetaData.toSchemaNoColumns() currently don't write the row markers, which leads to certain queries not returning the expected results, e.g.

select keyspace_name, columnfamily_name from system.schema_columnfamilies where keyspace_name = 'system' and columnfamily_name = 'hints' -> [].",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/May/13 00:21;aleksey;5572.txt;https://issues.apache.org/jira/secure/attachment/12583410/5572.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,328222,,,Thu May 16 13:51:02 UTC 2013,,,,,,,,,,"0|i1kmpb:",328566,,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,,"16/May/13 08:08;slebresne;lgtm, +1.

Makes me think it might simplify things a bit to use range tombstones in dropFromSchema. And maybe we could start using CQL3 queries (with processInternal) to avoid having to deal with row markers manually? But anyway, we can definitively do that later. ;;;","16/May/13 13:39;aleksey;Yeah, thought about range tombstones here as well yesterday. But, later.;;;","16/May/13 13:51;aleksey;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL version is wrong either in doc or in code,CASSANDRA-5570,12647754,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,jeromatron,jeromatron,15/May/13 14:49,16/Apr/19 09:32,14/Jul/23 05:53,15/May/13 14:56,,,,,,,0,,,,,"The textile doc in 1.2.2+ for cql3 says it's version 3.0.2.  The code says it's 3.0.1.

In 1.2-branch and trunk it says it is 3.0.2 even though it should be 3.0.3 and 3.1.0 respectively.

https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/cql3/QueryProcessor.java#L44
https://github.com/apache/cassandra/blob/cassandra-1.2/doc/cql3/CQL.textile

https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/cql3/QueryProcessor.java#L43
https://github.com/apache/cassandra/blob/trunk/doc/cql3/CQL.textile",,aleksey,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,328110,,,Wed May 15 14:56:39 UTC 2013,,,,,,,,,,"0|i1km0f:",328454,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"15/May/13 14:56;aleksey;ninja-resolved;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Every stream operation requires checking indexes in every SSTable,CASSANDRA-5569,12647639,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,rbranson,rbranson,rbranson,14/May/13 23:46,16/Apr/19 09:32,14/Jul/23 05:53,16/May/13 16:46,1.2.6,,,,,,0,streaming,,,,"It looks like there's a streaming performance issue when leveled compaction and vnodes get together. To get the candidate set of chunks to stream, the streaming system gets references to every SSTable for a CF. This is probably a perfectly reasonable assumption for non-vnode cases, because the data being streamed is likely distributed across the full SSTable set. This is also probably a perfectly reasonable assumption for size-tiered compaction, because the data is, again, likely distributed across the full SSTable set. However, for each vnode repair performed on LCS CF's, this scan across potentially tens of thousands of SSTables is wasteful considering that only a small percentage of them will actually have data for a given range.

This manifested itself as ""hanging"" repair operations with tasks backing up on the MiscStage thread pool.

The attached patch changes the streaming code so that for a given range, only SSTables for the requested range are checked to be included in streaming.",,awinter,christianmovi,rbranson,rcoli,rektide,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/May/13 04:57;rbranson;5569-v2.txt;https://issues.apache.org/jira/secure/attachment/12583280/5569-v2.txt","15/May/13 16:05;rbranson;5569-v3.txt;https://issues.apache.org/jira/secure/attachment/12583333/5569-v3.txt","14/May/13 23:46;rbranson;5569.txt;https://issues.apache.org/jira/secure/attachment/12583242/5569.txt",,,,,,,,,,,,,,,,,,3.0,rbranson,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,327995,,,Thu May 16 16:46:08 UTC 2013,,,,,,,,,,"0|i1klav:",328339,,,,,,,,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,,"15/May/13 00:58;jbellis;This could cause a lot of reference churn, can you make DataTracker.markReferenced take a list of Bounds objects instead so we can uniquify the set before acquiring references?;;;","15/May/13 01:30;rbranson;How do you feel about the safety of refactoring the markReferenced(RowPosition start, RowPosition end) signature call the new markReferenced to cut down on the duplication between these methods?;;;","15/May/13 01:35;jbellis;+1;;;","15/May/13 04:57;rbranson;v2 patch attached.

Decided not to rock the boat a bunch in terms of allocation & copying on the markReferenced() for the single range common case, which it looks like is the path for a bunch of read operations. This patch contains a refactor that DRYs up the markReferenced() calls.;;;","15/May/13 07:17;rbranson;It looks like StreamingRepairTask is also suffering this problem. Going to work up a v3 to include that as well.;;;","15/May/13 15:15;rbranson;http://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/streaming/StreamingRepairTask.java#L133

^^^ is there a good reason we're calling StreamOut.transferSSTables directly instead of StreamOut.transferRanges?;;;","15/May/13 15:28;yukim;I think we can use transferRanges there even though we do blocking flush before streaming.;;;","15/May/13 15:35;rbranson;What I'm working with now is adding a second method signature with a ""flush"" boolean that allows the behavior to be turned off for StreamingRepairTask.;;;","15/May/13 16:05;rbranson;v3 patch (rebased against current 1.2 branch).

Adds the same optimization to StreamingRepairTask. Note that the only apparent difference is that StreamingRepairTask wasn't doing a blocking flush, so I made that behavior optional in StreamOut.transferRanges(). Also took the opportunity to run through and fix some naming irregularities from the v2 patch.;;;","15/May/13 22:14;rbranson;FWIW -- we're running the v3 patch in production on our vnodes + LCS cluster, and repairs are something like 25x faster.;;;","16/May/13 16:46;yukim;+1 and committed.
Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DESCRIBE TABLES is empty after case insensitive use,CASSANDRA-5567,12647581,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,mbulman,mbulman,14/May/13 19:33,16/Apr/19 09:32,14/Jul/23 05:53,15/May/13 13:09,1.2.5,,,Legacy/Tools,,,0,,,,,"Using trunk @ 02547747198c0d14f9a4102a920b914bcfd57a23

{noformat}
trunk*:~/src/cassandra$ bin/cqlsh -3
Connected to Test Cluster at localhost:9160.
[cqlsh 3.0.2 | Cassandra 2.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.37.0]
Use HELP for help.
cqlsh> create keyspace Test with replication={'class':'SimpleStrategy', 'replication_factor':1};
cqlsh> use Test;
cqlsh:Test> CREATE TABLE users (   user_name varchar PRIMARY KEY,   password varchar,   gender varchar,   session_token varchar,   state varchar,   birth_year bigint );
cqlsh:Test> describe tables;

<empty>

cqlsh:Test> use test;
cqlsh:test> describe tables;

users

cqlsh:test>
{noformat}",,aleksey,mbulman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/May/13 00:08;aleksey;5567.txt;https://issues.apache.org/jira/secure/attachment/12583249/5567.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,327937,,,Wed May 15 13:09:03 UTC 2013,,,,,,,,,,"0|i1kkxz:",328281,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"15/May/13 01:31;brandon.williams;+1;;;","15/May/13 13:09;aleksey;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"java/org/apache/cassandra/dht/BytesToken.java returns invalid string token (with prefix   Token(bytes["" + Hex.bytesToHex(token) + ""]))",CASSANDRA-5566,12647517,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,juras,juras,juras,14/May/13 15:05,16/Apr/19 09:32,14/Jul/23 05:53,14/May/13 19:03,1.2.5,,,,,,0,,,,,"BytesToken java class method toString returns invalid token. 
Changing java/org/apache/cassandra/dht/BytesToken.java
line 44 from:
return ""Token(bytes["" + Hex.bytesToHex(token) + ""])"";
to:
return Hex.bytesToHex(token);
shuffle works fine.

when you run:
./cassandra-shuffle -h localhost create
cassandra throws:
Exception in thread ""main"" java.lang.NumberFormatException: Non-hex characters in Token(bytes[d439e5e5ad484679a0c1f045fea7b2a3])
        at org.apache.cassandra.utils.Hex.hexToBytes(Hex.java:60)
        at org.apache.cassandra.dht.AbstractByteOrderedPartitioner$1.fromString(AbstractByteOrderedPartitioner.java:168)
        at org.apache.cassandra.tools.Shuffle.createShuffleBatchInsert(Shuffle.java:580)
        at org.apache.cassandra.tools.Shuffle.shuffle(Shuffle.java:358)
        at org.apache.cassandra.tools.Shuffle.main(Shuffle.java:678)
","debian x86_64 GNU/Linux, eff3c455ac73ac482a6fcf5cec111abc  apache-cassandra-1.2.4-src.tar.gz",juras,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,juras,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,327873,,,Tue May 14 18:37:23 UTC 2013,,,,,,,,,,"0|i1kkjr:",328217,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"14/May/13 18:37;jbellis;Kind of bizarre that we're passing String representations of tokens around, but this is probably the easiest fix.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix memorySize bugs,CASSANDRA-5564,12647367,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,carlyeks,jbellis,jbellis,13/May/13 20:04,16/Apr/19 09:32,14/Jul/23 05:53,21/May/13 03:50,1.2.5,,,,,,0,,,,,,,carlyeks,cburroughs,christianmovi,hsn,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/May/13 20:47;carlyeks;5564-v2.patch;https://issues.apache.org/jira/secure/attachment/12582992/5564-v2.patch","13/May/13 20:15;jbellis;5564.txt;https://issues.apache.org/jira/secure/attachment/12582976/5564.txt",,,,,,,,,,,,,,,,,,,2.0,carlyeks,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,327723,,,Mon May 13 21:30:29 UTC 2013,,,,,,,,,,"0|i1kjmf:",328067,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"13/May/13 20:13;jbellis;It looks like getFieldSize and getArraySize are only meant to calculate *shallow* sizes.  Deep sizes must be added in separately.  Fixes attached, based on Carl's fix at the bottom of CASSANDRA-4860.;;;","13/May/13 20:47;carlyeks;This seems to improve the readability of the memory size overall.

I thought that the ObjectSizes class could use some comments.

Also, adding comments so that it is easier to figure out where each of the additions is coming from. Also switched to using the TypeSizes instead of just adding seeming arbitrary numbers (plus adds a little self documentation since the field name is included).

There was also another small bug in the RowIndexEntry; it was adding a reference to the size of position. There is only a single long in the class other than a static. This wouldn't significantly overallocate, but just seemed like it should be fixed.;;;","13/May/13 21:30;jbellis;Committed, w/ one final change to IndexedEntry to include the size of its superclass;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Growing pending compactions,CASSANDRA-5554,12646967,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,m0nstermind,m0nstermind,m0nstermind,10/May/13 06:25,16/Apr/19 09:32,14/Jul/23 05:53,13/May/13 18:45,1.2.5,,,,,,0,,,,,"I noticed on one of our new cassandra production server, that ""pending compactions"" number is steadily growing. The cluster is under low write load, so ""compactions are not keeping up"" was not the case.

A quick investigation shown, that compactions are stopping far before all pending tasks are completed. I also found, that if concurrent_compactors=1, background compactions are not happening at all.

The bug is in BackgroundCompactionTask rescheduling logic. The executor pool ""room control"" code in CompactionManager.submitBackground() does not reschedule next background cycle, if executor.getActiveCount reach maximun pool size, so it is lost forever.

So I patched it to always schedule single background cycle, regardless of the free room in executor pool.",,jjordan,m0nstermind,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/13 06:42;m0nstermind;patch.diff;https://issues.apache.org/jira/secure/attachment/12582588/patch.diff","10/May/13 06:42;m0nstermind;pending_compactions_fixed.png;https://issues.apache.org/jira/secure/attachment/12582587/pending_compactions_fixed.png",,,,,,,,,,,,,,,,,,,2.0,m0nstermind,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,327324,,,Mon May 13 18:45:54 UTC 2013,,,,,,,,,,"0|i1kh5r:",327668,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"13/May/13 18:45;jbellis;committed; thanks!

(unable to add Oleg to the Contributor role, for some reason...);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
intersects the bounds not right,CASSANDRA-5551,12646780,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,wy96f,wy96f,wy96f,09/May/13 10:01,16/Apr/19 09:32,14/Jul/23 05:53,13/May/13 10:14,1.1.12,1.2.5,,,,,0,,,,,intersecs the bound includes the left of bound instead of right,,rcoli,slebresne,wy96f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/May/13 05:50;wy96f;0001-correctly-intersects-the-bounds.patch;https://issues.apache.org/jira/secure/attachment/12582579/0001-correctly-intersects-the-bounds.patch",,,,,,,,,,,,,,,,,,,,1.0,wy96f,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,327138,,,Tue May 28 18:38:31 UTC 2013,,,,,,,,,,"0|i1kg0f:",327482,,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,,"09/May/13 13:22;jbellis;You're going to need to get a *lot* more specific.;;;","10/May/13 05:50;wy96f;the intersects(Bounds<T> that) function in Range.java  should contains(that.left) instead of right;;;","13/May/13 10:14;slebresne;Make sense. +1, committed, thanks.;;;","13/May/13 14:05;jbellis;Is this also a problem in 1.1?;;;","13/May/13 14:12;slebresne;It is apparently. Forgot to look sorry. I'll backport there.;;;","28/May/13 17:27;rcoli;What impact does/did this bug have, on what code paths?;;;","28/May/13 18:38;jbellis;Basically nil.  The effect is that cleanup will take not take the fast ""discard entire sstable path"" occasionally.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hadoop jobs assigns only one mapper in task,CASSANDRA-5544,12646363,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,shamim_ru,shamim_ru,07/May/13 07:00,16/Apr/19 09:32,14/Jul/23 05:53,29/May/13 17:55,1.2.6,,,,,,4,,,,,"We have got very strange beheviour of hadoop cluster after upgrading 
Cassandra from 1.1.5 to Cassandra 1.2.1. We have 5 nodes cluster of Cassandra, where three of them are hodoop slaves. Now when we are submitting job through Pig script, only one map assigns in task running on one of the hadoop slaves regardless of 
volume of data (already tried with more than million rows).
Configure of pig as follows:
export PIG_HOME=/oracle/pig-0.10.0
export PIG_CONF_DIR=${HADOOP_HOME}/conf
export PIG_INITIAL_ADDRESS=192.168.157.103
export PIG_RPC_PORT=9160
export PIG_PARTITIONER=org.apache.cassandra.dht.Murmur3Partitioner


Also we have these following properties in hadoop:
 <property>
 <name>mapred.tasktracker.map.tasks.maximum</name>
 <value>10</value>
 </property>
 <property>
 <name>mapred.map.tasks</name>
 <value>4</value>
 </property>","Red hat linux 5.4, Hadoop 1.0.3, pig 0.11.1",alexliu68,cscetbon,julien_campan,shamim_ru,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5604,,,,,,,,,,,"28/May/13 20:45;alexliu68;5544-1.txt;https://issues.apache.org/jira/secure/attachment/12585083/5544-1.txt","29/May/13 17:06;alexliu68;5544-2.txt;https://issues.apache.org/jira/secure/attachment/12585247/5544-2.txt","30/May/13 18:10;alexliu68;5544-3.txt;https://issues.apache.org/jira/secure/attachment/12585442/5544-3.txt","28/May/13 17:14;alexliu68;5544.txt;https://issues.apache.org/jira/secure/attachment/12585045/5544.txt","26/May/13 13:00;shamim_ru;Screen Shot 2013-05-26 at 4.49.48 PM.png;https://issues.apache.org/jira/secure/attachment/12584867/Screen+Shot+2013-05-26+at+4.49.48+PM.png",,,,,,,,,,,,,,,,5.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,326721,,,Fri Jun 28 07:27:57 UTC 2013,,,,,,,,,,"0|i1kd5z:",327066,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"07/May/13 07:57;cscetbon;same issue with Cassandra 1.2.3. I've tested with both RandomPartitioner and Murmur3Partitioner;;;","07/May/13 08:54;shamim_ru;For more information, here is some threads from mail archive
1) http://www.mail-archive.com/user@cassandra.apache.org/msg29663.html
2) http://www.mail-archive.com/user@cassandra.apache.org/msg28016.html
3) http://www.mail-archive.com/user@cassandra.apache.org/msg29425.html;;;","07/May/13 19:45;brandon.williams;Does 1.1.11 have the same problem?;;;","26/May/13 12:56;shamim_ru;Cassandra version 1.1.11 have no such problem. I have test in single node cluster and it's created 15 map. 
See attach please.;;;","27/May/13 08:30;cscetbon;So something goes wrong with 1.2.x version;;;","27/May/13 17:27;brandon.williams;Can you take a look, Alex?  Nothing changed in pig as far I know.;;;","27/May/13 21:42;alexliu68;[~shamim] How many splits do you get for each hadoop node? You can set ConfigHelper.setInputSplitSize to a smaller number to get more mappers for your pig job. The existing CassandraStorage class doesn't set it, so it uses the defualt value of 64k. So if your nodes has less than 64k rows, it will have only one mapper.;;;","27/May/13 21:44;alexliu68;Some changes had been made to CassandraColumnInputFormat class since 1.1.5

e.g.
add describe_splits_ex providing improved split size estimate
patch by Piotr Kolaczkowski; reviewed by jbellis for CASSANDRA-4803;;;","27/May/13 22:08;cscetbon;[~alexliu68] I did some tests with more than 64k row and had only one mapper for the whole cluster. Even if we have less than 64k rows, why don't we have at least one mapper per node (in my case replication_factor=1) to work on rows using data locality. Vnodes are enabled on my cluster, can there be a relation with this option ?;;;","27/May/13 22:13;alexliu68;Yes, if vnode is enale, it creates a lot of smaller splits (which is not preferred, we will fix the vnode hadoop too many small splits issue later), so can you test it with vnode disable.;;;","27/May/13 22:19;cscetbon;But if there are many small splits it doesn't mean that we should have more mappers ? I'm saying that cause you propose to [~shamim_ru] to decrease ConfigHelper.setInputSplitSize exactly for that, right ?
I need one more day to test without vnodes.;;;","27/May/13 22:31;alexliu68;Current implementation only matches one mapper to a split. Existing code doesn't set InputSplitSize (which means we can't change it to a smaller number unless we change the code at setLocation method to do it), so we need more than 64k rows to have more than one mapper per node.

For vnode we need to support a virtual split which combines multiple small splits. ;;;","27/May/13 22:41;cscetbon;okay. I'll test without vnodes and give you a feedback except if [~shamim_ru] confirms that he didn't use vnodes, which I suppose as he upgraded from C* 1.1.5 to 1.2.1;;;","28/May/13 07:04;shamim_ru;[~alexliu68]
1) I am using pig and actually don't know how many split i had (i am very curious to know how to calculate the split count). However i have had more than 30 million rows.
2) I didn't use VNODES.
3) SET mapred.min.split.size 12500000; 
SET mapred.max.split.size 12500000;
 doesn't help at all
4) SET pig.noSplitCombination true; - did some magic trick, we got more than 100 maps but 2 of them (always two maps) got very large Map input records and runs more than hours. 
5) Observe one very interesting thing when used SET pig.noSplitCombination true, a lot of maps created with  	
Map input records 	0 
;;;","28/May/13 16:32;alexliu68;To get the splits for the node, call thrift API client.describe_splits_ex(cfName, range.start_token, range.end_token, splitsize) it returns the split for that node.

where range.start_token and range.end_token is the start and end token of the node, and splitsize is 64 *1024;;;","28/May/13 17:04;alexliu68;[~shamim] I think you already found the answer, SET pig.noSplitCombination true, so Pig doesn't combine the small splits into one mapper. HBase internal code does it as well. I found that C*-1.2.1 update Pig from 0.9.0 version to 0.10.0 version which may cause the behavior changes.

As far as number 4) and number 5) concerns, I think the empty maps/big maps are due to data skewness. If you can first print out the splits, then you can check the rows for each split.

I will add the following code to CassandraStorage.java

job.getConfiguration().setBoolean(""pig.noSplitCombination"", true);;;;","28/May/13 17:14;alexliu68;I attached the patch.;;;","28/May/13 19:46;cscetbon;AFAIK split combination is used to improve performance. Doesn't it mean the same for cassandra ?
And if performance decreases without split combination, will the performance decrease much more with vnodes ?;;;","28/May/13 20:20;alexliu68;CassandraColumnInputFormat define the split size, so we don't want Pig to override it by combining splits. We can always tune the split size to tune the performance. Next step, we can open up a little bit so that Pig user can specify split size configuration.

Vnode hadoop performance generally decreases, we can do the split combination at Cassandra side to improve the performance, which could be another ticket.;;;","28/May/13 20:45;alexliu68;Version 2 patch is attached. It allows user to define PIG_INPUT_SPLIT_SIZE in the system env;;;","29/May/13 07:06;shamim_ru;Alex, thank you very much for your quick response. 
However, i am afraid that above patch will not solve the problem i described ""we got more than 100 maps but 2 of them (always two maps) got very large Map input records and runs more than hours - point 4"" - this behavior is unexpected. This means Map input records is not evenly through cluster, most of the maps getting  Map input records = 10000 but only two of them getting more than millions.
Certainly i will do some test through thrift api as you described. 
One more things, would you kindly allows user to define PIG_INPUT_SPLIT_SIZE through cassandra store URL as ""STORE updated INTO 'cassandra://KEYSPACE/CF?allow_deletes=true&PIG_INPUT_SPLIT_SIZE=xxxxxx' USING CassandraStorage()"" instead of system environment.;;;","29/May/13 14:10;cscetbon;Or maybe via a SET PIG_INPUT_SPLIT_SIZE in Pig script ?
[~alexliu68] I open the second ticket to improve performance with vnodes except if you prefer to open it, which could be better :);;;","29/May/13 17:06;alexliu68;Version 3 is attached. I add split_size as a parameter.;;;","29/May/13 17:08;alexliu68;[~cscetbon] please open it, someone else may already open it.;;;","29/May/13 17:55;brandon.williams;Committed, with an update to the README to document split_size.;;;","30/May/13 18:10;alexliu68;Version 4 is attached, it removes getting split size as system env;;;","31/May/13 12:46;brandon.williams;I'm fine with leaving that in for now.;;;","31/May/13 13:36;shamim_ru;I have a plan to do some test in weekend ;;;","05/Jun/13 13:13;cscetbon;My tests confirm that I have multiple mappers (1025) and each mapper works on a range of my column family [http://pastebin.com/vL3uC5Ca]. Good job !;;;","05/Jun/13 13:44;shamim_ru;did you run map reduce job through Pig?;;;","05/Jun/13 13:47;cscetbon;Yes. I used Pig 0.11.1, Hadoop 1.1.2 (as newer versions are not supported [CASSANDRA-5201|https://issues.apache.org/jira/browse/CASSANDRA-5201]) and cassandra 1.2.3 (I added the current patch from git commits and built sources) ;;;","28/Jun/13 07:27;shamim_ru;At last , i could manage a few hours to try the fix. Definitely it's working now, every mapper works on their own range, however i have test in single node cluster with Hadoop 1.1.2 + Pig 0.11.1 and Cassandra 1.2.6. Thankx. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
BulkLoader is broken in trunk,CASSANDRA-5542,12646297,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,yukim,yukim,06/May/13 21:20,16/Apr/19 09:32,14/Jul/23 05:53,18/Jul/13 14:28,2.0 beta 2,,,,,,0,,,,,"After CASSANDRA-5015 and CASSANDRA-5521, we need CFMetaData to open SSTable(Reader), especially to get bloom_filter_fp_chance and index_interval.

When using bulkloader, CFMetaData is not available, so we cannot open SSTable to be streamed.",,slebresne,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Jul/13 20:31;yukim;0001-change-streaming-procedure.patch;https://issues.apache.org/jira/secure/attachment/12592845/0001-change-streaming-procedure.patch","16/Jul/13 02:37;yukim;0002-make-BulkLoader-work-with-SSTableReader.patch;https://issues.apache.org/jira/secure/attachment/12592469/0002-make-BulkLoader-work-with-SSTableReader.patch","16/Jul/13 02:37;yukim;0003-update-for-post-CASSANDRA-5171.patch;https://issues.apache.org/jira/secure/attachment/12592468/0003-update-for-post-CASSANDRA-5171.patch","06/Jun/13 18:37;yukim;5542-fix-NPE.txt;https://issues.apache.org/jira/secure/attachment/12586548/5542-fix-NPE.txt","29/May/13 15:55;yukim;5542.txt;https://issues.apache.org/jira/secure/attachment/12585231/5542.txt",,,,,,,,,,,,,,,,5.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,326655,,,Thu Jul 18 14:28:58 UTC 2013,,,,,,,,,,"0|i1kcrj:",327000,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"29/May/13 15:55;yukim;There are two classes that need to change:

* DatabaseDescriptor
  We need to set memoryAllocator in order to create IndexSummary.

* SSTableLoader
  Create dummy CFMetadata and pass it to SSTableReader.open, so it can use default values for bf_fp_chance and index_interval.

I'm not sure if that the best approach for second one though.;;;","29/May/13 16:45;jbellis;I'm a little skeptical about the SSTL changes as well.  Seems dangerous to pass a default CFM that claims e.g. comparator that doesn't actually match the data.

I note that we're passing null back in 1.2 as well; what changed?;;;","29/May/13 18:09;yukim;On trunk, we are using bloom_filter_fp_chance and index_interval from CFMetaData when opening.
If we pass null, we get NPE.
;;;","29/May/13 22:03;jbellis;Pushed the DatabaseDescriptor change to trunk.  CASSANDRA-5555 will solve the SSTableLoader problem.;;;","06/Jun/13 18:36;yukim;Still, sstableloader throws NPE on the execution path at SSTableReader.;;;","06/Jun/13 18:37;yukim;Attaching patch to switch using indexSummary instead of CFMetaData where affected.;;;","06/Jun/13 18:40;jbellis;sample key count is going to be nonsense, will that break anything?;;;","06/Jun/13 18:45;yukim;hmm, you are right, but it is used to tell the receiving node the estimated key count for creating BF.
Maybe we should query schema first, then bulk load.
I need that anyway if we switch to use cf ID for new streaming protocol(https://issues.apache.org/jira/browse/CASSANDRA-5286?focusedCommentId=13671413&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13671413).;;;","06/Jun/13 18:54;jbellis;It makes me sad to introduce a schema dependency, since that means you can't run sstableloader except on a cluster member.

What if we added key count and cfid to the stats/metadata sstable component?;;;","06/Jun/13 19:06;yukim;bq. It makes me sad to introduce a schema dependency, since that means you can't run sstableloader except on a cluster member.

It does query schema even on 1.2(CASSANDRA-4755), so it is fairly easy to fetch info we need.

bq. What if we added key count and cfid to the stats/metadata sstable component?

This can also be done, but the change requires stats.db to be always present when using bulkloader, which we don't today.;;;","06/Jun/13 19:27;jbellis;Oh, I'm fine with querying remotely a la 4755.  I thought we were talking about opening up the system keyspace locally.;;;","08/Jul/13 22:58;yukim;I added ""querying schema"" part to bulk loader and successfully opened SSTables, but now I'm stuck on the change in streaming.
New streaming now connects from both endpoints, so bulk loader also needs to listen on storage port to receive message from the loading nodes. I think we don't want to make bulk loader a fat client again...
[~slebresne] Any suggestion for fixing this issue?;;;","09/Jul/13 09:40;slebresne;Ahah, yeah that sucks, didn't though of that.

Well, one solution I think might be to change a bit the solution I used in CASSANDRA-5699. Instead of having the initiator node open one connection and the other one opening the other one afterwards, we could make the initiator node open both connections instead sequentially (sending a StreamInit on both, but with a ""isIncoming"" flag to say if the connection should be used by the remote as incoming or ongoing connection).;;;","09/Jul/13 21:00;yukim;Thanks Sylvain.

Attaching 2 patches, 0001 changes streaming 2.0 procedure based on Sylvain's comment above(initiator opens 2 connections), and 0002 to query schema to open SSTableReader for streaming.
;;;","12/Jul/13 14:23;slebresne;On the protocol changes, I'm not sure I'm a big fan of adding 2 new stream messages just for the purpose of setting up connections. I was also liking the idea of having a setup phase that creates both connections before starting to use it, instead of creating one and partially using it to synchronize the creation of the other one as done in the patch.

What I had imagined is that the initiator could just create 2 connections (without really waiting for anything from the other side), sending a StreamInit on each with a flag saying if the connection is the incoming or ongoing one. On receiving any of those connection, the other side would fetch the session and attach the connection if it's the second one, or create the session if it's the first one. The initiator could then start the prepare phase as soon as both connection are open (without waiting for any special message from the other side). Seems a bit simpler to me overall.

What do you think?

The 2nd patch lgtm.
;;;","16/Jul/13 02:37;yukim;Uploaded update patches.

* 0001 changes so that streaming initiator makes two connections. It also changes how StreamManager manages current connection in order to distinguish plans inside the same JVM (for bulkload JMX op and unit test).
* 0002 is updated for sstableloader command output.
* 0003 introduces change in how we create socket from streaming session. CASSANDRA-5171 makes OutboundTCPConnection pool require system table which client like sstableloader doesn't have. Instead, I made newSocket static method so we just can obtain socket for specific host.;;;","16/Jul/13 07:13;slebresne;Looks mostly good. A couple of minor remarks/nits:
 * In ConnectionHandler, initiateOnReceivingSide and the pair attach(Incoming/Outgoing)Socket are basically duplicates. We should keep only one of them.
 * We use to call the StreamSession.start(Socket, ...) (the one on the receiving side) on the streamExecutor because it was creating a connection (and was thus blocking on I/O). Since it's not the case anymore, it's probably overkill. So we could ditch that whole start function and do the attachSocket in SRF.initReceivingSide, which would improve the symmetry of that function.
 * Does separating initiated and receiving stream in StreamManager really buys us anything? (outside slightly complicating things)
 * On failure (StreamSession.onError), we could (and probably should) send a message as long as the outgoing connection is opened (even if the incoming one is broken), so the current isConnected() is possibly a bit restrictive.
 * For consistency sake, can't we rename forOutput to isOutgoing everywhere, and same for isFirst in ConnectionHandler.sendInitMessage.
 * The comment on StreamInitMessage.isForOutgoing is outdated (talk of ACK, INIT_ACK)
 * The comment on on top StreamSession still talks of InitCompleteMessage.
 * The comment in IncomingStreamingSession still mentions INIT_ACK
;;;","17/Jul/13 20:31;yukim;Thanks for the review.
Updated 0001 patch.

bq. Does separating initiated and receiving stream in StreamManager really buys us anything? (outside slightly complicating things)

We need to keep StreamResultFuture of the same plan ID when we are doing streaming on the same JVM. This happens when we bulk load via JMX method, or running StreamingTransferTest unit test.

bq. On failure (StreamSession.onError), we could (and probably should) send a message as long as the outgoing connection is opened (even if the incoming one is broken), so the current isConnected() is possibly a bit restrictive.

You are right. I changed to put null check on outgoing message handler before sending message instead.
;;;","18/Jul/13 08:32;slebresne;One last nit: In ConnectionHandler.sendMessage(), I don't like throwing messages on the floor without warning. I'd rather add some isOutgoingConnected() method in ConnectionHandler and have the caller check that before calling sendMessage. And otherwise, sendMessage() would throw a RuntimeException in that case too.

But other than that, lgtm, ship it!;;;","18/Jul/13 14:28;yukim;Committed with above nit fix.
Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Concurrent secondary index updates remove rows from the index,CASSANDRA-5540,12646193,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,alexeibakanov,alexeibakanov,06/May/13 11:04,16/Apr/19 09:32,14/Jul/23 05:53,09/May/13 22:42,1.2.5,,,Feature/2i Index,,,1,,,,,"Existing rows disappear from secondary index when doing simultaneous updates of a row with the same secondary index value.

Here is a little pycassa script that reproduces a bug. The script inserts 4 rows with same secondary index value, reads those rows back and check that there are 4 of them.
Please run two instances of the script simultaneously in two separate terminals in order to simulate concurrent updates:
{code}
-----scrpit.py START-----
import pycassa
from pycassa.index import *

pool = pycassa.ConnectionPool('ks123')
cf = pycassa.ColumnFamily(pool, 'cf1')

while True:
    for rowKey in xrange(4):
        cf.insert(str(rowKey), {'indexedColumn': 'indexedValue'})

    index_expression = create_index_expression('indexedColumn', 'indexedValue')
    index_clause = create_index_clause([index_expression])
    rows = cf.get_indexed_slices(index_clause)
    length = len(list(rows))
    if length == 4:
        pass
    else:
        print 'found just %d rows out of 4' % length

pool.dispose()

---script.py FINISH---

---schema cli start---
create keyspace ks123
  with placement_strategy = 'NetworkTopologyStrategy'
  and strategy_options = {datacenter1 : 1}
  and durable_writes = true;

use ks123;

create column family cf1
  with column_type = 'Standard'
  and comparator = 'AsciiType'
  and default_validation_class = 'AsciiType'
  and key_validation_class = 'AsciiType'
  and read_repair_chance = 0.1
  and dclocal_read_repair_chance = 0.0
  and populate_io_cache_on_flush = false
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy'
  and caching = 'KEYS_ONLY'
  and column_metadata = [
    {column_name : 'indexedColumn',
    validation_class : AsciiType,
    index_name : 'INDEX1',
    index_type : 0}]
  and compression_options = {'sstable_compression' : 'org.apache.cassandra.io.compress.SnappyCompressor'};
---schema cli finish---
{code}
Test cluster created with 'ccm create --cassandra-version 1.2.4 --nodes 1 --start testUpdate'",,alexeibakanov,rcoli,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/May/13 21:59;samt;0001-Use-different-index-updater-for-live-updates-compact.patch;https://issues.apache.org/jira/secure/attachment/12582179/0001-Use-different-index-updater-for-live-updates-compact.patch","09/May/13 12:10;samt;5540.txt;https://issues.apache.org/jira/secure/attachment/12582457/5540.txt",,,,,,,,,,,,,,,,,,,2.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,326551,,,Fri Sep 20 22:25:28 UTC 2013,,,,,,,,,,"0|i1kc4f:",326896,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"06/May/13 20:01;jbellis;Hmm.  It looks like this can happen when multiple inserts happen at the same timestamp, since we delete the existing entry with its own timestamp.  But if the replacement has the same timestamp, then the tombstone wins the tie.

Any clever ideas to fix this [~beobal]?;;;","07/May/13 12:30;samt;I don't think this is caused by the index updates in KeysSearcher. There, we only compare the values & since this test always writes the same values the index entry is never deemed stale, and so we don't ever write a tombstone. 

The test script does reproduce the issue completely reliably though, so I'll dig in and find the actual cause.;;;","07/May/13 14:58;samt;Sorry [~jbellis] I misunderstood, I see what you mean now. No clever ideas yet, but I'm working on it.;;;","07/May/13 15:32;jbellis;Can we just special-case StandardUpdater.update to check for oldColumn.equals(column) and no-op that?

(NB I think the compaction code that calls {{remove}} would need to check for {{.equals}} instead of {{==}} as well.);;;","07/May/13 15:43;samt;Can we just remove the deletion from StandardUpdater.update() altogether? That seems to be fine for realtime updates (I no longer see the missing rows & all unit tests are passing) but will it screw with compaction?;;;","07/May/13 15:48;jbellis;Right, the reason that's there is that compaction can't know to purge the stale index entries for values that never made it into an sstable.;;;","07/May/13 21:59;samt;Checking oldColumn.equals(column) in SU.update() isn't sufficient. I found that even with the short circuit, occasionally the test script would return only 3 of the 4 expected columns. My suspicion is that this is caused by the delete & subsequent insert in SU.update() being non-atomic, though I haven't proved this. Rather than go down that rabbit hole, I've split the Updater implementation into 2 subclasses - LiveUpdater & CompactionUpdater. The difference between them is that the CU behaves like SU and always purges old values, whereas LU just upserts into the index. SIM.updaterFor() now takes a second argument to determine whether the updater is for processing live updates or for use during compaction.

Unit tests pass & the test script runs without issue.
;;;","08/May/13 15:32;jbellis;How does this avoid leaking index entries that will never be cleaned up by compaction?;;;","09/May/13 12:10;samt;yes, you're right that's dumb sorry. 

It took me a while, but there's actually 2 issues here. The first, as you identified, is caused by overwrites with identical timestamps and is fixed by making the case where oldColumn.equals(newColumn) a no-op. The second is the window of inconsistency that I mentioned earlier. When the 2 instances of the test script are running, its possible for one to query the index while inbetween the old index entry being deleted & the new one inserted, leading to a ""missing"" result. To address that, I've reversed the order so that the new entry is added before the old one is removed. This should be safe for readers due to the checking for stale values in the index searcher. ;;;","09/May/13 22:42;jbellis;LGTM; committed.;;;","10/May/13 05:43;alexeibakanov;Fabulous! Thank you very much for the fix!;;;","20/Sep/13 18:38;rcoli;Is it a correct assessment that this issues ""affects"" begins with the initial implementation of 2i in 0.7? If not, when?;;;","20/Sep/13 19:04;jbellis;No, affects 1.2.0+ as indicated.;;;","20/Sep/13 22:25;rcoli;Oh, weird. I see ""affects"" in the history but don't see it in the ""Details"" section.

Probably I just need to configure my JIRA better, sorry for the noise.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ColumnFamilyInputFormat demands OrderPreservingPartitioner when specifying InputRange with tokens,CASSANDRA-5536,12646032,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,lannyripple,lannyripple,03/May/13 19:53,16/Apr/19 09:32,14/Jul/23 05:53,27/May/13 16:28,1.2.6,,,,,,0,hadoop,,,,"When ColumnFamilyInputFormat starts getting splits (via getSplits(...) [ColumnFamilyInputFormat.java:101]) it checks to see if a `jobKeyRange` has been set.  If it has been set it attempts to set the `jobRange`.  However the if block (ColumnFamilyInputFormat.java:124) looks to see if the `jobKeyRange` has tokens but asserts that the OrderPreservingPartitioner must be in use.

This if block should be looking for keys (not tokens).  Code further down (ColumnFamilyInputFormat.java:147) already manages the range if tokens are used but can never be reached.",,alexliu68,lannyripple,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/May/13 21:53;jbellis;5536-v2.txt;https://issues.apache.org/jira/secure/attachment/12581966/5536-v2.txt","03/May/13 20:02;lannyripple;cassandra-1.2.3-5536.txt;https://issues.apache.org/jira/secure/attachment/12581742/cassandra-1.2.3-5536.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,326391,,,Mon May 27 16:28:12 UTC 2013,,,,,,,,,,"0|i1kb4v:",326736,,,,,,,,,alexliu68,,alexliu68,Normal,,,,,,,,,,,,,,,,,,"03/May/13 19:56;lannyripple;Note that the assertion requiring OrderPreservingPartition mentions ""ConfigHelper.setInputKeyRange()"" which no longer exists.;;;","03/May/13 20:02;lannyripple;Suggested changes.;;;","06/May/13 21:53;jbellis;Analysis LGTM, v2 attached with a bit more cleanup.;;;","21/May/13 04:01;jbellis;Does v2 look good to you, Lanny?;;;","27/May/13 15:47;jbellis;WDYT [~alexliu68]?;;;","27/May/13 16:21;alexliu68;It looks good.;;;","27/May/13 16:28;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Manifest file not fsynced,CASSANDRA-5535,12645997,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,cumarana,cumarana,03/May/13 15:54,16/Apr/19 09:32,14/Jul/23 05:53,04/May/13 18:59,1.1.12,1.2.5,,,,,0,,,,,"We had several cases where the the manifest file would get corrupted when doing power reset tests or iLO resets to mimic power failure scenarios, ungraceful resets, kernel panics etc. It wasn't clear at the time where the problem was, but I think the data below shows that Cassandra is missing an fsync call to the manifest file prior to closing it. This particular stack trace from below is on Cassandra 1.2.4.
The trace was captured using strace options:

strace -f -p 2200 -e trace=open,close,write,fsync,fdatasync,rename
[pid 9710] open(""/opt/mp/storage/persistent/cassandra/cassandra-lib/data/MSA/subinfo/subinfo-tmp.json"", O_WRONLY|O_CREAT|O_TRUNC, 0666) = 238
[pid 9710] write(238, ""{\n \""generations\"" : [ {\n \""gen""..., 3996) = 3996
[pid 9710] write(238, ""14, 263161, 263484, 270816, 2593""..., 3996) = 3996
[pid 9710] write(238, ""275136, 275137, 275138, 275139, ""..., 1173) = 1173
[pid 9710] close(238) = 0
[pid 9710] rename(""/opt/mp/storage/persistent/cassandra/cassandra-lib/data/MSA/subinfo/subinfo.json""
, ""/opt/mp/storage/persistent/cassandra/cassandra-lib/data/MSA/subinfo/subinfo-old.json"") = 0
[pid 9710] rename(""/opt/mp/storage/persistent/cassandra/cassandra-lib/data/MSA/subinfo/subinfo-tmp.j
son"", ""/opt/mp/storage/persistent/cassandra/cassandra-lib/data/MSA/subinfo/subinfo.json"") = 0
","RHEL 6.4
java -version
java version ""1.6.0_31""
Java(TM) SE Runtime Environment (build 1.6.0_31-b04)
Java HotSpot(TM) 64-Bit Server VM (build 20.6-b01, mixed mode)
",carlyeks,cumarana,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/May/13 18:36;marcuse;0001-CASSANDRA-5535-fsync-leveled-manifest.patch;https://issues.apache.org/jira/secure/attachment/12581724/0001-CASSANDRA-5535-fsync-leveled-manifest.patch",,,,,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,326356,,,Sat May 04 18:59:04 UTC 2013,,,,,,,,,,"0|i1kax3:",326701,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"03/May/13 16:01;jbellis;We've already fixed this in 2.0 by moving level information into sstable metadata (CASSANDRA-4872), but it may be reasonable to add an fsync to 1.2.  What do you think [~krummas]?;;;","03/May/13 16:04;marcuse;yeah sounds good, ill fix;;;","03/May/13 18:37;marcuse;fsyncs the file after flushing;;;","03/May/13 23:55;carlyeks;Looks good. The patch doesn't apply to the 1.1 branch; I'm sure it will be easy to fix up so that it does, assuming this is target to 1.1 and 1.2.;;;","04/May/13 05:09;jbellis;+1

(setting affects-version to when this was introduced);;;","04/May/13 18:59;marcuse;committed as fe910e6c90d81cc61c16859bcef9f0dcb42cc827, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Writing wide row causes high CPU usage after compaction,CASSANDRA-5534,12645855,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,enigmacurry,enigmacurry,02/May/13 18:53,13/Dec/22 12:21,14/Jul/23 05:53,05/Jul/13 08:44,2.0.0,,,,,,0,,,,,"Introduced in commit -e74c13ff08663d306dcc5cdc99c07e9e6c12ca21- (see below) there is a significant slow down when creating a wide row with cassandra-stress:

Testing with the prior (good) commit I used this to write a single wide row, which completed rather quickly:
{code}
$ ccm create -v git:60f09f0121e0801851b9ab017eddf7e326fa05fb wide-row
Fetching Cassandra updates...
Cloning Cassandra (from local cache)
Checking out requested branch (60f09f0121e0801851b9ab017eddf7e326fa05fb)
Compiling Cassandra 60f09f0121e0801851b9ab017eddf7e326fa05fb ...
Current cluster is now: wide-row
$ ccm populate -n 1
$ ccm start
$ time ccm node1 stress -c 10000 -S 1000 -n 1
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency/95th/99th,elapsed_time
1,0,0,273.3,273.3,273.3,0
END

real	0m7.106s
user	0m1.710s
sys	0m0.120s
{code}

Using the bugged commit (e74c13ff08663d306dcc5cdc99c07e9e6c12ca21) I get a significant slow down:
{code}
02:42 PM:~$ ccm create -v git:e74c13ff08663d306dcc5cdc99c07e9e6c12ca21 wide-row
Fetching Cassandra updates...
Current cluster is now: wide-row
02:42 PM:~$ ccm populate -n 1
02:42 PM:~$ ccm start
02:42 PM:~$ time ccm node1 stress -c 10000 -S 1000 -n 1
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency,95th,99th,elapsed_time
1,0,0,423.2,423.2,423.2,0
Total operation time      : 00:00:00
END

real	4m16.394s
user	0m2.230s
sys	0m0.137s

{code}

Interestingly, the commit in question just says it's a merge from cassandra-1.2, but I do not see this same slowdown using that branch, this only occurs in trunk.",,alexzar,enigmacurry,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5125,,,,,,,,,,,,,,,"02/May/13 18:54;enigmacurry;wide_row_stress.trunk.log.txt.gz;https://issues.apache.org/jira/secure/attachment/12581573/wide_row_stress.trunk.log.txt.gz",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,326214,,,Fri Jul 05 14:12:32 UTC 2013,,,,,,,,,,"0|i1ka1j:",326559,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,alexzar,,,"02/May/13 18:59;enigmacurry;The reason I titled this bug with the phrase ""after compaction"" is that the very last line of the log file that talked about compaction is what it stayed on for the majority of that 4 minutes that it ran for. I don't have any other clues that this is related to compaction. ;;;","02/May/13 22:38;jbellis;It takes multiple minutes for me on e74c13ff08663d306dcc5cdc99c07e9e6c12ca21^ as well.  Are you sure that's the problem?;;;","03/May/13 00:21;enigmacurry;No, you're right. I think my git-fu was failing me, I was just going by git log, but I think I need to do a full bisect. I'll update when I find the real cause.;;;","03/May/13 01:13;enigmacurry;The bisect brought me back to a950b9257f4c92d067eb5e1d437096699123ac9b which is from [CASSANDRA-5125|https://issues.apache.org/jira/browse/CASSANDRA-5125] - that's the first instance where this slowdown occurs.;;;","03/May/13 01:22;jbellis;It looks like there is slowdown in the schema creation as well as the inserts.

(I note that stress is creating 10,000 named columns which is not a normal ""wide row.""  There is no way to configure stress to create ""real"" wide rows, unfortunately.);;;","26/Jun/13 17:56;slebresne;Would you mind testing that again? I just tried the same test with current trunk and it's fast (< 2s for the whole stress).;;;","05/Jul/13 02:19;alexzar;latest test on cassandra-1.2:

$> ccm create -v git:cassandra-1.2 test-1.2.6
Fetching Cassandra updates...
Cloning Cassandra (from local cache)
Checking out requested branch (cassandra-1.2)
Compiling Cassandra cassandra-1.2 ...
Current cluster is now: test-1.2.6

$> ccm populate -n 1
$> ccm start

$> time ccm node1 stress -c 10000 -S 1000 -n 1
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency/95th/99th,elapsed_time
1,0,0,233.6,233.6,233.6,0
END

real    0m6.539s
user    0m1.714s
sys     0m0.137s

latest test on trunk:

$> ccm create -v git:trunk test-trunk
Fetching Cassandra updates...
Cloning Cassandra (from local cache)
Checking out requested branch (trunk)
Compiling Cassandra trunk ...
Current cluster is now: test-trunk

$> ccm populate -n 1
$> ccm start

$> time ccm node1 stress -c 10000 -S 1000 -n 1
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,latency,95th,99th,elapsed_time
1,0,0,343.1,343.1,343.1,0


Total operation time      : 00:00:00
END

real    0m7.333s
user    0m1.945s
sys     0m0.136s
;;;","05/Jul/13 08:44;slebresne;So closing since both our experience seems to show that this has been fixed somehow: it doesn't take ""multiple minutes"" anymore.;;;","05/Jul/13 14:12;enigmacurry;Alex, I think it would be useful to know what the root cause of this was, so we can track it going forward. Running a 'git bisect' should tell us.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Maven package installation broken by recent build changes,CASSANDRA-5532,12645685,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,samt,samt,samt,01/May/13 19:22,16/Apr/19 09:32,14/Jul/23 05:53,02/May/13 03:50,2.0 beta 1,,,Legacy/Tools,,,0,,,,,"CASSANDRA-3818 provides the ability to disable maven ant tests during the build. Part of the change is to refactor the maven-ant-tasks-retrieve-build target to wrapped by an antcall, with a guard condition to check if m-a-t should be used. The use of antcall causes the target & those targets it depends on to be called in a separate scope to the main build, which unfortunately means that the repository refs and ant macros which get defined there are not available once the antcall is completed. The final effect is that the mvn-install task fails as the install macro is not defined in its scope, and the artifacts task fails due to the repository refs being similarly undefined. I haven't tried it, but I suspect the publish task would be affected in the same way.",,dbrosius,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/13 19:23;samt;0001-Remove-antcall-around-m-a-t-retrieve-build.patch;https://issues.apache.org/jira/secure/attachment/12581399/0001-Remove-antcall-around-m-a-t-retrieve-build.patch",,,,,,,,,,,,,,,,,,,,1.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,326046,,,Thu May 02 03:50:28 UTC 2013,,,,,,,,,,"0|i1k907:",326391,,,,,,,,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,,"01/May/13 19:23;samt;Patch to revert the antcall wrapping around maven-ant-tasks-retrieve-build. I've added the check on without.maven directly to the target which seems to me to have the desired effect as the original patch;;;","02/May/13 00:35;dbrosius;+1;;;","02/May/13 03:50;dbrosius;committed to trunk as 2bc79a07474e48d57d9c17d2e597048006ff7bf2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Disallow renaming columns one at a time when when the table don't have CQL3 metadata yet,CASSANDRA-5531,12645635,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,01/May/13 14:21,16/Apr/19 09:32,14/Jul/23 05:53,01/May/13 14:27,1.2.5,,,,,,0,,,,,"As noted in CASSANDRA-5489, if you have a ""thrift"" CF, say:
{noformat}
[default@ks] create column family test with comparator='CompositeType(Int32Type, Int32Type, Int32Type)' and key_validation_class=UTF8Type and default_validation_class=UTF8Type;
{noformat}

And that trying to use it in CQL3 you rename the columns one at a time, you can get:
{noformat}
cqlsh:ks> DESC COLUMNFAMILY test;

CREATE TABLE test (
  key text,
  column1 int,
  column2 int,
  column3 int,
  value text,
  PRIMARY KEY (key, column1, column2, column3)
) WITH COMPACT STORAGE ...
cqlsh:ks> ALTER TABLE test RENAME column2 TO foo;
TSocket read 0 bytes
{noformat}

No, it happens that renaming the columns one at a time is a bad idea anyway as it can confuse the CQL3 code in some cases. So I suggest to disallow that and to force renaming all columns in one request the first you use a thrift CF from CQL3.

To be clear, you will still be able to rename columns one at a time in general, it's just for the first time you rename on a metadata-less CF. So overall that's a very small limitation and it simplify our lives code-wise.

See CASSANDRA-5489 for a bit more context here.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/13 14:25;slebresne;5531.txt;https://issues.apache.org/jira/secure/attachment/12581358/5531.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,325996,,,Wed May 01 14:25:50 UTC 2013,,,,,,,,,,"0|i1k8p3:",326341,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"01/May/13 14:25;slebresne;To be clear, that ticket is just to track the bits that go into 1.2 discussed on CASSANDRA-5489. I'm attaching the patch committed for info.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
thrift_max_message_length_in_mb makes long-lived connections error out,CASSANDRA-5529,12645568,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,rtimpe,rtimpe,01/May/13 00:54,16/Apr/19 09:32,14/Jul/23 05:53,21/May/13 18:55,1.1.12,1.2.6,,Legacy/CQL,,,0,,,,,"When running mapreduce jobs that read directly from cassandra, the job will sometimes fail with an exception like this:

java.lang.RuntimeException: com.rockmelt.org.apache.thrift.TException: Message length exceeded: 40
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.maybeInit(ColumnFamilyRecordReader.java:400)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.computeNext(ColumnFamilyRecordReader.java:406)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.computeNext(ColumnFamilyRecordReader.java:329)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getProgress(ColumnFamilyRecordReader.java:109)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.getProgress(MapTask.java:522)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:547)
	at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:771)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:375)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1132)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: com.rockmelt.org.apache.thrift.TException: Message length exceeded: 40
	at com.rockmelt.org.apache.thrift.protocol.TBinaryProtocol.checkReadLength(TBinaryProtocol.java:393)
	at com.rockmelt.org.apache.thrift.protocol.TBinaryProtocol.readBinary(TBinaryProtocol.java:363)
	at org.apache.cassandra.thrift.Column.read(Column.java:528)
	at org.apache.cassandra.thrift.ColumnOrSuperColumn.read(ColumnOrSuperColumn.java:507)
	at org.apache.cassandra.thrift.KeySlice.read(KeySlice.java:408)
	at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12422)
	at com.rockmelt.org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:696)
	at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:680)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.maybeInit(ColumnFamilyRecordReader.java:362)
	... 16 more


In ColumnFamilyRecordReader#initialize, a TBinaryProtocol is created as follows:

TTransport transport = ConfigHelper.getInputTransportFactory(conf).openTransport(socket, conf);
TBinaryProtocol binaryProtocol = new TBinaryProtocol(transport, ConfigHelper.getThriftMaxMessageLength(conf));
client = new Cassandra.Client(binaryProtocol);

But each time a call to cassandra is made, checkReadLength(int length) is called in TBinaryProtocol, which includes this:

readLength_ -= length;
if (readLength_ < 0) {
   throw new TException(""Message length exceeded: "" + length);
}

The result is that readLength_ is decreased each time, until it goes negative and exception is thrown.  This will only happen if you're reading a lot of data and your split size is large (which is maybe why people haven't noticed it earlier).  This happens regardless of whether you use wide row support.

I'm not sure what the right fix is.  It seems like you could either reset the length of TBinaryProtocol after each call or just use a new TBinaryProtocol each time.",,jjordan,rtimpe,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/May/13 04:58;jbellis;5529-1.1.txt;https://issues.apache.org/jira/secure/attachment/12581335/5529-1.1.txt","01/May/13 04:39;jbellis;5529.txt;https://issues.apache.org/jira/secure/attachment/12581334/5529.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,325929,,,Tue May 21 18:55:39 UTC 2013,,,,,,,,,,"0|i1k8a7:",326274,,,,,,,,,tjake,,tjake,Normal,,,,,,,,,,,,,,,,,,"01/May/13 04:39;jbellis;Rob, your analysis looks spot on.

WTF.  Creating a new TBinaryProtocol for each message would be pretty ludicrous.

The genesis of this readLength_ business is hidden in the murky archives of the Thrift incubator svn repro.  It looks to me like it's kind of a really ugly hack for pre-Framed transports that could call setReadLength in between messages based on some kind of per-application knowledge.  Because I can't think of any use for ""expiring"" a connection after X bytes otherwise.

I don't think we should be using it at all.  Attached is a patch that rips it out, on the Cassandra server side as well.  I feel sorry for any poor bastard who ever pulled his hair out over Cassandra erroring out his connection apparently randomly...;;;","01/May/13 04:58;jbellis;Thought I was on the 1.1 branch when I wrote that patch but it was really 1.2.  Here it is against 1.1 as well.;;;","01/May/13 05:04;jbellis;Note to self: remove our hacked TBinaryProtocol entirely in trunk.;;;","01/May/13 07:06;rtimpe;Thanks for the patch and the quick turnaround.  Verified on the 1.1 branch that it fixes my problem.

I'm not really familiar with this api, hence my notes about TBinaryProtocol.  You solution makes way more sense :);;;","21/May/13 03:50;jbellis;[~tjake] Can you review above?;;;","21/May/13 16:43;tjake;Looks fine.  I filed THRIFT-1975 to get this issue fixed in general
;;;","21/May/13 18:55;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prevent repair among the nodes of different version,CASSANDRA-5523,12645323,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,29/Apr/13 22:21,16/Apr/19 09:32,14/Jul/23 05:53,02/Aug/13 03:28,,,,,,,0,repair,,,,"Since streaming file to the node of different version is not allowed, and in fact it would be the cause of repair hang, there is no point to allow repairing among the nodes of different versions.
",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"29/Apr/13 22:24;yukim;5523-1.2.txt;https://issues.apache.org/jira/secure/attachment/12581059/5523-1.2.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,325685,,,Fri Aug 02 03:28:39 UTC 2013,,,,,,,,,,"0|i1k6s7:",326030,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"29/Apr/13 22:24;yukim;Patch to fail repair when the participants' versions are different.
Note that we don't need to check if the node is pre-1.1 for sequential repair, I removed that check also.;;;","29/Apr/13 22:35;jbellis;I would change the message to ""different protocol versions"" or ""different major releases"" since different minor releases should be okay.

Otherwise +1;;;","30/Apr/13 19:30;yukim;Committed with the message fix. Thanks!;;;","14/May/13 02:02;yukim;The fix was unreliable because every time connection is closed, protocol version for that connection is also reset (set to null), leading to not able to repair randomly.
Reverted from 1.2 branch and trunk for now.;;;","02/Aug/13 03:28;jbellis;Wontfixing since we should be able to do cross-version repair in 2.0.  Please reopen if I've misunderstood.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
move IndexSummary off heap,CASSANDRA-5521,12645079,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,vijay2win@yahoo.com,jbellis,jbellis,28/Apr/13 14:42,16/Apr/19 09:32,14/Jul/23 05:53,03/May/13 08:42,2.0 beta 1,,,,,,0,,,,,IndexSummary can still use a lot of heap for narrow-row sstables.  (It can also contribute to memory fragmentation because of the large arrays it creates.),,christianmovi,jal06,jeromatron,rcoli,vijay2win@yahoo.com,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,325441,,,Fri May 03 08:42:43 UTC 2013,,,,,,,,,,"0|i1k59z:",325786,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"28/Apr/13 20:21;vijay2win@yahoo.com;Pushed the changes to https://github.com/Vijay2win/cassandra/commits/5521

The idea is as mentioned in CASSANDRA-5506 comments, Since this ticket is marked for 2.0 i took the liberty of changing the index summary file format.
I am going to spend sometime on testing the performance difference (If any). Thanks!;;;","29/Apr/13 03:23;jbellis;Seems inefficient to make the ""get pair"" the unit of fetching memory, since usually you want the key or the position but not both.  (You wouldn't have to change the signature of getKey either, if you just fetched the key into a byte[] directly instead of wrapping half of it.)

What's the upgrade path?  Would prefer ""automatically rebuilds old summaries"" to ""user has to manually blow away summaries or it dies trying to start."";;;","29/Apr/13 03:37;vijay2win@yahoo.com;{quote}
Seems inefficient to make the ""get pair"" the unit of fetching memory,
{quote}
Ahaa good point will fix it.

{quote}
What's the upgrade path? Would prefer ""automatically rebuilds old summaries"" to ""user has to manually blow away summaries or it dies trying to start.""
{quote}
It is automatic, but until the user runs scrub or until the new SST's are created the startup will be slow.;;;","30/Apr/13 07:51;vijay2win@yahoo.com;Pushed an update to https://github.com/Vijay2win/cassandra/commits/5521_v2

This update doesn't use int[] which v1 used, v2 uses kind of offheap index over index summary which is stored at the beginning of the memory block... Moves index summary completely offheap... :)

I was not able to see any difference in performance between trunk and v2 using stress tool. 
Micro benchmark shows v2 is 6 seconds slower for 20 Bilion get's hence it is was not noticeable in stress tool, but the real benefit is less pauses and more data a node can hold...;;;","01/May/13 03:53;xedin;Have we considered using vint encoding on those arrays as we keep them in memory anyway to minimize space consumption?

Edit: i remember now why that is not a good idea :) I wonder though how what could memory footprint be if we use TreeMap inside and keys and offsets (in vint encoding) saved in native memory...;;;","01/May/13 06:17;xedin;I see v2 does byte[] allocation on every getKey(int) call which would be happening very frequently due to binary search which happens on very index lookup. So I don't think there is any real benefit in terms of GC friendliness from moving off-heap in this case as we have to copy data over multiple times anyway.

As an alternative to Unsafe we can try hybrid approach - identify if JNA is present and put summaries off-heap (using JNA's Memory) in combination with Pointer.getByteBuffer() which doesn't copy any data around but instead creates direct ByteBuffer, otherwise have IndexSummary on-heap but split byte[][] and long[] into pages so we don't have to allocate contiguous space for big SSTables which would be much GC friendlier. 

;;;","01/May/13 13:46;jbellis;As Pavel notes, the most important use of getKey is the one in binarySearch.  But here we only care about the comparison, we don't actually need the artifact of a ByteBuffer.  So why not compare directly without creating a buffer first?  No buffer at all is even cheaper than a native buffer.

(This would also mean that we only need to look at as many bytes as it takes before the first difference is found.);;;","01/May/13 16:12;jbellis;bq. So why not compare directly without creating a buffer first?

That doesn't quite work, since we need to compare the Token, not the key itself.

So, revised suggestion: allow the partitioner to decorate a Memory location, instead of forcing us to create a ByteBuffer first.;;;","01/May/13 23:54;xedin;if we do so we would have to change partitioner interface to decorateKey and getToken, DecoratedKey to have two ""key"" fields and change at least MurmurHash to accept both BB and M at the same time. I'm my opinion it's just too many changes just because we don't require JNA but with hybrid approach we don't have to do any of that work. Besides mentioned users would want to run with JNA in production anyway.;;;","02/May/13 01:58;jbellis;Just add a IPartitioner.compareToken method that does what we need, then.  Much better than creating extra objects that we don't care about.

I like where we are now, with JNA being mostly optional (more so in 2.0 where we require Java7, so we don't need JNA for snapshot).  Remember, we don't support JNA at all on Windows.  I'd rather use less JNA than more.;;;","02/May/13 02:11;xedin;But don't you still have to generate token from Memory and make changes to getToken(BB) and underlying methods or was is proposed interface for compareToken?

bq. I like where we are now, with JNA being mostly optional (more so in 2.0 where we require Java7, so we don't need JNA for snapshot). Remember, we don't support JNA at all on Windows. I'd rather use less JNA than more.

I just to clarify, the only thing we need from JNA in this case is Pointer.getByteBuffer() which is actually a JNI method but unfortunately Unsafe doesn't have it somehow :(

I agree tho that we should rely less on JNA but in this case we still pay the price of memory usage even putting off-heap so in non-JNA case we make it GC/allocation friendly it still makes a good improvement overall with almost no code changes.;;;","02/May/13 02:37;jbellis;Thinking about it, {{getToken(Memory, offset, length)}} is probably the right thing to add to IPartitioner.  The rest can live in IndexSummary.  That doesn't sound like a huge burden to me.  And as I said, creating no Buffer (or DK) objects is better than creating native ones. :);;;","02/May/13 03:08;xedin;So tokens that keep bb around right now would have to keep Memory and offset, size references? I'm not against this, just trying to clarify to myself if we would rather want to keep some kind of ROBuffer container for DK and Token to unify interface...;;;","02/May/13 03:23;jbellis;No, the Token wouldn't share any bytes with the key (with the exception of BOP, and I don't care about optimizing for that case), so there's no reason to not create a regular, on-heap Token.

Edit: keeping in mind that we don't actually need a full DK object, we just need a Token.;;;","02/May/13 04:03;vijay2win@yahoo.com;Honestly, glad to see the thread going in the same thinking process which i went though.... 

Changing the Partitioner is a bigger change... but before we go there, wondering if this optimization is going to help us? 
For BB is not cheap, but it is going to be good garbage which will live and die in young generation.

I can think of 2 other options...
1) We can serialize and deserialize Token in IndexSummary we still need additional function to serialize and deserialize from memory (for BOP we can serialize the key/byte[], we have also removed the token calculation overhead) so we can also try and compare incrementally.
2) We can use MMappedFile instead and get ByteBuffer (this could work in our favor, for the new SST's which is never queried there is zero overhead in memory ) :);;;","02/May/13 04:56;xedin;bq. For BB is not cheap, but it is going to be good garbage which will live and die in young generation.

Indeed, those are just containers so actual data is not copied and those buffers pretty GC friendly as you mentioned. But I think that option with using Memory with token is ok if we can encapsulate it properly.;;;","02/May/13 16:17;jbellis;bq. Changing the Partitioner is a bigger change

It does get ugly since you'd need to reimplement Murmur3.hash3_x64_128 on Memory objects.  (Not for the first time, I'm pissed that ByteBuffer isn't an interface...)

Let's go ahead and move forward with v2 and optimize later if we need to.

Nits:
# rename hasSummaries to offHeapSummaries
# InputStream.read has an overload that takes a length parameter, you don't need to realloc the buffer
# The comment doesn't match the code here.  Also, getIndex should be private.
{code}
.       // multiply by 4 and add the block start
        return bytes.getInt(index << 2);
{code}
# We can easily inline DK.compareTo instead of actually creating a DK object (i.e., call partitioner.getToken instead, then compare the tokens and keys without the DK wrapper)

The rest LGTM.
;;;","03/May/13 08:42;vijay2win@yahoo.com;Committed to Trunk!

Added following function to DK to support RowPosition

{code}
public static int compareTo(IPartitioner partitioner, ByteBuffer key, RowPosition position)
    {
        // delegate to Token.KeyBound if needed
        if (!(position instanceof DecoratedKey))
            return -position.compareTo(partitioner.decorateKey(key));

        DecoratedKey otherKey = (DecoratedKey) position;
        int cmp = partitioner.getToken(key).compareTo(otherKey.getToken());
        return cmp == 0 ? ByteBufferUtil.compareUnsigned(key, otherKey.key) : cmp;
    }
{code}

Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Reduce index summary memory use for cold sstables,CASSANDRA-5519,12645053,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,thobbs,jbellis,jbellis,27/Apr/13 22:03,16/Apr/19 09:32,14/Jul/23 05:53,22/Nov/13 18:13,2.1 beta1,,,,,,0,,,,,,,cburroughs,christianmovi,colinkuo,jeromatron,jjordan,rcoli,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6379,CASSANDRA-5515,"21/Nov/13 21:32;thobbs;5519-v1.txt;https://issues.apache.org/jira/secure/attachment/12615198/5519-v1.txt","22/Oct/13 22:53;thobbs;downsample.py;https://issues.apache.org/jira/secure/attachment/12609747/downsample.py",,,,,,,,,,,,,,,,,,,2.0,thobbs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,325415,,,Fri Nov 22 18:13:25 UTC 2013,,,,,,,,,,"0|i1k547:",325760,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"01/Aug/13 19:47;thobbs;An initial idea for the implementation:

Based on the recent (last 15m?) read rate (reads/sec), periodically down-sample the summary for SSTables which fall below the mean rate.  The down-sampling rate could use a sliding scale based on the ratio of the mean to that SSTable's rate.  As a example basic implementation, keep X% of the samples, where {{X = max(25, min(100, 100 * (rate / mean_rate)))}}, so the coldest SSTables keep only 25% of the samples in memory.

Presenting a way for the user to tune this (other than a simple on/off) is a little trickier.  Perhaps make the min (default 25%) adjustable?  Or start down-sampling at a configurable point (the default is the mean)?  Those could also be automatically adjusted based on memory pressure.;;;","07/Aug/13 02:41;jbellis;Good start, but it seems a little fragile to me if a bunch of sstables are suddenly warmed up.

What about this?

We could define a fixed-size memory pool, similar to what we do for memtables or cache, and allocate it to the sstables proportional to their hotness.  Every 15 minutes (which seems like a lot, maybe hourly?) we recalculate and rebuild the summaries.  Maybe we only rebuild the ones that are X% off of where they should be to make it lighter-weight.  Or if we're downsampling by more than 2x then we can just resample what we already have in memory instead of rebuilding ""correctly."";;;","07/Aug/13 19:03;thobbs;bq. We could define a fixed-size memory pool, similar to what we do for memtables or cache, and allocate it to the sstables proportional to their hotness.

It would be hard to describe this in text, so here's my pythonic psuedocode for distributing the fixed-size memory pool:

{noformat}
total_reads_per_sec = sum(sstable.reads_per_sec for sstable in sstables)
sstables_to_downsample = set()
leftover_entries = 0
for sstable in sstables:
    allocated_space = total_space * (sstable.reads_per_sec / total_reads_per_sec)
    num_entries = total_space / (SPACE_PER_ENTRY)  # space per entry = token + position + overhead
    if (num_entries > sstable.max_index_summary_entries):
        sstable.num_index_summary_entries = max_index_summary_entries
        leftover_entries = num_entries - sstable.max_index_summary_entries
    else
        sstable.num_index_summary_entries = num_entries
        sstables_to_downsample.add(sstable)

# distribute leftover_entries among sstables_to_downsample based on read rates
# (this probably ends up looking like a recursive or iterative function)
{noformat}

bq. Maybe we only rebuild the ones that are X% off of where they should be to make it lighter-weight.

That's a good idea. (I was thinking of using a step function.)  Instead of ""X% off of where they should be"", I would more precisely phrase that as ""X% away from their previous proportion"".

bq.  Or if we're downsampling by more than 2x then we can just resample what we already have in memory instead of rebuilding ""correctly.""

If you down-sample with a particular pattern, you can always down-sample using just the in-memory points; only up-samples need to read from disk.

I'm trying to generalize the down-sampling pattern, but the two main points are (assuming 1% granularity):
* For every 1% you down-sample, the number of points to remove from the in-memory summary is equal to 1% of the original (on-disk) count
* Each 1% down-sampling run starts at a different offset to evenly space the down-sampling

For example, to down-sample from 100% to 99%, you would remove every hundredth point, starting from index 0.  To down-sample from 99% to 98%, you would remove every 99th point, starting from index 50.  To down-sample from 98% to 97%, you would remove every 98th point, starting from index 24 or 74, and so on.;;;","08/Aug/13 15:21;jbellis;Sounds reasonable.;;;","22/Oct/13 22:53;thobbs;The attached downsample.py script demonstrates the downsampling algorithm.  It's a touch complex, but it would be easy to precompute or cache the downsampling patterns if needed.

An example run with an original index summary size of 16 and a ""resolution"" of 8, meaning each minimal downsample run will remove 1/8th of the original points.  The top row is the original index summary and each row below that represents one downsampling run:

{noformat}
~ $ ./downsample.py 16 8
  0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15
      1   2   3   4   5   6   7       9  10  11  12  13  14  15
      1   2   3       5   6   7       9  10  11      13  14  15
      1       3       5   6   7       9      11      13  14  15
      1       3       5       7       9      11      13      15
              3       5       7              11      13      15
              3               7              11              15
{noformat};;;","22/Oct/13 23:20;jbellis;LGTM;;;","31/Oct/13 22:20;thobbs;How do we want to handle the memory pool not being large enough to accommodate all of the index summaries (even after downsampling)?  Just make it a best-effort?;;;","12/Nov/13 23:34;thobbs;I need to put this through more thorough testing and benchmarking, but I think it's at a good point for a preliminary review: https://github.com/thobbs/cassandra/compare/CASSANDRA-5519

A few comments/questions:
* I went with best-effort for the memory pool (if all summaries don't fit in the allotted space even at the minimum sampling level, there's nothing we can do about it).  The amount of memory used may also temporarily exceed the limit while building new summaries.
* There are two new cassandra.yaml options: one for controlling the memory pool size and one for regulating how frequently summaries are resized.  These can also be set through JMX. We could conceivably also make the down/upsample thresholds and the minimum sampling level configurable.  All of these default values are just guesses.
* I went with a reference counting strategy for free'ing the IndexSummary's Memory.  This makes the API a bit unpleasant (mostly in SSTR), but it should have low overhead.  A ReadWriteLock might also work well instead of this with a cleaner API; let me know if I should benchmark the two for comparison.
* I'm triggering the IndexSummaryManager singleton's initialization in DatabaseDescriptor; this feels wrong, so I'm open to suggestions.;;;","13/Nov/13 23:49;jbellis;What is the relationship between BASE_SAMPLING_LEVEL and MIN_SAMPLING_LEVEL with indexInterval?

How many rows do we get for 5% of a 8GB heap?

Isn't it a minor bug to just ignore compacting sstables?  Suggest reducing memory pool to allocate to the uncompacting ones, by the amount allocated to the compacting ones.

Could we just resample at compaction time instead of dealing with refcounting or locking?  That probably gives up too much of the potential benefits.  But I think we could make it almost as elegant by using the datatracker replace mechanism originally for compaction, to build a new SSTR and swap it in w/o extra concurrency controls.

Is the idea behind touching it in DD to force the mbean to be loaded, or is there a circular dependency that breaks w/o that?;;;","14/Nov/13 17:06;thobbs;bq. What is the relationship between BASE_SAMPLING_LEVEL and MIN_SAMPLING_LEVEL with indexInterval?

{{BASE/MIN_SAMPLING_LEVEL}} are orthogonal to {{indexInterval}}.  {{BASE_SAMPLING_LEVEL}} essentially sets the granularity at which you can down/upsample.  {{MIN_SAMPLING_LEVEL}} sets a limit on how low you can downsample.  (I'll note that we could potentially raise {{indexInterval}} alongside these changes in order to have more summary entries for hot sstables.)

bq. How many rows do we get for 5% of a 8GB heap?

That gives us ~410 MiB to work with.  If we assume the average key length is 8 bytes, each summary entry uses 20 bytes of space, giving us ~21 million summary entries.

At full sampling, that's 21MM * 128 = 2.7 billion rows, assuming no overlap across sstables. At minimum sampling, that's ~11 billion rows.

If the avg key size is 16 bytes, that drops to ~2 and ~8 billion rows.

bq. Isn't it a minor bug to just ignore compacting sstables? Suggest reducing memory pool to allocate to the uncompacting ones, by the amount allocated to the compacting ones.

Good point, I agree.

bq. Could we just resample at compaction time instead of dealing with refcounting or locking? That probably gives up too much of the potential benefits.

Yeah, that would probably be okay for small sstables that are compacted frequently, but the large sstables would be tuned poorly, and those make up the majority of the memory use.

bq. I think we could make it almost as elegant by using the datatracker replace mechanism originally for compaction, to build a new SSTR and swap it in w/o extra concurrency controls.

That's a good idea; I think it would be fairly clean.  I'll give that a shot.

bq. Is the idea behind touching it in DD to force the mbean to be loaded, or is there a circular dependency that breaks w/o that?

Neither the {{IndexSummaryManager}} singleton nor the mbean are loaded without that.  No other classes use the {{IndexSummaryManager}},
so the static fields are never initialized.  (Just importing the classes doesn't seem to trigger the class loader.);;;","14/Nov/13 23:12;jbellis;Pushed my cleanup to https://github.com/jbellis/cassandra/commits/5519.

(Moved the ISM init to StorageService were we have some existing examples of similar.);;;","14/Nov/13 23:21;jbellis;Rather than expose MSL directly as a config option, how about changing index_interval to max_index_interval and adding a min_index_interval?  We could compute (as close as possible) MSL from min_index_interval.

(I don't think users will need to tune BSL.  128 lets us be accurate to with in 1% which seems totally reasonable to me.);;;","14/Nov/13 23:39;thobbs;The cleanup looks good overall, thanks.

bq. Rather than expose MSL directly as a config option, how about changing index_interval to max_index_interval and adding a min_index_interval? We could compute (as close as possible) MSL from min_index_interval.

That sounds good to me.

bq. (I don't think users will need to tune BSL. 128 lets us be accurate to with in 1% which seems totally reasonable to me.)

Agreed.;;;","19/Nov/13 23:00;thobbs;Would it be alright to split the replacement of {{index_interval}} by {{max_index_interval}} and {{min_index_interval}} into another ticket just for sanity's sake?  It looks like a lot of changes need to be done for that, and they're independent of the changes for this ticket.;;;","19/Nov/13 23:39;jbellis;WFM.  What does that leave for this one?;;;","19/Nov/13 23:44;thobbs;bq. WFM. What does that leave for this one?

I still need to account for spaced used by compacting SSTables, and I'm putting it through some more thorough testing.;;;","19/Nov/13 23:46;thobbs;Created CASSANDRA-6379 for the {{index_interval}} changes.;;;","21/Nov/13 18:08;thobbs;This should be good for a second round of reviewing.  I opened a pull request against my own repo so that you can comment inline, if you'd like: https://github.com/thobbs/cassandra/pull/1

Changes since the last review:
* The entire {{SSTableReader}} is replaced instead of just the IndexSummary.
* Space used by compacting SSTables is accounted for
* Enough extra space is reserved to cover rebuilding the largest summary
* In order to stay within the memory usage limit on startup, the on-disk Summary is replaced whenever it is resampled.  I increased the threshold for downsampling to make this less frequent.  The alternative would be to always keep the full summary on disk and have a somewhat more complicated startup procedure.  I would appreciate your thoughts on this.;;;","21/Nov/13 18:16;thobbs;I should also mention that if you want to test it out, I suggest setting logging to TRACE for o.a.c.io.sstable.IndexSummary manager, {{index_summary_capacity_in_mb}} to 1, and {{index_summary_resize_interval_in_minutes}} to 1.  That should give you a good picture of what's going on.;;;","21/Nov/13 18:37;jbellis;bq. the on-disk Summary is replaced whenever it is resampled

Good call; startup time is a big pain point for some people and we don't want to make that worse.;;;","21/Nov/13 21:32;thobbs;[~jbellis] Attached patch 5519-v1.txt includes your suggested changes.  (My CASSANDRA-5519 branch is still good, as well.);;;","21/Nov/13 22:19;jbellis;I'm pretty sure we can get rid of the isReplaced flag now.;;;","21/Nov/13 22:34;thobbs;bq. I'm pretty sure we can get rid of the isReplaced flag now.

We still need it in order to do the proper cleanup on the replaced SSTR once all references are released, unless I'm missing something.;;;","21/Nov/13 23:36;jbellis;Hmm.  I see two uses of isReplaced:

# releaseReference, which can be reverted back to trunk form since isReplaced == !isCompacted
# close, which is only called by snapshot repair (and releaseReference) which will never do any index summary replacements;;;","22/Nov/13 17:17;thobbs;bq. releaseReference, which can be reverted back to trunk form since isReplaced == !isCompacted

True

bq. close, which is only called by snapshot repair (and releaseReference) which will never do any index summary replacements

We still need to have different behavior for the {{close()}} call by snapshot repair, as it needs to perform the full close even though {{isCompacted}} will be false.  While we could add a parameter to close() or define a {{closeReplacedReader()}} method, it seems clearer and more future-proof to keep the isReplaced flag.;;;","22/Nov/13 18:13;jbellis;WFM.  Committed!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra crashes at start with segmentation fault,CASSANDRA-5517,12644819,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,urandom,sknaumov,sknaumov,26/Apr/13 11:57,16/Apr/19 09:32,14/Jul/23 05:53,15/Aug/13 19:15,,,,,,,0,,,,,"Sometimes Cassandra fails at start with segmentation fault:

# /usr/sbin/cassandra -f
xss =   -ea -javaajent:/usr/share/cassandra/lib/jamm-0.2.5.jar -XX:+UseThreadPriorities -XX:ThreadPriorityPolicy=42 -Xms1024M -Xmx1024M -Xmn100M -XX:+HeapDumpOnOutOfMemoryError -Xss180k
Segmentation fault

It seems that not only me encountered this bug: http://snapwebsites.org/known-issues/cassandra-crashes-java-segmentation-fault

Solution proposed on this link works.","VirtualBox 4.2.6 VM with 4GB RAM, Xubuntu 12.10 as host and guest OS.
Cassandra 1.2.4 installed on guest as Debian package.",cscetbon,jeromatron,rcoli,sknaumov,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5895,,,,,,,,,CASSANDRA-2441,,,,,,,,,,,"08/Jul/13 18:31;urandom;0001-CASSANDRA-5517-up-xss-to-256k-for-openjdk-1.6.patch;https://issues.apache.org/jira/secure/attachment/12591246/0001-CASSANDRA-5517-up-xss-to-256k-for-openjdk-1.6.patch","08/Jul/13 18:31;urandom;0002-optional-upgrade-dependency-to-OpenJDK-7.patch;https://issues.apache.org/jira/secure/attachment/12591247/0002-optional-upgrade-dependency-to-OpenJDK-7.patch",,,,,,,,,,,,,,,,,,,2.0,urandom,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,325182,,,Wed Nov 27 17:04:16 UTC 2013,,,,,,,,,,"0|i1k3of:",325527,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"26/Apr/13 12:52;jeromatron;What version of Java are you running?  Can you paste the output of running java -version?;;;","29/Apr/13 05:56;sknaumov;# java -version
java version ""1.6.0_27""
OpenJDK Runtime Environment (IcedTea6 1.12.3) (6b27-1.12.3-0ubuntu1~12.10.1)
OpenJDK 64-Bit Server VM (build 20.0-b12, mixed mode);;;","29/Apr/13 11:15;jeromatron;Can you try to see if you can reproduce the problem while running with a Sun/Oracle 1.6 JDK (non-openjdk), one of the more recent versions?  In the 1.6 line, openjdk is pretty behind the Sun/Oracle JDK.  I wouldn't be surprised if it was just an oddity with openjdk.;;;","08/Jul/13 18:36;urandom;The attached patch, 0001-CASSANDRA-5517-up-xss-to-256k-for-openjdk-1.6, raises -Xss to 256k for OpenJDK 1.6 only.

Optionally, 0002-optional-upgrade-dependency-to-OpenJDK-7 raises the Debian package dependency to OpenJDK 1.7.  You could argue it unwise to change this in a point release, but OpenJDK 1.7 seems to be a _much_ better default at this point.;;;","10/Aug/13 21:23;jbellis;According to https://github.com/apache/cassandra/pull/18, recent Oracle JDK7 also needs a larger stack.  Maybe we should just increase it to 256 across the board; with a fairly solid native protocol + HSHA story, keeping per-thread memory down isn't quite as important as it was when we started.

+1 on switching to OpenJDK 1.7 for Debian.;;;","15/Aug/13 13:39;urandom;
{quote}
According to https://github.com/apache/cassandra/pull/18, recent Oracle JDK7 also needs a larger stack. Maybe we should just increase it to 256 across the board; with a fairly solid native protocol + HSHA story, keeping per-thread memory down isn't quite as important as it was when we started.
{quote}

That sounds reasonable to me, are we confident enough in this change to land it in cassandra-2.0.0? Or cassandra-2.0 (my comfort sweet-spot, I think)?;;;","15/Aug/13 15:00;jbellis;2.0 WFM.;;;","15/Aug/13 19:15;urandom;committed to cassandra-2.0 and trunk; closing;;;","27/Nov/13 17:04;cscetbon;I think you should update JVM_OPTS in test/cassandra.in.sh too and the comment message in conf/cassandra-env.sh to :
u45 and greater need need 256k;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
forceTablePrimaryRange fails with nullpointer exception,CASSANDRA-5512,12644433,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,etobgra,etobgra,24/Apr/13 20:21,16/Apr/19 09:32,14/Jul/23 05:53,25/Apr/13 17:38,1.2.5,,,Legacy/Tools,,,0,,,,,"Running JMX operation forceTableRepairRange fails with nullpointer exception.
Three node cluster with one keyspace and one large columnfamily.

Works when running nodetool -pr but not over JMX.
Stacktrace:

ERROR [MiscStage:1] 2013-04-24 09:53:02,884 CassandraDaemon.java (line 164) Exception in thread Thread[MiscStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.SnapshotVerbHandler.doVerb(SnapshotVerbHandler.java:38)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)","Linux red hat, java 1.6.43
three nodes",etobgra,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/13 21:39;yukim;5512-1.2.txt;https://issues.apache.org/jira/secure/attachment/12580389/5512-1.2.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,324800,,,Thu Apr 25 17:38:58 UTC 2013,,,,,,,,,,"0|i1k1bz:",325146,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"24/Apr/13 20:40;etobgra;UPDATE
======
The jmx operation is successfully invoked on node 1 according to system.log (AntiEntropySession started) but the other two nodes generates nullpointer exception.

;;;","24/Apr/13 21:18;jbellis;Unclear to me how you can generate a null snapshot message.  Can you test with 1.2 git branch?  May be fixed by CASSANDRA-5424.

In the meantime you can also pass isSequential=false to avoid creating snapshots.;;;","24/Apr/13 21:39;yukim;forceTablePrimaryRange's second boolean parameter is for sequential repair using snapshot(you can do this with nodetool repair -snapshot too), and when it's on, looks like it fails as described.

We somehow omitted registering Snapshot message's serializer in 1.2 branch. Trivial patch attached.;;;","24/Apr/13 22:12;jbellis;Good eyes! +1;;;","25/Apr/13 05:26;etobgra;Great!
Thank you very much for the quick response.
;;;","25/Apr/13 17:38;yukim;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Clean up backwards compatibility complexity for 2.0,CASSANDRA-5511,12644273,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,24/Apr/13 03:37,16/Apr/19 09:32,14/Jul/23 05:53,26/Apr/13 19:07,2.0 beta 1,,,,,,0,,,,,"We've supported rolling upgrades (network-compatible for read/write operations) for several releases, but both 1.0 -> 1.1 and 1.1 -> 1.2 required being on a recent release of the immediately prior major series for this to work as desired.

Meanwhile, we still support reading sstables at least back to 0.6 and possibly even earlier.  This makes dealing with changes to the sstable quite challenging; the recently written-and-reverted CASSANDRA-5487 comes to mind.

2.0 is a good place to drop support for sstables older than 1.2.5.  Our experience with network compatibility demonstrates that this is not an unreasonable burden to impose, and the major version number change suggests that this is a logical time to make such a change.",,marcuse,slebresne,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/13 20:54;marcuse;CASSANDRA-5511-on_1.2.patch;https://issues.apache.org/jira/secure/attachment/12580377/CASSANDRA-5511-on_1.2.patch","24/Apr/13 20:54;marcuse;CASSANDRA-5511-on_trunk.patch;https://issues.apache.org/jira/secure/attachment/12580376/CASSANDRA-5511-on_trunk.patch",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,324640,,,Fri Apr 26 19:07:52 UTC 2013,,,,,,,,,,"0|i1k0c7:",324985,,,,,,,,,marcuse,,marcuse,Normal,,,,,,,,,,,,,,,,,,"24/Apr/13 04:43;jbellis;Work-in-progress pushed to https://github.com/jbellis/cassandra/commits/5511.

Stuck on two things:

# Not sure how [~krummas] generated the test sstable for LegacyLeveledManifestTest; need to generate a new one at version ic
# Wanted to clean up messages too, but not sure why SuperColumns.java hardcodes VERSION_10.  [~slebresne]?;;;","24/Apr/13 06:08;marcuse;iirc i created a throwaway unit test that does a few RMs with a breakpoint to stop SchemaLoader from removing the sstable, then copied it into the legacy dir

want me to do it again for ic?;;;","24/Apr/13 06:12;marcuse;does this mean we will support upgrading 1.2 -> 2.1? would be nice if we required 2.0 as well, to get rid of the deprecated json leveled manifest code;;;","24/Apr/13 06:35;slebresne;bq. not sure why SuperColumns.java hardcodes VERSION_10

That was an admittedly clumsy way to have just the DeletionTime of the DeletionInfo serialized (i.e. without serializing the range tombstones part at all (that should be empty anyway)). This can be replaced by a direct call to the DeletionTime serializer. ;;;","24/Apr/13 12:33;jbellis;bq. want me to do it again for ic?

That would be great.  (Ideally I'd like to leave the code to generate it commented out like we do with LegacySSTableTest.)

bq. does this mean we will support upgrading 1.2 -> 2.1?

I guess we can see how much it bugs us by then. :);;;","24/Apr/13 20:54;marcuse;adds an ignored test to 1.2 that generates sstables and refactors the test in trunk

- apply 1.2-patch
- merge 1.2 -> trunk
- apply trunk-patch

if the patches look ok, i can do the git-gymnastics (ie, they can be applied on current trunk/1.2 without breaking anything);;;","25/Apr/13 15:10;jbellis;Updates pushed to https://github.com/jbellis/cassandra/commits/5511-2.  Done except for LLMT, I think.;;;","25/Apr/13 15:24;jbellis;Oops, missed Marcus's update somehow.  Will have a look.;;;","25/Apr/13 15:52;jbellis;Marcus's patches LGTM.;;;","25/Apr/13 19:41;jbellis;Rebased and pushed to https://github.com/jbellis/cassandra/commits/5511-3.  Tests pass.;;;","26/Apr/13 09:32;marcuse;* we can remove Directories.sstablesNeedMigration(...) (and related)
* remove DefsTable.fixSchemaNanoTimestamps()
* SystemTable.upgradeSystemData() can be cleaned up (LocationInfo is gone in 1.2 right?)
* Remove SystemTable.OLD_STATUS_CF and SystemTable.OLD_HINTS_CF
* CFMetaData has a bunch of @Deprecated fields that can probably be removed.
* Nit: remove sstable.decorateKey(..) and use sstable.partitioner.decorateKey(...) everywhere
* why keep _SHA in FilterFactory.Type? (couldnt find any .ordinal() use)
* Do we use MURMUR2 anywhere?
* I guess we can remove everything testing ""version < MessagingService.VERSION_12"" (or throw appropriate exceptions etc)
;;;","26/Apr/13 16:08;jbellis;Pushed commits to address above, thanks!;;;","26/Apr/13 17:27;marcuse;lgtm!;;;","26/Apr/13 19:07;jbellis;committed!

{{121 files changed, 370 insertions(+), 2257 deletions(-)}};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
The native protocol server is not correctly stopped on shutdown,CASSANDRA-5507,12644081,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,23/Apr/13 09:57,16/Apr/19 09:32,14/Jul/23 05:53,23/Apr/13 11:52,1.2.5,,,,,,0,,,,,,,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Apr/13 09:59;slebresne;5507.txt;https://issues.apache.org/jira/secure/attachment/12580015/5507.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,324448,,,Tue Apr 23 11:52:45 UTC 2013,,,,,,,,,,"0|i1jz5j:",324793,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"23/Apr/13 09:59;slebresne;Trivial patch attached (we were only stopping the RPC server).;;;","23/Apr/13 11:21;aleksey;+1;;;","23/Apr/13 11:52;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Eternal iteration when using older hadoop version due to next() call and empty key value,CASSANDRA-5504,12643918,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,ifesdjeen,ifesdjeen,ifesdjeen,22/Apr/13 11:50,16/Apr/19 09:32,14/Jul/23 05:53,25/Apr/13 23:02,1.2.5,,,,,,2,,,,,"Currently, when using newer hadoop versions, due to the call to 

next(ByteBuffer key, SortedMap<ByteBuffer, IColumn> value)

within ColumnFamilyRecordReader, because `key.clear();` is called, key is emptied. That causes the StaticRowIterator and WideRowIterator to glitch, namely, when Iterables.getLast(rows).key is called, key is already empty. This will cause Hadoop to request the same range again and again all the time.

Please see the attached patch/diff, it simply adds lastRowKey (ByteBuffer) and saves it for the next iteration along with all the rows, this allows query for the next range to be fully correct.

This patch is branched from 1.2.3 version.

Tested against Cassandra 1.2.3, with Hadoop 1.0.3, 1.0.4 and 0.20.2",,budlight,christianmovi,lannyripple,qwertymaniac,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Apr/13 22:47;jbellis;5504-v3.txt;https://issues.apache.org/jira/secure/attachment/12580610/5504-v3.txt","22/Apr/13 11:56;ifesdjeen;patch.diff;https://issues.apache.org/jira/secure/attachment/12579825/patch.diff","22/Apr/13 17:09;ifesdjeen;patch2.diff;https://issues.apache.org/jira/secure/attachment/12579852/patch2.diff",,,,,,,,,,,,,,,,,,3.0,ifesdjeen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,324285,,,Sat Jun 29 20:55:00 UTC 2013,,,,,,,,,,"0|i1jy5r:",324630,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"22/Apr/13 15:47;budlight;I believe this issue also affects 1.1.11 and 1.2.4,  I tried both had the same issue as on http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Thrift-message-length-exceeded-td7587006.html and http://stackoverflow.com/questions/15487540/pig-cassandra-message-length-exceeded

I then tried 1.1.9 (skipped .10) and it worked correctly;;;","22/Apr/13 16:48;budlight;patch doesn't fix my issue still get:

{quote}
java.lang.RuntimeException: org.apache.thrift.TException: Message length exceeded: 21
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.maybeInit(ColumnFamilyRecordReader.java:384)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.computeNext(ColumnFamilyRecordReader.java:390)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.computeNext(ColumnFamilyRecordReader.java:313)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getProgress(ColumnFamilyRecordReader.java:103)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.getProgress(PigRecordReader.java:158)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.getProgress(MapTask.java:514)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:539)
	at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Unknown Source)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
Caused by: org.apache.thrift.TException: Message length exceeded: 21
	at org.apache.thrift.protocol.TBinaryProtocol.checkReadLength(TBinaryProtocol.java:393)
	at org.apache.thrift.protocol.TBinaryProtocol.readBinary(TBinaryProtocol.java:363)
	at org.apache.cassandra.thrift.Column.read(Column.java:528)
	at org.apache.cassandra.thrift.ColumnOrSuperColumn.read(ColumnOrSuperColumn.java:507)
	at org.apache.cassandra.thrift.KeySlice.read(KeySlice.java:408)
	at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12905)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:734)
	at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:718)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.maybeInit(ColumnFamilyRecordReader.java:346)
{quote}

Maybe these are separate issues;;;","22/Apr/13 16:56;ifesdjeen;Sorry, my bad, I have forgotten to sync changes for static rows, give me a sec.
Faced that one, too.
;;;","22/Apr/13 17:11;ifesdjeen;Ben, I've been putting lastRowKey in a wrong place, now it's after get_range_slices occuring, which should be correct.

I've had same exact stack trace.
Hope that solves issue for you.;;;","22/Apr/13 17:59;budlight;still not working for me.  I can't really tell what is going on due to the magic thrift import line 

bq. import org.apache.cassandra.thrift.*;;;;","22/Apr/13 18:15;ifesdjeen;Hm... That's quite weird. 
Maybe there's something different with Thrift.

I've seen people having trouble because of the message size, too, though. There're two settings, one for framed and one non-framed thrift.;;;","22/Apr/13 18:18;budlight;relevant changes since 1.1.9 https://git-wip-us.apache.org/repos/asf?p=cassandra.git;a=blobdiff;f=src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java;h=dfeacc39a6ba49d7eaf6336251875e5b55528bb0;hp=a40e6c56c1fd0b52c482832d04e73387db001699;hb=4feb87d37544b9fde722786555475f2f790059ca;hpb=73d828e4e8023b9f7ca8fafd12becec34eb59211;;;","22/Apr/13 18:31;budlight;actually the diff between 1.1.11 and 1.1.9 is very simple 

bq. git diff cassandra-1.1.9 cassandra-1.1.11 -- src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordReader.java

{code}
-            TTransport transport = ConfigHelper.getInputTransportFactory(conf).openTransport(socket);
-            TBinaryProtocol binaryProtocol = new TBinaryProtocol(transport);
+            TTransport transport = ConfigHelper.getInputTransportFactory(conf).openTransport(socket, conf);
+            TBinaryProtocol binaryProtocol = new TBinaryProtocol(transport, ConfigHelper.getThriftMaxMessageLength(conf));
{code}

that might be the difference;;;","22/Apr/13 18:36;budlight;reverting that change to TBinaryProtocol fixes it, now the question is do I just have something setup wrong since ConfigHelper.getThriftMaxMessageLength obviously returns some really low value;;;","22/Apr/13 18:52;ifesdjeen;You can configure it through the ConfigHelper.setThriftMaxMessageLength(), can you execute java code there? (e.q. not only pig queries);;;","22/Apr/13 18:56;ifesdjeen;However, unfortunately, when using it with Cascading, I still get eternal iterations :/ so it'd still be good if someone could take a look at the patch :/;;;","22/Apr/13 19:00;budlight;yeah but i'm using pig, which means I shouldn't have to redo the whole hadoop backend to use pig;;;","22/Apr/13 19:10;budlight;I might be seeing the same bug you originally reported now that the job has started. I can't help but wonder if the reason none of this works is because the main codebase is now in DSE and its all modified to work with DSE and not with hadoop.   ;;;","24/Apr/13 19:48;lannyripple;Instead of reverting the changes to TBinaryProtocol you probably need to use ConfigHelper to set the thrift_framed-transport_size_in_mb and thrift_max_message_length_in_mb to much larger values (if ConfigHelper is exposed for you).  These values, prior to 1.10, were ignored (and a later version fixed a bug with getting them from ConfigHelper as well).  Setting the values to 2047 and 2048 respectively got us working again.

Oleksandr -- patch2 works for us.  Thanks!;;;","25/Apr/13 22:47;jbellis;Thanks for the patch, Oleksandr.

It looks to me like the root of the problem is that {{key.put(this.getCurrentKey())}} destructively modifies currentKey.  Attached is a patch to duplicate the buffer first.

This has the added benefit that we don't have to impose any overhead on the new mapreduce api to solve this problem in the old mapred one.;;;","25/Apr/13 23:00;jbellis;While investigating whether this was also a problem in 1.1, I found that this was fixed for 1.1.7 in CASSANDRA-4834, with the same .duplicate() solution, but not merged forward.  I've applied this fix to the 1.2 branch.;;;","29/Jun/13 20:55;ifesdjeen;Can anyone confirm if 1.2.4 contains the fix, too?
It seems to work, it's just not clear wether fix made it there or it's just a coincidence...

UPDATE: sorry, I've tested against 1.2.5, so nevermind :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible NPE on EACH_QUORUM writes,CASSANDRA-5498,12643583,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,19/Apr/13 13:22,16/Apr/19 09:32,14/Jul/23 05:53,26/Jun/13 15:02,1.2.6,,,,,,0,each_quorum,ec2,,,"When upgrading from 1.0 to 1.1, we observed that DatacenterSyncWriteResponseHandler.assureSufficientLiveNodes() can throw an NPE if one of the writeEndpoints has a DC that is not listed in the keyspace while one of the nodes is down. We observed this while running in EC2, and using the Ec2Snitch. The exception typically was was brief, but a certain segment of writes (using EACH_QUORUM) failed during that time.

This ticket will address the NPE in DSWRH, while a followup ticket will be created once we get to the bottom of the incorrect DC being reported from Ec2Snitch.
",,jasobrown,jjordan,timiblossom,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5660,,,,,,,,,,,"19/Apr/13 13:25;jasobrown;5498-v1.patch;https://issues.apache.org/jira/secure/attachment/12579537/5498-v1.patch","19/Apr/13 14:28;jasobrown;5498-v2.patch;https://issues.apache.org/jira/secure/attachment/12579543/5498-v2.patch",,,,,,,,,,,,,,,,,,,2.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,323950,,,Wed Jun 19 13:15:05 UTC 2013,,,,,,,,,,"0|i1jw3b:",324295,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"19/Apr/13 13:25;jasobrown;This patch simply checks if we got a null value from the keyspace-dc map, and if so, throws UnavailableException.;;;","19/Apr/13 13:56;jbellis;I'm not convinced that we should turn an internal bug into a UAE, although I'd be fine with an assert to make more clear what we expect.  (Pretty sure Thrift will catch it and return an InternalError, so clients shouldn't be just left hanging.)

Separately, I note that TokenMetadata.getWriteEndpoints is ReplicationStrategy-agnostic.  If you have any bootstrap action going on, that could be causing your problem.  (Similar bug in CASSANDRA-5424.);;;","19/Apr/13 14:14;jasobrown;Good point about not not throwing a UAE and having assert instead. With the NPE, thrift did return an InternalError to clients, so there was reasonable commincation for subclasses of Exception (hopefully the same for AssertionError?). Bootstrap is not going on at the time, but I'll check out that ticket, as well. For my testing, I left one of the nodes down intentionally and was able to reproduce sporadically, but still digging in.;;;","19/Apr/13 14:28;jasobrown;v2 has assert rather than throwing UAE.;;;","19/Apr/13 14:40;jbellis;bq. With the NPE, thrift did return an InternalError to clients

Now I'm puzzled, because I don't see how this works...  Thrift 0.7 appears to be missing the internal error catch; it was added back in THRIFT-1658 for Thrift 0.8.;;;","23/Apr/13 22:01;jasobrown;bq. Now I'm puzzled, because I don't see how this works...

Are you referring to the NPE or the AssertionError? In my statement above, I mean the NPE was caught and the client app got a TApplicationException. Here the output via an astyanax client:

{code}2013-04-15 18:27:29,618 ERROR com.netflix.cassandra.NetflixConnectionPoolMonitor:428 [http-0.0.0.0-7101-33] [trackError] Unknown operation error
com.netflix.astyanax.connectionpool.exceptions.ThriftStateException: ThriftStateException: [host=ec2-54-234-29-24.compute-1.amazonaws.com(10.29.141.78):7102, latency=2(2), attempts=1]org.apache.thrift.TApplicationException:
 Internal error processing batch_mutate
        at com.netflix.astyanax.thrift.ThriftConverter.ToConnectionPoolException(ThriftConverter.java:177)
        at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:65)
        at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:28)
        at com.netflix.astyanax.thrift.ThriftSyncConnectionFactoryImpl$ThriftConnection.execute(ThriftSyncConnectionFactoryImpl
{code}

I'll test again with throwing the AE, but I can see from your thrift bug report that it might, in fact, be an issue. Sorry if my early statement was unclear.
;;;","24/Apr/13 01:23;jbellis;I mean that I don't see how Thrift 0.7 catches *any* internal error as TAE, so my mental picture must be somehow incomplete (and therefore I'm not comfortable changing things until that's repaired).;;;","24/Apr/13 01:24;jbellis;[~tjake] [~bterm] Can you shed any light here?;;;","24/Apr/13 03:16;tjake;The exception is thrown on the client.  It's the default exception in Java impl when none of the declared exceptions are a match.;;;","22/May/13 15:45;jbellis;But I don't see how 0.7 would return an exception at all; it would just close the connection uncleanly.;;;","03/Jun/13 17:39;jjordan;I don't think an exception is being caught and passed to the client.  I think the connection closes so org.apache.thrift.TApplicationException gets thrown.

[~jasobrown] have you done any more debug on what is causing this?;;;","03/Jun/13 18:52;jasobrown;[~jjordan] working on it now on #cassandra-dev IRC. My suspicion is a problem with Gossiper.addSavedEndopint(), which clears out the endpoint's previous data from the endpointStateMap when a node with a greater messaging version attempts to connect. Which then causes the downstream affect in DSWRH when it requests the DC data from the EC2Snitch, which gets it from Gossiper.endopintStateMap.

Here's the server-side stacktrace:

{code}ERROR [RPC-Thread:150339] 2013-05-08 17:29:55,048 Cassandra.java (line 3462) Internal error processing batch_mutate 
java.lang.NullPointerException 
at org.apache.cassandra.service.DatacenterSyncWriteResponseHandler.assureSufficientLiveNodes(DatacenterSyncWriteResponseHandler.java:109) 
at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:253) 
at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:194) 
at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:639) 
at org.apache.cassandra.thrift.CassandraServer.internal_batch_mutate(CassandraServer.java:590) 
at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:598) 
at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.process(Cassandra.java:3454) 
at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889) 
at org.apache.thrift.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:631) 
at org.apache.cassandra.thrift.CustomTHsHaServer$Invocation.run(CustomTHsHaServer.java:105) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:662){code};;;","19/Jun/13 13:15;jasobrown;The cause of this NPE is due to the Gossiper issue in the linked ticket;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Use allocator information to improve memtable memory usage estimate,CASSANDRA-5497,12643299,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,18/Apr/13 18:31,16/Apr/19 09:32,14/Jul/23 05:53,19/Apr/13 13:46,1.1.12,1.2.5,,,,,0,,,,,"A user reported that Cassandra's estimate of memtable space used was off by a factor of between 3 and 10 for his counter columnfamilies.

We may or may not be able to fix the counter estimate (counter merging is a cranky best, and unlike normal merging can involve allocating new objects), but we can definitely use the SlabAllocator information to cap the error in our estimate at a fairly low amount.",,jasobrown,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/13 18:48;jbellis;5497-v2.txt;https://issues.apache.org/jira/secure/attachment/12579389/5497-v2.txt","18/Apr/13 18:33;jbellis;5497.txt;https://issues.apache.org/jira/secure/attachment/12579385/5497.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,323709,,,Fri Apr 19 13:46:44 UTC 2013,,,,,,,,,,"0|i1julr:",324054,,,,,,,,,jasobrown,,jasobrown,Low,,,,,,,,,,,,,,,,,,"18/Apr/13 18:33;jbellis;attached.;;;","18/Apr/13 18:48;jbellis;v2 attached to track allocations larger than MAX_CLONED_SIZE;;;","18/Apr/13 18:49;jasobrown;lgtm. This should bound the estimate reasonably well. EDIT: [~jbellis] attached v2 while i was typing. will review that one now;;;","18/Apr/13 19:16;jasobrown;v2 lgtm;;;","19/Apr/13 13:46;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix SemanticVersion.isSupportedBy patch/minor handling,CASSANDRA-5496,12643271,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,18/Apr/13 16:32,16/Apr/19 09:32,14/Jul/23 05:53,18/Apr/13 16:48,1.2.5,,,,,,0,,,,,"Currently we use the following logic:
{noformat}
return major == version.major && minor <= version.minor && patch <= version.patch;
{noformat}

This requires both minor and patch vers to be less or equal, which means that, for example, '3.0.3' is not supported by '3.1.2', because, while 0 <= 1 (minor), 3 > 2 (patch). This is clearly not the intent since 3.1.2 > 3.0.3.

CQL3 doc: ""Minor version increments occur when new, but backward compatible, functionality is introduced"". Hence '3.0.3' is supposed to be supported by '3.1.0'.

This doesn't really affect 1.2, but breaks a lot of trunk dtests post CASSANDRA-3919.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Apr/13 16:33;aleksey;5496.txt;https://issues.apache.org/jira/secure/attachment/12579359/5496.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,323681,,,Thu Apr 18 16:48:58 UTC 2013,,,,,,,,,,"0|i1jufj:",324026,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"18/Apr/13 16:40;jbellis;+1;;;","18/Apr/13 16:48;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage throws NullPointerException (NPE) when widerows is set to 'true',CASSANDRA-5488,12643010,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,sgosrani,sgosrani,sgosrani,17/Apr/13 19:51,16/Apr/19 09:32,14/Jul/23 05:53,22/May/13 16:21,1.1.12,1.2.6,,,,,0,cassandra,hadoop,pig,,"CassandraStorage throws NPE when widerows is set to 'true'. 

2 problems in getNextWide:
1. Creation of tuple without specifying size
2. Calling addKeyToTuple on lastKey instead of key

java.lang.NullPointerException
    at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
    at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
    at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
    at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:93)
    at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:34)
    at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:26)
    at org.apache.cassandra.hadoop.pig.CassandraStorage.addKeyToTuple(CassandraStorage.java:313)
    at org.apache.cassandra.hadoop.pig.CassandraStorage.getNextWide(CassandraStorage.java:196)
    at org.apache.cassandra.hadoop.pig.CassandraStorage.getNext(CassandraStorage.java:224)
    at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:194)
    at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
    at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
    at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
    at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
    at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
    at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
    at java.security.AccessController.doPrivileged(Native Method)
    at javax.security.auth.Subject.doAs(Subject.java:415)
    at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1121)
    at org.apache.hadoop.mapred.Child.main(Child.java:249)
2013-04-16 12:28:03,671 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task","Ubuntu 12.04.1 x64, Cassandra 1.2.4",jeromatron,sgosrani,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/May/13 16:11;jeromatron;5488-2.txt;https://issues.apache.org/jira/secure/attachment/12584029/5488-2.txt","17/Apr/13 22:08;sgosrani;5488.txt;https://issues.apache.org/jira/secure/attachment/12579212/5488.txt",,,,,,,,,,,,,,,,,,,2.0,sgosrani,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,323420,,,Wed May 22 16:21:42 UTC 2013,,,,,,,,,,"0|i1jstj:",323765,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"17/Apr/13 20:04;sgosrani;This patch (5488.txt) fixes the issue.;;;","01/May/13 12:29;brandon.williams;Can you add a test to examples/pig/test/test_storage.pig that demonstrates the problem?;;;","17/May/13 13:58;jeromatron;I've reproduced this with 1.1.9 as well.;;;","17/May/13 16:16;jeromatron;Looks like it's from CASSANDRA-5098;;;","21/May/13 16:11;jeromatron;An alternative way to do it with consolidating the two methods and checking for null in that method.;;;","21/May/13 16:15;brandon.williams;Committed v2, and also flipped the copy test to use widerow mode as a smoke test.;;;","22/May/13 10:45;jeromatron;There ended up being a secondary problem that was hidden by the first NPE.  It seems to be related to getting the AbstractType.  The NPE was for this line: https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/hadoop/pig/CassandraStorage.java#L307 which I decomposed to find out what it was NPEing on, and got this:
{code}
            List<AbstractType> atList = getDefaultMarshallers(cfDef);
            AbstractType at = atList.get(2);
            Object o = at.compose(key); //NPE from this line
            setTupleValue(tuple, 0, o);
            //setTupleValue(tuple, 0, getDefaultMarshallers(cfDef).get(2).compose(key));
{code}

So it seems unrelated to the original NPE, but still matches the description of this ticket.

To reproduce, here is my schema:
{code}
CREATE KEYSPACE circus
with placement_strategy = 'SimpleStrategy'
and strategy_options = {replication_factor:1};

use circus;

CREATE COLUMN FAMILY acrobats
WITH comparator = UTF8Type
AND key_validation_class=UTF8Type
AND default_validation_class = UTF8Type;
{code}

Here is a pycassa script to create the data:
{code}
from pycassa.pool import ConnectionPool
from pycassa.columnfamily import ColumnFamily

pool = ConnectionPool('circus')
col_fam = pycassa.ColumnFamily(pool, 'acrobats')

for i in range(1, 10):
    for j in range(1, 200000):
        col_fam.insert('row_key' + str(i), {str(j): 'val'})
{code}

Here is the pig (0.9.2) that I'm running in local mode:
{code}
rows = LOAD 'cassandra://circus/acrobats?widerows=true&limit=200000' USING CassandraStorage();
filtered = filter rows by key == 'row_key1';
columns = foreach filtered generate flatten(columns);
counted = foreach (group columns all) generate COUNT($1);
dump counted;
{code};;;","22/May/13 16:21;brandon.williams;v2 was a little too aggressive in function consolidation.  I reverted it and applied v1.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support custom secondary indexes in CQL,CASSANDRA-5484,12642817,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,bcoverston,bcoverston,16/Apr/13 21:31,16/Apr/19 09:32,14/Jul/23 05:53,03/May/13 16:41,1.2.5,,,Feature/2i Index,,,0,cql3,index,,,"Through thrift users can add custom secondary indexes to the column metadata.

The following syntax is used in PLSQL, and I think we could use something similar.

CREATE INDEX <NAME> ON <TABLE> (<COLUMN>) [INDEXTYPE IS (<TYPENAME>) [PARAMETERS (<PARAM>[, <PARAM>])]",,aleksey,bcoverston,jjordan,PuerTea,rcoli,sbtourist,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4027,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,323231,,,Fri May 03 16:41:46 UTC 2013,,,,,,,,,,"0|i1jrnj:",323576,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"16/Apr/13 21:39;jbellis;I have a mild preference for PostgreSQL syntax, which adds no additional keywords (we already have USING and WITH): 

{{CREATE INDEX [<name>] ON <table> [USING <method>] (<columns>) [WITH <parameters>]}};;;","16/Apr/13 21:45;jbellis;Actually if it wouldn't confuse the parser too much I'd prefer no keyword at all for the index implementation: 

{{CREATE [<method>] INDEX [<name>] ON <table> (<columns>) [WITH <parameters>]}};;;","16/Apr/13 21:53;jbellis;Let's also split out parameterization to another ticket -- I don't think we support that yet for any index type.;;;","16/Apr/13 21:57;bcoverston;wfm;;;","22/Apr/13 10:55;slebresne;bq. Let's also split out parameterization to another ticket – I don't think we support that yet for any index type.

Actually, I think we do need it here because that's the way we pass the class name for CUSTOM indexes. So I think this could look like:
{noformat}
CREATE CUSTOM INDEX ON <table>(<column>) WITH class='MyCustomIndex'; 
{noformat}
(I do note that thrift uses 'class_name' in the index options, but for CQL3, using just 'class' would be somewhat more coherent with the compaction strategy.

I will also note that on the thrift side, we have 3 different IndexType: Keys, Composites and Custom. But for CQL3, I am not convinced that making the different between Keys and Composites is a good idea: CQL3 knows enough about the tables to know with one to use. In other words, I think the only <method> we should support here is 'CUSTOM' (or nothing).

And while in theory it is possible to create a KEYS index on a composite table on the thrift side, it's unclear whether it is useful or not for CQL3 and should be supported. So even if we do want to consider it, then I think that this part should be split to a separate ticket.;;;","01/May/13 00:52;aleksey;I was going for
{noformat}
CREATE INDEX ON <table>(<column) WITH type = CUSTOM AND options = {'class_name': 'MyCostomIndex', ...};
{noformat}

to allow parametrization for KEYS/COMPOSITES index in the future, but I can't think of what we could actually want to parametrize with them, so scratch that. Let's only leave parametrization for CUSTOM.
But for the sake of consistency with CREATE KEYSPACE and compaction,compression,etc. options for CREATE TABLE, I'd like options to be a map (and replace 'class_name' key with 'class' internally, also for consistency), e.g.

{noformat}
CREATE INDEX ON <table>(<column>) WITH options = {'class': 'MyCustomIndex',...}; 
{noformat}

Not sure about CREATE CUSTOM INDEX vs. just treating CREATE INDEX with non-null options as custom, implicitly. I believe that every time we add a keyword to CQL, even an unreserved one, a kitten dies somewhere, so I'd like to avoid doing that.;;;","01/May/13 07:48;slebresne;bq. I'd like options to be a map (and replace 'class_name' key with 'class' internally, also for consistency)

Make sense to me.

bq. just treating CREATE INDEX with non-null options as custom, implicitly

That would work now, but that slightly frighten me for the future because:
* what if we add some other type of non custom indexes, like say bitmap indexes.
* what if we want to add options for non custom indexes (while this is nice to avoid option when we can, it's not hard to imagine that future improvements to the 2ndary index code might require tweaking knobs for instance).

bq. every time we add a keyword to CQL, even an unreserved one, a kitten dies somewhere

I agree we should avoid new keyword when possible. But that being said, when we add unreserved ones I think there is no real downside for clients. Yes it add some marginal delta to the parser and it's definitively sad for the kitten, but typically I'm not sold that it's worth taking the risk of being blocked if we want to add options to non-custom index later just to avoid adding an unreserved keyword now.

;;;","01/May/13 12:40;aleksey;Makes sense.;;;","03/May/13 00:52;aleksey;https://github.com/iamaleksey/cassandra/compare/5484 (https://github.com/iamaleksey/cassandra/compare/5484.patch)

Also refactored CreateIndexStatement to split validation into validate() instead of the mess in announceMigration().

Once this has been reviewed, but before resolving the issue, will need to:
1. Write a dtest
2. Update cqlsh completion
3. Update NEWS + cql docs;;;","03/May/13 14:02;slebresne;lgtm, +1;;;","03/May/13 16:41;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQLSH exception handling could leave a session in a bad state,CASSANDRA-5481,12642733,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jpi,jpi,jpi,16/Apr/13 14:18,16/Apr/19 09:32,14/Jul/23 05:53,29/Jul/14 19:01,1.2.19,2.0.10,,,,,0,,,,,"Playing with CTRL+C in a cqlsh session can leave the (Thrift|Native) connection in a bad state.

To reproduce :
1) Run a long running COPY FROM command (COPY test (k, v) FROM '/tmp/test.csv')
2) Interrupt the importer with CTRL+C

Repeat step 1 and 2 until you start seeing weird things in the cql shell (see attached screenshot)

The reason is, I believe, the connection (and the cursor) is not correclty closed and reopened on interruption.

I am working to propose a fix.

Jordan","cqlsh 2.3.0 | Cassandra 1.2.4 | CQL spec 3.0.0 | Thrift protocol 19.35.0",jpi,mshuler,philipthompson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"17/Apr/13 08:17;jpi;5481.diff;https://issues.apache.org/jira/secure/attachment/12579110/5481.diff","16/Apr/13 14:20;jpi;CQLSession.png;https://issues.apache.org/jira/secure/attachment/12578942/CQLSession.png",,,,,,,,,,,,,,,,,,,2.0,jpi,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,323147,,,Tue Jul 29 22:39:21 UTC 2014,,,,,,,,,,"0|i1jr4v:",323492,1.2.18,2.0.9,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"16/Apr/13 14:20;jpi;Broken CQL shell session;;;","16/Apr/13 22:46;jpi;Patch attached.

It also removes the need to escape the keyspace name since it is now handled by the driver;;;","29/Jul/14 16:35;mshuler;[~enigmacurry] could we see if this repros on the major branches and if the patch helps? (patch might need to be fuzzed, if there's much change, due to the age of this ticket);;;","29/Jul/14 18:30;philipthompson;The issue reproduces on 1.2-HEAD and 2.0-HEAD, but not on 2.1. The patch still applies relatively cleanly on 1.2 and 2.0, and fixes the issue on both.;;;","29/Jul/14 19:01;brandon.williams;Committed.;;;","29/Jul/14 22:39;jpi;Great !
Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodetool clearsnapshot incorrectly reports to have requested a snapshot,CASSANDRA-5478,12642699,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,cathodion,cathodion,16/Apr/13 10:34,16/Apr/19 09:32,14/Jul/23 05:53,25/May/13 17:02,1.2.6,,,Tool/nodetool,,,0,exception-reporting,lhf,nodetool,,"When running ""nodetool clearsnapshot"" all existing snapshots are removed, but the following message is printed:

./nodetool clearsnapshot
Requested snapshot for: all keyspaces 

Instead it should just print a single line stating that all snapshots have been removed.","MacOS, Datastax Cassandra 1.2.2",cathodion,dbrosius,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,3600,3600,,0%,3600,3600,,,,,,,,,,,CASSANDRA-5477,,,,,,,,,,,,,,,,,,,,"25/May/13 00:54;dbrosius;5478.txt;https://issues.apache.org/jira/secure/attachment/12584805/5478.txt",,,,,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,323113,,,Sat May 25 17:02:06 UTC 2013,,,,,,,,,,"0|i1jqxb:",323458,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"25/May/13 00:54;dbrosius;changed to

Requested creating snapshot for:

or

Requested clearing snapshot for:;;;","25/May/13 13:33;jbellis;+1;;;","25/May/13 17:02;dbrosius;committed to cassandra-1.2 as 7c9f17f9525c7736010be31f596888cccd398cfd;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Exceptions in 1.1 nodes with 1.2 nodes in ring,CASSANDRA-5476,12642673,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,dctrwatson,dctrwatson,16/Apr/13 07:36,16/Apr/19 09:32,14/Jul/23 05:53,22/Jun/13 13:58,1.2.6,,,,,,0,,,,,"As 1.1.9 nodes were being upgraded to 1.2.3 nodes, the 1.1.9 nodes started having this exception:

{noformat}
    Exception in thread Thread[RequestResponseStage:19496,5,main]
    java.io.IOError: java.io.EOFException
            at org.apache.cassandra.service.AbstractRowResolver.preprocess(AbstractRowResolver.java:71)
            at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:155)
            at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:45)
            at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
            at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
            at java.lang.Thread.run(Thread.java:662)
    Caused by: java.io.EOFException
            at java.io.DataInputStream.readFully(DataInputStream.java:180)
            at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:100)
            at org.apache.cassandra.db.ReadResponseSerializer.deserialize(ReadResponse.java:81)
            at org.apache.cassandra.service.AbstractRowResolver.preprocess(AbstractRowResolver.java:64)
            ... 6 more
{noformat}

As more 1.2.3 nodes were upgraded, the 1.2.3 nodes began logging for 1.1.9 node IPs:

{noformat}
    Unable to store hint for host with missing ID, /10.37.62.71 (old node?)
{noformat}",,aleksey,christianmovi,dctrwatson,edevil,sbtourist,winsdom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Jun/13 13:18;sbtourist;0001.patch;https://issues.apache.org/jira/secure/attachment/12589252/0001.patch",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,323087,,,Sat Jun 22 13:58:44 UTC 2013,,,,,,,,,,"0|i1jqrj:",323432,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"22/Jun/13 13:17;sbtourist;I run into this one too: it is caused by ReadResponse not correctly computing the serialized payload size in case of digest reads; this is not a problem for 1.2, which doesn't really use the payload size to read from the stream, but actually is for 1.1 which uses it for deserializing the ReadResponse coming from a 1.2 node.
To reproduce, just run a replicated cluster with mixed 1.1 and 1.2 nodes, increment the read repair chance to 1, insert some data and query from a 1.1 node (as the problem is the response back from 1.2 nodes).;;;","22/Jun/13 13:58;aleksey;Thanks. Committed in 57eb87b57cc7c69d99238ced08e50cc23b0127ba.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"remove dead classes (ArrayUtil.java, CreationTimeAwareFuture.java)",CASSANDRA-5475,12642643,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,dbrosius,dbrosius,16/Apr/13 01:13,16/Apr/19 09:32,14/Jul/23 05:53,16/Apr/13 02:40,1.2.5,,,,,,0,,,,,,,aleksey,dbrosius,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Apr/13 01:14;dbrosius;5475.txt;https://issues.apache.org/jira/secure/attachment/12578849/5475.txt",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,323057,,,Tue Apr 16 02:40:43 UTC 2013,,,,,,,,,,"0|i1jqkv:",323402,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"16/Apr/13 01:31;aleksey;+1;;;","16/Apr/13 02:40;dbrosius;committed to cassandra-1.2 as 40e7aba6b2f694017df5fbba90fd44caa0d43fc9;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Timeuuid with CLUSTERING ORDER DESC cannot be used with the dateOf CQL3 function,CASSANDRA-5472,12642541,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,gcollins,gcollins,15/Apr/13 14:04,16/Apr/19 09:32,14/Jul/23 05:53,29/Apr/13 07:32,1.2.5,,,,,,0,,,,,"I originally raised this issue in the mailing lists:

http://www.mail-archive.com/user@cassandra.apache.org/msg29185.html

Here is what I tried:

cqlsh:location> create table test_y (message_id timeuuid, name text,
PRIMARY KEY (name,message_id));
cqlsh:location> insert into test_y (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_y (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_y (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_y (message_id,name) VALUES (now(),'foo');
cqlsh:location> select dateOf(message_id) from test_y;

 dateOf(message_id)
--------------------------
 2013-04-13 00:33:42-0400
 2013-04-13 00:33:43-0400
 2013-04-13 00:33:43-0400
 2013-04-13 00:33:44-0400

cqlsh:location> create table test_x (message_id timeuuid, name text,
PRIMARY KEY (name,message_id)) WITH CLUSTERING ORDER BY (message_id DESC);
cqlsh:location> insert into test_x (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_x (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_x (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_x (message_id,name) VALUES (now(),'foo');
cqlsh:location> insert into test_x (message_id,name) VALUES (now(),'foo');
cqlsh:location> select dateOf(message_id) from test_x;
Bad Request: Type error:
org.apache.cassandra.cql3.statements.Selection$SimpleSelector@1e7318 cannot
be passed as argument 0 of function dateof of type timeuuid

It should be possible to use dateOf on message_id in table test_x",,aleksey,gcollins,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/13 09:15;slebresne;5472.txt;https://issues.apache.org/jira/secure/attachment/12580263/5472.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322955,,,Mon Apr 29 07:32:37 UTC 2013,,,,,,,,,,"0|i1jpy7:",323300,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"24/Apr/13 09:15;slebresne;Attaching patch to fix. The main problem is that the selector was still using the AbstractType to check for type equality, so the ReversedType was messing things up. Fixed to use the CQL3 type instead as in other places. Other that that, the patch also adds toString() methods so that the error looks readable. ;;;","28/Apr/13 21:45;aleksey;Makes sense and lgtm. +1;;;","29/Apr/13 07:32;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Spelling and grammar errors in cassandra.yaml,CASSANDRA-5471,12642535,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jeromatron,jeromatron,jeromatron,15/Apr/13 13:27,16/Apr/19 09:32,14/Jul/23 05:53,19/Apr/13 23:47,1.2.5,,,,,,0,,,,,There are various spelling and grammar errors in cassandra.yaml.,,dbrosius,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Apr/13 13:29;jeromatron;5471.txt;https://issues.apache.org/jira/secure/attachment/12578722/5471.txt",,,,,,,,,,,,,,,,,,,,1.0,jeromatron,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322949,,,Mon Apr 22 15:34:03 UTC 2013,,,,,,,,,,"0|i1jpwv:",323294,,,,,,,,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,,"15/Apr/13 13:29;jeromatron;Also tried to clarify a bit in the row cache description and standardize on \_word_ instead of both that and \*word*.  Patch is against trunk.;;;","15/Apr/13 15:42;dbrosius;+1;;;","19/Apr/13 23:47;dbrosius;committed (all but CAS documentation) to cassandra-1.2 as d5c0c7ae301116e9be8dcb84b91fc948ee931778;;;","22/Apr/13 15:34;dbrosius;committed v2.0 specific changes to trunk as commit c85d4722bee1952756fd1c1f70f45bb6ddba53b2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Prepared statements from default keyspace are broken,CASSANDRA-5468,12642467,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,pchalamet,pchalamet,14/Apr/13 20:24,16/Apr/19 09:32,14/Jul/23 05:53,15/Apr/13 21:58,1.2.5,,,,,,0,,,,,"Tested under CQL 3 binary protocol.
Preparing a statement from the default keyspace of the connection (statement scoped with keyspace) and then running it will always throw the error ""no keyspace has been specified"".

{code}
Exec: CREATE KEYSPACE Tests WITH replication = {'class': 'SimpleStrategy', 'replication_factor' : 1}

Exec: CREATE TABLE Tests.AllTypes (a int, b int, primary key (a))

Prepare: insert into Tests.AllTypes (a, b) values (?, ?)
{code}

Exec prepared statement and exception ""no keyspace has been specified"" is thrown.

Doing a use Tests before preparing the statement solves the issue.
This used to work in 1.2.3.","Windows 8 x64, java 1.7.0_11 x64",aleksey,pchalamet,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/13 23:04;aleksey;5468.txt;https://issues.apache.org/jira/secure/attachment/12578655/5468.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322881,,,Mon Apr 15 21:58:58 UTC 2013,,,,,,,,,,"0|i1jphr:",323226,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"14/Apr/13 23:06;aleksey;CASSANDRA-5352 caused this. QueryProcessor.storePreparedStatement() handles keyspace==null case, but it'll never get to this point, because ClientState.getKeyspace() call in prepare() will throw when keyspace==null. 

The attached trivial fix replaces getKeyspace() call with getRawKeyspace(), that will return null without throwing.;;;","15/Apr/13 20:33;pchalamet;works for me. Thanks.;;;","15/Apr/13 21:02;jbellis;+1;;;","15/Apr/13 21:58;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
isRunning flag set prematurely in org.apache.cassandra.transport.Server,CASSANDRA-5467,12642448,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,jsanda,jsanda,14/Apr/13 15:07,16/Apr/19 09:32,14/Jul/23 05:53,26/Apr/13 15:21,1.2.5,,,,,,0,jmx,server,,,"In org.apache.cassandra.transport.Server, the start() method sets the isRunning flag before calling the run() method. In the event of an initialization error like a port conflict an exception will be thrown at line 136 which is,

    Channel channel = bootstrap.bind(socket);

It seems like it might make more sense to set the isRunning flag after binding to the socket. I have a tool that deploys a node and then verifies it is ready to receive CQL requests. I do this via JMX. Unless I use a delay before making that check, the JMX call will return true even though there is a port conflict. 

",,jsanda,marcuse,mbulman,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/13 08:30;slebresne;5467.txt;https://issues.apache.org/jira/secure/attachment/12580256/5467.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322862,,,Fri Apr 26 15:21:47 UTC 2013,,,,,,,,,,"0|i1jpdj:",323207,,,,,,,,,marcuse,,marcuse,Low,,,,,,,,,,,,,,,,,,"24/Apr/13 08:30;slebresne;That's definitively not unreasonable. Initially, isRunning was set first so we don't start the server twice if the {{start()}} method was called twice. But tbh it's not really useful a protection so attaching a trivial patch that just move setting isRunning once the server is indeed started.;;;","25/Apr/13 12:50;marcuse;lgtm;;;","26/Apr/13 15:21;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Shuffle fails to write to system.range_xfers,CASSANDRA-5465,12642366,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,dbrosius,dbrosius,13/Apr/13 07:46,16/Apr/19 09:32,14/Jul/23 05:53,13/Apr/13 21:27,2.0 beta 1,,,,,,0,,,,,"On trunk, only, executing

INSERT INTO system.range_xfers (token_bytes, requested_at) VALUES ('5e29cbe30503a9ec', 'now');

fails with

Exception in thread ""main"" java.lang.RuntimeException: InvalidRequestException(why:Invalid STRING constant (5e29cbe30503a9ec) for token_bytes of type blob)
	at org.apache.cassandra.tools.Shuffle.executeCqlQuery(Shuffle.java:516)
	at org.apache.cassandra.tools.Shuffle.shuffle(Shuffle.java:359)
	at org.apache.cassandra.tools.Shuffle.main(Shuffle.java:681)


patch validates blob->string
",,aleksey,dbrosius,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/13 07:47;dbrosius;5465.txt;https://issues.apache.org/jira/secure/attachment/12578588/5465.txt","13/Apr/13 16:54;dbrosius;5465_2.txt;https://issues.apache.org/jira/secure/attachment/12578604/5465_2.txt",,,,,,,,,,,,,,,,,,,2.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322780,,,Sat Apr 13 21:27:34 UTC 2013,,,,,,,,,,"0|i1jovj:",323125,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"13/Apr/13 13:12;aleksey;This fix is wrong. BLOB is not there for a reason, and the reason is that in 2.0 'cafe' syntax for blobs is no longer valid. You should fix shuffle instead to use 0xcafe syntax (and do that against 1.2 as well, to avoid the deprecation warning in Cassandra logs).;;;","13/Apr/13 15:07;urandom;So we're introducing a breaking (non-backward compatible) change?;;;","13/Apr/13 15:14;aleksey;It was introduced back in 1.2.2. In 1.2 both versions will continue to coexist, so you are able to upgrade. See CASSANDRA-5198 and Jake's comment - https://issues.apache.org/jira/browse/CASSANDRA-5198?focusedCommentId=13569019&page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel#comment-13569019.;;;","13/Apr/13 15:27;urandom;So we introduced a breaking change.;;;","13/Apr/13 16:34;dbrosius;So Aleksey, i take it you are saying that Shuffle needs to use prepared statements so that the token can be sent as a ByteBuffer (byte[]) ? ;;;","13/Apr/13 16:42;jbellis;bq. So we introduced a breaking change.

My understanding is that we did not: the old format continues to work.  What we did do was allow a new format that previously would have errored, and deprecated blob-strings for removal in 2.0.;;;","13/Apr/13 16:54;dbrosius;oh i see, remove ' 's, and add 0x... done


patch on 1.2 -- 5465_2.txt;;;","13/Apr/13 17:13;urandom;bq. My understanding is that we did not: the old format continues to work. What we did do was allow a new format that previously would have errored, and deprecated blob-strings for removal in 2.0.

Right, that's what I meant, we introduced a breaking change, 1.2 -> 2.0.  CQL 4, then?;;;","13/Apr/13 18:29;aleksey;+1;;;","13/Apr/13 21:27;dbrosius;added commit to cassandra-1.2 as commit a25ac14e0d245f648e97f888ee4b64acc50c76d1;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
use slf4j not commons-logging,CASSANDRA-5464,12642363,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,dbrosius,dbrosius,13/Apr/13 04:02,16/Apr/19 09:32,14/Jul/23 05:53,13/Apr/13 05:05,1.1.11,,,,,,0,,,,,,,dbrosius,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Apr/13 04:03;dbrosius;5464.txt;https://issues.apache.org/jira/secure/attachment/12578578/5464.txt",,,,,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322777,,,Sat Apr 13 05:05:34 UTC 2013,,,,,,,,,,"0|i1jouv:",323122,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"13/Apr/13 04:51;jbellis;+1;;;","13/Apr/13 05:05;dbrosius;committed to cassandra-1.1 as commit 822bda77aba24fda8b234590bedb292e32443aa5;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid serializing keyspace redundantly in RowMutation,CASSANDRA-5458,12642150,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,11/Apr/13 22:31,16/Apr/19 09:32,14/Jul/23 05:53,12/Apr/13 02:07,2.0 beta 1,,,,,,0,,,,,"We can infer the table from the CFID, so there's no need to de/serialize it.",,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/13 22:31;jbellis;5458.txt;https://issues.apache.org/jira/secure/attachment/12578301/5458.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322564,,,Fri Apr 12 02:07:19 UTC 2013,,,,,,,,,,"0|i1jnjj:",322909,,,,,,,,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Low,,,,,,,,,,,,,,,,,,"12/Apr/13 01:33;vijay2win@yahoo.com;+1 LGTM;;;","12/Apr/13 02:07;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Large number of bootstrapping nodes cause gossip to stop working,CASSANDRA-5456,12642102,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,okibirev,okibirev,okibirev,11/Apr/13 18:44,16/Apr/19 09:32,14/Jul/23 05:53,12/Apr/13 17:13,1.1.11,1.2.5,,,,,0,,,,,"Long running section of code in PendingRangeCalculatorService is synchronized on bootstrapTokens. This causes gossip to stop working as it waits for the same lock when a large number of nodes (hundreds in our case) are bootstrapping. Consequently, the whole cluster becomes non-functional. 

I experimented with the following change in PendingRangeCalculatorService.java and it resolved the problem in our case. Prior code had synchronized around the for loop.

synchronized(bootstrapTokens) {
    bootstrapTokens = new LinkedHashMap<Token, InetAddress>(bootstrapTokens);
}

for (Map.Entry<Token, InetAddress> entry : bootstrapTokens.entrySet())
{
   InetAddress endpoint = entry.getValue();

   allLeftMetadata.updateNormalToken(entry.getKey(), endpoint);
   for (Range<Token> range : strategy.getAddressRanges(allLeftMetadata).get(endpoint))
   pendingRanges.put(range, endpoint);
   allLeftMetadata.removeEndpoint(endpoint);
}
 ",,jjordan,okibirev,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/13 20:07;okibirev;PendingRangeCalculatorService.patch;https://issues.apache.org/jira/secure/attachment/12578272/PendingRangeCalculatorService.patch",,,,,,,,,,,,,,,,,,,,1.0,okibirev,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322516,,,Fri Apr 12 17:13:38 UTC 2013,,,,,,,,,,"0|i1jn8v:",322861,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"11/Apr/13 20:05;okibirev;Making a copy of bootstrapTokens rather than holding a lock on the same for entire time consuming loop.;;;","11/Apr/13 20:07;okibirev;Making a copy of bootstrapTokens before a time consuming loop rather than holding a synchronized lock for the whole duration;;;","12/Apr/13 17:13;brandon.williams;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove PBSPredictor,CASSANDRA-5455,12642058,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,11/Apr/13 16:06,16/Apr/19 09:32,14/Jul/23 05:53,11/Apr/13 16:30,2.0 beta 1,,,,,,0,,,,,"It was a fun experiment, but it's unmaintained and the bar to understanding what is going on is high.  Case in point: PBSTest has been failing intermittently for some time now, possibly even since it was created.  Or possibly not and it was a regression from a refactoring we did.  Who knows?",,jjordan,marcuse,pbailis,rbranson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/13 16:12;jbellis;5455.txt;https://issues.apache.org/jira/secure/attachment/12578235/5455.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322472,,,Tue May 28 03:51:19 UTC 2013,,,,,,,,,,"0|i1jmz3:",322817,,,,,,,,,marcuse,,marcuse,Normal,,,,,,,,,,,,,,,,,,"11/Apr/13 16:25;marcuse;there is an ant target ""pbs-test"" that should go away as well

lgtm other than that;;;","11/Apr/13 16:30;jbellis;fixed + committed;;;","10/May/13 17:58;pbailis;I am one of the original authors of CASSANDRA-4261 and was previously unaware of this change. I'm happy to make any changes to the tests, perform necessary code refactoring, or write additional documentation (but was unable to do so given the window between ticket creation and commit). That is, I will maintain this functionality given the opportunity to do so.

Could you please elaborate on what you'd like to see fixed? I suspect it'll be fairly straightforward, and, if anyone ""knows"" how to make the changes, I (and Shivaram) probably do.

If the answer is that ""we don't want this functionality,"" then that's a different case. But that's not what I'm getting from this ticket or CASSANDRA-4261 or am hearing from users.;;;","10/May/13 18:11;jbellis;Honestly, it was probably a mistake (mine) to commit it in the first place.  In my defense, it's hard to say No when someone shows up with working code...  but I should have; it doesn't solve an actual pain point for our users, so none of the maintainers was motivated to get familiar enough with it to fix the kind of regressions we ran into.  I don't see that changing if we were to resurrect it.  My sincere apologies for the time you put into it.;;;","10/May/13 18:46;rbranson;Correct me if I'm wrong, but it seems like only a small amount of the code in the original patch is actually necessary to have in core to satisfy the data requirements of doing the PBS prediction. I would love to be able to feed these metrics into our monitoring systems as well. Would it be acceptable to refactor the code to support the latency metric collection & expose them through a JMX call, and allow implementing the actual PBS logic in a third party tool?;;;","10/May/13 19:24;jbellis;I don't see why not.  Aren't these the same latency numbers that we track in StorageProxy?;;;","10/May/13 19:32;pbailis;I don't believe that the StorageProxy tracks the latencies according to the same granularity. For example, the PBS latency tracking will record both how long it took for the request to reach a remote replica and be processed as well as how long the return trip takes.

That said, it shouldn't be too difficult to either 1.) simply expose the recorded latencies via an optional module providing a ""finer granularity tracing"" interface via JMX [thereby removing all actual prediction code but keeping the logging in place for folks who might want this] or 2.) modifying StorageProxy to log these latencies in addition to the coarser granularity measurements it already takes.

I can provide assistance with either.;;;","10/May/13 19:51;jbellis;bq. the PBS latency tracking will record both how long it took for the request to reach a remote replica and be processed as well as how long the return trip takes.

Hmm, I don't see that happening on the 1.2 branch.  It looks to me like like PBS was trying to measure raw message RTT (slightly incorrectly since it was clicking Start when we enqueued the message rather than when we sent).  Which, granted, *is* different from the SP ""time to complete request"" metrics.

Would the latter be ""close enough?""  I'd rather only track one set of mostly similar metrics, given the choice.

bq. modifying StorageProxy to log these latencies in addition to the coarser granularity measurements it already takes

I'm fine with either ""provide it as SPMBean methods"" or ""create a separate MBean that we only kick off if individual data point collection is enabled.""

Should we make collection be a fraction (0..1) rather than on/off?  ISTM that 10% or 1% of requests could provide enough information on a busy system, and those CLQ objects could become a contention point w/ enough cores busy.;;;","14/May/13 23:48;pbailis;I've thought some more about different options for enabling metrics that are useful to both PBS (in an external module, if committers prefer) and anyone else who would be interested in finer-grained tracing.

To start, I *do* think that there is interest in a PBS module: if an eventually consistent store is returning stale data, how stale *is* it? Especially given that many (most?) Cassandra client libraries (including the Datastax java-driver) choose CL=ONE by default, I'd expect most users would prefer to understand how their choice of N,R, and W affects their latency and consistency.

I've been contacted by several Cassandra users who are interested in and/or using this functionality and understand that several developers are interested in PBS for Riak (notably, Andy Gross highlighted PBS in his 2013 RICON East keynote as a useful feature Basho would like). We originally chose Cassandra based on our familiarity with the code base and on early discussions with Jonathan but we plan to integrate PBS functionality into Riak with the help of their committers in the near-term future. So I do think there is interest, and, if you're curious about *use cases* for this functionality, Shivaram and I will be demoing PBS in Cassandra 1.2 at the upcoming SIGMOD 2013 conference. Our demo proposal sketches three application vignettes, including the obvious integration with monitoring tools but also automatically tuning N,R, and W and and providing consistency and latency SLAs:
http://www.bailis.org/papers/pbs-demo-sigmod2013.pdf

So, on the more technical side, there are two statistics that aren't currently measured (in trunk) that are required for accurate PBS predictions. First, PBS requires per-server statistics. Currently, the ColumnFamily RTT read/write latency metrics are aggregated across all servers. Second, PBS requires a measure how how long a read/write request takes before it is processed (i.e., how long it took from a client sending  each read/write request to when it was performed). This requires knowledge of one-way request latencies as well as read/write request-specific logic.

The 1.2 PBS patch provided both of these, aggregating by server and measuring the delay until processing. As Jonathan notes above, the latter measurement was conservative; the remote replica recorded the time that it enqueued its response rather than the exact moment a read or write was performed, namely for simplicity of code. The coordinating server could then closely approximate the return time as RTT-(remote timestamp).

Given these requirements and the current state of trunk, there are a few ways forward to support an external PBS prediction module:

1a.) Modify Cassandra to store latency statistics on a per-server and per-ColumnFamily granularity. As Rick Branson has pointed out, this is actually useful for monitoring other than PBS and can be used to detect slower replicas.

1b.) Modify Cassandra to store local processing times for requests (i.e., expand StorageMetrics, which currently does not track the time required to, say, fulfill a local read stage). This also has the benefit of understanding whether a Cassandra node is slow due to network or disk.

2.) Use the newly developed tracing functionality to reconstruct latencies for selected requests. Performing any sort of profiling will require tracing to be enabled (this appears to be somewhat heavyweight given the amount of data that is logged for each request , and reconstructing latencies from the trace table may be expensive (i.e., amount to a many-way self-join).

3.) Use RTT/2 based on ColumnFamily LatencyMetrics as an inaccurate but already supported external predictor.

4.) Leave the PBS latency sampling as in 1.2 but remove the PBS predictor code. Expose the latency samples via an Mbean for users like Rick who would benefit from it.

Proposal #1 has benefits for many users and seems a natural extension to the existing metrics but requires changes to the existing code. Proposal #2 puts substantial burden on an end-user and, without a fixed schema for the trace table, may amount to a fair bit of code munging. Proposal #3 is inaccurate but works on trunk. Proposal #4 is essentially 1.2.0 without the requirement to maintain any PBS-specific code and is a reasonable stop-gap before proposal #1. All of these proposals are amenable to sampling.

I'd welcome your feedback on these proposals and next steps.;;;","16/May/13 05:15;jbellis;I'd prefer option 1 but I'm also fine with option 3.

For option 1 we do need to distinguish between ""track latency stuff on a per-CF basis"" (which is universally useful as, say, an EstimatedHistogram) and ""keep a window of individual latency times"" which is pretty much only a PBS requirement.  As I said above, we'd want to keep the latter disabled unless specified otherwise.;;;","25/May/13 01:06;pbailis;Okay. #1 will likely require more extensive code changes: basically, it'll require EstimatedHistograms for each of the servers acting as replicas for a given ColumnFamily and will require EstimatedHistogram tracing in the StorageProxy (to separate network-based latency from disk-based latency). Are these changes feasible?

re: ""a window of individual latency times,"" looking at the Metrics implementation of EstimatedHistogram, EstimatedHistogram.values() should provide a reasonable enough sample (especially since, as you mention, since it has other uses as well).

Perhaps the simplest strategy is to go with #3 for now but implement #1 in the future if there's interest. #3 is easy; I've already written an example external module to do RTT/2 predictions: https://github.com/pbailis/pbs-predictor/blob/9d31acd1667b08affa609278689b540d8e0380f5/pbspredictor/src/main/java/edu/berkeley/pbs/cassandra/CassandraLatencyTrace.java
;;;","27/May/13 15:32;jbellis;bq. I've already written an example external module to do RTT/2 predictions

Do we need any core changes at all, then?  (Under the ""#3 for now"" plan.);;;","28/May/13 03:51;pbailis;bq. Do we need any core changes at all, then? (Under the ""#3 for now"" plan.)

Nope; the predictor I linked uses the per-CF latency metrics. The downside is accuracy.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Changing column_index_size_in_kb on different nodes might corrupt files,CASSANDRA-5454,12642024,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,11/Apr/13 12:29,16/Apr/19 09:32,14/Jul/23 05:53,01/Jul/13 07:41,1.2.7,2.0 beta 1,,,,,0,,,,,"RangeTombstones requires that we sometimes repeat a few markers in the data file at index boundaries. Meaning that the same row with different column_index_size_in_kb will not have the same data size.

This is a problem for streaming, because if the column_index_size_in_kb is different in the source and the destination, the resulting row should have a different size on the destination, but streaming rely on the data size not changing in 1.2.

Now, while having different column_index_size on different nodes is probably not extremely useful in the long run, you may still have temporal discrepancies because there is no real way to change the setting on all node atomically. Besides, it's not to hard to get different setting on different nodes due to human error. And currently, the result is that if a file is stream while the setting is not consistent, then we'll end up corrupting the received file (due to the fix from CASSANDRA-5418 to be precise).

I don't see a good way to fix this in 1.2, so users will have to be careful not to have streaming happening while they change the column_index_size_in_kb setting. But in 2.0, once CASSANDRA-4180 is committed, we won't have the problem of having to respect the dataSize from the source on the destination anymore. So basically we should revert the fix from CASSANDRA-5418 (though we may still want to avoid repeating unneeded marker, but the tombstoneTracker can give us that easily).",,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4180,"27/Jun/13 14:28;slebresne;5454.txt;https://issues.apache.org/jira/secure/attachment/12589912/5454.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322438,,,Mon Jul 01 07:41:58 UTC 2013,,,,,,,,,,"0|i1jmrj:",322783,,,,,,,,,jbellis,,jbellis,Normal,,1.2.5,,,,,,,,,,,,,,,,"27/Jun/13 14:28;slebresne;Attaching patch for this. As said above, this basically revert the changes from CASSANDRA-5418, which is ok now that we don't write the row size or column count at the start of the row.

I've checked that the test added by Yukim for CASSANDRA-5418 does pass with this patch. ;;;","29/Jun/13 05:38;jbellis;+1;;;","01/Jul/13 07:41;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AssertionError exception in system.log,CASSANDRA-5453,12641987,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,winsdom,winsdom,11/Apr/13 06:23,16/Apr/19 09:32,14/Jul/23 05:53,15/Jan/14 09:41,,,,,,,0,,,,,"We've lot of error appeared in one node,
ERROR [Thrift:91817] 2013-03-24 03:26:17,528 AbstractCassandraDaemon.java (line 134) Exception in thread Thread[Thrift:91817,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.SliceFromReadCommand.maybeGenerateRetryCommand(SliceFromReadCommand.java:78)
        at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:775)
        at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:605)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:115)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:270)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:354)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:314)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.getResult(Cassandra.java:2847)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.getResult(Cassandra.java:2835)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:186)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
After such error occur, it will crash. Restart cassandra will work temporary, but a month later, the case still happened. Not sure if it's due to data corrupted.",five nodes in cluster running on Cent O.S 5.4,lyubent,winsdom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322401,,,Wed Jan 15 09:41:07 UTC 2014,,,,,,,,,,"0|i1jmjb:",322746,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"15/Jan/14 09:33;lyubent;[~winsdom] Could you add more details about your environment, what version of C*, what RF, what load does your cluster handle? Were you doing anything in particular that triggered this?;;;","15/Jan/14 09:41;winsdom;Hi Lyuben,
    The issue has fixed after we upgrade to 1.1.11. I'll mark the ticket as closed now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Allow empty blob literals in CQL3,CASSANDRA-5452,12641878,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,10/Apr/13 18:07,16/Apr/19 09:32,14/Jul/23 05:53,11/Apr/13 09:22,1.2.5,,,,,,0,,,,,"The current grammar don't allow empty blob literals (so '0x'). The goal here is to allow the following syntax for that:
{noformat}
INSERT INTO test(k, b) VALUES (0, 0x)
{noformat}
I'll admit that '0x' is not the most beautiful syntax ever, but I think that's the only thing that make sense.

I'll note that currently there is 2 workaround to insert empty blobs: you can either use prepared statement (not a bad idea when using blobs anyway) or, because we've deprecated but still support until 2.0 using strings as blob (to allow upgrade from 1.2.0 to 1.2.1), you can use an empty string. I'll note that this latter workaround will trigger a deprecation warning in the log however and will stop working in 2.0.",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Apr/13 18:08;slebresne;5452.txt;https://issues.apache.org/jira/secure/attachment/12578046/5452.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322293,,,Thu Apr 11 09:22:22 UTC 2013,,,,,,,,,,"0|i1jlvb:",322638,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"10/Apr/13 18:08;slebresne;Trivial patch attached.;;;","10/Apr/13 21:09;aleksey;+1;;;","11/Apr/13 09:22;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cluster : Host ID collision,CASSANDRA-5450,12641826,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,nivance,nivance,10/Apr/13 12:39,16/Apr/19 09:32,14/Jul/23 05:53,11/Apr/13 02:14,1.2.0,,,Local/Config,,,0,cluster,,,,"I follow the guide ""Initializing a multiple node cluster"" in url:
http://www.datastax.com/docs/1.2/initialize/cluster_init

the exception occurs below when start the second node:

java.lang.RuntimeException: Host ID collision between active endpoint /192.168.0.193 and /192.168.0.194 (id=4ebdbeea-2712-475b-a3e3-64b6e6b099a9)
	at org.apache.cassandra.locator.TokenMetadata.updateHostId(TokenMetadata.java:227)
	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1296)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1157)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1895)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:805)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:883)
	at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:43)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)


I track the source code. the function ""getHostId"" in class org.apache.cassandra.gms.Gossiper, diff endpoint it returns the same value,like:
/192.168.0.194	4ebdbeea-2712-475b-a3e3-64b6e6b099a9
/192.168.0.88	c375010a-c464-40c9-a7db-61de319a4cba
/192.168.3.21	4ebdbeea-2712-475b-a3e3-64b6e6b099a9
/192.168.0.193	4ebdbeea-2712-475b-a3e3-64b6e6b099a9
so, the exception occurs.


Is it a bug or is my Configuration wrong?
","centos-6-x86_64, jdk1.6.0_30, cassandra1.2.0
4 nodes, 2 datacenter, 2 nodes for each datacenter",nivance,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,60,60,,0%,60,60,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322241,,,Thu Apr 11 02:18:13 UTC 2013,,,,,,,,,,"0|i1jljr:",322586,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"10/Apr/13 13:23;brandon.williams;bq. Is it a bug or is my Configuration wrong?

Are these virtual machines you cloned or in any way copied the system tables between?;;;","11/Apr/13 01:42;nivance;yes, I make one node configured, then tar the dir of cassandra, scp to the other three nodes.;;;","11/Apr/13 02:13;nivance;It's my fault. 

The system tables of Every node is the same.

The cluster run well after I delete the system tables in each node.

Thks Williams.;;;","11/Apr/13 02:18;brandon.williams;You're welcome.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
counters upgrade test fails,CASSANDRA-5448,12641689,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,09/Apr/13 19:00,16/Apr/19 09:32,14/Jul/23 05:53,09/May/14 01:40,,,,,,,0,,,,,"{noformat}
Test for bug of #4436 ... FAIL

======================================================================
FAIL: Test for bug of #4436
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/srv/cassandra-dtest/counter_tests.py"", line 108, in upgrade_test
    check(1)
  File ""/srv/cassandra-dtest/counter_tests.py"", line 96, in check
    assert row[1] == i * updates, ""Unexpected value %s"" % str(row)
AssertionError: Unexpected value [23, 5011]

----------------------------------------------------------------------
{noformat}

I tested as far back as 1.2.0, so this is pretty old.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322105,,,Fri May 09 01:40:42 UTC 2014,,,,,,,,,,"0|i1jkpj:",322450,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"10/Apr/13 15:49;slebresne;I think the test is to blame. It uses 2 nodes with RF=2, but uses CL.ONE to do his inserts and checks, which looks wrong. I pushed a fix to that test to use QUORUM instead. With that, I was able to get the test go past line 108 ... until line 109. But then it broke because I was using the current 1.2 branch and CASSANDRA-5187 has broken ccm, so I'll need to fix ccm. I'll test on some older C* version but the test is so damn long that fell free to test it on your side.;;;","30/Sep/13 22:22;jbellis;How does this look now that ccm is fixed?;;;","09/May/14 01:40;brandon.williams;Fixed as of CASSANDRA-7036;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Include fatal errors in trace events,CASSANDRA-5447,12641685,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,09/Apr/13 18:50,16/Apr/19 09:32,14/Jul/23 05:53,09/Apr/13 19:10,1.2.5,,,Legacy/Tools,,,0,qa-resolved,,,,This would help tracking down which query is causing errors.,,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/13 18:51;jbellis;5447.txt;https://issues.apache.org/jira/secure/attachment/12577853/5447.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322101,,,Tue Apr 09 19:10:58 UTC 2013,,,,,,,,,,"0|i1jkon:",322446,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,enigmacurry,,,"09/Apr/13 18:51;jbellis;Trivial patch attached.;;;","09/Apr/13 18:59;aleksey;+1;;;","09/Apr/13 19:10;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PerRowSecondaryIndex isn't notified of row-level deletes,CASSANDRA-5445,12641658,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,samt,samt,samt,09/Apr/13 16:15,16/Apr/19 09:32,14/Jul/23 05:53,09/Apr/13 21:55,1.2.5,,,,,,0,,,,,"Following CASSANDRA-5297, the way PerRowSecondaryIndex updates are handled in AtomicSortedColumns is still not right as it doesn't cater for row level deletes properly. The key is only added MixedIndexUpdater's list of deferred updates if there's a column value being modified. Where an entire row is deleted, we never hit that code so the indexer.commit() in ASC.addAllWithSizeDelta becomes a no-op & the index is not correctly updated. 

Also, SecondaryIndexManager.updaterFor is not actually as efficient as it first seems. For a CF with no per-column indexes and a single per-row index defined, during compaction (when includeRowIndexes == false), we'd expect to be using nullUpdater to no-op the index update for each row being compacted given the updaterFor implementation:

{code}
	return (includeRowIndexes && !rowLevelIndexMap.isEmpty())
           		? new MixedIndexUpdater(key)
               		: indexesByColumn.isEmpty() ? nullUpdater : new PerColumnIndexUpdater(key);
{code}

However, this isn't the case as indexesByColumn is never empty, the reason being that any index *must* be attached to a column, there's just no way to register a PRSI without attaching it to one of the CF's columns. So where a PRSI is present, a new PCIU instance is created for each row during compaction regardless. With that in mind, I'd propose removing the includeRowLevelIndexes argument and just returning nullIndexer if no indexes (PRSI or PRCI) are configured (although I totally acknowledge that fixing index registration so we can register a PRSI without attaching it to a column would be desirable in the long-term).",,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/13 16:16;samt;5445.txt;https://issues.apache.org/jira/secure/attachment/12577817/5445.txt",,,,,,,,,,,,,,,,,,,,1.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,322074,,,Tue Apr 09 21:55:39 UTC 2013,,,,,,,,,,"0|i1jkin:",322419,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"09/Apr/13 16:16;samt;patch against 1.2 attached;;;","09/Apr/13 21:55;jbellis;LGTM; committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ISE during short reads,CASSANDRA-5440,12641485,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,08/Apr/13 23:32,16/Apr/19 09:32,14/Jul/23 05:53,09/Apr/13 22:05,2.0 beta 1,,,,,,0,,,,,"On trunk:

{noformat}
ERROR [Thrift:2] 2013-04-08 15:06:56,468 ProcessFunction.java (line 41) Internal error processing execute_cql3_query
java.lang.IllegalStateException
    at java.util.AbstractList$Itr.remove(AbstractList.java:356)
    at org.apache.cassandra.db.filter.SliceQueryFilter.trim(SliceQueryFilter.java:187)
    at org.apache.cassandra.db.SliceFromReadCommand.maybeTrim(SliceFromReadCommand.java:101)
    at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:902)
    at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:831)
    at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:128)
    at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:56)
    at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:130)
    at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:141)
    at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1836)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4232)
    at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4216)
    at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:39)
    at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:39)
    at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Apr/13 21:22;jbellis;5440.txt;https://issues.apache.org/jira/secure/attachment/12577894/5440.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,321901,,,Tue Apr 09 22:05:25 UTC 2013,,,,,,,,,,"0|i1jjg7:",322246,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"08/Apr/13 23:32;brandon.williams;I'll bisect this.;;;","09/Apr/13 18:08;brandon.williams;Bisect says:

{noformat}
There are only 'skip'ped commits left to test.
The first bad commit could be any of:
aa76394764bcb4af54150f12528fed9ddfa66044
fbe99b711aa9be3fcfd1e5e0f37c4d45a1717864
{noformat}

Obviously between these it has to be aa76394764bcb4af54150f12528fed9ddfa66044 which is CASSANDRA-5403;;;","09/Apr/13 21:22;jbellis;patch attached.;;;","09/Apr/13 21:35;brandon.williams;+1;;;","09/Apr/13 22:05;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
allow STCS options to apply to the L0 compaction performed by LCS,CASSANDRA-5439,12641419,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,carlyeks,jbellis,jbellis,08/Apr/13 18:21,16/Apr/19 09:32,14/Jul/23 05:53,21/Apr/13 23:08,2.0 beta 1,,,,,,0,lcs,,,,(See CASSANDRA-5371),,carlyeks,jjordan,rcoli,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Apr/13 02:54;jbellis;5439-v2.txt;https://issues.apache.org/jira/secure/attachment/12578632/5439-v2.txt","13/Apr/13 20:43;carlyeks;5439.patch;https://issues.apache.org/jira/secure/attachment/12578617/5439.patch",,,,,,,,,,,,,,,,,,,2.0,carlyeks,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,321835,,,Sun Apr 21 23:08:36 UTC 2013,,,,,,,,,,"0|i1jj1j:",322180,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"13/Apr/13 20:43;carlyeks;Patch adds a new file, SizeTieredCompactionStrategyOptions, which is used by both the SizeTiered and the Leveled compaction strategies.;;;","14/Apr/13 02:53;jbellis;v2 attached to pass the options from LCS constructor into the manifest.;;;","21/Apr/13 19:09;carlyeks;Changes to the original patch look good.;;;","21/Apr/13 23:08;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair Freeze/Gossip Invisibility Issues 1.2.4,CASSANDRA-5432,12641162,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,vijay2win@yahoo.com,arya,arya,06/Apr/13 01:02,16/Apr/19 09:32,14/Jul/23 05:53,06/May/13 07:16,1.2.5,,,,,,1,,,,,"Read comment 6. This description summarizes the repair issue only, but I believe there is a bigger problem going on with networking as described on that comment. 


Since I have upgraded our sandbox cluster, I am unable to run repair on any node and I am reaching our gc_grace seconds this weekend. Please help. So far, I have tried the following suggestions:

- nodetool scrub
- offline scrub
- running repair on each CF separately. Didn't matter. All got stuck the same way.

The repair command just gets stuck and the machine is idling. Only the following logs are printed for repair job:

 INFO [Thread-42214] 2013-04-05 23:30:27,785 StorageService.java (line 2379) Starting repair command #4, repairing 1 ranges for keyspace cardspring_production
 INFO [AntiEntropySessions:7] 2013-04-05 23:30:27,789 AntiEntropyService.java (line 652) [repair #cc5a9aa0-9e48-11e2-98ba-11bde7670242] new session: will sync /X.X.X.190, /X.X.X.43, /X.X.X.56 on range (1808575600,42535295865117307932921825930779602032] for keyspace_production.[comma separated list of CFs]
 INFO [AntiEntropySessions:7] 2013-04-05 23:30:27,790 AntiEntropyService.java (line 858) [repair #cc5a9aa0-9e48-11e2-98ba-11bde7670242] requesting merkle trees for BusinessConnectionIndicesEntries (to [/X.X.X.43, /X.X.X.56, /X.X.X.190])
 INFO [AntiEntropyStage:1] 2013-04-05 23:30:28,086 AntiEntropyService.java (line 214) [repair #cc5a9aa0-9e48-11e2-98ba-11bde7670242] Received merkle tree for ColumnFamilyName from /X.X.X.43
 INFO [AntiEntropyStage:1] 2013-04-05 23:30:28,147 AntiEntropyService.java (line 214) [repair #cc5a9aa0-9e48-11e2-98ba-11bde7670242] Received merkle tree for ColumnFamilyName from /X.X.X.56

Please advise. ","Ubuntu 10.04.1 LTS
C* 1.2.3
Sun Java 6 u43
JNA Enabled
Not using VNodes",arya,colinkuo,ondrej.cernos,rcoli,slebresne,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5332,,,,,CASSANDRA-5493,,,,,,,,,,,,,,,"25/Apr/13 21:10;vijay2win@yahoo.com;0001-CASSANDRA-5432.patch;https://issues.apache.org/jira/secure/attachment/12580591/0001-CASSANDRA-5432.patch",,,,,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,321578,,,Mon May 06 07:16:38 UTC 2013,,,,,,,,,,"0|i1jhgf:",321923,,,,,,,,,brandon.williams,,brandon.williams,Critical,,,,,,,,,,,,,,,,,,"06/Apr/13 03:50;yukim;If that's the only log you get so far, then the node is waiting merkle tree response from /x.x.x.190.
Check if you have any error on that node.
;;;","06/Apr/13 06:07;arya;Thanks Yuki. node x.x.x.190 is the node which I triggered repair on (self). The logs above belong to that node and stop right there. There is no error or exception.;;;","06/Apr/13 06:16;arya;I really need to get this going before sunday. I also looked at all other nodes logs, and nothing interesting. ;;;","06/Apr/13 09:14;arya;OK, I found the problem, but something is changed in this release regarding the networking that is not clear to me. I use EC2. I had to open all TCP ports to the world for the repairs to work. They didn't even work when I allowed all TCP within our C*'s security group. This is not acceptable as it is a security risk. What was changed in 1.2.3 in terms of repair routing? Shouldn't it just use the storage port?

We use Ec2MultiRegionSnitch, so it returns DNS that resolved to local ips for in-region communication and public ips for cross-region communication. I have a C* 1.1.10 cluster in production and it is working fine without having to open the security group wide open. 

Please advice.;;;","06/Apr/13 15:40;brandon.williams;Nothing changed with port usage.  There are standard ways to see what ports a process is using to check this, though.;;;","16/Apr/13 21:44;arya;I narrowed this down to non-ssl storage port, and they must be opened on the public IPs. Here are the steps to reproduce:

This is a working configuration:
Cassandra 1.1.10 Cluster with 12 nodes in us-east-1 and 12 nodes in us-west-2
Using Ec2MultiRegionSnitch and SSL enabled for DC_ONLY and NetworkTopologyStrategy with strategy_options: us-east-1:3;us-west-2:3;
C* instances have a security group called 'cluster1'
security group 'cluster1' in each region is configured as such
Allow TCP:
7199 from cluster1 (JMX)
1024 - 65535 from cluster1 (JMX Random Ports)
7100 from cluster1 (Configured Normal Storage)
7103 from cluster1 (Configured SSL Storage)
9160 from cluster1 (Configured Thrift RPC Port)
9160 from <client_group>
foreach node's public IP we also have this rule set to enable cross region comminication:
7103 from public_ip

The above is a functioning and happy setup. You run repair, and it finishes successfully.

Broken Setup:

Upgrade to 1.2.4 without changing any of the above security group settings:

Run repair. The repair will not receive the Merkle Tree for itself. Thus hanging. See description. The test in description was done with one region with strategy of us-east-1:3, but other settings were exactly the same.

Now for each public_ip add a security group rule as such to cluster1 security group:

Allow TCP: 7100 from public_ip

Run repair. Things will magically work now. 

If nothing in terms of port and networking has changed in 1.2, then why the above is happening? I can constantly reproduce it. 

This also affects gossip. If you don't have the JMX Ports open on public ips, then gossip would not see any node except itself after a snap restart of all nodes all at once. 

;;;","16/Apr/13 22:21;jbellis;Gossip does not touch JMX.  JMX is not used internally at all; it's only there to let nodetool invoke methods.

Please see the user mailing list for troubleshooting help, Jira is not a good place for that.;;;","16/Apr/13 22:24;arya;I have used the IRC channel already. It was suggested to me to open a JIRA ticket as no one could help.;;;","16/Apr/13 22:39;arya;I added a correction. It is not JMX Jonathan, you are right. It is opening the non-ssl storage port on public IPs that fixes it. We didn't have to do this on 1.1.10.;;;","17/Apr/13 08:35;jbellis;You said above that you had it configured this way in 1.1 as well:

{quote}
7100 from cluster1 (Configured Normal Storage)
7103 from cluster1 (Configured SSL Storage)
{quote}

In any case, it is not a bug for you to need both open; Cassandra will use SSL between datacenters (regions), and non-ssl on the private IP within the same one.;;;","19/Apr/13 05:29;arya;> non-ssl on the private IP within the same one [region]

OK, a little more digging, and I found the root cause which I believe is a bug, so I am re-opening this.

See this log snippet for a repair sessions I triggered on a node in a single region in AWS:

 INFO [AntiEntropySessions:1] 2013-04-19 04:28:16,587 AntiEntropyService.java (line 651) [repair #8e59b7c0-a8a9-11e2-ba85-d39d57f66b97] new session: will sync /54.242.X.YYY, /54.224.XX.YYY, /50.17.XXX.YYY on range (99249023685273718510150927169407637270,127605887595351923798765477788721654890] for cardspring_production.[App]
 INFO [AntiEntropySessions:1] 2013-04-19 04:28:16,591 AntiEntropyService.java (line 857) [repair #8e59b7c0-a8a9-11e2-ba85-d39d57f66b97] requesting merkle trees for App (to [/54.224.XX.YYY, /50.17.XXX.YYY, /54.242.X.YYY])
DEBUG [WRITE-/50.17.159.210] 2013-04-19 04:28:16,592 OutboundTcpConnection.java (line 260) attempting to connect to /10.170.XX.YYY
DEBUG [WRITE-/54.224.36.214] 2013-04-19 04:28:16,593 OutboundTcpConnection.java (line 260) attempting to connect to /10.121.XX.YYY
DEBUG [WRITE-/54.242.1.111] 2013-04-19 04:28:16,593 OutboundTcpConnection.java (line 260) attempting to connect to /54.242.X.YYY

Notice the last line. This is the public IP of the node running repair. Why is this picking up the public ip address for itself to send the tree request? This is the source of problem. In AWS you cannot communicated through public ip address with security group rules that are defined based on group names in the same region, which is a common use case. Hence the tree request gets stuck at sending point to itself. 



;;;","19/Apr/13 17:44;vijay2win@yahoo.com;Arya, 
The first time we start the communication to a node we try to Initiate communications we use the public IP and eventually once we have the private IP we will switch back to local ip's.

I am confused with the analysis, because the nodes should have been connected and communicating and Tree request is another message in the same channel as any other message. 
Are the nodes up in the first place?

{code}
                this.treeRequests = new RequestCoordinator<TreeRequest>(isSequential)
                {
                    public void send(TreeRequest r)
                    {
                        MessagingService.instance().sendOneWay(r.createMessage(), r.endpoint);
                    }
                };
{code};;;","20/Apr/13 00:07;arya;Hey Vijay,

Good to see you here. Sorry if my analysis is unclear. Here is my take:

> The first time we start the communication to a node we try to Initiate communications we use the public IP and eventually once we have the private IP we will switch back to local ip's.

Has this always been the case? Because if you are using public ips (not public dns name), there has to be explicit security rules on public ips to allow this. Otherwise, if in security groups you are opening the ports to the machines in the same group using their security group name, it allows traffic only within their private ips, so this won't work. 

We use Priam (your awesome tooling), and as you know, it opens up only the SSL port on the public IPs for cross region communication. And from the operator's perspective, that is the correct thing to do. I only have the SSL port open on public IPs and don't want to open the non SSL port for security reasons. Now, all other ports like non SSL, JMX, etc are opened the way I described using security group names and it allows traffic on private IPs. It is just the way AWS has been. So, if within the same region, you are trying to connect to any machine using public ip, it won't work. 

Here is how I achieved the scenario above and I believe they are all co-related to the statement you said that all machine connect to public IPs first.

Setup a cluster as I described in my previous comment. It can be a single region. Restart all machines at the same time. Each machine would only see itself as UP. Everyone else is reported to be DOWN in nodetool ring. I am guessing that it is because they are trying to send gossips to public IPs but only SSL port is open on public IPs. The cluster is configured to only do SSL cross datacenter/region not within the same region. So, now I am left with bunch of nodes that only see themselves in the ring. I go to my AWS console, open up the non SSL port on every single public IP in that security group. Now all the nodes see each other. 

By now, I had a theory about nodes wanting to communicate through the public ip which is not possible, so I stepped into troubleshooting repairs. I know that with current settings repair would succeed. Since the nodes see each other now, I go to security groups and remove the non SSL on public IP rules that I added in previous step. Start the repair, and I ended up with the log message as above. The public ip mentioned in the log, belongs to the node that owns the log and is running repair, so it tried to communicated to itself using its own public IP. 

Did I make sense? I can call you to describe it over the phone, but basically this setup used to work on 1.1.10 but does not work on 1.2.4. I have attached the debugger to a node and am trying to trace  the code. I'll let you know if I find something new.;;;","20/Apr/13 05:29;vijay2win@yahoo.com;Hi Arya, Thanks and you can call me anytime but it will help others if we keep the discussion here.

{quote}
Has this always been the case? 
{quote}
As far as i know, yes.

{quote}
 I go to security groups and remove the non SSL on public IP rules that I added in previous step.
{quote}
I think you should not remove the IP's. Priam opens up ports for the local nodes and also the remote nodes within the security group (http://goo.gl/l9Q1T). Looks like you shouldn't do the above because you are now disabling cassandra from restarting the connections.

Also the reason you are seeing all the nodes to be UP in a multi region case event though they cannot communicate within the DC is because of the issue mentioned in CASSANDRA-3533, I can almost bet that the read/write requests will be failing in the local DC, If not try after restarting nodes. :)

Let me know if you still have issues or disagree.
;;;","21/Apr/13 02:53;arya;Priam only opens one port, and that is the SSL port on public IPs (see line 74): http://goo.gl/vY8WX 

I did not remove the IPs from security group. I left the IP rules for the SSL port as were set by Priam. I only remove the NON SSL port rules on public IPs which I had added manually to work around this issue.;;;","21/Apr/13 23:54;vijay2win@yahoo.com;Priam opens port for other DC's to talk to each other but nothing to do within, i still doubt the SG setup coz all IP's within a security group should be opened for both ports. 
May be CASSANDRA-5171 created a side effect, which i am not sure.

[~jbrown] do you mind verifying it with 1.2.4? Verifying it with Priam is a bigger undertaking for me now :);;;","23/Apr/13 06:06;arya;I was actually suspicious about that. I can roll back that patch and try it. Give me till end of the week. My hands are tied up right now.;;;","24/Apr/13 23:38;arya;So, I rolled back CASSANDRA-5171. Pushed it to my test cluster. The gossip issue where nodes after restart didn't see each other got fixed. The repair still tried to connect to the machine running repair (self) with its public IP for requesting MerkleTree where it gets stuck, so it has the same issue. Some behavior changed though, and the OutBoundTCPConnection didn't report connecting to other 2 replicas for requesting MerkleTree, so I only saw the message when trying to connect. Here is the snippet: 

 INFO [Thread-458] 2013-04-24 23:21:16,543 StorageService.java (line 2407) Starting repair command #1, repairing 1 ranges for keyspace app_production
DEBUG [Thread-458] 2013-04-24 23:21:16,580 StorageService.java (line 2547) computing ranges for 1808575600, 7089215977519551322153637656637080005, 14178431955039102644307275311465584410, 4253529586511
7307932921825930779602030, 49624511842636859255075463585608106435, 56713727820156410577229101240436610840, 85070591730234615865843651859750628460, 92159807707754167187997289514579132865, 9924902368527
3718510150927169407637270, 127605887595351923798765477788721654890, 134695103572871475120919115443550159295, 141784319550391026443072753098378663700
 INFO [AntiEntropySessions:1] 2013-04-24 23:21:16,587 AntiEntropyService.java (line 651) [repair #a9a87e40-ad35-11e2-945a-050d956ff11b] new session: will sync /YYY.XX.98.11, /YY.XXX.107.137, /YY.XXX.133.163 on range (99249023685273718510150927169407637270,127605887595351923798765477788721654890] for cardspring_production.[App]
 INFO [AntiEntropySessions:1] 2013-04-24 23:21:16,598 AntiEntropyService.java (line 857) [repair #a9a87e40-ad35-11e2-945a-050d956ff11b] requesting merkle trees for App (to [/XX.YYY.107.137, /XX.YYY.133.163, /XXX.YY.98.11])
DEBUG [WRITE-/107.20.98.11] 2013-04-24 23:21:16,601 OutboundTcpConnection.java (line 260) attempting to connect to /XXX.YY.98.11
 INFO [AntiEntropyStage:1] 2013-04-24 23:21:19,111 AntiEntropyService.java (line 213) [repair #a9a87e40-ad35-11e2-945a-050d956ff11b] Received merkle tree for App from /XX.YYY.133.163
DEBUG [ScheduledTasks:1] 2013-04-24 23:21:19,409 GCInspector.java (line 121) GC for ParNew: 54 ms for 1 collections, 669806384 used; max is 4211081216
 INFO [AntiEntropyStage:1] 2013-04-24 23:21:20,408 AntiEntropyService.java (line 213) [repair #a9a87e40-ad35-11e2-945a-050d956ff11b] Received merkle tree for App from /XX.YYY.107.137

See the debug line with OutboundTcpConnection. It is trying to connect to public IP of self (XXX.YY.98.11), which is still an issue. What I was expecting to see before this line was two other consecutive lines like before where it showed OutboundTcpConnection trying to connect to other nodes as well. Despite them returning the MerkleTrees, those log lines did not show. So, connection was made successfully to the other nodes somehow. ;;;","25/Apr/13 09:16;ondrej.cernos;I have exactly the same issue as Arya.

I also had to open non-SSL ports from within the datacenter in order to create the cluster.

I was wondering if it could be a networking issue (we use mixed aws-private cloud setup), so it is good to see we are not alone with this.;;;","25/Apr/13 09:25;ondrej.cernos;Please see also CASSANDRA-5493 - the MessagingService also reports dropped messages on _itself_ using it's public IP. The output displays 3 public IPs and 2 private (the private IP of the node itself is not included), while the remote DC is reported correctly. This seems related.;;;","25/Apr/13 21:10;vijay2win@yahoo.com;attached reverts CASSANDRA-5171 and adds handling of local endpoint.

Arya, mind testing this?;;;","26/Apr/13 00:40;arya;Sure, I should be able to get back to you either tonight or tomorrow.;;;","26/Apr/13 05:06;arya;+1 works for me. Thank you.;;;","29/Apr/13 15:02;jbellis;Why does ""let's use the last-known location of this node"" cause problems?;;;","29/Apr/13 17:05;vijay2win@yahoo.com;The problem is that we need Private_ip to communicate within DC/region is not available until the gossiping with nodes. 
Since we dont have the private information but we do have the rest (DC/RACK), we are trying to connect via public IP.

Removing that optimization forces us to assume it is in other DC and hence using public IP and SSL port, eventually when we receive the private IP we reset the status to use the right (private_ip) connection.
You may ask why not store the private IP? well we could but currently the reset connection (to private IP) logic is in the snitch.;;;","01/May/13 12:04;brandon.williams;I never thought CASSANDRA-5171 was a really big gain anyway, but it looked innocuous enough at the time. +1 on reverting it.;;;","06/May/13 07:16;slebresne;Took the liberty to commit as I want to re-roll 1.2.5.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-shuffle with JMX usernames and passwords,CASSANDRA-5431,12641138,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,michalm,edong,edong,05/Apr/13 21:44,16/Apr/19 09:32,14/Jul/23 05:53,09/Apr/13 03:23,1.2.4,,,,,,0,,,,,"Unlike nodetool, cassandra-shuffle doesn't allow passing in a JMX username and password. This stops those who want to switch to vnodes from doing so if JMX access requires a username and a password.

Patch to follow.",,dbrosius,edong,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Apr/13 16:17;michalm;5431-v2.txt;https://issues.apache.org/jira/secure/attachment/12577556/5431-v2.txt","08/Apr/13 20:45;michalm;5431-v3.txt;https://issues.apache.org/jira/secure/attachment/12577615/5431-v3.txt","05/Apr/13 22:06;edong;CASSANDRA-5431-whitespace.patch;https://issues.apache.org/jira/secure/attachment/12577303/CASSANDRA-5431-whitespace.patch",,,,,,,,,,,,,,,,,,3.0,michalm,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,321554,,,Tue Apr 09 03:23:37 UTC 2013,,,,,,,,,,"0|i1jhb3:",321899,,,,,,,,,dbrosius,,dbrosius,Normal,,,,,,,,,,,,,,,,,,"05/Apr/13 22:06;edong;My intended changes touch files that have formatting issues relative to the Cassandra formatter settings for Eclipse, so I'm posting a whitespace patch first.;;;","08/Apr/13 08:12;michalm;As far as I remember I wrote a patch for this some time ago when experimenting a bit with switching to vnodes. If you did not start to work on this task (so I won't ""double"" your work ;-) ), I'll check it later today. ;;;","08/Apr/13 16:17;michalm;Yup, I have it (updated to make it apply on your whitespace patch). ;;;","08/Apr/13 18:04;edong;Hi Michał,

Thanks for the information--- I have not started work on this, so please feel free to submit your patch!;;;","08/Apr/13 20:45;michalm;OK. Attaching ""merged"" patch.;;;","08/Apr/13 22:36;dbrosius;There's quite a bunch of miscellaneous formatting changes in 5431-v3.txt that obfuscate this patch. Mind reposting a patch with only the changes for the patch itself?;;;","08/Apr/13 22:48;jbellis;v2 is just the changes, v3 is changes squashed with the whitespace patch.;;;","09/Apr/13 02:03;dbrosius;5431-v2 doesn't apply cleanly to cassandra-1.2. When fixes up are applied, patch looks good to me, on it's own.

I'd note that passing passwords as Strings is probably not a good idea as there's no way to flush the password from the server memory space, as there's no guarantee that gc will toss that string. Would be better if the interface was char[]'s that can be cleared.

But that's not this patch's problem. +1;;;","09/Apr/13 03:23;dbrosius;thanks, committed as fe8939075999359b8a26a1a0dcf1a7c7bc4531bb to cassandra-1.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Update scrub and scrubtest for single-pass compaction format,CASSANDRA-5429,12641060,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,jbellis,jbellis,05/Apr/13 15:41,16/Apr/19 09:32,14/Jul/23 05:53,08/Jul/13 17:05,2.0 beta 1,,,,,,0,,,,,,,jasobrown,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4180,,,,,,,,,,,,,,,,"26/Jun/13 23:14;jasobrown;5429-v1.diff;https://issues.apache.org/jira/secure/attachment/12589812/5429-v1.diff",,,,,,,,,,,,,,,,,,,,1.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,321476,,,Mon Jul 08 17:05:20 UTC 2013,,,,,,,,,,"0|i1jgtr:",321821,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"28/May/13 22:38;jbellis;Have you had a chance to start on this, Jason?;;;","29/May/13 12:25;jasobrown;Will try to knock it out this week.;;;","26/Jun/13 16:32;slebresne;[~jasobrown] ping. Is this still on your radar? Cause it does sound like something we need to fix for 2.0.;;;","26/Jun/13 23:14;jasobrown;Attached patch fixes Scrubber to read/scrub Descriptor versions ic and ja. 

I tested by hand using tables created from both 1.2.6 and 2.0, and was able to successfully scrub both. It'll take a day or two to sort out the test (revive ScrubTest and get some relevant test sstables in place), but the core work that we'll need in 2.0 is in this patch.;;;","27/Jun/13 16:01;slebresne;Nit: We can move the ""Index doublecheck"" log line in the else of the ""if(!sstable.descriptor.version.hasRowSizeAndColumnCount)"" since it's not really meaningful anymore in 2.0.

But otherwise lgtm, +1.

It'll be nice to resurrect the ScrubTest indeed, though it's clearly not a blocker for beta1.;;;","27/Jun/13 16:51;jasobrown;Committed to trunk (with nit addressed). Will work on the tests today.;;;","08/Jul/13 17:05;slebresne;Don't want to hold back beta1, so created CASSANDRA-5730 as a follow up to add back scrubTest.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 don't validate that collections haven't more than 64K elements,CASSANDRA-5428,12641019,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,05/Apr/13 10:56,16/Apr/19 09:32,14/Jul/23 05:53,03/Dec/13 13:56,1.2.13,,,,,,0,,,,,"This is somewhat similar to CASSANDRA-5355 but with a twist. When we serialize collections, not only does the size of the elements is limited to 64K, but the number of elements is too because it is also an unsigned short.

Now the same argument than in CASSANDRA-5355 that collections are ""places to denormalize small amounts of data"" is true here too. So the fact that collections are limited to 64K elements is something I could live with. However, we don't validate that no more than 64K elements are inserted. And in fact, we can't validate it if the elements are added one by one.

So in practice, you can insert more than 64K elements, but if you try to read it, you will only get back some subset of the collection. And the number of elements returned will correspond to the 2 last bytes of the real size (so a collection of 65536 elements will be returned as 1 element). Imo, that's more problematic.

So since unfortunately we can't validate this at insertion, I suggest that as a first step we:
# document that limitation (in http://cassandra.apache.org/doc/cql3/CQL.html typically)
# when we read a collection that has > 64K elements, we detect it and when serializing that for the client, we:
** return as much as we can, i.e. the 64K first ones
** log a warning that something is wrong

On the longer term, for 2.0, maybe we should just change the serialization format and use an int for the collection size, using an unsigned short was probably misguided. Of course that changes said serialization format so we have to bump the native protocol version for that (and thus can't do that in 1.2).",,aleksey,ondrej.cernos,slebresne,vongocminh,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5559,CASSANDRA-7698,,,,,,,,,,,,,,,,,,,"02/Dec/13 12:10;slebresne;5428.txt;https://issues.apache.org/jira/secure/attachment/12616542/5428.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,321441,,,Tue Dec 03 13:56:55 UTC 2013,,,,,,,,,,"0|i1jglz:",321786,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"05/Apr/13 13:37;jbellis;Sounds like a reasonable plan.;;;","02/Dec/13 10:54;slebresne;Attaching patch that does the part 2. above, detecting when we're about to serialize a collection with > 64K elements, logging an error and sending only the first 64K elements. On the documentation part, I've already ninja-committed a new paragraph to the doc for the limitations (no pushed online yet though).

As already said above, this is obviously not perfect and next version of the native protocol we do, we should probably switch to an int to encode collection sizes.;;;","02/Dec/13 12:10;slebresne;First version of the patch was incorrect, attaching corrected version.;;;","02/Dec/13 12:35;aleksey;+1;;;","03/Dec/13 13:56;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
disablebinary nodetool command,CASSANDRA-5425,12640639,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,michalm,j.casares,j.casares,03/Apr/13 21:59,16/Apr/19 09:32,14/Jul/23 05:53,05/Apr/13 17:03,1.2.4,,,,,,0,datastax_qa,,,,"The following commands are available via `nodetool`:
{CODE}
  disablehandoff         - Disable the future hints storing on the current node
  disablegossip          - Disable gossip (effectively marking the node dead)
  disablethrift          - Disable thrift server
{CODE}

Is it possible to get disablebinary added to help with the testing of binary client drivers?",,j.casares,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Apr/13 17:00;michalm;5425-statusbinary.txt;https://issues.apache.org/jira/secure/attachment/12577237/5425-statusbinary.txt","04/Apr/13 17:44;michalm;5425-v1.txt;https://issues.apache.org/jira/secure/attachment/12577021/5425-v1.txt",,,,,,,,,,,,,,,,,,,2.0,michalm,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,321098,,,Fri Apr 05 17:03:12 UTC 2013,,,,,,,,,,"0|i1jei7:",321443,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"03/Apr/13 22:40;brandon.williams;That seems reasonable.;;;","04/Apr/13 17:44;michalm;Disable/enable native binary transport using nodetool (plus extending one unrelated log message for thrift server).;;;","05/Apr/13 14:54;brandon.williams;Committed, thanks!;;;","05/Apr/13 15:22;jjordan;Should we add ""statusbinary"" as well?  Like we have statusthrift?;;;","05/Apr/13 16:59;michalm;Good point, I think it's reasonable.;;;","05/Apr/13 17:00;michalm;Added statusbinary command.;;;","05/Apr/13 17:03;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
nodetool repair -pr on all nodes won't repair the full range when a Keyspace isn't in all DC's,CASSANDRA-5424,12640546,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,yukim,jjordan,jjordan,03/Apr/13 16:58,16/Apr/19 09:32,14/Jul/23 05:53,19/Apr/13 21:34,1.2.5,,,,,,0,,,,,"nodetool repair -pr on all nodes won't repair the full range when a Keyspace isn't in all DC's

Commands follow, but the TL;DR of it, range (127605887595351923798765477786913079296,0] doesn't get repaired between .38 node and .236 node until I run a repair, no -pr, on .38

It seems like primary arnge calculation doesn't take schema into account, but deciding who to ask for merkle tree's from does.

{noformat}
Address         DC          Rack        Status State   Load            Owns                Token                                       
                                                                                           127605887595351923798765477786913079296     
10.72.111.225   Cassandra   rack1       Up     Normal  455.87 KB       25.00%              0                                           
10.2.29.38      Analytics   rack1       Up     Normal  40.74 MB        25.00%              42535295865117307932921825928971026432      
10.46.113.236   Analytics   rack1       Up     Normal  20.65 MB        50.00%              127605887595351923798765477786913079296     

create keyspace Keyspace1
  with placement_strategy = 'NetworkTopologyStrategy'
  and strategy_options = {Analytics : 2}
  and durable_writes = true;

-------
# nodetool -h 10.2.29.38 repair -pr Keyspace1 Standard1
[2013-04-03 15:46:58,000] Starting repair command #1, repairing 1 ranges for keyspace Keyspace1
[2013-04-03 15:47:00,881] Repair session b79b4850-9c75-11e2-0000-8b5bf6ebea9e for range (0,42535295865117307932921825928971026432] finished
[2013-04-03 15:47:00,881] Repair command #1 finished

root@ip-10-2-29-38:/home/ubuntu# grep b79b4850-9c75-11e2-0000-8b5bf6ebea9e /var/log/cassandra/system.log
 INFO [AntiEntropySessions:1] 2013-04-03 15:46:58,009 AntiEntropyService.java (line 676) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] new session: will sync a1/10.2.29.38, /10.46.113.236 on range (0,42535295865117307932921825928971026432] for Keyspace1.[Standard1]
 INFO [AntiEntropySessions:1] 2013-04-03 15:46:58,015 AntiEntropyService.java (line 881) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] requesting merkle trees for Standard1 (to [/10.46.113.236, a1/10.2.29.38])
 INFO [AntiEntropyStage:1] 2013-04-03 15:47:00,202 AntiEntropyService.java (line 211) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] Received merkle tree for Standard1 from /10.46.113.236
 INFO [AntiEntropyStage:1] 2013-04-03 15:47:00,697 AntiEntropyService.java (line 211) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] Received merkle tree for Standard1 from a1/10.2.29.38
 INFO [AntiEntropyStage:1] 2013-04-03 15:47:00,879 AntiEntropyService.java (line 1015) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] Endpoints /10.46.113.236 and a1/10.2.29.38 are consistent for Standard1
 INFO [AntiEntropyStage:1] 2013-04-03 15:47:00,880 AntiEntropyService.java (line 788) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] Standard1 is fully synced
 INFO [AntiEntropySessions:1] 2013-04-03 15:47:00,880 AntiEntropyService.java (line 722) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] session completed successfully

root@ip-10-46-113-236:/home/ubuntu# grep b79b4850-9c75-11e2-0000-8b5bf6ebea9e /var/log/cassandra/system.log
 INFO [AntiEntropyStage:1] 2013-04-03 15:46:59,944 AntiEntropyService.java (line 244) [repair #b79b4850-9c75-11e2-0000-8b5bf6ebea9e] Sending completed merkle tree to /10.2.29.38 for (Keyspace1,Standard1)

root@ip-10-72-111-225:/home/ubuntu# grep b79b4850-9c75-11e2-0000-8b5bf6ebea9e /var/log/cassandra/system.log
root@ip-10-72-111-225:/home/ubuntu# 

-------
# nodetool -h 10.46.113.236  repair -pr Keyspace1 Standard1
[2013-04-03 15:48:00,274] Starting repair command #1, repairing 1 ranges for keyspace Keyspace1
[2013-04-03 15:48:02,032] Repair session dcb91540-9c75-11e2-0000-a839ee2ccbef for range (42535295865117307932921825928971026432,127605887595351923798765477786913079296] finished
[2013-04-03 15:48:02,033] Repair command #1 finished

root@ip-10-46-113-236:/home/ubuntu# grep dcb91540-9c75-11e2-0000-a839ee2ccbef /var/log/cassandra/system.log
 INFO [AntiEntropySessions:5] 2013-04-03 15:48:00,280 AntiEntropyService.java (line 676) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] new session: will sync a0/10.46.113.236, /10.2.29.38 on range (42535295865117307932921825928971026432,127605887595351923798765477786913079296] for Keyspace1.[Standard1]
 INFO [AntiEntropySessions:5] 2013-04-03 15:48:00,285 AntiEntropyService.java (line 881) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] requesting merkle trees for Standard1 (to [/10.2.29.38, a0/10.46.113.236])
 INFO [AntiEntropyStage:1] 2013-04-03 15:48:01,710 AntiEntropyService.java (line 211) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] Received merkle tree for Standard1 from a0/10.46.113.236
 INFO [AntiEntropyStage:1] 2013-04-03 15:48:01,943 AntiEntropyService.java (line 211) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] Received merkle tree for Standard1 from /10.2.29.38
 INFO [AntiEntropyStage:1] 2013-04-03 15:48:02,031 AntiEntropyService.java (line 1015) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] Endpoints a0/10.46.113.236 and /10.2.29.38 are consistent for Standard1
 INFO [AntiEntropyStage:1] 2013-04-03 15:48:02,032 AntiEntropyService.java (line 788) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] Standard1 is fully synced
 INFO [AntiEntropySessions:5] 2013-04-03 15:48:02,032 AntiEntropyService.java (line 722) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] session completed successfully

root@ip-10-2-29-38:/home/ubuntu# grep dcb91540-9c75-11e2-0000-a839ee2ccbef /var/log/cassandra/system.log
 INFO [AntiEntropyStage:1] 2013-04-03 15:48:01,898 AntiEntropyService.java (line 244) [repair #dcb91540-9c75-11e2-0000-a839ee2ccbef] Sending completed merkle tree to /10.46.113.236 for (Keyspace1,Standard1)

root@ip-10-72-111-225:/home/ubuntu# grep dcb91540-9c75-11e2-0000-a839ee2ccbef /var/log/cassandra/system.log
root@ip-10-72-111-225:/home/ubuntu# 

-------
# nodetool -h 10.72.111.225  repair -pr Keyspace1 Standard1
[2013-04-03 15:48:30,417] Starting repair command #1, repairing 1 ranges for keyspace Keyspace1
[2013-04-03 15:48:30,428] Repair session eeb12670-9c75-11e2-0000-316d6fba2dbf for range (127605887595351923798765477786913079296,0] finished
[2013-04-03 15:48:30,428] Repair command #1 finished

root@ip-10-72-111-225:/home/ubuntu# grep eeb12670-9c75-11e2-0000-316d6fba2dbf /var/log/cassandra/system.log
 INFO [AntiEntropySessions:1] 2013-04-03 15:48:30,427 AntiEntropyService.java (line 676) [repair #eeb12670-9c75-11e2-0000-316d6fba2dbf] new session: will sync /10.72.111.225 on range (127605887595351923798765477786913079296,0] for Keyspace1.[Standard1]
 INFO [AntiEntropySessions:1] 2013-04-03 15:48:30,428 AntiEntropyService.java (line 681) [repair #eeb12670-9c75-11e2-0000-316d6fba2dbf] No neighbors to repair with on range (127605887595351923798765477786913079296,0]: session completed

root@ip-10-46-113-236:/home/ubuntu# grep eeb12670-9c75-11e2-0000-316d6fba2dbf /var/log/cassandra/system.log
root@ip-10-46-113-236:/home/ubuntu# 

root@ip-10-2-29-38:/home/ubuntu# grep eeb12670-9c75-11e2-0000-316d6fba2dbf /var/log/cassandra/system.log
root@ip-10-2-29-38:/home/ubuntu# 

---
root@ip-10-2-29-38:/home/ubuntu# nodetool -h 10.2.29.38 repair Keyspace1 Standard1
[2013-04-03 16:13:28,674] Starting repair command #2, repairing 3 ranges for keyspace Keyspace1
[2013-04-03 16:13:31,786] Repair session 6bb81c20-9c79-11e2-0000-8b5bf6ebea9e for range (42535295865117307932921825928971026432,127605887595351923798765477786913079296] finished
[2013-04-03 16:13:31,786] Repair session 6cb05ed0-9c79-11e2-0000-8b5bf6ebea9e for range (0,42535295865117307932921825928971026432] finished
[2013-04-03 16:13:31,806] Repair session 6d24a470-9c79-11e2-0000-8b5bf6ebea9e for range (127605887595351923798765477786913079296,0] finished
[2013-04-03 16:13:31,807] Repair command #2 finished

root@ip-10-2-29-38:/home/ubuntu# grep 6d24a470-9c79-11e2-0000-8b5bf6ebea9e /var/log/cassandra/system.log
 INFO [AntiEntropySessions:7] 2013-04-03 16:13:31,065 AntiEntropyService.java (line 676) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] new session: will sync a1/10.2.29.38, /10.46.113.236 on range (127605887595351923798765477786913079296,0] for Keyspace1.[Standard1]
 INFO [AntiEntropySessions:7] 2013-04-03 16:13:31,065 AntiEntropyService.java (line 881) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] requesting merkle trees for Standard1 (to [/10.46.113.236, a1/10.2.29.38])
 INFO [AntiEntropyStage:1] 2013-04-03 16:13:31,751 AntiEntropyService.java (line 211) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] Received merkle tree for Standard1 from /10.46.113.236
 INFO [AntiEntropyStage:1] 2013-04-03 16:13:31,785 AntiEntropyService.java (line 211) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] Received merkle tree for Standard1 from a1/10.2.29.38
 INFO [AntiEntropyStage:1] 2013-04-03 16:13:31,805 AntiEntropyService.java (line 1015) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] Endpoints /10.46.113.236 and a1/10.2.29.38 are consistent for Standard1
 INFO [AntiEntropyStage:1] 2013-04-03 16:13:31,806 AntiEntropyService.java (line 788) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] Standard1 is fully synced
 INFO [AntiEntropySessions:7] 2013-04-03 16:13:31,806 AntiEntropyService.java (line 722) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] session completed successfully

root@ip-10-46-113-236:/home/ubuntu# grep 6d24a470-9c79-11e2-0000-8b5bf6ebea9e /var/log/cassandra/system.log 
 INFO [AntiEntropyStage:1] 2013-04-03 16:13:31,665 AntiEntropyService.java (line 244) [repair #6d24a470-9c79-11e2-0000-8b5bf6ebea9e] Sending completed merkle tree to /10.2.29.38 for (Keyspace1,Standard1)
{noformat}
",,alprema,christianmovi,jjordan,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5550,CASSANDRA-6852,,,,,,,,CASSANDRA-7317,,,,,,,,,,,"10/Apr/13 17:56;yukim;5424-1.1.txt;https://issues.apache.org/jira/secure/attachment/12578041/5424-1.1.txt","16/Apr/13 04:19;yukim;5424-v2-1.2.txt;https://issues.apache.org/jira/secure/attachment/12578865/5424-v2-1.2.txt","16/Apr/13 20:00;yukim;5424-v3-1.2.txt;https://issues.apache.org/jira/secure/attachment/12579001/5424-v3-1.2.txt",,,,,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,321006,,,Mon Jun 03 17:15:21 UTC 2013,,,,,,,,,,"0|i1jdwv:",321347,,,,,,,,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,,"04/Apr/13 19:15;jjordan;Tested back on 1.1.7 (before some recent repair changes) and it has the same issue.;;;","09/Apr/13 16:36;yukim;CASSANDRA-3912 changed the behavior of repair to not perform if given range is not part of the local node for Keyspace that does not have replica.
For above case, StorageService#getLocalRanges [here|https://github.com/apache/cassandra/blob/cassandra-1.1.9/src/java/org/apache/cassandra/service/AntiEntropyService.java#L160] would return null for /10.72.111.225 and range (127605887595351923798765477786913079296,0].

Repair always sends merkle tree request to local node and synchronizes with others, so the desired behavior would be to just send merkle tree requests to those who have replica and let them synchronize.;;;","10/Apr/13 17:16;yukim;Patch attached against 1.1.

It is basically rewrite of AntiEntropyService.getNeighbors, but I moved that static method to StorageService and renamed as getReplicaNodes because I felt that is more suitable place. And the method returns addresses of the replica nodes for given KS and range. Previously the method does not return the address of the local node, but the new version does only when the local node holds the replica.

So for the above case, /10.72.111.225 sends tree request only to other nodes in Analytics DC for the range it holds, and if there is difference, the node let others to repair the data each other.;;;","15/Apr/13 16:17;jbellis;As you know, I'm pretty leery of making anything but the most superficial changes to 1.1.x at this point.

Am I correct that a workaround would be, ""only run repair against a node that is an owner of the given range?"";;;","15/Apr/13 16:29;jjordan;The work around is always use repair no -pr;;;","15/Apr/13 16:43;jbellis;Thinking about it, -pr really should NOT affect ranges that aren't replicated to the node in question.  That's the whole point of that option!

It looks to me like the real bug here is that repair is not NTS-aware: the ""primary range"" for .38 for Keyspace1 should be (127605887595351923798765477786913079296, 42535295865117307932921825928971026432], not (0, 42535295865117307932921825928971026432].;;;","16/Apr/13 04:19;yukim;Ok, this time I created patch against 1.2.

We've been calculating the primary range just from the tokens of the node. The patch changes this to use replication strategy's calculateNaturalEndpoint, and use the first one returned by the method as ""the primary range"". In order to do this in NTS though, I have to tweak a little bit(Set instead of List to use internally).

By this way, we get the primary ranges for .38 for Keyspace1 above are (127...296, 0] and (0, 425...32]. For .225, it returns empty range(btw I had to fix repair for empty range also).
When using vnodes, it is not guaranteed to have consecutive ranges, so I decided to return in two separate ranges.;;;","16/Apr/13 15:42;jbellis;Some questions:
- Were we relying on the Set behavior to de-duplicate entries in {{replicas}} before copying it into an ArrayList at the end, or was that just a case of being over-cautious?
- Why don't we need to check {{ranges.size > 0}} any more in {{forceRepairAsync}}?
- Do we need to fix other uses of {{tokenMetadata.getPrimaryRangesFor}} such as {{SS.sampleKeyRange}}?
- Can we use {{getCachedEndpoints}} instead of {{calculateNaturalEndpoints}}?

Also:
- It's probably worth adding some comments to {{getPrimaryRangesForEndpoint}} -- superficially, it looks like it is incorrect since it is still using the non-Strategy-aware {{metadata.getPredecessor}}, but after working some examples I am satisfied that it does the right thing, as it does here.
;;;","16/Apr/13 16:06;yukim;bq. Were we relying on the Set behavior to de-duplicate entries in replicas before copying it into an ArrayList at the end, or was that just a case of being over-cautious?

hmm, I think we need to check if we have duplicates.

bq. Why don't we need to check ranges.size > 0 any more in forceRepairAsync?

I added 'isEmpty' check at the beginning instead. Without that, repair command hangs on client side.

bq. Do we need to fix other uses of tokenMetadata.getPrimaryRangesFor such as SS.sampleKeyRange?

I was not sure if we need to fix. It looks like sampleKeyRange is only used by nodetool.

bq. Can we use getCachedEndpoints instead of calculateNaturalEndpoints?

Probably we can use getNaturalEndpoints, which uses cached endpoints.

I'll brush up my patch with comments and unit tests.;;;","16/Apr/13 16:19;jbellis;bq. It looks like sampleKeyRange is only used by nodetool

It's a minor problem (looks like it's mostly there to support OPP: CASSANDRA-2917) but we should probably fix it.

Also, it looks like Bootstrap is using it to determine where to bisect ranges.  We should fix that one way or another (where ""another"" might be ""get rid of token selection on bootstrap and force people to either use vnodes or specify token manually"").  Separate ticket as followup is fine here IMO.;;;","16/Apr/13 16:26;jbellis;bq. get rid of token selection on bootstrap and force people to either use vnodes or specify token manually

To clarify: this would be best done in 2.0.;;;","16/Apr/13 20:00;yukim;v3 attached.

- NTS now uses LinkedHashSet in calculateNaturalEndpoint to preserve insertion order while eliminating duplicates.

- I think it is unsafe to use cached endpoints through getNaturalEndpoints since tokenMetadata cannot be consistent inside getPrimaryRangesForEndpoint, so I stick with impl from v2.

- fix sampleKeyRange. I think the problem is that the name tokenMetadata.getPrimaryRangeFor is confusing. Probably we should rename that to just getRangeFor.

- Added test for getPrimaryRangesForEndpoint to StorageServiceServerTest.
;;;","19/Apr/13 20:51;jbellis;I think this is fine the way it was:

{code}
-        if (ranges.size() > 0)
-        {
-            new Thread(createRepairTask(cmd, keyspace, ranges, isSequential, isLocal, columnFamilies)).start();
-        }
+        new Thread(createRepairTask(cmd, keyspace, ranges, isSequential, isLocal, columnFamilies)).start();
{code}

Otherwise LGTM.  Created CASSANDRA-5499 for followup.;;;","19/Apr/13 21:34;yukim;Committed with above fix. Thanks!;;;","21/May/13 19:37;rcoli;{quote}
get rid of token selection on bootstrap and force people to either use vnodes or specify token manually
{quote}

This has seemed operationally sane to me since approximately 0.6 series. We gain almost nothing (noobs will really be discouraged by having to set a token manually?) and expose ourselves to unnecessary complexity and edge cases like this. +1
;;;","21/May/13 19:39;jbellis;Done in CASSANDRA-5518.;;;","03/Jun/13 12:19;alprema;We just applied 1.2.5 on our cluster and the repair hanging is fixed, but the -pr is still not working as expected.
Our cluster has two datacenters, let's call them dc1 and dc2, we created a Keyspace Test_Replication with replication factor _\{ dc1: 3 \}_ (no info for dc2) and ran a nodetool repair Test_Replication (that used to hang) on dc2 and it exited saying there was nothing to do (which is OK).
Then we changed the replication factor to _\{ dc1: 3, dc2: 3 \}_ and started a nodetool repair -pr Test_Replication on cassandra11@dc2 which output this:
{code}
user@cassandra11:~$ nodetool repair -pr Test_Replication
[2013-06-03 13:54:53,948] Starting repair command #1, repairing 1 ranges for keyspace Test_Replication
[2013-06-03 13:54:53,985] Repair session 676c00f0-cc44-11e2-bfd5-3d9212e452cc for range (0,1] finished
[2013-06-03 13:54:53,985] Repair command #1 finished
{code}
But even after flushing the Keyspace, there was no data on the server.
We then ran a full repair:
{code}
user@cassandra11:~$ nodetool repair  Test_Replication
[2013-06-03 14:01:56,679] Starting repair command #2, repairing 6 ranges for keyspace Test_Replication
[2013-06-03 14:01:57,260] Repair session 63632d70-cc45-11e2-bfd5-3d9212e452cc for range (0,1] finished
[2013-06-03 14:01:57,260] Repair session 63650230-cc45-11e2-bfd5-3d9212e452cc for range (56713727820156410577229101238628035243,113427455640312821154458202477256070484] finished
[2013-06-03 14:01:57,260] Repair session 6385d0a0-cc45-11e2-bfd5-3d9212e452cc for range (1,56713727820156410577229101238628035242] finished
[2013-06-03 14:01:57,260] Repair session 639f7320-cc45-11e2-bfd5-3d9212e452cc for range (56713727820156410577229101238628035242,56713727820156410577229101238628035243] finished
[2013-06-03 14:01:57,260] Repair session 63af51a0-cc45-11e2-bfd5-3d9212e452cc for range (113427455640312821154458202477256070484,113427455640312821154458202477256070485] finished
[2013-06-03 14:01:57,295] Repair session 63b12660-cc45-11e2-bfd5-3d9212e452cc for range (113427455640312821154458202477256070485,0] finished
[2013-06-03 14:01:57,295] Repair command #2 finished
{code}
After which we could find the data on dc2 as expected.

So it seems that -pr is still not working as expected, or maybe we're doing/understanding something wrong.
(I was not sure if I should open a new ticket or comment this one so please let me know if I should move it);;;","03/Jun/13 14:01;jbellis;What *should* happen is that if you repair -pr on each node in dc2, then you will repair the full token space.  But for a single node, YMMV.  In particular, it's quite possible that this is correct:

bq. Repair session 676c00f0-cc44-11e2-bfd5-3d9212e452cc for range (0,1] finished

Note the tiny range involved.  (This indicates that your dc2 tokens are not balanced, btw.);;;","03/Jun/13 14:43;jbellis;bq. This indicates that your dc2 tokens are not balanced, btw

Hmm.  Actually I don't see how repair could generate only a single range in a 2-DC setup and NTS.  Can you post your ring?;;;","03/Jun/13 14:52;jjordan;With the following replication:
{noformat}
{ dc1: 3, dc2: 3 }
{noformat}

And the following ring:
{noformat}
node dc  token
n0   dc1 0
n1   dc2 1
{noformat}

That is the expected output from ""nodetool -h n1 repair -pr"".  Do a ""nodetool -h n0 repair -pr"" and n1 will get a bunch of data.  -pr only repairs from current token to previous token, if you don't have any data with a token of ""1"", then repair -pr won't do much for repairing n1.;;;","03/Jun/13 15:28;jbellis;I should have said, 2-DC setup, NTS, and replicas in both DC.  And more than one node in each DC.

In any case, I do see the problem now.  Working on a fix.;;;","03/Jun/13 15:54;jjordan;If there is a problem, glad you found it, but I don't see how multiple nodes changes the fact that the primary range of n1 is only (0,1] if both DC's have replicas.;;;","03/Jun/13 16:03;alprema;*[EDIT] I didn't see your latests posts before posting, but I hope the extra data can help anyway*

You were right to say that I need to run the repair -pr on the three nodes, because I only have one row (it's a test) in the CF so I guess I had to run the repair -pr on the node in charge of this key.
But I restarted my test and did the repair on all three nodes, and it didn't work either; here's the output:
{code}
user@cassandra11:~$ nodetool repair -pr Test_Replication
[2013-06-03 13:54:53,948] Starting repair command #1, repairing 1 ranges for keyspace Test_Replication
[2013-06-03 13:54:53,985] Repair session 676c00f0-cc44-11e2-bfd5-3d9212e452cc for range (0,1] finished
[2013-06-03 13:54:53,985] Repair command #1 finished
{code}

{code}
user@cassandra12:~$ nodetool repair -pr Test_Replication
[2013-06-03 17:33:17,844] Starting repair command #1, repairing 1 ranges for keyspace Test_Replication
[2013-06-03 17:33:17,866] Repair session e9f38c50-cc62-11e2-af47-db8ca926a9c5 for range (56713727820156410577229101238628035242,56713727820156410577229101238628035243] finished
[2013-06-03 17:33:17,866] Repair command #1 finished
{code}

{code}
user@cassandra13:~$ nodetool repair -pr Test_Replication
[2013-06-03 17:33:29,689] Starting repair command #1, repairing 1 ranges for keyspace Test_Replication
[2013-06-03 17:33:29,712] Repair session f102f3a0-cc62-11e2-ae98-39da3e693be3 for range (113427455640312821154458202477256070484,113427455640312821154458202477256070485] finished
[2013-06-03 17:33:29,712] Repair command #1 finished
{code}

The data is still not copied to the new datacenter, and I don't understand why the repair is made for those ranges (a range of 1??), it could be a problem of unbalanced cluster as you suggested, but we distributed the tokens as advised (+1 on the nodes of the new datacenter) as you can see in the following nodetool status:

{code}
user@cassandra13:~$ nodetool status
Datacenter: dc1
=====================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Owns   Host ID                               Token                                    Rac
UN  cassandra01     102 GB     33.3%  fa7672f5-77f0-4b41-b9d1-13bf63c39122  0                                        RC1
UN  cassandra02     88.73 GB   33.3%  c799df22-0873-4a99-a901-5ef5b00b7b1e  56713727820156410577229101238628035242   RC1
UN  cassandra03     50.86 GB   33.3%  5b9c6bc4-7ec7-417d-b92d-c5daa787201b  113427455640312821154458202477256070484  RC1
Datacenter: dc2
======================
Status=Up/Down
|/ State=Normal/Leaving/Joining/Moving
--  Address         Load       Owns   Host ID                               Token                                    Rac
UN  cassandra11     51.21 GB   0.0%   7b610455-3fd2-48a3-9315-895a4609be42  1                                        RC2
UN  cassandra12     45.02 GB   0.0%   8553f2c0-851c-4af2-93ee-2854c96de45a  56713727820156410577229101238628035243   RC2
UN  cassandra13     36.8 GB    0.0%   7f537660-9128-4c13-872a-6e026104f30e  113427455640312821154458202477256070485  RC2
{code}

Furthermore the full repair works, as you can see in this log:

{code}
user@cassandra11:~$ nodetool repair  Test_Replication
[2013-06-03 17:44:07,570] Starting repair command #5, repairing 6 ranges for keyspace Test_Replication
[2013-06-03 17:44:07,903] Repair session 6d37b720-cc64-11e2-bfd5-3d9212e452cc for range (0,1] finished
[2013-06-03 17:44:07,903] Repair session 6d3a0110-cc64-11e2-bfd5-3d9212e452cc for range (56713727820156410577229101238628035243,113427455640312821154458202477256070484] finished
[2013-06-03 17:44:07,903] Repair session 6d4d6200-cc64-11e2-bfd5-3d9212e452cc for range (1,56713727820156410577229101238628035242] finished
[2013-06-03 17:44:07,903] Repair session 6d581060-cc64-11e2-bfd5-3d9212e452cc for range (56713727820156410577229101238628035242,56713727820156410577229101238628035243] finished
[2013-06-03 17:44:07,903] Repair session 6d5ea010-cc64-11e2-bfd5-3d9212e452cc for range (113427455640312821154458202477256070484,113427455640312821154458202477256070485] finished
[2013-06-03 17:44:07,934] Repair session 6d604dc0-cc64-11e2-bfd5-3d9212e452cc for range (113427455640312821154458202477256070485,0] finished
[2013-06-03 17:44:07,934] Repair command #5 finished
{code}

I hope this information can help, please let me know if you think it's a configuration issue, in which case I would talk to the mailing list.;;;","03/Jun/13 16:12;jjordan;[~alprema] you need to run it on all 6 nodes.  repair -pr only repairs the primary range, when ever you use repair -pr you must run repair on every node which owns the data for the KS you are repairing.  If the KS is only in DC1, that is 3 nodes, if it is in DC1 and DC2 that is 6 nodes.;;;","03/Jun/13 16:13;jbellis;I was right the first time; this is correct behavior.  Quoting from CASSANDRA-5608:

bq. The right way to use -pr is still to repair everywhere the data exists; if we made -pr affect everything in the DC regardless of other replicas, then repairing the full cluster would repair each range 1x for each DC, which is not what we want;;;","03/Jun/13 17:15;alprema;I redid the same test (creating the keyspace with data, then changing its replication factor so it's replicated in DC2, then repairing) and it turns out that if you don't run a repair on DC2 before changing the replication factor, the repair -pr works fine \-_\-.

Anyway, your solution worked, thank you for your help and sorry I polluted JIRA with my questions.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PasswordAuthenticator is incompatible with various Cassandra clients,CASSANDRA-5423,12640545,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,sdelmas,sdelmas,03/Apr/13 16:55,16/Apr/19 09:32,14/Jul/23 05:53,04/Apr/13 21:18,1.2.4,,,,,,0,security,,,,"Evidently with the old authenticator it was allowed to set keyspace, and then login.  With the org.apache.cassandra.auth.PasswordAuthenticator you have to login and then setkeyspace

For backwards compatibility it would be good to allow setting before login, and perform the actual operation/validation later after the login.

",,aleksey,jjordan,sdelmas,soverton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/13 20:57;aleksey;5423.txt;https://issues.apache.org/jira/secure/attachment/12576850/5423.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,321005,,,Thu Apr 04 21:18:00 UTC 2013,,,,,,,,,,"0|i1jdwn:",321346,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"04/Apr/13 16:52;jbellis;Shouldn't we set some flag on clientState so that we can validate it when the user performs a query?;;;","04/Apr/13 16:55;aleksey;Why? We validate keyspaces anyway on each select/insert/update/drop/etc.;;;","04/Apr/13 17:08;jbellis;Where does that happen on e.g. the insert path?  Throwing NPE doesn't count. :)  (ensureHasAccess doesn't work either for AllowAll...  Even for other authenticators, I'm not sure we want to return ""no access"" instead of ""doesn't exist."");;;","04/Apr/13 17:11;aleksey;Well, a keyspace can be dropped after you call set_keyspace(). So I assumed we always validate keyspace existence. If we don't, we should.;;;","04/Apr/13 17:15;jbellis;I don't think we do.  (Related: CASSANDRA-5358);;;","04/Apr/13 19:18;aleksey;We do. Look at 'find usages' of ThriftValidation validateTable and both validateColumnFamily methods (that do in turn call validateTable).

CASSANDRA-5358 is orthogonal (and IMO should just be resolved with 'not a problem').;;;","04/Apr/13 21:01;jbellis;SGTM, +1;;;","04/Apr/13 21:18;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Native protocol sanity check,CASSANDRA-5422,12640517,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,danielnorberg,jbellis,jbellis,03/Apr/13 13:56,16/Apr/19 09:32,14/Jul/23 05:53,19/Jun/13 15:09,1.2.6,,,Legacy/CQL,,,1,,,,,"With MutationStatement.execute turned into a no-op, I only get about 33k insert_prepared ops/s on my laptop.  That is: this is an upper bound for our performance if Cassandra were infinitely fast, limited by netty handling the protocol + connections.

This is up from about 13k/s with MS.execute running normally.

~40% overhead from netty seems awfully high to me, especially for insert_prepared where the return value is tiny.  (I also used 4-byte column values to minimize that part as well.)",,aleksey,colinkuo,danielnorberg,daubman,edison,jeromatron,jjordan,rcoli,rschildmeijer,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/13 13:59;jbellis;5422-test.txt;https://issues.apache.org/jira/secure/attachment/12576786/5422-test.txt","19/May/13 01:10;danielnorberg;ExecuteMessage Profiling - Call Tree.png;https://issues.apache.org/jira/secure/attachment/12583748/ExecuteMessage+Profiling+-+Call+Tree.png","19/May/13 01:10;danielnorberg;ExecuteMessage Profiling - Hot Spots.png;https://issues.apache.org/jira/secure/attachment/12583749/ExecuteMessage+Profiling+-+Hot+Spots.png",,,,,,,,,,,,,,,,,,3.0,danielnorberg,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,320977,,,Wed Jun 19 15:06:15 UTC 2013,,,,,,,,,,"0|i1jdqf:",321318,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"03/Apr/13 13:59;jbellis;Patch to disable MS.execute (and batch_mutate) attached.

To run the binary protocol stress test,

{{mvn install -Dmaven.test.skip=true}} from java-driver root (https://github.com/datastax/java-driver)

then {{mvn assembly:single}} from driver-example/stress

finally, {{java -jar target/cassandra-driver-examples-stress-1.0.0-beta2-APSHOT-jar-with-dependencies.jar insert_prepared --value-size 4}};;;","07/May/13 19:54;danielnorberg;The main issues I identified:

* Contention in the driver, i.e. per connection locks taken for every request
* Expensive serialization, i.e. multiple layers of ChannelBuffers used in the ExecuteMessage codec.
* No write batching, i.e. every message results in an expensive syscall.
* Contention in the stress application, bottlenecking on a shared work queue and spawning of one thread per asynchronous worker.

After eliminating contention in the driver and the stress application, optimizing serialization and adding write batching I get a throughput of 200k+ requests per second on my laptop (four core 2Ghz i7 mpb) when making asynchronous requests at a concurrency level of 500. This is with request execution and mutation disabled with the above patch and running both cassandra and the stress tool with Java 7. With this throughput, the benchmark uses a bandwidth of ~60 MB/sec so server grade hardware should be able to saturate 1 Gbit ethernet interfaces, especially with larger payloads.

https://github.com/danielnorberg/java-driver/tree/optimization
https://github.com/danielnorberg/cassandra/tree/transport-benchmark

\\
{noformat}
5/7/13 2:59:54 PM ==============================================================
com.datastax.driver.stress.Reporter:
  latencies:
             count = 352558280
         mean rate = 230848.13 calls/s
     1-minute rate = 223475.90 calls/s
     5-minute rate = 224159.41 calls/s
    15-minute rate = 190931.94 calls/s
               min = 0.27ms
               max = 124.37ms
              mean = 2.16ms
            stddev = 1.63ms
            median = 1.69ms
              75% <= 2.43ms
              95% <= 5.57ms
              98% <= 6.64ms
              99% <= 8.76ms
            99.9% <= 26.57ms

  requests:
             count = 352559217
         mean rate = 230848.12 requests/s
     1-minute rate = 223474.50 requests/s
     5-minute rate = 224159.75 requests/s
    15-minute rate = 190950.27 requests/s
{noformat}

Suggestions for further work:

* Use uniform histogram instead of biased (default) as the biased histogram takes expensive read-write locks for every update, i.e. every request. Or find some way to eliminate the read-write locking in the biased histogram.
* Make StorageProxy non-blocking and use the jsr166e ForkJoinPool instead of normal TPE for a nice throughput boost when working with a large volume of small messages.
* Change protocol to allow more than 128 outstanding requests per connection.

When running normally with request execution enabled I get ~24k rps. Quick profiling indicates that there's some contention points that could be removed, e.g. the ReentrantReadWriteLock (switchLock) in Table. We should be able to optimize the whole stack to the point where a cassandra node can achieve a sustained rate of 100k+ writes per second.

;;;","07/May/13 20:01;jbellis;I have a branch that kills switchLock over on http://github.com/jbellis/cassandra/branches/5064, I'll see about dusting that off...;;;","08/May/13 04:49;jbellis;Created CASSANDRA-5549 for switchLock removal.;;;","13/May/13 06:50;slebresne;bq. Make StorageProxy non-blocking

For info, CASSANDRA-5239 is open for this (and I have 3 quarters of a patch written, which I'm going to attach soonish).;;;","17/May/13 14:02;slebresne;Thanks a lot Daniel for taking the time to look into that.

I was curious to understand from where the main benefits were coming from so I tried benching the optimizations separately.

First, the baseline on my machine (a quad-core i5 2.80Ghz) with the current java driver and C* 1.2 (with ModificationStatement execute commented out) is about 66K req/s (I'll note that I've already committed some patch to remove the contention on returnConnection in the Java driver and it's included in that baseline). That's with 50 threads (and it's slightly worth with 500 threads).

Long story short, the first bottleneck is the Java driver ""stress"" application.  Which I can't say is a surprise since it was a fairly quick hack primarily meant to check the driver wasn't crashing with more than one thread. [~danielnorberg], I'm happy committing your patch optimizing this, though the patch removes the Apache license from one file and adds some copyright, so wondering if the patches were meant for inclusion or not?

Anyway, even with the stress patch committed, I don't get much improvement yet.  More precisely, by default (synchronous mode, 50 threads) I get 74K, which is slightly better but not amazing. If I try the async mode with 500 threads (to compare with what's coming next), I actually get about 49K.

At this point, the main bottleneck by far seems to be the ArrayBlockingQueue used in the RequestThreadPoolExecutor. Changing it to LinkedBlockingQueue, we get 163K with stress in async mode and 500 threads (which is then the fastest mode: in synchronous mode, I get 95K with 50 threads and 117K with 500 threads).

So, I've committed that part (to 1.2) since that's such a trivial patch and is clearly the main bottleneck, at least Cassandra side. On trunk, I'll note that if we go ahead with CASSANDRA-5239, it'll remove RequestThreadPoolExecutor altogether which could improve things even more (though it's possible that once switched to LinkedBlockingQueue, it's not much of a bottleneck anymore).

bq. Expensive serialization, i.e. multiple layers of ChannelBuffers used in the ExecuteMessage codec.

The vague rational here was to avoid a copy of the values (when they are not trivially small). I did tried to quickly bench that patch separately (on top of the other optims) and didn't really saw a difference. Though I didn't saw much difference increasing the value size tbh (could be there is some other bottleneck, like the generation of bigger values for instance, I haven't checked). In any case, before changing the serialization of all messages it's probably worth some more thorough investigation. But I'm not sure we have a ton to win here, if any.

bq. No write batching

I agree that write batching is a good idea. That being said, write batching is often a trade-off between throughput and latency, so ideally I'd like to expose some of the tweaking knobs and/or test it on more realistic and varied scenario.

That being said, testing it (both client and server side) on top of the ABQ->LBQ patch, I get 180K req/s (versus 163K), so about 10% improvement on that test which ain't bad.

As a side note, same question on the license/copyright for the batching parts of the patch than above.
;;;","19/May/13 01:16;danielnorberg;The patches were mostly to show the optimisations I applied to arrive at my benchmark results.

I've fixed the license headers, although I must admit that I'm unsure of what to do about the copyright notice, if something needs to be done.

The ExecuteMessage serialisation optimisation gives me a ~20% throughput boost on top of all other optimisations, which is not bad. I attached two screenshots of the VisualVM profiler highlighting the CompositeChannelBuffer.decompose() method, which gets called to flatten the serialised ExecuteMessage.

The write batcher will only batch writes if they arrive within less than a configurable time interval, default 100 microseconds, after another write. I.e., in low throughput scenarios no batching will be done, keeping latencies low. In high throughput scenarios the buffer is likely to be flushed quickly, providing a negligible latency impact. That said, more testing in order to better understand the behaviour and performance characteristics sounds like a good idea.

Anyway, 100k+ rps is a great improvement over the thrift interface. Great job on the new driver! I'm looking forward to using it =)
;;;","19/May/13 03:36;jbellis;bq. I've fixed the license headers

I didn't see any changes to the github branches above; am I looking in the wrong place?

bq. although I must admit that I'm unsure of what to do about the copyright notice

The ASF requires copyright assignment to it.  So having a {{Copyright (c) 2012-2013 Spotify AB}} in there doesn't work for us.  If Spotify has a policy that they own everything you write unless specified otherwise, we should get them to sign a Corporate CLA: http://www.apache.org/licenses/cla-corporate.txt;;;","19/May/13 10:36;danielnorberg;I amended the commits to restore/add the apache license in the source files and force pushed those branches.

Would Spotify signing the corporate CLA be enough or do we also need to assign copyright to the ASF?

;;;","20/May/13 02:03;jbellis;It looks like I was wrong; they do not actually require copyright assignment, just the grant of a license covered in the CLA:

{quote}
Grant of Copyright License. Subject to the terms and conditions of this Agreement, You hereby grant to the Foundation and to recipients of software distributed by the Foundation a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare derivative works of, publicly display, publicly perform, sublicense, and distribute Your Contributions and such derivative works.
{quote};;;","20/May/13 02:08;danielnorberg;Ok, I'll try to get the corporate CLA signed asap.;;;","20/May/13 02:09;jbellis;That said, the appropriate place for a Spotify copyright notice would be the NOTICES file:

{quote}
If [a] source file is submitted with a copyright notice included in it, the copyright owner (or owner's agent) must either:
# remove such notices, or
# move them to the NOTICE file associated with each applicable project release, or
# provide written permission for the ASF to make such removal or relocation of the notices.
{quote}

http://www.apache.org/legal/src-headers.html;;;","20/May/13 02:14;jbellis;The rest of that page is interesting reading as well; basically, you retain copyright for your code whether or not it's actually explicitly specified, precisely since no copyright assignment is involved.  So while it's okay to add an entry to NOTICES, it's not actually useful for anything.

Very long thread about this at http://thread.gmane.org/gmane.comp.apache.legal.discuss/95;;;","20/May/13 02:48;danielnorberg;Sounds reasonable. 

Interesting thread. Silly that there needs to be so much legal busywork in open source.;;;","31/May/13 20:24;jbellis;Any progress?;;;","01/Jun/13 14:35;danielnorberg;I've gotten confirmation that my ICLA has been received by the ASF. Waiting for confirmation that our Corporate CLA has also been successfully submitted.

My changes to the message serialization can be merged at will as they should be covered by the ICLA.

The changes I made to the driver and stress application I have donated to the driver project so you can use them as you wish.

I believe the write batcher could be incorporated in org.apache.cassandra.transport and then used both in the driver and server without any legal difficulties.;;;","03/Jun/13 23:07;danielnorberg;Our corporate CLA has also been submitted.;;;","18/Jun/13 19:25;jbellis;/throws up the [~slebresne] signal;;;","19/Jun/13 15:06;slebresne;Let sum this up. The most important improvements (in term of performance gain) have already been committed to the Cassandra and the driver. So with the current code, I get up to 170K req/s for the ""sanity check"" (with MutationStatement.execute turned into a no-op). So imo this is saner: the stupid bottlenecks have been removed. So for the sake of a better tracking of changes, I move that we close this ticket as resolved and open specific tickets for the remaining improvements.

Talking of those remaining improvements, the 2 ones from Daniel's patches that remained to be considered/committed are:
# Write batching. I've opened CASSANDRA-5663. for this. I think we're pretty much good to go on that one, but [~danielnorberg], the batch code is in the commit that the other stuffs on you github branch. Would you mind extracting those bits and attach it to CASSANDRA-5663? Also, following what Jonathan pasted above, I think we'd want to move the Spotify copyright from the file header to the NOTICE file.
# Improving serialization. I've opened CASSANDRA-5664 for that one. I'd want to look that one further because 1) I haven't seen the improvement that Daniel has seen so I want to redo my testing, 2) the patch as it stands need a few modifications (it doesn't handle 'null' values correctly for instance) and 3) if we're going to do that change, I want to do it everywhere, not just for execute messages.
;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming fails,CASSANDRA-5418,12640335,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,radev,radev,02/Apr/13 17:11,16/Apr/19 09:32,14/Jul/23 05:53,11/Apr/13 16:13,1.2.5,,,,,,1,,,,,"When I run *nodetool repair* on cas01 node it get's stuck at some point.

I see following exceptions in cas01 system.log:
{quote}
ERROR [Streaming to /10.10.45.60:28] 2013-04-02 09:03:55,353 CassandraDaemon.java (line 132) Exception in thread Thread[Streaming to /10.10.45.60:28,5,main]
java.lang.RuntimeException: java.io.EOFException
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:193)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:114)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 3 more


ERROR [Thread-2076] 2013-04-02 09:07:12,261 CassandraDaemon.java (line 132) Exception in thread Thread[Thread-2076,5,main]
java.lang.AssertionError: incorrect row data size 130921 written to /var/lib/cassandra/data/EDITED/content_list/footballsite-content_list-tmp-ib-3660-Data.db; correct is 131074
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:285)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)

{quote}

On other machines there are some exceptions too:
{quote}
ERROR [Thread-1424] 2013-04-02 09:07:12,248 CassandraDaemon.java (line 132) Exception in thread Thread[Thread-1424,5,main]
java.lang.AssertionError: incorrect row data size 130921 written to /var/lib/cassandra/data/EDITED/content_list/footballsite-content_list-tmp-ib-2268-Data.db; correct is 131074
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:285)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
ERROR [Streaming to /10.10.45.58:55] 2013-04-02 09:07:12,263 CassandraDaemon.java (line 132) Exception in thread Thread[Streaming to /10.10.45.58:55,5,main]
java.lang.RuntimeException: java.io.EOFException
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:193)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:114)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 3 more

{quote}

Then I see frozen status in *nodetool netstats* and repair never completes.","5 nodes, vnodes enabled, encryption disabled, compression enabled, RackInferring snitch, Centos 6, Oracle JVM with JNA enabled.",anttiko,marcuse,radev,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Apr/13 15:31;yukim;0001-add-RangeTombstone-transfer-test.patch;https://issues.apache.org/jira/secure/attachment/12578229/0001-add-RangeTombstone-transfer-test.patch","11/Apr/13 10:32;radev;5418-1.2-v2.txt;https://issues.apache.org/jira/secure/attachment/12578187/5418-1.2-v2.txt","11/Apr/13 10:37;radev;5418-1.2-v3.txt;https://issues.apache.org/jira/secure/attachment/12578191/5418-1.2-v3.txt","10/Apr/13 21:56;radev;5418-1.2.txt;https://issues.apache.org/jira/secure/attachment/12578090/5418-1.2.txt","11/Apr/13 12:35;slebresne;5418-v4.txt;https://issues.apache.org/jira/secure/attachment/12578203/5418-v4.txt",,,,,,,,,,,,,,,,5.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,320798,,,Thu Apr 11 16:13:43 UTC 2013,,,,,,,,,,"0|i1jcmn:",321139,,,,,,,,,yukim,,yukim,Critical,,,,,,,,,,,,,,,,,,"02/Apr/13 18:43;brandon.williams;You have corruption and need to run scrub first.  That said, we could probably at least abort the repair session in this case.  What do you think [~yukim]?;;;","03/Apr/13 02:44;radev;I've run *nodetool scrub* on each node, it went over all column families.
And yet on next run of *nodetool repair* I still see exceptions in logs:
{code}
 INFO [AntiEntropyStage:1] 2013-04-02 19:37:11,095 StreamOutSession.java (line 162) Streaming to /10.10.45.59
ERROR [Thread-2171] 2013-04-02 19:37:11,184 CassandraDaemon.java (line 132) Exception in thread Thread[Thread-2171,5,main]
java.lang.AssertionError: incorrect row data size 729492 written to /var/lib/cassandra/data/footballsite/content_list/footballsite-content_list-tmp-ib-2235-Data.db; correct is 731241
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:285)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
ERROR [Thread-2173] 2013-04-02 19:37:11,187 CassandraDaemon.java (line 132) Exception in thread Thread[Thread-2173,5,main]
java.lang.AssertionError: incorrect row data size 241378 written to /var/lib/cassandra/data/footballsite/content_list/footballsite-content_list-tmp-ib-2236-Data.db; correct is 241696
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:285)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
 INFO [Streaming to /10.10.45.60:6] 2013-04-02 19:37:11,190 StreamReplyVerbHandler.java (line 44) Successfully sent /var/lib/cassandra/data/footballsite/content_list/footballsite-content_list-ib-2176-Data.db to /10.10.45.60
 INFO [Streaming to /10.10.45.59:4] 2013-04-02 19:37:11,216 StreamReplyVerbHandler.java (line 44) Successfully sent /var/lib/cassandra/data/footballsite/content_list/footballsite-content_list-ib-2176-Data.db to /10.10.45.59
ERROR [Streaming to /10.10.45.60:6] 2013-04-02 19:37:11,243 CassandraDaemon.java (line 132) Exception in thread Thread[Streaming to /10.10.45.60:6,5,main]
java.lang.RuntimeException: java.io.EOFException
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:193)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:114)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 3 more
 INFO [AntiEntropyStage:1] 2013-04-02 19:37:11,251 StreamingRepairTask.java (line 223) [streaming task #62cdba10-9c07-11e2-a79f-1fa905df867b] task succeeded
ERROR [Streaming to /10.10.45.59:4] 2013-04-02 19:37:11,265 CassandraDaemon.java (line 132) Exception in thread Thread[Streaming to /10.10.45.59:4,5,main]
java.lang.RuntimeException: java.io.EOFException
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:193)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:114)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 3 more

{code};;;","03/Apr/13 05:47;marcuse;are you running internode encryption?;;;","03/Apr/13 05:55;radev;No, but I'm using vnodes, if that's making any difference.;;;","05/Apr/13 15:04;radev;Since that I've tried to run offline sstablescrub, it didn't help. As was suggested on IRC, I've tried sstable2json on data file and it worked. But when joining node tries to stream file - it fails repeatedly on the same file.;;;","05/Apr/13 15:06;radev;Also, I've tried shutting down one node, moving it's files to another box and starting it up with different IP/hostname - node was up and serving properly, but problem persisted.;;;","05/Apr/13 18:04;arya;I upgraded our 4 node sandbox cluster from 1.1.10 to 1.2.3. It is impossible to run repair on any node. They all get stuck without any exception in the log. Could my issue be related? Is there a workaround? I have 2 more days till my gc_grace.;;;","07/Apr/13 20:48;radev;I've tried to build latest source from 1.2 branch. And it worked properly, our cluster is repairing again and performing normally.
;;;","10/Apr/13 12:29;radev;Actually, it helped only temporarily, and appendFromStream now asserts when I try to bootstrap new node.;;;","10/Apr/13 12:31;radev;We are not using internode encryption, but compression is used.;;;","10/Apr/13 15:25;yukim;Igor, can you provide more info about this?
Do you see the same AssertionError for every CFs or the specific one? If the latter, can you post the definition of that CF?
;;;","10/Apr/13 20:57;radev;It's the same column family. We're doing lot's of deletes for it.
Seems that assertion is caused by element written twice on ColumnIndexer block boundary.
But column_index_size_in_kb is same on every node and set to default 64k.;;;","10/Apr/13 21:55;radev;Avoid duplication of columns on index block boundary when appending from stream (source stream already duplicated them).;;;","10/Apr/13 21:56;radev;Patch against branch 1.2;;;","11/Apr/13 03:32;yukim;Igor, thanks for the patch.
I think that would probably work, since the only code path that could write extra bytes is there, but I want to confirm by writing unit test for this. I'm working on it right now.;;;","11/Apr/13 10:32;radev;I've looked over the ColumnIndex.Builder code again and saw that it can build incorrect index (endPosition updated twice). So, added fromStream flag and skip logic to ColumnIndex.Builder.;;;","11/Apr/13 10:37;radev;v3 includes assertion, maybe will catch if column_index_size_in_kb is changed.;;;","11/Apr/13 12:35;slebresne;I agree on the source of the problem. On the patch however, since the goal should be to write only what we get from the stream (since we've used the dataSize from the stream), it would feel more natural to me to just skip tombstoneTracker.writeOpenedMarker (in which case we really can skip the tombstone tracker completely and save a few CPU cycles). I'm attaching a v5 patch that implement this (imo simpler) alternative.

Now as was noted above, this fix (whatever version of the patch we use) has the small downside that if the source and destination don't have the same column_index_size_in_kb, we'll be screwed. This is definitively a much less problem that this issue and so we should still fix this, but for 2.0, once CASSANRA-4180 gets in, then we should more or less revert this fix because it won't be necessary anymore. I've create CASSANRA-5454 so we don't forget about it.
;;;","11/Apr/13 15:31;yukim;So I created unit test to stream RangeTombstones between column index boundaries. (Patch attached)
It fails with the same stack trace here on current 1.2 branch, but it passes with 5418-v4.
So I will commit v4 and test.;;;","11/Apr/13 16:13;yukim;Committed.
Thanks Igor and Sylvain!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Batch with timestamp failed,CASSANDRA-5415,12640301,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aytereschenko,aytereschenko,02/Apr/13 13:41,16/Apr/19 09:32,14/Jul/23 05:53,08/Apr/13 13:01,1.2.4,,,,,,0,qa-resolved,,,,"When I create a prepared statement with the following CQL3 using Thrift protocol:
{code}
BEGIN BATCH USING TIMESTAMP <number>
<some INSERT INTO or UPDATE statements ....>
APPLY BATCH
{code}

and execute this statement in a loop, I randomly get the error:
*InvalidRequestException(why:Timestamp must be set either on BATCH or individual statements)*

All statements inside the batch have no individual USING TIMESTAMP.",,aleksey,aytereschenko,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Apr/13 20:51;aleksey;5415.txt;https://issues.apache.org/jira/secure/attachment/12577467/5415.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,320764,,,Mon Apr 08 13:01:16 UTC 2013,,,,,,,,,,"0|i1jcf3:",321105,,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,enigmacurry,,,"07/Apr/13 20:43;aleksey;BatchStatement.getMutations() currently modifies its nested modification statements when you set a batch-level timestamp:

{noformat}
for (ModificationStatement statement : statements)
{
    if (isSetTimestamp())
        statement.setTimestamp(getTimestamp(now));
{noformat}

Because of this, only the first execution of such a prepared statement will succeed. All subsequent attempts will fall because of this check in BatchStatement.validate():

{noformat}
for (ModificationStatement statement : statements)
{
    if (isSetTimestamp() && statement.isSetTimestamp())
        throw new InvalidRequestException(""Timestamp must be set either on BATCH or individual statements"");
{noformat}

The attached patch fixes this.;;;","08/Apr/13 08:17;slebresne;+1;;;","08/Apr/13 13:01;aleksey;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
incremental backups race,CASSANDRA-5410,12640117,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,01/Apr/13 14:01,16/Apr/19 09:32,14/Jul/23 05:53,04/Apr/13 21:43,1.2.4,,,,,,0,compaction,,,,"incremental backups does not mark things referenced or compacting, so it could get compacted away before createLinks runs.  Occasionally you can see this happen during ColumnFamilyStoreTest.  (Since it runs on the background tasks stage, it does not fail the test.)

{noformat}
    [junit] java.lang.RuntimeException: Tried to hard link to file that does not exist build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ja-8-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1066)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.run(DataTracker.java:168)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
{noformat}",,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/13 16:33;jbellis;5410.txt;https://issues.apache.org/jira/secure/attachment/12576804/5410.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,320580,,,Thu Apr 04 21:43:53 UTC 2013,,,,,,,,,,"0|i1jba7:",320921,,,,,,,,,marcuse,,marcuse,Low,,,,,,,,,,,,,,,,,,"03/Apr/13 16:33;jbellis;Patch to snapshot synchronously before creating a new View.;;;","04/Apr/13 21:43;marcuse;lgtm, committed as 8a422179c0a0a50e4d4b2e9274cbaf7259e90b2a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair exception when getPositionsForRanges returns empty iterator,CASSANDRA-5407,12639981,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,marcuse,marcuse,31/Mar/13 13:44,16/Apr/19 09:32,14/Jul/23 05:53,03/Apr/13 20:23,,,,,,,0,,,,,"CASSANDRA-5250 broke repair, this re-adds the code from CASSANDRA-5249",,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/13 09:30;marcuse;0001-CASSANDRA-5407-v2.patch;https://issues.apache.org/jira/secure/attachment/12576528/0001-CASSANDRA-5407-v2.patch","31/Mar/13 13:44;marcuse;0001-Fix-repair-bug-where-getPositionsForRanges-returns-e.patch;https://issues.apache.org/jira/secure/attachment/12576283/0001-Fix-repair-bug-where-getPositionsForRanges-returns-e.patch",,,,,,,,,,,,,,,,,,,2.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,320449,,,Wed Apr 03 20:23:47 UTC 2013,,,,,,,,,,"0|i1jah3:",320790,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"01/Apr/13 17:08;jbellis;What is causing the breakage?  Is it possible to add a test that exposes the problem?;;;","01/Apr/13 17:28;marcuse;my fix in CASSANDRA-5250 just fixed it for LCS (tests if intersecting sstables is empty in LeveledScanner), and this re-broke it for STCS

i'll try to write a unit test for this;;;","02/Apr/13 09:30;marcuse;adds a unit test that would have found the bug;;;","03/Apr/13 20:23;jbellis;LGTM, committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE during cql3 select with token(),CASSANDRA-5404,12639852,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,29/Mar/13 18:00,16/Apr/19 09:32,14/Jul/23 05:53,02/Apr/13 17:24,1.2.4,,,,,,0,,,,,"A query such as: select * from ""Standard1"" where token(key) > token(int(3030343330393233)) limit 1;

Produces:


{noformat}
 WARN 17:53:44,448 Inputing CLQ3 blobs as strings (like key = '') is now deprecated and will be removed in a future version. You should convert client code to use a blob constant (key = 0x) instead (see http://cassandra.apache.org/doc/cql3/CQL.html changelog section for more info).
ERROR 17:57:52,312 Error occurred during processing of message.
java.lang.NullPointerException
        at org.apache.cassandra.cql3.functions.FunctionCall$Raw.isAssignableTo(FunctionCall.java:135)
        at org.apache.cassandra.cql3.functions.Functions.validateTypes(Functions.java:131)
        at org.apache.cassandra.cql3.functions.Functions.get(Functions.java:92)
        at org.apache.cassandra.cql3.functions.FunctionCall$Raw.prepare(FunctionCall.java:103)
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.updateRestriction(SelectStatement.java:1246)
        at org.apache.cassandra.cql3.statements.SelectStatement$RawStatement.prepare(SelectStatement.java:959)
        at org.apache.cassandra.cql3.QueryProcessor.getStatement(QueryProcessor.java:271)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:140)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1726)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)
{noformat}",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Apr/13 11:10;slebresne;5404.txt;https://issues.apache.org/jira/secure/attachment/12576541/5404.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,320321,,,Tue Apr 02 17:24:11 UTC 2013,,,,,,,,,,"0|i1j9on:",320662,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"02/Apr/13 11:10;slebresne;Patch attached to handle the {{null}} correctly.

Let's note that the query itself is invalid because 'int' is not a function that exists.;;;","02/Apr/13 17:17;brandon.williams;bq. Let's note that the query itself is invalid because 'int' is not a function that exists.

Yeah, I'll admit I was trying to do something whackass there.  +1;;;","02/Apr/13 17:24;slebresne;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Clean up ColumnFamily, ISortedColumns heirarchy",CASSANDRA-5403,12639839,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,29/Mar/13 16:22,16/Apr/19 09:32,14/Jul/23 05:53,02/Apr/13 23:01,2.0 beta 1,,,,,,0,,,,,"CF wraps ISC but adds no real functionality of its own (post-supercolumn cleanup).

This means that we're wasting allocations every time we create a CF/ISC pair when really one would do.

Other things that would be nice to clean up:
# We often create an empty CF as a placeholder that should not be modified, and rely on convention to avoid such modification.  We could enforce this with a new CF/ISC subclass.
# Many places still use TMBSC where ABSC would be adequate and cheaper
# Other places still (ModificationStatement; others?) would be fine using a column container with no sorting requirement at all",,carlyeks,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Apr/13 17:01;jbellis;Screen Shot 2013-04-01 at 12.00.09 PM.png;https://issues.apache.org/jira/secure/attachment/12576398/Screen+Shot+2013-04-01+at+12.00.09+PM.png",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,320308,,,Tue Apr 02 23:01:31 UTC 2013,,,,,,,,,,"0|i1j9lr:",320649,,,,,,,,,carlyeks,,carlyeks,Normal,,,,,,,,,,,,,,,,,,"29/Mar/13 16:25;jbellis;Patchset pushed to https://github.com/jbellis/cassandra/tree/5403.  (To save me rebase pain, it builds on the patches for CASSANDRA-5395, which are not yet committed, but you don't need to re-review those.  So the first commit to review here is 20ebbe37867820e83f0fef63f95fc0cb9e7982ea: make getColumnNames return Iterable.);;;","01/Apr/13 16:18;jbellis;Rebased on top of trunk (post-5395-commit) and fixed some test errors: https://github.com/jbellis/cassandra/commits/5403-2;;;","01/Apr/13 16:50;jbellis;According to YourKit, this takes the time in CF.addColumn from 50% of mutationsForKey to 10%.;;;","01/Apr/13 17:01;jbellis;Most of the rest of the time is taken up by CompositeType construction. :-|;;;","02/Apr/13 22:45;carlyeks;LGTM; ship it :)

For reference, merged in trunk and pushed to https://github.com/carlyeks/cassandra/tree/5403.;;;","02/Apr/13 23:01;jbellis;Rebased + committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Updates to PerRowSecondaryIndex don't use most current values,CASSANDRA-5397,12639634,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,samt,samt,samt,28/Mar/13 17:28,16/Apr/19 09:32,14/Jul/23 05:53,04/Apr/13 18:24,1.2.4,,,,,,0,,,,,"The way that updates to secondary indexes are performed using  SecondaryIndexManager.Updater is flawed for PerRowSecondaryIndexes.  Unlike PerColumnSecondaryIndexes, which only require the old & new values for a single column,  the expectation is that a PerRow indexer can be given just a key which it will use to retrieve the entire row (or as many columns as it requires) and perform its indexing on those columns.  As the indexes are updated before the memtable atomic swap occurs, a per-row indexer may only read the previous values for the row, not the new ones that are being written. In the case of an insert, there is no previous value and so nothing is added to the index.",,christianmovi,jjordan,marcuse,rcoli,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Apr/13 15:56;jbellis;5397-1.2-v3.txt;https://issues.apache.org/jira/secure/attachment/12576992/5397-1.2-v3.txt","04/Apr/13 17:03;samt;5397-1.2-v4.txt;https://issues.apache.org/jira/secure/attachment/12577011/5397-1.2-v4.txt","28/Mar/13 17:30;samt;5397.txt;https://issues.apache.org/jira/secure/attachment/12575897/5397.txt","04/Apr/13 09:35;samt;5397_12.txt;https://issues.apache.org/jira/secure/attachment/12576954/5397_12.txt","04/Apr/13 09:35;samt;5397_trunk.txt;https://issues.apache.org/jira/secure/attachment/12576955/5397_trunk.txt",,,,,,,,,,,,,,,,5.0,samt,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,320103,,,Fri Apr 05 08:57:28 UTC 2013,,,,,,,,,,"0|i1j8c7:",320444,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"28/Mar/13 17:30;samt;Patch extends the SecondaryIndexManager.Updater interface to add a commit() method.  Behaviour for PerColumn indexes remains unchanged, with updates applied to the index immediately and so the commit is a no-op. For PerRow indexes, the updates are deferred until after the memtable update occurs, then actioned via the call to commit.  I missed this in CASSANDRA-2897 partially because there are no implementations of PerRowSecondaryIndex in the codebase, so I've also added a unit test with a dummy implementation.;;;","01/Apr/13 17:15;jbellis;TBH my preferred fix here would be to make PerRowSI also do ""lazy updates"" a la CASSANDRA-2897.  Is that possible?;;;","02/Apr/13 13:56;samt;I don't really see how we can do that, especially as PRSI is typically implemented outside of C* and the contract we give it is expressed in the method signature {{public abstract void index(ByteBuffer rowKey);}} So the assumption is that an update to a PRSI will be able to access the entire row at index time. 

Changes to AtomicSortedColumns are applied a column at a time so MixedIndexUpdater has a guard to ensure that even when a mutation changes multiple columns in the row,  the index is only updated once. Obviously though, until the last of these column updates occurs, the row is not fully updated. So I see 2 options, defer the per-row indexing until we've finished updating the row (as in my first patch), or remove the guard and apply the per-row update as each column is updated. The second option has the benefit of not changing the SIM.Updater api, but is potentially very inefficient.;;;","03/Apr/13 20:14;jbellis;Is this intended for 1.2 or 2.0?  I'm getting lots of conflicts on both.;;;","04/Apr/13 09:34;samt;The patch was originally against 1.2, but it needed rebasing after CASSANDRA-5395 was committed. I'm attaching 2 new versions, one each for 1.2 and trunk.;;;","04/Apr/13 15:56;jbellis;v3 against 1.2 fixes some formatting and removes the {{if (column.isMarkedForDelete()) return}} from MIU.update, since it would re-introduce one of the problems fixed in CASSANDRA-5395.

Not sure if it should actually be moved to the ""not instanceof PCSI"" block -- if so, how does PRSI remove stale entries?;;;","04/Apr/13 15:59;jbellis;Also, remove is only called by compaction, so there will be no commit (so adding to deferred is a bad idea).

If we're assuming that PRSI always keeps the index exactly up to date, remove can be a no-op.;;;","04/Apr/13 17:03;samt;Yes, you're right about the {{if( column.isMarkedForDelete()) return}} being a regression.

Its down to the PRSI implementation to figure out whether an update is actually an update or whether it actually calls for a delete. As the PRSI only has the key to work with & is going to be inspecting the whole row anyway this shouldn't be difficult, but it does make the whole SI/PCSI/PRCI hierarchy a bit ugly. 

Also, we do/should assume that PRSI always keeps the index exactly up to date, so I'm +1 with making remove a no-op there.  

attached v4 for 1.2 (v3 + the no-op remove for PRSI)
;;;","04/Apr/13 18:24;jbellis;Odd, I'm seeing the following with v4:

{noformat}
formite:git johnathanellis$ patch -p1 < ~/.JIRAClient/download/5397-1.2-v4.txt 
patching file src/java/org/apache/cassandra/db/AtomicSortedColumns.java
patching file src/java/org/apache/cassandra/db/index/SecondaryIndexManager.java
patch: **** malformed patch at line 159: diff --git a/test/unit/org/apache/cassandra/SchemaLoader.java b/test/unit/org/apache/cassandra/SchemaLoader.java
{noformat}

I committed what I think is the same code based on v3, please doublecheck it.;;;","04/Apr/13 19:44;samt;lgtm, thanks.;;;","04/Apr/13 20:24;jjordan;Does this effect 1.1?  Or is it a problem with the new faster 1.2 indexes?;;;","05/Apr/13 08:57;marcuse;pushed a build fix to trunk (basically only fixed the test case): 6afbed371c0d12a15a969e4f52ba670998bab282 RowMutations do not take QueryPath in trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compaction doesn't remove index entries as designed,CASSANDRA-5395,12639433,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,27/Mar/13 19:32,16/Apr/19 09:32,14/Jul/23 05:53,04/Apr/13 15:48,1.2.4,,,,,,0,qa-resolved,,,,"PerColumnIndexUpdater ignores updates where the new value is a tombstone.  It should still remove the index entry on oldColumn.

(Note that this will not affect user-visible correctness, since KeysSearcher/CompositeSearcher will issue deletes against stale index entries, but having more stale entries than we ""should"" could affect performance.)",,samt,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/13 20:02;jbellis;5395-2.txt;https://issues.apache.org/jira/secure/attachment/12575756/5395-2.txt","27/Mar/13 19:34;jbellis;5395.txt;https://issues.apache.org/jira/secure/attachment/12575751/5395.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,319903,,,Fri Mar 29 17:08:24 UTC 2013,,,,,,,,,,"0|i1j73r:",320244,,,,,,,,,samt,,samt,Low,,,,,,,,,,,,,,,enigmacurry,,,"27/Mar/13 20:02;jbellis;second patch attached (on top of the first) that also avoids creating duplicate index entries during PrecompactedRow.merge.  (Calling indexer.update(A, B) would remove the entry for A and add one for B, but since we're compacting we know that an entry for B already exists.)

switched to a merge-column-at-a-time approach similar to what LCR uses.

also tweaked LCR's reducer to short-circuit the column lookup if there is no index involved.;;;","27/Mar/13 22:06;jbellis;Fixed a test failure and pushed to http://github.com/jbellis/cassandra/tree/5395.  (Original patches were also half 1.2, half trunk.  Now both against 1.2.);;;","28/Mar/13 01:58;jbellis;... And pushed a 3rd commit applying the fix in patch to to ParallelCompactionIterable.

This also has the side effect of switching from TSM to ABSM for PrecompactedRow and ParallelCompactionIterable, so there may be a performance improvement.;;;","28/Mar/13 20:04;samt;lgtm, just have 2 trivial queries:

In LCR & PCR, if the purpose of the additional clauses is to omit unnecessary column lookups, should the column lookup be the last of the &&'d  conditions?

{code}
 if (indexer != SecondaryIndexManager.nullUpdater
                    && !column.isMarkedForDelete()
                    && container.getColumn(column.name()) != column)
{code}


Class documentation in IdentityQueryFilter states ""Only for use in testing; will read entire CF into memory."" Seeing as its being used in non-test code we should probably amend the docstring
;;;","29/Mar/13 17:08;jbellis;Committed with suggested improvements.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add retry mechanism to OTC for non-droppable_verbs,CASSANDRA-5393,12639405,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jasobrown,jjordan,jjordan,27/Mar/13 17:59,16/Apr/19 09:32,14/Jul/23 05:53,18/Apr/13 20:56,1.1.12,1.2.5,2.0 beta 1,,,,1,,,,,"Can we add an Ack/Retry around passing merle tree's around in repair?  If the following fails, the repair hangs for ever on the coordinating node.

https://github.com/apache/cassandra/blob/cassandra-1.1.10/src/java/org/apache/cassandra/service/AntiEntropyService.java#L242

{noformat}
            Message message = TreeResponseVerbHandler.makeVerb(local, validator);
            if (!validator.request.endpoint.equals(FBUtilities.getBroadcastAddress()))
                logger.info(String.format(""[repair #%s] Sending completed merkle tree to %s for %s"", validator.request.sessionid, validator.request.endpoint, validator.request.cf));
            ms.sendOneWay(message, validator.request.endpoint);
{noformat}

If the message asking for merkle tree's gets lost, coordinating node hangs for ever as well.",,arya,christianmovi,jasobrown,jjordan,marcuse,rcoli,soverton,timiblossom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5426,"18/Apr/13 00:08;jasobrown;5393-v2.patch;https://issues.apache.org/jira/secure/attachment/12579245/5393-v2.patch","18/Apr/13 04:57;jasobrown;5393-v3.patch;https://issues.apache.org/jira/secure/attachment/12579269/5393-v3.patch","18/Apr/13 14:38;jbellis;5393-v4.txt;https://issues.apache.org/jira/secure/attachment/12579329/5393-v4.txt","18/Apr/13 00:02;jasobrown;5393.patch;https://issues.apache.org/jira/secure/attachment/12579244/5393.patch",,,,,,,,,,,,,,,,,4.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,319875,,,Thu Apr 18 20:56:01 UTC 2013,,,,,,,,,,"0|i1j6xj:",320216,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"02/Apr/13 07:34;jasobrown;We've got an idea we're testing out here, and will hopefully post a patch in a day or so.;;;","03/Apr/13 23:48;jasobrown;Yuki's ticket is more comprehensive than this one;;;","18/Apr/13 00:01;jasobrown;At the end of the day, this is what I see happening:

{code}INFO [AntiEntropyStage:1] 2013-03-27 22:48:55,390 AntiEntropyService.java (line 239) repair #80fe25a0-9730-11e2-0000-ebe7011631ff Sending completed merkle tree to /54.246.XXX.YYY for (Geo,GeoCountryMetadata)
DEBUG [WRITE-/54.246.XXX.YYY] 2013-03-27 22:48:55,392 OutboundTcpConnection.java (line 165) error writing to ec2-54-246-XXX.YYY.eu-west-1.compute.amazonaws.com/54.246.XXX.YYY
java.net.SocketException: Connection timed out
at java.net.SocketOutputStream.socketWrite0(Native Method)
at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:92)
at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
at com.sun.net.ssl.internal.ssl.OutputRecord.writeBuffer(OutputRecord.java:358)
at com.sun.net.ssl.internal.ssl.OutputRecord.write(OutputRecord.java:346)
at com.sun.net.ssl.internal.ssl.SSLSocketImpl.writeRecordInternal(SSLSocketImpl.java:781)
at com.sun.net.ssl.internal.ssl.SSLSocketImpl.writeRecord(SSLSocketImpl.java:753)
at com.sun.net.ssl.internal.ssl.AppOutputStream.write(AppOutputStream.java:100)
at java.io.BufferedOutputStream.flushBuffer(BufferedOutputStream.java:65)
at java.io.BufferedOutputStream.write(BufferedOutputStream.java:104)
at java.io.DataOutputStream.write(DataOutputStream.java:90)
at java.io.FilterOutputStream.write(FilterOutputStream.java:80)
at org.apache.cassandra.net.OutboundTcpConnection.write(OutboundTcpConnection.java:200)
at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:152)
at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:126)
{code}

The interesting thing is the ""Connection timed out"" exception message, rather than socket reset (or something similar). So, I'm thinking this might be to keepalive timing out after the connection is broken. I was able to reproduce this exception several times by having my test cluster setup in three ec2 regions (us-west-2, us-east-1, eu-west-1 - three nodes in each), and not sending any traffic for multiple hours. Basically, I'm waiting for the connection to get dropped. Thus, when I went to triggered repair on one of the nodes (usu. starting with us-west-2), I could see where the eu-west-1 nodes would get the request to build the merkle tree, but then failed on sending the tree response with the above exception. I was able to get similar problems when trying a schema update after many hours of cluster idleness.

The attached patch catches the exception when the socket is dead (for whatever reason), and attempts a simple retry by requeueing the message at the end of the backlog queue, with the hope that the next pass will successfully recreate the socket. Note that I'm excluding MessagingService.DROPPABLE_VERBS from retries as it's OK to drop reads/mutates, but it's really those AES and other schema-related messages that I think we'd want to retry.

Admittedly this is a simple mechanism that doesn't try to do anything fancy like exponential backoff, n-levels of configurable retrys, and so on. I'm open to discussion on that, but I'm not sure how much complexity we'd want to build in for that at this point. I think an incremental improvement would go a long way here as we're currently obscuring when messages can't be sent (which is OK for DROPPABLE_VERBS, but those other ones are ones are really important), so added visibility and a retry mechanism will help. 


 ;;;","18/Apr/13 00:08;jasobrown;v2 addresses a potential race condition between disconnecting the bad socket and re-enqueueing the failed message.;;;","18/Apr/13 00:42;marcuse;+1;;;","18/Apr/13 01:41;jbellis;Can we make an Entry subclass instead of saddling each Entry with an extra field that will mostly be unused?

Also, space before open paren. :);;;","18/Apr/13 04:57;jasobrown;v3 includes Jonathan's suggestions. Created RetryableEntry as a subclass of Entry. Added method shouldRetry() to Entry; Entry will always return false, and RetryableEntry will check it's member boolean.
;;;","18/Apr/13 14:31;jbellis;v4 attached -- easier to show than explain what I meant in English. :);;;","18/Apr/13 14:38;jbellis;sorry for the attachment churn, decided to improve the comments too :);;;","18/Apr/13 17:26;jasobrown;I think your patch and my patch are rather similar, but I'm game either way :). However, there is a small bug in Entry.shouldRetry(); you have

{code}return MessagingService.DROPPABLE_VERBS.contains(message.getVerb());{code}

but should be

{code}return !MessagingService.DROPPABLE_VERBS.contains(message.getVerb());{code}

Otherwise we would retry the DROPPABLE_VERBS, which we want to drop.

With that small fix, lgtm.;;;","18/Apr/13 18:18;jbellis;Ship it!;;;","18/Apr/13 20:56;jasobrown;Changed name of ticket to better describe the change.

Committed to 1.1, 1.2, and trunk.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-all 1.2.0 pom missing netty dependency,CASSANDRA-5392,12639348,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,sgbridges,sgbridges,sgbridges,27/Mar/13 15:56,16/Apr/19 09:32,14/Jul/23 05:53,01/Apr/13 18:57,1.2.4,,,Packaging,,,0,,,,,"It seems that cassandra depends on netty now, however the pom excludes this dependency.  This was previously reported as CASSANDRA-5181, but the fix for 5181 added netty to the dependency-management section of the pom, not the depencies section",,dbrosius,eparusel,sgbridges,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/13 15:58;sgbridges;CASSANDRA-5392.txt;https://issues.apache.org/jira/secure/attachment/12575723/CASSANDRA-5392.txt",,,,,,,,,,,,,,,,,,,,1.0,sgbridges,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,319818,,,Mon Apr 01 18:57:32 UTC 2013,,,,,,,,,,"0|i1j6kv:",320159,,,,,,,,,dbrosius,,dbrosius,Normal,,,,,,,,,,,,,,,,,,"01/Apr/13 18:57;dbrosius;+1, committed to cassandra-1.2 as e06875ed2cd47f7bd77eaae9cc70dee5a3c0371a;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
SSL problems with inter-DC communication,CASSANDRA-5391,12639310,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,yukim,ondrej.cernos,ondrej.cernos,27/Mar/13 13:43,16/Apr/19 09:32,14/Jul/23 05:53,04/Apr/13 00:33,1.2.4,,,,,,0,,,,,"I get SSL and snappy compression errors in multiple datacenter setup.

The setup is simple: 3 nodes in AWS east, 3 nodes in Rackspace. I use slightly modified Ec2MultiRegionSnitch in Rackspace (I just added a regex able to parse the Rackspace/Openstack availability zone which happens to be in unusual format).

During {{nodetool rebuild}} tests I managed to (consistently) trigger the following error:

{noformat}
2013-03-19 12:42:16.059+0100 [Thread-13] [DEBUG] IncomingTcpConnection.java(79) org.apache.cassandra.net.IncomingTcpConnection: IOException reading from socket; closing
java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:391)
	at org.apache.cassandra.io.compress.SnappyCompressor.uncompress(SnappyCompressor.java:93)
	at org.apache.cassandra.streaming.compress.CompressedInputStream.decompress(CompressedInputStream.java:101)
	at org.apache.cassandra.streaming.compress.CompressedInputStream.read(CompressedInputStream.java:79)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:337)
	at org.apache.cassandra.utils.BytesReadTracker.readUnsignedShort(BytesReadTracker.java:140)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:160)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:226)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:166)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:66)
{noformat}

The exception is raised during DB file download. What is strange is the following:

* the exception is raised only when rebuildig from AWS into Rackspace
* the exception is raised only when all nodes are up and running in AWS (all 3). In other words, if I bootstrap from one or two nodes in AWS, the command succeeds.

Packet-level inspection revealed malformed packets _on both ends of communication_ (the packet is considered malformed on the machine it originates on).

Further investigation raised two more concerns:

* We managed to get another stacktrace when testing the scenario. The exception was raised only once during the tests and was raised when I throttled the inter-datacenter bandwidth to 1Mbps.

{noformat}
java.lang.RuntimeException: javax.net.ssl.SSLException: bad record MAC
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.lang.Thread.run(Thread.java:662)
Caused by: javax.net.ssl.SSLException: bad record MAC
	at com.sun.net.ssl.internal.ssl.Alerts.getSSLException(Alerts.java:190)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1649)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.fatal(SSLSocketImpl.java:1607)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readRecord(SSLSocketImpl.java:859)
	at com.sun.net.ssl.internal.ssl.SSLSocketImpl.readDataRecord(SSLSocketImpl.java:755)
	at com.sun.net.ssl.internal.ssl.AppInputStream.read(AppInputStream.java:75)
	at org.apache.cassandra.streaming.compress.CompressedInputStream$Reader.runMayThrow(CompressedInputStream.java:151)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 1 more
{noformat}

This is pure SSL error with no snappy interference.

* I managed to trigger the exception during {{nodetool repair}} tests when replacing dead node with a new one _on the aws side_, which means the problem is not restricted to the one-way scenario only.

{noformat}
2013-03-27 14:06:03.033+0100 [Thread-137] [INFO] StreamInSession.java(136) org.apache.cassandra.streaming.StreamInSession: Streaming of file /path/to/cassandra/data/ks/cf/ks-cf-ib-2-Data.db sections=3 progress=0/20513 - 0% for org.apache.cassandra.streaming.StreamInSession@14450ae7 failed: requesting a retry.
2013-03-27 14:06:03.033+0100 [Thread-138] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-98-Data.db
2013-03-27 14:06:03.033+0100 [Thread-138] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-98-Filter.db
2013-03-27 14:06:03.034+0100 [Thread-138] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-98-TOC.txt
2013-03-27 14:06:03.034+0100 [Thread-137] [DEBUG] IncomingTcpConnection.java(91) org.apache.cassandra.net.IncomingTcpConnection: IOException reading from socket; closing
java.io.IOException: FAILED_TO_UNCOMPRESS(5)
	at org.xerial.snappy.SnappyNative.throw_error(SnappyNative.java:78)
	at org.xerial.snappy.SnappyNative.rawUncompress(Native Method)
	at org.xerial.snappy.Snappy.rawUncompress(Snappy.java:391)
	at org.apache.cassandra.io.compress.SnappyCompressor.uncompress(SnappyCompressor.java:93)
	at org.apache.cassandra.streaming.compress.CompressedInputStream.decompress(CompressedInputStream.java:101)
	at org.apache.cassandra.streaming.compress.CompressedInputStream.read(CompressedInputStream.java:79)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
	at org.apache.cassandra.utils.BytesReadTracker.readUnsignedShort(BytesReadTracker.java:140)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:160)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
{noformat}","$ /etc/alternatives/jre_1.6.0/bin/java -version
java version ""1.6.0_23""
Java(TM) SE Runtime Environment (build 1.6.0_23-b05)
Java HotSpot(TM) 64-Bit Server VM (build 19.0-b09, mixed mode)
$ uname -a
Linux hostname 2.6.32-358.2.1.el6.x86_64 #1 SMP Tue Mar 12 14:18:09 CDT 2013 x86_64 x86_64 x86_64 GNU/Linux
$ cat /etc/redhat-release 
Scientific Linux release 6.3 (Carbon)
$ facter | grep ec2
...
ec2_placement => availability_zone=us-east-1d
...
$ rpm -qi cassandra
cassandra-1.2.3-1.el6.cmp1.noarch
(custom built rpm from cassandra tarball distribution)",aleksey,jan.chochol,marcuse,mikelococo,ondrej.cernos,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5381,,,,,,,,,CASSANDRA-5390,,,,,,,,,,,"03/Apr/13 13:15;yukim;5391-1.2.3.txt;https://issues.apache.org/jira/secure/attachment/12576777/5391-1.2.3.txt","29/Mar/13 03:34;yukim;5391-1.2.txt;https://issues.apache.org/jira/secure/attachment/12576020/5391-1.2.txt","03/Apr/13 13:15;yukim;5391-v2-1.2.txt;https://issues.apache.org/jira/secure/attachment/12576778/5391-v2-1.2.txt",,,,,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,319780,,,Mon Jul 01 23:14:59 UTC 2013,,,,,,,,,,"0|i1j6cf:",320121,,,,,,,,,aleksey,,aleksey,Critical,,,,,,,,,,,,,,,,,,"27/Mar/13 14:23;ondrej.cernos;With SSL switched off all the scenarios work well.;;;","27/Mar/13 15:24;ondrej.cernos;After clarifying CASSANDRA-5390 I tried to switch to {{DeflateCompressor}} SSTable compression algorithm. The problem is compression-algorithm independent:

{noformat}
2013-03-27 16:19:57.633+0100 [Thread-31] [INFO] StreamInSession.java(136) org.apache.cassandra.streaming.StreamInSession: component=c4 Streaming of file /mnt/ebs/cassandra/data/c4/user_profile_settings/c4-user_profile_settings-ib-2-Data.db sections=130 progress=0/1628502 - 0% for org.apache.cassandra.streaming.StreamInSession@20f92649 failed: requesting a retry.
2013-03-27 16:19:57.633+0100 [Thread-31] [DEBUG] IncomingTcpConnection.java(91) org.apache.cassandra.net.IncomingTcpConnection: component=c4 IOException reading from socket; closing
java.io.IOException: CRC unmatched
	at org.apache.cassandra.streaming.compress.CompressedInputStream.decompress(CompressedInputStream.java:111)
	at org.apache.cassandra.streaming.compress.CompressedInputStream.read(CompressedInputStream.java:79)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
	at org.apache.cassandra.utils.BytesReadTracker.readUnsignedShort(BytesReadTracker.java:140)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:160)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
{noformat};;;","27/Mar/13 16:19;ondrej.cernos;Update:

With SSTable compression switched off the bug disappears. When I run nodetool rebuild us-east on a Rackspace node, it fetches the data correctly and when I compare the md5 of the DB file on an AWS node (after flush and compaction), it is exactly the same as on the Rackspace node.
It means the problem is only with compressed SSTables, but the problem is independent on chosen compression algorithm. And only with SSL switched on for inter-DC communication.;;;","27/Mar/13 16:29;jbellis;Can you shed any light, Jake?;;;","27/Mar/13 16:49;tjake;Nope, we don't use SSL.  Does it work when you disable internode compression?;;;","27/Mar/13 17:03;ondrej.cernos;Internode compression settings didn't have any influence on the problem. The case is very strange:

* it happens only when SSTables are compressed (see the update above)
* it is independent on SSTable compression implementation however (also see above)
* it happens only when ""enough"" (all 3) nodes are switched on in AWS. With 2 or 1 only the problem disappears

Do you have a recommendation on what to investigate further? I already asked the network team to check networking - they say all is ok - and our operations, who also cannot identify anything, except for the fact MTU is different in Rackspace and AWS, so packet from AWS to Rackspace get fragmented.;;;","28/Mar/13 14:44;ondrej.cernos;I am becoming quite sure the problem is a race condition in Cassandra code handling decompression of sstables when these are streamed from the remote datacenter.

Both traces - when snappy is used and when the java zip is used - share the same calls, see above.

I switched trace level in log4j and this is what I found:

* when 2 and more nodes live in the remote DC, cassandra fires two threads downloading the same file
* when only 1 node lives in the remote DC, only one thread downloads the file

This is how it looks in log:

{noformat}
2013-03-28 13:44:57.301+0100 [Thread-22] [DEBUG] StreamInSession.java(104) org.apache.cassandra.streaming.StreamInSession: Adding file /path/to/cassandra/data/ks/cf/ks-cf-ib-2-Data.db to Stream Request queue
2013-03-28 13:44:57.301+0100 [Thread-22] [DEBUG] StreamInSession.java(104) org.apache.cassandra.streaming.StreamInSession: Adding file /path/to/cassandra/data/ks/cf/ks-cf-ib-1-Data.db to Stream Request queue
2013-03-28 13:44:57.338+0100 [Thread-23] [DEBUG] StreamInSession.java(104) org.apache.cassandra.streaming.StreamInSession: Adding file /path/to/cassandra/data/ks/cf/ks-cf-ib-2-Data.db to Stream Request queue
2013-03-28 13:44:57.340+0100 [Thread-23] [DEBUG] StreamInSession.java(104) org.apache.cassandra.streaming.StreamInSession: Adding file /path/to/cassandra/data/ks/cf/ks-cf-ib-1-Data.db to Stream Request queue
{noformat}

And here is the result grepped on the two threads:

{noformat}
2013-03-28 13:44:57.477+0100 [Thread-22] [TRACE] SSTableWriter.java(145) org.apache.cassandra.io.sstable.SSTableWriter: wrote DecoratedKey(-8516046549581000893, 6663363133663230623932663663303732623735653332643964616261623165) at 183591
2013-03-28 13:44:57.477+0100 [Thread-22] [TRACE] SSTableWriter.java(463) org.apache.cassandra.io.sstable.SSTableWriter: wrote index entry: org.apache.cassandra.db.RowIndexEntry@7b553d18 at 16192
2013-03-28 13:44:57.477+0100 [Thread-22] [TRACE] SSTableWriter.java(145) org.apache.cassandra.io.sstable.SSTableWriter: wrote DecoratedKey(-8513551951874950453, 3934363831326161323235653165613662613039346233356264386461653735) at 183995
2013-03-28 13:44:57.478+0100 [Thread-22] [TRACE] SSTableWriter.java(463) org.apache.cassandra.io.sstable.SSTableWriter: wrote index entry: org.apache.cassandra.db.RowIndexEntry@d5f0688 at 16238
2013-03-28 13:44:57.501+0100 [Thread-22] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-1-Data.db
2013-03-28 13:44:57.501+0100 [Thread-22] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-1-Filter.db
2013-03-28 13:44:57.501+0100 [Thread-22] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-1-TOC.txt
2013-03-28 13:44:57.501+0100 [Thread-22] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-1-CompressionInfo.db
2013-03-28 13:44:57.502+0100 [Thread-22] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-1-Index.db
2013-03-28 13:44:57.502+0100 [Thread-22] [DEBUG] SSTable.java(154) org.apache.cassandra.io.sstable.SSTable: Deleted /path/to/cassandra/data/ks/cf/ks-cf-tmp-ib-1
2013-03-28 13:44:57.503+0100 [Thread-22] [INFO] StreamInSession.java(136) org.apache.cassandra.streaming.StreamInSession: Streaming of file /path/to/cassandra/data/ks/cf/ks-cf-ib-2-Data.db sections=130 progress=67628/1583497 - 4% for org.apache.cassandra.streaming.StreamInSession@21400eb0 failed: requesting a retry.
2013-03-28 13:44:57.504+0100 [Thread-22] [DEBUG] IncomingTcpConnection.java(91) org.apache.cassandra.net.IncomingTcpConnection: IOException reading from socket; closing
java.io.IOException: CRC unmatched
        at org.apache.cassandra.streaming.compress.CompressedInputStream.decompress(CompressedInputStream.java:111)
        at org.apache.cassandra.streaming.compress.CompressedInputStream.read(CompressedInputStream.java:79)
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
        at org.apache.cassandra.utils.BytesReadTracker.readUnsignedShort(BytesReadTracker.java:140)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
        at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:160)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
        at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)

2013-03-28 13:44:58.070+0100 [Thread-23] [TRACE] SSTableWriter.java(463) org.apache.cassandra.io.sstable.SSTableWriter: wrote index entry: org.apache.cassandra.db.RowIndexEntry@db766c1 at 106582
2013-03-28 13:44:58.071+0100 [Thread-23] [TRACE] SSTableWriter.java(145) org.apache.cassandra.io.sstable.SSTableWriter: wrote DecoratedKey(-4829320003365722996, 6333383266393230353964313666633136356335333437353637373735653065) at 1217942
2013-03-28 13:44:58.071+0100 [Thread-23] [TRACE] SSTableWriter.java(463) org.apache.cassandra.io.sstable.SSTableWriter: wrote index entry: org.apache.cassandra.db.RowIndexEntry@3bb0ff0 at 106628
2013-03-28 13:44:58.071+0100 [Thread-23] [TRACE] SSTableWriter.java(145) org.apache.cassandra.io.sstable.SSTableWriter: wrote DecoratedKey(-4827623571007838156, 6162376162333238393739643930336266616566393039376131366238386166) at 1218191
2013-03-28 13:44:58.071+0100 [Thread-23] [TRACE] SSTableWriter.java(463) org.apache.cassandra.io.sstable.SSTableWriter: wrote index entry: org.apache.cassandra.db.RowIndexEntry@6e135779 at 106674
2013-03-28 13:44:58.091+0100 [Thread-23] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-2-Data.db
2013-03-28 13:44:58.091+0100 [Thread-23] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-2-Filter.db
2013-03-28 13:44:58.091+0100 [Thread-23] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-2-TOC.txt
2013-03-28 13:44:58.091+0100 [Thread-23] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-2-CompressionInfo.db
2013-03-28 13:44:58.091+0100 [Thread-23] [DEBUG] FileUtils.java(110) org.apache.cassandra.io.util.FileUtils: Deleting ks-cf-tmp-ib-2-Index.db
2013-03-28 13:44:58.091+0100 [Thread-23] [DEBUG] SSTable.java(154) org.apache.cassandra.io.sstable.SSTable: Deleted /path/to/cassandra/data/ks/cf/ks-cf-tmp-ib-2
2013-03-28 13:44:58.091+0100 [Thread-23] [INFO] StreamInSession.java(136) org.apache.cassandra.streaming.StreamInSession: Streaming of file /path/to/cassandra/data/ks/cf/ks-cf-ib-2-Data.db sections=131 progress=406399/1638227 - 24% for org.apache.cassandra.streaming.StreamInSession@37d40164 failed: requesting a retry.
2013-03-28 13:44:58.092+0100 [Thread-23] [DEBUG] IncomingTcpConnection.java(91) org.apache.cassandra.net.IncomingTcpConnection: IOException reading from socket; closing
java.io.IOException: CRC unmatched
        at org.apache.cassandra.streaming.compress.CompressedInputStream.decompress(CompressedInputStream.java:111)
        at org.apache.cassandra.streaming.compress.CompressedInputStream.read(CompressedInputStream.java:79)
        at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
        at org.apache.cassandra.utils.BytesReadTracker.readUnsignedShort(BytesReadTracker.java:140)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
        at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:160)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
        at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
{noformat}

This is common for snappy and java zip:

{noformat}
	at org.apache.cassandra.streaming.compress.CompressedInputStream.read(CompressedInputStream.java:79)
	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:320)
	at org.apache.cassandra.utils.BytesReadTracker.readUnsignedShort(BytesReadTracker.java:140)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:361)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:371)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:160)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:238)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:178)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:78)
{noformat}

When the download runs in single thread, the problem disappears.;;;","28/Mar/13 15:32;ondrej.cernos;How does cassandra compute the number of threads involved in streaming?;;;","28/Mar/13 16:06;yukim;When sending file, it is single-threaded per destination.
;;;","28/Mar/13 17:55;ondrej.cernos;How does this information match the observed behaviour? I can clearly see two threads downloading the file.;;;","29/Mar/13 03:34;yukim;CompressedFileStreamTask is not sending the right part of the file when using inter-node encryption, and that causes various IOException described here.

Patch attached for fix.;;;","29/Mar/13 08:38;aleksey;+1;;;","29/Mar/13 14:59;yukim;Committed. Thanks!;;;","03/Apr/13 09:18;jan.chochol;Hi everyone, I am working with Ondrej on same problem.
I just looked to patch (git commit cc6429e2722e764dce8cad77660732146ed596ab) and I am not sure that it is exactly correct.
Problematic situation is, when {{length}} > {{CHUNK_SIZE}} (in {{CompressedFileStreamTask.stream()}}). In this case data will be sent in more chunks, but before every chunk this code will be executed:
{noformat}
file.seek(section.left);
{noformat}
Sending only first chunk every time will probably lead to described error.
I would suggest to move {{file.seek}} before beginning of {{while}} cycle (than file pointer will be moved by {{readFully}}) or change mentioned code to
{noformat}
file.seek(section.left + bytesTransferred);
{noformat};;;","03/Apr/13 09:40;marcuse;that sounds and looks reasonable, reopening to let @yukim have a look;;;","03/Apr/13 09:49;ondrej.cernos;I tested the patch and verified it doesn't fix the issue.;;;","03/Apr/13 11:29;ondrej.cernos;I tried Jan's proposal to move the seek out of the while loop and let the pointer be moved by the readFully method call, but with no luck. I'll let someone with more Cassandra internals knowledge to dive into this.;;;","03/Apr/13 13:15;yukim;Jan, Ondřej,

Thanks for reporting.
What I wanted to do was to position the file pointer to the beginning of section for each loop, as uncompressed version do.
I attached two patches (-1.2.3 to apply for 1.2.3 release and -1.2 for current 1.2 branch). Can you try these?;;;","03/Apr/13 15:58;ondrej.cernos;[This|https://issues.apache.org/jira/secure/attachment/12576777/5391-1.2.3.txt] patch seems to work. So the issue may be resolved now. Thanks!;;;","04/Apr/13 00:33;yukim;Committed, thanks!;;;","01/Jul/13 23:14;mikelococo;I don't think I see any code changes in the 1.1.x branch as a result of this bug. Does the bug not apply to 1.1.x (aka, it was introduced in the 1.2.0 streaming refactor?), or does 1.1.12 (and 1.1.9, on which Datastax Enterprise is based) still suffer from this?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unit tests fail due to ant/junit problem,CASSANDRA-5388,12639209,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,enigmacurry,enigmacurry,26/Mar/13 22:09,16/Apr/19 09:32,14/Jul/23 05:53,27/May/13 16:35,,,,,,,0,,,,,"Intermittently, but more often than not I get the following error when running 'ant test' on Windows 7 (also encountered on Linux now):

{code}
BUILD FAILED
c:\Users\Ryan\git\cassandra3\build.xml:1121: The following error occurred while executing this line:
c:\Users\Ryan\git\cassandra3\build.xml:1064: Using loader AntClassLoader[C:\Program Files\Java\apache-ant-1.9.0\lib\ant-launcher.jar;c:\Program Files\Java\apache-ant-1.9.0\lib\ant.jar;c:\Program Files\Java\apache-ant-1.9.0\lib\ant-junit.jar;c:\Program Files\Java\apache-ant-1.9.0\lib\ant-junit4.jar;c:\Users\Ryan\git\cassandra3\build\classes\main;c:\Users\Ryan\git\cassandra3\build\classes\thrift;c:\Users\Ryan\git\cassandra3\lib\antlr-3.2.jar;c:\Users\Ryan\git\cassandra3\lib\avro-1.4.0-fixes.jar;c:\Users\Ryan\git\cassandra3\lib\avro-1.4.0-sources-fixes.jar;c:\Users\Ryan\git\cassandra3\lib\commons-cli-1.1.jar;c:\Users\Ryan\git\cassandra3\lib\commons-codec-1.2.jar;c:\Users\Ryan\git\cassandra3\lib\commons-lang-2.6.jar;c:\Users\Ryan\git\cassandra3\lib\compress-lzf-0.8.4.jar;c:\Users\Ryan\git\cassandra3\lib\concurrentlinkedhashmap-lru-1.3.jar;c:\Users\Ryan\git\cassandra3\lib\guava-13.0.1.jar;c:\Users\Ryan\git\cassandra3\lib\high-scale-lib-1.1.2.jar;c:\Users\Ryan\git\cassandra3\lib\jackson-core-asl-1.9.2.jar;c:\Users\Ryan\git\cassandra3\lib\jackson-mapper-asl-1.9.2.jar;c:\Users\Ryan\git\cassandra3\lib\jamm-0.2.5.jar;c:\Users\Ryan\git\cassandra3\lib\jbcrypt-0.3m.jar;c:\Users\Ryan\git\cassandra3\lib\jline-1.0.jar;c:\Users\Ryan\git\cassandra3\lib\json-simple-1.1.jar;c:\Users\Ryan\git\cassandra3\lib\libthrift-0.9.0.jar;c:\Users\Ryan\git\cassandra3\lib\log4j-1.2.16.jar;c:\Users\Ryan\git\cassandra3\lib\lz4-1.1.0.jar;c:\Users\Ryan\git\cassandra3\lib\metrics-core-2.0.3.jar;c:\Users\Ryan\git\cassandra3\lib\netty-3.5.9.Final.jar;c:\Users\Ryan\git\cassandra3\lib\servlet-api-2.5-20081211.jar;c:\Users\Ryan\git\cassandra3\lib\slf4j-api-1.7.2.jar;c:\Users\Ryan\git\cassandra3\lib\slf4j-log4j12-1.7.2.jar;c:\Users\Ryan\git\cassandra3\lib\snakeyaml-1.11.jar;c:\Users\Ryan\git\cassandra3\lib\snappy-java-1.0.4.1.jar;c:\Users\Ryan\git\cassandra3\lib\snaptree-0.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\apache-rat-0.6.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\apache-rat-core-0.6.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\apache-rat-tasks-0.6.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\asm-3.2.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\avro-1.3.2.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-beanutils-1.7.0.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-beanutils-core-1.8.0.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-cli-1.2.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-codec-1.4.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-collections-3.2.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-configuration-1.6.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-digester-1.8.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-el-1.0.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-httpclient-3.0.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-lang-2.4.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-logging-1.1.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-math-2.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\commons-net-1.4.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\core-3.1.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\hadoop-core-1.0.3.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\hsqldb-1.8.0.10.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jackson-core-asl-1.0.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jackson-mapper-asl-1.0.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jasper-compiler-5.5.12.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jasper-runtime-5.5.12.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jets3t-0.7.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jetty-6.1.26.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jetty-util-6.1.26.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jna-3.2.7.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jopt-simple-3.2.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jsp-2.1-6.1.14.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\jsp-api-2.1-6.1.14.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\junit-4.6.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\kfs-0.3.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\oro-2.0.8.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\paranamer-2.2.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\paranamer-ant-2.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\paranamer-generator-2.1.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\pig-0.10.0.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\qdox-1.10.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\servlet-api-2.5-20081211.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\servlet-api-2.5-6.1.14.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\slf4j-api-1.5.11.jar;c:\Users\Ryan\git\cassandra3\build\lib\jars\xmlenc-0.52.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\ant-1.6.5-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\apache-rat-core-0.6-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\apache-rat-tasks-0.6-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\avro-1.3.2-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-beanutils-1.7.0-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-cli-1.2-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-codec-1.4-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-collections-3.2-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-configuration-1.6-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-digester-1.8-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-el-1.0-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-httpclient-3.0.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-lang-2.4-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-logging-1.1.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-math-2.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\commons-net-1.4.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jackson-core-asl-1.0.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jackson-mapper-asl-1.0.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jets3t-0.7.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jetty-6.1.26-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jetty-util-6.1.26-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jna-3.2.7-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jopt-simple-3.2-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jsp-2.1-6.1.14-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\jsp-api-2.1-6.1.14-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\junit-4.6-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\oro-2.0.8-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\paranamer-2.2-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\paranamer-ant-2.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\paranamer-generator-2.1-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\pig-0.10.0-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\qdox-1.10-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\servlet-api-2.5-20081211-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\servlet-api-2.5-6.1.14-sources.jar;c:\Users\Ryan\git\cassandra3\build\lib\sources\slf4j-api-1.5.11-sources.jar;c:\Users\Ryan\git\cassandra3\build\test\classes;c:\Users\Ryan\git\cassandra3\test\conf] on class org.apache.tools.ant.taskdefs.optional.junit.XMLJUnitResultFormatter: java.lang.NoClassDefFoundError: junit/framework/TestListener
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:791)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:142)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:449)
	at java.net.URLClassLoader.access$100(URLClassLoader.java:71)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:361)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	at org.apache.tools.ant.AntClassLoader.findBaseClass(AntClassLoader.java:1385)
	at org.apache.tools.ant.AntClassLoader.loadClass(AntClassLoader.java:1064)
	at org.apache.tools.ant.util.SplitClassLoader.loadClass(SplitClassLoader.java:58)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:791)
	at org.apache.tools.ant.AntClassLoader.defineClassFromData(AntClassLoader.java:1128)
	at org.apache.tools.ant.AntClassLoader.getClassFromStream(AntClassLoader.java:1299)
	at org.apache.tools.ant.AntClassLoader.findClassInComponents(AntClassLoader.java:1354)
	at org.apache.tools.ant.AntClassLoader.findClass(AntClassLoader.java:1315)
	at org.apache.tools.ant.util.SplitClassLoader.loadClass(SplitClassLoader.java:52)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:266)
	at org.apache.tools.ant.taskdefs.optional.junit.FormatterElement.createFormatter(FormatterElement.java:286)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.logVmExit(JUnitTask.java:1653)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.logTimeout(JUnitTask.java:1606)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeAsForked(JUnitTask.java:1096)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:851)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.executeOrQueue(JUnitTask.java:1899)
	at org.apache.tools.ant.taskdefs.optional.junit.JUnitTask.execute(JUnitTask.java:800)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.taskdefs.Sequential.execute(Sequential.java:68)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.taskdefs.MacroInstance.execute(MacroInstance.java:396)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	at org.apache.tools.ant.Task.perform(Task.java:348)
	at org.apache.tools.ant.Target.execute(Target.java:435)
	at org.apache.tools.ant.Target.performTasks(Target.java:456)
	at org.apache.tools.ant.Project.executeSortedTargets(Project.java:1393)
	at org.apache.tools.ant.Project.executeTarget(Project.java:1364)
	at org.apache.tools.ant.helper.DefaultExecutor.executeTargets(DefaultExecutor.java:41)
	at org.apache.tools.ant.Project.executeTargets(Project.java:1248)
	at org.apache.tools.ant.Main.runBuild(Main.java:851)
	at org.apache.tools.ant.Main.startAnt(Main.java:235)
	at org.apache.tools.ant.launch.Launcher.run(Launcher.java:280)
	at org.apache.tools.ant.launch.Launcher.main(Launcher.java:109)
Caused by: java.lang.ClassNotFoundException: junit.framework.TestListener
	at java.net.URLClassLoader$1.run(URLClassLoader.java:366)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:355)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:354)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:423)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:356)
	... 62 more
{code}

This isn't a specific unit test failing, this is ant itself crashing. The log specifies that junit-4.6.jar is on the classpath in the build/lib/jars directory and this file exists on disk, and the md5sum is the same as the official jar (37dc57962c1275ebc572726a6f5cdd13), so I cannot understand why the class cannot be found.

Steps to reproduce:
* Use Windows 7. (Reproduced on Linux now too)
* clone C* trunk
* run 'ant clean test'
* See the error in the log.

Interestingly, this is not 100% reproducible. While attempting to debugging this, I deleted my entire checkout, did a fresh 'git clone' and ran 'ant test' and *one time* I ran all the tests to completion, but I cannot reproduce this again, it fails every time I try now with the error above.","Windows 7 or Linux
java 1.7.0_17
ant 1.9.0",antoine@apache.org,enigmacurry,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5383,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,319679,,,Mon May 27 16:35:16 UTC 2013,,,,,,,,,,"0|i1j5pz:",320020,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"05/Apr/13 15:49;jbellis;I wonder if this could be related to pulling a second copy of ant down to build/lib/jars.  Maybe try manually removing build/lib/jars/ant-1.6.5.jar and run ant test -Dwithout.maven ?;;;","09/Apr/13 18:39;marcuse;i have seen this error with ant 1.9.0 on linux as well, downgrading to 1.8.2 fixed it (on linux);;;","12/Apr/13 14:45;enigmacurry;I'm seeing this on linux now too, albeit intermittently.

Unfortunately, I can't get the cobertura code coverage stuff to work at all with -Dwithout.maven set.

I haven't seen this on ant 1.8.2, so that seems to be a solution for me anyway.;;;","06/May/13 08:51;antoine@apache.org;Can you check whether it helps if you replace the 
c:\Users\Ryan\git\cassandra3\build\lib\jars\junit-4.6.jar by a junit-4.11.jar ?

Also, can you build the current ant 1.9.1-alpha from source by checking it out from subversion ? There are instructions on this page : http://ant.apache.org/manual/install.html#buildingant

Alternatively, you can use the file [ant1.9.1-alpha.tgz|http://people.apache.org/~antoine/dist/ant1.9.1-alpha.tgz]  
 
;;;","07/May/13 16:04;enigmacurry;Upgrading to junit-4.11 did not have an effect.

Upgrading to ant-1.9.1-alpha from the tarball you uploaded, worked! I'm able to get all the way through 'ant clean test' repeatably. Thanks.;;;","07/May/13 16:20;antoine@apache.org;Happy about that. This will be an incentive to release ant 1.9.1 fast.;;;","27/May/13 15:39;jbellis;Ant 1.9.1 was released last week.;;;","27/May/13 16:35;enigmacurry;Tested with ant 1.9.1 - issue resolved!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL Not Handling Descending Clustering Order On A timeuuid Correctly,CASSANDRA-5386,12639171,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,gcollins,gcollins,26/Mar/13 19:29,16/Apr/19 09:32,14/Jul/23 05:53,27/Mar/13 15:18,1.2.4,,,,,,0,,,,,"I raised this issue as a question in the mailing list:

http://www.mail-archive.com/user@cassandra.apache.org/msg28787.html

If I create a table (cqlsh) with the following schema:

CREATE TABLE mytable ( column1 text,
      column2 text,
      messageId timeuuid,
      message blob,
      PRIMARY KEY ((column1, column2), messageId));

I can quite happily add rows to this table:

insert into client_queue (column1,column2,messageId,message) VALUES
('string1','string2',now(),'ABCCDCC123');

If I however create a table with a desc clustering order on messageid:

CREATE TABLE mytable ( column1 text,
      column2 text,
      messageId timeuuid,
      message blob,
      PRIMARY KEY ((column1, column2), messageId)) WITH CLUSTERING
ORDER BY (messageId DESC);

Inserts are failing. I am getting the following error:

insert into client_queue2 (column1,column2,messageId,message) VALUES
('string1','string2',now(),'ABCCDCC123');

I get the following error:

Bad Request: Type error: cannot assign result of function now (type
timeuuid) to messageid (type
'org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.TimeUUIDType)')

","Apache Cassandra 1.2.3, Mac OS X (Lion), cql 3",gcollins,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Mar/13 10:27;slebresne;5386.txt;https://issues.apache.org/jira/secure/attachment/12575686/5386.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,319641,,,Wed Mar 27 15:18:57 UTC 2013,,,,,,,,,,"0|i1j5hj:",319982,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"27/Mar/13 10:27;slebresne;Patch attached to fix (to ignore ReversedType when we do a type comparison).;;;","27/Mar/13 12:49;jbellis;+1;;;","27/Mar/13 15:18;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IndexHelper.skipBloomFilters won't skip non-SHA filters,CASSANDRA-5385,12639161,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,carlyeks,carlyeks,carlyeks,26/Mar/13 19:03,16/Apr/19 09:32,14/Jul/23 05:53,03/Apr/13 23:17,1.2.4,2.0 beta 1,,,,,0,,,,,"Currently, if the bloom filter is not of SHA type, we do not properly skip the bytes. We need to read out the number of bytes, as happens in the Murmur deserializer, then skip that many bytes instead of just skipping the hash size. The version needs to be passed into the method as well, so that it knows what type of index it is, and does the appropriate skipping.",,arya,carlyeks,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4885,,,,,,,,,,,,,,,,,,,,,,,,"03/Apr/13 04:19;carlyeks;5385-v2.patch;https://issues.apache.org/jira/secure/attachment/12576713/5385-v2.patch","26/Mar/13 19:04;carlyeks;5385.patch;https://issues.apache.org/jira/secure/attachment/12575563/5385.patch",,,,,,,,,,,,,,,,,,,2.0,carlyeks,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,319631,,,Sun Apr 21 22:50:06 UTC 2013,,,,,,,,,,"0|i1j5fb:",319972,,,,,,,,,jasobrown,,jasobrown,Normal,,,,,,,,,,,,,,,,,,"26/Mar/13 19:16;jasobrown;LGTM. Nice catch, will commit in a few minutes. Thanks!

EDIT: actually, looking into a side question that carl raised about OOM problem.;;;","27/Mar/13 01:40;jasobrown;After talking on the IRC with [~carlyeks], this does affect 1.2 (and starts there) so resetting the affects/fix versions;;;","02/Apr/13 18:51;jasobrown;There is a problem with ScrubTest, and we decided, on the IRC, to can it for now. It will be rewritten as part of #4180.

Committed to cassandra-1.2 and trunk. Thanks, Carl!;;;","03/Apr/13 04:19;carlyeks;The problem with the OOM is actually something that shows what is wrong (it's not a symptom). We can add back the scrub test, as it should pass now.

Before sstable version ia, the size of the bloom filter was written in the column header, along with the bloom filter. In 1.2, the Murmur3 partitioner was added, which dropped the size of the bloom filter as the first parameter. If we tried to read a bloom filter that was encoded in a pre-ia table, we would fail, skip it (using index helper), and it would skip the correct number of bytes. Now, we need to distinguish between the bloom filter stored in the sstable and the one stored in the filter component.

This patch fixes the LegacySSTableTest by distinguishing between the sstables written in the ia format (which didn't write the size of the bloom filter up front) and the pre-ia format.;;;","03/Apr/13 13:39;jasobrown;Committed to trunk, and ScrubTest and LegacySSTableTest now pass. [~carlyeks], does this need to be applied to 1.2, as well? Seems like it should, but the ScrubTest and LegacySSTableTest pass on 1.2 without this.;;;","03/Apr/13 23:16;jasobrown;Backported and committed to cassandra-1.2. Thanks for the effort, Carl!;;;","19/Apr/13 15:34;jbellis;Hmm, looks like the backport blew away scrubtest on 1.2 as well.  Oversight?;;;","19/Apr/13 17:41;jbellis;I'm confused by this whole ticket.  Which of these is incorrect?

# skipBloomFilters is only called on data in the Data componenet
# skipBloomFilters is only called against a 1.1 sstable
# 1.1 sstables only use Murmur2 Bloom filters

(Because if all of these are correct, then we shouldn't need the changes to IndexHelper introduced here.);;;","21/Apr/13 18:48;carlyeks;{quote}

Hmm, looks like the backport blew away scrubtest on 1.2 as well. Oversight?

{quote}
Yes, was an oversight.

Just for clarification, skipBloomFilters is old.

#1 and #2 are incorrect.

1: The skipBloomFilters gets called on the index file in RowIndexEntry line 104.
2: In 2.0, the skipBloomFilters will be called on a 1.2 sstable; also, the skipBloomFilters currently is called against any sstable when scrubbing.;;;","21/Apr/13 22:50;jbellis;Ah, I see.  So, my assertions are correct for 1.2, which is what I've been working in lately.  I'll make sure it stays that way as I merge CASSANDRA-5497 and CASSANDRA-5492 forward.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Windows 7 deleting/renaming files problem,CASSANDRA-5383,12638924,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,enigmacurry,enigmacurry,25/Mar/13 17:36,16/Apr/19 09:32,14/Jul/23 05:53,04/Oct/13 07:45,2.0.2,,,Legacy/Testing,,,0,qa-resolved,,,,"Two unit tests are failing on Windows 7 due to errors in renaming/deleting files:


org.apache.cassandra.db.ColumnFamilyStoreTest: 
{code}
    [junit] Testsuite: org.apache.cassandra.db.ColumnFamilyStoreTest
    [junit] Tests run: 27, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 13.904 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] ERROR 13:06:46,058 Unable to delete build\test\cassandra\data\Keyspace1\Indexed2\Keyspace1-Indexed2.birthdate_index-ja-1-Data.db (it will be removed on server restart; we'll also retry after GC)
    [junit] ERROR 13:06:48,508 Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
    [junit] java.lang.RuntimeException: Tried to hard link to file that does not exist build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-7-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.createHardLink(FileUtils.java:72)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:1057)
    [junit] 	at org.apache.cassandra.db.DataTracker$1.run(DataTracker.java:168)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:98)
    [junit] 	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:206)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testSliceByNamesCommandOldMetatada(org.apache.cassandra.db.ColumnFamilyStoreTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-6-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-6-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-6-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-6-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStoreTest.testSliceByNamesCommandOldMetatada(ColumnFamilyStoreTest.java:885)
    [junit] 
    [junit] 
    [junit] Testcase: testRemoveUnifinishedCompactionLeftovers(org.apache.cassandra.db.ColumnFamilyStoreTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra\build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ja-2-Data.db
    [junit] FSWriteError in build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ja-2-Data.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:103)
    [junit] 	at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:139)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:507)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStoreTest.testRemoveUnifinishedCompactionLeftovers(ColumnFamilyStoreTest.java:1246)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra\build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ja-2-Data.db
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.ColumnFamilyStoreTest FAILED
{code}


org.apache.cassandra.db.ScrubTest:
{code}
    [junit] Testcase: testScrubFile(org.apache.cassandra.db.ScrubTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Super5\Keyspace1-Super5-f-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Super5\Keyspace1-Super5-f-2-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Super5\Keyspace1-Super5-f-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Super5\Keyspace1-Super5-f-2-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.db.ScrubTest.testScrubFile(ScrubTest.java:94)
    [junit] 
    [junit] 
    [junit] Testcase: testScubOutOfOrder(org.apache.cassandra.db.ScrubTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ia-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ia-1-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ia-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard3\Keyspace1-Standard3-ia-1-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.db.ScrubTest.testScubOutOfOrder(ScrubTest.java:201)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.ScrubTest FAILED
{code}

Reproduced in a Windows 7 VM:
java 1.6.0_43-b01
ant 1.9.0
C* trunk
run 'ant clean test'",,Andie78,cmessaoud,enigmacurry,marcuse,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5388,,,,,,,,,,,,,,,,,CASSANDRA-6283,CASSANDRA-3613,,,,,,,,,,"05/Jun/13 08:45;marcuse;0001-CASSANDRA-5383-cant-move-a-file-on-top-of-another-fi.patch;https://issues.apache.org/jira/secure/attachment/12586278/0001-CASSANDRA-5383-cant-move-a-file-on-top-of-another-fi.patch","28/May/13 18:47;marcuse;0001-CASSANDRA-5383-v2.patch;https://issues.apache.org/jira/secure/attachment/12585061/0001-CASSANDRA-5383-v2.patch","27/Mar/13 10:18;marcuse;0001-use-Java7-apis-for-deleting-and-moving-files-and-cre.patch;https://issues.apache.org/jira/secure/attachment/12575685/0001-use-Java7-apis-for-deleting-and-moving-files-and-cre.patch","03/Oct/13 12:19;marcuse;5383-v3.patch;https://issues.apache.org/jira/secure/attachment/12606575/5383-v3.patch","28/May/13 19:40;enigmacurry;5383_patch_v2_system.log;https://issues.apache.org/jira/secure/attachment/12585066/5383_patch_v2_system.log","06/Jun/13 21:46;enigmacurry;cant_move_file_patch.log;https://issues.apache.org/jira/secure/attachment/12586595/cant_move_file_patch.log","28/May/13 19:24;enigmacurry;test_log.5383.patch_v2.log.txt;https://issues.apache.org/jira/secure/attachment/12585065/test_log.5383.patch_v2.log.txt","06/Jun/13 21:00;enigmacurry;v2+cant_move_file_patch.log;https://issues.apache.org/jira/secure/attachment/12586585/v2%2Bcant_move_file_patch.log",,,,,,,,,,,,,8.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,319394,,,Wed Jun 17 08:55:07 UTC 2015,,,,,,,,,,"0|i1j3yf:",319735,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,enigmacurry,,,"25/Mar/13 17:41;jbellis;The mutateLevel problem looks like what I predicted -- Windows won't let you rename over an existing file.;;;","25/Mar/13 17:41;jbellis;(Not sure about CFST...  maybe we should take advantage of Java 7 and switch to using the Path api, which will give us better error messages.);;;","25/Mar/13 21:01;enigmacurry;I had a problem that prevented me from running the full test suite, I've resolved that and found several more errors that look similar to my eyes:

{code}
    [junit] Testcase: org.apache.cassandra.config.CFMetaDataTest:	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\data
    [junit] FSWriteError in build\test\cassandra\data
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:399)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.SchemaLoader.loadSchema(SchemaLoader.java:65)
    [junit] 	at org.apache.cassandra.SchemaLoader.loadSchema(SchemaLoader.java:59)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\data
{code}

{code}
    [junit] Testcase: org.apache.cassandra.db.RemoveSubColumnTest:	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\data\Keyspace4
    [junit] FSWriteError in build\test\cassandra\data\Keyspace4
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:399)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.SchemaLoader.loadSchema(SchemaLoader.java:65)
    [junit] 	at org.apache.cassandra.SchemaLoader.loadSchema(SchemaLoader.java:59)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\data\Keyspace4
    [junit] 
    [junit] 
    [junit] Testcase: org.apache.cassandra.db.RemoveSubColumnTest:	Caused an ERROR
    [junit] null
    [junit] java.lang.NullPointerException
    [junit] 	at org.apache.cassandra.gms.Gossiper.stop(Gossiper.java:1084)
    [junit] 	at org.apache.cassandra.SchemaLoader.stopGossiper(SchemaLoader.java:99)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.RemoveSubColumnTest FAILED
{code}

{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LegacyLeveledManifestTest
    [junit] Tests run: 3, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 2.698 sec
    [junit] 
    [junit] Testcase: doMigrationTest(org.apache.cassandra.db.compaction.LegacyLeveledManifestTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.compaction.LegacyLeveledManifest.migrateManifests(LegacyLeveledManifest.java:102)
    [junit] 	at org.apache.cassandra.db.compaction.LegacyLeveledManifestTest.doMigrationTest(LegacyLeveledManifestTest.java:50)
    [junit] 
    [junit] 
    [junit] Testcase: validateSSTableMetadataTest(org.apache.cassandra.db.compaction.LegacyLeveledManifestTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\legacyleveled\Keyspace1-legacyleveled-hf-2-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.compaction.LegacyLeveledManifest.migrateManifests(LegacyLeveledManifest.java:102)
    [junit] 	at org.apache.cassandra.db.compaction.LegacyLeveledManifestTest.validateSSTableMetadataTest(LegacyLeveledManifestTest.java:74)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LegacyLeveledManifestTest FAILED
{code}

{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest
    [junit] Tests run: 3, Failures: 1, Errors: 1, Skipped: 0, Time elapsed: 20.217 sec
    [junit] 
    [junit] Testcase: testValidationMultipleSSTablePerLevel(org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest.testValidationMultipleSSTablePerLevel(LeveledCompactionStrategyTest.java:89)
    [junit] 
    [junit] 
    [junit] Testcase: testMutateLevel(org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\StandardLeveled\Keyspace1-StandardLeveled-ja-60-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\StandardLeveled\Keyspace1-StandardLeveled-ja-60-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\StandardLeveled\Keyspace1-StandardLeveled-ja-60-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\StandardLeveled\Keyspace1-StandardLeveled-ja-60-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest.testMutateLevel(LeveledCompactionStrategyTest.java:180)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.LeveledCompactionStrategyTest FAILED
{code}

{code}
    [junit] Testsuite: org.apache.cassandra.dht.OrderPreservingPartitionerTest
    [junit] Tests run: 7, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 1.716 sec
    [junit] 
    [junit] Testcase: testMidpoint(org.apache.cassandra.dht.OrderPreservingPartitionerTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] FSWriteError in build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:388)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.dht.OrderPreservingPartitionerTest.initPartitioner(OrderPreservingPartitionerTest.java:31)
    [junit] 	at org.apache.cassandra.dht.PartitionerTestCase.clean(PartitionerTestCase.java:39)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 
    [junit] 
    [junit] Testcase: testMidpointMinimum(org.apache.cassandra.dht.OrderPreservingPartitionerTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] FSWriteError in build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:388)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.dht.OrderPreservingPartitionerTest.initPartitioner(OrderPreservingPartitionerTest.java:31)
    [junit] 	at org.apache.cassandra.dht.PartitionerTestCase.clean(PartitionerTestCase.java:39)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 
    [junit] 
    [junit] Testcase: testMidpointWrapping(org.apache.cassandra.dht.OrderPreservingPartitionerTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] FSWriteError in build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:388)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.dht.OrderPreservingPartitionerTest.initPartitioner(OrderPreservingPartitionerTest.java:31)
    [junit] 	at org.apache.cassandra.dht.PartitionerTestCase.clean(PartitionerTestCase.java:39)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 
    [junit] 
    [junit] Testcase: testTokenFactoryBytes(org.apache.cassandra.dht.OrderPreservingPartitionerTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] FSWriteError in build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:388)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.dht.OrderPreservingPartitionerTest.initPartitioner(OrderPreservingPartitionerTest.java:31)
    [junit] 	at org.apache.cassandra.dht.PartitionerTestCase.clean(PartitionerTestCase.java:39)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 
    [junit] 
    [junit] Testcase: testTokenFactoryStrings(org.apache.cassandra.dht.OrderPreservingPartitionerTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] FSWriteError in build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:388)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.dht.OrderPreservingPartitionerTest.initPartitioner(OrderPreservingPartitionerTest.java:31)
    [junit] 	at org.apache.cassandra.dht.PartitionerTestCase.clean(PartitionerTestCase.java:39)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 
    [junit] 
    [junit] Testcase: testDescribeOwnership(org.apache.cassandra.dht.OrderPreservingPartitionerTest):	Caused an ERROR
    [junit] java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] FSWriteError in build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:112)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:340)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:336)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanup(SchemaLoader.java:388)
    [junit] 	at org.apache.cassandra.SchemaLoader.cleanupAndLeaveDirs(SchemaLoader.java:374)
    [junit] 	at org.apache.cassandra.dht.OrderPreservingPartitionerTest.initPartitioner(OrderPreservingPartitionerTest.java:31)
    [junit] 	at org.apache.cassandra.dht.PartitionerTestCase.clean(PartitionerTestCase.java:39)
    [junit] Caused by: java.io.IOException: Failed to delete c:\Users\Ryan\git\cassandra2\build\test\cassandra\commitlog\CommitLog-3-1364244476201.log
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.dht.OrderPreservingPartitionerTest FAILED
{code}

{code}
    [junit] Testsuite: org.apache.cassandra.io.sstable.SSTableReaderTest
    [junit] Tests run: 8, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 11.607 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit]  WARN 16:49:23,669 setting live ratio to maximum of 64.0 instead of 97.26984126984127
    [junit]  WARN 16:49:24,542 setting live ratio to maximum of 64.0 instead of 79.42857142857143
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testPersistentStatistics(org.apache.cassandra.io.sstable.SSTableReaderTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-2-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-2-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ja-2-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.clearAndLoad(SSTableReaderTest.java:170)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.testPersistentStatistics(SSTableReaderTest.java:163)
    [junit] 
    [junit] 
    [junit] Testcase: testPersistentStatisticsWithSecondaryIndex(org.apache.cassandra.io.sstable.SSTableReaderTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.clearAndLoad(SSTableReaderTest.java:170)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.assertIndexQueryWorks(SSTableReaderTest.java:324)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.testPersistentStatisticsWithSecondaryIndex(SSTableReaderTest.java:220)
    [junit] 
    [junit] 
    [junit] Testcase: testPersistentStatisticsFromOlderIndexedSSTable(org.apache.cassandra.io.sstable.SSTableReaderTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\Indexed1\Keyspace1-Indexed1.626972746864617465-ja-1-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.clearAndLoad(SSTableReaderTest.java:170)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.assertIndexQueryWorks(SSTableReaderTest.java:324)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableReaderTest.testPersistentStatisticsFromOlderIndexedSSTable(SSTableReaderTest.java:248)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.io.sstable.SSTableReaderTest FAILED
{code}

{code}
    [junit] Testsuite: org.apache.cassandra.io.sstable.SSTableSimpleWriterTest
    [junit] Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 8.72 sec
    [junit] 
    [junit] Testcase: testSSTableSimpleUnsortedWriter(org.apache.cassandra.io.sstable.SSTableSimpleWriterTest):	Caused an ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\StandardInteger1\Keyspace1-StandardInteger1-ja-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\StandardInteger1\Keyspace1-StandardInteger1-ja-1-Statistics.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\StandardInteger1\Keyspace1-StandardInteger1-ja-1-Statistics.db-tmp to build\test\cassandra\data\Keyspace1\StandardInteger1\Keyspace1-StandardInteger1-ja-1-Statistics.db
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:133)
    [junit] 	at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:122)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledManifest.mutateLevel(LeveledManifest.java:575)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:589)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableSimpleWriterTest.testSSTableSimpleUnsortedWriter(SSTableSimpleWriterTest.java:89)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.io.sstable.SSTableSimpleWriterTest FAILED
{code}

;;;","26/Mar/13 22:13;enigmacurry;I cannot currently run a full 'ant test' on Windows due to a bug in ant/junit on windows #5388;;;","27/Mar/13 10:18;marcuse;not yet tested on windows due to CASSANDRA-5388

;;;","28/May/13 08:59;marcuse;[~enigmacurry] - could you test this now that CASSANDRA-5388 is fixed?;;;","28/May/13 16:33;enigmacurry;[~krummas] I can't get this patch to apply on the current trunk (It did back in March when I first tried this.)

It does apply to cassandra-1.2, but it doesn't work there:

{code}
$ ant clean test -Dtest.name=ColumnFamilyStoreTest
[...]

    [junit] Testsuite: org.apache.cassandra.db.ColumnFamilyStoreTest
    [junit] Tests run: 27, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 9.162 sec
    [junit]
    [junit] Testcase: testSliceByNamesCommandOldMetatada(org.apache.cassandra.db.ColumnFamilyStoreTest):        Caused a
n ERROR
    [junit] Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ic-8-Index.db to build\te
st\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ic-9-Index.db
    [junit] java.lang.RuntimeException: Failed to rename build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standar
d1-ic-8-Index.db to build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ic-9-Index.db
    [junit]     at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:155)
    [junit]     at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:139)
    [junit]     at org.apache.cassandra.io.sstable.SSTableWriter.rename(SSTableWriter.java:409)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.loadNewSSTables(ColumnFamilyStore.java:504)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStoreTest.testSliceByNamesCommandOldMetatada(ColumnFamilyStoreTes
t.java:925)
    [junit] Caused by: java.nio.file.FileSystemException: build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standa
rd1-ic-8-Index.db -> build\test\cassandra\data\Keyspace1\Standard1\Keyspace1-Standard1-ic-9-Index.db: The process cannot
 access the file because it is being used by another process.
    [junit]
    [junit]     at sun.nio.fs.WindowsException.translateToIOException(WindowsException.java:86)
    [junit]     at sun.nio.fs.WindowsException.rethrowAsIOException(WindowsException.java:97)
    [junit]     at sun.nio.fs.WindowsFileCopy.move(WindowsFileCopy.java:301)
    [junit]     at sun.nio.fs.WindowsFileSystemProvider.move(WindowsFileSystemProvider.java:286)
    [junit]     at java.nio.file.Files.move(Files.java:1345)
    [junit]     at org.apache.cassandra.io.util.FileUtils.atomicMoveWithFallback(FileUtils.java:169)
    [junit]     at org.apache.cassandra.io.util.FileUtils.renameWithConfirm(FileUtils.java:151)
    [junit]
    [junit]
    [junit] Test org.apache.cassandra.db.ColumnFamilyStoreTest FAILED

{code};;;","28/May/13 18:47;marcuse;rebased patch, probably wont work either though

the error is quite strange - had a quick look and the files should all be closed before renaming...;;;","28/May/13 19:24;enigmacurry;Uploaded test_log.5383.patch_v2.log.txt

This log is the entire test suite run against trunk with the v2 patch. Windows 7, java 1.7.0_17 64bit;;;","03/Jun/13 08:32;marcuse;im stumped (and cant really debug since i dont own a windows machine)

anyone with windows skills want to take a look?;;;","05/Jun/13 08:45;marcuse;hacks around the problem by deleting the Statistics file on windows before moving

could in cases end up without a metadata file on windows;;;","06/Jun/13 21:00;enigmacurry;[~krummas] Is the new patch (0001-CASSANDRA-5383-cant-move-a-file-on-top-of-another-fi.patch) meant to be applied on top of the v2 patch? 

I ran it that way and attached v2+cant-move-file-patch.log. I'm still seeing numberous FSWriteErrors.;;;","06/Jun/13 21:16;marcuse;[~enigmacurry] no the last patch should be applied alone, it is just a hack around the problem, not being able to replace files in windows;;;","06/Jun/13 21:46;enigmacurry;OK, I reran it with just the 0001-CASSANDRA-5383-cant-move-a-file-on-top-of-another-fi.patch applied to trunk. Attached cant_move_file_patch.log. I'm still seeing FSWriteErrors.;;;","17/Jun/13 06:57;marcuse;those errors are not related to the mutateLevel issue anymore - mutatelevel only touches the *-Statistics.db files (and those are not mentioned in the logs with the latest patch it seems)

[~jbellis] got any idea what these are? seems we are not closing files before trying to rename them?;;;","25/Sep/13 08:16;marcuse;[~jbellis] wdyt?;;;","25/Sep/13 14:22;jbellis;bq. seems we are not closing files before trying to rename them?

That's what it looks like, but damned if I know why.

We should at least get the java7 refactoring committed though...;;;","03/Oct/13 12:19;marcuse;rebased against trunk, both starts using the java7 apis and removes a file before moving another file on top of it in windows;;;","03/Oct/13 15:44;jbellis;+1;;;","04/Oct/13 07:45;marcuse;commited!;;;","28/Oct/13 17:59;Andie78;Release 2.0.2 still not working under Win7 32-bit with Java build 1.7.0_45-b18. system.log:
""SSTableDeletingTask.java (line 81) Unable to delete D:\Programme\cassandra\data\nieste\nfiles\nieste-nfiles-jb-2387-Data.db (it will be removed on server restart; we'll also retry after GC"". After heavy data changing (update data every 3 - 10 sec) up to 24.000 files in nfiles (data) folder and cassandra crashes. I tried LZ4Compressor, SnappyCompressor, DeflateCompressor and No Compression. I also tried Snapshot v2.1 last week without success. Is there a way to reduce frequency of writing sstables (new files) until issue is fixed? Rarely changed CFs are not affected.;;;","14/May/14 16:01;cmessaoud;Hi, i am having the delete file problem on Windows 7, i downloaded your 5383-v3.patch fix, but don't know how to apply it onto my Cassandra platform. Anyone can shed some light on how to do that please ? Thanks;;;","17/Jun/15 08:55;Andie78;I put it manually into the source:
/src/org/apache/cassandra/io/util/RandomAccessReader.java
Put (overwrite) the finalizer there.

To avoid unnecessary logs after patching u can un-comment the warning (logger.error) in /sstable/SSTableDeletingTask.java

To build u need maven (I use 3.0.5) and ant (1.9.2) and an internet-connection. I have to use ant on a dedicated server in front of the proxy since I couldn't get proxy-settings working in ant.
After the modifications u can type ""ant jar"" if u just need the patched jar(s). ""ant release"" builds u the compressed bin-package.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ConcurrentModificantionException on server when multiple CQL3 read requests received on single column family simultaneously.,CASSANDRA-5382,12638905,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,rettet181,rettet181,25/Mar/13 16:24,16/Apr/19 09:32,14/Jul/23 05:53,27/Mar/13 10:42,1.2.4,,,,,,0,,,,,"The exception below is thrown on the server when two reads are performed at the exact same time on the same column family. This causes the query to fail. 

The problem appears to be caused by the 'name' list in org.apache.cassandra.cql3.ResultSet$Metadata. The reference is passed in to the constructor and iterated without copying and without a synch block. When two of these ResultSet instances are created from the same metadata list at the same time, a ConcurrentModificationException is thrown.

The error:

ERROR [Thrift:860] 2013-03-25 09:27:39,467 CustomTThreadPoolServer.java (line 217) Error occurred during processing of message.
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.cql3.ResultSet$Metadata.allInSameCF(ResultSet.java:237)
        at org.apache.cassandra.cql3.ResultSet$Metadata.<init>(ResultSet.java:219)
        at org.apache.cassandra.cql3.ResultSet.<init>(ResultSet.java:47)
        at org.apache.cassandra.cql3.statements.Selection$ResultSetBuilder.<init>(Selection.java:239)
        at org.apache.cassandra.cql3.statements.Selection$ResultSetBuilder.<init>(Selection.java:221)
        at org.apache.cassandra.cql3.statements.Selection.resultSetBuilder(Selection.java:211)
        at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:655)
        at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:147)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:136)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:132)
        at org.apache.cassandra.cql3.QueryProcessor.processPrepared(QueryProcessor.java:254)
        at org.apache.cassandra.thrift.CassandraServer.execute_prepared_cql3_query(CassandraServer.java:1851)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query.getResult(Cassandra.java:4166)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_prepared_cql3_query.getResult(Cassandra.java:4154)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
        at java.lang.Thread.run(Thread.java:662)
","SLES, Sun JDK 1.6.0_43",rettet181,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Mar/13 11:44;slebresne;0001-Copy-ResultSet-in-makeCountResult.txt;https://issues.apache.org/jira/secure/attachment/12575508/0001-Copy-ResultSet-in-makeCountResult.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,319375,,,Wed Mar 27 10:42:56 UTC 2013,,,,,,,,,,"0|i1j3u7:",319716,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"26/Mar/13 11:16;slebresne;Have you identified which queries have triggered this exactly? Is it possible some ""SELECT count(*) ..."" is involved?;;;","26/Mar/13 11:44;slebresne;From a quick check the only place I see the metadata list being modified after Metadata creation is in makeCountResult. Even if I've missed another spot, not copying in makeCountResult was bad so attaching a simple patch to fix that.;;;","26/Mar/13 15:26;rettet181;Thanks for the patch, yes the query does involve a count, specifically:

""SELECT count(*) FROM node WHERE state_identifier = 1"";;;","26/Mar/13 15:37;jbellis;+1;;;","27/Mar/13 10:42;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra startup fails when reported Java version contains alphanumeric characters,CASSANDRA-5380,12638846,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,dynamind,dynamind,25/Mar/13 10:31,16/Apr/19 09:32,14/Jul/23 05:53,10/Aug/13 21:32,,,,,,,0,,,,,"There is a new Java version check in the CassandraDaemon.java setup method that triggers a NumberFormatException when parsing a Java version containing alphanumeric characters such as ""1.7.0_12-ea""","OSX 10.8.3, Java version ""1.7.0_12-ea""",dynamind,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,319320,,,Mon Mar 25 12:50:29 UTC 2013,,,,,,,,,,"0|i1j3i7:",319661,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"25/Mar/13 12:50;jbellis;Added a catch in 4fa3418c0bb8008ab2d2f1e2b4a2044596a47a4a; should fix it better to be able to actually parse that.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fat Client: No longer works in 1.2,CASSANDRA-5378,12638545,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,carlyeks,carlyeks,carlyeks,22/Mar/13 17:28,16/Apr/19 09:32,14/Jul/23 05:53,07/Aug/13 17:31,1.2.9,,,,,,0,client,,,,"The current client only example doesn't compile. After doing some updates, the fat client still won't work, mainly because the schema is not being pushed to the fat client.

I've made changes to the client to support CQL3 commands, to the ServiceManager to wait until a migration has completed before starting the client, and to the MigrationManager to not try to pull schemas from a fat client.",,carlyeks,slebresne,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Mar/13 18:50;carlyeks;5378-1.2.txt;https://issues.apache.org/jira/secure/attachment/12575068/5378-1.2.txt","15/Jun/13 01:52;carlyeks;5378-schema-writing.patch;https://issues.apache.org/jira/secure/attachment/12587943/5378-schema-writing.patch","29/Mar/13 15:50;carlyeks;5378-v2.txt;https://issues.apache.org/jira/secure/attachment/12576086/5378-v2.txt","22/Mar/13 17:29;carlyeks;5378.txt;https://issues.apache.org/jira/secure/attachment/12575045/5378.txt",,,,,,,,,,,,,,,,,4.0,carlyeks,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,319021,,,Wed Aug 07 17:31:18 UTC 2013,,,,,,,,,,"0|i1j1nr:",319362,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"22/Mar/13 18:46;carlyeks;Was actually developing against trunk for this patch, so the fix version should be 2.0.

I'm fixing it for 1.2.4 as well, but this patch doesn't apply cleanly.;;;","22/Mar/13 18:50;carlyeks;This patch is applied against the 1.2 branch - the unclean file wasn't needed.;;;","29/Mar/13 13:54;brandon.williams;Committed, thanks!;;;","29/Mar/13 14:15;brandon.williams;http://buildbot.datastax.com:8020/builders/cassandra-1.2/builds/179/steps/shell/logs/stdio StorageServiceClientTest is failing, can you take a look?;;;","29/Mar/13 15:50;carlyeks;This fixes the test.;;;","29/Mar/13 17:04;brandon.williams;Committed as well, thanks.;;;","05/Apr/13 13:04;slebresne;This broke a dtests, namely cql_tests.py:TestCQL.bug_5240_test.

The reason is that the patch modifies SelectStatement.java and call CFMetadata.getKeyName(), but that method should not be called from CQL3 ever. I'm not sure I understand why fixing the fat client would involve making any kind of change in SelectStatement tbh.;;;","05/Apr/13 17:57;carlyeks;The change to select statement is because the fat client can't use the CFS as it doesn't have the data. So, the only things that we have instead is the metadata. Since CQL3 is the way that most clients will want to interact with the service, I thought it made sense to try to update those statements to not use the CFS; any direction on how to fix this?;;;","05/Apr/13 18:36;slebresne;Well looking at that code, what was the rational of adding the offending line was added in the first place:
{noformat}
+                indexedNames.add(cfm.getKeyName());
{noformat}
Seems to me it could just be removed.;;;","05/Apr/13 18:51;carlyeks;If we don't have that, we won't be able to validate a query against the row key.

That is, if the schema is:
{code}
CREATE COLUMNFAMILY standard1 ( id ascii PRIMARY KEY , name ascii , value blob ) ;
SELECT * FROM standard1 WHERE id='abc';
{code}

That select statement will not be validated, as no column is indexed, but the row key is. It'll throw the ""No indexed columns present in by-columns clause with Equal operator"" exception.;;;","08/Apr/13 09:40;slebresne;I'm not sure what you are talking about. The example above have always worked and does not require the row key to be added to the indexedNames (which is an addition of this patch). The row key is ""primarily"" indexed but it's not indexed by a secondary index. And adding a secondary index on the row key is not something supported (largely because it would be useless).

So anyway, I have removed that line in commit 69f05a704aafa90f2151db721312f3c5907abb2f if only because that restore the behavior of before that patch and let all cql dtests pass. If you still think there is a problem, please open a separate issue with a test case showing the problem you are talking about. ;;;","01/May/13 20:34;tjake;The fix for the test breaks the fat client 5378-v2.txt

It can no longer get the schema from the non-fat clients and instead throws:

{code}
java.lang.NullPointerException
	at org.apache.cassandra.service.MigrationManager.maybeScheduleSchemaPull(MigrationManager.java:123)
	at org.apache.cassandra.service.MigrationManager.onAlive(MigrationManager.java:98)
	at org.apache.cassandra.gms.Gossiper.markAlive(Gossiper.java:773)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:816)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:901)
	at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:50)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

{code}

Can we put the change back in or somehow fix this NPE?;;;","13/May/13 10:40;carlyeks;The point of the second patch was to prevent us from using the file system. This means that the migration manager probably needs to be able to operate exclusively in memory.

I'll take a look at this issue later today.;;;","15/Jun/13 01:52;carlyeks;I'm attaching a fix that adds back the Schema.instance.updateVersion(), but prevents writing out to disk by that call.

This fixes the test, and the startup of the fat client.;;;","02/Aug/13 03:25;jbellis;Marking Patch Available.;;;","07/Aug/13 17:31;brandon.williams;Looks like I ninja'd this exact patch in right after committing the original, to fix the test.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MemoryMeter miscalculating memtable live ratio,CASSANDRA-5377,12638529,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,abashir,abashir,22/Mar/13 16:02,16/Apr/19 09:32,14/Jul/23 05:53,19/Nov/13 22:32,1.2.12,,,,,,0,,,,,"I've noticed the following logs in our running cluster:
WARN [MemoryMeter:1] 2013-03-17 23:15:55,876 Memtable.java (line 197) setting live ratio to minimum of 1.0 instead of 0.6378445488771007

It seems odd for the deep size calculation to be smaller than the aggregate sum of serialized columns.  Perhaps it's because we're mutating on a bunch of existing columns, or perhaps there's some miscalculation somewhere

Our column families all have regular columns (no super columns, expiring columns, etc)",,abashir,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,319005,,,Tue Nov 19 22:32:05 UTC 2013,,,,,,,,,,"0|i1j1k7:",319346,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"19/Nov/13 22:32;jbellis;This can happen when the cell-name interning (CASSANDRA-1255) is particularly effective.  Changed the logging to {{debug}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CQL3: IN clause on last key not working when schema includes set,list or map",CASSANDRA-5376,12638518,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,voodooless,voodooless,22/Mar/13 14:48,16/Apr/19 09:32,14/Jul/23 05:53,27/Mar/13 10:43,1.2.4,,,,,,0,,,,,"This is an exception on the fix of https://issues.apache.org/jira/browse/CASSANDRA-5230

Looks like any schema using map,list or set won't work with IN clauses on the last key (in this example c)

Schema:
{code}
CREATE TABLE foo2 (
  key text,
  c bigint,
  v text,
  x set<text>,
  PRIMARY KEY (key, c)
);
{code}

Query:
{code}select * from foo2 where key = 'foo' and c in (1,3,4) ;{code}

This will lead to an assertion error on the nodes:

{code}java.lang.AssertionError
        at org.apache.cassandra.cql3.statements.SelectStatement.buildBound(SelectStatement.java:540)
        at org.apache.cassandra.cql3.statements.SelectStatement.getRequestedBound(SelectStatement.java:568)
        at org.apache.cassandra.cql3.statements.SelectStatement.makeFilter(SelectStatement.java:308)
        at org.apache.cassandra.cql3.statements.SelectStatement.getSliceCommands(SelectStatement.java:219)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:132)
        at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62)
        at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:132)
        at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:143)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1726)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
        at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
        at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)

{code}",,aleksey,liqusha,sh123,slebresne,voodooless,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5375,,,,,CASSANDRA-5230,,,,,,,,,,,,,,,"25/Mar/13 12:39;slebresne;5376.txt;https://issues.apache.org/jira/secure/attachment/12575310/5376.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,318994,,,Mon Jun 03 16:07:38 UTC 2013,,,,,,,,,,"0|i1j1hr:",319335,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"25/Mar/13 12:39;slebresne;Unfortunately, handling collections is slightly harder than what CASSANDRA-5230 aimed for, because we can't do a name query. So this will have to wait for CASSANDRA-4762. In the meantime, we should obviously not throw an assertion error so attaching a patch to improve validation. The patch also slightly optimize matters as it uses name queries as much as possible if the select doesn't include a collection (versus, never using them on CF with collections even if no selection is selected). 
;;;","27/Mar/13 09:39;aleksey;+1;;;","27/Mar/13 10:43;slebresne;Committed, thanks;;;","03/Jun/13 16:00;sh123;how to apply patch ?
;;;","03/Jun/13 16:07;aleksey;bq. how to apply patch ?

It's been already applied to 1.2.4. Also, make sure to read Sylvain's comment.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Broken default values for min/max timestamp,CASSANDRA-5372,12638321,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,21/Mar/13 17:33,16/Apr/19 09:32,14/Jul/23 05:53,25/Mar/13 14:33,1.1.11,1.2.4,,,,,0,,,,,"When the SStableMetadata are not present (or are too hold), the default for the min and max timestamp used is not always correct. Namely, the default (i.e. when we don't know anything) for the min timestamp should be MIN_VALUE and the max timestamp should be MAX_VALUE.

And there is 2 places where we need to apply those default:
* if the metadata is an old one that don't have the info
* if we don't have any metadata component at all

The only default that is correct is the case fixed by CASSANDRA-5153, but even then it missed a number of occurrences of the problem.",,jasobrown,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/13 17:37;slebresne;5372.txt;https://issues.apache.org/jira/secure/attachment/12574837/5372.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,318797,,,Mon Mar 25 14:33:38 UTC 2013,,,,,,,,,,"0|i1j09z:",319138,,,,,,,,,jasobrown,,jasobrown,Low,,,,,,,,,,,,,,,,,,"21/Mar/13 17:37;slebresne;Patch attached to apply the correct default. The patch is against 1.2 though part of it affect 1.1 too (but 1.1 doesn't track the min timestamp). I'll happily commit the relevant part to 1.1 but attaching against 1.2 to have the whole fix.

The patch also remove SSTableMetadata.defaultInstance() because the deserializer already know what to do when the stat component is not here.;;;","25/Mar/13 13:12;jasobrown;LGTM;;;","25/Mar/13 14:33;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Perform size-tiered compactions in L0 (""hybrid compaction"")",CASSANDRA-5371,12638256,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jbellis,jbellis,21/Mar/13 15:22,16/Apr/19 09:32,14/Jul/23 05:53,08/Apr/13 18:24,2.0 beta 1,,,,,,0,lcs,,,,"If LCS gets behind, read performance deteriorates as we have to check bloom filters on man sstables in L0.  For wide rows, this can mean having to seek for each one since the BF doesn't help us reject much.

Performing size-tiered compaction in L0 will mitigate this until we can catch up on merging it into higher levels.",,br1985,christianmovi,colinkuo,deag,jay.zhuang,jeromatron,jjordan,marcuse,ravilr,rbranson,rcoli,rlow,rskvazh,slebresne,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-3854,,,,,,,,,,,,,,,,,,,,"21/Mar/13 20:52;tjake;HybridCompactionStrategy.java;https://issues.apache.org/jira/secure/attachment/12574880/HybridCompactionStrategy.java",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,318732,,,Sun Aug 30 16:23:25 UTC 2015,,,,,,,,,,"0|i1izvj:",319073,,,,,,,,,tjake,,tjake,Normal,,,,,,,,,,,,,,,,,,"21/Mar/13 19:32;jjordan;I could see this possibly helping if you left the non L0 alone and went to STCS for L0 until the write load stopped. But you have to come up with a heuristic to know when to shut off LCS.  So a cluster with a periodic write load, which was too high for LCS's increased IO needs, would revert to STCS of L0 only until the load dropped.  You would then have to play catchup shoving all that L0 data into the other levels. I could see use cases where this would be useful, such as periodic large data dumps into a cluster. You would have to be careful that there was enough down time between dumps for LCS to catchup.;;;","21/Mar/13 20:52;tjake;This initial version puts the newly flushed memtables into a queue and when there are 4 it size tiers them.  So you get 1/4 the sstables in L0.;;;","03/Apr/13 22:17;jbellis;Alternate implementation pushed to http://github.com/jbellis/cassandra/commits/5371 with the following improvements:

- Only applies STCS to L0 if L0 gets behind (defined as ""accumulates more than MAX_COMPACTING_L0 sstables"")
- Performs true STCS, rather than ""compact in sets of four and then never again"";;;","04/Apr/13 02:02;tjake;Oh good, this is what I wanted the implementation to end up being.

In LeveledManifest.getCompactionCandidates:

I think there is a bug in the size tier candidate checks.  You seem to be size tiering across all the non-compacting sstables and not the level0 ones.  I think you mean't to intersect the level0 sstables with the non-compacting ones.  You should also add a check after that to make sure the non-compacting level0 sstables are still > MAX_COMPACTING_L0

Also, the code only checks for STCS when a higher level is ready to be compacted.  Maybe move this to the top before the higher level checks. We know the higher levels are seek bounded but the code should try to keep up with level 0 flushes as much as possible.;;;","04/Apr/13 02:33;jbellis;bq. I think you mean't to intersect the level0 sstables with the non-compacting ones

Right.  Fix pushed.

bq. the code only checks for STCS when a higher level is ready to be compacted

The idea is, we'd prefer to do normal LCS compaction on L0.  So if the higher levels are okay, we'll treat L0 the same as before.  But if we do need to compact a higher level, we'll first check and see if L0 is far enough behind that we should do an STCS round there as a stop-gap.

bq. You should also add a check after that to make sure the non-compacting level0 sstables are still > MAX_COMPACTING_L0

I think it's more correct as written -- basically, we're doing L0 out-of-turn, since for max throughput we'd do the higher level next.  So, we'll do L0 STCS until it's under MCL0, then we'll go back to the higher levels until we catch up and can actually apply leveling to L0.;;;","04/Apr/13 11:07;slebresne;For my own curiosity, do we have performance numbers for this (including, not only on SSD tests)?

A priori, I'm not fully sold on this being always a win (or even most of the time of ""L0 is being""). That is, I understand the reasoning that lots of SSTables in L0 is bad for reads, but at the same time, if you compact STCS things in L0, a lot of the work you've done you will redo when you compact your now bigger L0 sstable against L1. I.e. those STCS compactions don't help you make progress as far as leveling is concerned, so it seems like it waste work overall. Besides, in theory, our LCS is supposed to be able to compact large amount of L0 sstables into L1 to help with the ""I'm behind on L0 but it's just a pike in load"". Now I guess if you've pushed a lot of data in L1 and get behind again in L0, then it's not fun because all of L1 need to be including in L0 compaction. But if you are constantly behind, doesn't that mean you have bigger problems (and/or that you should just use STSC)?

Basically I wonder if there won't be a number of scenario where because you get a bit behind on L0 once, then the I/O you ""waste"" doing STSC in L0 will help you get even more and more behind on your leveling and you'd end up doing mostly STSC, while letting LCS do its job would have been fine overall.

That is, I'm happy with this if that makes things clearly better in practice more often than not, it's just that intellectually it's not obvious to me that it's the case (note that I'm not saying that it's obvious it's a bad idea either).

;;;","04/Apr/13 12:02;tjake;[~jbellis] let me run some tests but the code looks good now.


[~slebresne]
bq. do we have performance numbers for this (including, not only on SSD tests)?

We only have SSD and I think LCS only ever makes sense on SSD.  If we want to support HDD then I agree this is def more IO overall.  The performance numbers for our use case went from all read timeouts using LCS to reads rarely timing out with the original patch.  The stress tool doesn't have a wide row scenario so it's hard to simulate out of the box.  

bq. doesn't that mean you have bigger problems (and/or that you should just use STSC)

You are right, this does require writes die down at somepoint otherwise you end up with STCS. ellis mentions this in the comments.

STCS isn't viable for the LCS use cases.  I don't see how having this (on SSD) would not help all LCS use cases since LCS is for wide row or heavy updates. The point of this is to avoid the situation where all sstables in L0 contain a portion of the row which requires reading them all.   One thing to keep in mind is if you do have a wide row and you end up with a STCS compacted row of 10MB and LCS has a 5MB limit you still end up with a 10MB sstable with a single row in it so the higher levels do benefit from STCS in this case.;;;","04/Apr/13 12:51;slebresne;bq. The stress tool doesn't have a wide row scenario so it's hard to simulate out of the box

Agreed, and that's definitively lacking. I believe there is a few knobs that allow to do wideish rows, but that's probably not very realistic.

{quote}
I think LCS only ever makes sense on SSD
since LCS is for wide row or heavy updates
{quote}

I'm not sure I agree. Maybe there is some truth to it in our current implementation, but that would then be more of a quirk of the implementation that the goal. Typically, I'm not really sure why only wide rows would benefit it. There is certainly nothing in theory that makes it so. As for ""it's for heavy updates only"", I think that LCS has a number of nice properties (like avoiding huge files that require half of you disk in free space) that are nice even if you have a moderate to low update rate (and in that case you can definitively afford LCS on HDD). More concretely, I'm pretty sure we have tons on users on LCS on HDD.

Anyway, all this to say that I don't necessary agree on optimizing LCS for heavy writes + wide rows + SSD *if* that's done at the expense of all other type of workload (and I'm not saying that's what this patch is doing, just that discarding other type of workload as unimportant is not ok imo).

bq. LCS has a 5MB limit you still end up with a 10MB sstable with a single row

If having 10MB sstables being split due to row too wide is a problem, then you should either not use LCS or pick a 10MB limit for LCS, not 5MB.

Anyway, I'm not vetoing this or anything like this. Just trying to get a better understanding of why this is a good thing to do in general.;;;","04/Apr/13 13:05;jbellis;bq. Basically I wonder if there won't be a number of scenario where because you get a bit behind on L0 once, then the I/O you ""waste"" doing STSC in L0 will help you get even more and more behind on your leveling

That's exact;y the case, which is why we only apply STCS to L0 when it's fairly badly behind, i.e., we can conclude two things:

# if the current workload continues, it's not going to magically catch up any time soon
# reads are starting to get into trouble

Note that #2 will cause a vicious cycle, slowing down compaction in turn.

So while I can hypothesize workloads that burst just long enough to cause STCS to kick in before stopping, thus ""wasting"" iops, I think for the vast majority this is a good ""safety valve,"" and specifically not worth adding a config option to disable.

However, I do think it's worth creating a ticket to allow STCS config options to be applied to the size-tiering done by LCS, and specifically allow configuring MAX_COMPACTION_L0 via the max sstables threshold, which I think may adequately address your concern.;;;","08/Apr/13 15:05;tjake;I'm going to test this out to show how it helps our workload.  

In the meantime I think this is fine to commit for 2.0 if you'd like to get it in now.;;;","08/Apr/13 18:24;jbellis;All right.  Rebased and committed, and created CASSANDRA-5439 for the options application.;;;","03/May/13 15:36;rbranson;Is this just waiting on [~tjake]'s test to backport to 1.2? 

Yesterday we bootstrapped our first new node on our first LCS cluster where each node only had ~50GB of data, and it took 6 hours to complete the bootstrap, even after running the CPUs hot by bumping compaction throughput up to 64MB. We probably could have stood to raise this to 128MB/sec and pegged them, but I dread to think of what this would be like if we moved some larger, read-heavy data sets to Cassandra under LCS. Jake seems to think this patch will help with that.

http://i.imgur.com/LpdAKyc.png
http://i.imgur.com/ZsgEB9G.png

This is on an EC2 hi1.4xlarge, which is a 16-core box w/60GB RAM, 2TB of SSD storage, and 10GigE. 

We also have a cluster of m1.xlarges (4-core, 15G, 2TB rust) each with ~300GB of relatively cold data under STCS. Considering the spinning rust cluster w/1GigE and 16MB/s compaction throughput can bootstrap a new node in < 2 hours with 6x as much data we will definitely be trying this HCS on the SSD cluster running LCS at the moment.;;;","03/May/13 15:46;jbellis;I'm not backporting this to a stable release.  It's a lot more involved than the 4KB proof of concept.  You can probably collaborate w/ Jake on a 1.2-appropriate alternative though.

That said, the main benefit of this is not that it magically makes LCS faster (it doesn't), but that when it does get behind your reads don't suffer so much.;;;","03/May/13 15:58;tjake;Right, it takes a long time but it will keep reads happier.

Why do you need LCS on your dataset Rick? is it a wide row?;;;","26/Dec/13 14:21;br1985;Hi,

We hit the same bug in production recently. We walked around it by switching to STCS for a few days, letting it stabilize and then going back to LCS. Quite long, but fully successful trip.

In our case we have a lot of sstables at L0 as a result of migration. Because of another bug in sstableloader (CASSANDRA-6527), we finally ended up simply copying all sstable files from the old cluster to the new one.

After the migration we had over 10k sstables (160MB per file) on each node. Of course, STCS-fallback activates automatically in that case.

I wonder if similar situation will happen after the classic bootstrap? Will streaming during bootstrapping put sstables at L0 or at the original level?

If it will put them all at L0 then I'm not sure if falling back to STCS is the best way to handle the situation. I've read the comment in the code and I'm aware why it is a good thing to do if we have to many sstables at L0 as a result of too many random inserts. We have a lot of sstables, each of them covers the whole ring, there's simply no better option. 

However, after the bootstrap situation looks a bit different. The loaded sstables already have vary small ranges! We just have to tidy up a bit and everything should be OK. STCS ignores that completely and after a while we have a bit less sstables but each of them covers the whole ring instead of just a small part. I believe that in that case letting LCS do the job is a better option that allowing STCS mix everything up before.

Is there a way to disable STCS fallback? I'll be glad to test this option the next time we do similar operation.
;;;","25/Jan/14 03:20;ravilr;+1 on [~br1985 ] comment. 
Even during dead node replace (using replace_address), streaming puts all sstables in L0. 2.0.x switches to STCS, in doing so, also creates larger sstables, which means more free disk space to be left, in order for them to be compacted later into higher levels. LCS is known to lower the amount of free disk space (headroom) needed for compaction. this is no more true with LCS in above scenarios.
Is there a way to disable STCS fallback, please?
;;;","25/Jan/14 11:46;br1985;Ravi, could you confirm that streaming puts all tables in L0? In that case I think we should open a separate issue instead of commenting on a closed one.;;;","25/Jan/14 16:54;brandon.williams;They have to go to L0 since preserving the level across machines doesn't make any sense.  Please do open a new issue.;;;","26/Jan/14 16:24;br1985;I've just created CASSANDRA-6621 describing the issue from the last comments.;;;","30/Aug/15 16:23;deag;I'm benchmarking our solution that uses LCS with C* 2.1.8 and we have the scenario here by [~jjordan] every week we need to burst C* with a batch job that takes 18-20h. The system behaves very well dureing the whole week, but during that time degrades considerably on the highest percentiles (>p99) were we get reads rocketing to latencies > 200ms. 

I'm working on understanding what is happening with the system since I don't see IO or CPU exhausted. Wonder what is the usual rate of compaction with LCS. In my system log I see that compaction tasks are only doing 2-3MB/s

{code}
INFO  [CompactionExecutor:93] 2015-08-30 17:06:22,114  CompactionTask.java:274 - Compacted 9 sstables to [/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20266,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20274,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20280,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20286,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20291,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20297,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20302,/mnt/ssd/cassandra/data/pulse/metrics-ea89aeb0465e11e586e7575910f56afe/pulse-metrics-ka-20307,].  1,383,544,305 bytes to 1,335,157,959 (~96% of original) in 450,225ms = 2.828154MB/s.  153,450 total partitions merged to 130,214.  Partition merge counts were {1:106978, 2:23236, }
{code}

Is this normal? ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ClosedChannelException on shutdown,CASSANDRA-5368,12638011,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,20/Mar/13 16:23,16/Apr/19 09:32,14/Jul/23 05:53,20/Mar/13 16:40,1.2.4,,,,,,0,,,,,"Catching AsynchronousCloseException isn't enough:

{noformat}
ERROR [ACCEPT-/127.0.0.3] 2013-03-20 11:10:38,087 CassandraDaemon.java (line 169) Exception in thread Thread[ACCEPT-/127.0.0.3,5,main]
java.lang.RuntimeException: java.nio.channels.ClosedChannelException
    at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:892)
Caused by: java.nio.channels.ClosedChannelException
    at sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:135)
    at sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    at org.apache.cassandra.net.MessagingService$SocketThread.run(MessagingService.java:881)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/13 16:24;brandon.williams;5368.txt;https://issues.apache.org/jira/secure/attachment/12574565/5368.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,318489,,,Wed Mar 20 16:40:57 UTC 2013,,,,,,,,,,"0|i1iydj:",318830,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"20/Mar/13 16:24;brandon.williams;Trivial patch to catch this but only mention it at debug.;;;","20/Mar/13 16:37;jbellis;+1;;;","20/Mar/13 16:40;brandon.williams;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Guava should be bumped to 13.0.1 in maven dependency declaration.,CASSANDRA-5364,12637890,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,xedin,xedin,xedin,20/Mar/13 04:28,16/Apr/19 09:32,14/Jul/23 05:53,20/Mar/13 21:42,1.2.4,,,,,,0,,,,,"Otherwise following error accours because generated pom says 12.0 and RateLimiter was introduced by 13.0

{noformat}
java.lang.NoClassDefFoundError: com/google/common/util/concurrent/RateLimiter
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpointInternal(HintedHandOffManager.java:316)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:252)
        at org.apache.cassandra.db.HintedHandOffManager.access$300(HintedHandOffManager.java:89)
        at org.apache.cassandra.db.HintedHandOffManager$4.runMayThrow(HintedHandOffManager.java:459)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1146)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:679)
Caused by: java.lang.ClassNotFoundException: com.google.common.util.concurrent.RateLimiter
        at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:266)

{noformat}",,slebresne,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/13 04:28;xedin;CASSANDRA-5364.patch;https://issues.apache.org/jira/secure/attachment/12574486/CASSANDRA-5364.patch",,,,,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,318370,,,Wed Mar 20 21:42:30 UTC 2013,,,,,,,,,,"0|i1ixn3:",318711,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"20/Mar/13 09:25;slebresne;+1;;;","20/Mar/13 21:42;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Missing files in debian /etc/cassandra/conf folder,CASSANDRA-5363,12637883,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,sdelmas,sdelmas,20/Mar/13 02:42,16/Apr/19 09:32,14/Jul/23 05:53,20/Mar/13 19:04,1.2.4,,,Packaging,,,0,,,,,"The standard debian installation puts;

cassandra-env.sh cassandra.yaml log4j-server.properties

into /etc/cassandra. However there seem to be additional files that might make sense there:

commitlog_archiving.properties cassandra-rackdc.properties log4j-tools.properties cassandra-topology.properties

(these are at least installed by the DSC rpm installer). So should those be added in debian or removed in rpms?",Debian,sdelmas,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Mar/13 16:05;brandon.williams;5363.txt;https://issues.apache.org/jira/secure/attachment/12574562/5363.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,318363,,,Thu Mar 21 12:23:06 UTC 2013,,,,,,,,,,"0|i1ixlj:",318704,,,,,,,,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,,"20/Mar/13 16:05;brandon.williams;We already had rackdc properties, but here's a patch for the others.;;;","20/Mar/13 18:59;urandom;LGTM; +1;;;","20/Mar/13 19:04;brandon.williams;Committed;;;","21/Mar/13 12:23;sdelmas;Thanks for the quick turnaround.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Transposed KS/CF arguments,CASSANDRA-5362,12637842,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,j.casares,j.casares,19/Mar/13 22:19,16/Apr/19 09:32,14/Jul/23 05:53,20/Mar/13 08:43,1.2.4,,,,,,0,datastax_qa,,,,"*Reproduction*
Using https://github.com/joaquincasares/java-driver's integrationtests branch, run `mvn test` from the root directory.

*Issue*
The test will fail due to https://github.com/joaquincasares/java-driver/blob/integrationtests/driver-core/src/main/java/com/datastax/driver/core/ResultSetFuture.java being swapped here:
{CODE}
case ALREADY_EXISTS:
    org.apache.cassandra.exceptions.AlreadyExistsException aee = (org.apache.cassandra.exceptions.AlreadyExistsException)te;
    return new AlreadyExistsException(aee.ksName, aee.cfName);
{CODE}

*Error*
{CODE}
repeatSchemaDefinition(com.datastax.driver.core.ExceptionsTest)  Time elapsed: 0.501 sec  <<< FAILURE!
org.junit.ComparisonFailure: expected:<Table repeatschema[ks.repeatschemacf] already exists> but was:<Table repeatschema[cf.repeatschemaks] already exists>
{CODE}",,j.casares,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,318322,,,Wed Mar 20 08:43:08 UTC 2013,,,,,,,,,,"0|i1ixcf:",318663,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"20/Mar/13 08:43;slebresne;The transposition was in MigrationManager.announceNewColumnFamily. Took the liberty to commit (c1332ef) directly without review as this is a trivial fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
realclean does not show up with ant -p,CASSANDRA-5356,12637542,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,cburroughs,cburroughs,cburroughs,18/Mar/13 14:24,16/Apr/19 09:32,14/Jul/23 05:53,23/Mar/13 04:07,1.2.4,,,Packaging,,,0,,,,,"This has bothered me for years!

-    <target name=""realclean"" depends=""clean"">
+    <target name=""realclean"" depends=""clean"" description=""Remove the entire build directory and downloaded artifacts"">
",,cburroughs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,cburroughs,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,318033,,,Sat Mar 23 04:07:21 UTC 2013,,,,,,,,,,"0|i1ivk7:",318374,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"23/Mar/13 04:07;jbellis;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Collection values size is not validated.,CASSANDRA-5355,12637512,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,18/Mar/13 10:46,16/Apr/19 09:32,14/Jul/23 05:53,21/Mar/13 12:58,1.2.4,,,,,,0,,,,,"Collections values are currently limited to 64K because the serialized form used uses shorts to encode the elements length (and for sets elements and key map, because they are part of the internal column name that is itself limited to 64K).

However, there is no check on the collection elements size currently so we don't refuse values > 64K (except for sets elements and map keys because we check internal column name sizes), even though they can't be decoded correctly client side.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Mar/13 11:24;slebresne;5355.txt;https://issues.apache.org/jira/secure/attachment/12574776/5355.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,318003,,,Thu Mar 21 12:58:27 UTC 2013,,,,,,,,,,"0|i1ivdj:",318344,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"18/Mar/13 10:55;slebresne;I note that 64k might sound arbitrary low and we might want to lift the limitation. However, as said above, for sets elements we are constrained by the current internal column name size. We could of course lift that limitation, but it's not trivial. Of course, we could lift the limitation only for map and list values, but even that would unfortunately break the current format currently used to send collections to the client, so at a minimum it would require a binary protocol version bump.;;;","18/Mar/13 13:30;jbellis;I'm fine with the 64k limit since I've always thought of collections as places to denormalize small amounts of data, not a replacement for creating a new table.;;;","21/Mar/13 11:24;slebresne;Patch attached to do 2 things:
# validate the 64K limit on inserts. Note that the patch does the validation for set values and map keys too so as to provide a more meaningful error message, but in practice slightly slower value may be rejected if the column name limit is reached.
# fix a small ""bug"" in the collection types {{compose}} method that was not reading the size unsigned, thus artificially limiting the size to 32K.
;;;","21/Mar/13 12:41;jbellis;+1;;;","21/Mar/13 12:58;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CL regression in the presence of bootstrapping nodes,CASSANDRA-5354,12637479,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,18/Mar/13 04:08,16/Apr/19 09:32,14/Jul/23 05:53,18/Mar/13 13:46,1.2.4,,,,,,0,,,,,"It looks like CASSANDRA-4858 broke CASSANDRA-833 again; pendingEndpoints is not provided to or accounted by blockFor.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Mar/13 12:53;slebresne;5354.txt;https://issues.apache.org/jira/secure/attachment/12574137/5354.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,317970,,,Mon Mar 18 13:46:38 UTC 2013,,,,,,,,,,"0|i1iv67:",318311,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"18/Mar/13 12:53;slebresne;For the record and from what I can tell, this is no due to CASSANDRA-4858 that did not changed the prior behavior, but the fix for CASSANDRA-833 never really worked. What happened is that:
* CASSANDRA-833 was committed and reversed right away (we don't remember why).
* CASSANDRA-3979 was committed. That patch made {{blockFor}} a constant initialized in WriteResponseHandler ctor to consistencyLevel.blockFor() (and used to initialize the size of {{responses}} in particular). This was correct at the time, CASSANDRA-833 had been reverted.
* The rebase of CASSANDRA-833 was committed. That rebase changed blockFor() in AbstractWriteResponseHandler to be blockForCL() (the ""old"" blockFor) + the pending endpoints. However, WriteResponseHandler was not modify to use that blockFor() method to initial the {{responses}} variable.

As a result, CASSANDRA-833 never ""worked"".

Anyway, attaching patch to hopefully fix this. I note that it includes a small part of the original 833 patch in DatacenterSyncWriteResponseHandler that apparently didn't made it in the rebase but I think is needed.
;;;","18/Mar/13 13:28;jbellis;LGTM, thanks for tracking that down!;;;","18/Mar/13 13:46;slebresne;Alright, committed. Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PreparedStatements get mixed up between Keyspaces,CASSANDRA-5352,12637210,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,davedamoon,davedamoon,davedamoon,15/Mar/13 15:07,16/Apr/19 09:32,14/Jul/23 05:53,18/Mar/13 08:34,1.2.4,,,,,,0,,,,,"I found this behavior while running the same application using two different keyspaces connected to the same node.

The prepared statements uses the keyspace that was set while the statement was perpared (public final CFDefinition cfDef).
When reusing the Statement only the cql-query is used to create a key and the keyspace is ignored. When the same query is prepared and used for two different Keyspaces the wrong keyspace can be used.

The fix is not to ignore the keyspace when reusing the statement.",,davedamoon,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Mar/13 15:13;davedamoon;CassandraServerCql3Test.java;https://issues.apache.org/jira/secure/attachment/12573884/CassandraServerCql3Test.java","15/Mar/13 15:16;davedamoon;patch.txt;https://issues.apache.org/jira/secure/attachment/12573885/patch.txt",,,,,,,,,,,,,,,,,,,2.0,davedamoon,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,317702,,,Mon Mar 18 08:34:11 UTC 2013,,,,,,,,,,"0|i1itin:",318043,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"15/Mar/13 15:13;davedamoon;the testcase for select querys;;;","15/Mar/13 15:16;davedamoon;change to involve the keyspace while creating the key for statment storage;;;","18/Mar/13 08:34;slebresne;Good catch. Committed with a slight modification to handle the case where the current keyspace is {{null}}.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix ColumnFamily opening race,CASSANDRA-5350,12637068,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,yukim,yukim,14/Mar/13 18:45,16/Apr/19 09:32,14/Jul/23 05:53,14/Mar/13 21:02,1.1.11,1.2.4,,,,,0,,,,,"(Moving from CASSANDRA-5151)

Currently, MeteredFlusher is scheduled inside static block of ColumnFamilyStore,  and it accesses all ColumnFamilyStore when it runs every 1 sec. Scheduling is done when JVM first load ColumnFamilyStore class, so after that, there is always a chance to open SSTables before doing scrub directory/remove compaction left overs.
We should move the content of static block at the end of CassandraDaemon setup.",,colinkuo,yulinyen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5469,,,,,,,,,,,"14/Mar/13 18:46;yukim;0001-move-scheduling-MeteredFlusher-to-CassandraDaemon.patch;https://issues.apache.org/jira/secure/attachment/12573749/0001-move-scheduling-MeteredFlusher-to-CassandraDaemon.patch",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,317560,,,Thu Mar 14 21:02:19 UTC 2013,,,,,,,,,,"0|i1isn3:",317901,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"14/Mar/13 18:46;yukim;Patch for 1.2 branch attached.;;;","14/Mar/13 18:48;yukim;Patch is for 1.2, but 1.1.x has the same problem, so maybe it is good to patch that as well.;;;","14/Mar/13 19:22;jbellis;LGTM. +1 for 1.1.x and 1.2.x.;;;","14/Mar/13 21:02;yukim;Committed to 1.1 branch and above;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Potential problem with GarbageCollectorMXBean,CASSANDRA-5345,12637020,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,JoshuaMcKenzie,mbyrd,mbyrd,14/Mar/13 13:45,16/Apr/19 09:32,14/Jul/23 05:53,18/Jul/14 23:15,2.0.10,2.1.1,,Legacy/Observability,,,1,,,,,"I am not certain this is definitely a bug, but I thought it might be worth posting to see if someone with more JVM//JMX knowledge could disprove my reasoning. Apologies if I've failed to understand something.

We've seen an intermittent problem where there is an uncaught exception in the scheduled task of logging gc results in GcInspector.java:

{code}
...
 ERROR [ScheduledTasks:1] 2013-03-08 01:09:06,335 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[ScheduledTasks:1,5,main]
java.lang.reflect.UndeclaredThrowableException
        at $Proxy0.getName(Unknown Source)
        at org.apache.cassandra.service.GCInspector.logGCResults(GCInspector.java:95)
        at org.apache.cassandra.service.GCInspector.access$000(GCInspector.java:41)
        at org.apache.cassandra.service.GCInspector$1.run(GCInspector.java:85)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: javax.management.InstanceNotFoundException: java.lang:name=ParNew,type=GarbageCollector
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1094)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getAttribute(DefaultMBeanServerInterceptor.java:662)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.getAttribute(JmxMBeanServer.java:638)
        at com.sun.jmx.mbeanserver.MXBeanProxy$GetHandler.invoke(MXBeanProxy.java:106)
        at com.sun.jmx.mbeanserver.MXBeanProxy.invoke(MXBeanProxy.java:148)
        at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:248)
        ... 13 more
...
{code}

I think the problem, may be caused by the following reasoning:

In GcInspector we populate a list of mxbeans when the GcInspector instance is instantiated:

{code}
...
List<GarbageCollectorMXBean> beans = new ArrayList<GarbageCollectorMXBean>();
        MBeanServer server = ManagementFactory.getPlatformMBeanServer();
        try
        {
            ObjectName gcName = new ObjectName(ManagementFactory.GARBAGE_COLLECTOR_MXBEAN_DOMAIN_TYPE + "",*"");
            for (ObjectName name : server.queryNames(gcName, null))
            {
                GarbageCollectorMXBean gc = ManagementFactory.newPlatformMXBeanProxy(server, name.getCanonicalName(), GarbageCollectorMXBean.class);
                beans.add(gc);
            }
        }
        catch (Exception e)
        {
            throw new RuntimeException(e);
        }
...
{code}

Cassandra then periodically calls:

{code}
...
    private void logGCResults()
    {
        for (GarbageCollectorMXBean gc : beans)
        {
            Long previousTotal = gctimes.get(gc.getName());
...
{code}

In the oracle javadocs, they seem to suggest that these beans could disappear at any time.(I'm not sure why when or how this might happen)
http://docs.oracle.com/javase/6/docs/api/
See: getGarbageCollectorMXBeans

{code}
...
public static List<GarbageCollectorMXBean> getGarbageCollectorMXBeans()
Returns a list of GarbageCollectorMXBean objects in the Java virtual machine. The Java virtual machine may have one or more GarbageCollectorMXBean objects. It may add or remove GarbageCollectorMXBean during execution.
Returns:
a list of GarbageCollectorMXBean objects.
...
{code}

Correct me if I'm wrong, but do you think this might be causing the problem? That somehow the JVM decides to remove the GarbageCollectorMXBean temporarily or permanently (causing said exception) and if this is expected behaviour, should it be handled in some way?
Also I'd like to point out that this may be an issue on other versions as well as I don't believe this code has changed in quite a long time.
Unfortunately I haven't been able to reproduce this outside of the production environment, if you have any tips, questions or are able to explain//disprove my concerns, I'd be very grateful.

Thanks,
Matt","JVM:JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_30  typical 6 node 2 availability zone Mutli DC cluster on linux vms with
and mx4j-tools.jar and jna.jar both on path. Default configuration bar token setup(equispaced), sensible cassandra-topology.properties file and use of said snitch.",arya,bonifaido,enigmacurry,JoshuaMcKenzie,mbyrd,mishail,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jun/14 15:33;JoshuaMcKenzie;5345_v1.txt;https://issues.apache.org/jira/secure/attachment/12652626/5345_v1.txt","18/Jul/14 15:15;JoshuaMcKenzie;5345_v2.txt;https://issues.apache.org/jira/secure/attachment/12656521/5345_v2.txt",,,,,,,,,,,,,,,,,,,2.0,JoshuaMcKenzie,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,317512,,,Fri Jul 18 23:15:20 UTC 2014,,,,,,,,,,"0|i1iscf:",317853,1.0.7,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"14/Mar/13 19:46;brandon.williams;Personally, I doubt this theory, the JVM has no reason to make PN disappear, and this code has been around for a long time with no similar reports.  I think you might have a build problem.;;;","25/Mar/14 01:11;arya;Running 1.2.14 on Ubuntu 12.04 with HotSpot Java 1.7.0_21-b11. 

For the past couple of days our nodes started to produce this exception every second. This exception started to get produced after we decommissioned one DC, and it continued to get produced until the nodes started producing client errors as we got alerts. No suspicious other log entries were found and no full GCs were recorded in the GC logs. The GC logs looked normal however system.log was getting filled with this exception. 

We decommissioned the DC by first altering our keyspace to not replication to that DC as we are using NetworkTopologyStrategy. Then we issued nodetool decommission on each node in that DC. 

Why this exception started to get produced is not known. However, restarting the nodes fixed the problem. Another observation was that the nodetool info command which we use to collect heap size statistics was not working as well and was tossing this exception:

Exception in thread ""main"" java.lang.IllegalArgumentException: javax.management.InstanceNotFoundException: java.lang:type=Memory
        at java.lang.management.ManagementFactory.newPlatformMXBeanProxy(ManagementFactory.java:610)
        at org.apache.cassandra.tools.NodeProbe.connect(NodeProbe.java:175)
        at org.apache.cassandra.tools.NodeProbe.<init>(NodeProbe.java:116)
        at org.apache.cassandra.tools.NodeCmd.main(NodeCmd.java:1138)
Caused by: javax.management.InstanceNotFoundException: java.lang:type=Memory
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.getMBean(DefaultMBeanServerInterceptor.java:1095)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.isInstanceOf(DefaultMBeanServerInterceptor.java:1401)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.isInstanceOf(JmxMBeanServer.java:1082)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1492)
        at javax.management.remote.rmi.RMIConnectionImpl.access$300(RMIConnectionImpl.java:96)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1327)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1419)
        at javax.management.remote.rmi.RMIConnectionImpl.isInstanceOf(RMIConnectionImpl.java:957)
        at sun.reflect.GeneratedMethodAccessor82.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:322)
        at sun.rmi.transport.Transport$1.run(Transport.java:177);;;","25/Mar/14 01:13;arya;Matt Byrd, please assign version and increase the priority as this did in fact bring our cluster to halt. ;;;","25/Mar/14 03:51;brandon.williams;Ryan, can your team repro?;;;","25/Mar/14 06:30;mbyrd;The cluster in question was running on 1.0.7, however the code in question has remained static since well before that and doesn't look to have changed since. (though admittedly the problem could somehow be being caused elsewhere, jvm maybe?) 
I've upped the priority to major.

Have you been able to reproduce? or seen the problem anywhere else?
Any further details about your environment is set up and how you deploy may also help those trying to reproduce.
Some common but perhaps co-incidental things about the two occurrences:
1. virtual machines (though not both AWS)
2. multi D.C , wouldn't have though this would be relevant but Arya does seem to see the problem after removing a d.c.
3. Slightly old Jvm versions...

I no longer have access to the cluster where we saw this previously but let me know if I can help in any other way.;;;","25/Mar/14 19:56;arya;To give you more details about our setup:

AWS
us-east-1 3 nodes. 
us-west-2 3 nodes.
Classic EC2 hi1.4xlarge machines
NetworkTopologyStrategy us-east-1:3, us-west-2:3
Effective load on each machine is 100% about 360Gb
We run a 24Gb Heap with MaxTenuringThreshold = 20

Average read 4k/sec on each node
Average write 1.5k/sec on each node

We have a mix of SizeTiered and Leveled CFs.
We turned off read repair. 
We use mmap_index_only;;;","25/Jun/14 21:30;JoshuaMcKenzie;[~enigmacurry] - did you or your team have any luck reproducing this?

It should be trivial to throw a gc.isValid() check in the logGCResults loop and if invalid, flag to rebuild the List<GarbageCollectorMXBean> we're iterating across in that function as well as to introduce some exception handling for the UndeclaredThrowableException.  I'm not finding much on the logic behind *when* these MXBeans can be removed from the system and I agree with Brandon that it seems incredibly odd for a JVM to punt and re-init a garbage collector MXBean on the fly with such infrequency that we haven't seen this more often.

The fact that Arya had a nodetool failure connecting to the Memory subsystem:
{code:title=Memory failure}
Exception in thread ""main"" java.lang.IllegalArgumentException: javax.management.InstanceNotFoundException: java.lang:type=Memory
at java.lang.management.ManagementFactory.newPlatformMXBeanProxy(ManagementFactory.java:610)
at org.apache.cassandra.tools.NodeProbe.connect(NodeProbe.java:175)
{code}
is concerning as it would seem to imply some deeper problems w/the JMX integration of the Memory subsystem on these JVM's given that we're querying the factory by String there rather than trying to use an invalid reference.;;;","25/Jun/14 21:51;enigmacurry;[~JoshuaMcKenzie] no, I haven't reproduced it. :(;;;","26/Jun/14 15:33;JoshuaMcKenzie;Attaching a v1 that tries to gracefully check for GC MXBean validity on log and if an invalid GC is found, skip it and rebuild the list of cached MXBean's for next logging.

Given that I can't find any info on *when* these things are getting recycled, I also put in some exception handling in-case the change happens in the middle of our logging process.

This doesn't address the failure of nodetool info that Arya saw with nodetool info; that issue seems like a related but perhaps different (and less critical) effort than this ticket.;;;","17/Jul/14 21:59;jbellis;I couldn't get this to apply to the repo as of Jun 26.  Can you rebase against current 2.0 branch?;;;","18/Jul/14 15:15;JoshuaMcKenzie;rebased against cassandra-2.0;;;","18/Jul/14 23:15;jbellis;Added a comment pointing to this issue and committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
1.2 creates LZ4 compressed users table which is incompatible in mixed 1.2/1.1 cluster ,CASSANDRA-5343,12636845,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,enigmacurry,enigmacurry,13/Mar/13 19:52,16/Apr/19 09:32,14/Jul/23 05:53,13/Mar/13 21:15,,,,,,,0,,,,,"* Start with a 3 node 1.1.9 cluster.
* Take down node1, replace with 1.2.2.
* Bring node1 back up, the cluster now has mixed versions.

When node1 comes up, it tries to create a new keyspace: system_auth, which looks like this:

{code}
CREATE KEYSPACE system_auth WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '1'
};

USE system_auth;

CREATE TABLE users (
  name text PRIMARY KEY,
  super boolean
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=7776000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'LZ4Compressor'};
{code}

The system_auth.users table is using *LZ4Compressor* which does not work with the 1.1.9 nodes. The 1.1.9 nodes return this error in the logs:

{code}
ERROR [MigrationStage:1] 2013-03-13 15:24:44,246 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: Could not create Compression for type org.apache.cassandra.io.compress.LZ4Compressor
	at org.apache.cassandra.config.CFMetaData.fromSchemaNoColumns(CFMetaData.java:1234)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1247)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:299)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:462)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:374)
	at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:352)
	at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.config.ConfigurationException: Could not create Compression for type org.apache.cassandra.io.compress.LZ4Compressor
	at org.apache.cassandra.io.compress.CompressionParameters.parseCompressorClass(CompressionParameters.java:141)
	at org.apache.cassandra.io.compress.CompressionParameters.<init>(CompressionParameters.java:70)
	at org.apache.cassandra.io.compress.CompressionParameters.create(CompressionParameters.java:63)
	at org.apache.cassandra.config.CFMetaData.fromSchemaNoColumns(CFMetaData.java:1224)
	... 13 more
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.io.compress.LZ4Compressor
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:169)
	at org.apache.cassandra.io.compress.CompressionParameters.parseCompressorClass(CompressionParameters.java:137)
	... 16 more
ERROR [ReadStage:33] 2013-03-13 15:24:54,319 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[ReadStage:33,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown table/cf pair (system_auth.users)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:71)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException: Unknown table/cf pair (system_auth.users)
	at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:159)
	at org.apache.cassandra.service.RangeSliceVerbHandler.executeLocally(RangeSliceVerbHandler.java:44)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:63)
	... 4 more
{code} 

It appears that this means that the table creation failed on the 1.1.9 nodes. 

From one of the 1.1.9 nodes:
{code}
cqlsh> DESCRIBE KEYSPACE system_auth ;

CREATE KEYSPACE system_auth WITH replication = {
  'class': 'SimpleStrategy',
  'replication_factor': '1'
};

USE system_auth;

CqlTableDef instance has no attribute 'key_aliases'
{code}

No mention of the users table.

I know that schema changes in mixed version 1.1/1.2 clusters are not supported, but this table is not one that the user tried to create, it was created automatically on startup. 

h2. Steps to reproduce:
{code}
ccm create -v 1.1.9 1.1.9
ccm populate -n 3
ccm start
ccm node1 stress
ccm node1 stop
{code}

edit ~/.ccm/1.1.9/cluster.conf and configure cassandra_dir to point to 1.2.2. Edit node1's cassandra.yaml to be 1.2 compliant.
{code}
ccm node1 start
{code}

At this point check node2's log file for the exception.",,aleksey,enigmacurry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5321,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,317337,,,Wed Mar 13 21:15:23 UTC 2013,,,,,,,,,,"0|i1ir9j:",317678,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"13/Mar/13 20:58;enigmacurry;[~iamaleksey]'s proposed patch for CASSANDRA-5334 mostly fixes this, but this is still sitting in the 1.1.9 nodes log:

{code}
ERROR [ReadStage:33] 2013-03-13 16:50:47,202 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[ReadStage:33,5,main]
java.lang.AssertionError: Unknown keyspace system_auth
        at org.apache.cassandra.db.Table.<init>(Table.java:287)
        at org.apache.cassandra.db.Table.open(Table.java:119)
        at org.apache.cassandra.db.Table.open(Table.java:97)
        at org.apache.cassandra.service.RangeSliceVerbHandler.executeLocally(RangeSliceVerbHandler.java:44)
        at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:63)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

AssertionError doesn't sound that bad to me, but it would be nice if this were a warning instead so as not to make dtests fail.;;;","13/Mar/13 21:11;aleksey;This AssertionError is harmless. I don't think it can/should be replaced with a warning, though. I'm afraid you'll have to find a way around it in dtests.;;;","13/Mar/13 21:15;enigmacurry;fair enough, will do the exception handling in the test then.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ancestors are not cleared in SSTableMetadata after compactions are done and old SSTables are removed,CASSANDRA-5342,12636841,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,weizhu,weizhu,13/Mar/13 19:02,16/Apr/19 09:32,14/Jul/23 05:53,08/Jul/13 10:10,1.2.7,,,,,,0,lcs,,,,"We are using LCS and have total of 38000 SSTables for one CF. During LCS, there could be over a thousand SSTable involved. All those SSTable IDs are stored in ancestors field of SSTableMetatdata for the new table. In our case, it consumes more than 1G of heap memory for those field. Put it in perspective, the ancestors consume 2 - 3 times more memory than bloomfilter (fp = 0.1 by default) in LCS. 
We should remove those ancestors from SSTableMetadata after the compaction is finished and the old SSTable is removed. It  might be a big deal for Sized Compaction since there are small number of SSTable involved. But it consumes a lot of memory for LCS. 
At least, we shouldn't load those ancestors to the memory during startup if the files are removed. 
I would love to contribute and provide patch. Please let me know how to start. ",,al@ooyala.com,christianmovi,marcuse,weizhu,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Jun/13 14:35;marcuse;0001-CASSANDRA-5342-wip-v2.patch;https://issues.apache.org/jira/secure/attachment/12588846/0001-CASSANDRA-5342-wip-v2.patch","20/Jun/13 08:56;marcuse;0001-CASSANDRA-5342-wip.patch;https://issues.apache.org/jira/secure/attachment/12588796/0001-CASSANDRA-5342-wip.patch","13/Mar/13 19:17;weizhu;Screen Shot 2013-03-13 at 12.05.08 PM.png;https://issues.apache.org/jira/secure/attachment/12573558/Screen+Shot+2013-03-13+at+12.05.08+PM.png",,,,,,,,,,,,,,,,,,3.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,317333,,,Mon Jul 08 10:10:15 UTC 2013,,,,,,,,,,"0|i1ir8n:",317674,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"13/Mar/13 19:48;yukim;Ancestors are used to track compaction leftovers. We are implementing better way to achieve that in CASSANDRA-5151 using system table.;;;","24/May/13 15:03;jbellis;(Setting affects-version to 1.1.3, where it was introduced by CASSANDRA-4436.);;;","24/May/13 15:06;jbellis;WDYT [~krummas]?;;;","25/May/13 14:25;marcuse;have not really followed #5151 so these is my quite out-of-context thoughts:

as far as i can see they are only read during startup, maybe we could not keep a reference to them after flushing the metadata, and then reading them on startup only to be able to remove the compaction leftovers? and then clear them out the same way we mutate sstable level?

i could have a go;;;","25/May/13 16:31;jbellis;Yeah, it sounds easy enough in principle, but it might get messy in practice since we've had this ""immutable SSTableMetadata"" concept.;;;","20/Jun/13 08:56;marcuse;removes ancestors from SSTableMetadata and instead makes deserialize(..) return a Pair<SSTM, Set<Integer>> so that the caller can decide if the ancestors are needed. This allows us to keep SSTM as immutable as possible.

this forces us to re-deserialize the metadata when trying to figure out ancestors during compaction

opted not to mutate the ancestors on-disk since it makes my skin crawl;;;","20/Jun/13 14:35;marcuse;use Pair.create...;;;","06/Jul/13 01:10;jbellis;LGTM.

Nit: since we only ever use either the metadata, or the anscestors, it may be cleaner to split deserializeAncestors into a separate method.  OTOH that might make it easier to make mistakes in compatibility.  I'm fine either way.;;;","08/Jul/13 10:10;marcuse;thanks, committed as 83b75754ff143d4d77b01ef76a813da47779c6f4

opted to leave the Pair in, no strong feelings either;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select writetime  Exception when  data doesn't exist,CASSANDRA-5341,12636785,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,julien_campan,julien_campan,13/Mar/13 15:15,16/Apr/19 09:32,14/Jul/23 05:53,02/Apr/13 09:40,1.2.4,,,Legacy/CQL,,,0,,,,,"Hi,

When I try do to a “select writetime” on a empty column, I have an Exception. 
For example : 
Create table test (id  int, value int, value2 int, primary key (id));	
INSERT INTO test (id, value ) VALUES ( 1,100) ;

select * from test where id =1; 
==>
 id | value | value2
----+-------+--------
  1 |   100 |   null	

It’s working.

select WRITETIME(value) from test where id =1; 
==>
 writetime(value)
------------------
 1363184789539000
It’s working


select WRITETIME(value2) from test where id =1; 

==>TSocket read 0 bytes

I have an Exception.
",,aleksey,julien_campan,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Mar/13 09:57;slebresne;5341.txt;https://issues.apache.org/jira/secure/attachment/12575295/5341.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,317277,,,Tue Apr 02 09:40:13 UTC 2013,,,,,,,,,,"0|i1iqw7:",317618,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"23/Mar/13 04:19;jbellis;What exception do you see in your server log?;;;","25/Mar/13 09:57;slebresne;Patch attached.;;;","25/Mar/13 10:19;julien_campan;This is the exception : 

ERROR [Thrift:25] 2013-03-25 11:18:38,889 CustomTThreadPoolServer.java (line 217) Error occurred during processing of message.
java.lang.NullPointerException
	at org.apache.cassandra.cql3.statements.Selection$ResultSetBuilder.add(Selection.java:254)
	at org.apache.cassandra.cql3.statements.SelectStatement.handleGroup(SelectStatement.java:842)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:709)
	at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:147)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:136)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:62)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:132)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:143)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1733)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)
;;;","25/Mar/13 12:44;jbellis;Why are we adding null columns in the first place?;;;","25/Mar/13 12:55;slebresne;Cause this method is expected to be called for every CQL3 column of the result set and this for every row, even if the result to return is 'null'.;;;","02/Apr/13 00:32;aleksey;+1;;;","02/Apr/13 09:40;slebresne;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
scripts fail when paths contain space,CASSANDRA-5338,12636639,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,amichai,amichai,12/Mar/13 19:31,16/Apr/19 09:32,14/Jul/23 05:53,08/Jul/13 19:33,,,,Legacy/Tools,,,0,,,,,The shell scripts fail when the cassandra or java dirs contain spaces.,"Kubuntu 12.10 (GNU bash 4.2.37), and Windows XP (msysgit GNU bash 3.1.0)",amichai,urandom,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"12/Mar/13 19:32;amichai;fix_spaces_in_paths.patch;https://issues.apache.org/jira/secure/attachment/12573396/fix_spaces_in_paths.patch",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,317131,,,Mon Jul 08 19:33:46 UTC 2013,,,,,,,,,,"0|i1ipzr:",317472,,,,,,,,,urandom,,urandom,Low,,,,,,,,,,,,,,,,,,"12/Mar/13 19:32;amichai;The patch needs testing on other platforms.;;;","13/Mar/13 09:22;amichai;btw, this problem would be easily alleviated if there wasn't so much duplication in the scripts, some of which get updated and others forgotten... why not consolidate the duplication into a single master script? For backwards compatibility, the existing scripts can continue to exist but just call the master script with appropriate parameters.;;;","13/Mar/13 15:32;brandon.williams;See CASSANDRA-5301 for the duplication.;;;","01/Apr/13 22:52;jbellis;So this is redundant post-5301?;;;","02/Apr/13 06:50;amichai;The duplication thing is a related thought that could prevent the issue from creeping back again in the future, and making the fix easier, but the actual bug is the whitespace handling, which is unrelated to bug #5301 (which is resolved as wontfix in any case).;;;","08/Jul/13 19:33;urandom;committed; thanks, and sorry for the long delay;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Don't announce migrations to pre-1.2 nodes,CASSANDRA-5334,12636457,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,enigmacurry,enigmacurry,11/Mar/13 22:23,16/Apr/19 09:32,14/Jul/23 05:53,14/Mar/13 00:11,1.2.3,,,,,,0,qa-resolved,,,,"I have a mixed version cluster consisting of two 1.1.9 nodes and one 1.2.2 node upgraded from 1.1.9. 

The upgrade works, and I don't see any end user problems, however I see this exception in the logs on the non-upgraded nodes:

{code}
ERROR [MigrationStage:1] 2013-03-11 18:09:09,001 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
	at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:256)
	at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:397)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:373)
	at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:352)
	at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}


Steps to reproduce:
{code}
ccm create -v 1.1.9 1.1.9
ccm populate -n 3
ccm start
ccm node1 stress
ccm node1 stop
{code}

edit ~/.ccm/1.1.9/cluster.conf and configure cassandra_dir to point to 1.2.2. Edit node1's cassandra.yaml to be 1.2 compliant.

{code}
ccm node1 start
{code}

The cluster is now a mixed version, and works for user queries, but with the exception above.",,aleksey,christianmovi,enigmacurry,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5321,,,,,,,,,,,,,,,,,,,,,,,,"13/Mar/13 22:43;aleksey;5334-extra.txt;https://issues.apache.org/jira/secure/attachment/12573604/5334-extra.txt","13/Mar/13 20:18;aleksey;5334-v2.txt;https://issues.apache.org/jira/secure/attachment/12573574/5334-v2.txt","13/Mar/13 18:18;slebresne;5334.patch;https://issues.apache.org/jira/secure/attachment/12573550/5334.patch","13/Mar/13 17:00;enigmacurry;cassandra.trunk.yaml;https://issues.apache.org/jira/secure/attachment/12573533/cassandra.trunk.yaml",,,,,,,,,,,,,,,,,4.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,316949,,,Thu Mar 14 00:11:50 UTC 2013,,,,,,,,,,"0|i1iovr:",317291,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,enigmacurry,,,"11/Mar/13 22:29;enigmacurry;I've added a [dtest|https://github.com/riptano/cassandra-dtest/commit/4432c02cc09fb61adce3121a72429132bcc29451] to test this scenario as well.;;;","12/Mar/13 18:02;brandon.williams;Do you know which step is causing the exception?;;;","12/Mar/13 18:06;enigmacurry;It only happens once you bring node1 up on the new version, the error appears at that moment on the other nodes.;;;","13/Mar/13 09:54;slebresne;This was introduced by CASSANDRA-4433, that removed the ""name"" column of the schema_keyspaces table, but that one, while redundant, is used by 1.1 nodes.

This boils down to the problem I described in CASSANDRA-4603: we cannot easily remove anything in the schema tables without breaking rolling upgrades.

So attaching a patch that reintroduce the ""name"" column in 1.2 (we don't use it but we write it for compatibility sake). 
;;;","13/Mar/13 16:00;enigmacurry;[~slebresne], applying your patch and reproducing the steps above I get this in node2's log (running 1.1.9). The first exception is similar but different, perhaps another field needs migrating? The second exception I'm not sure if it's related or needs another ticket.

{code}
ERROR [MigrationStage:1] 2013-03-13 11:57:13,722 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[MigrationStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.toInt(ByteBufferUtil.java:414)
	at org.apache.cassandra.cql.jdbc.JdbcInt32.compose(JdbcInt32.java:98)
	at org.apache.cassandra.db.marshal.Int32Type.compose(Int32Type.java:37)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getInt(UntypedResultSet.java:97)
	at org.apache.cassandra.config.CFMetaData.fromSchemaNoColumns(CFMetaData.java:1202)
	at org.apache.cassandra.config.CFMetaData.fromSchema(CFMetaData.java:1247)
	at org.apache.cassandra.config.KSMetaData.deserializeColumnFamilies(KSMetaData.java:299)
	at org.apache.cassandra.db.DefsTable.mergeColumnFamilies(DefsTable.java:462)
	at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:374)
	at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:352)
	at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [ReadStage:33] 2013-03-13 11:57:23,794 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[ReadStage:33,5,main]
java.lang.AssertionError: Unknown keyspace system_auth
	at org.apache.cassandra.db.Table.<init>(Table.java:287)
	at org.apache.cassandra.db.Table.open(Table.java:119)
	at org.apache.cassandra.db.Table.open(Table.java:97)
	at org.apache.cassandra.service.RangeSliceVerbHandler.executeLocally(RangeSliceVerbHandler.java:44)
	at org.apache.cassandra.service.RangeSliceVerbHandler.doVerb(RangeSliceVerbHandler.java:63)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code};;;","13/Mar/13 16:43;slebresne;Quick question: does the dtest create new tables in the mixed version cluster? The last NPE seems related to the old CF id removed by CASSANDRA-3794, however we still write the old id for column families that have one in 1.2, so in theory this should only happen if a new column family is created. If that is the case (that the test create a column family in the mixed version cluster), then I'm afraid this is not supported between 1.1 and 1.2 (due to CASSANDRA-3794).

For the 2nd error (the system_auth one), is the dtest doing anything related to authorization in the mixed version cluster? ;;;","13/Mar/13 16:53;enigmacurry;I thought maybe the dtest was doing something I didn't want to have happen, so I'm not using it for this bug report. The two exceptions in my previous comment were from issuing ccm commands directly on the command line exactly as I have in the original report (no dtest used.)

Both exceptions appear in the logs directly after starting the upgraded node. The only queries issued in this test is the stress command before the upgrade, no queries were issued after the upgrade.;;;","13/Mar/13 17:00;enigmacurry;The cassandra.yaml for the 1.1.9 nodes are the default that ccm creates.

Attached is my cassandra.yaml that I use for the upgraded node. This is basically the default one that ccm creates for 1.2.2 modified to use the RandomPartitioner that the node is using and some path fixes.;;;","13/Mar/13 18:17;slebresne;Alright. The problem is that due to CASSANDRA-3794 (and to a lower extend CASSANDRA-4433), new keyspaces/column families cannot be created in a mixed 1.1/1.2 cluster (keyspace lacks the ""name"" column and column families lack an old-style CfId).

This is a limitation we accepted in CASSANDRA-3794, but unfortunately, 1.2 creates the system_auth keyspace/column families on startup if they don't exist which triggers pretty much all the stacktrace on this issue.

One hacky solution I can see is to 1) reintroduce the ""name"" column for keyspace, and 2) we manually provide an old-style cfId for the Auth.USERS_CF. Attaching an updated patch that does both. This is ugly however.
;;;","13/Mar/13 18:37;jbellis;Alternatively, could we hack 1.1.x to ignore the auth CFs?;;;","13/Mar/13 19:37;enigmacurry;I've tested [~slebresne]'s patch; it works to resolve all the NPEs, assuming that's the way we want to go. I'm seeing some other issues after the upgrade, but I'll break those out into separate bugs as I'm pretty sure they're unrelated to this report.

;;;","13/Mar/13 20:05;aleksey;Should MigrationManager.announce() send migrations to 1.1 nodes at all?

{noformat}
            // don't send migrations to the nodes with the versions older than < 1.1
            if (MessagingService.instance().getVersion(endpoint) < MessagingService.VERSION_11)
                continue;
{noformat}

Should change it to '< MessagingService.VERSION_12'. Any downsides to it?;;;","13/Mar/13 20:11;jbellis;That sounds simplest to me, and should fix CASSANDRA-5343.  (IIRC we already have code to prevent 1.1 from pulling the schema from 1.2 nodes, so we just need to stop 1.2 from pushing it as well.);;;","13/Mar/13 20:18;aleksey;v2 prevents 1.2 nodes from pushing migrations to 1.1 nodes.;;;","13/Mar/13 20:55;enigmacurry;Tested [~iamaleksey]'s patch fixes all NPEs and resolves this issue. It mostly fixes CASSANDRA-5343, too, but there is a lingering AsseritionError.;;;","13/Mar/13 21:47;enigmacurry;This is intermittently reoccurring for me, I'll assign it to me until I can come up with a dependable way to reproduce it.;;;","13/Mar/13 21:51;enigmacurry;hmm, looks like the same NPE is coming from a different place than before:

{code}
ERROR [InternalResponseStage:1] 2013-03-13 17:41:01,836 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[InternalResponseStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
        at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
        at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:77)
        at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:97)
        at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:35)
        at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:87)
        at org.apache.cassandra.config.KSMetaData.fromSchema(KSMetaData.java:256)
        at org.apache.cassandra.db.DefsTable.mergeKeyspaces(DefsTable.java:397)
        at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:373)
        at org.apache.cassandra.db.DefsTable.mergeRemoteSchema(DefsTable.java:352)
        at org.apache.cassandra.service.MigrationManager$MigrationTask$1.response(MigrationManager.java:453)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:45)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code};;;","13/Mar/13 22:07;enigmacurry;giving this back to [~iamaleksey] to revisit ""IIRC we already have code to prevent 1.1 from pulling the schema from 1.2 nodes."";;;","13/Mar/13 22:43;aleksey;5334-extra patch modifies 1.2 MigrationRequestVerbHandler to return an empty collection to 1.1 nodes' schema requests.;;;","13/Mar/13 23:07;enigmacurry;Great, 5334-extra seems to clear up the intermittent failures. At least, I ran my dtest 10 times before and it failed half the time, and now I ran it 10 times with no failures.;;;","14/Mar/13 00:05;jbellis;+1;;;","14/Mar/13 00:11;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair -pr with vnodes incompatibilty,CASSANDRA-5329,12636355,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,julien_campan,julien_campan,11/Mar/13 12:38,16/Apr/19 09:32,14/Jul/23 05:53,11/Mar/13 21:24,1.2.3,,,,,,2,,,,,"Hi,

I have a cluster on 1.2.2 .

This cluster is composed of 16 nodes in two datacenters (8 and 8) with an RF 3 :3.

 I used virtual nodes, 256 on each node.

When I do “repair –pr"" on a node, I can see that  it’s doing repair only on the first vnode :

[2013-03-07 14:42:56,922] Starting repair command #7, repairing 1 ranges for keyspace pns_fr

[2013-03-07 14:42:57,835] Repair session eb38dfa0-872c-11e2-af2d-f36fae36bab1 for range (-9064588501660224828,-9063047458265491057] finished

[2013-03-07 14:42:57,835] Repair command #7 finished

[2013-03-07 14:42:57,852] Starting repair command #8, repairing 1 ranges for keyspace hbxtest

[2013-03-07 14:42:59,307] Repair session ebc6c7c0-872c-11e2-af2d-f36fae36bab1 for range (-9064588501660224828,-9063047458265491057] finished

So if  I understand well, when I do a ""repair –pr""  on each node, I will repair only the first vnode on each node. (16 token ranges on 4096 ranges). 

This method doesn’t guarantee the consistency of the dataset.

It seems to me that the ""repair –pr"" is not compatible with vnode cluster.  ",,julien_campan,marco.matarazzo,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Mar/13 19:42;yukim;5329.txt;https://issues.apache.org/jira/secure/attachment/12573156/5329.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,316847,,,Mon Mar 11 21:24:32 UTC 2013,,,,,,,,,,"0|i1io93:",317189,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"11/Mar/13 14:28;marco.matarazzo;I second this, we see exactly the same behaviour.;;;","11/Mar/13 19:42;yukim;We should have used getLocalPrimaryRanges instead of getLocalPrimaryRange for 1.2 and above. Fix attached.;;;","11/Mar/13 20:46;brandon.williams;+1;;;","11/Mar/13 21:24;yukim;Committed. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
MalformedObjectNameException: Invalid character ':' in value part of property in StreamingMetrics for IPv6,CASSANDRA-5328,12636178,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,michalm,michalm,michalm,09/Mar/13 08:36,16/Apr/19 09:32,14/Jul/23 05:53,11/Mar/13 14:17,,,,,,,0,,,,,"See CASSANDRA-5298 for the details - it's the same case, but in different Metrics' class.
Attaching patch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Mar/13 08:37;michalm;5328.patch;https://issues.apache.org/jira/secure/attachment/12572906/5328.patch",,,,,,,,,,,,,,,,,,,,1.0,michalm,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,316670,,,Mon Mar 11 14:17:48 UTC 2013,,,,,,,,,,"0|i1in5r:",317012,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"11/Mar/13 14:17;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Backport on-startup manifest repair to 1.2,CASSANDRA-5327,12636099,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,08/Mar/13 19:00,16/Apr/19 09:32,14/Jul/23 05:53,08/Mar/13 23:43,1.2.3,,,,,,0,compaction,,,,Initially added to trunk for CASSANDRA-4872,,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/13 19:00;jbellis;5327.txt;https://issues.apache.org/jira/secure/attachment/12572798/5327.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,316591,,,Fri Mar 08 23:43:43 UTC 2013,,,,,,,,,,"0|i1imo7:",316933,,,,,,,,,marcuse,,marcuse,Low,,,,,,,,,,,,,,,,,,"08/Mar/13 19:42;marcuse;lgtm;;;","08/Mar/13 23:43;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Tests broken on Java7,CASSANDRA-5315,12635648,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,marcuse,marcuse,06/Mar/13 19:47,16/Apr/19 09:32,14/Jul/23 05:53,06/Mar/13 22:26,1.1.11,,,Legacy/Testing,,,0,,,,,"Tests are broken when running them with java7

Seems to be related to this:
http://intellijava.blogspot.se/2012/05/junit-and-java-7.html
(getDeclaredMethods now returns the methods without any ordering, in java6 it was returned in the same order as they were defined in the .java file)",,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/13 20:13;marcuse;0001-fix-tests-for-java7.patch;https://issues.apache.org/jira/secure/attachment/12572381/0001-fix-tests-for-java7.patch","06/Mar/13 21:32;marcuse;5315-cassandra1.1.patch;https://issues.apache.org/jira/secure/attachment/12572403/5315-cassandra1.1.patch",,,,,,,,,,,,,,,,,,,2.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,316141,,,Wed Mar 06 22:26:04 UTC 2013,,,,,,,,,,"0|i1ijwf:",316484,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"06/Mar/13 20:13;marcuse;this adds @RunWith on all tests that were failing for me, there might be more;;;","06/Mar/13 21:32;marcuse;patch against 1.1;;;","06/Mar/13 22:26;brandon.williams;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Replaying old batches can 'undo' deletes,CASSANDRA-5314,12635489,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,06/Mar/13 04:01,16/Apr/19 09:32,14/Jul/23 05:53,21/May/13 21:42,1.2.6,,,,,,0,,,,,Batchlog manager does not subtract the time spent in the batchlog from hints' ttls and this may cause undoing deletes. The attached patch fixes it.,,aleksey,jjordan,liqusha,slebresne,weideng,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-9917,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,315982,,,Tue May 21 21:42:36 UTC 2013,,,,,,,,,,"0|i1iix3:",316325,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"06/Mar/13 04:16;aleksey;https://github.com/iamaleksey/cassandra/compare/5314;;;","11/Mar/13 21:56;jbellis;Hmm, this is tricky: as you say, we don't want to undo more-recent deletes, but we also don't want to discard other rows in the batch, or even the row with low gcgs, that may be necessary to preserve our Atomic Batch guarantees.  I.e.: replaying the batch may be incorrect, but not replaying it may also be incorrect.

I think the only solution is to require that gcgs be greater than the batch timeout we use.;;;","12/Mar/13 18:10;slebresne;Isn't that a problem we already have with normal hints? After all, if an insert takes a long time to get delivered and you have a short gc_grace, some delete that override an hint could get gced before the hint gets delivered.

In any case, I agree with Jonathan: saying that you shouldn't have a gc_grace too short if you do deletes seems fair to me, in the sense that it's what gc_grace is about: providing some time frame after which you consider everything has been delivered.;;;","12/Mar/13 18:31;jbellis;We TTL hints with gcgs to prevent this, iirc.;;;","13/Mar/13 08:03;slebresne;bq. We TTL hints with gcgs to prevent this

Interesting. There is 2 (somewhat conflicting) things that seems weird to me with that:
# the intent seems broken if gc_grace==0, since a TTL of 0 means no TTL at all.
# in the (not uncommon) case of using TTL on all columns without doing manual deletion, it makes sense to use a very low gc_grace. In this case, this seems to make hints useless, which I'm not sure is what people expect (I'll admit I was unaware of that).;;;","03/May/13 15:21;jbellis;I don't think there's a quick fix here; pushing to 2.0.;;;","07/May/13 14:57;aleksey;So, for now:
1. Apply the original patch (subtract the time spent in the batchlog from hints' ttls)
2. If the target nodes are up (of at least FD says so), try writing the mutations directly instead of hinting them
3. Reduce the batch replay interval to, say, 1 minute
4. Fix hints to not write the hint at all if it's computed TTL == 0 (when gc_grace is 0, for example).

Am I missing anything?;;;","07/May/13 15:00;jbellis;5. Update hint ttl to be sum(gcgs, cf default ttl)

(Updating fix-for back to 1.2);;;","17/May/13 19:00;aleksey;https://github.com/iamaleksey/cassandra/compare/5314  ('push -f'd)

Haven't updated ttl here to be sum (gcgs, def ttl) since 1.2 doesn't have default ttl.;;;","21/May/13 18:27;jbellis;Why do we add this?

{code}
        cf.addColumn(new Column(columnName(""""), ByteBufferUtil.EMPTY_BYTE_BUFFER, timestamp));
{code}

Nit: CopyOnWriteArraySet is a bit of an odd choice, since we expect to mutate it once for each entry.;;;","21/May/13 18:48;aleksey;bq. Why do we add this?

It's the CQL3 row marker. Technically we don't have to do this, but it's just the right way to do it and I would sleep better with it in place. Could prevent something like CASSANDRA-5572 from happening again in the future (although it's true that we only remove the whole rows in the batchlog so the probability of something like this is low).

bq. Nit: CopyOnWriteArraySet is a bit of an odd choice, since we expect to mutate it once for each entry.

It's mostly irrelevant here - just used the first thread-safe set that came to mind.;;;","21/May/13 18:56;jbellis;+1;;;","21/May/13 21:42;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Thrift CQLPreparedResult don't include type arguments (for collection in particular),CASSANDRA-5311,12635162,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,04/Mar/13 17:31,16/Apr/19 09:32,14/Jul/23 05:53,04/Mar/13 17:56,1.2.3,,,,,,0,,,,,,,marcinszymaniuk,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Mar/13 17:34;slebresne;5311.txt;https://issues.apache.org/jira/secure/attachment/12571898/5311.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,315655,,,Mon Mar 04 17:56:58 UTC 2013,,,,,,,,,,"0|i1igwf:",315998,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"04/Mar/13 17:34;slebresne;Trivial patch attached.;;;","04/Mar/13 17:42;jbellis;+1;;;","04/Mar/13 17:56;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Add a note to cassandra-cli ""show schema"" explaining that it is cql-oblivious",CASSANDRA-5309,12635109,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,jayadevan.maymala@ibsplc.com,jayadevan.maymala@ibsplc.com,04/Mar/13 11:37,16/Apr/19 09:32,14/Jul/23 05:53,05/Mar/13 04:05,1.2.3,,,Legacy/Tools,,,0,,,,,"If I create a table using cqlsh, that table  does not show when I do a describe in cassandra-cli. For example, I create a table in cqlsh.
cqlsh:system> CREATE KEYSPACE testkp WITH  replication =  {'class':'SimpleStrategy', 'replication_factor':2};

cqlsh:testkp> CREATE TABLE test (
          ...   k int PRIMARY KEY,
          ...   v1 int,
          ...   v2 int
          ... );
cqlsh:testkp>

In cassandra-cli, I get this 

[default@testkp] show schema;
create keyspace testkp
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = {replication_factor : 2}
  and durable_writes = true;

use testkp;




[default@testkp] describe;
Keyspace: testkp:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:2]
  Column Families:

No Column Family is shown. But if I do 
[default@testkp] create column family test;
Cannot add already existing column family ""test"" to keyspace ""testkp""
[default@testkp] list test;
Using default limit of 100
Using default column limit of 100
-------------------
RowKey: 1
=> (column=, value=, timestamp=1362395851188000)
=> (column=v1, value=00000002, timestamp=1362395851188000)
=> (column=v2, value=00000003, timestamp=1362395851188000)

1 Row Returned.
Elapsed time: 105 msec(s).

So obviously the table/column family is there.","Linus (CentOS), 64 bit.",aleksey,jayadevan.maymala@ibsplc.com,karpa13a,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5320,,,,,,,,,,,,,,,,,,,,"05/Mar/13 01:01;aleksey;5309.txt;https://issues.apache.org/jira/secure/attachment/12572006/5309.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,315602,,,Tue Mar 05 04:04:34 UTC 2013,,,,,,,,,,"0|i1igkn:",315945,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"04/Mar/13 14:02;jbellis;We should build a notice into the cli since this keeps coming up.;;;","04/Mar/13 14:23;karpa13a;created CF with cqlsh -3 not visible(and unusable) in thrift api (phpcassa and cassandra-cli)
but CF created in phpcassa/cassandra-cli visible and accessible in cqlsh -3 and phpcassa/cassandra-cli
C* 1.2.2

related to this ticket?
or this is feature?;;;","04/Mar/13 14:26;aleksey;This is the intended behavior.;;;","05/Mar/13 03:12;jbellis;+1;;;","05/Mar/13 03:24;aleksey;Committed.;;;","05/Mar/13 03:30;karpa13a;may be add this warning after 'create column family ...' statement in cqlsh -3 too?;;;","05/Mar/13 03:37;jayadevan.maymala@ibsplc.com;I am a newcomer in the world of NoSQL databases. Whether we create a column family using cli or a table using cqlsh, aren't the same things happening under the covers? So why is an object created using one interface not visible in the other?;;;","05/Mar/13 03:42;aleksey;[~jayadevan.maymala@ibsplc.com] CASSANDRA-4377;;;","05/Mar/13 03:52;brandon.williams;Hang on, printing to stdout means you can no longer round-trip a 'show schema' through a file.  stderr seems a better choice.;;;","05/Mar/13 04:04;aleksey;bq. Hang on, printing to stdout means you can no longer round-trip a 'show schema' through a file. stderr seems a better choice.

This is good - this way when schema fails to replay they'll definitely notice the warning.
Just kidding - fixed in b1abbf2d05d8cdf46c08a7a6e5289c9ade98dd3b. ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
clqsh COPY is broken after strictening validation in 1.2.2 (quotes ints),CASSANDRA-5305,12634867,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,01/Mar/13 17:41,16/Apr/19 09:32,14/Jul/23 05:53,11/Mar/13 20:13,1.2.3,,,Legacy/Tools,,,0,cqlsh,,,,"cqlsh COPY is quoting values when it shouldn't, and that's throwing IRE in 1.2.2.",,aleksey,marcinszymaniuk,pushkarp,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,315360,,,Mon Mar 11 20:13:33 UTC 2013,,,,,,,,,,"0|i1if3b:",315704,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"02/Mar/13 07:51;pushkarp;Any workaround for this issue? Not being able to use COPY cmd makes things difficult..;;;","02/Mar/13 16:13;aleksey;Not yet - fixing this turned out to be trickier than expected, but I'm on it.;;;","06/Mar/13 23:46;aleksey;https://github.com/iamaleksey/cassandra/compare/5305

Updated COPY FROM to only quote ascii, text, timestamp, and inet values.
This also required changing the way we treat csv null, since we can no longer insert '' into, say, an int column.

Empty string (default null representation) is treated as a null now - the way PostgreSQL does it (""When using COPY FROM, any data item that matches this string will be stored as a null value, so you should make sure that you use the same string as you used with COPY TO."")

And since we need metadata from CQL3 system.schema_columns and system.schema_columnfamilies now, COPY requires cqlsh to be run in CQL3 mode (which still allows importing/exporting thrift/cql2 cfs, just using CQL3).;;;","07/Mar/13 20:51;aleksey;Actually, I'm not sure if this treatment of nulls is enough or if we should go further and issue DELETEs for all the columns that are null (since we can't INSERT null). The suggested implementation simply omits these columns from the import INSERT queries.;;;","07/Mar/13 20:52;aleksey;Yeah, I think that's how it used to work back when '' was a valid value for all the data types. Issuing DELETEs is in line with the old implementation.;;;","09/Mar/13 17:37;aleksey;Added proper null handling in the second commit (https://github.com/iamaleksey/cassandra/compare/5305);;;","09/Mar/13 17:39;aleksey;I think the only non-working part now is import of collections. But that can wait.;;;","11/Mar/13 19:26;brandon.williams;+1;;;","11/Mar/13 20:13;aleksey;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Insufficient validation of UPDATE queries against counter cfs,CASSANDRA-5300,12634654,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,aleksey,aleksey,28/Feb/13 18:19,16/Apr/19 09:32,14/Jul/23 05:53,01/Mar/13 13:30,1.2.3,,,,,,0,,,,,"{noformat}
CREATE TABLE demo (
  id int PRIMARY KEY,
  c counter
)
{noformat}

This is expected:
{noformat}
insert into demo (id, c) VALUES ( 0, 20);
Bad Request: INSERT statement are not allowed on counter tables, use UPDATE instead
{noformat}

This should also be forbidden, but it is not:
{noformat}
update demo set c = 20 where id = 0;
select * from demo;

 id | c
----+----
  0 | 20
{noformat}",,aleksey,ggargani,marco.matarazzo,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"01/Mar/13 10:28;slebresne;5300-v2.txt;https://issues.apache.org/jira/secure/attachment/12571561/5300-v2.txt","28/Feb/13 19:41;aleksey;5300.txt;https://issues.apache.org/jira/secure/attachment/12571453/5300.txt",,,,,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,315147,,,Fri Mar 01 13:30:26 UTC 2013,,,,,,,,,,"0|i1idrz:",315491,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"28/Feb/13 18:24;aleksey;1.1 is not affected - this is a 1.2 regression.

{noformat}
update demo set c = 20 where id = 0;
Bad Request: invalid operation for commutative columnfamily demo
{noformat};;;","01/Mar/13 10:00;marco.matarazzo;I was hoping this was a feature, and was about to be extended to INSERT too, it can be very useful during table initialization/import using COPY FROM on cqlsh or just a bunch of saved CQL.
;;;","01/Mar/13 10:04;ggargani;I welcomed it as nice feature, too. Forcing a defined value in a counter does have uses (i.e., when you have a cap to the value a counter can reach);;;","01/Mar/13 10:28;slebresne;I'd prefer doing the validation in Operation.SetValue directly because that where we have all the other type validation now (and it's cleaner that way imo), as done in the attached v2.

bq. it can be very useful during table initialization/import

For the record, nobody contesting being able to set a counter would be useful.  But it doesn't work and I'm pretty sure it cannot work with the current implementation of counters. Don't be fooled by the example above, it's not complete. If you try an increment after the last update, you will end up with a server side exception because you've basically corrupted your data.
;;;","01/Mar/13 12:39;aleksey;bq. I'd prefer doing the validation in Operation.SetValue directly because that where we have all the other type validation now (and it's cleaner that way imo), as done in the attached v2.

wfm, +1;;;","01/Mar/13 13:30;slebresne;v2 commmitted, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
isLocalDc() in OutboundTcpConnection class retrieves local IP in wrong way,CASSANDRA-5299,12634619,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,marcuse,michalm,michalm,28/Feb/13 14:49,16/Apr/19 09:32,14/Jul/23 05:53,01/Mar/13 01:30,1.2.3,,,,,,0,,,,,"My question from dev mailing list:

bq. Can someone explain me why isLocalDc() in OutboundTcpConnection class uses DatabaseDescriptor.getRpcAddress() for retrieving ""local"" IP, instead of using DD.getListenAddress() or - even better - FBUtilities.getLocalAddress()? I mean - I don't get why RPC address is checked before initializing internode (so not RPC-based)  communication, which will use IP address returned by (in OTCPool) FBUtilities.getLocalAddress()?

And response by Marcus Eriksson:

bq. That should probably be FBUtilities.getBroadCastAddress even, could you file a ticket?
",,marcuse,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/13 16:11;marcuse;0001-CASSANDRA-5299-use-broadCastAddress-when-figuring-ou.patch;https://issues.apache.org/jira/secure/attachment/12571424/0001-CASSANDRA-5299-use-broadCastAddress-when-figuring-ou.patch",,,,,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,315112,,,Fri Mar 01 01:30:46 UTC 2013,,,,,,,,,,"0|i1idk7:",315456,,,,,,,,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Normal,,,,,,,,,,,,,,,,,,"28/Feb/13 16:11;marcuse;use broadCastAddress when deciding local datacenter;;;","28/Feb/13 19:21;vijay2win@yahoo.com;Oops +1;;;","01/Mar/13 01:30;vijay2win@yahoo.com;Committed to 1.2 and trunk, Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"MalformedObjectNameException in ConnectionMetrics for IPv6 nodes because of "":"" characters",CASSANDRA-5298,12634583,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,michalm,michalm,michalm,28/Feb/13 09:59,16/Apr/19 09:32,14/Jul/23 05:53,04/Mar/13 10:46,1.2.3,,,,,,0,,,,,"After upgrading node to 1.2.1, during C* startup, for all ConnectionMetrics I get exception like this one:

{noformat} WARN [GossipStage:1] 2013-02-27 12:14:55,431 JmxReporter.java (line 388) Error processing org.apache.cassandra.metrics:type=Connection,scope=2001:4c28:20:177:0:1:2:4,name=Timeouts
javax.management.MalformedObjectNameException: Invalid character ':' in value part of property
	at javax.management.ObjectName.construct(ObjectName.java:602)
	at javax.management.ObjectName.<init>(ObjectName.java:1403)
	at com.yammer.metrics.reporting.JmxReporter.onMetricAdded(JmxReporter.java:386)
	at com.yammer.metrics.core.MetricsRegistry.notifyMetricAdded(MetricsRegistry.java:516)
	at com.yammer.metrics.core.MetricsRegistry.getOrAdd(MetricsRegistry.java:491)
	at com.yammer.metrics.core.MetricsRegistry.newMeter(MetricsRegistry.java:240)
	at com.yammer.metrics.Metrics.newMeter(Metrics.java:245)
	at org.apache.cassandra.metrics.ConnectionMetrics.<init>(ConnectionMetrics.java:102)
	at org.apache.cassandra.net.OutboundTcpConnectionPool.<init>(OutboundTcpConnectionPool.java:53)
	at org.apache.cassandra.net.MessagingService.getConnectionPool(MessagingService.java:481)
	at org.apache.cassandra.net.MessagingService.getConnection(MessagingService.java:489)
	at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:612)
	at org.apache.cassandra.net.MessagingService.sendOneWay(MessagingService.java:581)
	at org.apache.cassandra.gms.GossipDigestSynVerbHandler.doVerb(GossipDigestSynVerbHandler.java:85)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

(...){noformat}

Looking at ObjectName source code (e.g. http://grepcode.com/file/repository.grepcode.com/java/root/jdk/openjdk/6-b14/javax/management/ObjectName.java ) I can see that "":"" is not a valid character, so my idea for solving this problem is to use one of the following:

1) URLEncode.encode() - seems to be more ""proper"" solution, but produces a bit unreadable metric scope like: MBean org.apache.cassandra.metrics:type=Connection,scope=2001%3A4c28%3A10%3A168%3A0%3A2%3A3%3A6,name=CommandCompletedTasks

2) <String>.replaceAll() - we can simply replace "":"" with ""."" which wouldn't give us valid IPv6 address, but will be much more readable. 

Second one seems to be a better choice for me, but I attach two patches.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Feb/13 10:00;michalm;ipv6-connection-metrics-URLEncoder.patch;https://issues.apache.org/jira/secure/attachment/12571394/ipv6-connection-metrics-URLEncoder.patch","28/Feb/13 10:00;michalm;ipv6-connection-metrics-replaceAll.patch;https://issues.apache.org/jira/secure/attachment/12571393/ipv6-connection-metrics-replaceAll.patch",,,,,,,,,,,,,,,,,,,2.0,michalm,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,315076,,,Mon Mar 04 10:46:21 UTC 2013,,,,,,,,,,"0|i1idc7:",315420,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"01/Mar/13 06:50;michalm;D'oh, I attached the files, but did not mark this issue as ""Patch available""!;;;","04/Mar/13 10:46;jbellis;committed (and added a comment for) the replaceAll patch.  thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 shouldn't lowercase DC names for NTS,CASSANDRA-5292,12634202,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,26/Feb/13 17:16,16/Apr/19 09:32,14/Jul/23 05:53,27/Feb/13 14:46,1.2.3,,,,,,0,,,,,"In CREATE and ALTER statements, when a property map is given (replication, compaction and compression options), CQL3 lowercase the map keys to provide case insensitivity. The goal is to allow things like:
{noformat}
  replication = { 'Class' : 'SimpleStrategy', 'Replication_factor' : '1' }
{noformat}

However, this messes up with NTS, as it ends up lower-casing the datacenter names. As a consequence,
{noformat}
  replication = { 'class' : 'NetworkTopologyStrategy', 'DC1' : '1' }
{noformat}
will currently create a DC that is really called 'dc1', which is a problem because DC names are interpreted case sensitively otherwise (at least in PropertyFileSnitch).

That's the problem. Now I'm kind of hesitant on what is the right fix. I see the following possibilities:
# Remove the CQL3 lower-casing completely. I'll admit that providing case insensitivy for property map keys may not have been such a good idea in the first place. After all, those map keys are string literal, which rather suggest case sensitivity. However, making that change would be a break strictly speaking.
# Make DC name case insensitive. As much as I think DC names ought to be case insensitive, I'm not sure that's very doable in practice because that would imply storing DC names lower-cased internally, but DC names are exchanged over gossip and whatnot, so that would probably break all hell loose.
# Keep CQL3 case insensitivity for property map keys in general but special case internally for NTS. The problems I see with that is that 1) this will be ugly and 2) if we special case too much, we might break potential custom strategies inspired by NTS. I also had the idea of changing strategy options internally from Map<String, String> to some custom object that would be essentially a case insensitive string map (general case), but that would also hold the original case of keys so NTS (and any other likely-minded strategy) can do its stuff. This happens to not be a small patch however (I'm attaching the patch for reference because I wrote it, but I'm seriously wondering if it's not too overkill).
",,marcinszymaniuk,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/13 17:17;slebresne;5292-option3.txt;https://issues.apache.org/jira/secure/attachment/12571005/5292-option3.txt","26/Feb/13 18:06;slebresne;5292.txt;https://issues.apache.org/jira/secure/attachment/12571012/5292.txt",,,,,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,314695,,,Wed Feb 27 14:46:50 UTC 2013,,,,,,,,,,"0|i1iazj:",315039,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"26/Feb/13 17:38;jbellis;IMO there are two principles we should be following here:

# schema options should be ""just a Map.""  This is the most straightforward and trying to be clever here will surprise people more than it will help.  Nor do I want to maintain ""almost-Map"" structures in the code.
# CQL should not ""helpfully"" lowercase Map keys.

My understanding is that we observe #2 elsewhere, but schema options are not-quite-a-Map in this respect.  In that case I think we should suck it up and take the backwards incompatibility to fix this.

If we violate #2 in general, that is a bitter pill to swallow, but it's still a bug that should be fixed.;;;","26/Feb/13 18:06;slebresne;bq. My understanding is that we observe #2 elsewhere

Yes, we do.

bq. In that case I think we should suck it up and take the backwards incompatibility to fix this.

Yeah, I think I agree really. I'm attaching the trivial patch to do that.

bq. Nor do I want to maintain ""almost-Map"" structures in the code.

For the record, schema options and ""normal"" map don't really share any code outside the parser. But I agree on the rest, just saying :)
;;;","26/Feb/13 19:10;jbellis;bq. For the record, schema options and ""normal"" map don't really share any code outside the parser. But I agree on the rest, just saying

Right.  But we're going to fix that, right? :)

+1 on the trivial patch.;;;","27/Feb/13 14:46;slebresne;Alright, committed the trivial fix then. Thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
DatabaseDescriptor.hasExistingNoSystemTables return true even with only system keyspace,CASSANDRA-5289,12634066,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,cywjackson,cywjackson,26/Feb/13 05:43,16/Apr/19 09:32,14/Jul/23 05:53,07/Mar/13 21:18,,,,,,,0,,,,,"The hasExistingNoSystemTables method in DatabaseDescriptor checks for directory only. 

On a new start, system KS would be created. This method current return true because of it, resulting incorrect/confusing log:

 logger.info(""Found table data in data directories. Consider using the CLI to define your schema."")     ;",,cywjackson,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Feb/13 05:47;cywjackson;5289.diff;https://issues.apache.org/jira/secure/attachment/12570933/5289.diff",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,314559,,,Thu Mar 07 21:18:03 UTC 2013,,,,,,,,,,"0|i1ia5b:",314903,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"26/Feb/13 05:47;cywjackson;patch to add check the name not equal to Table.SYSTEM_KS

patch based on 1.1 branch

note for trunk, Table.SYSTEM_KS is changed to Table.SYSTEM_TABLE
;;;","04/Mar/13 13:18;jasobrown;Honestly, you could debate whether we need that check at all. In either case, all we do after calling hasExistingNoSystemTables() is tell the user to create some CFs - I think that should be obvious by now :). I think we can just eliminate that method and the not overwhelmingly helpful logging, and have startup be nanoseonds more efficient.;;;","07/Mar/13 01:29;cywjackson;I understand your point. 

We had a EC2 node outage last week, and subsequently the ops team rebuilt a node to replace it. 

However, I am not part of the ops team and so wasn't aware of such happened. But I was reviewing the logs for other issues, and reaching here was really confusing. (i saw node was started with new token indicating a new start, but then saw the log msg seems suggesting there were data).

Hence I would prefer keep the log but fix the bug to make it clear, instead of seeking for that sub nanoseconds improvement. But I would agree taking the log out would make it less confusing and gain that sub nanoseconds improvement.

Either way, your call :);;;","07/Mar/13 21:18;jbellis;I still see people copying their data files to a new cluster and wondering why Cassandra reports no table definitions.  So I'd prefer to fix the message than remove it.

Jackson's fix LGTM.  Committed, and in 1.2+ fixed the method name typo (should be Non-.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
stress percentile label does not match what is returned,CASSANDRA-5288,12634010,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,cburroughs,cburroughs,25/Feb/13 22:33,16/Apr/19 09:32,14/Jul/23 05:53,28/Aug/13 03:46,1.2.10,2.0.1,,Legacy/Tools,,,0,,,,,"We say it's the 99th: https://github.com/apache/cassandra/blob/trunk/tools/stress/src/org/apache/cassandra/stress/StressAction.java#L65

But return 99.9th https://github.com/apache/cassandra/blob/trunk/tools/stress/src/org/apache/cassandra/stress/StressAction.java#L145

Not sure which is intended.",,cburroughs,enigmacurry,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5944,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,314504,,,Wed Aug 28 03:46:04 UTC 2013,,,,,,,,,,"0|i1i9t3:",314848,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"26/Aug/13 13:29;cburroughs;[~Ryan McGuire] I think you might have originally added these in CASSANDRA-5243.  Could you comment on if 99 or 99.9 was intended?  I'd also like to fix the header to it uses all commas for easier parsing while we are at it.;;;","27/Aug/13 20:11;enigmacurry;As far as I know, it's actually reporting 99.9th percentile, so I agree, the heading is mislabled.;;;","27/Aug/13 20:14;enigmacurry;As in, we should change the heading, not the statistic. Too many things rely on that number the way it is.;;;","28/Aug/13 03:46;jbellis;Ninja'd.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Composite comparators for super CFs broken in 1.2,CASSANDRA-5287,12633993,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,thobbs,thobbs,25/Feb/13 21:03,16/Apr/19 09:32,14/Jul/23 05:53,28/Feb/13 13:11,1.2.3,,,,,,0,,,,,"In Cassandra 1.2.0 through 1.2.2, attempting to insert data into a super column family with a CompositeType comparator results in the following stacktrace:

{noformat}
ERROR 14:56:49,920 Error occurred during processing of message.
java.lang.IllegalArgumentException
	at java.nio.Buffer.limit(Buffer.java:249)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getBytes(AbstractCompositeType.java:51)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getWithShortLength(AbstractCompositeType.java:60)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.split(AbstractCompositeType.java:126)
	at org.apache.cassandra.config.CFMetaData.getColumnDefinitionFromColumnName(CFMetaData.java:920)
	at org.apache.cassandra.thrift.ThriftValidation.validateColumnData(ThriftValidation.java:404)
	at org.apache.cassandra.thrift.ThriftValidation.validateColumnOrSuperColumn(ThriftValidation.java:287)
	at org.apache.cassandra.thrift.ThriftValidation.validateMutation(ThriftValidation.java:343)
	at org.apache.cassandra.thrift.CassandraServer.createMutationList(CassandraServer.java:704)
	at org.apache.cassandra.thrift.CassandraServer.batch_mutate(CassandraServer.java:752)
	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.getResult(Cassandra.java:3622)
	at org.apache.cassandra.thrift.Cassandra$Processor$batch_mutate.getResult(Cassandra.java:3610)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

Client-side, you'll just see ""{{TTransportException: TSocket read 0 bytes}}"".

Cassandra 1.1 doesn't have the same problem.",,marcinszymaniuk,slebresne,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/13 21:04;thobbs;5287-pycassa-repro.py;https://issues.apache.org/jira/secure/attachment/12570859/5287-pycassa-repro.py","27/Feb/13 18:45;slebresne;5287.txt;https://issues.apache.org/jira/secure/attachment/12571242/5287.txt",,,,,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,314487,,,Thu Feb 28 13:11:29 UTC 2013,,,,,,,,,,"0|i1i9pb:",314831,,,,,,,,,thobbs,,thobbs,Normal,,,,,,,,,,,,,,,,,,"25/Feb/13 21:04;thobbs;Attached script reproduces the issue with pycassa.;;;","26/Feb/13 18:20;slebresne;Patch attached (I haven't tested it yet tbh but pretty sure that's *a* problem if not *the* problem).;;;","27/Feb/13 18:40;thobbs;Is the patch supposed to be against the 1.2 branch? It doesn't compile there.;;;","27/Feb/13 18:46;slebresne;Hum, that's what you get for not even compiling before attaching a patch I suppose. The newly attached version should work better. ;;;","27/Feb/13 19:29;thobbs;The patch seems fine to me, but I'm not terribly familiar with this area of the code.  With it applied, all of the pycassa and phpcassa tests pass, and they cover this pretty well.;;;","28/Feb/13 13:11;slebresne;bq. The patch seems fine to me, but I'm not terribly familiar with this area of the code.

For the record, what happens is that in CQL3 you can have a ColumnDefinition that applies to only a component of the column name, hence the special code for composite in CFMetadata.getColumnDefinitionFromColumnName. However, for super columns we pass to this method the subcolumn name, so using the comparator is bogus. But since super columns don't exist in CQL3, and since thrift don't allow to define ColumnDefinition that applies to only sub-component of composites, the patch just always use the 'else' branch that does the right thing.

bq. all of the pycassa and phpcassa tests pass, and they cover this pretty well

Alright, I'll consider that +1 enough :). So committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
PropertyFileSnitch default DC/Rack behavior is broken in 1.2,CASSANDRA-5285,12633955,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,25/Feb/13 17:46,16/Apr/19 09:32,14/Jul/23 05:53,01/Mar/13 16:05,1.2.3,,,,,,0,,,,,"CASSANDRA-4728 added a check that the local node was in the property file but didn't took a potential default into account.

Typically, using the shipped cassandra-topology.properties (that has a default DC and rack) with a node on localhost raises a ConfigurationException on startup).",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/13 17:56;slebresne;5285.txt;https://issues.apache.org/jira/secure/attachment/12570825/5285.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,314449,,,Fri Mar 01 16:05:52 UTC 2013,,,,,,,,,,"0|i1i9gv:",314793,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"25/Feb/13 17:56;slebresne;Attaching simple fix.;;;","01/Mar/13 14:55;jbellis;+1;;;","01/Mar/13 16:05;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Possible assertion triggered in SliceFromReadCommand,CASSANDRA-5284,12633925,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,25/Feb/13 15:54,16/Apr/19 09:32,14/Jul/23 05:53,26/Feb/13 10:03,1.1.11,,,,,,0,,,,,"In SliceFromReadCommand.maybeGenerateRetryCommand, the following assertion
{noformat}
assert maxLiveColumns <= count;
{noformat}
may actually fail. Namely, it asserts that no node has returned more columns that what was asked for, which in general is true, but can not be if an expiring column is counted as dead by the replica (but still send as a tombstone) but, due to clock difference, is actually counted live by the coordinator.

I note that this is similar to CASSANDRA-5149 in that fixing CASSANDRA-5149 would fix this too, but in the meantime, this edge case is harmless so there is probably not much point in keeping the assertion.",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Feb/13 16:02;slebresne;5284.txt;https://issues.apache.org/jira/secure/attachment/12570806/5284.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,314419,,,Tue Feb 26 10:03:34 UTC 2013,,,,,,,,,,"0|i1i9af:",314764,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"25/Feb/13 19:06;jbellis;+1;;;","26/Feb/13 10:03;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes with Ec2Snitch and Ec2MultiRegionSnitch cannot start due to cassandra-rackdc.properties not getting installed by packages!,CASSANDRA-5281,12633735,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,,arya,arya,23/Feb/13 05:44,16/Apr/19 09:32,14/Jul/23 05:53,23/Feb/13 06:05,,,,,,,0,,,,,The patch in CASSANDRA-5155 breaks the node startup if cassandra-rackdc.properties is not present for nodes using Ec2 and Ec2MultiRegion snitches as a RuntimeException is thrown in SnitchProperties.java. I think we should include cassandra-rackdc.properties in the Debian package or have the default empty property being returned on FileNotFoundException. What do you think?,"Cassandra 1.2.1
Ubuntu 10.04",arya,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,314230,,,Sat Feb 23 06:05:24 UTC 2013,,,,,,,,,,"0|i1i84f:",314575,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"23/Feb/13 05:47;arya;Also it would be nice to include a note in change log or upgrade notes for this.;;;","23/Feb/13 05:50;arya;I noticed that the rackdc properties file is in the source, but it doesn't get built into the debian package. Not sure about the CentOS package.;;;","23/Feb/13 06:05;arya;This appears to be fixed in trunk. Thanks Vijay.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hanging system after OutOfMemory. Server cannot die due to uncaughtException handling,CASSANDRA-5273,12633165,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,ignaced,ignaced,20/Feb/13 10:32,16/Apr/19 09:32,14/Jul/23 05:53,22/May/13 15:33,1.2.6,,,,,,0,,,,,"On out of memory exception, there is an uncaughtexception handler that is calling System.exit(). However, multiple threads are calling this handler causing a deadlock and the server cannot stop working. See http://www.mail-archive.com/user@cassandra.apache.org/msg27898.html. And see stack trace in attachement.","linux, 64 bit",ignaced,marcuse,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Apr/13 10:00;marcuse;0001-CASSANDRA-5273-add-timeouts-to-the-blocking-commitlo.patch;https://issues.apache.org/jira/secure/attachment/12580265/0001-CASSANDRA-5273-add-timeouts-to-the-blocking-commitlo.patch","24/Apr/13 07:44;marcuse;0001-CASSANDRA-5273-add-timeouts-to-the-blocking-commitlo.patch;https://issues.apache.org/jira/secure/attachment/12580250/0001-CASSANDRA-5273-add-timeouts-to-the-blocking-commitlo.patch","22/May/13 13:59;jbellis;5273-v2.txt;https://issues.apache.org/jira/secure/attachment/12584307/5273-v2.txt","22/May/13 15:27;jbellis;5273-v3.txt;https://issues.apache.org/jira/secure/attachment/12584316/5273-v3.txt","20/Feb/13 10:46;ignaced;CassHangs.txt;https://issues.apache.org/jira/secure/attachment/12570109/CassHangs.txt",,,,,,,,,,,,,,,,5.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,313661,,,Wed May 22 15:33:04 UTC 2013,,,,,,,,,,"0|i1i4lz:",314006,,,,,,,,,marcuse,,marcuse,Low,,,,,,,,,,,,,,,,,,"20/Feb/13 10:46;ignaced;Stack trace hanging system;;;","23/Mar/13 21:58;jbellis;Do we have any better options than just adding a timeout to the appendingThread.join call?;;;","24/Apr/13 07:44;marcuse;this is very hard to reproduce (and i have never seen it happen in any of our clusters) so i simply added timeouts to the Thread#join methods in CommitLog#shutdownBlocking;;;","24/Apr/13 07:56;ignaced;Just an idea : one could say that the problem is caused by the java runtime that is holding a lock during System.exit(). At the same time, the cassandra code (the uncaught exception handler) is potentially calling System.exit() many times. Would it not be more safe and clean for the code in the handler to call at most once System.exit(), avoiding the jre lock and letting everything die in a 'normal' way?;;;","24/Apr/13 10:00;marcuse;adds a lock to make sure only one thread calls System.exit

the threads that would have called System.exit will block (unclear (to me) what would happen if they would be allowed to continue to run where they should have exited);;;","29/Apr/13 14:29;jbellis;bq. the threads that would have called System.exit

I don't think we're very rigorous about calling Thread.setDaemon, so I think this will actually deadlock it -- System.exit will wait for daemon threads to die, and the daemon threads will park at the lock acquisition.;;;","21/May/13 15:48;marcuse;[~jbellis] you think the timeouts would be enough?;;;","22/May/13 13:59;jbellis;Thinking about it more, I think adding a lock doesn't change anything.  System.exit already locks/synchronizes the important parts.  So we still have the deadlock problem, which we can hack around with timeouts but I'd rather not.

Patch attached against 1.2 to call System.exit from a new thread instead.;;;","22/May/13 15:27;jbellis;v3 preallocates the Thread.;;;","22/May/13 15:29;marcuse;lgtm;;;","22/May/13 15:33;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove token generator,CASSANDRA-5261,12632530,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,snazy,jbellis,jbellis,15/Feb/13 05:44,16/Apr/19 09:32,14/Jul/23 05:53,26/Oct/15 12:40,3.0.0,,,Legacy/Tools,,,0,triaged,,,,Obsoleted by vnodes,,aleksey,cburroughs,mishail,philipthompson,snazy,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,snazy,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,313026,,,Mon Oct 26 12:40:49 UTC 2015,,,,,,,,,,"0|i1i0p3:",313372,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"26/Oct/15 12:23;aleksey;+1, but don't forget to add a NEWS.txt entry.;;;","26/Oct/15 12:40;snazy;Thanks!
Committed as db7feb4c252f3ed12fbc40beb2d465a7070f9b0d;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"""Memory was freed"" AssertionError During Major Compaction",CASSANDRA-5256,12632473,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,jbellis,cscotta,cscotta,14/Feb/13 21:17,16/Apr/19 09:32,14/Jul/23 05:53,20/Feb/13 13:39,1.2.2,,,,,,1,compaction,,,,"When initiating a major compaction with `./nodetool -h localhost compact`, an AssertionError is thrown in the CompactionExecutor from o.a.c.io.util.Memory:

ERROR [CompactionExecutor:41495] 2013-02-14 14:38:35,720 CassandraDaemon.java (line 133) Exception in thread Thread[CompactionExecutor:41495,1,RMI Runtime]
java.lang.AssertionError: Memory was freed
  at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:146)
	at org.apache.cassandra.io.util.Memory.getLong(Memory.java:116)
	at org.apache.cassandra.io.compress.CompressionMetadata.chunkFor(CompressionMetadata.java:176)
	at org.apache.cassandra.io.compress.CompressedRandomAccessReader.reBuffer(CompressedRandomAccessReader.java:88)
	at org.apache.cassandra.io.util.RandomAccessReader.read(RandomAccessReader.java:327)
	at java.io.RandomAccessFile.readInt(RandomAccessFile.java:755)
	at java.io.RandomAccessFile.readLong(RandomAccessFile.java:792)
	at org.apache.cassandra.utils.BytesReadTracker.readLong(BytesReadTracker.java:114)
	at org.apache.cassandra.db.ColumnSerializer.deserializeColumnBody(ColumnSerializer.java:101)
	at org.apache.cassandra.db.OnDiskAtom$Serializer.deserializeFromSSTable(OnDiskAtom.java:92)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumnsFromSSTable(ColumnFamilySerializer.java:149)
	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.getColumnFamilyWithColumns(SSTableIdentityIterator.java:235)
	at org.apache.cassandra.db.compaction.PrecompactedRow.merge(PrecompactedRow.java:109)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:93)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:162)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:76)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:57)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:114)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:97)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:158)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:71)
	at org.apache.cassandra.db.compaction.CompactionManager$6.runMayThrow(CompactionManager.java:342)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

---

I've invoked the `nodetool compact` three times; this occurred after each. The node has been up for a couple days accepting writes and has not been restarted.

Here's the server's log since it was started a few days ago: https://gist.github.com/cscotta/4956472/raw/95e7cbc68de1aefaeca11812cbb98d5d46f534e8/cassandra.log

Here's the code being used to issue writes to the datastore: https://gist.github.com/cscotta/20cbd36c2503c71d06e9

---

Configuration: One node, one keyspace, one column family. ~60 writes/second of data with a TTL of 86400, zero reads. Stock cassandra.yaml.

Keyspace DDL:

create keyspace jetpack;
use jetpack;
create column family Metrics with key_validation_class = 'UTF8Type' and comparator = 'IntegerType';","Linux ashbdrytest01p 3.2.0-37-generic #58-Ubuntu SMP Thu Jan 24 15:28:10 UTC 2013 x86_64 x86_64 x86_64 GNU/Linux

java version ""1.6.0_30""
Java(TM) SE Runtime Environment (build 1.6.0_30-b12)
Java HotSpot(TM) 64-Bit Server VM (build 20.5-b03, mixed mode)

Ubuntu 12.04.2 LTS",cscotta,jdilloyd,marcinszymaniuk,ngrigoriev@gmail.com,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-8134,,,,,,,,,,,"15/Feb/13 05:00;jbellis;5256-v2.txt;https://issues.apache.org/jira/secure/attachment/12569459/5256-v2.txt","16/Feb/13 06:04;jbellis;5256-v4.txt;https://issues.apache.org/jira/secure/attachment/12569646/5256-v4.txt","19/Feb/13 14:54;jbellis;5256-v5.txt;https://issues.apache.org/jira/secure/attachment/12569947/5256-v5.txt","15/Feb/13 01:04;jbellis;5256.txt;https://issues.apache.org/jira/secure/attachment/12569431/5256.txt",,,,,,,,,,,,,,,,,4.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,312969,,,Wed Oct 29 15:16:08 UTC 2014,,,,,,,,,,"0|i1i0cf:",313315,,,,,,,,,slebresne,,slebresne,Critical,,,,,,,,,,,,,,,,,,"14/Feb/13 21:23;jbellis;Some race is allowing sstables to participate in multiple concurrent compactions; when the first finishes, it frees the compression metadata, which causes the above AssertionError in the other.  Look at sstable 2110 for example.;;;","14/Feb/13 21:25;jbellis;Is this LeveledCompactionStrategy or SizeTiered?;;;","14/Feb/13 21:56;brandon.williams;It's STS.;;;","15/Feb/13 00:59;jbellis;compactionLock is ugly and confusing but I think the logic is correct as far as it goes.  I think the only problem is that STCS.getMaximalTask doesn't mark its victims as compacting, so a minor compaction is free to attempt them as well.

There is a similar problem with STCS.getUserDefinedCompaction.  Both addressed in attached patch.

I've also created CASSANDRA-5259 to make getNextBackgroundTask try harder before deciding there is no work to do.

Finally, I have a patch to remove compactionLock entirely that I'll attach to CASSANDRA-3430.  I think the prudent course is to do that for trunk only.;;;","15/Feb/13 04:58;jbellis;v2 does a little more cleanup.  removed unnecessary reference-keeping in submitUserDefined.  standardized concurrency control on markCompacting instead of mix of that + synchronized.;;;","15/Feb/13 21:57;yukim;Overall, looks good and simplifies things.
One problem I had was CompactionsTest was throwing AssertionError on testSingleSSTableCompactionWithLeveledCompaction, when trying to find overlapping sstables to determine droppable ones.

Patch for that pushed to https://github.com/yukim/cassandra/commits/5256-3.;;;","15/Feb/13 22:48;slebresne;lgtm, +1.

Nits:
* we could also move the try \{ task.execute() \} finally \{ task.unmarkSSTables(); \} inside AbstractCompactionTask to 1) avoid repeating it 3 times and 2) making sure we don't forget about it.
* maybe the ctor of AbstractCompactionTask could assert that the sstable it gets are 'marked compacting' to avoid that kind of mistake in the future.;;;","16/Feb/13 05:59;jbellis;bq.  One problem I had was CompactionsTest was throwing AssertionError on testSingleSSTableCompactionWithLeveledCompaction, when trying to find overlapping sstables to determine droppable ones.

I'm worried this patch fixes a symptom instead of a deeper bug:
# We already start with {{Set<SSTableReader> candidates = cfs.getUncompactingSSTables();}}; fixing a race by adding additional checks seems like it will just make the race less likely rather than eliminate it entirely
# I'm not sure why compacting status should affect getOverlappingSSTables

For the record, here's the stack trace in question:

{noformat}
    [junit] Caused by: java.util.concurrent.ExecutionException: java.lang.AssertionError
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
    [junit] 	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
    [junit] 	at org.apache.cassandra.utils.FBUtilities.waitOnFuture(FBUtilities.java:373)
    [junit] Caused by: java.lang.AssertionError
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.getOverlappingSSTables(ColumnFamilyStore.java:882)
    [junit] 	at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:184)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.findDroppableSSTable(LeveledCompactionStrategy.java:296)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getMaximalTask(LeveledCompactionStrategy.java:107)
    [junit] 	at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:95)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:185)
{noformat};;;","16/Feb/13 06:04;jbellis;v4 attached that incorporates Sylvain's suggestions.  Still fails CompactionsTest since I'm not sure what's going on w/ the getOverlappingSSTables assert.;;;","19/Feb/13 14:07;jbellis;I think there are two problems here:

# I was wrong, we're not starting with non-compacting candidates (this is the case for STCS, but not LCS, which checks for compacting-ness later on).  
# The assert is broken in a concurrent compactions world; it's possible that the sstable we're checking for overlaps with, gets compacted away already by a different task before that assert fires.

v5 edits findDroppableSSTable to check for compacting status, and removes the getOverlappingSSTables assert.;;;","20/Feb/13 02:56;slebresne;+1 on v5.

Nits: the comment of CompactionTask.execute is now outdated about the unmarking part (I would also probably have move the unmarking part in AbsractCompactionTask rather than CompactionTask as that felt ""safer"" to me in principle, but that doesn't really matter).;;;","20/Feb/13 13:39;jbellis;Committed with nits addressed;;;","28/Oct/14 18:00;ngrigoriev@gmail.com;I have just got this problem on multiple nodes. Cassandra 2.0.10 (DSE 4.5.2). Should I reopen?

{code}
ERROR [CompactionExecutor:1196] 2014-10-28 17:14:50,124 CassandraDaemon.java (line 199) Exception in thread Thread[CompactionExecutor:1196,1,main]
java.lang.AssertionError: Memory was freed
        at org.apache.cassandra.io.util.Memory.checkPosition(Memory.java:259)
        at org.apache.cassandra.io.util.Memory.getInt(Memory.java:211)
        at org.apache.cassandra.io.sstable.IndexSummary.getIndex(IndexSummary.java:79)
        at org.apache.cassandra.io.sstable.IndexSummary.getKey(IndexSummary.java:84)
        at org.apache.cassandra.io.sstable.IndexSummary.binarySearch(IndexSummary.java:58)
        at org.apache.cassandra.io.sstable.SSTableReader.getSampleIndexesForRanges(SSTableReader.java:692)
        at org.apache.cassandra.io.sstable.SSTableReader.estimatedKeysForRanges(SSTableReader.java:663)
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.worthDroppingTombstones(AbstractCompactionStrategy.java:328)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.findDroppableSSTable(LeveledCompactionStrategy.java:354)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getMaximalTask(LeveledCompactionStrategy.java:125)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.getNextBackgroundTask(LeveledCompactionStrategy.java:113)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:192)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask.run(FutureTask.java:262)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:745)
{code};;;","29/Oct/14 15:16;jbellis;That is a different error; please open a new ticket.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
dsnitch severity is not correctly set for compaction info,CASSANDRA-5255,12632418,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,brandon.williams,brandon.williams,14/Feb/13 17:30,16/Apr/19 09:32,14/Jul/23 05:53,02/Mar/13 02:45,1.2.3,,,,,,0,,,,,"We're doing two things wrong in CI.  First, load can change between calls, which can cause a negative severity even though it meant to subtract whatever it added before.  Second, we should report based on how much IO we're using, since a 1T throttled to 5MB/s is less impactful than a 100MB running at full speed.",,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Feb/13 06:47;vijay2win@yahoo.com;0001-CASSANDRA-5255.patch;https://issues.apache.org/jira/secure/attachment/12569774/0001-CASSANDRA-5255.patch",,,,,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,312914,,,Sat Mar 02 02:45:57 UTC 2013,,,,,,,,,,"0|i1i007:",313260,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"15/Feb/13 19:35;jbellis;Another improvement (maybe for another ticket) would be to incorporate streaming MBps into severity as well.;;;","18/Feb/13 06:44;vijay2win@yahoo.com;Attached patch fixes the negative severity.

{quote}
 we should report based on how much IO we're using, since a 1T throttled to 5MB/s is less impactful than a 100MB running at full speed
{quote}

Well I am not sure about that, 100MB without throttle may be on SSD's and 1TB may be on Spindles.... So looking at the throttle might not help and we should assume the user did the right thing on throttle.

Another possible option: Create a way to get IOStat data into the JVM and measure the IOWait time but the problem is how do we support MS Windows? not sure if there is a generic way to do so.

I would suggest we do both Streaming and IOWait monitor in another ticket, Makes sense?;;;","18/Feb/13 15:18;jbellis;bq. Another possible option: Create a way to get IOStat data into the JVM and measure the IOWait time but the problem is how do we support MS Windows? not sure if there is a generic way to do so.

I like that idea a lot better than trying to special-case streaming and compaction.

No idea how to get that info on Windows though.  Guess we could fall back to MBPs compaction/streaming there.  (Yes, this is not perfect if you mix SSDs + HDDs, but it's a LOT better than total MB of compaction.);;;","01/Mar/13 15:05;jbellis;+1 on the fix for negative severity.  Please open followup tickets per above discussion.;;;","02/Mar/13 02:45;vijay2win@yahoo.com;Committed to 1.2 and trunk, opened CASSANDRA-5306 as a followup. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Nodes can be marked up after gossip sends the goodbye command,CASSANDRA-5254,12632412,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,14/Feb/13 17:09,16/Apr/19 09:32,14/Jul/23 05:53,06/Mar/13 22:10,1.1.11,,,,,,0,,,,,"Finally tracked this down on dtestbot after setting the rpc_timeout to ridiculous levels:

{noformat}
==> logs/last/node1.log <==
 INFO [FlushWriter:1] 2013-02-14 10:01:10,311 Memtable.java (line 305) Completed flushing /tmp/dtest-iaYzzR/test/node1/data/system/schema_columns/system-schema_columns-hf-2-Data.db (558 bytes) for commitlog position ReplayPosition(segmentId=1360857665931, position=4770)
 INFO [MemoryMeter:1] 2013-02-14 10:01:10,974 Memtable.java (line 213) CFS(Keyspace='ks', ColumnFamily='cf') liveRatio is 20.488836662749705 (just-counted was 20.488836662749705).  calculation took 96ms for 144 columns
 INFO [GossipStage:1] 2013-02-14 10:01:12,119 Gossiper.java (line 831) InetAddress /127.0.0.3 is now dead.

==> logs/last/node2.log <==
 INFO [GossipStage:1] 2013-02-14 10:01:12,119 Gossiper.java (line 831) InetAddress /127.0.0.3 is now dead.
 INFO [GossipStage:1] 2013-02-14 10:01:12,238 Gossiper.java (line 817) InetAddress /127.0.0.3 is now UP
 INFO [GossipTasks:1] 2013-02-14 10:01:26,386 Gossiper.java (line 831) InetAddress /127.0.0.3 is now dead.

==> logs/last/node3.log <==
 INFO [StorageServiceShutdownHook] 2013-02-14 10:01:11,115 Gossiper.java (line 1134) Announcing shutdown
 INFO [StorageServiceShutdownHook] 2013-02-14 10:01:12,118 MessagingService.java (line 549) Waiting for messaging service to quiesce
 INFO [ACCEPT-/127.0.0.3] 2013-02-14 10:01:12,119 MessagingService.java (line 705) MessagingService shutting down server thread.
{noformat}

node2 receives the goodbye command from node3, and node1 has already marked node3 down, but some kind of signal is still coming from node3 to node2 marking it up again.",,jasobrown,marcinszymaniuk,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/13 20:28;brandon.williams;5254-v2.txt;https://issues.apache.org/jira/secure/attachment/12572386/5254-v2.txt","05/Mar/13 19:40;brandon.williams;5254.txt;https://issues.apache.org/jira/secure/attachment/12572150/5254.txt",,,,,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,312908,,,Wed Mar 06 22:10:48 UTC 2013,,,,,,,,,,"0|i1hzyv:",313254,,,,,,,,,jasobrown,,jasobrown,Low,,,,,,,,,,,,,,,,,,"05/Mar/13 19:40;brandon.williams;This is a pernicious thing to debug, since the timing condition is so tight; enabling DEBUG or TRACE even on just the gossiper does not let it reproduce.  However, careful examination of the INFO messages tells us that handleMajorStateChange is not being called since there is no 'node restarted' message, which means applyStateLocally is the only other option, and that is called in the ack/ack2 handlers. This tells us that we're in the middle of a gossip round when we send the shutdown message, so the easiest thing to do is sleep for more than one round.  Trivial patch to do so, which has solved this on the dtests.;;;","06/Mar/13 04:51;vijay2win@yahoo.com;+1;;;","06/Mar/13 20:28;brandon.williams;v2 adds just a touch more protection and ignores ack2 when gossip is disabled, which we were already doing in ack/syn.;;;","06/Mar/13 21:48;jasobrown;+1 on v2;;;","06/Mar/13 22:10;brandon.williams;Committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
NPE while loading Saved KeyCache,CASSANDRA-5253,12632384,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,ahmedgc,ahmedgc,14/Feb/13 14:11,16/Apr/19 09:32,14/Jul/23 05:53,20/Feb/13 02:43,,,,,,,0,,,,,"This bug occurred in the Beta version and was marked as fixed in this Jira: CASSANDRA-4553

However it seems to have reoccurred in the production 1.2.1 release. This is the first install I have made of Cassandra (so a clean install), which I downloaded prepackaged from http://www.apache.org/dyn/closer.cgi?path=/cassandra/1.2.1/apache-cassandra-1.2.1-bin.tar.gz

I have created a keyspace but not inserted any data, so that is not the issue either.

Here is a sample from the logs all the way from startup
{code}
 INFO [main] 2013-02-07 19:48:54,109 CassandraDaemon.java (line 101) Logging initialized
 INFO [main] 2013-02-07 19:48:54,125 CassandraDaemon.java (line 123) JVM vendor/version: Java HotSpot(TM) Client VM/1.7.0_11
 INFO [main] 2013-02-07 19:48:54,125 CassandraDaemon.java (line 124) Heap size: 1067057152/1067057152
 INFO [main] 2013-02-07 19:48:54,126 CassandraDaemon.java (line 125) Classpath: C:\Cassandra\\conf;C:\Cassandra\\lib\antlr-3.2.jar;C:\Cassandra\\lib\apache-cassandra-1.2.1.jar;C:\Cassandra\\lib\apache-cassandra-clientutil-1.2.1.jar;C:\Cassandra\\lib\apache-cassandra-thrift-1.2.1.jar;C:\Cassandra\\lib\avro-1.4.0-fixes.jar;C:\Cassandra\\lib\avro-1.4.0-sources-fixes.jar;C:\Cassandra\\lib\commons-cli-1.1.jar;C:\Cassandra\\lib\commons-codec-1.2.jar;C:\Cassandra\\lib\commons-lang-2.6.jar;C:\Cassandra\\lib\compress-lzf-0.8.4.jar;C:\Cassandra\\lib\concurrentlinkedhashmap-lru-1.3.jar;C:\Cassandra\\lib\guava-13.0.1.jar;C:\Cassandra\\lib\high-scale-lib-1.1.2.jar;C:\Cassandra\\lib\jackson-core-asl-1.9.2.jar;C:\Cassandra\\lib\jackson-mapper-asl-1.9.2.jar;C:\Cassandra\\lib\jamm-0.2.5.jar;C:\Cassandra\\lib\jline-1.0.jar;C:\Cassandra\\lib\json-simple-1.1.jar;C:\Cassandra\\lib\libthrift-0.7.0.jar;C:\Cassandra\\lib\log4j-1.2.16.jar;C:\Cassandra\\lib\metrics-core-2.0.3.jar;C:\Cassandra\\lib\netty-3.5.9.Final.jar;C:\Cassandra\\lib\servlet-api-2.5-20081211.jar;C:\Cassandra\\lib\slf4j-api-1.7.2.jar;C:\Cassandra\\lib\slf4j-log4j12-1.7.2.jar;C:\Cassandra\\lib\snakeyaml-1.6.jar;C:\Cassandra\\lib\snappy-java-1.0.4.1.jar;C:\Cassandra\\lib\snaptree-0.1.jar;C:\Cassandra\\build\classes\main;C:\Cassandra\\build\classes\thrift;C:\Cassandra\\lib\jamm-0.2.5.jar
 INFO [main] 2013-02-07 19:48:54,130 CLibrary.java (line 61) JNA not found. Native methods will be disabled.
 INFO [main] 2013-02-07 19:48:54,147 DatabaseDescriptor.java (line 131) Loading settings from file:/C:/Cassandra/conf/cassandra.yaml
 INFO [main] 2013-02-07 19:48:54,515 DatabaseDescriptor.java (line 150) 32bit JVM detected.  It is recommended to run Cassandra on a 64bit JVM for better performance.
 INFO [main] 2013-02-07 19:48:54,516 DatabaseDescriptor.java (line 190) DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
 INFO [main] 2013-02-07 19:48:54,516 DatabaseDescriptor.java (line 204) disk_failure_policy is stop
 INFO [main] 2013-02-07 19:48:54,524 DatabaseDescriptor.java (line 267) Global memtable threshold is enabled at 339MB
 INFO [main] 2013-02-07 19:48:55,099 CacheService.java (line 111) Initializing key cache with capacity of 50 MBs.
 INFO [main] 2013-02-07 19:48:55,109 CacheService.java (line 140) Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO [main] 2013-02-07 19:48:55,110 CacheService.java (line 154) Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
 INFO [main] 2013-02-07 19:48:55,117 CacheService.java (line 166) Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,452 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_keyspaces\system-schema_keyspaces-ib-1 (258 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,484 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_keyspaces\system-schema_keyspaces-ib-3 (262 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,489 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_keyspaces\system-schema_keyspaces-ib-2 (262 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,517 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-ib-1 (4420 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,522 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-ib-3 (4424 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,525 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columnfamilies\system-schema_columnfamilies-ib-2 (4424 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,543 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columns\system-schema_columns-ib-3 (3750 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,548 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columns\system-schema_columns-ib-1 (3747 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,553 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\schema_columns\system-schema_columns-ib-2 (3748 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,588 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\local\system-local-ib-16 (119 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,594 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\local\system-local-ib-18 (436 bytes)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:55,600 SSTableReader.java (line 164) Opening C:\Cassandra\data\system\local\system-local-ib-17 (109 bytes)
 INFO [main] 2013-02-07 19:48:55,610 AutoSavingCache.java (line 139) reading saved cache C:\Cassandra\saved_caches\system-local-KeyCache-b.db
 WARN [main] 2013-02-07 19:48:55,614 AutoSavingCache.java (line 160) error reading saved cache C:\Cassandra\saved_caches\system-local-KeyCache-b.db
java.io.EOFException
	at java.io.DataInputStream.readInt(Unknown Source)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
	at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
	at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
	at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:277)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:392)
	at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:364)
	at org.apache.cassandra.db.Table.initCf(Table.java:337)
	at org.apache.cassandra.db.Table.<init>(Table.java:280)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.db.Table.open(Table.java:88)
	at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:421)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:177)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)
 INFO [SSTableBatchOpen:1] 2013-02-07 19:48:56,212 SSTableReader.java (line 164) Opening C:\Cassandra\data\system_auth\users\system_auth-users-ib-1 (72 bytes)
 INFO [main] 2013-02-07 19:48:56,242 CassandraDaemon.java (line 224) completed pre-loading (3 keys) key cache.
{code}","JVM vendor/version: Java HotSpot(TM) Client VM/1.7.0_11
OS: Windows 7 Enterprise SP1, x64.",ahmedgc,dbrosius,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4916,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,312880,,,Wed Feb 20 02:37:28 UTC 2013,,,,,,,,,,"0|i1hzsn:",313226,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"20/Feb/13 02:37;dbrosius;applied CASSANDRA-4916 to cassandra-1.2, which was mistakenly only applied to trunk until now.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Starting Cassandra throws EOF while reading saved cache,CASSANDRA-5252,12632291,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,dbrosius,drew_kutchar,drew_kutchar,13/Feb/13 22:11,16/Apr/19 09:32,14/Jul/23 05:53,24/Mar/13 01:38,1.2.2,,,,,,1,,,,,"I just saw this exception happen on Cassandra 1.2.1. I thought this was fixed by CASSANDRA-4916. Wasn't CASSANDRA-4916 part of the 1.2.1 release?

I'm on Mac OS X 10.8.2, Oracle JDK 1.7.0_11, using snappy-java 1.0.5-M3 from Maven (not sure if that's the cause).
I'm attaching my data and log directory as data.zip.


{code}
 WARN [main] 2013-02-12 17:50:11,714 AutoSavingCache.java (line 160) error reading saved cache /Users/services/cassandra/data/saved_caches/system-schema
_columnfamilies-KeyCache-b.db
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:277)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:392)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:364)
        at org.apache.cassandra.db.Table.initCf(Table.java:337)
        at org.apache.cassandra.db.Table.<init>(Table.java:280)
        at org.apache.cassandra.db.Table.open(Table.java:110)
        at org.apache.cassandra.db.Table.open(Table.java:88)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:421)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:177)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)
 INFO [SSTableBatchOpen:1] 2013-02-12 17:50:11,722 SSTableReader.java (line 164) Opening /Users/services/cassandra/data/data/system/schema_columns/syste
m-schema_columns-ib-6 (193 bytes)
 INFO [SSTableBatchOpen:2] 2013-02-12 17:50:11,722 SSTableReader.java (line 164) Opening /Users/services/cassandra/data/data/system/schema_columns/syste
m-schema_columns-ib-5 (3840 bytes)
 INFO [main] 2013-02-12 17:50:11,725 AutoSavingCache.java (line 139) reading saved cache /Users/services/cassandra/data/saved_caches/system-schema_colum
ns-KeyCache-b.db
 WARN [main] 2013-02-12 17:50:11,725 AutoSavingCache.java (line 160) error reading saved cache /Users/services/cassandra/data/saved_caches/system-schema
_columns-KeyCache-b.db
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:277)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:392)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:364)
        at org.apache.cassandra.db.Table.initCf(Table.java:337)
        at org.apache.cassandra.db.Table.<init>(Table.java:280)
        at org.apache.cassandra.db.Table.open(Table.java:110)
        at org.apache.cassandra.db.Table.open(Table.java:88)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:421)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:177)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)
 INFO [SSTableBatchOpen:1] 2013-02-12 17:50:11,736 SSTableReader.java (line 164) Opening /Users/services/cassandra/data/data/system/local/system-local-i
b-14 (458 bytes)
 INFO [main] 2013-02-12 17:50:11,738 AutoSavingCache.java (line 139) reading saved cache /Users/services/cassandra/data/saved_caches/system-local-KeyCac
he-b.db
 WARN [main] 2013-02-12 17:50:11,739 AutoSavingCache.java (line 160) error reading saved cache /Users/services/cassandra/data/saved_caches/system-local-
KeyCache-b.db
java.io.EOFException
        at java.io.DataInputStream.readInt(DataInputStream.java:392)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:349)
        at org.apache.cassandra.service.CacheService$KeyCacheSerializer.deserialize(CacheService.java:378)
        at org.apache.cassandra.cache.AutoSavingCache.loadSaved(AutoSavingCache.java:144)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:277)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:392)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:364)
        at org.apache.cassandra.db.Table.initCf(Table.java:337)
        at org.apache.cassandra.db.Table.<init>(Table.java:280)
        at org.apache.cassandra.db.Table.open(Table.java:110)
        at org.apache.cassandra.db.Table.open(Table.java:88)
        at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:421)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:177)
        at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
        at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)
{code}",,dbrosius,drew_kutchar,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4916,,,,,,,,,,CASSANDRA-4916,,,,,,,,,,,"13/Feb/13 22:11;drew_kutchar;data.zip;https://issues.apache.org/jira/secure/attachment/12569277/data.zip",,,,,,,,,,,,,,,,,,,,1.0,dbrosius,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,312787,,,Sun Mar 24 01:38:23 UTC 2013,,,,,,,,,,"0|i1hz7z:",313133,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"23/Mar/13 22:06;jbellis;Is it possible there's still a problem here, Dave?;;;","24/Mar/13 01:37;dbrosius;well the problem was this fix didn't go in till 1.2.2. 

so you won't see the above problem in 1.2.2. 

Of course if the write gets interrupted on shutdown or such, you may have this issue, but the exception is no longer at error, and so it's unlikely you'll see it.

fix was documented in 4916. will close this one too.;;;","24/Mar/13 01:38;dbrosius;fixed by 4916;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix timestamp-based tomstone removal logic,CASSANDRA-5248,12632187,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,13/Feb/13 11:03,16/Apr/19 09:32,14/Jul/23 05:53,15/Feb/13 18:31,1.2.2,,,,,,0,,,,,"Quoting the description of CASSANDRA-4671:
{quote}
In other words, we should force CompactionController.shouldPurge() to return true if min_timestamp(non-compacted-overlapping-sstables) > max_timestamp(compacted-sstables)
{quote}
but somehow this was translating in the code to:
{noformat}
if (sstable.getBloomFilter().isPresent(key.key) && sstable.getMinTimestamp() >= maxDeletionTimestamp)
    return false;
{noformat}
which, well, is reversed.

Attaching the trivial patch to fix. I note that we already had a test that catched this (CompactionsTest.testDontPurgeAccidentaly), but that test was racy in that most of the time the compaction was done in the same second than the removal done prior to that and thus the compaction wasn't considering the tombstone gcable even though gcgrace was 0. I've already pushed the addition of a 1 second delay to make sure the patch reliably catch this bug.
",,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"13/Feb/13 11:04;slebresne;5248.txt;https://issues.apache.org/jira/secure/attachment/12569176/5248.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,312683,,,Fri Feb 15 18:31:15 UTC 2013,,,,,,,,,,"0|i1hykn:",313029,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"13/Feb/13 14:25;jbellis;+1;;;","13/Feb/13 15:16;yukim;Wait, we do want to delete even if key exits in other sstable but the sstable timestamp was older than max deletion time, don't we?
The patch changes the behavior to not delete even if such key exists.;;;","13/Feb/13 20:25;yukim;Sorry, I was wrong. I just got confused. :(
You're right that the comparison should be opposite.
;;;","15/Feb/13 18:31;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-cli should exit with error-exit status on all errors which cause it to exit.,CASSANDRA-5247,12632103,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,steve_p,rthille,rthille,12/Feb/13 22:23,16/Apr/19 09:32,14/Jul/23 05:53,29/Jun/13 18:38,1.2.7,,,Legacy/Tools,,,0,lhf,,,,"running cassandra-cli with a --file argument which does not exist returns success:
ubuntu@host:~$ cassandra-cli --file does-not-exist ; echo $?
does-not-exist (No such file or directory)
0
",,lakemove,rthille,steve_p,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jun/13 03:51;steve_p;CASSANDRA-5247.txt;https://issues.apache.org/jira/secure/attachment/12589542/CASSANDRA-5247.txt",,,,,,,,,,,,,,,,,,,,1.0,steve_p,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,312599,,,Sat Jun 29 18:38:40 UTC 2013,,,,,,,,,,"0|i1hy1z:",312945,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"22/Apr/13 12:21;lakemove;inFileMode swallows the exception (which should be thrown to outer)
{code}
if(sessionState.isFileMode(){
  FileReader fileReader;
  try {
    fileReader = new FileReader(sessionState.filename);
  }catch(IOException e) {
    sessionState.err.println(e.getMessage());
    return;//here returns main method and exit JVM. should re-throw exception instead.
  }
}
{code};;;","25/Jun/13 03:55;steve_p;This patch resolves the problem mentioned in the ticket by doing a ""System.exit(1)"" instead of a return as mentioned in the first comment.  Also the code after the try...catch is moved into the try due to errors thrown by the compiler complaining that fileReader was now possibly undefined.  This should be OK since evaluateFileStatements() also throws IOExceptions so errors thrown while reading the input file would now be caught by the catch that now has the exit().;;;","29/Jun/13 18:38;jbellis;Committed, thanks!

(Ignored the rebuild of Thrift code with an older? newer? compiler.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AnitEntropy/MerkleTree Error,CASSANDRA-5245,12631964,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,drohr,drohr,12/Feb/13 15:43,16/Apr/19 09:32,14/Jul/23 05:53,12/Mar/13 18:03,1.2.3,,,,,,2,,,,,"We are seeing AntiEntropy errors when performing repair jobs in one of our Cassandra clusters. It seems to have started with 1.2. (maybe an issue with vnodes) The exceptions occur almost every time we try to do a repair on all column families in the cluster. Doing the same task on 1.1 does not trigger this.

6 nodes cluster (vnodes, murmur3, rf:3)
very low activity
running a nodetool repair -pr loop on the cluster nodes
nodetool hangs, and same big stacktrace in logs.

root 11025 0.0 0.0 106100 1436 pts/0 S+ Feb11 0:00 _ /bin/sh /usr/bin/nodetool -h HOST -p 7199 -pr repair KEYSPACE COLUMN_FAMILY

ERROR [AntiEntropyStage:3] 2013-02-11 17:08:12,630 CassandraDaemon.java (line 133) Exception in thread Thread[AntiEntropyStage:3,5,main]
java.lang.AssertionError
	at org.apache.cassandra.utils.MerkleTree.inc(MerkleTree.java:137)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:245)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.difference(MerkleTree.java:227)
	at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer.run(AntiEntropyService.java:982)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

This issue was partially solved earlier but seems to be back with vnodes: https://issues.apache.org/jira/browse/CASSANDRA-3014
",,anttiko,drohr,joeyi,mkjellman,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Mar/13 18:50;yukim;5245-1.2.txt;https://issues.apache.org/jira/secure/attachment/12572367/5245-1.2.txt","07/Mar/13 14:00;slebresne;5245-diffHelper.txt;https://issues.apache.org/jira/secure/attachment/12572536/5245-diffHelper.txt",,,,,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,312460,,,Tue Mar 12 18:03:47 UTC 2013,,,,,,,,,,"0|i1hx73:",312806,,,,,,,,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,,"12/Feb/13 15:55;jbellis;That assertion is over 3 years old, so it's not as simple as ""1.2 added a bogus assert.""  (Which is not what you claimed, of course.);;;","12/Feb/13 16:11;drohr;What do you mean? That assertion is from my logs yesterday? Been able to repro it 3-4 times during the last couple of days. I couldn't repro it on 1.1. (at least not so far). If it is a common assertion during repairs, then it should not be marked as closed.

The question was asked in the Cassandra User list to open a new bug or just edit the old one. Thats why I created a new one. (http://www.mail-archive.com/user@cassandra.apache.org/msg27686.html);;;","27/Feb/13 13:50;anttiko;We have the same problem even with Cassandra 1.2.2. ;;;","06/Mar/13 03:39;joeyi;I have also run into this issue today.

1.2.2, Vnodes, Murmur3Partitioner

3 Nodes, RF3

All nodes have the following stacktrace 1+ times:

{noformat}
ERROR [AntiEntropyStage:10] 2013-03-06 01:46:21,159 CassandraDaemon.java (line 132) Exception in thread Thread[AntiEntropyStage:10,5,main]
java.lang.AssertionError
	at org.apache.cassandra.utils.MerkleTree.inc(MerkleTree.java:137)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:245)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
	at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
	at org.apache.cassandra.utils.MerkleTree.difference(MerkleTree.java:227)
	at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer.run(AntiEntropyService.java:983)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
{noformat}

Another example of stacktrace:
{noformat}
ERROR [AntiEntropyStage:9] 2013-03-05 22:24:41,730 CassandraDaemon.java (line 132) Exception in thread Thread[AntiEntropyStage:9,5,main]
java.lang.AssertionError
        at org.apache.cassandra.utils.MerkleTree.inc(MerkleTree.java:137)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:245)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
{noformat};;;","06/Mar/13 13:55;joeyi;I am able to reproduce this after restarting the cluster and trying to run nodetool repair -pr (instead of just nodetool repair)

{noformat}
        
ERROR [AntiEntropyStage:8] 2013-03-06 07:56:24,794 CassandraDaemon.java (line 132) Exception in thread Thread[AntiEntropyStage:8,5,main]
java.lang.AssertionError
        at org.apache.cassandra.utils.MerkleTree.inc(MerkleTree.java:137)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:245)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:256)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.differenceHelper(MerkleTree.java:267)
        at org.apache.cassandra.utils.MerkleTree.difference(MerkleTree.java:227)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer.run(AntiEntropyService.java:983)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{noformat};;;","06/Mar/13 14:23;yukim;I just look at the code and found this part: https://github.com/apache/cassandra/blob/cassandra-1.2.2/src/java/org/apache/cassandra/service/AntiEntropyService.java#L299

I don't test anything and I'm not sure this is related yet, but if we are using M3P, then we do manually splitting the tree based on the key samples between given range. We can have not enough splits initially.;;;","06/Mar/13 15:51;slebresne;Using the key sample for M3P was definitively not the intention. That instanceof should be changed to a call to preserveOrder() really. I'm less surprised that the code for the ordering partition may get in an infinite recursion, though we should obviously fix it nonetheless.;;;","06/Mar/13 18:50;yukim;Patch to fix as Sylvain's suggestion.
I think this is enough to fix AssertionError when using M3P.;;;","07/Mar/13 02:42;mkjellman;+1 on patch.

Reproduced repair issue with ccm and stress pretty easily with 256 tokens and M3. Calling preserveOrder() fixes this.;;;","07/Mar/13 14:00;slebresne;Yes, +1 on Yuki's patch because that's a problem. But this doesn't fix the fact that differenceHelper might recurse one time too much if the tree ends up being Byte.MAX_VALUE-1 deep (which can happen with the method used to build the tree for OrderPreservingPartitioner, which was mistakenly used for M3P). So attaching a 2nd patch that I believe should fix the recursion problem.
;;;","07/Mar/13 15:55;yukim;+1. Sylvain, could you commit both?;;;","12/Mar/13 17:49;joeyi;Any chance these fixes will make it into the 1.2.3 release?;;;","12/Mar/13 18:03;slebresne;Yep, committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Compactions don't work while node is bootstrapping,CASSANDRA-5244,12631944,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,brandon.williams,jihartik,jihartik,12/Feb/13 13:41,16/Apr/19 09:32,14/Jul/23 05:53,14/Feb/13 04:56,1.2.2,,,,,,0,gossip,,,,"It seems that there is a race condition in StorageService that prevents compactions from completing while node is in a bootstrap state.

I have been able to reproduce this multiple times by throttling streaming throughput to extend the bootstrap time while simultaneously inserting data to the cluster.

The problems lies in the synchronization of initServer(int delay) and reportSeverity(double incr) methods as they both try to acquire the instance lock of StorageService through the use of synchronized keyword. As initServer does not return until the bootstrap has completed, all calls to reportSeverity will block until that. However, reportSeverity is called when starting compactions in CompactionInfo and thus all compactions block until bootstrap completes. 

This might severely degrade node's performance after bootstrap as it might have lots of compactions pending while simultaneously starting to serve reads.

I have been able to solve the issue by adding a separate lock for reportSeverity and removing its class level synchronization. This of course is not a valid approach if we must assume that any of Gossiper's IEndpointStateChangeSubscribers could potentially end up calling back to StorageService's synchronized methods. However, at least at the moment, that does not seem to be the case.

Maybe somebody with more experience about the codebase comes up with a better solution?

(This might affect DynamicEndpointSnitch as well, as it also calls to reportSeverity in its setSeverity method)",,jihartik,mkjellman,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5129,,,,,,,,,,,,,,,,,,,,,,,,"14/Feb/13 04:51;brandon.williams;5244.txt;https://issues.apache.org/jira/secure/attachment/12569310/5244.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,312440,,,Thu Feb 14 05:47:17 UTC 2013,,,,,,,,,,"0|i1hx2n:",312786,,,,,,,,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Critical,,,,,,,,,,,,,,,,,,"12/Feb/13 15:37;jbellis;Thanks for the detective work, Jouni.  I'll let Brandon comment on solutions; in the meantime, marking Minor since while inconvenient this does not compromise correctness.;;;","14/Feb/13 03:43;brandon.williams;This is more severe than we originally thought, and causes CASSANDRA-5129 when there is a secondary index:

{noformat}
""CompactionExecutor:1"" daemon prio=10 tid=0x00007effbc03c800 nid=0x7abf waiting for monitor entry [0x00007effc843a000]
   java.lang.Thread.State: BLOCKED (on object monitor)
    at org.apache.cassandra.service.StorageService.reportSeverity(StorageService.java:905)
    - waiting to lock <0x00000000ca576ac8> (a org.apache.cassandra.service.StorageService)
    at org.apache.cassandra.db.compaction.CompactionInfo$Holder.started(CompactionInfo.java:141)
    at org.apache.cassandra.metrics.CompactionMetrics.beginCompaction(CompactionMetrics.java:90)
    at org.apache.cassandra.db.compaction.CompactionManager$9.run(CompactionManager.java:813)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat};;;","14/Feb/13 04:27;brandon.williams;It seems to me the only reason we're synchronizing here is for the increment, and we don't need to get our own severity out of gossip, so we can just track a local AtomicDouble instead.;;;","14/Feb/13 04:36;vijay2win@yahoo.com;+1;;;","14/Feb/13 04:56;brandon.williams;Committed.;;;","14/Feb/13 05:47;mkjellman;this looks good.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Directories.migrateFile() does not handle -old or -tmp LDB manifests,CASSANDRA-5242,12631802,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,amorton,amorton,11/Feb/13 19:40,16/Apr/19 09:32,14/Jul/23 05:53,02/Apr/13 18:08,1.1.11,,,,,,0,,,,,"During LDB compaction a -old.json file is created with the previous manifest.

Directories.migrateFile() only checks for the .json extension and uses the length to determine the CF name. 

This can result in the -old.json manifest getting copied to a CF-old directory in the new layout. 

see http://www.mail-archive.com/user@cassandra.apache.org/msg27583.html for an example ",,aleksey,marcuse,mauzhang,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Mar/13 13:58;marcuse;0001-CASSANDRA-5242-v2.patch;https://issues.apache.org/jira/secure/attachment/12575229/0001-CASSANDRA-5242-v2.patch","24/Mar/13 07:43;marcuse;0001-CASSANDRA-5242.patch;https://issues.apache.org/jira/secure/attachment/12575211/0001-CASSANDRA-5242.patch",,,,,,,,,,,,,,,,,,,2.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,312298,,,Tue Apr 02 18:08:51 UTC 2013,,,,,,,,,,"0|i1hw73:",312644,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"23/Mar/13 22:07;jbellis;Should probably fix even though it's obsolete in 2.0.;;;","24/Mar/13 07:43;marcuse;patch against 1.1;;;","24/Mar/13 13:58;marcuse;didnt read title properly it seems, this handles -tmp files as well;;;","24/Mar/13 14:21;jbellis;Can you review, [~amorton]?;;;","02/Apr/13 18:08;aleksey;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 has error with Compund row keys when secondray index involved,CASSANDRA-5240,12631721,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,shsedghi,shsedghi,11/Feb/13 13:25,16/Apr/19 09:32,14/Jul/23 05:53,20/Feb/13 16:54,1.2.2,,,,,,0,,,,,"CREATE TABLE  test(
    interval text,
    seq int,
    id int,
    severity int,
    PRIMARY KEY ((interval, seq), id))
    WITH CLUSTERING ORDER BY (id DESC);
--
CREATE INDEX ON test(severity);

insert into test(interval, seq, id , severity) values('t',1, 1, 1);
insert into test(interval, seq, id , severity) values('t',1, 2, 1);
insert into test(interval, seq, id , severity) values('t',1, 3, 2);
insert into test(interval, seq, id , severity) values('t',1, 4, 3);
insert into test(interval, seq, id , severity) values('t',2, 1, 3);
insert into test(interval, seq, id , severity) values('t',2, 2, 3);
insert into test(interval, seq, id , severity) values('t',2, 3, 1);
insert into test(interval, seq, id , severity) values('t',2, 4, 2);

select * from test where severity = 3 and  interval = 't' and seq =1;

Bad Request: Start key sorts after end key. This is not allowed; you probably should not specify end key at all under random partitioner


The following works fine

CREATE TABLE  test(
    interval text,
    id int,
    severity int,
    PRIMARY KEY (interval, id))
    WITH CLUSTERING ORDER BY (id DESC);
--
CREATE INDEX ON test(severity);

insert into test(interval, id , severity) values('t1', 4, 1);
insert into test(interval, id , severity) values('t1', 1, 3);
insert into test(interval, id , severity) values('t1', 2, 2);
insert into test(interval, id , severity) values('t1', 3, 3);
insert into test(interval, id , severity) values('t2', 3, 3);
 insert into test(interval, id , severity) values('t2', 1, 3);
 insert into test(interval, id , severity) values('t2', 2, 1);

select * from test where severity = 3 and  interval = 't1';
interval | id | severity
----------+----+----------
       t1 |  3 |        3
       t1 |  1 |        3


",Linux centos 6.3,liqusha,shsedghi,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"20/Feb/13 06:28;slebresne;5240.patch;https://issues.apache.org/jira/secure/attachment/12570086/5240.patch",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,312217,,,Wed Feb 20 16:54:08 UTC 2013,,,,,,,,,,"0|i1hvp3:",312563,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"20/Feb/13 06:28;slebresne;The underlying code, that uses a range slice since a 2ndary index is involved, was using an end-of-component for the end key of the range, which outside of being useless in that case, is actually wrong with random partitioner. Patch attached.;;;","20/Feb/13 13:23;jbellis;+1 (send to Testing for unit test after commit);;;","20/Feb/13 16:54;slebresne;Committed, thanks.

I did pushed a dtest for this as far as testing is concerned.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Migration during shutdown can cause AE,CASSANDRA-5236,12631536,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,brandon.williams,brandon.williams,08/Feb/13 23:08,16/Apr/19 09:32,14/Jul/23 05:53,27/Feb/13 16:38,1.1.11,,,,,,0,,,,,"Mostly just a problem for the dtests on occasion:

{noformat}
 INFO [FlushWriter:1] 2013-02-08 17:03:49,727 Memtable.java (line 453) Writing Memtable-schema_columns@1757448463(793/793 serialized/live bytes, 15 ops)
 INFO [FlushWriter:1] 2013-02-08 17:03:49,752 Memtable.java (line 487) Completed flushing /tmp/dtest-gEW6x2/test/node3/data/system/schema_columns/system-schema_columns-ib-4-Data.db (339 bytes) for commitlog position ReplayPosition(segmentId=1360364624598, position=66887)
 INFO [CompactionExecutor:2] 2013-02-08 17:03:49,754 CompactionTask.java (line 112) Compacting [SSTableReader(path='/tmp/dtest-gEW6x2/test/node3/data/system/schema_columns/system-schema_columns-ib-4-Data.db'), SSTableReader(path='/tmp/dtest-gEW6x2/test/node3/data/system/schema_columns/system-schema_columns-ib-3-Data.db'), SSTableReader(path='/tmp/dtest-gEW6x2/test/node3/data/system/schema_columns/system-schema_columns-ib-2-Data.db'), SSTableReader(path='/tmp/dtest-gEW6x2/test/node3/data/system/schema_columns/system-schema_columns-ib-1-Data.db')]
 INFO [CompactionExecutor:3] 2013-02-08 17:03:49,759 CompactionTask.java (line 272) Compacted 4 sstables to [/tmp/dtest-gEW6x2/test/node3/data/system/schema_columnfamilies/system-schema_columnfamilies-ib-5,].  7,473 bytes to 5,595 (~74% of original) in 32ms = 0.166744MB/s.  6 total rows, 4 unique.  Row merge counts were {1:3, 2:0, 3:1, 4:0, }
ERROR [InternalResponseStage:1] 2013-02-08 17:03:49,787 CassandraDaemon.java (line 135) Exception in thread Thread[InternalResponseStage:1,5,main]
java.lang.AssertionError
    at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:320)
    at org.apache.cassandra.config.Schema.updateVersionAndAnnounce(Schema.java:458)
    at org.apache.cassandra.db.DefsTable.mergeSchema(DefsTable.java:347)
    at org.apache.cassandra.service.MigrationTask$1.response(MigrationTask.java:66)
    at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:47)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"22/Feb/13 21:05;brandon.williams;5236.txt;https://issues.apache.org/jira/secure/attachment/12570537/5236.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,312032,,,Wed Feb 27 16:38:40 UTC 2013,,,,,,,,,,"0|i1hujz:",312378,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"22/Feb/13 21:05;brandon.williams;The simplest thing to do is remove this assertion, since the gossiper actually doesn't need to be running to add a state, and a migration may come in right after disabling gossip (or someone could disable gossip then issue a migration against the node in thrift.);;;","27/Feb/13 14:28;aleksey;Agreed. +1;;;","27/Feb/13 16:38;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Error when executing a file contains CQL statement in cqlsh,CASSANDRA-5235,12631397,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,shamim_ru,shamim_ru,08/Feb/13 08:25,16/Apr/19 09:32,14/Jul/23 05:53,08/Feb/13 18:56,1.1.10,1.2.2,,Legacy/Tools,,,0,,,,,"When executing a file contains CQL statement returns following error:
cqlsh> source '/tmp/src/xyz.cql';
Shell instance has no attribute 'cql_ver_tuple'","Redhat linux 5, windows 7",aleksey,shamim_ru,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/13 16:24;aleksey;5235.txt;https://issues.apache.org/jira/secure/attachment/12568595/5235.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,311893,,,Sat Feb 09 12:03:24 UTC 2013,,,,,,,,,,"0|i1htpb:",312239,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"08/Feb/13 16:52;aleksey;1.1.9 has the same indentation issue, but it doesn't actually affect SOURCE. Still, should probably be fixed there as well, just in case.;;;","08/Feb/13 18:54;brandon.williams;+1;;;","09/Feb/13 12:03;shamim_ru;it works, thankx;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Table created through CQL3 are not accessble to Pig 0.10,CASSANDRA-5234,12631394,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,alexliu68,shamim_ru,shamim_ru,08/Feb/13 08:13,16/Apr/19 09:32,14/Jul/23 05:53,28/Jun/13 23:09,1.2.7,,,,,,4,,,,,"Hi,
  i have faced a bug when creating table through CQL3 and trying to load data through pig 0.10 as follows:
java.lang.RuntimeException: Column family 'abc' not found in keyspace 'XYZ'
	at org.apache.cassandra.hadoop.pig.CassandraStorage.initSchema(CassandraStorage.java:1112)
	at org.apache.cassandra.hadoop.pig.CassandraStorage.setLocation(CassandraStorage.java:615).
This effects from Simple table to table with compound key. ",Red hat linux 5,alexliu68,cscetbon,jjordan,jlemire,kkonrad,marcostrama,ondrej.cernos,shamim_ru,tharigopla,xcbsmith,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jun/13 16:13;alexliu68;5234-1-1.2-patch.txt;https://issues.apache.org/jira/secure/attachment/12588405/5234-1-1.2-patch.txt","14/Jun/13 05:57;alexliu68;5234-1.2-patch.txt;https://issues.apache.org/jira/secure/attachment/12587782/5234-1.2-patch.txt","20/Jun/13 22:28;alexliu68;5234-2-1.2branch.txt;https://issues.apache.org/jira/secure/attachment/12588946/5234-2-1.2branch.txt","25/Jun/13 22:19;alexliu68;5234-3-1.2branch.txt;https://issues.apache.org/jira/secure/attachment/12589677/5234-3-1.2branch.txt","26/Jun/13 23:06;alexliu68;5234-3-trunk.txt;https://issues.apache.org/jira/secure/attachment/12589811/5234-3-trunk.txt","30/May/13 18:24;alexliu68;5234.tx;https://issues.apache.org/jira/secure/attachment/12585444/5234.tx","10/Jul/13 08:24;shamim_ru;fix_where_clause.patch;https://issues.apache.org/jira/secure/attachment/12591607/fix_where_clause.patch","29/Jul/13 14:33;xcbsmith;pigCounter-patch.txt;https://issues.apache.org/jira/secure/attachment/12594708/pigCounter-patch.txt",,,,,,,,,,,,,8.0,alexliu68,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,311890,,,Thu Sep 05 18:35:48 UTC 2013,,,,,,,,,,"0|i1hton:",312236,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"08/Feb/13 16:12;aleksey;This is not a bug - CQL3 tables are intentionally not included in thrift describe_keyspace(s) (CASSANDRA-4377).;;;","08/Feb/13 19:03;brandon.williams;See CASSANDRA-4421;;;","27/Mar/13 13:15;cscetbon;It means that CQL3 column families are not accessible through thrift and for me it's an issue (I do not agree with your Resolution label). That's why Pig 0.11 cannot use it. Is there a way to solve it ?
I can help you if necessary;;;","13/May/13 08:48;cscetbon;It should be fixed after [CASSANDRA-4421|https://issues.apache.org/jira/browse/CASSANDRA-4421];;;","16/May/13 04:13;alexliu68;I will work on it once I am done with other assignments soon.;;;","16/May/13 05:44;cscetbon;thanks Alex !;;;","16/May/13 22:03;alexliu68;To fix it, we need modify CassandraStorage to get CF meta data from system table instead of thrift describe_keyspace because of the CQL3 table doesn't show up in thrift describe_keyspace call.;;;","30/May/13 18:24;alexliu68;Patch is attached. It's on top of the 4421 patch;;;","30/May/13 18:32;alexliu68;pull @ https://github.com/alexliu68/cassandra/pull/3

Use CassandraStorage for any cql3 tables, you will have composite columns in ""columns"" bag

Use CqlStorage for any cql3 table.
{code}
cassandra://[username:password@]<keyspace>/<columnfamily>[?[page_size=<size>]
[&columns=<col1,col2>][&output_query=<prepared_statement>]
[&where_clause=<clause>][&split_size=<size>][&partitioner=<partitioner>]]
{code}

where 
  page_size is the number of cql3 rows per page (the default is 1000, it's optional)

  columns is the column names for the cql3 select query, it's optional
 
  where_clause is the user defined where clause on the indexed column, it's optional

  split_size is the number of C* rows per split which can be used to tune the number of mappers

  output_query is the prepared query for inserting data to cql3 table (replace the = by @ and ? by #,
      because Pig can't take = and ? as url parameter values)

Output row are in the following format
{code}
(((name, value), (name, value)), (value ... value), (value...value))
{code}

where the name and value tuples are key name and value pairs.


The input schema: ((name, value), (name, value), (name, value)) where keys are in the front.;;;","14/Jun/13 05:51;alexliu68;I attach 5234-1.2-patch.txt to patch 1.2 branch. It update the last patch with the latest 4421 changes.;;;","14/Jun/13 05:53;alexliu68;The patch resolves the following issues.

1. allow access to cql3 type table through CassandraStorage.

2. create new CqlStorage to easy access cql3 tables.;;;","14/Jun/13 09:23;cscetbon;Do you think I can test them now ?;;;","14/Jun/13 15:34;alexliu68;yes, I have done some testing.;;;","14/Jun/13 17:46;cscetbon;Okay, I'll give it a try :)
thanks;;;","17/Jun/13 13:54;cscetbon;My Pig script doesn't work anymore. I suppose you changed the input format ? 
I get :
Projected field [filtre] does not exist in schema: key:chararray,columns:bag{:tuple(name:tuple(),value:byte array)}
when column family is created without COMPACT STORAGE

Something weird is that I get no values when I should get some :
cqlsh:k1> SELECT * FROM cf1 WHERE ISE='XXXX';

 ise  | filtre | value_1
------+--------+---------
 XXXX |      1 |   81056

2013-06-17 13:13:01,372 [main] INFO  org.apache.pig.backend.hadoop.executionengine.util.MapRedUtil - Total input paths to process : 1
(XXXX,{((),),((filtre),),((value_1),<?)}) <-- value not printable for filtre ?;;;","17/Jun/13 15:26;alexliu68;Can you post your schema for the table and pig script, so I can test it.;;;","17/Jun/13 15:31;cscetbon;http://pastebin.com/Fub9t6j9 <-- my column family
http://pastebin.com/HwKxsC4f <-- my pig script;;;","17/Jun/13 15:48;alexliu68;Can you just dump(data) to check whether you have all the data? Then do the following filter scripts.;;;","17/Jun/13 15:57;cscetbon;You definitely broke something as I get different results when dumping data for cql2 and cql3 tables (same structure).
CQL2 : 
{code}
* format is 
x: {key: chararray,value_1: (name: chararray,value: int),filtre: (name: chararray,value: int),columns: {(name: chararray,value: bytearray)}}
* rows are
(XXXX,(value_1,18584),(filtre,0),{})
(YYYY,(value_1,49926),(filtre,2),{})
{code}
CQL3 : 
{code}
* format is 
x: {key: chararray,columns: {(name: (),value: bytearray)}}
* rows are
(XXXX,{((),),((filtre),),((value_1),??)})
(YYYY,{((),),((filtre),),((value_1),??)})
{code}

;;;","17/Jun/13 16:25;alexliu68;CQL2 has different structure from CQL3, CQL2 is more to the legacy thrift type CF(Check the developer blog from datastax). You should follow the cql3 format to change your pig script. or use CqlStorage for easy data mapping for cql3 type table.;;;","17/Jun/13 16:28;alexliu68;Use the following script to find the structure of cql3 table, then change your pig script

{code}
   rows = LOAD 'cassandra://MyKeyspace/MyColumnFamily' USING CassandraStorage();
   dump(rows);
{code};;;","17/Jun/13 16:54;cscetbon;Great, CqlStorage() helps a lot to get same input.

Thanks;;;","17/Jun/13 20:55;alexliu68;5234-1-1.2-patch.txt is attached to allow CassandraStorage to pass partitioner as parameter.;;;","20/Jun/13 22:28;alexliu68;5234-2-1.2branch.txt is attached to use ""cql://"" instead of ""cassandra://"" for CqlStorage;;;","24/Jun/13 16:21;brandon.williams;Hmm, I'm seeing errors when running the examples/pig/test tests that don't use cql3.;;;","24/Jun/13 20:17;alexliu68;I will fix it today.;;;","25/Jun/13 21:08;alexliu68;The failed test is where there is filter for COUNT(columns)

{code}
-- filter to fully visible rows (no uuid columns) and dump
visible = FILTER rows BY COUNT(columns) == 0;
dump visible;
{code};;;","25/Jun/13 22:21;alexliu68;It turns out to wide row schema issue which could be an error from previous version.

5234-3-1.2branch.txt is attached to fixd failing to run examples/pig/test ;;;","26/Jun/13 20:05;tharigopla;@Alex I pulled the branch CASSANDRA-5234 and tried building it. But the build fails when compiling.;;;","26/Jun/13 20:16;alexliu68;It works for me.

try the commands

{code}
git checkout cassandra-1.2
patch -p1 < 5234-3-1.2branch.txt 
{code};;;","26/Jun/13 21:45;tharigopla;@Alex: I did check out cassandra-1.2 and tried to apply the patch using ""patch -p1 < 5234-3-1.2branch.txt"" But it didnt work it threw and exception < is reserveed for future use.
So, I downloaded the patch 5234-3-1.2branch.txt and did a git apply and then committed the unstaged changes. Now when I browsed the file location cassandra ""/ src / java / org / apache / cassandra / hadoop / pig /"" the patch is not applied. Sorry for my naive questions, I am very new to git and cassandra.;;;","26/Jun/13 21:49;tharigopla;The patch is applied. But do I have to worry about the 

5234-3-1.2branch.txt:22: trailing whitespace.
                partitioner = FBUtilities.newPartitioner(client.describe_partitioner());
5234-3-1.2branch.txt:735: trailing whitespace.

5234-3-1.2branch.txt:925: trailing whitespace.

5234-3-1.2branch.txt:1297: trailing whitespace.
        }
5234-3-1.2branch.txt:1321: trailing whitespace.

;;;","26/Jun/13 21:56;brandon.williams;Committed, and created CASSANDRA-5709 for example follow-up.  Thanks!;;;","26/Jun/13 21:57;brandon.williams;[~tharigopla] it's in the 1.2 branch now, just do a git pull.;;;","26/Jun/13 22:12;tharigopla;Thanks Brandon;;;","26/Jun/13 22:58;alexliu68;5234-3-trunk.txt patch for trunk branch is attached.;;;","27/Jun/13 15:55;tharigopla;@Brandon. Thanks for your patience. I did a git pull on 1.2 and ran the build. It fails again.;;;","27/Jun/13 16:09;brandon.williams;I recommend sending an email to user list with the details, as the 1.2 branch is obviously compiling for everyone and JIRA isn't a support forum.;;;","09/Jul/13 15:07;shamim_ru;Hello Alex!
  I have got error when trying to send where_clause in cqlStorage URL as follows:
rows = LOAD 'cql://keyspace1/test?page_size=1&columns=title,age&split_size=4&where_clause=age=41' USING CqlStorage();
and haven't find any way to escape the equal operator.
  It will be better to send the where_clause with URL encoding, for example where_clause=age%3D41.
However for a quick fix i have slightly modify the method getQueryMap as follows:
    /** decompose the query to store the parameters in a map*/
    public static Map<String, String> getQueryMap(String query) throws Exception
    {
        String[] params = query.split(""&"");
        Map<String, String> map = new HashMap<String, String>();
        for (String param : params)
        {
            String[] keyValue = param.split(""="");
            map.put(keyValue[0], URLDecoder.decode(keyValue[1],""UTF-8""));
			
        }
        return map;
    }
and now i can send the query with URL-Encoding character.
;;;","09/Jul/13 15:55;alexliu68;Thx, I will update the patch for it.;;;","10/Jul/13 08:25;shamim_ru;add the patch as a temporary fix;;;","19/Jul/13 14:00;kkonrad;I was trying it locally and I'm not sure whether it fully works. My stacktrace and cassandra table schema is here: http://pastebin.com/uPUAs9T2 (i tried using old cassandra:// ... - it worked for other table) and output when I used cql://.. is here: http://pastebin.com/b0bKd7G3   . I have worked on this with Pig in local mode. It might be important or not: this table contained counters.

I was working on cassandra-1.2 with latest commit 27efded38d855b24f41e5332ffb29cd13d98f8da;;;","29/Jul/13 03:41;xcbsmith;Just adding that I'm getting the same result as Konrad Kurdej. I'm using DSE 3.1, but I think this is the same bug. I was able to isolate the problem specifically to counter fields. Here's a simple set up and test case:

cqlsh> create keyspace pigtest with REPLICATION = { 'class': 'SimpleStrategy', 'replication_factor': 1};
cqlsh> use pigtest;
cqlsh:pigtest> create table foo ( key_1 text primary key, value_1 bigint );
cqlsh:pigtest> create table foo2 ( key_1 text primary key, value_1 counter );
cqlsh:pigtest> update foo set value_1 = 1 where key_1 = 'foo';
cqlsh:pigtest> update foo2 set value_1 = value_1 + 2 where key_1 = 'foo2';
cqlsh:pigtest> select * from foo;

 key_1 | value_1
-------+---------
   foo |       1

cqlsh:pigtest> select * from foo2;

 key_1 | value_1
-------+---------
  foo2 |       2

Now, the following grunt commands:

counts = LOAD 'cassandra://pigtest/foo' USING CassandraStorage();
dump counts;

Will work, but:

counts = LOAD 'cassandra://pigtest/foo2' USING CassandraStorage();
dump counts;

Will fail with the same stack trace that Konrad mentioned.;;;","29/Jul/13 14:33;xcbsmith;This patch against the head appears to have fixed the problem for me. I applied it to DSE 3.1 and it also worked for me.

Basic deal is to use LongType for validation too.;;;","29/Jul/13 17:47;alexliu68;Use CqlStorage for your use case. CassandraStorage has some drawbacks.;;;","29/Jul/13 18:06;alexliu68;CassandraStorage is legacy for any none-CQL3 tables. Use CqlStorage for CQL3 tables.;;;","30/Jul/13 01:55;marcostrama;Cassandra 1.2.8 was released (after regression in 1.2.7), but this last patch for counters appears to be left. Anyone can confirm this?

I downloaded the .deb from http://people.apache.org/~eevans/ but when in grunt i dump a table with counter column, get the error:

java.lang.IndexOutOfBoundsException
	at java.nio.Buffer.checkIndex(Buffer.java:537)
	at java.nio.HeapByteBuffer.getLong(HeapByteBuffer.java:410)
	at org.apache.cassandra.db.context.CounterContext.total(CounterContext.java:477)
	at org.apache.cassandra.db.marshal.AbstractCommutativeType.compose(AbstractCommutativeType.java:34)
	at org.apache.cassandra.db.marshal.AbstractCommutativeType.compose(AbstractCommutativeType.java:25)
	at org.apache.cassandra.hadoop.pig.AbstractCassandraStorage.columnToTuple(AbstractCassandraStorage.java:137)
	at org.apache.cassandra.hadoop.pig.CqlStorage.getNext(CqlStorage.java:110)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:211)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
	at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:416)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1149)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)

My table:
cqlsh:test> desc table votes_count_period3;

CREATE TABLE votes_count_period3 (
  period text,
  poll timeuuid,
  votes counter,
  PRIMARY KEY (period, poll)
) WITH
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  populate_io_cache_on_flush='false' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};

;;;","30/Jul/13 02:20;marcostrama;I confirmed this issue. I downloaded the src, applied the patch and after build the cassandra jar, pig works with counter. This will be patched in 1.2.9?;;;","30/Jul/13 03:31;alexliu68;+1;;;","30/Jul/13 04:40;xcbsmith;[~alexliu68] The patch is to the base class shared by CqlStorage and CassandraStorage. They were both broken and they are now both fixed.;;;","05/Sep/13 18:35;jlemire;The last patch for counters to AbstractCassandraStorage.java has not been applied either to cassandra-1.2 or to the trunk. Consequently, the problem still exists in 1.2.9, as I could verify myself today. I found another recent bug report on SO for the same problem: http://stackoverflow.com/questions/18553230/error-with-cassandra-pig-cql-counter-column.

Should I open a new bug report for the counters bug even though we have a working patch or will you reopen the current issue?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
ALTER TABLE ADD - data loss,CASSANDRA-5232,12631375,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,vasily.v.d,vasily.v.d,08/Feb/13 02:21,16/Apr/19 09:32,14/Jul/23 05:53,08/Feb/13 17:54,1.2.2,,,,,,0,,,,,"cqlsh:Test> CREATE TABLE t1 (id int PRIMARY KEY, t text);
cqlsh:Test> UPDATE t1 SET t = '111' WHERE id = 1;
cqlsh:Test> ALTER TABLE t1 ADD l list<text>;
cqlsh:Test> SELECT * FROM t1;

 id | l    | t
----+------+-----
  1 | null | 111

cqlsh:Test> ALTER TABLE t1 ADD m map<int, text>;
cqlsh:Test> SELECT * FROM t1;
cqlsh:Test>

Last query doesn't return any data.
",,marcinszymaniuk,slebresne,vasily.v.d,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Feb/13 13:02;slebresne;5232.txt;https://issues.apache.org/jira/secure/attachment/12568562/5232.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,311871,,,Fri Feb 08 17:54:49 UTC 2013,,,,,,,,,,"0|i1htkf:",312217,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"08/Feb/13 13:02;slebresne;That's a bug in DataTracker.replaceFlushed, that doesn't handle correctly the case where there is no sstable created by the flush (and in that case one such flush is triggered by the ALTER). Namely, it ""removes"" (from the DataTracker instance, not from disk) all existing sstables. This actually comes from the CASSANDRA-4667 (the second patch committed). So in practice that means that ALTER TABLE is just one of the thing this broke, but this also break the atomic batch guarantees (as the batchlog sstables might be discarded wrongfully).

Trivial patch attached.
;;;","08/Feb/13 13:09;slebresne;Btw, in the test case above the data is not really lost. Restarting the node should have him pick up the ignored sstables.;;;","08/Feb/13 15:25;jbellis;+1;;;","08/Feb/13 17:54;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cql3 doesn't support multiple clauses on primary key components,CASSANDRA-5230,12631343,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,brandon.williams,brandon.williams,07/Feb/13 21:52,16/Apr/19 09:32,14/Jul/23 05:53,20/Feb/13 16:53,1.2.2,,,,,,0,,,,,"In trying to write a dtest for CASSANDRA-5225, I noticed that given a table such as:

{noformat}
CREATE TABLE foo (
  key text,
  c text,
  v text,
  PRIMARY KEY (key, c)
)
{noformat}

It is possible to slice the values of 1 or 2 for c:
{noformat}
select c from foo where key = 'foo' and c > '0' and c < '3';
{noformat}

However, there is no way to get these explicitly by name, even though it should be possible:
{noformat}
cqlsh:Keyspace1> select c from foo where key = 'foo' and c in ('1', '2');
Bad Request: PRIMARY KEY part c cannot be restricted by IN relation
{noformat}",,aleksey,happyoli,liqusha,marcinszymaniuk,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5376,,,,,,,,,,,"20/Feb/13 06:59;slebresne;5230.patch;https://issues.apache.org/jira/secure/attachment/12570092/5230.patch",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,311839,,,Thu Apr 11 20:18:37 UTC 2013,,,,,,,,,,"0|i1htdb:",312185,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"20/Feb/13 06:59;slebresne;Agreed that we could allow that, especially since we allow it for COMPACT
STORAGE (the reason we were not allowing it previously is that it wasn't so
simple in the original code of CQL3 for reason that are not relevant anymore).
;;;","20/Feb/13 13:26;jbellis;+1;;;","20/Feb/13 16:53;slebresne;Committed, thanks;;;","11/Apr/13 19:34;happyoli;If I add another key like:

CREATE TABLE foo (
  key text,
  c text,
  d text,
  v text,
  PRIMARY KEY (key, c, d)
);

select c from foo where key = 'foo' and c in ('1', '2') will still result in a Bad Request error.

Is this the expected behavior?;;;","11/Apr/13 20:18;aleksey;bq. Is this the expected behavior?

Yes, only the last part of the key can be restricted by IN. In your case it's 'd'.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"after a IOException is thrown during streaming, streaming tasks hang in netstats",CASSANDRA-5229,12631334,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,mkjellman,mkjellman,07/Feb/13 20:44,16/Apr/19 09:32,14/Jul/23 05:53,06/May/13 07:08,1.2.5,,,,,,0,,,,,"After an IOExcpetion, streaming tasks marked as ""successful"" in the logs are hung in netstats

With TRACE debugging on streaming on the receiving node everything about the sstable in the log (not very much)

{code}
 INFO [AntiEntropyStage:1] 2013-02-07 11:23:44,717 StreamOut.java (line 151) Stream context metadata [/data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-5-Data.db sections=3068 progress=0/2785204713 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-25-Data.db sections=2696 progress=0/758409465 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-60-Data.db sections=3099 progress=0/238876436 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-63-Data.db sections=1166 progress=0/2125323 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-38-Data.db sections=2507 progress=0/515992757 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-26-Data.db sections=3153 progress=0/994857654 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-57-Data.db sections=3116 progress=0/129398170 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-58-Data.db sections=217 progress=0/72286 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-59-Data.db sections=3146 progress=0/3357709019 - 0%], 27 sstables.
 INFO [AntiEntropyStage:1] 2013-02-07 11:23:52,964 StreamOut.java (line 151) Stream context metadata [/data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-5-Data.db sections=2930 progress=0/2799914560 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-25-Data.db sections=2590 progress=0/761266059 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-60-Data.db sections=2956 progress=0/241362497 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-63-Data.db sections=1153 progress=0/2125323 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-38-Data.db sections=2422 progress=0/522126371 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-26-Data.db sections=3004 progress=0/998401202 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-57-Data.db sections=2974 progress=0/129722346 - 0%, /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-58-Data.db sections=220 progress=0/72286 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-59-Data.db sections=2998 progress=0/3375554099 - 0%], 27 sstables.
{code}

node that is streaming out thinks that the streaming session was successful 
{code}
 INFO [MiscStage:1] 2013-02-07 11:23:38,022 StreamOut.java (line 151) Stream context metadata [/data/cassandra/evidence/fingerprints/evidence-fingerprints-ia-472-Data.db sections=1727 progress=0/210208515 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-919-Data.db sections=1746 progress=0/119438030 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-920-Data.db sections=1681 progress=0/54498226 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-922-Data.db sections=16 progress=0/13490 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-918-Data.db sections=632 progress=0/70019542 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-921-Data.db sections=1644 progress=0/39870238 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-497-Data.db sections=1569 progress=0/208331077 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-923-Data.db sections=1572 progress=0/30870478 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-925-Data.db sections=167 progress=0/1845123 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-703-Data.db sections=1574 progress=0/287386471 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-913-Data.db sections=811 progress=0/103776521 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-915-Data.db sections=1539 progress=0/141864261 - 0%], 12 sstables.
 INFO [MiscStage:1] 2013-02-07 11:23:49,938 StreamOut.java (line 151) Stream context metadata [/var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-919-Data.db sections=3153 progress=0/994857654 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-918-Data.db sections=2507 progress=0/515992757 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-923-Data.db sections=3153 progress=0/131969347 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-703-Data.db sections=3068 progress=0/2784807967 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-913-Data.db sections=2696 progress=0/758409465 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ia-472-Data.db sections=3150 progress=0/1792868996 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-920-Data.db sections=3123 progress=0/240094510 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-929-Data.db sections=18 progress=0/29468 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-922-Data.db sections=217 progress=0/13490 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-921-Data.db sections=3124 progress=0/130320291 - 0%, /data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-497-Data.db sections=3055 progress=0/1749539598 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-925-Data.db sections=1608 progress=0/13357410 - 0%, /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-915-Data.db sections=3096 progress=0/1241397203 - 0%], 13 sstables.
 INFO [Streaming to /10.8.25.132:2] 2013-02-07 11:24:18,780 StreamReplyVerbHandler.java (line 44) Successfully sent /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-919-Data.db to /10.8.25.132
{code}

node that is being streamed to has nothing in the logs about this particular sstable


{code}
Streaming from: /10.8.30.13
evidence: /var/lib/cassandra/data2/evidence/fingerprints/evidence-fingerprints-ib-919-Data.db sections=3153 progress=218225400/994857654 - 21%

_______

Streaming from: /10.138.12.10
evidence: /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-26-Data.db sections=3153 progress=218225400/994857654 - 21%
{code}",Ubuntu 12.04,mkjellman,rbranson,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5105,,,,,,,,,,,,,,,,,,,,,"03/May/13 17:03;yukim;5229-1.2.txt;https://issues.apache.org/jira/secure/attachment/12581712/5229-1.2.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,311830,,,Mon May 06 07:08:59 UTC 2013,,,,,,,,,,"0|i1htbb:",312176,,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,,"10/Feb/13 02:19;rbranson;Seeing this as well.;;;","11/Feb/13 18:15;yukim;Do you have stack trace of IOException?

Is the last log above is from /10.8.25.132?
It seems that both 919( and 26) sstables are sent twice for different ranges. And the ""Successfully sent"" line in the second log can be one of those files.;;;","11/Feb/13 19:34;mkjellman;ERROR [Streaming to /10.8.25.114:4] 2013-02-09 11:02:09,992 CassandraDaemon.java (line 135) Exception in thread Thread[Streaming to /10.8.25.114:4,5,main]
java.lang.RuntimeException: java.io.IOException: Broken pipe
        at com.google.common.base.Throwables.propagate(Throwables.java:160)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.IOException: Broken pipe
        at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
        at sun.nio.ch.FileChannelImpl.transferToDirectly(FileChannelImpl.java:420)
        at sun.nio.ch.FileChannelImpl.transferTo(FileChannelImpl.java:552)
        at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:90)
        at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        ... 3 more;;;","12/Feb/13 23:13;yukim;Looks like the problem is streaming session silently hangs if the exception thrown is not IOException(like RuntimeException in CASSANDRA-5105). When the error was IOException, then the node would send retry or session failure message.;;;","14/Feb/13 05:50;mkjellman;fixed.;;;","14/Feb/13 05:56;jbellis;Re-opening since it sounds like we should still fix the silent hang on unexpected exception.;;;","03/May/13 17:03;yukim;I think the best we can do here is to catch RuntimeException and let session fail.
Patch attach for that.;;;","06/May/13 07:08;slebresne;+1 (took the liberty to commit it since I'd like to re-roll 1.2.5 shortly).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Track maximum ttl and use to expire entire sstables,CASSANDRA-5228,12631278,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,jbellis,jbellis,07/Feb/13 15:29,16/Apr/19 09:32,14/Jul/23 05:53,22/Mar/13 10:43,2.0 beta 1,,,,,,1,,,,,It would be nice to be able to throw away entire sstables worth of data when we know that it's all expired.,,aleksey,batalbot,carlyeks,christianmovi,colinkuo,jabr,jeromatron,jjordan,jshook,marcuse,rcoli,sebastian.estevez@datastax.com,slebresne,vjevdokimov,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5685,,,,,,,,,,,"07/Mar/13 13:06;marcuse;0001-track-max-local-deletiontime-v2.patch;https://issues.apache.org/jira/secure/attachment/12572529/0001-track-max-local-deletiontime-v2.patch","07/Mar/13 14:19;marcuse;0001-track-max-local-deletiontime-v3.patch;https://issues.apache.org/jira/secure/attachment/12572539/0001-track-max-local-deletiontime-v3.patch","22/Mar/13 07:35;marcuse;0001-track-max-local-deletiontime-v4.patch;https://issues.apache.org/jira/secure/attachment/12574976/0001-track-max-local-deletiontime-v4.patch","06/Mar/13 09:40;marcuse;0001-track-max-ttl-v1.patch;https://issues.apache.org/jira/secure/attachment/12572297/0001-track-max-ttl-v1.patch","11/Mar/13 15:14;marcuse;0002-CASSANDRA-5228-add-a-nodetool-command-that-drops-ent.patch;https://issues.apache.org/jira/secure/attachment/12573099/0002-CASSANDRA-5228-add-a-nodetool-command-that-drops-ent.patch","13/Mar/13 20:00;marcuse;0002-CASSANDRA-5228-drop-entire-sstables-if-all-tombstone-v2.patch;https://issues.apache.org/jira/secure/attachment/12573565/0002-CASSANDRA-5228-drop-entire-sstables-if-all-tombstone-v2.patch","22/Mar/13 07:35;marcuse;0002-CASSANDRA-5228-v3.patch;https://issues.apache.org/jira/secure/attachment/12574975/0002-CASSANDRA-5228-v3.patch",,,,,,,,,,,,,,7.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,311774,,,Fri Mar 22 10:43:15 UTC 2013,,,,,,,,,,"0|i1hsyv:",312120,,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,,"07/Feb/13 15:33;jbellis;I'm skeptical that this would be useful with our general-purpose compaction strategies.  Certainly LCS and probably STCS as well are good enough at combining newer data with old that we'd end up with a mix of expirations in most sstables.

But, if we have an append-mostly workload of ttl data, we could create a separate compaction strategy that doesn't bother merging sstables, just throws out expired ones (and relies on the bloom filters until then to avoid checking too many sstables on reads).;;;","08/Feb/13 08:52;slebresne;bq. I'm skeptical that this would be useful with our general-purpose compaction strategies

For the record, I don't fully share that pessimism :). Just to say that imo it's worth separating what the title says (and doing that first) from creating a new compaction strategy. Could be you didn't intended otherwise anyway but just wanted to chime in.;;;","08/Feb/13 15:53;carlyeks;I think this would really help with the Hints cf. When all of the hints in an sstable have been handed off, then we can easily delete the whole SSTable rather than running a compaction.

Also, think we need to track maximum ttl.;;;","13/Feb/13 19:00;jbellis;Hints is a different case; I'd rather re-code the delivery mechanism to operate sstable-at-a-time, than keep them around until the TTL expires (which is usually much, much longer than ""until we deliver it"").

You're right about wanting to track max ttl; edited.;;;","20/Feb/13 19:50;batalbot;It sounds like this would be an ""Age Tiered Compaction Strategy""?

One of our busiest use cases involves logging performance data with the same TTL for all columns of a row.  The row is never updated once all columns are written to once.  For example, metrics per-second might be kept for 3 days, metrics per-minute kept for 3 weeks, and metrics per-hour or day kept longer.  It would be great to avoid all the excess compaction IO work on rows that never change and haven't expired yet.  This sounds like it would greatly extend the performance of cassandra (by reducing compaction-IO overhead) for us and many other similar workloads.
;;;","06/Mar/13 09:40;marcuse;first attempt at a first part that just tracks max ttl on sstables, figured this ticket could be split in 2 patches

next part will drop sstables after maxTimestamp + maxTTL + gc_grace_seconds - we need to take gc_grace in account here since deletes don't have a ttl, meaning we could drop tombstones too early. (another approach might be to say that DeletedColumn TTL is gc_grace_seconds?);;;","06/Mar/13 10:11;slebresne;bq. another approach might be to say that DeletedColumn TTL is gc_grace_seconds?

Kind of, but I wouldn't put it that way exactly. I don't think we should track ttl per-se, but rather the maximum time at which all columns in the sstable are deleted. I.e. we should track the max localDeletionTime (which is MAX_VALUE for normal columns, which is what we want). So there shouldn't be any special casing of tombstone versus expired column in that ticket in that the Colum.getLocalDeletionTime() does the right thing already.

And thus dropping sstable should just be checking if current_time > max(localDeletionTime). Though for the record, I note that having all column GCable is *not* sufficient to drop the sstable, we'd typically need to check that the sstable maxTimestamp is smaller than all other sstable minTimestamp. Otherwise, we might drop tombstone (or expiring columns for that matter) that are GCable but shard another column in another sstable.;;;","06/Mar/13 12:14;marcuse;ah nice, that will make this patch a lot cleaner, will fix;;;","07/Mar/13 13:06;marcuse;track max local deletion time instead

again, only tracks the information, does not yet use it;;;","07/Mar/13 14:13;slebresne;I've only skimmed over the patch quickly, but while that looks fine on principle, one thing that is not ok is:
{noformat}
+            int maxLocalDeletionTime = desc.version.tracksMaxLocalDeletionTime ? dis.readInt() : Integer.MIN_VALUE;
{noformat}
If !tracksMaxLocalDeletionTime, this should return Integer.MAX_VALUE, because MIN_VALUE means ""everything in that sstable is GCable"".
;;;","07/Mar/13 14:19;marcuse;nice catch, old maxttl-patch leftover;;;","11/Mar/13 15:14;marcuse;adds a nodetool command to expire entire sstables if ttl has expired (and maxtimestamp is less than mintimestamp of all other sstables)

applies on top of the v3 track max local deletiontime patch.

unsure if doing this as a nodetool command is the right way, suggestions where to put if not as a nodetool command it are welcome;;;","12/Mar/13 16:53;slebresne;I suspect we'd rather make that automatic rather a nodetool command if possible. In fact, we could do that for every compaction. Typically, when we create the compaction iterator, we could skip sstables that can be dropped fully (but still consider them as compacted).

We may also want to tweak slightly AbstractCompactionStrategy.worthDroppingTombstones so it return yes if the maxLocalDeletion time is less than gcBefore (though the current version should work reasonably well already).;;;","13/Mar/13 20:00;marcuse;tries to drop sstables in CompactionTask#runWith

also checks if it is possible to drop sstable in AbstractCompactionStrategy#worthDroppingTombstones

still applies on top of v3 of the max deletiontime patch;;;","21/Mar/13 15:55;slebresne;* In SSTableMetadata default ctor, we need to use MAX_VALUE, not MIN_VALUE, same as when we read metadata that don't track the deletion time.
* Nit: In the Descriptor version, it uses version ""ic"" but trunk version is now ""ja"".
* Nit: I don't know why CompactionController don't already keep a reference to the compacted sstable, but let's do it. It's weird to have to pass it to getTTLExpiredSSTables even though CompactionController has been created with the sstables in the first place.
* In CompactionTask, we could use actuallyCompact for the getPreheatKeyCache branch. But we shouldn't use it in the createCompactionWriter call.
* Concerning getTTLExpiredSSTables:
** The {{candidate.getMaxTimestamp() > minTimestamp}} check should use a '>=' because tombstone wins over normal insert on a timestamp tie.
** we should pass gcBefore as argument for the static version, and use the one of the controller for the non-static version, rather than recomputing it from scratch.
** Nit: Let's rename the method to say getFullyExpiredSSTables (or maybe getDroppableSSTables). This is not TTL specific, but about gcable tombstones (that might not come from TTL).
** I don't think it's ok to drop sstables without having done the min_timestamp check, unless overlapping is empty (and I'm not sure it's worth special casing). Overall, I find the method a bit hard to follow. I would suggest a slightly refactored version like:
{noformat}
List<SSTableReader> candidates = new ArrayList<SSTableReader>();
long minTimestamp = Integer.MAX_VALUE;

for (SSTableReader sstable : overlapping)
    minTimestamp = Math.min(minTimestamp, sstable.getMinTimestamp());

for (SSTableReader candidate : compacting)
{
    if (candidate.maxLocalDeletionTime() < gcBefore)
        candidates.add(candidate);
    else
        minTimestamp = Math.min(minTimestamp, sstable.getMinTimestamp());
}

// we still need to keep candidates that might shadow something in a
// non-candidate sstable. And if we remove a sstable from the candidates, we
// must take it's timestamp into account (hence the sorting below).
Collections.sort(candidates, SSTable.maxTimestampComparator);

Iterator<SSTableReader> iterator = candidates.iterator();
while (iterator.hasNext())
{
    SSTableReader candidate = iterator.next();
    if (candidate.getMaxTimestamp() >= minTimestamp)
    {
        minTimestamp = Math.min(candidate.getMinTimestamp(), minTimestamp);
        iterator.remove();
    }
    else
    {
        logger.debug(""Dropping TTL Expired SSTable {} (maxLocalDeletionTime={}, localTime={})"",
                candidate, candidate.getSSTableMetadata().maxLocalDeletionTime, localTimeSeconds);
    }
}
return new HashSet<SSTableReader>(candidates);
{noformat}
;;;","22/Mar/13 07:35;marcuse;attaching a rebased track local deletiontime patch as well

the updated drop-sstable patch addresses the comments and:
* fixes the shaky mutate sstablelevel unit test that seemed to break 50% of the times i ran it
* updates current_version to ""ja"" in Descriptor
* fixes the BootstrapTest which needs a current-version filename to pass;;;","22/Mar/13 10:43;slebresne;Alright, lgtm. Committed, thanks.

Note that I do think we might want to be more aggressive in checking when there is droppable sstable. Currently, we do it as part of worthDroppingTombstone, which is fine, but that in turn is only checked if there is no other compaction to do. And we might want to check if there is fully droppable sstable before we check for other compactions, though that check wouldn't be totally free so it's unclear if it's desirable in general. Anyway, this a matter for a later ticket anyway (an alternative being the specific compaction strategy suggested by Jonathan, that would not really compact, just use the droppable check). ;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Missing columns, errors when requesting specific columns from wide rows",CASSANDRA-5225,12631118,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,slebresne,thobbs,thobbs,06/Feb/13 19:19,16/Apr/19 09:32,14/Jul/23 05:53,13/Feb/13 08:21,1.2.2,,,,,,0,,,,,"With Cassandra 1.2.1 (and probably 1.2.0), I'm seeing some problems with Thrift queries that request a set of specific column names when the row is very wide.

To reproduce, I'm inserting 10 million columns into a single row and then randomly requesting three columns by name in a loop.  It's common for only one or two of the three columns to be returned.  I'm also seeing stack traces like the following in the Cassandra log:

{noformat}
ERROR 13:12:01,017 Exception in thread Thread[ReadStage:76,5,main]
java.lang.RuntimeException: org.apache.cassandra.io.sstable.CorruptSSTableException: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0 (/var/lib/cassandra/data/Keyspace1/CF1/Keyspace1-CF1-ib-5-Data.db, 14035168 bytes remaining)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1576)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: org.apache.cassandra.io.sstable.CorruptSSTableException: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0 (/var/lib/cassandra/data/Keyspace1/CF1/Keyspace1-CF1-ib-5-Data.db, 14035168 bytes remaining)
	at org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:69)
	at org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:81)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:68)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:133)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:65)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1358)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1215)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1127)
	at org.apache.cassandra.db.Table.getRow(Table.java:355)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:64)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:1052)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1572)
	... 3 more
{noformat}

This doesn't seem to happen when the row is smaller, so it might have something to do with incremental large row compaction.",,abashir,brandon.williams,christianmovi,colinkuo,dmeyer,eldenbishop,mck,mukundneharkar,slebresne,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5210,,,,,,,,,,,,,,,,,"07/Feb/13 18:59;slebresne;5225.txt;https://issues.apache.org/jira/secure/attachment/12568441/5225.txt","18/Jun/13 22:58;dmeyer;corrected-pycassa-repro.py;https://issues.apache.org/jira/secure/attachment/12588474/corrected-pycassa-repro.py","06/Feb/13 19:20;thobbs;pycassa-repro.py;https://issues.apache.org/jira/secure/attachment/12568275/pycassa-repro.py",,,,,,,,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,311614,,,Tue Jun 18 22:58:17 UTC 2013,,,,,,,,,,"0|i1hrzb:",311960,,,,,,,,,brandon.williams,,brandon.williams,Critical,,,,,,,,,,,,,,,dmeyer,,,"06/Feb/13 19:20;thobbs;Attached python script reproduces the issue with pycassa.;;;","06/Feb/13 23:00;brandon.williams;Bisect says the winner is CASSANDRA-3885, but I never encountered the corrupt sstable exception.;;;","07/Feb/13 17:24;yukim;It looks like cassandra is reading from wrong column index here(https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/columniterator/SSTableNamesIterator.java#L236).

Suppose we have col indexes of [[1..5][6..10][11..15][16..20]](numbers are column names), and we want to 'SELECT 2, 18 FROM CF';
First, we check '2' against indexes and get indexes[0]. Next, we check '18' against indexes with lastIndexIdx of 0.
Now, because we are limiting the second index check to the sublist of indexes[0, lastIndexIdx + 1] here(https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/io/sstable/IndexHelper.java#L186), it only checks against only first two indexes and gets wrong index position of indexes[2]. So it thinks '20' is not in the sstable.

In fact, if I removed sublisting part from IndexHelper.indexFor, SSTableNamesIterator started returning collect values. But I don't know that's the right way to do. [~slebresne]?;;;","07/Feb/13 17:54;slebresne;With just eyeballing the code, I would say that the line at https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/io/sstable/IndexHelper.java#L179 should be:
{noformat}
if (!reversed)
{noformat}
i.e. both branch should be inverted. The goal of the lastIndex parameter is to ignore index block we know are ""behind"" us. So when we go forward (not reversed) you'd want to look at [lastIndex, index.size()], not the contrary.;;;","07/Feb/13 18:59;slebresne;So attaching patch for this. Interestingly enough, the IndexHelperTest were broken too (which kind of make it obvious this was doing the wrong thing).

I haven't tried the pycassa repro script to validate this fixes thing though.;;;","07/Feb/13 19:05;brandon.williams;It still doesn't pass :(;;;","08/Feb/13 09:12;slebresne;Are you sure you applied the patch correctly? I just tested the pycassa-repro.py test above and it fails every time without the patch but haven't failed once with the patch.;;;","08/Feb/13 09:56;mukundneharkar;Yes..The patch works with my test code too..
;;;","08/Feb/13 18:59;brandon.williams;I applied the patch correctly, but the bug is in the pycassa script itself... I was hitting an edge case where it asked for the same column twice.;;;","12/Feb/13 19:17;eldenbishop;This patch also fixes CASSANDRA-5210. I'll mark that one as a dupe.;;;","13/Feb/13 03:18;mukundneharkar;I am new to JIRA. By my mistake this bug gets assigned to me when I clicked on Testing.  Can someone please help me to reassign this to other?;;;jira-users","13/Feb/13 03:22;brandon.williams;+1;;;","13/Feb/13 08:21;slebresne;Committed, thanks.;;;","07/Mar/13 19:09;abashir;This affects 1.1.x as well; will the fix be a part of 1.1.11?;;;","07/Mar/13 21:23;jbellis;If you have a test case that fails against 1.1, please post it.;;;","08/Mar/13 23:10;abashir;The reporter for CASSANDRA-5210 (marked as a dupe of this) tested as far back as 0.8;;;","08/Mar/13 23:17;jbellis;Nevertheless, the bug fixed here was a regression introduced by CASSANDRA-3885 for 1.2.0.;;;","18/Jun/13 19:41;dmeyer;Added a dtest to cover this scenario:
https://github.com/riptano/cassandra-dtest/blob/75bffeba0af410a41eb97b269ae1c94f4227c312/wide_rows_test.py;;;","18/Jun/13 22:58;dmeyer;Fixed a small bug in the repro script.  Please use the 'corrected' version.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
fix bug on decommission and removeNode,CASSANDRA-5216,12630576,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,wy96f,wy96f,wy96f,03/Feb/13 15:42,16/Apr/19 09:32,14/Jul/23 05:53,04/Feb/13 19:10,1.2.2,,,,,,0,gossip,,,,"1. If one node decommissioned, the epState.isAlive is always true since it is DEAD_STATES in convict(). 0001 patch fixes that.
2. If we removeNode B on A,  we should put expireTime of B in expireTimeEndpointMap of A, otherwise the epState of B will never be removed from gossip entirely. 0002 patch fixes that.
3. After removeNode B, C reboots and receives epState of B. Since B is not in the tokenMetadata, C just wipes away B without recording B's expireTime. In this case, B will be always in gossip. 0003 patch fixes that.",,rcoli,wy96f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/13 15:45;wy96f;0001-fix-isAlive-bug-on-decommission.patch;https://issues.apache.org/jira/secure/attachment/12567771/0001-fix-isAlive-bug-on-decommission.patch","03/Feb/13 15:45;wy96f;0002-fix-expireTime-bug-on-removeNode.patch;https://issues.apache.org/jira/secure/attachment/12567772/0002-fix-expireTime-bug-on-removeNode.patch","03/Feb/13 15:45;wy96f;0003-fix-expireTime-bug-on-handleStateRemoving-after-rebo.patch;https://issues.apache.org/jira/secure/attachment/12567773/0003-fix-expireTime-bug-on-handleStateRemoving-after-rebo.patch",,,,,,,,,,,,,,,,,,3.0,wy96f,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,311072,,,Mon Feb 04 19:10:08 UTC 2013,,,,,,,,,,"0|i1honb:",311420,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"04/Feb/13 19:10;brandon.williams;Good work, thanks.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
AE in DataTracker.markCompacting,CASSANDRA-5214,12630359,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,brandon.williams,brandon.williams,01/Feb/13 11:53,16/Apr/19 09:32,14/Jul/23 05:53,04/Feb/13 16:51,1.2.2,,,,,,0,,,,,"On 1.2 branch:

{noformat}
 INFO [main] 2013-02-01 05:50:07,709 CassandraDaemon.java (line 103) Logging initialized
 INFO [main] 2013-02-01 05:50:07,730 CassandraDaemon.java (line 125) JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26
 INFO [main] 2013-02-01 05:50:07,731 CassandraDaemon.java (line 126) Heap size: 1046937600/1046937600
 INFO [main] 2013-02-01 05:50:07,731 CassandraDaemon.java (line 127) Classpath: /tmp/dtest-4ju3_j/test/node1/conf:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/build/classes/main:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/build/classes/thrift:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/antlr-3.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/avro-1.4.0-fixes.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/avro-1.4.0-sources-fixes.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/commons-cli-1.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/commons-codec-1.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/commons-lang-2.6.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/compress-lzf-0.8.4.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/concurrentlinkedhashmap-lru-1.3.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/guava-13.0.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/high-scale-lib-1.1.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jackson-core-asl-1.9.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jackson-mapper-asl-1.9.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jamm-0.2.5.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jline-1.0.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/json-simple-1.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/libthrift-0.7.0.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/log4j-1.2.16.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/metrics-core-2.0.3.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/netty-3.5.9.Final.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/servlet-api-2.5-20081211.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/slf4j-api-1.7.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/slf4j-log4j12-1.7.2.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/snakeyaml-1.6.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/snappy-java-1.0.4.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/snaptree-0.1.jar:/var/lib/buildbot/slaves/slave/cassandra-1_2/build/lib/jamm-0.2.5.jar
 INFO [main] 2013-02-01 05:50:07,733 CLibrary.java (line 61) JNA not found. Native methods will be disabled.
 INFO [main] 2013-02-01 05:50:07,748 DatabaseDescriptor.java (line 131) Loading settings from file:/tmp/dtest-4ju3_j/test/node1/conf/cassandra.yaml
 INFO [main] 2013-02-01 05:50:08,168 DatabaseDescriptor.java (line 190) DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO [main] 2013-02-01 05:50:08,168 DatabaseDescriptor.java (line 204) disk_failure_policy is stop
 INFO [main] 2013-02-01 05:50:08,174 DatabaseDescriptor.java (line 265) Global memtable threshold is enabled at 332MB
 INFO [main] 2013-02-01 05:50:08,911 CacheService.java (line 111) Initializing key cache with capacity of 49 MBs.
 INFO [main] 2013-02-01 05:50:08,923 CacheService.java (line 140) Scheduling key cache save to each 14400 seconds (going to save all keys).
 INFO [main] 2013-02-01 05:50:08,924 CacheService.java (line 154) Initializing row cache with capacity of 0 MBs and provider org.apache.cassandra.cache.SerializingCacheProvider
 INFO [main] 2013-02-01 05:50:08,931 CacheService.java (line 166) Scheduling row cache save to each 0 seconds (going to save all keys).
 INFO [main] 2013-02-01 05:50:09,438 DatabaseDescriptor.java (line 542) Couldn't detect any schema definitions in local storage.
 INFO [main] 2013-02-01 05:50:09,440 DatabaseDescriptor.java (line 545) Found table data in data directories. Consider using the CLI to define your schema.
 INFO [CompactionExecutor:1] 2013-02-01 05:50:09,579 ColumnFamilyStore.java (line 678) Enqueuing flush of Memtable-local@524805736(133/133 serialized/live bytes, 6 ops)
 INFO [FlushWriter:1] 2013-02-01 05:50:09,592 Memtable.java (line 447) Writing Memtable-local@524805736(133/133 serialized/live bytes, 6 ops)
 INFO [FlushWriter:1] 2013-02-01 05:50:09,670 Memtable.java (line 481) Completed flushing /tmp/dtest-4ju3_j/test/node1/data/system/local/system-local-ib-1-Data.db (176 bytes) for commitlog position ReplayPosition(segmentId=1359719409398, position=425)
ERROR [CompactionExecutor:2] 2013-02-01 05:50:09,681 CassandraDaemon.java (line 135) Exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.AssertionError
    at org.apache.cassandra.db.DataTracker.markCompacting(DataTracker.java:183)
    at org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy.getNextBackgroundTask(SizeTieredCompactionStrategy.java:128)
    at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:185)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
{noformat}",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310854,,,Mon Feb 04 16:51:17 UTC 2013,,,,,,,,,,"0|i1hnaf:",311199,,,,,,,,,,,,Normal,,,,,,,,,,,,,,,,,,"01/Feb/13 12:26;brandon.williams;CASSANDRA-5151 looks like my guess here.;;;","04/Feb/13 16:51;jbellis;should be fixed in 82de0ec75689d84ee6a4fa2d9a960f3326e5387a.  (CliTest no longer throws.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unable to start when using Ec2Snitch,CASSANDRA-5212,12630222,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,jared.biel@bolderthinking.com,jared.biel@bolderthinking.com,31/Jan/13 21:37,16/Apr/19 09:32,14/Jul/23 05:53,01/Feb/13 20:38,,,,,,,0,,,,,"Hello, we're using vanilla Cassandra in an EC2 environment and I just tested upgrading from 1.2.0 to 1.2.1 on two test instances. Cassandra fails to start because it's unable to load the Ec2Snitch. Version 1.2.0 was working OK. I have tried this on uninitialized/empty instances and received the same result. Cassandra successfully starts when switching to SimpleSnitch. Log output is below. We're using the official debian package from apache.org. Please let me know if you need any more details, thanks!

output.log
{code}
ERROR 21:25:06,684 Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Error instantiating snitch class 'org.apache.cassandra.locator.Ec2Snitch'.
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:475)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:525)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:338)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:122)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:151)
	at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:315)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:532)
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:457)
	... 10 more
Caused by: java.lang.ExceptionInInitializerError
	at org.apache.cassandra.locator.Ec2Snitch.<init>(Ec2Snitch.java:65)
	... 15 more
Caused by: java.lang.RuntimeException: Unable to read cassandra-rackdc.properties
	at org.apache.cassandra.locator.SnitchProperties.<clinit>(SnitchProperties.java:39)
	... 16 more
Caused by: java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:435)
	at java.util.Properties.load0(Properties.java:354)
	at java.util.Properties.load(Properties.java:342)
	at org.apache.cassandra.locator.SnitchProperties.<clinit>(SnitchProperties.java:35)
	... 16 more
Error instantiating snitch class 'org.apache.cassandra.locator.Ec2Snitch'.
Fatal configuration error; unable to start server.  See log for stacktrace.
Service exit with a return value of 1
{code}

system.log
{code}
INFO [main] 2013-01-31 21:25:05,016 CassandraDaemon.java (line 123) JVM vendor/version: OpenJDK 64-Bit Server VM/1.6.0_24
<SNIP>
ERROR [main] 2013-01-31 21:24:52,028 DatabaseDescriptor.java (line 509) Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Error instantiating snitch class 'org.apache.cassandra.locator.Ec2Snitch'.
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:475)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:525)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:338)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:122)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:151)
	at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:315)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:532)
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:457)
	... 10 more
Caused by: java.lang.ExceptionInInitializerError
	at org.apache.cassandra.locator.Ec2Snitch.<init>(Ec2Snitch.java:65)
	... 15 more
Caused by: java.lang.RuntimeException: Unable to read cassandra-rackdc.properties
	at org.apache.cassandra.locator.SnitchProperties.<clinit>(SnitchProperties.java:39)
	... 16 more
Caused by: java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:435)
	at java.util.Properties.load0(Properties.java:354)
	at java.util.Properties.load(Properties.java:342)
	at org.apache.cassandra.locator.SnitchProperties.<clinit>(SnitchProperties.java:35)
	... 16 more
<SNIP>
 INFO [main] 2013-01-31 21:25:06,656 DatabaseDescriptor.java (line 267) Global memtable threshold is enabled at 407MB
ERROR [main] 2013-01-31 21:25:06,684 DatabaseDescriptor.java (line 509) Fatal configuration error
org.apache.cassandra.exceptions.ConfigurationException: Error instantiating snitch class 'org.apache.cassandra.locator.Ec2Snitch'.
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:475)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:525)
	at org.apache.cassandra.config.DatabaseDescriptor.loadYaml(DatabaseDescriptor.java:338)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:122)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:151)
	at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:315)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:616)
	at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
Caused by: java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:57)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:532)
	at org.apache.cassandra.utils.FBUtilities.construct(FBUtilities.java:457)
	... 10 more
Caused by: java.lang.ExceptionInInitializerError
	at org.apache.cassandra.locator.Ec2Snitch.<init>(Ec2Snitch.java:65)
	... 15 more
Caused by: java.lang.RuntimeException: Unable to read cassandra-rackdc.properties
	at org.apache.cassandra.locator.SnitchProperties.<clinit>(SnitchProperties.java:39)
	... 16 more
Caused by: java.lang.NullPointerException
	at java.util.Properties$LineReader.readLine(Properties.java:435)
	at java.util.Properties.load0(Properties.java:354)
	at java.util.Properties.load(Properties.java:342)
	at org.apache.cassandra.locator.SnitchProperties.<clinit>(SnitchProperties.java:35)
	... 16 more
{code}","Ubuntu 12.04 x64
OpenJDK 64-Bit Server VM/1.6.0_24
m1.large EC2 Instance

MAX_HEAP_SIZE=""1242M""
HEAP_NEWSIZE=""200M""
",jared.biel@bolderthinking.com,jasobrown,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5215,,,,,,,,,,,,,,,,,,,,"01/Feb/13 00:39;vijay2win@yahoo.com;0001-CASSANDRA-5212.patch;https://issues.apache.org/jira/secure/attachment/12567467/0001-CASSANDRA-5212.patch",,,,,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310718,,,Fri Feb 01 20:38:28 UTC 2013,,,,,,,,,,"0|i1hmg7:",311063,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"31/Jan/13 23:34;jbellis;It tells you what the problem is:

Caused by: java.lang.RuntimeException: Unable to read cassandra-rackdc.properties
;;;","31/Jan/13 23:51;jared.biel@bolderthinking.com;Thanks - I didn't see that needle in the (hay) stack. It looks like as a result of CASSANDRA-5155 the file cassandra-rackdc.properties is now required to be in /etc/cassandra. However, this file is not distributed with the standard Debian Cassandra package. Maybe this should be documented in cassandra.yaml or the file should be distributed as part of the Debian package? Thanks for your help.;;;","31/Jan/13 23:54;jbellis;Hmm, yes.  We should make it optional and/or add to the debian package.;;;","01/Feb/13 12:42;brandon.williams;+1, but instead of silently swallowing the exception maybe a WARN would be more appropriate.;;;","01/Feb/13 13:02;jasobrown;I agree with @driftx that we should add the WARN statement. Otherwise, +1;;;","01/Feb/13 20:38;vijay2win@yahoo.com;Committed to 1.1, 1.2 and trunk, Thanks Brandon...;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Migrating Clusters with gossip tables that have old dead nodes causes NPE, inability to join cluster",CASSANDRA-5211,12630218,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,rbranson,rbranson,31/Jan/13 21:18,16/Apr/19 09:32,14/Jul/23 05:53,12/Feb/13 22:55,1.2.2,,,,,,0,,,,,"I had done a removetoken on this cluster when it was 1.1.x, and it had a ""ghost"" entry for the removed node still in the stored ring data. When the nodes loaded the table up after conversion to 1.2 and attempting to migrate to VNodes, I got the following traceback:

ERROR [WRITE-/10.0.0.0] 2013-01-31 18:35:44,788 CassandraDaemon.java (line 133) Exception in thread Thread[WRITE-/10.0.0.0,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:167)
	at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:124)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.getString(JdbcUTF8.java:73)
	at org.apache.cassandra.cql.jdbc.JdbcUTF8.compose(JdbcUTF8.java:93)
	at org.apache.cassandra.db.marshal.UTF8Type.compose(UTF8Type.java:32)
	at org.apache.cassandra.cql3.UntypedResultSet$Row.getString(UntypedResultSet.java:96)
	at org.apache.cassandra.db.SystemTable.loadDcRackInfo(SystemTable.java:402)
	at org.apache.cassandra.locator.Ec2Snitch.getDatacenter(Ec2Snitch.java:117)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:127)
	at org.apache.cassandra.net.OutboundTcpConnection.isLocalDC(OutboundTcpConnection.java:74)
	at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:270)
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:142)

This is because these ghost nodes had a NULL tokens list in the system/peers table. A workaround was to delete the offending row in the system/peers table and restart the node.",,rbranson,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/13 14:20;brandon.williams;5211.txt;https://issues.apache.org/jira/secure/attachment/12568226/5211.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310714,,,Tue Feb 12 22:55:49 UTC 2013,,,,,,,,,,"0|i1hmfb:",311059,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"06/Feb/13 13:56;brandon.williams;Rick, I don't suppose you might happen to have one of these problem system tables left anywhere?  The trace indicates that the rack was missing but the dc wasn't, and we only write those together in a single insert.;;;","06/Feb/13 14:20;brandon.williams;Confirmed that a null tokens list won't cause this.  Regardless of how we got here, it's more correct to confirm the existence of the dc and rack than just the dc, so patch to do so.;;;","12/Feb/13 22:17;jbellis;+1;;;","12/Feb/13 22:55;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cli shouldn't set default username and password,CASSANDRA-5208,12630190,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,31/Jan/13 19:00,16/Apr/19 09:32,14/Jul/23 05:53,31/Jan/13 19:17,1.2.2,,,Legacy/Tools,,,0,,,,,"Currently cli sets default username and password if none are set (in CliOptions.processArgs). Because of this cli will always authenticate, whether or not this was the intent of the user and CliMain.connect() ""if ((sessionState.username != null) && (sessionState.password != null))"" condition will always be true.

This breaks authentication in at least two scenarios:
1. Authenticator allows anonymous access and a user wants to login anonymously - instead he will get AuthenticationException because user ""default"" will most likely not exist.
2. Authenticator doesn't user username/password pairs for login but something like Kerberos instead. Thrift's login with u:default, p:"""" will still be called and AuthenticationException will be thrown, again.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/13 19:00;aleksey;5208.txt;https://issues.apache.org/jira/secure/attachment/12567410/5208.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310686,,,Thu Jan 31 19:17:52 UTC 2013,,,,,,,,,,"0|i1hm93:",311031,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"31/Jan/13 19:09;jbellis;+1;;;","31/Jan/13 19:17;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Validate login for USE queries,CASSANDRA-5207,12630185,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,31/Jan/13 18:39,16/Apr/19 09:32,14/Jul/23 05:53,31/Jan/13 19:09,1.2.2,,,,,,0,,,,,"CASSANDRA-5144 added login validation to Thrift set_keyspace method. Same should be done for CQL2 and CQL3 USE queries, otherwise C* will leak keyspace existence to strangers even when the configured authenticator requires login.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/13 18:40;aleksey;5207.txt;https://issues.apache.org/jira/secure/attachment/12567404/5207.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310681,,,Thu Jan 31 19:09:31 UTC 2013,,,,,,,,,,"0|i1hm7z:",311026,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"31/Jan/13 18:49;jbellis;+1;;;","31/Jan/13 19:09;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Repair command should report error when replica node is dead,CASSANDRA-5203,12630040,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,30/Jan/13 23:58,16/Apr/19 09:32,14/Jul/23 05:53,01/Feb/13 21:35,1.1.10,1.2.2,,,,,0,,,,,"CASSANDRA-4767 makes nodetool repair command to print progress, but when replica node is dead and repair cannot be proceeded, nodetool repair just report session finished. Instead, nodetool should report session is failed.

Also, it is nice to exit command with status code of 1 when repair failed.
",,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/13 23:01;yukim;5203-1.1.txt;https://issues.apache.org/jira/secure/attachment/12567453/5203-1.1.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310536,,,Fri Feb 01 21:35:48 UTC 2013,,,,,,,,,,"0|i1hlbz:",310881,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"31/Jan/13 23:01;yukim;Patch to fail on node dead and exit nodetool with error status code.;;;","01/Feb/13 00:44;brandon.williams;+1;;;","01/Feb/13 21:35;yukim;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"CFs should have globally and temporally unique CF IDs to prevent ""reusing"" data from earlier incarnation of same CF name",CASSANDRA-5202,12629944,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,marat,marat,30/Jan/13 17:17,16/Apr/19 09:32,14/Jul/23 05:53,15/Jan/14 03:13,2.1 beta1,,,,,,9,qa-resolved,test,,,"Attached is a driver that sequentially:

1. Drops keyspace
2. Creates keyspace
4. Creates 2 column families
5. Seeds 1M rows with 100 columns
6. Queries these 2 column families

The above steps are repeated 1000 times.

The following exception is observed at random (race - SEDA?):

ERROR [ReadStage:55] 2013-01-29 19:24:52,676 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[ReadStage:55,5,main]
java.lang.AssertionError: DecoratedKey(-1, ) != DecoratedKey(62819832764241410631599989027761269388, 313a31) in C:\var\lib\cassandra\data\user_role_reverse_index\business_entity_role\user_role_reverse_index-business_entity_role-hf-1-Data.db
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:60)
	at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:67)
	at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:79)
	at org.apache.cassandra.db.CollationController.collectAllData(CollationController.java:256)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:64)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1367)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1229)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1164)
	at org.apache.cassandra.db.Table.getRow(Table.java:378)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:69)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:822)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1271)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)


This exception appears in the server at the time of client submitting a query request (row slice) and not at the time data is seeded. The client times out and this data can no longer be queried as the same exception would always occur from there on.

Also on iteration 201, it appears that dropping column families failed and as a result their recreation failed with unique column family name violation (see exception below). Note that the data files are actually gone, so it appears that the server runtime responsible for creating column family was out of sync with the piece that dropped them:

Starting dropping column families
Dropped column families
Starting dropping keyspace
Dropped keyspace
Starting creating column families
Created column families
Starting seeding data
Total rows inserted: 1000000 in 5105 ms
Iteration: 200; Total running time for 1000 queries is 232; Average running time of 1000 queries is 0 ms
Starting dropping column families
Dropped column families
Starting dropping keyspace
Dropped keyspace
Starting creating column families
Created column families
Starting seeding data
Total rows inserted: 1000000 in 5361 ms
Iteration: 201; Total running time for 1000 queries is 222; Average running time of 1000 queries is 0 ms
Starting dropping column families
Starting creating column families
Exception in thread ""main"" com.netflix.astyanax.connectionpool.exceptions.BadRequestException: BadRequestException: [host=127.0.0.1(127.0.0.1):9160, latency=2468(2469), attempts=1]InvalidRequestException(why:Keyspace names must be case-insensitively unique (""user_role_reverse_index"" conflicts with ""user_role_reverse_index""))
	at com.netflix.astyanax.thrift.ThriftConverter.ToConnectionPoolException(ThriftConverter.java:159)
	at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:60)
	at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:27)
	at com.netflix.astyanax.thrift.ThriftSyncConnectionFactoryImpl$1.execute(ThriftSyncConnectionFactoryImpl.java:140)
	at com.netflix.astyanax.connectionpool.impl.AbstractExecuteWithFailoverImpl.tryOperation(AbstractExecuteWithFailoverImpl.java:69)
	at com.netflix.astyanax.connectionpool.impl.AbstractHostPartitionConnectionPool.executeWithFailover(AbstractHostPartitionConnectionPool.java:255)
	at com.netflix.astyanax.thrift.ThriftKeyspaceImpl.createKeyspace(ThriftKeyspaceImpl.java:569)
	at com.nuance.mca.astyanax.App.recreateKeyspaceSchema(App.java:139)
	at com.nuance.mca.astyanax.App.main(App.java:88)
Caused by: InvalidRequestException(why:Keyspace names must be case-insensitively unique (""user_role_reverse_index"" conflicts with ""user_role_reverse_index""))
	at org.apache.cassandra.thrift.Cassandra$system_add_keyspace_result.read(Cassandra.java:30010)
	at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
	at org.apache.cassandra.thrift.Cassandra$Client.recv_system_add_keyspace(Cassandra.java:1285)
	at org.apache.cassandra.thrift.Cassandra$Client.system_add_keyspace(Cassandra.java:1272)
	at com.netflix.astyanax.thrift.ThriftKeyspaceImpl$14.internalExecute(ThriftKeyspaceImpl.java:584)
	at com.netflix.astyanax.thrift.ThriftKeyspaceImpl$14.internalExecute(ThriftKeyspaceImpl.java:572)
	at com.netflix.astyanax.thrift.AbstractOperationImpl.execute(AbstractOperationImpl.java:55)
	... 7 more



","OS: Windows 7, 
Server: Cassandra 1.1.9 release drop
Client: astyanax 1.56.21, 
JVM: Sun/Oracle JVM 64 bit (jdk1.6.0_27)",aleksey,ash2k,bcoverston,cburroughs,christianmovi,colinkuo,elreydetodo@gmail.com,goody44,jay.zhuang,jeromatron,jjordan,johnny15676,jshook,lacho,marat,mishail,olegdulin,palacsint,rcoli,rfwagner@gmail.com,richdawe,sebastian.estevez@datastax.com,soverton,stantonk,wadey,xedin,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5905,CASSANDRA-6411,CASSANDRA-8350,,,CASSANDRA-9179,CASSANDRA-7846,,,CASSANDRA-6060,CASSANDRA-4687,,,,,,,,,,"10/Jan/14 00:23;yukim;0001-make-2i-CFMetaData-have-parent-s-CF-ID.patch;https://issues.apache.org/jira/secure/attachment/12622301/0001-make-2i-CFMetaData-have-parent-s-CF-ID.patch","10/Jan/14 00:23;yukim;0002-Don-t-scrub-2i-CF-if-index-type-is-CUSTOM.patch;https://issues.apache.org/jira/secure/attachment/12622302/0002-Don-t-scrub-2i-CF-if-index-type-is-CUSTOM.patch","10/Jan/14 00:23;yukim;0003-Fix-user-defined-compaction.patch;https://issues.apache.org/jira/secure/attachment/12622303/0003-Fix-user-defined-compaction.patch","10/Jan/14 00:23;yukim;0004-Fix-serialization-test.patch;https://issues.apache.org/jira/secure/attachment/12622304/0004-Fix-serialization-test.patch","10/Jan/14 17:45;yukim;0005-Create-system_auth-tables-with-fixed-CFID.patch;https://issues.apache.org/jira/secure/attachment/12622407/0005-Create-system_auth-tables-with-fixed-CFID.patch","10/Jan/14 23:10;aleksey;0005-auth-v2.txt;https://issues.apache.org/jira/secure/attachment/12622458/0005-auth-v2.txt","03/Jan/14 02:44;yukim;5202.txt;https://issues.apache.org/jira/secure/attachment/12621224/5202.txt","30/Jan/13 17:19;marat;astyanax-stress-driver.zip;https://issues.apache.org/jira/secure/attachment/12567169/astyanax-stress-driver.zip",,,,,,,,,,,,,8.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310440,,,Thu Mar 20 15:22:09 UTC 2014,,,,,,,,,,"0|i1hkqn:",310785,,,,,,,,,xedin,,xedin,Normal,,,,,,,,,,,,,,,shawn.kumar,,,"30/Jan/13 17:19;marat;Unpack, run: mvn install, mvn eclipse:eclipse;;;","30/Jan/13 17:35;marat;The sever is a single node Cassandra 1.1.9 release drop and there is one and only client running at the same time.;;;","30/Jan/13 17:47;marat;Also, I can no longer create this column family - even though when I try to drop it - it errors with an exception that this column family does not exists, but then when I try to create it I get an exception that ""why:Keyspace names must be case-insensitively unique (""user_role_reverse_index"" conflicts with ""user_role_reverse_index"""";;;","30/Jan/13 18:44;yukim;Thanks Marat, your testcase greatly helped me find the cause.
I tested your driver against current 1.1 branch.

The key cache is kept in memory beyond drop/create keyspace.
So you create first ks/cf and do insert/read, which causes key cache to fill up.
Then, you drop ks/cf and do the same insert/read again. This time in read, it finds the reading position from key cache from previous read, but that position is for dropped ks/cf not the current one.

We definitely need to clear caches when we drop columnfamilies.

I think this is also the cause of CASSANDRA-4687.;;;","30/Jan/13 18:53;jbellis;The cache key is a {{Pair<Descriptor, DecoratedKey>}} -- I would expect that to be robust across drop/recreate unless there's a bug in Descriptor HashCode/Equals.;;;","30/Jan/13 19:46;marat;Thanks Yuki. When do you expect the fix to be available for testing and released? 
Also, what about failure to recreate column family because system thinks that it still exists even though the data files are gone?;;;","30/Jan/13 19:56;yukim;When cf is created again, it starts with file generation of 1 because SSTables are once removed when cf is dropped. That makes Descriptor collision of newly created SSTables and Descriptors in key cache.

Patch attached to clear key cache when cf is dropped.;;;","30/Jan/13 20:12;jbellis;Is this race-proof vs cache additions from concurrent reads?;;;","30/Jan/13 20:20;jbellis;One way to race-proof it would be to

# make Descriptor use CFMetadata.cfId uuid instead of ksname/cfname (bonus: would make lookups faster).  May have to make a separate class instead of actual Descriptor since that's really more about FS-level data.
# make CFM.getId generate random uuids instead of deterministic ones;;;","30/Jan/13 23:09;marat;Clarification. I think what happened at iteration 201 was that dropping of keyspace failed, so the subsequent attempts of running the driver would fail as dropping column families and dropping the keyspace were part of the same try-catch block, and attempt to drop non existing column family would throw and so the code would attempt to create an already existing keyspace, which was not being dropped because an exception thrown by column family drop never gave column family drop a chance to execute.

So the issue was not in dropping the column family but rather the keyspace.;;;","11/Mar/13 18:41;yukim;First try for random CF Id for trunk: https://github.com/yukim/cassandra/commits/5202-3

I added 'cf_id' column to schema_columnfamilies to track current CF Id so I made this patch to trunk. I think this should work on 1.2 branch though.
I still need to work on loading saved key cache, because when opening files like saved key caches, those would have the CF ID at the time of opening because right now only keyspace and column family names are available. Same thing can be said to SSTables.
I think it is better to store CF ID along with the files so that we can determine which version those files are written. But I'dont want to embed UUID cfId to file name.

I'm considering to modify file format, so that saved key cache has CF ID in its header. But for SSTables, I don't have better way to embed CF ID.;;;","13/Mar/13 14:46;copumpkin;I was getting the same issue and found this bug. As a workaround, I tried changing my testsuite to avoid dropping the keyspace each time, and simply truncate every CF in the keyspace instead, but that seems to lead to the same symptoms. Just thought I'd point that out in case the truncate code path was being overlooked.;;;","13/Mar/13 15:04;jbellis;bq. I don't want to embed UUID cfId to file name

It's ugly, but that's the logical solution, isn't it?  We're saying that sstables for table foo on Tuesday, and table foo on Wednesday after drop and re-create, are really different things, so distinguishing them as such on the FS is if nothing else a good extra layer of protection against mixing the two.

And your patch already stores cfid in the system table, so users could still find out which sstables belong to which table definitions, although granted it's not as convenient.;;;","13/Mar/13 15:07;jbellis;bq. truncate ... seems to lead to the same symptoms

I think we can fix that by recording the sstable generation somewhere, instead of just assuming that ""if no files of generation >= X exist, we should start creating new sstables with X.""

(Which I think would actually be a simpler fix for the issue in general...  but giving different ""versions"" of a table distinct IDs is the Right Solution, so I'm still +1 on that unless Yuki thinks the cost is too high for the practical benefits.);;;","19/Mar/13 02:12;jjordan;bq. I don't want to embed UUID cfId to file name
I like this idea a lot.  I have seen some users with issues of files not be deleted after truncate/drop cf, but the cf is dropped from the schema.  On re-create those old files are going to get re-imported.  Now, this does make it a pain for when you want to restore a cf to another cluster, or to the same cluster after a drop, as you have to rename all the files, or bulk load them and make bulk loader ignore the UUID or something...
;;;","19/Mar/13 13:27;jbellis;-For truncate, we added some code in CASSANDRA-4940 that attempts to fix it but it is race-prone (a new entry can be added to the cache, even after it's cleared).  So we should remove that code as part of our fix.- (Edit: this was row cache, not key cache.);;;","19/Mar/13 17:52;copumpkin;As an additional data point around our truncate issues, it seems like although ALTER cf WITH caching = 'none' helps significantly with the drop/create-related issues, it doesn't seem to help with the truncate issues. On the other hand, disabling key cache altogether in cassandra.yaml seems to help permanently, and using nodetool invalidatekeycache helps temporarily until we access keys after a truncate again. It seems to suggest that caching = 'none' doesn't disable key cache use altogether (perhaps if one already existed before the ALTER, it stays in use?)

Does that jibe with everyone's current understanding of the causes of the issue?;;;","20/Mar/13 19:57;yukim;I took the route to add CFID to file name for both SSTable and serialized cache. https://github.com/yukim/cassandra/commits/5202-4

File's CF ID is checked on CF initialization. I did't change the behavior of sstableloader so it can still stream to the CF of different version.
As Jeremiah stated above, downside is that you cannot just place the file from backup and restore CF without changing the filename.

For truncate issue, I want to see more detail about your operation and setup. SSTable generation number is not reset when you truncate, so usually there is no collision between the generation in keycache and in sstable after truncate.;;;","26/Mar/13 15:41;jbellis;Since we're talking 2.0 anyway for CFID-in-the-filename...

What if we switched to using CAS for CFID creation?  Then everyone would agree on the same CFID, and we could use that in our MessagingService instead of table + cf strings.  (We could also use an int instead of a UUID.)

Would that make any difference here?  Or should we just proceed with this change now and worry about ""global CFIDs"" later?;;;","26/Mar/13 17:04;yukim;If we want to switch (back again) to int for CFID, then CAS would be good.
But we already use UUID for that, I don't feel we need to switch.
;;;","27/Mar/13 16:54;copumpkin;Yuki: I'll try to reduce the truncate issue we're seeing when I get some time. It definitely appears connected to this, and might suggest that there's a broader issue here. 

For what it's worth, we're now randomly also getting the following error message, as described in the closely related CASSANDRA-4687 bug:

Caused by: java.io.EOFException: unable to seek to position X in Y (Z bytes) in read-only mode

also cured by clearing the key caches.
;;;","12/Aug/13 17:21;yukim;Attaching patch which is rebased version of my previous 5202-4 branch against cassandra-2.0.0.

It adds UUID CF ID to SSTable descriptor(== file name).
I tested upgrading from latest 1.2, bulk loader, and truncate and they all worked fine.

I think 2.0 is good number for this change, but it is now on RC1 stage, I'm not sure we should wait till 2.1.;;;","12/Aug/13 21:32;jbellis;So right now our file layout looks like

ks/cf/ks-cf-version-generation-component.db

This would change it to

ks/cf/ks-cf-uuid-version-generation-component.db

Could we clean up the redundancy a little by moving the ID into the directory name?

ks/cf-uuid/version-generation-component.db

;;;","13/Aug/13 12:49;aleksey;I've got a feeling that this patch breaks (concurrent) auth setup and certain DSE tables setup as well (all issuing CREATE TABLE statements for each of the required tables on startup, unless a table already exists). With non-deterministic cf uuids they'll potentially end up with different ids. Then, when schema versions converge to a single id per table, we might lose data (on the nodes that 'lost').;;;","13/Aug/13 13:01;aleksey;We could work around by parsing the create statements instead, setting the CFMetaData id to a predetermined one and manually calling announceMigration(), or adding something like 'WITH id = <uuid>' syntax to CREATE TABLE for these special cases.;;;","13/Aug/13 13:39;jbellis;What if we were willing to give up automagic auth setup?  ""Run this script to enable auth on your cluster"" seems reasonable to me.;;;","13/Aug/13 13:43;aleksey;Auth is just one example. DSE would be affected as well. Probably others who rely on 1.2 schema code with deterministic ids.;;;","13/Aug/13 13:45;aleksey;Auth is the simplest case - there *is* a workaround *and* we control it. I'm more worried about surprising other users unpleasantly.;;;","13/Aug/13 13:56;jbellis;# I'm 100% convinced that (a) this is a problem worth fixing, and (b) we need to switch to nondeterministic IDs to fix it.  Deterministic IDs was, in retrospect, a mistake.  (More pedantically: it's not nondeterminism that fixes it, but unique IDs per unique CREATE; as I've said, an int id incremented via CAS would also work.)  So, you have an uphill battle if you are arguing that it's fine the way it is!
# Adding a ""WITH ID"" workaround doesn't break people any less, it's just a different (hackier, IMO) workaround than moving things to an external script.
# Startup is messy enough without auth/dse special cases, moving them out of core into scripts that can run ""normally"" sounds like a fantastic idea to me even without uuid breakage as a motivation.
# I would be shocked if there is anyone else besides DSE abusing the internals this way, but anyone who does surely understands that if you wire yourself that closely to the internals, you have absolutely no grounds to complain if we break compatibility.;;;","13/Aug/13 14:03;aleksey;I'm not saying that it's worth fixing. Just mentioning what gets broken once the patch is committed.;;;","13/Aug/13 14:50;jbellis;I do think you are right in that we should do this in 2.1 rather than 2.0.1.;;;","13/Aug/13 15:05;yukim;Targeting this to be fixed in 2.1.

[~jbellis] I think we can change path and file name. I also want to try and see how switching to CAS would fix this.;;;","13/Aug/13 23:49;xedin;I actually looked at CASSANDRA-4687 yesterday myself and I came to the same conclusions. I actually like [~jbellis] proposition to include uuid into directory name, instead of including it into file name, which would make it a lot simpler to migrate sstables over to different cluster or backup.

I also think that it could be a good bonus to also clean the cache because of LRU nature of it, if previous entries where hot enough it would just waste time/work to push them out where we would still suffer cache miss every time we read from new cf (in multi ks/cf setup that is still fine as index read is not that bad).;;;","20/Sep/13 16:14;jbellis;CASSANDRA-6060 is related since it also contemplates changing CFID assignment (back to unique ints via CAS).;;;","07/Oct/13 19:14;jbellis;Is there anything that we want to do as part of this ticket instead of 6060?;;;","07/Oct/13 19:21;yukim;Add CF ID to directory name if we still want to distinguish one KS/CF directory to another.
Updating key cache key to use CF ID is another one, but I think that will be done through 6060.;;;","07/Oct/13 19:27;jbellis;bq. Add CF ID to directory name if we still want to distinguish one KS/CF directory to another.

All right, I'm down to narrow the scope here to that.;;;","03/Jan/14 02:44;yukim;(also: https://github.com/yukim/cassandra/commits/5202)

Patch attached for review.

* CF ID is generated randomly upon new CFMetaData creation.
    CF under system keyspaces and ones from older version have deterministic CF ID based on their name.
* SSTable directories are created as ks/cf-cfid. cfid here is hex encoding of UUID bytes. When upgrading, older format ks/cf is still used.
* Saved key cache file name also has cfid appended at the end, and key cache look up is CF ID aware.
;;;","03/Jan/14 23:25;xedin;@jbellis I can review in case Markus has more important stuff on his plate.;;;","04/Jan/14 00:57;jbellis;Thanks, Pavel!;;;","07/Jan/14 06:32;xedin;+1;;;","07/Jan/14 16:35;yukim;Thank you, Pavel.
Committed with update on NEWS.txt.;;;","08/Jan/14 21:29;yukim;Some unit test failures revealed I have to work on this a little bit more.
With committed version, secondary indexes can get wrong directory.
Will post fix here.;;;","08/Jan/14 21:33;aleksey;[~yukim] any way you could alter auth setup code (creation of auth tables) to use the old deterministic cfIds explicitly, while at it?;;;","08/Jan/14 21:56;yukim;[~iamaleksey] sure. Is it just enough to special case tables under 'system_auth' keyspace?;;;","08/Jan/14 21:57;xedin;[~yukim] Sorry, Yuki, I completely forgot about secondary indexes, tested standard scenarios with different setups with stress tho. Will be happy to review v2.;;;","08/Jan/14 22:01;aleksey;[~yukim] I was thinking about announcing the migration for all the auth tables with cf_id preset to what 2.0 would've generated. No special cases anywhere else.;;;","10/Jan/14 00:23;yukim;0001: Fix for 2i CF

I decided to force 2i CF to have the same CF ID from parents'. I think this is safe because 2i CFs are never managed by Schema.

0002: Fix scrub directory for 2i CF

had to exclude scrubbing CUSTOM index type.

0003: Fix user defined compaction

User defined compaction via JMX was broken. It assumes SSTable files are under the directory with older name format.

0004: Fix SerializationsTest
Serialization test on Row object was failing. This is due to the serialized CF ID on test file not matching non deterministic CF ID generated for every test run.
I decided to just generate test file every time.

> [~iamaleksey]

Sorry, still figuring out how to handle auth.
I think current code is sufficient to handle update from older version or setting up authnz. When upgrade, it uses existing directory with deterministic CF ID. Is there any concern that I'm not aware of?

Anyway, I have a feeling we need some way of forcing CF ID, especially when CASSANDRA-6060 comes in. ""WITH ID=xxx"" seems good to me. Maybe separate ticket?;;;","10/Jan/14 00:27;aleksey;[~yukim] Upgrading's not a concern. Concurrent auth CFs creation on fresh 2.1 is, though - they will end up w/ different cf ids. So yes, we need to force a deterministic one.;;;","10/Jan/14 00:44;aleksey;[~yukim] Specifically, instead of simply executing those ""CREATE TABLE""s in PA, CA, and Auth, prepare them, and extract the generated CFMetaData object. Then set cf_id to the deterministic ones (pre-2.1 ones), explicitly. And then call MigrationManager.announceNewColumnFamily() manually, with that final CFMetaData, with the cf_id preset.

This way you only have to change the setup code in PasswordAuthenticator/CassandraAuthorizer/Auth, and not special-case anything beyond that.

Am I making any sense? ;;;","10/Jan/14 17:45;yukim;[~iamaleksey] 0005 attached for system_auth tables with fixed CF ID as we have currently.
Can you give it a review?;;;","10/Jan/14 23:10;aleksey;[~yukim] LGTM. Attaching a v2 though, b/c I don't want to involve overriding the parameters set by newSystemMetadata(), uncluding memtableFlushPeriod, that probably wasn't being overriden at all - just the cf id.;;;","13/Jan/14 14:18;yukim;[~iamaleksey] thanks, I like your approach.

[~xedin] can you review rest of the patches(0001-0004)?;;;","14/Jan/14 21:23;xedin;[~yukim] Looked at 1-4 patches, everything looks good but I have one question to clarify regarding #3 - we can do sstable.descriptor.equals(descriptor) now as ""descriptor"" would also have an absolute path because of the new ""find"" method, is that correct?;;;","14/Jan/14 22:20;yukim;Yes, that is my intention. Since descriptor begins to get directory part from Directories, there is no need to compare file names in string.;;;","15/Jan/14 00:07;xedin;sounds good to me, +1.;;;","15/Jan/14 03:13;yukim;Committed, thanks!;;;","20/Feb/14 19:59;jbellis;Did we give up on this?

bq. Could we clean up the redundancy a little by moving the ID into the directory name?  e.g., ks/cf-uuid/version-generation-component.db

I'm worried about path length, which is limited on Windows.

Edit: to give a specific example, for KS foo Table bar we now have

/var/lib/cassandra/flush/foo/bar-2fbb89709a6911e3b7dc4d7d4e3ca4b4/foo-bar-ka-1-Data.db

I'm proposing

/var/lib/cassandra/flush/foo/bar-2fbb89709a6911e3b7dc4d7d4e3ca4b4/ka-1-Data.db
;;;","20/Mar/14 15:22;brandon.williams;Perhaps we should open a new ticket for that?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Cassandra/Hadoop does not support current Hadoop releases,CASSANDRA-5201,12629942,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,bcoverston,jeltema,jeltema,30/Jan/13 17:11,04/Oct/19 16:20,14/Jul/23 05:53,05/Mar/14 19:00,2.0.6,2.1 beta2,,,,,10,,,,,"Using Hadoop 0.22.0 with Cassandra results in the stack trace below.
It appears that version 0.21+ changed org.apache.hadoop.mapreduce.JobContext
from a class to an interface.


Exception in thread ""main"" java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.JobContext, but class was expected
	at org.apache.cassandra.hadoop.ColumnFamilyInputFormat.getSplits(ColumnFamilyInputFormat.java:103)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:445)
	at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:462)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:357)
	at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1045)
	at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1042)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:415)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1153)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1042)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1062)
	at MyHadoopApp.run(MyHadoopApp.java:163)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
	at MyHadoopApp.main(MyHadoopApp.java:82)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
	at java.lang.reflect.Method.invoke(Method.java:601)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:192)
",,bcoverston,bhoyt,brandon.williams,brian.jeltema@digitalenvoy.net,bwtakacy,claudio.romo.otto,cscetbon,dbrosius,dmmata@gmail.com,dvryaboy,feestend,genx7up,hkropp,hodgesz,jeltema,jeromatron,kkrugler,mck,moliware,nyo,psanford,wadey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Mar/13 05:53;dbrosius;5201_a.txt;https://issues.apache.org/jira/secure/attachment/12572704/5201_a.txt","05/Mar/14 18:55;bcoverston;hadoop-compat-2.1-merge.patch;https://issues.apache.org/jira/secure/attachment/12632877/hadoop-compat-2.1-merge.patch","03/Dec/13 22:58;bcoverston;hadoopCompat.patch;https://issues.apache.org/jira/secure/attachment/12616870/hadoopCompat.patch","11/Feb/14 21:26;bcoverston;hadoopcompat-trunk.patch;https://issues.apache.org/jira/secure/attachment/12628339/hadoopcompat-trunk.patch","28/Feb/14 16:57;bcoverston;progressable-fix.patch;https://issues.apache.org/jira/secure/attachment/12631782/progressable-fix.patch","04/Mar/14 22:33;bcoverston;progressable-wrapper.patch;https://issues.apache.org/jira/secure/attachment/12632687/progressable-wrapper.patch",,,,,,,,,,,,,,,6.0,bcoverston,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310438,,,Wed Mar 05 19:00:44 UTC 2014,,,,,,,,,,"0|i1hkq7:",310783,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"30/Jan/13 17:29;jeromatron;The current stable hadoop version is still the 1.0.x line according to http://hadoop.apache.org/releases.html#Download As long as the 1.0.x support isn't adversely affected, I would think a patch to provide support for all hadoop versions would be welcome.;;;","31/Jan/13 00:47;brandon.williams;I think the last time I looked into this I couldn't find a way to support both versions, so we're either stuck with 0.21+ being broken, or breaking it for everyone that's on a lower version already.;;;","02/Feb/13 19:42;genx7up;Do you have a patch?;;;","05/Mar/13 02:11;dbrosius;what if the hadoop directory was split out into two separate sub-projects which produced two separate jars, thus a jar for old hadoop support and one for new hadoop. then folks could choose which jar to add to the cassandra classpath?;;;","05/Mar/13 03:43;brandon.williams;That's not a bad idea, but I'm not sure how it'd work entirely, care to formulate a patch? :);;;","05/Mar/13 06:40;dbrosius;Actually, i only see 0.20.*, 1.0.* (both of which have JobContext as a class) on the central maven repository, and on apache's download page... 

Does the 0.21+ jars still exist, (supported)?

;;;","07/Mar/13 20:23;genx7up;We are using the newer 0.23.x & are facing this integration issue. Any workaround?;;;","08/Mar/13 05:04;dbrosius;ah ok, here it is:

groupid: org.apache.hadoop
artifactid: hadoop-mapreduce-client-core
;;;","08/Mar/13 05:53;dbrosius;initial patch against 1.2
  - pulls hadoop code into src/hadoop/hadoop-legacy
  - replicates that into src/hadoop/hadoop-new with changes for api changes.

pulls new hadoop dependencies and builds separate jars for each.

which jar you run against is still manual... command line switch?

posted version a so i didn't lose stuff.

This uses 0.23.6 versions of jars, which i assume is compatible with the 2.0a versions of the api.;;;","08/Mar/13 10:35;jeromatron;I wonder if it would be preferable to have all of the code in tree in the same jar still, but have different package paths, like org.apache.cassandra.hadoop (for backwards compatibility) and org.apache.cassandra.hadoop2.  Would that work?;;;","08/Mar/13 12:06;brian.jeltema@digitalenvoy.net;No workaround that I'm aware of. We don't actually have this problem; I just became aware of it by
change while looking into a problem report, and filed an issue.

Brian



;;;","09/Mar/13 01:12;dbrosius;{quote} wonder if it would be preferable to have all of the code in tree in the same jar still, but have different package paths, like org.apache.cassandra.hadoop (for backwards compatibility) and org.apache.cassandra.hadoop2. Would that work?{quote}

probably could be done. here's some 'issues' with that approach.

1) external code won't work against both versions with the same code base. Obviously the modifications are insignificant, (packages) but still.
2) Makes the build.xml file marginally more complicated as you have to use <exclude> elements in the build
3) Building in IDEs is complicated as you need to add resource excludes, since these packages need to be built as a separate project (at least one of them does).
4) If you have two IDE projects targetting the same classes dir, cleaning one project cleans the other.

i could do it either way... whatever people think.
;;;","19/May/13 07:49;mck;What about simply putting the hadoop2 package into a github project?
it would become available for others to use, and c* can switch to it when they feel ready to drop support for hadoop-0.20

otherwise i'm in favour of separate jar files (apache-cassandra-hadoop-legacy-XXX.jar and apache-cassandra-hadoop-XXX.jar). c* already bundles too much into the one jar file IMHO.;;;","19/May/13 08:37;mck;{quote}What about simply putting the hadoop2 package into a github project?{quote}

Done @ https://github.com/michaelsembwever/cassandra-hadoop
 (i refactor the new package to hadoop1 instead of hadoop2, to better match the hadoop version we are actually supporting).;;;","11/Oct/13 12:27;mck;I've updated the github project so to be a [patch|https://github.com/michaelsembwever/cassandra-hadoop/commit/6d7555ea205354a606907e40c16db35072004594] off the InputFormat and OutputFormat classes as found in cassandra-1.2.10
It works against hadoop-0.22.0;;;","13/Oct/13 16:55;jbellis;How common is hadoop2 usage now?  Can we drop hadoop1 for 2.1?  /cc [~bcoverston];;;","25/Oct/13 00:55;kkrugler;Re Hadoop 1.x vs. Hadoop 2.x - most companies I see are using a 1.x version of Hadoop. Usage of Hadoop 2.x is ramping up fast, but I'm guessing it will be at least another year (probably more) before you'd want to start thinking about phasing out support for Hadoop 1.x;;;","25/Oct/13 07:07;mck;Hadoop-2 only just came out of alpha/beta with hadoop-2.2.0;;;","25/Oct/13 08:34;nyo;Why dropping support for old hadoop versions while you can just use a different package name (e.g  org.apache.cassandra.hadoop2.<SomeClass>)? Imo, the code in here:
https://github.com/michaelsembwever/cassandra-hadoop/ should just be merged with mainstream implementation, so both 1.x and 2.x users can  be happy.

Or you can indeed make it a lot more complex by merging the 1.x and 2.x compatible implementations and deciding which one to use based on metadata obtained by polling the cluster, but that is a bit too sophisticated. For most people the first option, where they can choose which package to use, would be sufficient.;;;","29/Oct/13 16:18;claudio.romo.otto;[~mck] How could be used your patch to make work Cassandra + Hadoop 2.2 + Pig? I have your library compiled but cannot figure how to apply to Cassandra or Pig to make it work, because I still hit the exception from Cassandra:
java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.JobContext, but class was expected
	at org.apache.cassandra.hadoop.AbstractColumnFamilyInputFormat.getSplits(AbstractColumnFamilyInputFormat.java:115);;;","29/Oct/13 16:32;nyo;[~claudio.romo.otto] 
It is a bit complex but basically you need to use the org.apache.cassandra.hadoop2.ColumnFamilyInputFormat stuff from https://github.com/michaelsembwever/cassandra-hadoop/
As far as hadoop goes, I ran succesfully with a cdh 4.4.1 cloudera hadoop lib, and with a cassandra 1.2.10. ;;;","29/Oct/13 19:36;claudio.romo.otto;So the correct way is to update cassandra 1.2.10 using org.apache.cassandra.hadoop2.ColumnFamilyInputFormat, right?;;;","03/Dec/13 22:58;bcoverston;Poking around at other projects this generally gets solved in one of two ways: Ship two versions of their Hadoop integrations (one compiled for the old, and one compiled for the new), or use a little reflection to make things work across the board.

I'm attaching a patch that uses the hadoopCompat subproject of elephantbird. This will allow us to compile a single binary and run with the new and old context objects.

I've tested this patch with HDP 2.0, and Apache Hadoop 1.0.4 and it works fine with both (including Hive in DSE). With Pig I needed to compile our (optional) pig dependency with:

bq. ant clean jar-withouthadoop -Dhadoopversion=23

Only really needed if you're using one of the current versions of thrift with the new JobContext.
;;;","03/Dec/13 23:36;bcoverston;These changes also depend on CASSANDRA-6309 for anything to work.;;;","04/Dec/13 22:33;jbellis;WDYT [~dbrosius]?;;;","17/Dec/13 03:31;jbellis;[~mck]?  [~jeromatron]?;;;","04/Jan/14 23:29;jeromatron;Seems reasonable if they're keeping their code up to date with all of the releases.  The code appears lightweight to make to use it as well.  I've also tried to reach out to [~dvryaboy] via twitter to see if he has any feedback about the approach of having elephant bird as a dependency, e.g. any hidden costs or pitfalls being more familiar with the project.;;;","05/Jan/14 22:10;dvryaboy;The EB HadoopCompat is what we use in production at Twitter, and plan to keep up to date in the foreseeable future. Glad you are finding it useful.

Maybe you can send that Reporter implementation as a pull request for hadoop-compat? Seems generally applicable.

Thanks to this ping, I looked around and noticed that our Parquet project uses a slightly different version of the same code -- we'll take a look and merge things. Shouldn't change anything significantly for this patch.

Also note that Tom White has a handy findbugs plugin to look for Hadoop incompatibility problems: https://github.com/tomwhite/hadoop-incompatibility-findbugs-detector 

Here's how you'd use it: https://github.com/Parquet/parquet-mr/pull/77/files;;;","06/Jan/14 13:30;jeromatron;Thanks [~dvryaboy]!

Just for completeness the twitter thread is https://twitter.com/jeromatron/status/419607697588510721

[~bcoverston] [~jbellis] what do you think?

As for me, it sounds like the EB (or if it makes more sense Parquet) dependency makes sense.  The hadoop incompatibility findbugs detector also sounds great to include to catch anything before it is committed.  So I'm +1 on this approach.;;;","06/Jan/14 18:02;bcoverston;I'll submit the reporter impl upstream. Thanks [~dvryaboy]!
;;;","10/Feb/14 19:19;jbellis;Are we good here [~brandon.williams]?;;;","11/Feb/14 15:39;brandon.williams;Committed to 2.0.  [~bcoverston] can you rebase a patch for trunk?;;;","11/Feb/14 21:26;bcoverston;Attached trunk-rebased patch.;;;","11/Feb/14 22:10;brandon.williams;Committed.;;;","27/Feb/14 13:37;moliware;Hi [~brandon.williams]

I'm using this patch and I keep having compatibility problems while trying to write something with pig using CqlStorage.
I'm using Hadoop 2.2 and Cassandra 2.0 branch.

This is the full stack trace:

{noformat}
java.lang.IncompatibleClassChangeError: Found interface org.apache.hadoop.mapreduce.TaskAttemptContext, but class was expected
        at org.apache.cassandra.hadoop.Progressable.progress(Progressable.java:45)
        at org.apache.cassandra.hadoop.cql3.CqlRecordWriter.write(CqlRecordWriter.java:183)        
        at org.apache.cassandra.hadoop.cql3.CqlRecordWriter.write(CqlRecordWriter.java:63)        
        at org.apache.cassandra.hadoop.pig.CqlStorage.sendCqlQuery(CqlStorage.java:440)        
        at org.apache.cassandra.hadoop.pig.CqlStorage.cqlQueryFromTuple(CqlStorage.java:414)        
        at org.apache.cassandra.hadoop.pig.CqlStorage.putNext(CqlStorage.java:362)        
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:139)        
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:98)        
        at org.apache.hadoop.mapred.ReduceTask$NewTrackingRecordWriter.write(ReduceTask.java:576)        
        at org.apache.hadoop.mapreduce.task.TaskInputOutputContextImpl.write(TaskInputOutputContextImpl.java:89)        
        at org.apache.hadoop.mapreduce.lib.reduce.WrappedReducer$Context.write(WrappedReducer.java:105)        
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.runPipeline(PigGenericMapReduce.java:467)        
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.processOnePackageOutput(PigGenericMapReduce.java:432)        
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:412)        
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigGenericMapReduce$Reduce.reduce(PigGenericMapReduce.java:256)        
        at org.apache.hadoop.mapreduce.Reducer.run(Reducer.java:171)        
        at org.apache.hadoop.mapred.ReduceTask.runNewReducer(ReduceTask.java:645)        
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:405)        
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:162)        
        at java.security.AccessController.doPrivileged(Native Method)        
        at javax.security.auth.Subject.doAs(Subject.java:415)        
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1491)        
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:157)
{noformat}

;;;","28/Feb/14 16:57;bcoverston;[~moliware],  attaching a patch that will fix the issue with progressable when used on the jobcontext object. I've tested this with the latest HDP release, and it works with pig. [~brandon.williams] take a look.;;;","28/Feb/14 17:51;moliware;Thanks [~bcoverston] I will test the patch probably on monday and will confirm you but it looks good. Thanks!;;;","03/Mar/14 08:24;moliware;[~bcoverston] Tested and It worked! Thanks!;;;","03/Mar/14 08:44;hkropp;Does this patch include deployment with maven classifiers, so that hadoop1 or hadoop2 are referenced dependencies?;;;","03/Mar/14 20:20;hkropp;I also was able to make this patch work with HDP 2.0. Nevertheless before this ticket is closed I would like to make 2 suggestions:
1. Use maven classifiers for deployment. If this is used in a hadoop2 project you don't want hadoop1 dependencies and vice versa. This approach is used by Parquet, Avro and EB. For detailed discussion please read [here|https://github.com/Parquet/parquet-mr/pull/32].
2. Change {{hadoop-core}} to {{hadoop-client}} dependency. {{hadoop-client}} is the preferred dependency. Read [here|http://www.cloudera.com/content/cloudera-content/cloudera-docs/CDH4/latest/CDH-Version-and-Packaging-Information/cdhvd_topic_8_1.html] and the link above.;;;","03/Mar/14 23:33;bcoverston;[~hkropp] I agree that we need to do the second change, but I don't see a need to add deployment classifiers. Because we're using Hadoop-Compat (from EB) a single binary will work for both.

There's a niggle right now with Progressable that I'm working on, but a single binary will work for new and legacy deployments.;;;","04/Mar/14 07:54;hkropp;I might be missing an important point, but Hadoop-Compat is nothing other than ContextUtil of hadoop2? It uses reflection to test what's on the classpath to decide how or better what to return. I made it work with just using ContextUtil before I found this patch here.

Therefor the binary works fine for both (btw I would not consider it legacy!), if you manage the dependencies on your own.

But if you would start a new project you would import Cassandra for example like this:
{code}
dependencies {
  ...
  compile 'org.apache.cassandra:cassandra:2.0.6'
  ...
}
{code}

What will happen now is, that this will load hadoop1 dependencies into my hadoop2 project for example, or not? To avoid this maven classifiers could be used, to call explicitly for hadoop2 dependencies:

{code}
dependencies {
  ....
  compile 'org.apache.cassandra:cassandra:2.0.6:hadoop2'
  ...
}
{code};;;","04/Mar/14 16:36;bcoverston;Take a look at the code, it works for Hadoop1 and Hadoop2 without recompliation, and without shipping two sets of dependencies. Basically it detects the current version of Hadoop that you're running and dynamically determines which TaskAttemptContext to use, the Class, or the Interface. There's no need to use deployment classifiers to solve this particular problem.;;;","04/Mar/14 21:31;bcoverston;Attaching a patch that brings in hadoop-compat, and adds the progressable wrapper to it. I'm going to submit this upstream to the elephant bird project, so we should be able to remove this code and add the dependency in the future.;;;","04/Mar/14 21:46;bcoverston;Updated progressable-wrapper.patch to conform to current namespaces.;;;","04/Mar/14 22:33;bcoverston;I removed the hadoop-compat dependency, as maven couldn't resolve the dependencies, reverted to the old version. This won't stop our input/output formats from being compatible with past and future releases.;;;","05/Mar/14 18:55;bcoverston;Patch for 2.1 branch;;;","05/Mar/14 19:00;brandon.williams;I had the same maven resolution problem with hadoop-compat.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Avoid serializing to byte[] on commitlog append,CASSANDRA-5199,12629836,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jbellis,jbellis,30/Jan/13 04:04,16/Apr/19 09:32,14/Jul/23 05:53,05/Feb/13 14:46,2.0 beta 1,,,,,,0,,,,,We used to avoid re-serializing RowMutations by caching the byte[] that we read off the wire.  We don't do that anymore since we fixed MessagingService to not create intermediate byte[].  So we should serialize the mutation directly to the commitlog.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-6714,,,,,,,,,,,"30/Jan/13 04:06;jbellis;5199-1.2.txt;https://issues.apache.org/jira/secure/attachment/12567102/5199-1.2.txt","30/Jan/13 04:09;jbellis;5199-2.0.txt;https://issues.apache.org/jira/secure/attachment/12567103/5199-2.0.txt",,,,,,,,,,,,,,,,,,,2.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310332,,,Tue Feb 05 14:46:28 UTC 2013,,,,,,,,,,"0|i1hk2n:",310677,,,,,,,,,yukim,,yukim,Low,,,,,,,,,,,,,,,,,,"30/Jan/13 04:06;jbellis;1.2 version just gets rid of the byte[] caching, since it's never actually re-used.;;;","30/Jan/13 04:09;jbellis;2.0 version adds ByteBufferOutputStream and ChecksummedOutputStream to get rid of byte[] serialization entirely.  Also fixes mutation-length checksumming to include the entire length, not just the first eight bits.;;;","04/Feb/13 19:06;yukim;+1 for both.
nit: you need the license header to new files in 2.0.;;;","05/Feb/13 14:46;jbellis;Committed.  (I'll let RAT add the licenses.);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Fix CQL3 loose type validation of constants,CASSANDRA-5198,12629829,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,appodictic,appodictic,30/Jan/13 03:23,16/Apr/19 09:32,14/Jul/23 05:53,04/Feb/13 09:53,1.2.2,,,,,,0,,,,,"This works as it should.

{noformat}
cqlsh:movies> select * from users where token (username) > token('') ;

 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
    bsmith |         null |  null |       bob |    smith |     null
 scapriolo |         null |  null |    stacey | capriolo |     null
 ecapriolo |         null |  null |    edward | capriolo |     null

cqlsh:movies> select * from users where token (username) > token('bsmith') ;

 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
 scapriolo |         null |  null |    stacey | capriolo |     null
 ecapriolo |         null |  null |    edward | capriolo |     null

cqlsh:movies> select * from users where token (username) > token('scapriolo') ;

 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
 ecapriolo |         null |  null |    edward | capriolo |     null

{noformat}

But look what happens when you supply numbers into the token function.


{noformat}
qlsh:movies> select * from users where token (username) > token(0) ;
 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
 ecapriolo |         null |  null |    edward | capriolo |     null
cqlsh:movies> select * from users where token (username) > token(1134314) ;

 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
    bsmith |         null |  null |       bob |    smith |     null
 scapriolo |         null |  null |    stacey | capriolo |     null
 ecapriolo |         null |  null |    edward | capriolo |     null

cqlsh:movies> select * from users where token (username) > token(113431431) ;
 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
 scapriolo |         null |  null |    stacey | capriolo |     null
 ecapriolo |         null |  null |    edward | capriolo |     null

cqlsh:movies> select * from users where token (username) > token(1134) ;
 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
 ecapriolo |         null |  null |    edward | capriolo |     null
cqlsh:movies> select * from users where token (username) > token(1134434) ;
 username  | created_date | email | firstname | lastname | password
-----------+--------------+-------+-----------+----------+----------
 scapriolo |         null |  null |    stacey | capriolo |     null
{noformat}

This does not make sense to me. The token function is apparently converting integers to strings leading to seemingly unpredictable results. 

However I find this syntax odd, I feel like I should be able to say 
'token(username) > 0 and token(username) < 10' because from a thrift side I can page tokens or I can page keys. In this case, I guess, I am only able to page keys because the token is not returned to the user.

Is token 0 = ''? How do I arrive at the minimal token for and int column. 

Should the token() function at least be smart enough to reject integers for string columns?",,aleksey,appodictic,slebresne,tjake,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"30/Jan/13 17:00;slebresne;0001-Respect-CQL3-constant-types.txt;https://issues.apache.org/jira/secure/attachment/12567160/0001-Respect-CQL3-constant-types.txt","30/Jan/13 17:00;slebresne;0002-Improve-printing-of-type-in-error-message.txt;https://issues.apache.org/jira/secure/attachment/12567161/0002-Improve-printing-of-type-in-error-message.txt","30/Jan/13 17:00;slebresne;0003-Respect-partitioner-type-for-Token-function.txt;https://issues.apache.org/jira/secure/attachment/12567162/0003-Respect-partitioner-type-for-Token-function.txt",,,,,,,,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310325,,,Mon Feb 04 09:52:58 UTC 2013,,,,,,,,,,"0|i1hk13:",310670,,,,,,,,,aleksey,,aleksey,Low,,,,,,,,,,,,,,,,,,"30/Jan/13 09:25;slebresne;So what happens is that {{token(0)}} is basically interpreted as the equivalent of {{token('0')}}.

Now this is not specific to the token method at all. With the table used above, you can do:
{noformat}
INSERT INTO users(username, firstname, lastname) VALUES (12, 42, 0)
{noformat}
and that will have the same effect than
{noformat}
INSERT INTO users(username, firstname, lastname) VALUES ('12', '42', '0')
{noformat}
In the same spirit, you can insert value {{'12'}} in an int column.

Now is that a good idea? I'm not sure indeed. This is not really intentional and is more of an oversight (more precisely it's an inheritance of CQL2 that has never been fixed).

I'm fine fixing it (thus fixing the token special case), and in fact in favor of fixing it, but of course that will break anyone that relies on this loose validation (which may be no-one). Though I guess ""not validating types"" is more of a bug than a feature.


bq. I feel like I should be able to say 'token(username) > 0 and token(username) < 10'

You can with the caveat that currently the token needs to be quoted, so {{token(username) > '0' and token(username) < '10'}}. The vague rational was that tokens are not always ints (i.e they are not in the case of OPP) so we only accept a string and pass that to the partitionner fromString method, oblivious to what the token type actually is. *But* this is definitively neither intuitive, nor coherent with the behavior described above. So I suggest that if we do the change suggested above of actually doing type validation, we also use the occasion for properly typing tokens.

bq. because the token is not returned to the user

On that part I'll not that it is true with thrift too. If you want to page tokens thrift side, you have to compute the token from the keys returned. That being said, I'm not opposed to allow the token function in select clause so you can do
{noformat}
SELECT username, token(username) FROM ...
{noformat}
to save the token computation client side.
;;;","30/Jan/13 17:00;slebresne;Attached 3 patches related to the proposed changes above:
# the first one adds proper type validation. In other word, it rejects a string value when the column is int, or reject an int value when the column is a blob (instead of interpreting it as an hex value which I'm pretty sure is counter-intuitive). This does however also reject a string value when the column is a blob, because I'm far from convince than interpreting the content of the string as an hex value is particularly intuitive. But to allow inserting blobs, it allow a new type of hex constants (that must start with '0x'). In other words, if b is a blob column:
{noformat}
UPDATE ... SET b = '00ff' ...
{noformat}
is not valid anymore, but
{noformat}
UPDATE ... SET b = 0x00ff ...
{noformat}
is. I note that the patch ain't tiny because it required a few refactoring here and there
to be done properly, but overall I think those refactor actually improve the
code.
# the second patch is mainly of cosmetic and make sure we use CQL3 type in CQL3 error message. I.e. 'map<text, int>' rather than 'org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type.Int32Type)'.
# the third patch make sure we take the partitioner token type into account. So if your partitioner is M3P you should provide a bigint value, if it's RP a varint one and if it's OPP a blob one.

Those patches don't add yet support for the token function in select clause that I talk above. I also want to add conversion function that allow to say convert a string or a uuid to a blob, but I want to refactor a bit the (currently ugly) handling of functions to do that so that will follow later (and it can be done in another ticket).
;;;","30/Jan/13 20:34;appodictic;I like #1.
I advocate proper type validation. We recently had a MySQL update that was wrapping booleans in 'T', 'F', 'true' and based on your database the results are different or non-intuitive. Personally, I do not like ""loose validation"" it encourages ambiguity. Hive went through something similar: http://grokbase.com/t/hive/dev/125sw56a78/non-string-partition-columns, there was much ambiguity and misconceptions around ""loose validation"" and it became tech-dept that was hard to dig out of.;;;","30/Jan/13 22:28;aleksey;+1 to the first three patches.;;;","31/Jan/13 11:24;slebresne;Alight, since we seem to be in agreement, committed (and I've updated the title to reflect the slightly broader scope).

I'll open a separate ticket for adding conversion functions.;;;","01/Feb/13 19:55;tjake;This is a breaking change for us or anyone who is using CQL with blobs.

Since we no longer accept '' hex but 0x didn't previously work till now, there is no way to upgrade 1.2.x to 1.2.2 due to this change.

I suggest you allow both formats for a single release and add a note in NEWS.txt, that way someone can deploy 1.2.2 then update their app to the new syntax;;;","01/Feb/13 19:58;aleksey;bq. I suggest you allow both formats for a single release and add a note in NEWS.txt, that way someone can deploy 1.2.2 then update their app to the new syntax

+1;;;","04/Feb/13 09:52;slebresne;Agreed. I've re-allowed strings-as-blobs in commit b251e7aec03273ac14eeae79bee13422068d508b while (hopefully) making it clear it is deprecated (in particular it logs a warning (only once to avoid flooding) if you use a string as blob).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Loading persisted ring state in a mixed cluster can throw AE,CASSANDRA-5197,12629825,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,30/Jan/13 02:23,16/Apr/19 09:32,14/Jul/23 05:53,13/Feb/13 23:01,1.2.2,,,,,,0,qa-resolved,,,,"{noformat}
 INFO 02:07:16,263 Loading persisted ring state
java.lang.AssertionError
        at org.apache.cassandra.locator.TokenMetadata.updateHostId(TokenMetadata.java:221)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:451)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:406)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:282)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:315)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:212)
{noformat}

We assume every host has a hostid, but this is not always true.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"31/Jan/13 19:31;brandon.williams;5197.txt;https://issues.apache.org/jira/secure/attachment/12567420/5197.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310321,,,Wed Feb 13 23:01:20 UTC 2013,,,,,,,,,,"0|i1hk07:",310666,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,enigmacurry,,,"31/Jan/13 19:31;brandon.williams;If you restart a 1.2 node in a mixed cluster, it assumes all hosts have a hostId, but the 1.1 nodes will not.;;;","13/Feb/13 22:43;aleksey;+1;;;","13/Feb/13 23:01;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
IllegalStateException thrown when running new installation with old data directories,CASSANDRA-5196,12629800,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,rstrickland,rstrickland,29/Jan/13 21:51,16/Apr/19 09:32,14/Jul/23 05:53,05/Feb/13 23:43,1.2.2,,,,,,0,,,,,"If you install 1.2.1 when there are existing data directories, the scrub operation fails, throwing this exception:

ERROR [main] 2013-01-29 15:05:06,564 FileUtils.java (line 373) Stopping the gossiper and the RPC server
ERROR [main] 2013-01-29 15:05:06,564 CassandraDaemon.java (line 387) Exception encountered during startup
java.lang.IllegalStateException: No configured daemon
	at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:314)
	at org.apache.cassandra.io.util.FileUtils.handleFSError(FileUtils.java:375)
	at org.apache.cassandra.db.Directories.<init>(Directories.java:113)
	at org.apache.cassandra.db.Directories.create(Directories.java:91)
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:403)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:174)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)

This condition should produce a more reasonable exception.",CentOS 5.5,aleksey,haifeng,rstrickland,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"05/Feb/13 20:52;aleksey;5196.txt;https://issues.apache.org/jira/secure/attachment/12568084/5196.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310296,,,Tue Feb 05 23:43:22 UTC 2013,,,,,,,,,,"0|i1hjun:",310641,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"31/Jan/13 23:22;aleksey;The exception is thrown by this method:

{noformat}
    public static void createDirectory(File directory)
    {
        if (!directory.exists())
        {
            if (!directory.mkdirs())
                throw new FSWriteError(new IOException(""Failed to mkdirs "" + directory), directory);
        }
    }
{noformat}

So it seems like the directory didn't exist, it tried to create one, but failed (I assume it had something to do with permissions).

Can't reproduce. Can you, [~rstrickland]?;;;","04/Feb/13 17:52;aleksey;Pretty sure this particular issue was caused by not having enough permissions to create the directories. Please reopen if that's not the case and you can reproduce.;;;","05/Feb/13 03:27;haifeng;Issue reproduced on my machine.
Environment:
Redhat Linux, Cassandra 1.2.1 unzipped to a NFS mounted folder (/mnt/storage/cassandra). 
Create data/commitlog folder manually in the NFS folder(mkdir -p /mnt/storage/cassandra/data, mkdir -p /mnt/storage/cassandra/commitlog).
config file updated.
Start Cassandra, ""bin/cassandra -f"", same error found.
{quote}
 INFO 22:15:06,283 Found table data in data directories. Consider using the CLI to define your schema.
ERROR 22:15:06,495 Stopping the gossiper and the RPC server
ERROR 22:15:06,505 Exception in thread Thread[OptionalTasks:1,5,main]
java.lang.IllegalStateException: No configured daemon
        at org.apache.cassandra.service.StorageService.stopRPCServer(StorageService.java:314)
        at org.apache.cassandra.io.util.FileUtils.handleFSError(FileUtils.java:375)
        at org.apache.cassandra.db.Directories.<init>(Directories.java:113)
        at org.apache.cassandra.db.Directories.create(Directories.java:91)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:379)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:364)
        at org.apache.cassandra.db.Table.initCf(Table.java:337)
        at org.apache.cassandra.db.Table.<init>(Table.java:280)
        at org.apache.cassandra.db.Table.open(Table.java:110)
        at org.apache.cassandra.db.Table.open(Table.java:88)
        at org.apache.cassandra.db.Table$1.apply(Table.java:82)
        at org.apache.cassandra.db.Table$1.apply(Table.java:79)
        at com.google.common.collect.Iterators$9.transform(Iterators.java:893)
        at com.google.common.collect.TransformedIterator.next(TransformedIterator.java:48)
        at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1664)
        at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:114)
        at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:41)
        at org.apache.cassandra.concurrent.DebuggableScheduledThreadPoolExecutor$UncomplainingRunnable.run(DebuggableScheduledThreadPoolExecutor.java:75)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$301(ScheduledThreadPoolExecutor.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
 INFO 22:15:06,560 No commitlog files found; skipping replay
{quote}

Remove above folders(data/commitlog) and restart Cannandra, issue resolved.;;;","05/Feb/13 03:34;aleksey;What OS user was used when you created the /mnt/storage/cassandra/data directory? Did you use sudo? Did you run bin/cassandra -f as the same user? ;;;","05/Feb/13 03:42;haifeng;RHEL 6.2, 64bit.
All operations were done by root user.
/mnt/storage/cassandra not removed. data and commitlog folders removed.
{quote}

drwxr-xr-x 4 root root      4096 Feb  4 22:18 cassandra
{quote};;;","05/Feb/13 04:20;aleksey;For some reason Java fails to create the directories. Or at least directory.mkdirs() call returns false. Unfortunately Java's file api won't give you any details, other than it thinks it failed - I don't see what we can do about those cases.

I'd like you to try one other thing, though.

remove /mnt/storage/cassandra/data

Start Cassandra (if I'm reading you comment right - it should start fine).

Stop Cassandra. Remove /mnt/storage/cassandra/data/* - all the sstable directories inside data, but NOT /mnt/storage/cassandra/data/ itself. Then start Cassandra again ant tell me if it works all right or fails as if you created it manually. Thanks. ;;;","05/Feb/13 06:24;haifeng;{quote}
Stop Cassandra. Remove /mnt/storage/cassandra/data/* - all the sstable directories inside data, but NOT /mnt/storage/cassandra/data/ itself. Then start Cassandra again ant tell me if it works all right or fails as if you created it manually. Thanks.
{quote}
Run Cassandra successfully, then stop it, remove ""/mnt/storage/cassandra/data/*"" and run Cassandra again, the Cassandra started without error.

Then I did following test, no error found.
Remove all folders/files under /mnt/storage/cassandra/, create data folder manually, start Cassandra. 

;;;","05/Feb/13 14:07;rstrickland;In my case the directories were owned by ""cassandra"" and the service was started as ""cassandra"" using init.  Even starting as root resulted in the same failure.  Only deleting the directories entirely resolved the issue.  This worked in my case because it was an old installation and I didn't need the data, but it took me a while to figure this out since the exception was so misleading.;;;","05/Feb/13 20:55;aleksey;While I don't think we can fix the directory creation itself in these cases, we can and should make failure handling user-friendlier. The attached patch changes the following things:
- doesn't attempt to stop RPC if it's not already started - gets rid of the non-obvious IllegalStateException
- separately logs directory creation failure
- stops native protocol as well if disk_failure_policy is 'stop'
- reduces log-spam when we fail to create multiple directories;;;","05/Feb/13 23:16;brandon.williams;+1 on the patch, but I don't understand what the directory creation problem is in the case of it existing as user X, but then even running as root we raise an error.;;;","05/Feb/13 23:20;aleksey;bq. but I don't understand what the directory creation problem is in the case of it existing as user X, but then even running as root we raise an error.

I don't understand either - and I can't reproduce it (I tried).;;;","05/Feb/13 23:33;brandon.williams;We can take a closer with the patch in place if it happens again.;;;","05/Feb/13 23:43;aleksey;Yep. Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Offline scrub does not migrate the directory structure on migration from 1.0.x to 1.1.x and causes the keyspace to disappear,CASSANDRA-5195,12629730,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,omid,omid,omid,29/Jan/13 17:08,16/Apr/19 09:32,14/Jul/23 05:53,22/Mar/13 14:26,1.1.11,,,,,,0,,,,,"Due to CASSANDRA-4411, upon migration from 1.0.x to 1.1.x containing LCS-compacted sstables, an offline scrub should be run before Cassandra 1.1.x is started. But Cassandra 1.1.x uses a new directory structure (CASSANDRA-2749) that offline scrubber doesn't detect or try to migrate.

How to reproduce:

1- Run cassandra 1.0.12.
2- Run stress tool, let Cassandra flush Keyspace1 or flush manually.
3- Stop cassandra 1.0.12
4- Run ./bin/sstablescrub Keyspace1 Standard1
  which returns ""Unknown keyspace/columnFamily Keyspace1.Standard1"" and notice the data directory isn't migrated.
5- Run cassandra 1.1.9. Keyspace1 doesn't get loaded and Cassandra doesn't try to migrate the directory structure. Also commitlog entries get skipped: ""Skipped XXXXX mutations from unknown (probably removed) CF with id 1000""

Without the unsuccessful step 4, Cassandra 1.1.9 loads and migrates the Keyspace correctly.

  ",,enigmacurry,marcuse,omid,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Mar/13 15:01;omid;0001-Flush-newly-migrated-system-CFs.patch;https://issues.apache.org/jira/secure/attachment/12574012/0001-Flush-newly-migrated-system-CFs.patch","29/Jan/13 17:26;omid;5195.patch;https://issues.apache.org/jira/secure/attachment/12567003/5195.patch",,,,,,,,,,,,,,,,,,,2.0,omid,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310226,,,Fri Mar 22 14:26:48 UTC 2013,,,,,,,,,,"0|i1hjf3:",310571,,,,,,,,,marcuse,,marcuse,Normal,,,,,,,,,,,,,,,,,,"29/Jan/13 17:20;jbellis;Ryan, can you reproduce?;;;","29/Jan/13 17:26;omid;I tried to fix the issue in offline-scrub but the patch doesn't fully fix the issue. Cassandra 1.1.9 with this patch only loads the migrated keyspaces after 2nd restart after offine-scrub has applies the migration.;;;","29/Jan/13 20:27;jbellis;2nd try of assign-to-ryan;;;","30/Jan/13 03:50;enigmacurry;I have reproduced this issue. Omid's patch works as he described: it does not fully fix the issue, but does allow the keyspace to be loaded on the 2nd restart of cassandra. Below is my verification workflow:

* Checkout/build 1.0.12
** cd $CASSANDRA_DIR
** git checkout -b 5195-1.0.12
** git reset --hard cassandra-1.0.12
** git clean -f -d
** ant build
* Run 1.0.12 test:
** sudo rm -rf /var/lib/cassandra
** sudo cassandra
** cd tool/stress
** ant build
** ./bin/stress
** sudo pkill -f CassandraDaemon
* Verify the keyspace/cf was created by stress:
** 10:20 PM:~/git/datastax/cassandra/tools/stress[5195-1.0.12*]$ cqlsh
   Connected to Test Cluster at localhost:9160.
   [cqlsh 2.0.0 | Cassandra unknown | CQL spec unknown | Thrift protocol 19.20.0]
   Use HELP for help.
   cqlsh> use Keyspace1 ;
   cqlsh:Keyspace1> select count(*) from Standard1;
   count
   -------
   10000
* Checkout/build 1.1.9
** cd $CASSANDRA_DIR
** git checkout -b 5195-1.1.9
** git reset --hard cassandra-1.1.9
** git clean -f -d
** ant build
* Run 1.1.9 test:
** sudo ./bin/sstablescrub Keyspace1 Standard1
*** stdout: Unknown keyspace/columnFamily Keyspace1.Standard1
** sudo cassandra
*** log:  INFO [main] 2013-01-29 22:28:44,800 CommitLogReplayer.java (line 103) Skipped 585748 mutations from unknown (probably removed) CF with id 1000
* Verify that Keyspace1 does or does not exist:
** 10:30 PM:~/git/datastax/cassandra[5195-1.1.9*]$ cqlsh
   Connected to Test Cluster at localhost:9160.
   [cqlsh 2.2.0 | Cassandra 1.1.9-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.33.0]
   Use HELP for help.
   cqlsh> use Keyspace1 ;
   Bad Request: Keyspace 'Keyspace1' does not exist
* Run 1.1.9 test again without the sstablescrub (restoring /var/lib/cassandra from before):
** sudo pkill -f CassandraDaemon    
** sudo cassandra
*** log:  INFO 22:33:01,240 Replaying /var/lib/cassandra/commitlog/CommitLog-1359515707503.log, /var/lib/cassandra/commitlog/CommitLog-1359515946450.log
    INFO 22:33:01,244 Replaying /var/lib/cassandra/commitlog/CommitLog-1359515707503.log
    INFO 22:33:02,318 CFS(Keyspace='Keyspace1', ColumnFamily='Standard1') liveRatio is 4.55084790673026 (just-counted was 4.55084790673026).  calculation took 866ms for 4590 columns
    INFO 22:33:02,930 CFS(Keyspace='Keyspace1', ColumnFamily='Standard1') liveRatio is 5.226616220760892 (just-counted was 5.226616220760892).  calculation took 357ms for 11635 columns
    INFO 22:33:04,186 CFS(Keyspace='Keyspace1', ColumnFamily='Standard1') liveRatio is 5.094053078093754 (just-counted was 4.9614899354266155).  calculation took 859ms for 26720 columns
* Verify that Keyspace1 does or does not exist:
** 10:36 PM:~/git/datastax/cassandra[5195-1.1.9*]$ cqlsh
   Connected to Test Cluster at localhost:9160.
   [cqlsh 2.2.0 | Cassandra 1.1.9-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.33.0]
   Use HELP for help.
   cqlsh> use Keyspace1;
   cqlsh:Keyspace1> select count(*) from Standard1;
   count
   -------
   10000
* Apply patch and retest:
** cd $CASSANDRA_DIR
** git apply ~/Downloads/5195.patch
** ant clean build 
** sudo rm -rf /var/lib/cassandra
** (restore /var/lib/cassandra from 1.0.12)
** sudo pkill -f CassandraDaemon
** sudo ./bin/sstablescrub Keyspace1 Standard1
*** stdout:
    Pre-scrub sstables snapshotted into snapshot pre-scrub-1359517364042
    Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-17-Data.db')
    Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-17-Data.db') complete: 63608 rows in new sstable and 0 empty (tombstoned) rows dropped
    Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-10-Data.db')
    Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-10-Data.db') complete: 258153 rows in new sstable and 0 empty (tombstoned) rows dropped
    Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-18-Data.db')
    Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-18-Data.db') complete: 65207 rows in new sstable and 0 empty (tombstoned) rows dropped
    Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-15-Data.db')
    Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-15-Data.db') complete: 254487 rows in new sstable and 0 empty (tombstoned) rows dropped
    Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-5-Data.db')
    Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-5-Data.db') complete: 243561 rows in new sstable and 0 empty (tombstoned) rows dropped
    Scrubbing SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-16-Data.db')
    Scrub of SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-hd-16-Data.db') complete: 64230 rows in new sstable and 0 empty (tombstoned) rows dropped
* Verify that Keyspace1 does or does not exist:
** sudo cassandra
** 10:44 PM:~/git/datastax/cassandra[5195-1.1.9*]$ cqlsh
   Connected to Test Cluster at localhost:9160.
   [cqlsh 2.2.0 | Cassandra 1.1.9-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.33.0]
   Use HELP for help.
   cqlsh> use Keyspace1;
   Bad Request: Keyspace 'Keyspace1' does not exist
* Restart cassandra as suggested:
** sudo pkill -f CassandraDaemon
** sudo cassandra
* Verify that Keyspace1 does or does not exist:
** 10:47 PM:~/git/datastax/cassandra[5195-1.1.9*]$ cqlsh
   Connected to Test Cluster at localhost:9160.
   [cqlsh 2.2.0 | Cassandra 1.1.9-SNAPSHOT | CQL spec 2.0.0 | Thrift protocol 19.33.0]
   Use HELP for help.
   cqlsh> use Keyspace1;
   cqlsh:Keyspace1> select count(*) from Standard1;
   count
   -------
   10000
   ;;;","30/Jan/13 04:07;enigmacurry;One other interesting aspect: Removing the patch, resetting /var/lib/cassandra to the 1.0.12 state, re-running sstablescrub, restarting cassandra (1,1.9) TWICE allows the keyspace to be read, but the table is empty! :


* 11:03 PM:~/git/datastax/cassandra[5195-1.1.9*]$ cqlsh
  Connected to Test Cluster at localhost:9160.
  [cqlsh 2.2.0 | Cassandra 1.1.9-SNAPSHOT | CQL spec 2.0.0 | Thrift   protocol 19.33.0]
  Use HELP for help.
  cqlsh> use Keyspace1;
  cqlsh:Keyspace1> select count(*) from Standard1;
   count
   -------
        0
;;;","16/Mar/13 15:01;omid;New patch to fix the condition that CFs show up after second restart.;;;","16/Mar/13 15:03;omid;I think upon loading the schema via offline scrubber, DefsTable.loadFromStorage migrates the old system tables to the new format. Therefore it drops the old ones but doesn't flush the commitlogs of new ones. Offline scrubber exits and on the next start, schema_keyspaces, schema_columnfamilies and schema_columns CFs have no persisted sstables, and the commitlog only gets replayed after Cassandra tries to load CF schemas, therefore finds none. This explains why after second restart, column families appear again (something force-flushes system CFs?).

A new patch (0001-Flush-newly-migrated-system-CFs.patch) is attached to fix the problem (although I'm not sure if there is a more proper fix for this or if there is a better place to put the foced flush.);;;","21/Mar/13 15:08;jbellis;[~krummas], can you review?;;;","22/Mar/13 08:43;marcuse;looks good to me and verified it works as expected

bug very similar to CASSANDRA-5061;;;","22/Mar/13 14:26;jbellis;committed, thanks all!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Memtable flushwriter can pick a blacklisted directory,CASSANDRA-5193,12629594,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,28/Jan/13 22:17,16/Apr/19 09:32,14/Jul/23 05:53,03/Feb/13 14:34,1.2.2,,,,,,0,,,,,"The top-level data directory will be picked by DiskAwareRunnable (directory = Directories.getLocationCapableOfSize(writeSize)), and the top-level data directory itself might not be blacklisted (most likely won't be).

For the same reason we can't just add a blacklist-check in the middle of Directories#getLocationCapableOfSize - most often it's the sstable directory that gets blacklisted.

The issue seems to be caused by/related to CASSANDRA-4292, which was committed just two days prior 2116-2118 and undid some blacklist-aware directory-picking logic.

Anyway, DiskAwareRunnable should be altered to respect directory blacklist.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Feb/13 02:37;aleksey;5193.txt;https://issues.apache.org/jira/secure/attachment/12567753/5193.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,310090,,,Sun Feb 03 14:34:04 UTC 2013,,,,,,,,,,"0|i1hikv:",310435,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"03/Feb/13 01:47;aleksey;The patch makes DiskAwareRunnable blacklist-aware and makes best_effort policy work properly again.;;;","03/Feb/13 10:41;jbellis;+1;;;","03/Feb/13 14:34;aleksey;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
compact storage metadata is broken,CASSANDRA-5189,12629420,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,27/Jan/13 04:35,16/Apr/19 09:32,14/Jul/23 05:53,29/Jan/13 18:10,1.2.2,,,Legacy/CQL,,,0,,,,,"{noformat}
cqlsh:foo> CREATE TABLE bar (
       ...     id int primary key,
       ...     i int
       ... ) WItH COMPACT STORAGE;

cqlsh:foo> INSERT INTO bar (id, i) VALUES (1, 2);
Bad Request: Missing PRIMARY KEY part column1
Perhaps you meant to use CQL 2? Try using the -2 option when starting cqlsh.

cqlsh:foo> INSERT INTO bar (id, column1) VALUES (1, 2);
Bad Request: Missing mandatory column i
Perhaps you meant to use CQL 2? Try using the -2 option when starting cqlsh.
{noformat}",,christianmovi,jasobrown,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"28/Jan/13 10:28;slebresne;5189.txt;https://issues.apache.org/jira/secure/attachment/12566741/5189.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,309916,,,Tue Jan 29 18:10:39 UTC 2013,,,,,,,,,,"0|i1hhhz:",310260,,,,,,,,,jasobrown,,jasobrown,Normal,,,,,,,,,,,,,,,,,,"29/Jan/13 14:46;jasobrown;Tested locally and it worked properly. LGTM. +1;;;","29/Jan/13 16:10;jbellis;I'm not actually sure how this fixes the problem, although it apparently does -- looks like only validation changes were made to CREATE.;;;","29/Jan/13 17:45;slebresne;It's not only validation change in fact, the change to the first {{if}} is what fixes it. ;;;","29/Jan/13 17:50;jbellis;+1
;;;","29/Jan/13 18:10;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
o.a.c.hadoop.ConfigHelper should support setting Thrift frame and max message sizes.,CASSANDRA-5188,12629380,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,xedin,xedin,xedin,26/Jan/13 05:49,16/Apr/19 09:32,14/Jul/23 05:53,28/Jan/13 18:47,1.1.10,1.2.2,,,,,0,,,,,Without such support people will be running into problems like https://github.com/thinkaurelius/faunus/issues/99 without any work around when custom frame and/or max message sizes are used.,,xedin,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"26/Jan/13 05:52;xedin;CASSANDRA-5188.patch;https://issues.apache.org/jira/secure/attachment/12566615/CASSANDRA-5188.patch",,,,,,,,,,,,,,,,,,,,1.0,xedin,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,309812,,,Mon Jan 28 18:47:52 UTC 2013,,,,,,,,,,"0|i1haw7:",309190,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"28/Jan/13 18:26;brandon.williams;+1;;;","28/Jan/13 18:47;xedin;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
be consistent in visible messages about DOWN versus dead state,CASSANDRA-5187,12629321,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,,rcoli,rcoli,25/Jan/13 21:14,16/Apr/19 09:32,14/Jul/23 05:53,23/Mar/13 23:08,1.2.4,,,,,,0,,,,,"Cassandra has two states that seem to be used interchangeably in some messages.

1) ""DOWN"" state, which simply means that the node has not responded in the last Gossip round.
2) ""dead"" which means a state listed in DEAD_STATES in Gossiper.java.

{noformat}
static final List<String> DEAD_STATES = Arrays.asList(VersionedValue.REMOVING_TOKEN, VersionedValue.REMOVED_TOKEN,VersionedValue.STATUS_LEFT, VersionedValue.HIBERNATE);
{noformat}

However, it seems to use the terms incorrectly in the following places :

a) the log message :
""
logger.info(""InetAddress {} is now dead."", addr);
""

this is in Gossiper::markDead (which ideally would also be renamed, because it does not seem to put anything in one of the DEAD_STATES..)

but in Gossiper::markAlive the paired log message is :
""
        logger.info(""InetAddress {} is now UP"", addr);
""

If being marked alive means you are UP, then being marked dead should mean you are DOWN.

b) NodeToolHelp.yaml has :
""
Disable gossip (effectively marking the node dead)
""

Attached is a patch against trunk which changes both ""dead""s to ""DOWN"".",,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"25/Jan/13 21:15;rcoli;DOWN.vs.dead.patch;https://issues.apache.org/jira/secure/attachment/12566558/DOWN.vs.dead.patch",,,,,,,,,,,,,,,,,,,,1.0,,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,309469,,,Sat Mar 23 23:08:14 UTC 2013,,,,,,,,,,"0|i1fawf:",297527,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"23/Mar/13 23:08;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
symlinks to data directories are broken in 1.2.0,CASSANDRA-5185,12629151,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,jjordan,jjordan,24/Jan/13 21:09,16/Apr/19 09:32,14/Jul/23 05:53,28/Jan/13 16:13,1.2.2,,,,,,0,,,,,"symlinks to data directories is broken in 1.2.0
{noformat}
cd ~
tar xzf apache-cassandra-1.2.0-bin.tar.gz
cd apache-cassandra-1.2.0/conf
vim cassandra.yaml
#set data/commitlog/savecache dirs to
#~/apache-cassandra-1.2.0/var/...
cd ../bin
# start once to make folders
./cassandra -f
#cntrl-c
cd ..
mkdir var/lib/cassandra2
mv var/lib/cassandra/data/system var/lib/cassandra2/system
cd var/lib/cassandra/data
ln -s ../../cassandra2/system .
cd ~/apache-cassandra-1.2.0/bin/
cassandra -f
#get lots of assertion errors see attached log file
{noformat}

{noformat}
 INFO 21:59:44,883 Enqueuing flush of Memtable-local@1578022692(52/52 serialized/live bytes, 2 ops)
 INFO 21:59:44,890 Writing Memtable-local@1578022692(52/52 serialized/live bytes, 2 ops)
ERROR 21:59:44,892 Exception in thread Thread[FlushWriter:1,5,main]
java.lang.AssertionError
	at org.apache.cassandra.io.sstable.Descriptor.<init>(Descriptor.java:190)
	at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:593)
	at org.apache.cassandra.db.ColumnFamilyStore.getTempSSTablePath(ColumnFamilyStore.java:588)
	at org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:428)
	at org.apache.cassandra.db.Memtable$FlushRunnable.runWith(Memtable.java:417)
	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

I traced it back some, I think it is coming from:
{noformat}
org.apache.cassandra.db.Memtable$FlushRunnable.writeSortedContents(Memtable.java:428)
which calls: cfs.directories.getLocationForDisk(dataDirectory)

    public File getLocationForDisk(File dataDirectory)
    {
        for (File dir : sstableDirectories)
        {
            if (FileUtils.getCanonicalPath(dir).startsWith(FileUtils.getCanonicalPath(dataDirectory)))
                return dir;
        }
        return null;
    }
{noformat}

My guess is that the FileUtils.getCanonicalPath calls aren't matching because of the symlinks.  So null is being returned there.",,jjordan,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"24/Jan/13 21:11;jjordan;5185-errors.log;https://issues.apache.org/jira/secure/attachment/12566374/5185-errors.log","24/Jan/13 22:51;yukim;5185.txt;https://issues.apache.org/jira/secure/attachment/12566394/5185.txt",,,,,,,,,,,,,,,,,,,2.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,308990,,,Mon Jan 28 16:13:16 UTC 2013,,,,,,,,,,"0|i1dxnj:",289549,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"24/Jan/13 22:51;yukim;File#getCannonicalPath returns path after resolving symlinks. Patch instead uses getAbsolutePath so symlinks under data dir works as expected.;;;","24/Jan/13 22:59;jbellis;If we cannonicalize both sizes of what we're comparing, shouldn't it work either way?;;;","24/Jan/13 23:08;yukim;For example, if you have data dir of ""/var/lib/cassandra/data"" and you symlinked system keyspace under that data dir to ""/var/lib/cassandra2/system"" as above, cannonicalized paths are 

{noformat}
/var/lib/cassandra/data -> /var/lib/cassandra/data
/var/lib/cassandra/data/system -> /var/lib/cassandra2/system
{noformat}

thus current comparison doesn't work.;;;","25/Jan/13 15:25;jbellis;+1;;;","28/Jan/13 16:13;yukim;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Deletable rows are sometimes not removed during compaction,CASSANDRA-5182,12628835,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,binhnv,binhnv,23/Jan/13 01:22,16/Apr/19 09:32,14/Jul/23 05:53,04/Mar/13 19:37,1.2.3,,,,,,0,,,,,"Our use case is write heavy and read seldom.  To optimize the space used, we've set the bloom_filter_fp_ratio=1.0  That along with the fact that each row is only written to one time and that there are more than 20 SSTables keeps the rows from ever being compacted. Here is the code:
https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/db/compaction/CompactionController.java#L162
We hit this conner case and because of this C* keeps consuming more and more space on disk while it should not.",,batalbot,binhnv,christianmovi,marcinszymaniuk,mauzhang,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/13 23:26;yukim;5182-1.1.txt;https://issues.apache.org/jira/secure/attachment/12566217/5182-1.1.txt","06/Feb/13 15:15;yukim;5182-1.2.txt;https://issues.apache.org/jira/secure/attachment/12568232/5182-1.2.txt","23/Jan/13 01:43;binhnv;test_ttl.tar.gz;https://issues.apache.org/jira/secure/attachment/12566071/test_ttl.tar.gz",,,,,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,308314,,,Mon Mar 04 19:37:12 UTC 2013,,,,,,,,,,"0|i1b46f:",272975,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"23/Jan/13 01:43;binhnv;Here is simple code to reproduce the issue. The simple code uses 10 threads to continuously write to C*. All the column has 300 in its TTL. While running the code, check the number of sstables of column family, you will see that it keeps growing and never stop
Here are steps to run:
- Start C* on your localhost
- Create a keyspace named test
- Use the following command to create column family cf
CREATE COLUMN FAMILY cf WITH comparator = UTF8Type AND key_validation_class = UTF8Type AND default_validation_class = UTF8Type AND gc_grace = 0 AND caching = none AND bloom_filter_fp_chance = 1.0 AND compaction_strategy='LeveledCompactionStrategy' AND compaction_strategy_options = { sstable_size_in_mb : 1 } AND compression_options = { chunk_length_kb : 64, sstable_compression : 'SnappyCompressor' };

- Extract the code switch to extracted directory
- Run mvn package
- Run java -jar target/test_ttl-1.0-SNAPSHOT-jar-with-dependencies.jar 10
;;;","23/Jan/13 20:42;batalbot;A mailing list thread with more details about the use case and symptoms can be found at http://www.mail-archive.com/user@cassandra.apache.org/msg27049.html;;;","23/Jan/13 23:26;yukim;Binh,

Thanks for investigation. You are right that purging row depends on bloom filter check, so if you have bloomfilter_fp_chance of 1.0, it is very likely that the row is not going to be purged.

In 1.2 and above, we use AlwaysPresentFilter which always returns true for row presence when fp_change is 1.0, so the row is never going to be purged when you set fp_change=1.0.

Simple fix is attached. Instead of only hitting bloom filter, we check through key cache and index file for actual row presence.;;;","23/Jan/13 23:32;yukim;Maybe it is better to check if fp_chance is high before going through index file, since it has performance penalty.;;;","23/Jan/13 23:52;binhnv;I agreed that we should find a better way since getPosition will check bloom filter, key cache and in the worst case (which is our case) it will scan whole index table. This will cause the performance issue.;;;","24/Jan/13 00:01;jbellis;Do you want a performance issue, or do you only want to remove tombstones during major compaction? :);;;","24/Jan/13 00:02;jbellis;Personally I am +1 on the fix; if you run a lot of deletes and can't cache your index files in ram, then don't disable bloom filters.;;;","24/Jan/13 00:25;batalbot;Using the test program attached, I've reproduce the problem using 1.1.9 and then upgraded that cluster (1 node on laptop) to 1.2.0.  The problem remains with the load and sstable count increasing.

However, when I run the test program on a fresh 1.2.0 cluster the problem does not come up.  My process to reproduce on upgrade is:

install fresh 1.1.9
run test to get 500 MB of data (20-30 mins)
drain and shutdown 1.1.9
start 1.2.0
run nodetool upgradesstables
run test and watch load grow to 2.5 GB while away at lunch


When running the test program on a fresh 1.2.0 installation, the load tops out at about 200 MB and 90 or so SSTables which is what is desired.
;;;","24/Jan/13 00:28;batalbot;About the check for a high fp_chance before checking indexes.  Did you mean to only check index files if fp_chance is high (say over 0.5 or something)?  That way the additional check is only incurred with bloom filters are effectively disabled and the common case using an effective (low fp) bloom filter is not impacted.
;;;","24/Jan/13 01:24;jbellis;getPosition does the right thing here: it checks the index file only on bloom filter positives, so a high bloom filter setting will benefit automatically.

The only improvement I think makes sense would be adding support for compaction strategy tombstone threshold.;;;","24/Jan/13 08:42;slebresne;bq. Maybe it is better to check if fp_chance is high before going through index file

Actually, I agree with Yuki on that and I'm kind of -1 on the patch in his current form. The current patch means that whatever your fp_chance is, each time the row is indeed present in a non compacted sstable (which does prevent gcing the row for this compaction but is not something that will necessarily be rare) might hit the disk (unless the key cache save you). So I'd be in favor of using getPosition only if fp_chance == 1, at least on 1.1 as we have no idea of the impact this can have on people that haven't disabled bloom filter and have no problem whatsoever with gcing tombstone.

As a side note, I've opened CASSANDRA-5183 that is related to this purge tombstone problem.;;;","24/Jan/13 14:28;jbellis;I don't think we should touch 1.1 at all.  Updated fixver, and updated affectsver to when we added configurable bf_fp_chance.;;;","06/Feb/13 15:15;yukim;Patch attached for 1.2 and above. It checks index file using getPosition if sstable has AlwaysPresentFilter as a bloom filter.;;;","12/Feb/13 23:37;jbellis;I'm still not comfortable with this.

If our goal is to throw out the maximum possible amount of obsolete data, we should perform getPosition across the board.

But if our goal is to be minimally impactful with compaction then we shouldn't do it at all, and rely instead on the timestamp check.  If that's not enough, then you shouldn't disable bloom filters on workloads that perform deletes.  I'm okay with that message.;;;","12/Feb/13 23:53;batalbot;Our use case doesn't require maximum effort to delete rows.  What we ran into was an unexpected interaction between two features: bloom filter tuned for low read rate, and deleting tombstoned rows.  With that configuration NO rows were being removed.  

As long as there is some reasonable effort to remove rows with bloom filter disabled OR it's clearly known that a reasonable FP setting is required to remove tombstones, I think we could have avoided a lot of headaches.

How does the new tombstone histogram feature in 1.2 affect this issue?  If that feature solves the problem already, maybe this fix is irrelevant.
;;;","13/Feb/13 09:48;slebresne;bq. If our goal is to throw out the maximum possible amount of obsolete data

I kind agree with Bryan, this doesn't have to be black and white. What we want is doing the best we can to remove obsolete rows without impacting compaction too much. Now if you do have active bloom filters, then I think just checking the bloom filters as we do now is the right trade-off: it maximize  with a very high probability the amount of removed data at the relatively cheap cost. Using getPosition in that case would be a bad idea, because the reward (a tiny fraction of additional data potentially removed) is not worth the cost (hitting disk each time a row we compact is also in a non-compacted sstable) imo, hence my opposition to the idea.

But if you deactivate bloom filters, you also fully destroy our bloom filter trade-off. So using getPosition does now provide a substantial benefit as it allows to go from 'no deletion' to 'maximize deletion'. The reward is, in that case, likely worth the cost, especially since people shouldn't desactivate bloom filters unless their index files fits in memory, in which case getPosition costs won't be that big.

So overall I do like the last patch attached by Yuki. Of course, the solution of just saying ""you shouldn't disable bloom filters on workloads that perform deletes"" works too, and I wouldn't oppose it, but it doesn't have my preference because I'm always a bit afraid of solving an issue by saying ""don't do this"", as it usually end up in people getting bitten first and hearing they shouldn't have done it second. ;;;","01/Mar/13 15:02;jbellis;bq. So overall I do like the last patch attached by Yuki. Of course, the solution of just saying ""you shouldn't disable bloom filters on workloads that perform deletes"" works too, and I wouldn't oppose it, but it doesn't have my preference because I'm always a bit afraid of solving an issue by saying ""don't do this"", as it usually end up in people getting bitten first and hearing they shouldn't have done it second. 

The problem is it's not as simple as ""people get bitten if we don't getPosition, and don't if we do"" -- they get bitten either way, and IMO the bite from getPosition is worse, since it will destroy compaction performance for any workload where index doesn't fit entirely in ram, which makes BF disabling almost useless.  But if we say ""only disable BF where you're not doing deletes,"" it has a legitimate if narrow use case.;;;","01/Mar/13 16:35;slebresne;bq.  if we say ""only disable BF where you're not doing deletes,"" it has a legitimate if narrow use case

I guess I agree on the principle that we should say ""only disable BF where you're not doing deletes"". That being said, if we do use getPosition, we extend the possible use cases, since it become ""only disable BF where you're not doing deletes or your index fit entirely in RAM"" (because getPosition will not destroy performance for the ""not doing delete case"", since we don't even call shouldPurge() unless we know there is tombstones).

bq. and IMO the bite from getPosition is worse, since it will destroy compaction performance

I'm not totally sure I agree on the worse. As said above, if people have not tombstone, it won't destroy compaction performance. So I guess the question is: for people that 1) do not follow recommendation (cause we should definitively say when disabling BF is ok or not) and that 2) do have deletes, is it better for them to be bitten by a) bad compaction performance or b) their tombstones not being purged ever.

I don't doubt that which of a) or b) is worse is a matter of perspective. That being said, my own personal preference goes to avoiding because:
* to me b) is a break of correctness which somewhat trumps performance consideration. It purely subjective though.
* accumulating tombstones forever is a pretty nasty time-bomb. Having compaction being slow because it hit disk more than it should on the other seems easier to me to detect (and thus fix by following the recommendation of not disabling BF when you shouldn't).

So, I still have a preference for using Yuki's last patch (and making it clear that you shall ""only disable BF where you're not doing deletes or your index fit entirely in RAM""). If only because that's a bit better than ""only disable BF where you're not doing deletes"". But if you still prefer keeping the status quo, I won't oppose, do feel free to close that issue (we still should write the recommendation on when to disable BF somewhere in any case).;;;","01/Mar/13 21:07;jbellis;bq. if we do use getPosition, we extend the possible use cases, since it become ""only disable BF where you're not doing deletes or your index fit entirely in RAM""

That makes sense.  Let's ship Yuki's patch.;;;","04/Mar/13 19:37;yukim;Committed to 1.2 and above. Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-all 1.2.0 pom missing netty dependency,CASSANDRA-5181,12628827,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,carllerche,carllerche,23/Jan/13 00:20,16/Apr/19 09:32,14/Jul/23 05:53,04/Mar/13 17:27,1.2.3,,,Packaging,,,0,,,,,"It seems that cassandra depends on netty now, however the pom excludes this dependency.",,carllerche,dbrosius,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jan/13 11:01;slebresne;5181.txt;https://issues.apache.org/jira/secure/attachment/12566113/5181.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,308305,,,Mon Mar 04 17:27:22 UTC 2013,,,,,,,,,,"0|i1b1mn:",272562,,,,,,,,,dbrosius,,dbrosius,Low,,,,,,,,,,,,,,,,,,"23/Jan/13 11:01;slebresne;I think the attached patch fixes that (emphasis on the ""think"").;;;","02/Mar/13 06:14;dbrosius;+1 lgtm;;;","04/Mar/13 17:27;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Hinted handoff sends over 1000 rows for one column change,CASSANDRA-5179,12628677,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,anttiko,anttiko,22/Jan/13 08:15,16/Apr/19 09:32,14/Jul/23 05:53,20/Mar/13 17:29,1.2.4,,,,,,2,,,,,"We have a small test environment with two datacenters (DC1 and DC2) running on Windows 7 laptops.
Both datacenters have one node. We use network topology strategy to replicate all data to both datacenters.

We started with empty db. 
1. Created a keyspace with strategy options [DC1:1, DC2:1]
2. Added one row to a column family with CLI to DC1. Change was replicated to DC2.
3. Disconnected network cable from DC2.
4. Gossiper noticed, that other DC is dead.
5. Added another row to DC1.
6. Reconnected cable on DC2.
7. DC1 started hinted handoff for DC2.
8. Hinted handoff is finished with message: ""Finished hinted handoff of 1969 rows to endpoint <DC2 ip>""

We repeated test with same results on Linux cluster with Cassandra 1.2.0. 

On Cassandra 1.1.5 Linux cluster, only one row was sent to endpoint. ""Finished hinted handoff of 1 rows to endpoint <DC2 ip>""","Windows 7
Java 1.6u38",aleksey,anttiko,christianmovi,cscetbon,jal06,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4761,,,,,,,,,,,,,,,"25/Jan/13 07:59;anttiko;cassandra_receiver.log;https://issues.apache.org/jira/secure/attachment/12566481/cassandra_receiver.log","25/Jan/13 07:59;anttiko;cassandra_sender.log;https://issues.apache.org/jira/secure/attachment/12566480/cassandra_sender.log",,,,,,,,,,,,,,,,,,,2.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,307202,,,Wed Mar 20 17:29:47 UTC 2013,,,,,,,,,,"0|i1ahvr:",269363,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"23/Jan/13 17:29;jbellis;Can you give us a debug log?;;;","25/Jan/13 07:59;anttiko;Here are the logs.

Number of sent rows varied quite much on our tests. When we ran HintedHandoffManager on debugger row by row, only one row was sent. 
Sometimes there were only couple of rows sent, but sometimes from 100 to 2000.;;;jira-users","04/Feb/13 16:02;cscetbon;It's written that it's fixed in 1.2.2
Do you know if a bugfix corrected it ? ;;;","04/Feb/13 16:23;jbellis;Fix-for is target fix version.  The issue is not fixed until resolved.;;;","04/Feb/13 16:25;cscetbon;Ok thanks for this information …;;;","12/Mar/13 04:14;aleksey;4881221363f984ab6610756cab38e1a016b79e15 (CASSANDRA-4761) broke it. Current pagination logic doesn't deal well with the fact that deleteHint is now being called from a response handler callback and can go through the same hint again and again and again until it's finally replaced with a tombstone. This becomes really visible in multi-dc setups, where it can take a while to complete the write, but I was able to trigger it with ccm as well (the same hint got sent up to 3 times).

Should we reopen CASSANDRA-4761 or deal with it here?;;;","12/Mar/13 04:33;jbellis;Let's fix it here.;;;","12/Mar/13 04:40;aleksey;k. This is the problematic branch btw: https://github.com/apache/cassandra/blob/cassandra-1.2/src/java/org/apache/cassandra/db/HintedHandOffManager.java#L336;;;","12/Mar/13 05:41;aleksey;https://github.com/iamaleksey/cassandra/compare/5179;;;","18/Mar/13 15:20;mkjellman;[~jbellis] patch looks good to me;;;","18/Mar/13 15:44;jbellis;If we time out we should probably cease further delivery attempts, to avoid hammering a node that is behind.;;;","18/Mar/13 17:47;aleksey;bq. If we time out we should probably cease further delivery attempts, to avoid hammering a node that is behind.

Probably. But that's not what causing this particular issue - the patch only fixes that faulty pagination logic.

I don't see an easy way to stop it on a timeout as we did in 1.1 now (yet), but that's a problem for another ticket anyway.;;;","18/Mar/13 18:17;jbellis;We can't put that code in the empty catch block here?;;;","18/Mar/13 18:20;aleksey;It'll be of no use - all the requests have been sent at that point. It's waiting for responses before forcing compaction.;;;","18/Mar/13 20:50;jbellis;What if we made it wait per page, instead of all at once?

Seems like that would be a good way to put a bound on how many callbacks we need to keep around too.;;;","18/Mar/13 23:46;aleksey;bq. What if we made it wait per page, instead of all at once?

This goes against CASSANDRA-4761 somewhat, but I think it's a good compromise between sending hints one at a time and pouring everything out. Updated https://github.com/iamaleksey/cassandra/compare/5179;;;","19/Mar/13 01:08;jbellis;You could use break-to-label instead of a bool in the get() loop, but shouldn't it just be a return instead of break?;;;","19/Mar/13 01:20;aleksey;bq. You could use break-to-label instead of a bool in the get() loop, but shouldn't it just be a return instead of break?

Yes I could. But some requests after the timed-out one could actually have succeeded - with the ratelimiter delay and all, and since we sent them, we might as well wait for the replies before triggering compaction (hints are deleted in that callback). And with return instead of break 1) compaction wouldn't be triggered, event if it's the last page of many and 2) ""Finished hinted handoff .."" message wouldn't be logged.;;;","19/Mar/13 01:24;jbellis;I think the reasoning for return instead of break in the FD block was that, if we haven't finished sending hints then we probably don't want to force a major compaction that will rewrite a bunch of undelivered hints.;;;","19/Mar/13 01:26;aleksey;Maybe. It was break in 1.1 though - https://github.com/apache/cassandra/blob/cassandra-1.1/src/java/org/apache/cassandra/db/HintedHandOffManager.java#L375;;;","19/Mar/13 01:33;jbellis;True...  and 1.0 had a ""if rowsReplayed > 0"" around it.  Not sure why we removed that, except maybe that it would almost always be true.

How about we make the 1.0 check smarter and compact if we delivered over half the hints?;;;","19/Mar/13 01:34;aleksey;bq. How about we make the 1.0 check smarter and compact if we delivered over half the hints?

wfm;;;","19/Mar/13 05:43;aleksey;Actually, it doesn't wfm. HHM is already complicated enough. I think returning completely is just fine there - as long as the log message mentions how many hints have been delivered.

There will be many more opportunities for compaction - when hints to another node are all sent, or after another attempt in <= 10 minutes.

Updated the branch (also did some *very* minor refactoring so that the paging logic would at least fit in a single screen, plus some even minorer changes that I just couldn't resist).;;;","20/Mar/13 15:50;jbellis;+1;;;","20/Mar/13 17:29;aleksey;Thanks, committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Unbounded (?) thread growth connecting to an removed node,CASSANDRA-5175,12628423,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,vijay2win@yahoo.com,jalkanen,jalkanen,19/Jan/13 19:52,16/Apr/19 09:32,14/Jul/23 05:53,23/Jan/13 06:48,1.1.10,1.2.1,,,,,0,,,,,"The following lines started repeating every minute in the log file

{noformat}
 INFO [GossipStage:1] 2013-01-19 19:35:43,929 Gossiper.java (line 831) InetAddress /10.238.x.y is now dead.
 INFO [GossipStage:1] 2013-01-19 19:35:43,930 StorageService.java (line 1291) Removing token 170141183460469231731687303715884105718 for /10.238.x.y
{noformat}

Also, I got about 3000 threads which all look like this:

{noformat}
Name: WRITE-/10.238.x.y
State: WAITING on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1bb65c0f
Total blocked: 0  Total waited: 3

Stack trace: 
 sun.misc.Unsafe.park(Native Method)
java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:2043)
java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:442)
org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:104)
{noformat}

A new thread seems to be created every minute, and they never go away.

The endpoint in question had been a part of the cluster weeks ago, and the node exhibiting the thread growth was added yesterday.

Anyway, assassinating the endpoint in question stopped thread growth (but kept the existing threads running), so this isn't a huge issue.  But I don't think the thread count is supposed to be increasing like this...","EC2, JDK 7u9, Ubuntu 12.04.1 LTS",jalkanen,jjordan,timiblossom,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"21/Jan/13 04:07;vijay2win@yahoo.com;0001-CASSANDRA-5175.patch;https://issues.apache.org/jira/secure/attachment/12565729/0001-CASSANDRA-5175.patch",,,,,,,,,,,,,,,,,,,,1.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,305577,,,Sat Jul 27 02:12:01 UTC 2013,,,,,,,,,,"0|i18ffr:",257303,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"19/Jan/13 23:43;brandon.williams;Can you take a look, Vijay?;;;","20/Jan/13 04:28;vijay2win@yahoo.com;I think the problem is that when we close the connection the thread will not be ended. Attached patch solves thread leak problem.;;;","22/Jan/13 19:06;brandon.williams;Hard to know for certain if this is truly the cause, but it sounds plausible. +1;;;","23/Jan/13 06:48;vijay2win@yahoo.com;Committed to trunk, 1.1 and 1.2. Thanks!;;;","23/Jan/13 07:46;vijay2win@yahoo.com;Also added a commit db8705294ba96fe2b746fea4f26a919538653ebd for test failure (dtest), basically we should not close the thread because we convicted the node. Thanks!;;;","26/Jul/13 19:11;timiblossom;Hi Vijay,

I am using your commit db8705294ba96fe2b746fea4f26a919538653ebd but I think the logic in this commit is not the same as the attached patch.  Please take a look.

if (m == CLOSE_SENTINEL)
             {
                 disconnect();
+                if (!isStopped)
+                    break;
                 continue;
             }


I think it should be :

       if (isStopped)
           break;

Thanks.
;;;","27/Jul/13 02:12;vijay2win@yahoo.com;Yes there was another commit on top the attached patch to fix the test cases, yes the logic has changed since calling close() is the only time we need to stop the thread.

Current code in the repo
{code}
            if (m == CLOSE_SENTINEL)
            {
                disconnect();
                if (isStopped)
                    break;
                continue;
            }
{code};;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Save EC2Snitch topology information in system table,CASSANDRA-5171,12628167,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,vijay2win@yahoo.com,vijay2win@yahoo.com,vijay2win@yahoo.com,18/Jan/13 07:43,16/Apr/19 09:32,14/Jul/23 05:53,10/Jul/13 02:11,2.0 beta 2,,,,,,1,,,,,EC2Snitch currently waits for the Gossip information to understand the cluster information every time we restart. It will be nice to use already available system table info similar to GPFS.,EC2,jasobrown,mishail,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jul/13 08:06;vijay2win@yahoo.com;0001-CASSANDRA-5171-v2.patch;https://issues.apache.org/jira/secure/attachment/12591115/0001-CASSANDRA-5171-v2.patch","18/Jan/13 07:45;vijay2win@yahoo.com;0001-CASSANDRA-5171.patch;https://issues.apache.org/jira/secure/attachment/12565443/0001-CASSANDRA-5171.patch",,,,,,,,,,,,,,,,,,,2.0,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,305041,,,Wed Jul 10 20:22:09 UTC 2013,,,,,,,,,,"0|i186lr:",255872,,,,,,,,,brandon.williams,,brandon.williams,Critical,,,,,,,,,,,,,,,,,,"18/Jan/13 15:09;brandon.williams;+1;;;","18/Jan/13 19:00;vijay2win@yahoo.com;Committed to 1.2 and trunk. Thanks!;;;","22/May/13 15:53;jbellis;This was reverted in CASSANDRA-5432, but I think the problem it solves is actually pretty severe, so I'm reopening it.

The problem is that pretty much everything from TokenMetadata to NetworkTopologyStrategy assumes that once we see a node, the snitch can tell us where it lives, and in particular that once the snitch tells us where a node lives it won't change its answer.

So this is problematic:

{code}
    public String getDatacenter(InetAddress endpoint)
    {
        if (endpoint.equals(FBUtilities.getBroadcastAddress()))
            return ec2region;
        EndpointState state = Gossiper.instance.getEndpointStateForEndpoint(endpoint);
        if (state == null || state.getApplicationState(ApplicationState.DC) == null)
            return DEFAULT_DC;
        return state.getApplicationState(ApplicationState.DC).value;
    }
{code}

That is, if we don't know where a node belongs (e.g., we just restarted and haven't been gosipped to yet), assume it's in {{DEFAULT_DC}}.

This can lead to data loss.  Consider node X in DC1, where keyspace KS is replicated.  Suddenly X is yanked out of DC1 and placed in DC2, where KS is not replicated.  Nobody will bother querying X for the data in KS that was formerly replicated to it.  Even repair will not see it.;;;","07/Jul/13 08:06;vijay2win@yahoo.com;Attached patch brings the reverted patch back to life, in addition it saves the reseted_ip in system table so when a connection is reopened to the host we will use the reseted_ip instead of endpoint address.;;;","07/Jul/13 17:16;vijay2win@yahoo.com;Looks like CASSANDRA-5669 forces the user to open public and private IP's for communication (and hence CASSANDRA-5432 no longer a problem) within a AZ and hence v2 is not needed and we can commit v1.;;;","09/Jul/13 12:20;jasobrown;While this patch (v1 actually) was reverted in CASANDRA-5432, it wasn't satisfactorily answered why the patch failed to work as expected. I'm adding details here so we can get this ticket done right :).

First it's helpful to explore how a node can start gossip in EC2MRS with inter-DC (inter-region) enabled (and a Priam-type setup).

# ec2 instance is started, Priam comes up first and adds publicIP/sslPort to the security group's ingress privileges (so this node can accept connections on it's publicIP/sslPort from anywhere). 
# c* starts, and gets seed node public hostnames from Priam
# gossip to one of the seeds - the public hostname will resolve to the node's public IP addr.
# When OTC goes to write the first message on the seed, it gets a socket from OTCP.newSocket(). newSocket() calls isEncryptedChannel() to determine if we need to encrypt the data on the wire. As we don't know anything yet about the seed node (remember we havn't started gossip yet with anyone), isEncryptedChannel() will always return true when the following are true:
## internode_encryption != none
## we don't know the DC or RACK info for the remote node (which is the case when using the EC2MRS). This step is a little funky as OTCP calls the snitch for the seed's DC/RACK, to which EC2MRS will return UNKNOWN-DC/UNKNOWN-RACK, which will just happen to not match a value like ""us-east-1"" (the current's node's DC). 
# create the socket using remote node's publicIP addr on the SSL port.
# create the connection from and send messages successfully, assuming you've opened the SSL port for public addresses on the security group (which Priam handles).

Thus, if we are connecting to a node in the same EC2 region, we connect on the publicIP (as expected) but use the SSL port.

After we learn, via gossip, about a remote node's DC/RACK/localIP, we can choose to reconnect to nodes in the same region on the localIP/nonSSLPort.

The reason why Vijay's patch had problems here was because on restart, we would already know the DC/RACK from the previous execution of c* on this node, and the check in OTCP.isEncryptedChannel() returns false (do not use encryption), so a we choose to use the non-SSL port when creating a connection to the publicIP. Thus the connection creation unltimately fails because the non-SSL port is not opened for traffic on the security group for the public IP (nor should it be). EDIT: The other part of the problem is that we start the connection on the publicIP rather than localIP (INTERNAL_IP) even if we already have the localIP.

To make this patch work then, I think getting the localIP address in the OTCP's ctor would work the best. Code would look something like this:

{code}
    OutboundTcpConnectionPool(InetAddress remoteEp)
    {
        EndpointState epState =  Gossiper.instance.getEndpointStateForEndpoint(remoteEp);
        if(epState != null && epState.getApplicationState(ApplicationState.INTERNAL_IP) != null
            && epState.getApplicationState(ApplicationState.DC).equals(snitch.getDatacenter(FBUtilities.getBroadcastAddress()))
        {
            id = epState.getApplicationState(ApplicationState.INTERNAL_IP);             
        }
        else
        {
            id = remoteEp;
        }
        
        cmdCon = new OutboundTcpConnection(this);
        cmdCon.start();
        ackCon = new OutboundTcpConnection(this);
        ackCon.start();

        metrics = new ConnectionMetrics(id, this);
    }
{code}

Then you would connect on the localIP addr with the correct port (SSL or non-SSL).;;;","09/Jul/13 15:20;jasobrown;Ahh, didn't look at Vijay's second patch, but it more or less does what I suggested - however, I thought we were already keeping the resettedAddr in the system table, but the second patch adds that in.;;;","09/Jul/13 16:44;jasobrown;I'm +1 on the v2 patch. As a minor nit, you might rename the ""communication_ip"" to something like ""preferred_ip"".

As an aside, I've never been thrilled with the default ""UNKNOWN-DC"" that can get returned when we don't know the DC. It's guaranteed to be wrong in almost all cases. However, I'm not sure what we'd do instead - returning null or empty string seems only slightly worse than the current default.;;;","09/Jul/13 17:47;vijay2win@yahoo.com;{quote}
I've never been thrilled with the default ""UNKNOWN-DC"" 
{quote}

I am not a fan either, if we throw an exception node will never start the gossiping.... 
Back in the days, it made sense with the old way of versioning etc. We should probably open another ticket to discuses this if needed.

I will commit with the nit in few min. Thanks!;;;","10/Jul/13 06:41;vijay2win@yahoo.com;PS: i only committed to 2.0 to be safe, let me know if you think otherwise. Thanks!;;;","10/Jul/13 13:19;jasobrown;I thought the patch was reasonable enough for 1.2. If you give me a few days, I can test it out in our env, and let you know if it borks everything or not.

EDIT: Yeah, will definitely want to test to make sure it's cool with CASSANDRA-5669 (i.e. they don't collide to not connect at all).;;;","10/Jul/13 17:26;vijay2win@yahoo.com;Thanks Jason!;;;","10/Jul/13 20:22;jasobrown;damn jira hotkeys - sorry this got reassigned (to me, and back), Vijay :);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
word count example fails with InvalidRequestException(why:Start key's token sorts after end token),CASSANDRA-5168,12628105,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,jeromatron,jeromatron,17/Jan/13 20:22,16/Apr/19 09:32,14/Jul/23 05:53,25/Jan/13 20:28,1.1.10,1.2.2,,,,,0,,,,,Tried with the latest 1.2 branch (commit d64dc2eb3a1a3c3771bbe3218af9ce9629ec67bf) and got this error.  Seems related to but different than CASSANDRA-5106.,,jeromatron,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5217,,,,,,,,,,,,,,,,,,,,"25/Jan/13 19:39;brandon.williams;5168.txt;https://issues.apache.org/jira/secure/attachment/12566541/5168.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,304956,,,Fri Jan 25 20:28:25 UTC 2013,,,,,,,,,,"0|i17x3z:",254334,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"25/Jan/13 18:58;brandon.williams;{noformat}
java.lang.RuntimeException: InvalidRequestException(why:Start key's token sorts after end token)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:475)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:481)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.computeNext(ColumnFamilyRecordReader.java:427)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getProgress(ColumnFamilyRecordReader.java:109)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.getProgress(MapTask.java:411)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:427)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: InvalidRequestException(why:Start key's token sorts after end token)
        at org.apache.cassandra.thrift.Cassandra$get_paged_slice_result.read(Cassandra.java:13685)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_paged_slice(Cassandra.java:731)
        at org.apache.cassandra.thrift.Cassandra$Client.get_paged_slice(Cassandra.java:715)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$WideRowIterator.maybeInit(ColumnFamilyRecordReader.java:460)
        ... 12 more
{noformat}

Only affects the wide row iterator.  In this case it's setting start key to -5551577223485402047 (key808) and comparing to -9223372036854775808;;;","25/Jan/13 19:39;brandon.williams;I think we need to check that end token is not minimum here too.;;;","25/Jan/13 20:21;jbellis;+1;;;","25/Jan/13 20:28;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Node isn't removed from system.peers after 'nodetool removenode',CASSANDRA-5167,12628028,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,brandon.williams,ng@issuu.com,ng@issuu.com,17/Jan/13 12:20,16/Apr/19 09:32,14/Jul/23 05:53,18/Jan/13 16:20,1.2.1,,,,,,0,,,,,"In a 3 node live cluster - After a replacement of a dead node, the old node remains in the system.peers table, even after running 'nodetool removenode <ID>'.

","Ubuntu 12.10, Java 1.7.0_09 (OpenJDK)",ng@issuu.com,slebresne,tbsalling,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5190,,,,,,,,,,,,,,,,,,,,"18/Jan/13 15:57;brandon.williams;5167-v2.txt;https://issues.apache.org/jira/secure/attachment/12565490/5167-v2.txt","17/Jan/13 19:52;brandon.williams;5167.txt;https://issues.apache.org/jira/secure/attachment/12565366/5167.txt",,,,,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,304876,,,Fri Jan 18 16:20:02 UTC 2013,,,,,,,,,,"0|i17wjz:",254243,,,,,,,,,slebresne,,slebresne,Low,,,,,,,,,,,,,,,,,,"17/Jan/13 19:22;brandon.williams;I'm not sure why we ever made a 'removeTokens' method instead of 'removeEndpoint' since we're never going to want to remove some subset of a node's tokens.  Patch to delete endpoints.;;;","18/Jan/13 09:37;slebresne;bq. I'm not sure why we ever made a 'removeTokens' method instead of 'removeEndpoint'

That slightly confused me too in CASSANDRA-4351, though I went for the lazy option of maintaining behavior blindly.

So definitely agreed on the idea. Two tiny remark on the patch though:
* in SS.handleStateRemoving, we call Gossiper.instance.removeEndpoint() but shouldn't we called SystemTable.removeEndpoint() too? In fact, it seems to me we may want to group both in a small utility method since at least when we call the Gossiper one, I think we always should call SystemTable one.
* in SS.handleStateNormal, ST.removeEndpoint should probably only be call if {{!isClient}} like in SS.excise().;;;","18/Jan/13 15:57;brandon.williams;v2 incorporates both ideas.;;;","18/Jan/13 16:08;slebresne;+1 with the nit that in excise I think we can call the new removeEndpoint instead of doing both call separately.;;;","18/Jan/13 16:20;brandon.williams;Committed w/nit fixed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Saved key cache is not loaded when opening ColumnFamily,CASSANDRA-5166,12627947,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,16/Jan/13 23:03,16/Apr/19 09:32,14/Jul/23 05:53,18/Jan/13 17:49,1.1.10,,,,,,0,,,,,"_This bug happens on cassandra version 1.1.3 ~ 1.1.x only_

In order to load key cache in SSTableReader#open, keyCache has to be set by calling setTrackedBy before loading. CASSANDRA-4436 changed the order to load then setTrackedBy, so saved key cache is never loaded.
In 1.2, key cache stores both key and position, and is loaded out side of SSTableReader so the issue only happens on 1.1.x branch.",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5552,,,,,,,,,,,,,,,,,,,,"16/Jan/13 23:04;yukim;0001-key-cache-loading-test.patch;https://issues.apache.org/jira/secure/attachment/12565202/0001-key-cache-loading-test.patch","16/Jan/13 23:04;yukim;0002-fix-loading-key-cache.patch;https://issues.apache.org/jira/secure/attachment/12565203/0002-fix-loading-key-cache.patch",,,,,,,,,,,,,,,,,,,2.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,304795,,,Fri Jan 18 17:49:12 UTC 2013,,,,,,,,,,"0|i17w0n:",254156,,,,,,,,,jbellis,,jbellis,Low,,1.1.3,,,,,,,,,,,,,,,,"16/Jan/13 23:04;yukim;Test and proposed fix attached.;;;","18/Jan/13 00:25;jbellis;+1;;;","18/Jan/13 17:49;yukim;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Need to trim DC and RACK names in cassandra-topology.properties file,CASSANDRA-5165,12627819,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,azotcsit,azotcsit,azotcsit,16/Jan/13 12:11,16/Apr/19 09:32,14/Jul/23 05:53,16/Jan/13 20:46,1.2.1,,,,,,0,snitch,,,,"Some misprints in cassandra-topology.properties file can be painful for debugging. For example extra symbols at the end of line like a space. So difference between ""DC1:RACK1"" and ""DC1:RACK1 "" couldn't be detected using nodetool ring command.

I think that symbols like a space shouldn't be allowed in DC/RACK names.  So I suggest to trim them.

The patch has been attached.
",cassandra-1.1.6 (DataStax distribution).,azotcsit,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/13 12:12;azotcsit;cassandra-1.2-5165_trim_spaces.txt;https://issues.apache.org/jira/secure/attachment/12565111/cassandra-1.2-5165_trim_spaces.txt",,,,,,,,,,,,,,,,,,,,1.0,azotcsit,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,304607,,,Fri Jan 18 11:00:14 UTC 2013,,,,,,,,,,"0|i17nnr:",252802,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"16/Jan/13 20:46;brandon.williams;I don't see a problem with having a space in the middle, like ""San Antonio"", but you're right about trimming. Committed with a small change to GPFS to match.;;;","18/Jan/13 11:00;azotcsit;Thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Invalid streamId in cql binary protocol when using invalid CL,CASSANDRA-5164,12627737,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,pchalamet,pchalamet,15/Jan/13 21:13,16/Apr/19 09:32,14/Jul/23 05:53,27/Mar/13 10:28,1.2.4,,,Legacy/CQL,,,0,,,,,"Execute a query using invalid CL (0x100 for example)

The response comes but does not use the request streamId (always 0).","Windows 8, java version ""1.6.0_37"" x86",pchalamet,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5438,,,,,,,,,,,,,,,,,,,,"26/Mar/13 22:28;pchalamet;5164-2.txt;https://issues.apache.org/jira/secure/attachment/12575598/5164-2.txt","25/Mar/13 13:59;slebresne;5164.txt;https://issues.apache.org/jira/secure/attachment/12575314/5164.txt",,,,,,,,,,,,,,,,,,,2.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,304524,,,Wed Mar 27 10:28:27 UTC 2013,,,,,,,,,,"0|i17n4f:",252715,,,,,,,,,pchalamet,,pchalamet,Low,,,,,,,,,,,,,,,,,,"23/Mar/13 23:10;jbellis;We should reject invalid CL.;;;","25/Mar/13 13:59;slebresne;Attaching patch to preserve the streamId in that case. I note that we don't do any effort to preserve the streamId if you screw up the frame format, but imo it's fair game. If you don't respect the protocol at that level, you have other problems to worry about than the streamId. ;;;","26/Mar/13 22:28;pchalamet;fix invalid cast;;;","26/Mar/13 22:30;pchalamet;The patch would be better by inverting lines 202 & 203 in ErrorMessage.java:
{code}
public static ErrorMessage fromException(Throwable e)
 ...
 streamId = ((WrappedException)e).streamId;
 e = e.getCause();
{code}

Otherwise, an invalid cast is raised. Patch enclosed - I've tested it and exception nicely flows to the client using the right stream id.;;;","27/Mar/13 10:28;slebresne;Oups, committed v2, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Remove SimpleTransportFactory,CASSANDRA-5162,12627715,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jbellis,jasobrown,jasobrown,15/Jan/13 19:03,16/Apr/19 09:32,14/Jul/23 05:53,15/Jan/13 19:06,2.0 beta 1,,,,,15/Jan/13 00:00,0,,,,,"We don't support non-framed thrift transport, so remove as o.a.c.cli.transport. SimpleTransportFactory it's an unneeded class",,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,304502,,,Tue Jan 15 19:06:13 UTC 2013,,,,,,,,,,"0|i17mxz:",252686,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"15/Jan/13 19:05;jasobrown;Jonathan will remove it - no patch necessary :);;;","15/Jan/13 19:06;jbellis;done in 83c014777a8d4597ca9b6cfc0f3dd9c77bd39d50;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CFS.allUserDefined() doesn't exclude system_auth and system_traces keysapces,CASSANDRA-5160,12627654,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jasobrown,jasobrown,jasobrown,15/Jan/13 14:51,16/Apr/19 09:32,14/Jul/23 05:53,15/Jan/13 15:10,1.2.1,,,,,14/Jan/13 00:00,0,,,,,Make sure CFS.allUserDefined() excludes all system-related keyspaces.,,jasobrown,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5738,,,,,,,,,,,,,,,"15/Jan/13 14:52;jasobrown;0001-Make-sure-CFS.allUserDefined-excludes-all-system-rel.patch;https://issues.apache.org/jira/secure/attachment/12564933/0001-Make-sure-CFS.allUserDefined-excludes-all-system-rel.patch",,,,,,,,,,,,,,,,,,,,1.0,jasobrown,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,304440,,,Tue Jan 15 15:10:47 UTC 2013,,,,,,,,,,"0|i17mjb:",252620,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"15/Jan/13 14:52;jasobrown;Very trivial one-line change;;;","15/Jan/13 15:10;brandon.williams;Committed, thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
max client timestamp,CASSANDRA-5153,12627370,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,wy96f,wy96f,14/Jan/13 03:16,16/Apr/19 09:32,14/Jul/23 05:53,24/Jan/13 14:58,1.1.10,1.2.1,,,,,0,,,,,"1. In public LazilyCompactedRow(CompactionController controller, List<? extends ICountableColumnIterator> rows)
   columnStats = new ColumnStats(reducer == null ? 0 : reducer.columns, reducer == null ? Long.MIN_VALUE : reducer.maxTimestampSeen,
                                      reducer == null ? new StreamingHistogram(SSTable.TOMBSTONE_HISTOGRAM_BIN_SIZE) : reducer.tombstones

  Tthe maxTimestampSeen should be max(emptyColumnFamily.deletionInfo().maxTimestamp(), reducer.maxTimestampSeen)?

2. In private ColumnFamily collectTimeOrderedData()
                // if we've already seen a row tombstone with a timestamp greater
                // than the most recent update to this sstable, we're done, since the rest of the sstables
                // will also be older
                if (sstable.getMaxTimestamp() < mostRecentRowTombstone)
                    break; 
   In the case that sstable.getMaxTimestamp == Long.MIN_VALUE, is it logical?

",,christianmovi,slebresne,wadey,wy96f,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/13 08:27;jbellis;5153.txt;https://issues.apache.org/jira/secure/attachment/12565093/5153.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,304154,,,Thu Jan 24 14:58:34 UTC 2013,,,,,,,,,,"0|i17js7:",252174,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"14/Jan/13 03:27;jbellis;What about max client timestamp?;;;","16/Jan/13 01:40;wy96f;Any ideas?
Thanks;;;","16/Jan/13 08:27;jbellis;I think you're right.  Patch attached to fix and to treat old sstables with MAX_VALUE instead of MIN.;;;","21/Jan/13 14:40;slebresne;I think the correct fix in LCR would be:
{noformat}
markedAt = emptyColumnFamily.getMarkedForDeleteAt();
maxTimestamp = reducer == null ? markedAt : Math.max(markedAt, reducer.maxTimestampSeen);
{noformat}
because {{reducer == null}} implies there is no columns for that row in any of the source SSTable, and thus guarantees we have a row tombstone (otherwise we wouldn't have written the row in the first place). And in fact, even if we were to write non-tombstoned empty rows, then MIN_VALUE would be the correct value for maxTimestamp.

Also, while the default of MIN_VALUE for old non-timestamp-tracking sstables is clearly bogus and should be fixed, I would almost suggest not bumping the sstable version (I'm still hesitant but leaning towards not doing it). The rational is that there are only 2 cases currently where a sstable can have a MIN_VALUE max timestamp:
# the sstable is a pre-1.0.10 one that don't track timestamp. Fixing SSTableMetada fixes that part.
# the sstable is entirely and uniquely composed of row tombstones.

The version bump is ""fixing"" only the latter but at the price of temporarly breaking the collectTimeOrderedData optimization for everyone that will upgrade to the version containing this. Granted collectTimeOrderedData is ""just an optimization"", but having a sstable entirely composed of row tombstones is a pretty remote risk. And besides, for it to trigger a problem you actually need at least 2 sstables entirely composed of row tombstones (and even then there is no guaranteed you'll get the bug). Feels even more than remote.;;;","24/Jan/13 14:58;jbellis;Committed w/ suggested changes.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Implement better way of eliminating compaction left overs.,CASSANDRA-5151,12627235,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,yukim,yukim,11/Jan/13 19:17,16/Apr/19 09:32,14/Jul/23 05:53,01/Jul/13 20:29,2.0 beta 1,,,,,,0,,,,,"This is from discussion in CASSANDRA-5137. Currently we skip loading SSTables that are left over from incomplete compaction to not over-count counter, but the way we track compaction completion is not secure.

One possible solution is to create system CF like:

{code}
create table compaction_log (
  id uuid primary key,
  inputs set<int>,
  outputs set<int>
);
{code}

to track incomplete compaction.",,aleksey,mkjellman,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Feb/13 21:02;yukim;0001-move-scheduling-MeteredFlusher-to-CassandraDaemon.patch;https://issues.apache.org/jira/secure/attachment/12568889/0001-move-scheduling-MeteredFlusher-to-CassandraDaemon.patch","17/Jan/13 21:16;yukim;5151-1.2.txt;https://issues.apache.org/jira/secure/attachment/12565372/5151-1.2.txt","18/Jan/13 21:10;yukim;5151-v2.txt;https://issues.apache.org/jira/secure/attachment/12565555/5151-v2.txt",,,,,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,304005,,,Mon Jul 01 14:30:09 UTC 2013,,,,,,,,,,"0|i17hfb:",251792,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"17/Jan/13 17:31;yukim;We want to remove left-overs before opening CFs by reading compaction log system CF. The problem is, commit log has to be recovered before reading from any CF, but CF initialization should be completed before commit log recovery. So there is no chance to scrub off left-overs before CF init. Obviously we don't want to flush every time we update compaction log.

Alternative way is to write compaction log file with inputs each time we start compaction, and remove it when complete.
If CF sees that compaction log files in its data directory, that means there are unfinished compactions and those sstable files with ancestors that are in compaction log files should be deleted. Does this sound likely to work?;;;","17/Jan/13 21:16;yukim;Initial patch attached to get feedback. It writes compaction log file to track incomplete compactions described in my previous comment.

SSTable left-overs are deleted during node start up with CFS.scrubDataDirectories. In order to get ancestors of SSTable files, it has to deserialize each Stats.db file so slows down node startup, but this only happens when incomplete compaction left compaction log file.

The patch does not rely on CQL3 collection at all, so it is possible to port into 1.1.x.;;;","18/Jan/13 00:38;jbellis;bq. commit log has to be recovered before reading from any CF

That can't be true, can it?  At the least we have to special case schema.;;;","18/Jan/13 01:52;yukim;It is. Except schema_* are flushed at the end of migration, other system cf used before commit log recovery like local also flushes every time after its data updated. If we go using system cf for this, we need to do the same, and doing so requires 2 flushes during one compaction, one for inserting log and one for removing at the end.

I think plain file under cf data dir is easy to implement both for writing log and recovery.;;;","18/Jan/13 02:27;jbellis;Flushing is roughly the same cost as the fsync you need to make sure the plain file is safely written, so I don't see a performance penalty.  And I'm nervous about more one-off files (manifest was bad enough).  Storing in the system table does seem cleaner to me.  Am I missing something still?;;;","18/Jan/13 17:43;yukim;If we use CQL, we cannot clean up left-overs in system keyspace before opening it since columnfamilies under system keyspace is open at the time we issue CQL.;;;","18/Jan/13 17:48;jbellis;You're saying there's a problem with storing data about compacting system tables, in a system table?

I think you're right, but can you spell it out with an example?;;;","18/Jan/13 18:09;yukim;Yes.

Storing compaction log in system keyspace can be done without problem.
But when the node starts up and tries to eliminate left overs based on query result from compaction log, there is a problem.

For example, the node goes down leaving system.schema_keyspaces' compaction unfinished and both original sstable(A) and produced sstable(A') remained in data dir. What we want when node restarts is that schema_keyspaces only references A and discards A'.
So we query compaction log from system ks to determine which sstables are left overs. But when we query system ks, cassandra opens whole columnfamilies under system ks, and schema_keyspaces already references both A and A' at this time. So even though we get the query result, we cannot do anything further.;;;","18/Jan/13 18:20;slebresne;That's true, but we don't store counters in the system tables (and I take on myself to veto whomever suggests to change that fact), so it's safe to always pick all sstables without consulting the compaction log. In other words I suggest special casing the system keyspace on startup so it don't bother with the compaction log. As far as I'm prefer, I prefer that many times to relying on an external file.;;;","18/Jan/13 21:10;yukim;Alright, CQL3 based v2 patch attached.

Below is newly introduced compaction_log schema:

{code}
CREATE TABLE compaction_log (
  id uuid PRIMARY KEY,
  keyspace_name text,
  columnfamily_name text,
  inputs set<int>
) WITH COMMENT='unfinished compactions'
{code}

data in compaction_log gets created at the beginning of compaction and removed when finished. It only tracks compaction inputs and the node uses them combined with each sstable's ancestors to remove leftovers.

compaction_log does not store logs from system columnfamilies(should we skip auth and trace also?).;;;","31/Jan/13 20:30;jbellis;Posted a patch to https://github.com/jbellis/cassandra/branches/5151 that adds removal of obsolete sstables for completed compactions, as well as some cosmetic changes.;;;","31/Jan/13 21:41;yukim;patch lgtm, +1.;;;","01/Feb/13 04:51;jbellis;committed;;;","06/Feb/13 01:15;aleksey;After this commit (aa90c88be14b337714739cd857c12cad2a9fedeb) two tests started failing from time to time (not 100% consistently, bit noticeably often):

1. ColumnFamilyStoreTest

{noformat}
    [junit] Testsuite: org.apache.cassandra.db.ColumnFamilyStoreTest
    [junit] Tests run: 28, Failures: 0, Errors: 0, Time elapsed: 5.919 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit]  WARN 04:11:00,492 setting live ratio to maximum of 64.0 instead of 102.56071964017991
    [junit]  WARN 04:11:00,610 setting live ratio to maximum of 64.0 instead of 303.2307692307692
    [junit] ERROR 04:11:01,182 Fatal exception in thread Thread[CompactionExecutor:2,1,main]
    [junit] java.lang.AssertionError: Incoherent new size -3 replacing [SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-4-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-3-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-6-Data.db'), SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-5-Data.db')] by [SSTableReader(path='build/test/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-7-Data.db')] in View(pending_count=1, sstables=[], compacting=[])
    [junit] 	at org.apache.cassandra.db.DataTracker$View.newSSTables(DataTracker.java:545)
    [junit] 	at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:517)
    [junit] 	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:320)
    [junit] 	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:227)
    [junit] 	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:1030)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:251)
    [junit] 	at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
    [junit] 	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:72)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:193)
    [junit] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:439)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:680)
    [junit] ------------- ---------------- ---------------
{noformat}

2. CompactionsTest

{noformat}
   [junit] Testsuite: org.apache.cassandra.db.compaction.CompactionsTest
    [junit] Tests run: 9, Failures: 1, Errors: 0, Time elapsed: 20.298 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit]  WARN 04:14:16,985 setting live ratio to maximum of 64.0 instead of 360.7111111111111
    [junit]  WARN 04:14:17,121 setting live ratio to maximum of 64.0 instead of 365.15555555555557
    [junit] ------------- ---------------- ---------------
    [junit] Testcase: testCompactionLog(org.apache.cassandra.db.compaction.CompactionsTest):	FAILED
    [junit] null
    [junit] junit.framework.AssertionFailedError
    [junit] 	at org.apache.cassandra.db.compaction.CompactionsTest.testCompactionLog(CompactionsTest.java:311)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.db.compaction.CompactionsTest FAILED
{noformat};;;","06/Feb/13 04:33;yukim;I need more time for #1, but for #2, I committed fix in f3091835e67ebdc0171991b2ca283e6cd357cda3.
;;;","09/Feb/13 02:06;mkjellman;[~yukim] looks like there might still be an unexpected condition here

{code}
java.lang.IllegalStateException: Unfinished compactions reference missing sstables. This should never happen since compactions are marked finished before we start removing the old sstables.
        at org.apache.cassandra.db.ColumnFamilyStore.removeUnfinishedCompactionLeftovers(ColumnFamilyStore.java:444)
        at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:213)
        at org.apache.cassandra.service.CassandraDaemon.init(CassandraDaemon.java:324)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:601)
        at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:188)
Cannot load daemon
Service exit with a return value of 3
{code};;;","09/Feb/13 02:47;mkjellman;wondering if this is related...

{code}
 INFO 18:43:25,360 Completed flushing /data/cassandra/system/compactions_in_progress/system-compactions_in_progress-ib-60-Data.db (192 bytes) for commitlog position ReplayPosition(segmentId=1360374628064, position=29215408)
 INFO 18:43:25,364 Compacting [SSTableReader(path='/var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2182-Data.db'), SSTableReader(path='/var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2180-Data.db'), SSTableReader(path='/var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2185-Data.db'), SSTableReader(path='/data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-2181-Data.db'), SSTableReader(path='/data/cassandra/evidence/fingerprints/evidence-fingerprints-ib-2184-Data.db'), SSTableReader(path='/var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2183-Data.db'), SSTableReader(path='/var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2179-Data.db')]
ERROR 18:43:25,367 Exception in thread Thread[CompactionExecutor:85,1,main]
java.lang.RuntimeException: java.io.FileNotFoundException: /var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2183-Data.db (No such file or directory)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:61)
        at org.apache.cassandra.io.sstable.SSTableReader.openDataReader(SSTableReader.java:1121)
        at org.apache.cassandra.io.sstable.SSTableScanner.<init>(SSTableScanner.java:51)
        at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:954)
        at org.apache.cassandra.io.sstable.SSTableReader.getDirectScanner(SSTableReader.java:966)
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:147)
        at org.apache.cassandra.db.compaction.AbstractCompactionStrategy.getScanners(AbstractCompactionStrategy.java:153)
        at org.apache.cassandra.db.compaction.CompactionTask.runWith(CompactionTask.java:124)
        at org.apache.cassandra.io.util.DiskAwareRunnable.runMayThrow(DiskAwareRunnable.java:48)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
        at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:63)
        at org.apache.cassandra.db.compaction.CompactionManager$BackgroundCompactionTask.run(CompactionManager.java:193)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/evidence/fingerprints/evidence-fingerprints-ib-2183-Data.db (No such file or directory)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
        at org.apache.cassandra.io.util.RandomAccessReader.<init>(RandomAccessReader.java:67)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.<init>(CompressedRandomAccessReader.java:78)
        at org.apache.cassandra.io.compress.CompressedRandomAccessReader.open(CompressedRandomAccessReader.java:57)
        ... 17 more
{code};;;","11/Feb/13 17:33;yukim;[~mkjellman] What else do you see about the file ""evidence-fingerprints-ib-2183-Data.db"" in your log file? Can you see the file is compacted elsewhere?;;;","11/Feb/13 20:33;yukim;I think there is concurrency problem between opening ColumnFamilyStore and deleting/scrubbing SSTables.

This static block(https://github.com/apache/cassandra/blob/cassandra-1.2.1/src/java/org/apache/cassandra/db/ColumnFamilyStore.java#L88) schedules MeteredFlusher which accesses all ColumnFamilyStore when it runs every 1 sec. Scheduling is done when JVM first load ColumnFamilyStore class, so after that, there is always a chance to open SSTables before doing scrub directory/remove compaction left overs.
We should move the content of static block at the end of CassandraDaemon setup.;;;","11/Feb/13 21:02;yukim;Patch for v1.2 attached to schedule MeteredFlusher in CassandraDaemon.
Probably it's better to patch 1.1 as well.;;;","13/Feb/13 16:22;mkjellman;It appears the patch seems to have resolved the FileNotFoundException but I'm still able to reproduce the IllegalStateException.

Also, it seems that once a node throws this exception, even after deleting the sstables in system/compactions_in_process that node will now throw the Exception on startup every time.;;;","13/Feb/13 17:01;brandon.williams;I move to revert the non-bugfix portion of this patch from 1.2 and push it to trunk, given the fallout we've seen thus far.;;;","13/Feb/13 17:06;mkjellman;[~brandon.williams] is the fallout due to bugs in the patch/new implementation or is it exposing unrelated bugs that were just being skipped before?;;;","13/Feb/13 17:11;brandon.williams;I'll let Yuki decide, but the fact that we failed a dtest, a utest, and most importantly the [~mkjellman] test is worrisome. ;);;;","13/Feb/13 17:22;yukim;I'm thinking of the cause of this can be CASSANDRA-5241, since this function relies on a lot of concurrent forceBlockingFlush. So there is a chance that compaction_in_progress flush is not complete at the end of the compaction.

If that is the case, we should wait until we fix CASSANDRA-5241.;;;","13/Feb/13 17:34;brandon.williams;bq. If that is the case, we should wait until we fix CASSANDRA-5241.

+1, that is a troublesome bug in many ways.;;;","14/Feb/13 16:30;yukim;OK, I will revert this from 1.2 branch but leave it in trunk.;;;","14/Feb/13 17:33;yukim;I reverted change in 1.2 and changed fixver to 2.0.
However, we still need to fix conflict in opening CFS with the patch attached before.
Should I create and move to the new ticket for that?;;;","13/Mar/13 19:50;yukim;We should also remove ancestors from sstable metadata because those could consume heap when you are using LCS(CASSANDRA-5342).;;;","24/May/13 14:41;jbellis;bq. However, we still need to fix conflict in opening CFS with the patch attached before.

(This was done in CASSANDRA-5350.);;;","26/Jun/13 16:29;slebresne;What's the status of this? Is there something more to do?;;;","27/Jun/13 16:47;slebresne;So let me see if I can sum what's going on here.

As far as trunk is concerned, the initial patch is still there but we suspect it may cause problems, namely:
# The AssertionError ""Incoherent new size -3 replacing ..."" error in ColumnFamilyStoreTest.
# The IllegalStateException ""Unfinished compactions reference missing sstables"" that Michael experienced.

As far as I can tell from reading the history, the other problems have been fixed, right?

Now, concerning 1) (the AssertionError), I ""think"" the problem is just with CFS.clearUnsafe(). Namely, it butcher the DataTracker for test purposes, but if a compaction runs concurrently, at the end of said compaction it will try to replace sstables that are not there anymore, hence the exception. I've committed a simple workaround in commit 3935587 that make clearUnsafe() stop compactions first. As far as I can tell, this did fixed the error (I've been able to run ColumnFamilyStoreTest a number of time without getting the stack while I had it consistently before).

Remains 2), the IllegalStateException. Yuki suggests that it may have been caused by CASSANDRA-5241 that is resolved now. So should we close this until further notice?  [~mkjellman] Do you remember if you were able to easily reproduce that error back when you tested this patch?
;;;","28/Jun/13 21:02;mkjellman;[~slebresne] man this was a while back but if I remember it was fairly easy to reproduce.;;;","01/Jul/13 07:26;slebresne;Ok, so any opposition to closing this now and re-opening if it turns Michael's bug hasn't been fix by CASSANDRA-5241?;;;","01/Jul/13 14:30;jbellis;SGTM.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json doesn't check SIGPIPE,CASSANDRA-5150,12627231,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,pmirski,oberman,oberman,11/Jan/13 18:56,16/Apr/19 09:32,14/Jul/23 05:53,03/May/13 08:55,2.0 beta 1,,,Legacy/Tools,,,0,lhf,,,,"I believe this explains the issue better than I can: http://stackoverflow.com/questions/11695500/how-do-i-get-java-to-exit-when-piped-to-head.

Basically, I expected that if I did: ""sstable2json SSTABLE | other-process"", and other-process had issues and/or died then the sstable2json process would die.  It doesn't.  

My workaround is using mkfifo FILE, and having sstable2json write to FILE, other-process read from FILE, and a 3rd overall process make sure the other two processes are working.  But, it would be _much_ simplier if sstable2json failed on SIGPIPE.

I looks like the fix is to periodically check System.out.checkError() in the Java.",,oberman,pmirski,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"27/Apr/13 13:25;pmirski;trunk-5150.txt;https://issues.apache.org/jira/secure/attachment/12580835/trunk-5150.txt",,,,,,,,,,,,,,,,,,,,1.0,pmirski,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,304001,,,Thu May 02 16:49:50 UTC 2013,,,,,,,,,,"0|i17hef:",251788,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"27/Apr/13 13:25;pmirski;Patch in attachment. ;;;","27/Apr/13 13:30;pmirski;In patch there is checkError every entry write, which causes flush. Maybe better idea is to call checkError once per few entries to improve performance?;;;","29/Apr/13 14:57;jbellis;Do you actually see a performance difference when you redirect to a file, for instance?  That's probably by far the most common use case.;;;","30/Apr/13 14:35;pmirski;Tested on Windows, it seems that performance drop is minimal (about 1%).;;;","02/May/13 16:49;jbellis;LGTM; committed.  Thanks, Pawel!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Respect slice count even if column expire mid-request,CASSANDRA-5149,12627182,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,slebresne,slebresne,11/Jan/13 15:25,16/Apr/19 09:32,14/Jul/23 05:53,18/Jun/13 16:16,2.0 beta 1,,,,,,0,,,,,"This is a follow-up of CASSANDRA-5099.

If a column expire just while a slice query is performed, it is possible for replicas to count said column as live but to have the coordinator seeing it as dead when building the final result. The effect that the query might return strictly less columns that the requested slice count even though there is some live columns matching the slice predicate but not returned in the result.",,aleksey,christianmovi,jjordan,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303951,,,Tue Jun 18 16:16:48 UTC 2013,,,,,,,,,,"0|i17h2v:",251736,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"11/Jan/13 15:30;slebresne;As said on CASSANDRA-5099, the only good way to fix this that I can see right now would be to have the coordinator determine an expireBefore value (the current time at the beginning of the request) and use that exclusively during the query to decide whether a query is expired or not (similar to what we do for LazilyCompactedRow but at the scale of the query).

Unfortunately, this means shipping that expireBefore value to replicas with the query and that implies a inter-node protocol change, which make this only viable for 2.0 now. Hence the 'fix version'. Of course if we can find a solution that don't require protocol change, then great.;;;","12/Feb/13 18:58;jbellis;What if we forced ExpiringColumn to either Column or DeletedColumn on the reply?;;;","13/Feb/13 09:55;slebresne;That's a good idea, I think that would work (at least I don't see why it wouldn't right away).;;;","25/Feb/13 15:44;slebresne;Actually, this is not as free of a lunch as it sound. We cannot really force non-expired ExpiringColumn to Column, because we need to return the expiring time to the client. So in practice, we would need to either add a dontExpire flag for ExpiringColumn or a special Column+TTL column type for just that purpose. Any of those will have impact on the inter-node protocol (and at the column serialization, where we don't yet handle versions well (because we never really had to)).

And if that's not significantly simpler, I think I prefer the expireBefore solution because that feels less hacky to me in that it fixes the issue of having column expire at uncontrolled times more generally (this is also how we deal with it in LazilyCompactedRow).;;;","25/Feb/13 19:05;jbellis;If it's fix-for 2.0, can we just omit returning expiration time?;;;","25/Feb/13 19:26;slebresne;I don't understand. Why would not returning the expiration time be more ok for 2.0 than 1.2? That is, even in CQL3 it's possible to query the expiration time of a column.;;;","25/Feb/13 20:04;jbellis;I have trouble coming up with a use case for it.;;;","26/Feb/13 10:48;slebresne;bq. I have trouble coming up with a use case for it.

You mean, for querying the expiration time of a column? My own experience with TTL leads me to believe that anyone using expiring columns will want to query the expiration time at one point or another. I personally had a case where if some TTLed columns were read, we were ""extending"" the TTL and how long it was extended depended on the current expiration time. Or, on the other end of the spectrum, wanting to update a column value without extending the TTL. I had also a number of case where not all columns had the same TTL, if any, and just knowing if the column was an expiring one was necessary (including but not limited to checking during development that the code was doing what it was supposed to do in terms of setting TTLs).

Also, if we were to transform expiring column to standard column on read, even ignoring the fact that we wouldn't be able to return the expiration time to clients, you'd have to be careful about read-repair ""cancelling"" your TTLs.  

Besides, as said above, especially if it's fix-for 2.0 (and so things like ""this may require a protocol change"" become largely irrelevant), I think normalizing all code on having expiration be based on a time fixed at well know places (like we do for gcBefore really) is the right long term solution.;;;","06/May/13 16:34;jjordan;Does CASSANDRA-4415 make this a non-issue (as long as it deals with this)?  To me, this might as well just be ""won't fix"", and you have to make the extra query and see if you only get one column back.  I guess there is an edge case where you could just get one column, and there really are more, but its been this way since 0.7, so meh.;;;","06/May/13 16:46;slebresne;bq. Does CASSANDRA-4415 make this a non-issue

I don't know. I think ""we'll only ever going to return less than asked columns if there is less than asked columns"" seems a reasonably semantic to have, and we fail that here with TTL. And even if CASSANDRA-4415 ends up hiding the problem, it's still a performance issue, because if you can't rely on the semantic above, you'll almost always have to do one more query than would be enough ""just to be sure"" (which is what getCount() does for thrift today).

So, without saying this is a big or pressing issue, I'm -1 on ""wont fixing it"" in the long run.;;;","28/May/13 10:14;slebresne;Actually, now that I think about it, I think CASSANDRA-4415 is why I'd really rather have this in 2.0.

Currently, because of this, when you page a slice query, you cannot trust a given page to return strictly results than you've asked only if paging is done, because you could have result expiring mid-request and thus you pretty much can never know if the paging is really done or if some columns expired on you. CASSANDRA-5099 ""solves"" this by waiting until basically a query return an empty page. However:
# this is really correct. In theory, you could have *all* of the columns fetch by the current patch that have expired mid-request, while there's still some live columns that match what your are trying to page. Granted, with a large enough page size it's very unlikely but still.
# this means you'll *always* do one more query (and that's StorageProxy level queries, it's not cheap than would be needed if this ticket was fixed. And while having paged get_count being slow don't really make me shed tears, it bugs me quite a bit more in the context of CASSANDRA-4415.
# this complicate reasoning about the logic for CASSANDRA-4415 imo. It's much easier not to have to care about ""oh, what if a column expires mid-request, is that ok?"".

Besides, I don't think fixing this is very complicated in practice. All we need is ship a 'queryServerTimestamp' with the read commands, and carry that down to the Column.isMarkedForDelete() method so it uses that instead of System.currentTimeMillis(). This might end up being a few lines of code to pass this timestamp down as parameter, but it's pretty trivial changes.;;;","09/Jun/13 23:32;aleksey;There is another method that is affected - ExpiringColumn.create() that returns either a DeletedColumn or an ExpiringColumn instance, called by ColumnSerializer.deserializeColumnBody(). Now, we already do pass expireBefore to it as a parameter (in a limited way) and it's not (was not) difficult to make it be derived from read requests' timestamp. Except in one case - row cache deserialization. The required modifications go beyond the need for Cache API change and I haven't found a good way to deal with it.
Is there any change we actually will get rid of row cache in 2.0?;;;","10/Jun/13 00:08;slebresne;Can't we create a version of ExpiringColumn.create() that never transform a DeletedColumn to an ExpiringColumn and use that for cache deserialization (or say do that when expireBefore is negative and pass -1 for expireBefore in the row cache code).

Because that behavior of ExpiringColumn.create() is really just an optimization. It would be ok to never transform expired columns to deleted ones from a correction point of view.;;;","10/Jun/13 02:06;aleksey;bq. Can't we create a version of ExpiringColumn.create() that never transform a DeletedColumn to an ExpiringColumn and use that for cache deserialization (or say do that when expireBefore is negative and pass -1 for expireBefore in the row cache code).

That is easy, actually, since you only have to special-case starting at SerializingCacheProvider.deserialize() method. But I'm afraid it's not enough. That is, special-casing it just for the row cache is not enough, must do the same for sstable deserialization (or remove the optimization entirely), or else a row serialized into the cache as a result of request (a) with timestamp Y might not be the right row for request (b) with timestamp X (< Y) coming out of order if a column expires between X and Y.

Making ExpiringColumn.create() to never return DeletedColumn instances would be the easiest way to deal with it, but what would impact compaction (and repair), so I suggest making NO expring->deleted optimization the default behavior, and only enabling it for compaction (incl. validation compaction) (and SSTableExport), or, in other words, special-case it for SSTII only.;;;","10/Jun/13 15:37;slebresne;bq. But I'm afraid it's not enough

I'm not sure I understand the case you are talking about. Whether or not ExpiringColumn.create() decides to return a deleted column or not, it still returns a column, so this should have no impact whatsoever on anything timestamp related. I.e. the code will treat an ExpiringColumn that is expired exactly as a deleted column, so it should always be safe to not transform an expired column to a DeletedColumn, even if it's only in parts of the code.

But it could be I just don't understand your example.;;;","10/Jun/13 16:18;aleksey;I wasn't clear enough. I'm talking about read request timestamps, not column timestamps. And I'm saying that to transfrom is not always correct, in the context of 5149. But *not to* transfrom is perfectly all right.;;;","10/Jun/13 17:14;slebresne;I still don't see where the problem is. Can you illustrate with a concrete example what is the problem we can run into if we just have Expiring.create() that doesn't transform in the case of the row cache but do transform based on the ""read request timestamp"" in all other cases?;;;","12/Jun/13 03:10;aleksey;https://github.com/iamaleksey/cassandra/commits/5149

The first patch adds timestamp field to all IReadCommands and makes sure it's always set appropriately.

The second patch updates the read path to respect the request timestamp. It also normalizes the argument order for CFS.getRangeSlice() and CFS.search(). This was not necessary, but the order they were in bothered me immensely. The third patch fixes the tests broken by this rearrangement.

I'll add some unit tests after the code review.;;;","14/Jun/13 13:18;slebresne;The overall approach looks good to me, though:
* I'd feel a bit better if we were just removing the isMarkedForDelete() (without arguments) call in Column, so that every caller knows he has to deal with it. It's otherwise hard to make sure we never call the no-argument version by accident. There's probably a few other places too (Row for instance) where I'd prefer avoiding having a shortcut calling System.currentTimeMillis().
* In RowIteratorFactory, I think it would make sense to use the filter timestamp rather than relying on System.currentTimeMillis() for gcBefore.
* There is possibly a few places where a function takes 'gcBefore' and 'now', either directly (collectReducedColumns), or because they have a filter and gcBefore (CFS.filterColumnFamily()). Maybe we could drop the gcBefore and just rely on the timestamp.
;;;","17/Jun/13 14:33;aleksey;Pushed the fourth commit to the same branch.

- Removed Column.isMarkedForDelete/0 and most similar methods (including QueryFilter getXFilter helper methods)
- RowIteratorFactory/System.currentTimeMillis() - done.
- CFS.filterColumnFamily() was the only place where I could get rid of passing gcBefore explicitly - thanks for catching that. In IDiskAtomFilter.collectReducedColumns() timestamp and gcbefore are sometimes unrelated (with Integer.MIN_VALUE passed as gcBefore, intentionally). Same goes for CFS.collateOnDiskAtom(). Usually it's either because of either compaction or the row cache.;;;","18/Jun/13 13:54;slebresne;bq. In IDiskAtomFilter.collectReducedColumns() timestamp and gcbefore are sometimes unrelated

Right, forgot about that.


Last version lgtm, +1.

A few optional minor nits for the commit:
* In CounterColumn.reconcile (and CounterMutation and ... in fact), we don't support expiring columns in counter tables so it's ok to just use say Long.MIN_VALUE (which a comment why).
* the comment inside DeletedColumn.isMarkedForDelete is obsolete (it's more confusing that helpful now :)).
;;;","18/Jun/13 13:58;aleksey;Thanks!

bq. In CounterColumn.reconcile (and CounterMutation and ... in fact), we don't support expiring columns in counter tables so it's ok to just use say Long.MIN_VALUE (which a comment why)

I know. Same with Column.getString() - it's overloaded by ExpiringColumn anyway. I was debating with myself what to use - Long.MIN_VALUE, 0, or just System.currentTimeMillis() where it doesn't matter, and went with System.currentTimeMillis(). Will change to Long.MIN_VALUE with a comment in both places.;;;","18/Jun/13 16:16;aleksey;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Add option to disable tcp_nodelay,CASSANDRA-5148,12627147,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,marcuse,marcuse,marcuse,11/Jan/13 09:34,16/Apr/19 09:32,14/Jul/23 05:53,11/Jan/13 22:16,1.2.1,,,,,,0,,,,,"Add option to disable TCP_NODELAY for cross-dc communication.

Reason is we are seeing huge amounts of packets being sent over our poor firewalls.

For us, disabling this for inter-dc communication increases average packet size from ~400 bytes to ~1300 bytes.",,colinkuo,marcuse,rcoli,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/13 09:35;marcuse;0001-Add-option-to-disable-TCP_NODELAY-for-inter-dc-commu.patch;https://issues.apache.org/jira/secure/attachment/12564384/0001-Add-option-to-disable-TCP_NODELAY-for-inter-dc-commu.patch",,,,,,,,,,,,,,,,,,,,1.0,marcuse,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303916,,,Fri Jan 11 22:16:14 UTC 2013,,,,,,,,,,"0|i17gv3:",251701,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"11/Jan/13 09:35;marcuse;patch for this against trunk;;;","11/Jan/13 22:16;jbellis;My initial reaction was that I'd prefer to manage the buffering explicitly instead of relying on nagle to get it right, but on further thought, I don't think it matters much: if the connection is busy, nagle will send out packets as soon as max segment size is reached; if it is not, then worst case 200-500ms delay from delayed ack is still tolerable if we are not treating cross-dc requests synchronously.

Committed defaulting to true for 1.2.1 and to false for 2.0.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair -pr hangs,CASSANDRA-5146,12627129,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,mkjellman,mkjellman,11/Jan/13 05:35,16/Apr/19 09:32,14/Jul/23 05:53,22/Feb/13 23:32,1.2.2,,,,,,0,,,,,"while running a repair -pr the repair seems to hang after getting a merkle tree

{code}
 INFO [AntiEntropySessions:9] 2013-01-10 18:23:01,652 AntiEntropyService.java (line 652) [repair #d29fd100-5b95-11e2-b9c7-dd50a26832ff] new session: will sync /10.8.25.101, /10.8.30.14 on range (28356863910078205288614550619314017620,42535295865117307932921825928971026436] for evidence.[fingerprints, messages]
 INFO [AntiEntropySessions:9] 2013-01-10 18:23:01,653 AntiEntropyService.java (line 857) [repair #d29fd100-5b95-11e2-b9c7-dd50a26832ff] requesting merkle trees for fingerprints (to [/10.8.30.14, /10.8.25.101])
 INFO [ValidationExecutor:7] 2013-01-10 18:23:01,654 ColumnFamilyStore.java (line 647) Enqueuing flush of Memtable-fingerprints@500862962(12960712/12960712 serialized/live bytes, 469 ops)
 INFO [FlushWriter:25] 2013-01-10 18:23:01,655 Memtable.java (line 424) Writing Memtable-fingerprints@500862962(12960712/12960712 serialized/live bytes, 469 ops)
 INFO [FlushWriter:25] 2013-01-10 18:23:02,058 Memtable.java (line 458) Completed flushing /data2/cassandra/evidence/fingerprints/evidence-fingerprints-ib-192-Data.db (11413718 bytes) for commitlog position ReplayPosition(segmentId=1357767160463, position=8921654)
 INFO [AntiEntropyStage:1] 2013-01-10 18:25:52,735 AntiEntropyService.java (line 214) [repair #d29fd100-5b95-11e2-b9c7-dd50a26832ff] Received merkle tree for fingerprints from /10.8.25.101
{code}",Ubuntu 12.04,anttiko,christianmovi,mkjellman,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5105,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303898,,,Fri Feb 22 23:32:11 UTC 2013,,,,,,,,,,"0|i17gr3:",251683,,,,,,,,,mkjellman,,mkjellman,Normal,,,,,,,,,,,,,,,,,,"11/Jan/13 07:28;mkjellman;node 10.8.25.101 is logging ""Received merkle tree for fingerprints from /10.8.25.101""

why would a node be receiving a merkle tree from itself?;;;","11/Jan/13 08:09;mkjellman;a working repair -pr seems to log ""requesting merkle trees for messages xx""

we should have an alarm of some type (when things are working this never takes that long in reality) in here if we don't get to this code block log that the repair failed...;;;","16/Jan/13 17:17;slebresne;bq. why would a node be receiving a merkle tree from itself?

Because we didn't bothered ""specializing"" the local case. So the repair coordinator sends itself a merkle tree request as any other node and later sends itself the tree oblivious to the fact it is itself the coordinator of said repair. It's definitively not very efficient, as we serialize/deserialize the tree uselessly, but on the other side it's not in any performance critical path. If it bugs your OCD (no judgment, we're an OCD-friendly project), feel free to open a ticket for the improvement (and maybe giving a shot at a patch?).

For the hanging problem, all the log above is saying is that 10.8.30.14 hasn't responded with his merkle tree. Maybe check the log of said node around the time the tree request was sent and check if something jumps out. Is that something you can reproduce? If it is, maybe reproducing with DEBUG logging on might shed some light.;;;","14/Feb/13 05:49;mkjellman;fixed with the patch in 5105;;;","15/Feb/13 18:23;mkjellman;[~slebresne] [~yukim] don't kill me, but i can still reproduce this with 5105.. :(;;;","20/Feb/13 05:07;slebresne;You will have to help us here, because as I said earlier the line of logs above doesn't really says much: Do you have steps to reproduce? Are you sure it really hangs and it's not just slow? Would you have the logs from all the nodes when a hanging happen maybe?;;;","20/Feb/13 05:14;mkjellman;positive it hangs, and yes i agree we need more to debug here.

i took a stack trace on the nodes that started the repair and there were no deadlocks.

In one case however, the repair triggered another node to stream for the repair session so maybe that's a place to look.

Nothing interesting/abnormal logged with TRACE logging on .streaming or cassandra.db and a few other classes I tried;;;","20/Feb/13 05:19;mkjellman;steps to repro is literally to run a 'nodetool repair -pr' on a node with > 300GB of data.

Other users on the users mailing list reporting the same behavior:
http://www.mail-archive.com/user@cassandra.apache.org/msg27891.html;;;","20/Feb/13 22:35;yukim;From the code, I see two points where repair can hang.

- Merkle tree calculation failure
- Data streaming failure

Those have been addressed in CASSANDRA-3112, so let's revisit that issue.
For point one, we need to change message to report back failure, so we have to do that in next major release.
But point two more likely to happen than point one, fix that alone should be in minor release.;;;","20/Feb/13 22:42;mkjellman;I agree that the vast majority of repair issues are actually caused by a streaming failures which is why i thought 5105 would fix it.

i'm thinking that #5151 might have created another exception which once again appeared to break repairs. After reverting 5151 and keeping the other fixes repair seems much more stable.;;;","22/Feb/13 23:32;mkjellman;so, after testing on 15 nodes, i think #5105 did actually fix the issue here but #5151 reintroduced the ""hangs"". with #5151 reverted we are good to go.

since we already have a feature request for 2.0 to improve reporting of repair tasks i think we can close this as fixed in 1.2.2;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL3 BATCH authorization caching bug,CASSANDRA-5145,12627102,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,aleksey,aleksey,aleksey,11/Jan/13 02:04,16/Apr/19 09:32,14/Jul/23 05:53,11/Jan/13 16:43,1.1.9,1.2.1,,,,,0,,,,,"cql3.BatchStatement:

{noformat}
    public void checkAccess(ClientState state) throws InvalidRequestException
    {
        Set<String> cfamsSeen = new HashSet<String>();
        for (ModificationStatement statement : statements)
        {
            // Avoid unnecessary authorizations.
            if (!(cfamsSeen.contains(statement.columnFamily())))
            {
                state.hasColumnFamilyAccess(statement.keyspace(), statement.columnFamily(), Permission.MODIFY);
                cfamsSeen.add(statement.columnFamily());
            }
        }
    }
{noformat}

In CQL3 we can use fully-qualified name of the cf and so a batch can contain mutations for different keyspaces. And when caching cfamsSeen, we ignore the keyspace. This can be exploited to modify any CF in any keyspace so long as the malicious user has CREATE+MODIFY permissions on some keyspace (any keyspace). All you need is to create a table in your ks with the same name as the table you want to modify and perform a batch update.

Example: an attacker doesn't have permissions, but wants to modify k1.demo table. The attacker controls k2 keyspace. The attacker creates k2.demo table and then does the following request:

{noformat}
cqlsh:k2> begin batch
      ... insert into k2.demo ..
      ... insert into k1.demo ..
      ... apply batch;
cqlsh:k2>
{noformat}

.. and successfully modifies k1.demo table since 'demo' cfname will be cached.

Thrift's batch_mutate and atomic_batch_mutate are not affected since the only allow mutations to a single ks. CQL2 batches are not affected since they don't do any caching.

We should either get rid of caching here or switch cfamsSeen to a Map<String, Set<String>>.

Personally, I'd rather do the latter now, and get rid of caching here completely once CASSANDRA-4295 is resolved. ",,aleksey,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/13 16:09;aleksey;5145.txt;https://issues.apache.org/jira/secure/attachment/12564432/5145.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303860,,,Fri Jan 11 16:43:39 UTC 2013,,,,,,,,,,"0|i17g3r:",251578,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"11/Jan/13 15:04;slebresne;bq. I'd rather do the latter now

I'm good with that, though I would not even bother with a map of sets, but just add {{statement.keyspace() + "":"" + statement.columnFamily()}} to cfamsSeen.;;;","11/Jan/13 15:23;aleksey;Thought about that, but if we later allow ':' in ks/cf names, for example, this would bite us again, since there would be now way to distinguish between (ks: ""ks:1"", cf: ""demo"") and (ks: ""ks"", cf: ""1:demo"") and a similar attack would happen.

Now, this may not be a valid concern, but I'd rather not risk by depending on cql grammar here.;;;","11/Jan/13 15:38;slebresne;I estimate the change of allowing ':' in ks/cf names before CASSANDRA-4295 to 0 :). In fact I kind of doubt we'll ever allow it (there's no real use and there's no point in potentially screwing up tools that rely on those name not containing ':'). But really, I'm perfectly fine with a map of sets.;;;","11/Jan/13 16:29;slebresne;+1 (nit: we can alias the Set to avoid 2 get()) ;;;","11/Jan/13 16:43;aleksey;Thanks, committed (with the nit taken care of).;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Validate login for Thrift describe_keyspace, describe_keyspaces and set_keyspace methods",CASSANDRA-5144,12627043,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,aleksey,aleksey,10/Jan/13 21:47,16/Apr/19 09:32,14/Jul/23 05:53,11/Jan/13 00:06,1.2.1,,,,,,0,,,,,"Not validating login leaks info about keyspaces and columnfamilies if the configured authenticator requires validation.

This change does not affect AllowAllAuthenticator, but if an implementation forbids anonymous access, we should deny this information to unauthenticated users.",,aleksey,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/13 21:48;aleksey;5144.txt;https://issues.apache.org/jira/secure/attachment/12564274/5144.txt",,,,,,,,,,,,,,,,,,,,1.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303736,,,Fri Jan 11 00:06:52 UTC 2013,,,,,,,,,,"0|i17e4v:",251259,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"10/Jan/13 23:43;jbellis;Thrift has AuthenticationException and AuthorizationException instead of IRE.  Otherwise +1;;;","11/Jan/13 00:06;aleksey;Can't use Thrift {noformat}{Authentication,Authorization}Exception{noformat} without breaking thrift interface.

Committed with changes: removed the conversion method from ThriftConversion.

Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Can not insert an empty map.,CASSANDRA-5141,12626959,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,krzysztof cieslinski,krzysztof cieslinski,10/Jan/13 15:29,16/Apr/19 09:32,14/Jul/23 05:53,11/Jan/13 17:01,1.2.1,,,,,,0,,,,,"It is not possible to insert an empty map. It looks like the ""{}"" is reserved only for Set.

So when for table:

{code}
CREATE TABLE users (
    id text PRIMARY KEY,
    surname text,
    favs map<text, text>
)
{code}

I try to insert map without any elements:

{code}
cqlsh:test> insert into users(id,surname,favs) values('aaa','aaa',{});
{code}

I get:

{code}
 Bad Request: Set operations are only supported on Set typed columns, but org.apache.cassandra.db.marshal.MapType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type) given.
text could not be lexed at line 1, char 63
{code}
",,dbrosius,krzysztof cieslinski,marcinszymaniuk,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"11/Jan/13 14:46;slebresne;5141.txt;https://issues.apache.org/jira/secure/attachment/12564410/5141.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303651,,,Thu Feb 07 15:22:10 UTC 2013,,,,,,,,,,"0|i17dhb:",251153,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"11/Jan/13 07:40;dbrosius;cql.g's 

set_literal and map_literal can't differentiate {} without outside knowledge, and so just choose set.;;;","11/Jan/13 14:46;slebresne;Yes, the parser can't distinguish between empty set and empty map, so it always pick empty set and delegate the real choice to when we have type information. Now there used to be code that was handling that in UpdateStatement but it seems to have gone away (haven't found when but haven't look very hard).

Anyway, attaching code that adds back the code to handle that.;;;","11/Jan/13 16:35;jbellis;s/differenciate/differentiate/

otherwise +1 :);;;","11/Jan/13 17:01;slebresne;bq. s/differenciate/differentiate/

Damn, my cover is blown. Committed with that fixed, thanks;;;","07/Feb/13 09:23;marcinszymaniuk;Its letting me to execute the insert with an empty mup but that doesnt really inserting the map. Example:
{code:sql}insert into users(id,surname,favs) values('key123','justAString',{});
select * from users where id='key123';
{code}
{code}id     | favs | surname
--------+------+-------------
key123 | null | justAString
{code}
The question is- is this what you aggreed for? Its a bit misleading for me - inserting an empty map means that it has some semantic but I loose it. 

The MapOperation.doPut method doesnt seem to care about empty map at all - its just iterating through a map. If its empty it does nothing.;;;","07/Feb/13 09:45;slebresne;In Cassandra, an empty map/set/list is equivalent to the column being null. In other words a collection only exists if it has elements. The reason for that semantic (versus having null being different from empty) is honestly mainly dictated by implementation concerns (internally a collection *does not* indeed exist unless it has elements). I.e. it's not because we think it is intrinsically better that way, but it is also true that, at least as far as I'm concern, I don't think it is intrinsically worst that way either.;;;","07/Feb/13 15:22;marcinszymaniuk;gotcha, thanks Sylvain;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Drop keyspace argument from forceUserDefinedCompactions,CASSANDRA-5139,12626858,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,jbellis,jbellis,09/Jan/13 21:52,16/Apr/19 09:32,14/Jul/23 05:53,14/Feb/13 03:28,2.0 beta 1,,,,,,0,jmx,,,,Redundant now that keyspace is encoded in filename.,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"06/Feb/13 05:24;yukim;5139-2.0.txt;https://issues.apache.org/jira/secure/attachment/12568166/5139-2.0.txt","15/Jan/13 18:48;yukim;5139.txt;https://issues.apache.org/jira/secure/attachment/12564970/5139.txt",,,,,,,,,,,,,,,,,,,2.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303548,,,Thu Feb 14 03:28:48 UTC 2013,,,,,,,,,,"0|i17c9z:",250957,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"15/Jan/13 18:48;yukim;Patch attached for new user defined compaction method. New method's behavior is slightly different from existing one, in the way that new method accepts mixed ks/cf files.
I'm not sure if it is OK to drop current API so I just deprecate it and create new method with one arg. On the other hand, I feel having two different methods with different args is confusing though.;;;","15/Jan/13 18:52;jbellis;I guess we should keep the API stable for 1.2.  Retargeting for 2.0 so we can rip out the old one.  I think nodetool will need an update?;;;","15/Jan/13 18:57;yukim;OK. I will update the patch for 2.0.
And I don't think nodetool has command for user defined compaction.;;;","06/Feb/13 05:24;yukim;Attaching patch for 2.0.;;;","12/Feb/13 22:24;jbellis;+1;;;","14/Feb/13 03:28;yukim;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Provide a better CQL error when table data does not conform to CQL metadata.,CASSANDRA-5138,12626841,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,boneill,boneill,09/Jan/13 20:26,16/Apr/19 09:32,14/Jul/23 05:53,24/Jul/13 15:04,2.0.0,,,,,,0,,,,,"When you create a table via CQL, then insert into it via Thrift.  If you inadvertently leave out a component of the column name, in CQL you receive a:
TSocket read 0 bytes

Server-side the following exception is logged:
ERROR 15:19:18,016 Error occurred during processing of message.
java.lang.ArrayIndexOutOfBoundsException: 3
	at org.apache.cassandra.cql3.statements.ColumnGroupMap.add(ColumnGroupMap.java:43)
	at org.apache.cassandra.cql3.statements.ColumnGroupMap.access$200(ColumnGroupMap.java:31)
	at org.apache.cassandra.cql3.statements.ColumnGroupMap$Builder.add(ColumnGroupMap.java:138)
	at org.apache.cassandra.cql3.statements.SelectStatement.process(SelectStatement.java:805)
	at org.apache.cassandra.cql3.statements.SelectStatement.processResults(SelectStatement.java:145)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:134)
	at org.apache.cassandra.cql3.statements.SelectStatement.execute(SelectStatement.java:61)
	at org.apache.cassandra.cql3.QueryProcessor.processStatement(QueryProcessor.java:132)
	at org.apache.cassandra.cql3.QueryProcessor.process(QueryProcessor.java:140)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql3_query(CassandraServer.java:1686)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4074)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql3_query.getResult(Cassandra.java:4062)
	at org.apache.thrift.ProcessFunction.process(ProcessFunction.java:32)
	at org.apache.thrift.TBaseProcessor.process(TBaseProcessor.java:34)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:199)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)

I'll submit a schema, and steps to reproduce.",Mac OS X running 1.2,boneill,elprans,liqusha,mauzhang,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"23/Jul/13 08:21;slebresne;5138-2.txt;https://issues.apache.org/jira/secure/attachment/12593656/5138-2.txt","22/Jul/13 14:51;slebresne;5138.txt;https://issues.apache.org/jira/secure/attachment/12593519/5138.txt","09/Jan/13 20:27;boneill;northpole.cql;https://issues.apache.org/jira/secure/attachment/12564030/northpole.cql",,,,,,,,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303532,,,Mon Sep 23 12:37:26 UTC 2013,,,,,,,,,,"0|i17c67:",250940,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"09/Jan/13 20:27;boneill;You can use this schema to reproduce the issue.  ;;;","09/Jan/13 20:29;boneill;Fire up cassandra-cli, and execute the following:
[default@unknown] use northpole;
Authenticated to keyspace: northpole
[default@northpole] set naughtyornicelist['naughty:USA']['PA:18964:michael.myers']='00';
Value inserted.
Elapsed time: 36 msec(s).

Then, go back to CQL and execute the following:
cqlsh> use northpole;
cqlsh:northpole> select * from naughtyornicelist ;
TSocket read 0 bytes
;;;","10/Jan/13 17:21;slebresne;I don't think it is reasonable to have CQL validate on every read that no bad data has been added to non compact table through thrift: outside of not being particularly simple to implement and error prone, it would be silly performance wise.

The fact is, *non* compact CQL3 tables are a CQL3 things and in theory they should not be accessible at all through thrift (exactly because thrift has the potential of screwing things up). But I do understand the willingness to use thrift to look under the cover to see how things are implemented and that definitively have educational value. Which imo leaves us with the following options:
# do nothing and be clear that 'you shalt not mess up with non compact CQL3 table through thrift' and that if you do, things may break in unexpected ways.
# add validation on the thrift write path.
# refuse write access to non compact CQL3 tables from thrift by default with an option to deactivate that protection ""at your own risk"".

In an ideal world 2) would be the best solution. However, adding said validation is not 2 lines of code, far from it, and there would be the maintenance cost of keeping said validation up to date with the evolutions of CQL3. So, and especially given that accessing non compact CQL3 table from thrift is difficult anyway due to not all metadata being exposed, I'm not convinced that adding said validation would be the best use of our developer resources. Of course if someone is willing to step up to write and maintain that validation code, that's fine, but in the absence of that I would suggest 3) as a good enough compromise.
;;;","10/Jan/13 17:42;jbellis;I thought #2 (validate on insert) was the plan for CASSANDRA-4377.

Unless it's really hairy I do think we should deliver on that.  Essentially nobody is going to be able to compose cql-inserts-from-thrift safely otherwise, and as crazy as it is people seem to want to do it.  If it makes people feel better about upgrading then I'm in favor.;;;","10/Jan/13 18:16;slebresne;bq. I thought #2 (validate on insert) was the plan for CASSANDRA-4377.

CASSANDRA-4377 stopped at ""make sure well formed thrift query do not crash"".

Making sure you don't do something that would break CQL3 require a bit more. For instance, in theory we should refuse a query that adds a column in a CQL row that doesn't exist in the metadata (it could be that doing so doesn't really ""break"" anything but honestly I have no clue if that's the case in practice). Anyway, maybe doing some basic validation is not too hard. But I'm afraid it will be easy to forget validating something that breaks CQL3 in subtle ways. So ok for adding validation on a best effort basis, but I think we should still be clear that this is not encouraged and you have to do it at your own risk.

;;;","11/Jan/13 03:27;boneill;Just to be clear, I'd be happy with a more informative message. (translate the ArrayOutOfBounds, to ""You may not have enough components in your column name."")

To elaborate on the craziness that is accessing tables from CQL that are populated by thrift...
We want to enable our ""data heads"" with cqlsh.  They are very excited about it because it looks like SQL, and to date Cassandra has been inaccessible to them.  (These are data gurus that are wizards at a SQL prompt, but do not have server-side access to Cassandra)  We have existing (and new) applications that use thrift to write (via Astyanax composite/compound columns using annotated classes).  We don't want to change our application development paradigm (yet), but we want cqlsh access to the data.  In that paradigm, we have encountered this error during development, especially if/when you get the translation wrong between a CQL schema and the thrift interpretation. (which isn't hard to do);;;","22/Jul/13 14:51;slebresne;Attaching relatively simple patch that simply assert a column name has the right number of components when inserting in a CQL3 table.

The patch does not validate that the CQL3 column inserted exists however. We could do it I suppose, but inserting such cell shouldn't really corrupt the data, the cell should simply be ignored by a select, so I figured it's probably not worth going further for thrift.;;;","22/Jul/13 15:48;jbellis;IMO we should do that extra check (also type information, unless that's already taken care of by the other validation).  If you really wanted unchecked composite inserts, you'd presumably create the table from Thrift.  So here we should assume that the user is doing his best to create a valid CQL row and we should reject invalid ones.;;;","22/Jul/13 15:59;slebresne;bq. IMO we should do that extra check

To complete the reason why I haven't add it, this is mostly because of DROP (CASSANDRA-3919). So in practice, people may have cell that don't correspond to a (currently) defined CQL3 column name, even if it's only temporary. For that, it could seem fair game to at least let user delete those from thrift. But I mean, I don't really care, I'm fine doing that extra check if you still think it's better, just wanted to present the full reasoning.

bq. also type information, unless that's already taken care of by the other validation

Thrift already does validation of the cell name and cell value (as well as partition key). So unless you were thinking of something else, we should be good.;;;","22/Jul/13 17:07;jbellis;I don't think I follow -- when we implement DROP it will effectively delete columns without the user explicitly having to do so.  If you're worried about concurrency, I think your reasoning about ""Don't Do That"" from the keyspaces ticket applies.;;;","23/Jul/13 08:21;slebresne;I guess all I meant is that since people tends to get touchy when you limit the thrift interface, doing the minimum validation so as not to crash CQL could be an option. Anyway, doesn't matter, attaching v2 patch that does the extra check.
;;;","24/Jul/13 14:24;jbellis;+1;;;","24/Jul/13 14:24;jbellis;... although we might want to make this 2.0-only to be on the safe side.;;;","24/Jul/13 15:04;slebresne;Agreed, committed to trunk only. Thanks;;;","16/Aug/13 19:40;elprans;Guys, this completely broke writing to CQL3 tables from Thrift for me.

The name check is done against CFDefinition.columns, which seems to be populated with clustering key columns only.  Thus, it is not possible to write into columns that are outside the PRIMARY KEY.  Is this intentional?;;;","19/Aug/13 09:20;slebresne;It is not allowed to insert cells (thrift columns if you will) that do not correspond to a declared CQL3 column no. But that's something that would qualify as wrong from a CQL3 point of view anyway so that does is intentional. Now from your description, it's hard to say if you were doing it wrong or if the added validation is indeed too restrictive, but at least looking quickly at the patch again I don't see anything wrong.

Maybe can you give a simple example of what you're trying to do and doesn't work (the CQL3 schema of the CF and and example of insertion that doesn't work with cassandra-cli for instance)? ;;;","19/Aug/13 14:55;elprans;I understand that columns not declared in CQL3 are supposed to fail validation.  The problem is that even _defined_ columns in non-compact tables fail now.  Something as simple as

CREATE TABLE test (
    id     text,
    attr   text,
    value  text,
 
    PRIMARY KEY (id, attr)
);

It is no longer possible to write into ""value"" from Thrift, because it's not a part of the clustering key, which is apparently what CFMetadata.columns is populated from and against what the check is done.;;;","21/Aug/13 07:04;slebresne;You're right, there was a typo in the initial patch, we were checking against the clustering keys at a place where we were suppose to check the non-PK columns. Anyway, I took the liberty to commit the trivial fix to 7a300c2 (and checked insertion in a table like the one above does work as expected).;;;","19/Sep/13 16:54;elprans;Another related issue.  This check seems to make it impossible to insert proper records into CQL3 tables via Thrift, since it bails out on an attempt to write a column with an empty last component.

{code:sql} 
CREATE TABLE test (
    id text,
    attr text,
    value text,

    PRIMARY KEY (id, attr)
);
{code}

The storage column structure for the table would be:

|<key>|{<attr>,}|[empty]
|<key>|{<attr>,""value""}|<value>

There is no way to insert the first column (record demarcation) as it fails with ""Invalid cell for CQL3 table test. The CQL3 column component () does not correspond to a defined CQL3 column""
;;;","19/Sep/13 17:48;elprans;I think the best fix would be to make CassandraServer insert row markers into CQL3 tables the same way CQL modification statements do.;;;","23/Sep/13 12:37;slebresne;You're right, but since this has been closed and has shipped already, opened CASSANDRA-6081 to fix letting row markers pass validation.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Make sure SSTables left over from compaction get deleted and logged,CASSANDRA-5137,12626804,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,yukim,yukim,09/Jan/13 17:31,16/Apr/19 09:32,14/Jul/23 05:53,11/Jan/13 19:18,1.1.9,1.2.1,,,,,0,,,,,"When opening ColumnFamily, cassandra checks SSTable files' ancestors and skips loading already compacted ones. Those files are expected to be deleted, but currently that never happens.
Also, there is no indication of skipping loading file in the log, so it is confusing especially doing upgradesstables.",,christianmovi,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5116,,,,,,,,,,,"10/Jan/13 21:49;yukim;5137-1.1-v2.txt;https://issues.apache.org/jira/secure/attachment/12564275/5137-1.1-v2.txt","11/Jan/13 17:38;yukim;5137-1.1-v3.txt;https://issues.apache.org/jira/secure/attachment/12564451/5137-1.1-v3.txt","09/Jan/13 17:32;yukim;5137-1.1.txt;https://issues.apache.org/jira/secure/attachment/12563965/5137-1.1.txt",,,,,,,,,,,,,,,,,,3.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303422,,,Fri Jan 11 19:18:16 UTC 2013,,,,,,,,,,"0|i17b2v:",250763,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"09/Jan/13 17:32;yukim;We need to mark skipping SSTable as compacted to be removed.;;;","09/Jan/13 17:58;jbellis;Hmm.

This patch is correct as far as it goes but I think the existing assumption is broken: that if we have any sstable with ancestor X, then X is safe to delete.

Specifically, LCS will create multiple sstables from a given set of ancestors, so unless we know that we finished the compaction (and finished writing all the resulting descendant sstables), we could lose data if we delete the ancestors themselves.

One possible fix:

# Add a flag to SSTM for ""this was the final sstable in the compaction""
# When we scan sstables, we can delete ancestors if we find that marker in any of the descendants
# Otherwise, we should delete the *descendants* and leave the ancestors alone (so we don't doublecount data for counters);;;","09/Jan/13 18:13;slebresne;I think you're right.

A fourth pseudo-solution could be to wait the end of the compaction to rename all the newly created writers (i.e remove the tmp markers). It's not bulletproof though as I don't think we can rename multiple files atomically but just wanted to mention it.

Maybe at least for 1.1, 3. is the best/simplest option. On the longer term, maybe 1. is better.;;;","09/Jan/13 18:33;jbellis;You're right, you don't actually need a marker since if compaction completes the next step is to remove the ancestors.  I think ""if ancestors are still alive, assume compaction didn't finish and delete the descendants"" is good enough.;;;","09/Jan/13 18:56;slebresne;bq. I think ""if ancestors are still alive, assume compaction didn't finish and delete the descendants"" is good enough.

yeah agreed.;;;","09/Jan/13 19:01;slebresne;Hum wait, it only works if we have all ancestors though. What if just one ancestor don't get deleted for some reason (or only some of the SSTableDeletingTask have executed before a crash)? It's easy enough to check that we have *all* ancestors, but what if we don't? We're back to square one :(;;;","09/Jan/13 20:13;jbellis;You're right.  Guess we need a compaction-finished flag after all.

Instead of storing it in sstable metadata, maybe we could store it in system.local the way we do with truncation information.  Unfortunately 1.1 doesn't support Maps so we'd be doing two separate implementations for 1.1 and 1.2.

Should we just say that for 1.1 we'll retain all sstables (counter users will get overcounts, everyone else just gets extra compaction work) and fix it better in 1.2?;;;","09/Jan/13 20:16;jbellis;Something like this...

{code}
create table compaction_log (
  id uuid primary key,
  inputs set<int>,
  outputs set<int>
);
{code}

When we start a compaction, we add it to the log.  When we finish, we remove it.  If we restart and compaction_log is not empty, we remove any sstables from the outputs set.;;;","10/Jan/13 08:57;slebresne;bq. Should we just say that for 1.1 we'll retain all sstables

For 1.1, I'd suggest doing my fourth pseudo-solution above, i.e. moving the renaming of newly created writes at the end of the compaction (it's trivial). Then at startup, we could indeed retain all sstables for normal CF, but for counter we would keep removing the predecessors as we do now. It wouldn't totally fix the risk of losing counters, but it would make it very unlikely (you'd need to fail exactly in the middle of the bulk renaming a newly create sstable writers), while just retaining all sstables would make it very easy to have overcounts.

For 1.2, you compaction_log solution does seem reasonable.;;;","10/Jan/13 21:49;yukim;V2 implements Sylvain's idea that renames written SSTables at the end of compaction.

For 1.2, let's open different issue for Jonathan's suggestion.;;;","11/Jan/13 15:17;slebresne;The code of v2 looks alright, but let's also disable the filtering in ColumnFamilyStore.ctor for non-counter CFs so we take zero chance of losing data (and since reusing an already compacted sstable is a bit inefficient but harmless).

bq. For 1.2, let's open different issue for Jonathan's suggestion

Agreed.;;;","11/Jan/13 17:38;yukim;Attached v3 that also changes the filtering part only for counter CF.;;;","11/Jan/13 17:47;slebresne;+1 (though do commit your v1 along the way, no way in keeping sstable we're not going to use, even if it's only for counters).;;;","11/Jan/13 19:18;yukim;Committed v1 + v3, and opened CASSANDRA-5151 for better solution.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair -pr triggers an Assertion,CASSANDRA-5136,12626795,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,carlyeks,mkjellman,mkjellman,09/Jan/13 16:51,16/Apr/19 09:32,14/Jul/23 05:53,09/Jan/13 21:59,1.2.1,,,,,,0,compaction,,,,"{code}
ERROR [ValidationExecutor:1] 2013-01-09 08:47:29,232 CassandraDaemon.java (line 133) Exception in thread Thread[ValidationExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.PrecompactedRow.update(PrecompactedRow.java:149)
        at org.apache.cassandra.service.AntiEntropyService$Validator.rowHash(AntiEntropyService.java:375)
        at org.apache.cassandra.service.AntiEntropyService$Validator.add(AntiEntropyService.java:367)
        at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:780)
        at org.apache.cassandra.db.compaction.CompactionManager.access$700(CompactionManager.java:75)
        at org.apache.cassandra.db.compaction.CompactionManager$8.call(CompactionManager.java:495)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
{code}",Ubuntu 12.04,carlyeks,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5077,,,,,,,,,,,"09/Jan/13 19:21;carlyeks;5136-repair-assertion.patch;https://issues.apache.org/jira/secure/attachment/12563999/5136-repair-assertion.patch",,,,,,,,,,,,,,,,,,,,1.0,carlyeks,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303413,,,Wed Jan 09 21:59:04 UTC 2013,,,,,,,,,,"0|i17b0v:",250754,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"09/Jan/13 16:56;mkjellman;assert compactedCf != null;;;;","09/Jan/13 19:24;carlyeks;It appears that the row is empty, but that doesn't get checked when doing a validation. I've added a check, as well as closing the row if it is empty (as this was the cause of CASSANDRA-4492).;;;","09/Jan/13 21:46;mkjellman;+1 on the patch and also verified this fixes the bug;;;","09/Jan/13 21:59;jbellis;committed (and removed the not-null filter too);;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
sstable2json always returns default value validator,CASSANDRA-5134,12626727,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,cangeli,cangeli,09/Jan/13 09:12,16/Apr/19 09:32,14/Jul/23 05:53,09/Jan/13 12:53,1.2.1,,,Legacy/Tools,,,0,,,,,"When exporting to JSON tables created using cqlsh, values are always exported to hexa : the serializeColumn function fails to get the correct value validator from the cfMetaData, since getColumnDefinition(column) returns null.

I'm not a java expert and doesn't really understand the use of ByteBuffer for Map : the workaround I found is to pass to cfMetaData.getValueValidator a new wrapped ByteBuffer of the column.name argument, then the validator is correctly found from the cfMetaData.column_metadata.",,cangeli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/13 11:16;slebresne;5134.txt;https://issues.apache.org/jira/secure/attachment/12563928/5134.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303335,,,Wed Jan 09 12:53:33 UTC 2013,,,,,,,,,,"0|i179if:",250509,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"09/Jan/13 11:16;slebresne;Attaching patch to fix. We weren't really using the right method post-CQL3.;;;","09/Jan/13 12:04;brandon.williams;+1;;;","09/Jan/13 12:53;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Nodes can't rejoin after stopping, when using GossipingPropertyFileSnitch",CASSANDRA-5133,12626666,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,exabytes18,exabytes18,08/Jan/13 23:10,16/Apr/19 09:32,14/Jul/23 05:53,16/Jan/13 18:29,1.2.1,,,,,,0,,,,,"I can establish a 1.2 ring with GossipingPropertyFileSnitch, but after killing a node and restarting it, the node cannot rejoin.

[Node 1] ./bin/cassandra -f
[Node 2] ./bin/cassandra -f
[Node 3] ./bin/cassandra -f

[Node 1] ./bin/nodetool ring
 ... ok ...

[Node 1] ^C
 ... node shutdown ...

[Node 1] ./bin/cassandra -f
 ... Exception! ...



ERROR 05:45:39,305 Exception encountered during startup
java.lang.RuntimeException: Could not retrieve DC for /10.114.18.51 from gossip and PFS compatibility is disabled
  at org.apache.cassandra.locator.GossipingPropertyFileSnitch.getDatacenter(GossipingPropertyFileSnitch.java:109)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:127)
	at org.apache.cassandra.locator.TokenMetadata$Topology.addEndpoint(TokenMetadata.java:1040)
	at org.apache.cassandra.locator.TokenMetadata.updateNormalTokens(TokenMetadata.java:185)
	at org.apache.cassandra.locator.TokenMetadata.updateNormalTokens(TokenMetadata.java:157)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:441)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:397)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:309)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:397)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:440)
java.lang.RuntimeException: Could not retrieve DC for /10.114.18.51 from gossip and PFS compatibility is disabled
	at org.apache.cassandra.locator.GossipingPropertyFileSnitch.getDatacenter(GossipingPropertyFileSnitch.java:109)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:127)
	at org.apache.cassandra.locator.TokenMetadata$Topology.addEndpoint(TokenMetadata.java:1040)
	at org.apache.cassandra.locator.TokenMetadata.updateNormalTokens(TokenMetadata.java:185)
	at org.apache.cassandra.locator.TokenMetadata.updateNormalTokens(TokenMetadata.java:157)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:441)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:397)
	at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:309)
	at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:397)
	at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:440)



Full environment + exceptions + stacktraces: https://gist.github.com/1e74ff02c2d4f622ce8f ","3 ec2 instances (CentOS 6.3; java 1.7.0_05; Cassandra 1.2)",exabytes18,tscanausa,vijay2win@yahoo.com,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"16/Jan/13 17:26;brandon.williams;5133-2.txt;https://issues.apache.org/jira/secure/attachment/12565147/5133-2.txt","15/Jan/13 23:25;brandon.williams;5133.txt;https://issues.apache.org/jira/secure/attachment/12565037/5133.txt",,,,,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303273,,,Wed Jan 16 18:29:33 UTC 2013,,,,,,,,,,"0|i1792f:",250437,,,,,,,,,vijay2win@yahoo.com,,vijay2win@yahoo.com,Normal,,,,,,,,,,,,,,,,,,"09/Jan/13 03:23;tscanausa;While trying to help Matt I can verify, that GossipPropertySnitch does not seem to work as expected.

Below is a detailed explanation of what happened to me while working to help Matt,

3 nodes A, B, C.

setup:
Node A -> seeds=""node A ip address""
Node B -> seeds=""node A ip address""
Node C -> seeds=""node A ip address"" 

1) start node A and let it fully start up. ( first node needs to be a seed of itself to start )
2) start node B and let it fully join the ring.
3) start node C and let it fully join the ring.
4) nodetool ring shows all nodes up
5) stop node C and update the cassandra-rackdc.protperites file to rack=2
6) start node C ( fails to start error about cant find DC for node A, maybe GossipPropertySnitch wont let a seed node talk about itself? )
7) stop node A and update seeds=""node B ip address"", to try and solve question above
8) start node A ( fails to start cant find DC of node C )
9) stuck not being able to start node A and C;;;","09/Jan/13 03:43;brandon.williams;Caused by CASSANDRA-3881, since before then we didn't need to know the dc/rack for saved endpoints at startup.  A workaround is to disable loading the persisted ring via -Dcassandra.load_ring_state=false;;;","11/Jan/13 21:06;exabytes18;Is there any risk to specifying cassandra.load_ring_state=false? In what version will this be resolved?;;;","11/Jan/13 21:07;brandon.williams;The main risk is if it is used as a coordinator when it starts up before it discovers the rest of the ring, it will think it owns all writes, even though it doesn't.;;;","15/Jan/13 20:29;jbellis;I don't think I understand the problem -- why can't we use rack + dc from system.peers?;;;","15/Jan/13 22:07;brandon.williams;I totally missed that we were already saving that info.  Patch to have GPFS load it, but only use it outside of compat mode, since with compat mode you've made a mistake if PFS can't load it either.;;;","16/Jan/13 00:35;vijay2win@yahoo.com;+1;;;","16/Jan/13 00:43;brandon.williams;Committed.;;;","16/Jan/13 03:01;brandon.williams;Still some kind of race on initial startup:

{noformat}
ERROR 02:58:45,376 Exception in thread Thread[WRITE-cassandra-1/10.179.65.102,5,main]
java.lang.RuntimeException: Could not retrieve DC for cassandra-1/10.179.65.102 from gossip and PFS compatibility is disabled
        at org.apache.cassandra.locator.GossipingPropertyFileSnitch.getDatacenter(GossipingPropertyFileSnitch.java:80)
        at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:127)
        at org.apache.cassandra.net.OutboundTcpConnection.isLocalDC(OutboundTcpConnection.java:73)
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:266)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:138)
{noformat};;;","16/Jan/13 03:06;brandon.williams;The problem in this case is cassandra-1 is the seed and when this node starts up it knows about it via being listed in the yaml as seed, but doesn't have dc info that OTC wants when connecting.;;;","16/Jan/13 03:11;vijay2win@yahoo.com;How about, instead of throwing an exception use something like EC2Snitch.DEFAULT_DC? 
I think we can live with this instead of doing the system table lookup too.;;;","16/Jan/13 17:26;brandon.williams;Patch on top of what's already committed to go the default rack/dc route.  I think it's better to keep the system table lookup since that's going to be correct 99% of the time, where the default only needs to be returned in the corner case of contacting a seed the very first time.;;;","16/Jan/13 18:25;vijay2win@yahoo.com;+1;;;","16/Jan/13 18:29;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unfriendly error message during create table map collection,CASSANDRA-5132,12626651,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,khahn,khahn,08/Jan/13 21:34,16/Apr/19 09:32,14/Jul/23 05:53,09/Jan/13 11:06,1.2.1,,,,,,0,,,,,"cqlsh:music> create table test (id uuid PRIMARY KEY, testmap map<timestamp, nonsense>);
Bad Request: Failed parsing statement: [create table test (id uuid PRIMARY KEY, testmap map<timestamp, nonsense>);] reason: NullPointerException null","[cqlsh 2.3.0 | Cassandra 1.2.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.35.0]
",khahn,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303258,,,Wed Jan 09 11:06:57 UTC 2013,,,,,,,,,,"0|i178z3:",250422,,,,,,,,,,,,Low,,,,,,,,,,,,,,,,,,"09/Jan/13 11:06;slebresne;Fix committed as commit cc0c9f3. Thanks for the report.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
streaming from another node results in a bogus % streamed on that sstable,CASSANDRA-5130,12626504,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,mkjellman,mkjellman,08/Jan/13 06:37,16/Apr/19 09:32,14/Jul/23 05:53,15/Jan/13 18:53,1.2.1,,,,,,0,,,,,"reproducible thru a repair -pr

5689557% from node 10.8.25.132 ??

{code}
root@scl-cas06:~# nodetool netstats
Mode: NORMAL
Streaming to: /10.8.25.132
   /data2/cassandra/evidence/messages/evidence-messages-ia-2378-Data.db sections=1504 progress=10877426741/11376970910 - 95%
   /data/cassandra/evidence/messages/evidence-messages-ia-2396-Data.db sections=1326 progress=0/747963031 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2412-Data.db sections=342 progress=0/29791995 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2413-Data.db sections=307 progress=0/42375841 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2375-Data.db sections=702 progress=0/5138101708 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2416-Data.db sections=220 progress=0/21729661 - 0%
   /data2/cassandra/evidence/messages/evidence-messages-ia-2382-Data.db sections=1498 progress=0/7731727694 - 0%
   /data2/cassandra/evidence/messages/evidence-messages-ia-2388-Data.db sections=698 progress=0/2097813910 - 0%
   /data2/cassandra/evidence/messages/evidence-messages-ia-2401-Data.db sections=889 progress=0/154218729 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2411-Data.db sections=290 progress=0/45457960 - 0%
   /data2/cassandra/evidence/messages/evidence-messages-ia-2414-Data.db sections=22 progress=0/1809989 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2407-Data.db sections=423 progress=0/55302683 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2409-Data.db sections=73 progress=0/6366250 - 0%
   /data2/cassandra/evidence/messages/evidence-messages-ia-2377-Data.db sections=1298 progress=0/815617310 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2389-Data.db sections=167 progress=0/7307457 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-2415-Data.db sections=126 progress=0/9260301 - 0%
 Nothing streaming to /10.8.30.16
Streaming from: /10.8.25.132
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-1091-Data.db sections=328 progress=0/52507334 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-1084-Data.db sections=1029 progress=0/198699737 - 0%
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-1097-Data.db sections=35 progress=0/731705 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-1079-Data.db sections=1504 progress=1126955908333568/19807443126 - 5689557%
Pool Name                    Active   Pending      Completed
Commands                        n/a         0         459801
Responses                       n/a         0        1061826
{code}",Ubuntu 12.04,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"15/Jan/13 17:55;yukim;5130.txt;https://issues.apache.org/jira/secure/attachment/12564959/5130.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303111,,,Tue Jan 15 18:53:39 UTC 2013,,,,,,,,,,"0|i17807:",250265,,,,,,,,,mkjellman,,mkjellman,Normal,,,,,,,,,,,,,,,,,,"08/Jan/13 06:38;mkjellman;1141023% on this node?

{code}
root@scl-cas10:~# nodetool netstats
Mode: NORMAL
Streaming to: /10.8.30.103
   /data/cassandra/evidence/messages/evidence-messages-ia-1079-Data.db sections=1504 progress=12737080525/19807443126 - 64%
   /data2/cassandra/evidence/messages/evidence-messages-ia-1097-Data.db sections=35 progress=0/731705 - 0%
   /data/cassandra/evidence/messages/evidence-messages-ia-1084-Data.db sections=1029 progress=0/198699737 - 0%
   /data2/cassandra/evidence/messages/evidence-messages-ia-1091-Data.db sections=328 progress=0/52507334 - 0%
Streaming from: /10.8.30.103
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2375-Data.db sections=702 progress=58626960326656/5138101708 - 1141023%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2389-Data.db sections=167 progress=0/7307457 - 0%
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-2388-Data.db sections=698 progress=0/2097813910 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2416-Data.db sections=220 progress=0/21729661 - 0%
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-2377-Data.db sections=1298 progress=0/815617310 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2409-Data.db sections=73 progress=0/6366250 - 0%
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-2414-Data.db sections=22 progress=0/1809989 - 0%
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-2382-Data.db sections=1498 progress=0/7731727694 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2407-Data.db sections=423 progress=0/55302683 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2411-Data.db sections=290 progress=0/45457960 - 0%
   evidence: /data/cassandra/evidence/messages/evidence-messages-ia-2415-Data.db sections=126 progress=0/9260301 - 0%
   evidence: /data2/cassandra/evidence/messages/evidence-messages-ia-2401-Data.db sections=889 progress=0/154218729 - 0%
Pool Name                    Active   Pending      Completed
Commands                        n/a         0         252046
Responses                       n/a         3         746069
{code};;;","08/Jan/13 06:39;mkjellman;related to https://issues.apache.org/jira/browse/CASSANDRA-4687 ? streaming wrong part of file?;;;","15/Jan/13 13:22;brandon.williams;Do you have key cache on?;;;","15/Jan/13 16:40;mkjellman;key_cache_size_in_mb: 0
;;;","15/Jan/13 17:55;yukim;I somehow screwed calculating progress of compressed file transferred. Fix attached.;;;","15/Jan/13 18:02;mkjellman;+1 on patch and i'll make a build to actually test it to be extra sure in a moment;;;","15/Jan/13 18:44;mkjellman;confirmed stream progress is being correctly calculated;;;","15/Jan/13 18:53;yukim;Committed, thanks for the review.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
newly bootstrapping nodes hang indefinitely in STATUS:BOOT while JOINING cluster,CASSANDRA-5129,12626499,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,mkjellman,mkjellman,08/Jan/13 06:27,16/Apr/19 09:32,14/Jul/23 05:53,14/Feb/13 05:48,1.2.2,,,,,,1,,,,,"bootstrapping a new node causes it to hang indefinitely in STATUS:BOOT

Nodes streaming to the new node report 

{code}
Mode: NORMAL
 Nothing streaming to /10.8.30.16
Not receiving any streams.
Pool Name                    Active   Pending      Completed
Commands                        n/a         0        1843990
Responses                       n/a         2         661750
{code}

the node being streamed to stuck in the JOINING state reports:

{code}
Mode: JOINING
Not sending any streams.
 Nothing streaming from /10.8.30.103
 Nothing streaming from /10.8.30.102
Pool Name                    Active   Pending      Completed
Commands                        n/a         0             10
Responses                       n/a         0         613577
{code}

it appears that the nodes in the ""nothing streaming"" state never sends a ""finished streaming"" to the joining node.

no exceptions are thrown during the streaming on either node while the node is in this state.

{code:name=""full gossip state of bootstrapping node""}
/10.8.30.16
  NET_VERSION:6
  RELEASE_VERSION:1.2.0
  STATUS:BOOT,127605887595351923798765477786913079289
  RACK:RAC1
  RPC_ADDRESS:0.0.0.0
  DC:DC1
  SCHEMA:5cd8420d-ce3c-3625-8293-67558a24816b
  HOST_ID:e20817ce-7454-4dc4-a1c6-b1dec35c4491
  LOAD:1.11824041581E11
{code}",Ubuntu 12.04,jdilloyd,joeyi,kcarlson,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5244,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,0.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303107,,,Wed Mar 13 11:27:41 UTC 2013,,,,,,,,,,"0|i177zb:",250261,,,,,,,,,mkjellman,,mkjellman,Normal,,,,,,,,,,,,,,,,,,"18/Jan/13 22:47;joeyi;I can confirm this bug on CentOS 6.3 as well;;;","14/Feb/13 03:36;mkjellman;it appears this is related to secondary indexes. after the bootstrapping node finishes streaming it submits an index build. This gets submitted but never makes any progress and hangs indefinitely.

{code}
 INFO [Thread-382] 2013-02-13 18:02:57,205 StreamInSession.java (line 199) Finished streaming session 4ae0be23-75fb-11e2-ba65-8f73c0b9d93d from /10.138.12.10
 INFO [Thread-540] 2013-02-13 18:17:42,526 SecondaryIndexManager.java (line 137) Submitting index build of [domain_metadata.classificationIdx, domain_metadata.domaintypeIdx] for data in SSTableReader(path='/data/cassandra/brts/domain_metadata/brts-domain_metadata-ib-1-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-2-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-3-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-4-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-5-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-6-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-7-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-8-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-9-Data.db'), SSTableReader(path='/data2/cassandra/brts/domain_metadata/brts-domain_metadata-ib-10-Data.db')
{code}

{code}
#nodetool compactionstats
pending tasks: 23
Active compaction remaining time :        n/a
{code}

also when C* is killed, the node hung with nothing streaming logs:
{code}
ERROR 19:37:42,274 Exception in thread Thread[Streaming to /10.138.12.11:1,5,main]
java.lang.RuntimeException: java.io.EOFException
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1145)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:615)
	at java.lang.Thread.run(Thread.java:722)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:392)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:193)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:101)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 3 more
{code};;;","14/Feb/13 03:38;brandon.williams;Easily repros with toy data from stress:

{noformat}
 INFO 03:30:47,313 JOINING: Starting to bootstrap...
 INFO 03:30:48,522 Submitting index build of [Standard1.Idx1] for data in SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-4-Data.db')
 INFO 03:30:48,526 Enqueuing flush of Memtable-compactions_in_progress@893461718(177/177 serialized/live bytes, 7 ops)
 INFO 03:30:48,527 Writing Memtable-compactions_in_progress@893461718(177/177 serialized/live bytes, 7 ops)
 INFO 03:30:48,546 Completed flushing /var/lib/cassandra/data/system/compactions_in_progress/system-compactions_in_progress-ib-1-Data.db (176 bytes) for commitlog position ReplayPosition(segmentId=1360812614633, position=75619)
 INFO 03:30:48,547 Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-3-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-1-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-4-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1/Keyspace1-Standard1-ib-2-Data.db')]
{noformat}

and stays like that forever.;;;","14/Feb/13 03:41;brandon.williams;Thread dump indicates this is actually CASSANDRA-5244 which has a good analysis, but is more severe than we thought.;;;","14/Feb/13 05:48;mkjellman;fixed with 5244.;;;","13/Mar/13 11:27;kcarlson;I'm still experiencing this on 1.2.2. Same exception and same result. I took a thread dump and it didn't have anything interesting in it.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
unsafeAssassinateEndpoint throws NullPointerException and fails to remove node from gossip,CASSANDRA-5127,12626481,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,mkjellman,mkjellman,08/Jan/13 03:28,16/Apr/19 09:32,14/Jul/23 05:53,08/Jan/13 19:26,1.2.1,,,,,,0,,,,,"unsafeAssassinateEndpoint() throws NullPointerException and the node still seems to be in gossip. 

gossip info for node in question:

{code}
/10.8.30.15
  HOST_ID:d84a5632-d6d5-4b06-8e1b-ae39ab185ca1
  RPC_ADDRESS:0.0.0.0
  RACK:RAC1
  DC:DC1
  REMOVAL_COORDINATOR:REMOVER,b63fe173-5d13-4905-a59f-a78790f4f980
  RELEASE_VERSION:1.2.0
  NET_VERSION:6
  LOAD:2.64185473948E11
  STATUS:removed,d84a5632-d6d5-4b06-8e1b-ae39ab185ca1,1357874470406
  SCHEMA:5cd8420d-ce3c-3625-8293-67558a24816b
{code}

{code}
ERROR 19:26:20,078 Exception in thread Thread[GossipStage:1,5,main]
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.getApplicationStateValue(StorageService.java:1192)
	at org.apache.cassandra.service.StorageService.getTokensFor(StorageService.java:1200)
	at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:1452)
	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1163)
	at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1895)
	at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:805)
	at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:856)
	at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:57)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:722)

{code}",Ubuntu 12.04,mkjellman,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"08/Jan/13 18:32;brandon.williams;5127-v2.txt;https://issues.apache.org/jira/secure/attachment/12563790/5127-v2.txt","08/Jan/13 15:53;brandon.williams;5127.txt;https://issues.apache.org/jira/secure/attachment/12563766/5127.txt",,,,,,,,,,,,,,,,,,,2.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,303088,,,Tue Jan 08 23:04:01 UTC 2013,,,,,,,,,,"0|i177uv:",250241,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"08/Jan/13 06:16;brandon.williams;This is because there is no TOKENS state for a non-vnode host in 1.2.  Mostly saying this for myself so I don't forget what to do tomorrow.;;;","08/Jan/13 15:37;brandon.williams;Hmm, to clarify, the stacktrace you're posting is from a node receiving the assassinate attempt? Because that stack doesn't point at uAE itself.;;;","08/Jan/13 15:41;mkjellman;node i'm trying to assassinate is down. Stacktrace is from the node I executed uAE on.;;;","08/Jan/13 15:44;mkjellman;{code}
 WARN [RMI TCP Connection(3166)-10.8.240.170] 2013-01-08 07:42:02,143 Gossiper.java (line 419) Assassinating /10.8.30.15 via gossip
 INFO [RMI TCP Connection(3166)-10.8.240.170] 2013-01-08 07:42:02,144 Gossiper.java (line 434) Sleeping for 30000ms to ensure /10.8.30.15 does not change
 INFO [RMI TCP Connection(3166)-10.8.240.170] 2013-01-08 07:42:32,416 Gossiper.java (line 767) InetAddress /10.8.30.15 is now dead.
{code}

so that looks okay.

jconsole threw ""Problem invoking unsafeAssassinateEndpoint: java.lang.NullPointerException""

{code}
/10.8.30.15
  DC:DC1
  SCHEMA:5cd8420d-ce3c-3625-8293-67558a24816b
  RELEASE_VERSION:1.2.0
  REMOVAL_COORDINATOR:REMOVER,b63fe173-5d13-4905-a59f-a78790f4f980
  LOAD:2.64185473948E11
  STATUS:LEFT,1357918952416,21210815907080254728491917739023519863
  RPC_ADDRESS:0.0.0.0
  NET_VERSION:6
  HOST_ID:d84a5632-d6d5-4b06-8e1b-ae39ab185ca1
  RACK:RAC1
{code}


node is still in gossipinfo though..

I assumed that stack trace which happened at the same time correlated to the npe.;;;","08/Jan/13 15:48;mkjellman;logs from another node in the cluster (aka didn't execute the jmx call and is not the downed node.

{code}
 INFO [GossipStage:1] 2013-01-08 07:42:33,258 Gossiper.java (line 767) InetAddress /10.8.30.15 is now dead.
ERROR [GossipStage:1] 2013-01-08 07:42:33,267 CassandraDaemon.java (line 133) Exception in thread Thread[GossipStage:1,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService.getApplicationStateValue(StorageService.java:1192)
        at org.apache.cassandra.service.StorageService.getTokensFor(StorageService.java:1200)
        at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:1452)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1163)
        at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1895)
        at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:805)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:856)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:57)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [GossipStage:2] 2013-01-08 07:42:35,274 CassandraDaemon.java (line 133) Exception in thread Thread[GossipStage:2,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.StorageService.getApplicationStateValue(StorageService.java:1192)
        at org.apache.cassandra.service.StorageService.getTokensFor(StorageService.java:1200)
        at org.apache.cassandra.service.StorageService.handleStateLeft(StorageService.java:1452)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1163)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:917)
        at org.apache.cassandra.gms.Gossiper.applyNewStates(Gossiper.java:908)
        at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:866)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:57)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code};;;","08/Jan/13 15:53;brandon.williams;There are a few cases where we assume that if a host has a HOST_ID, then it must use vnodes, but that is not the case.  Patch attached to refactor these cases into a usesVnodes method which checks for both HOST_ID and TOKENS.;;;","08/Jan/13 17:52;slebresne;Agreed on the analysis, but I'd rather hide this directly in getTokenFor (i.e. replace the '{{if vnodes then getTokensFor else TokenFactory.fromString(pieces[1])}}' by '{{getTokenFor(endpoing, pieces[1])}}' and handle the backward compatibility inside getTokenFor.

In particular, and though that's a nit, I'm not of fan of have a usesVnodes method because if I'm not mistaken any upgraded would have this method return true even if only one token is used, right? If so, I'm afraid the method would be misleading. ;;;","08/Jan/13 18:33;brandon.williams;v2 consolidates everything into getTokensFor.

bq.  I'm not of fan of have a usesVnodes method because if I'm not mistaken any upgraded would have this method return true even if only one token is used

Not quite, to have the TOKENS app state you must be on vnodes.  If there's only one token it's handled the old, ugly way by splitting the STATUS state.
;;;","08/Jan/13 19:21;slebresne;+1 on v2, though I still don't like the usesVnodes method name (basically I don't want anyone starting to use it for anything else than what getTokenFor is doing, which for me qualify as 'it shouldn't exist') but that's a nit.

bq. If there's only one token it's handled the old, ugly way by splitting the STATUS state.

I would suggest handling the one token case by feeding both the STATUS state *and* the new TOKENS state so that in some future version we can just remove the STATUS old ugly way, but that's work for another ticket in any case.;;;","08/Jan/13 19:26;brandon.williams;Committed;;;","08/Jan/13 23:04;mkjellman;+1, confirmed that the patch resolves the issue;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Ensure Jackson dependency matches lib,CASSANDRA-5126,12626386,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,zznate,zznate,zznate,07/Jan/13 18:43,16/Apr/19 09:32,14/Jul/23 05:53,09/Jan/13 03:06,1.2.1,,,,,,0,,,,,"Older version of Jackson ASL has a concurrency issue. See http://jira.codehaus.org/browse/JACKSON-237

This can be triggered in some environments when running M/R tasks and the wrong version of jackson gets picked up. ",,zznate,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-4183,,,,,,,,,,,,,,,,,"08/Jan/13 21:44;amorton;5126-2.txt;https://issues.apache.org/jira/secure/attachment/12563832/5126-2.txt","07/Jan/13 18:46;zznate;5126.txt;https://issues.apache.org/jira/secure/attachment/12563607/5126.txt",,,,,,,,,,,,,,,,,,,2.0,zznate,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302978,,,Wed Jan 09 03:06:00 UTC 2013,,,,,,,,,,"0|i176xb:",250090,,,,,,,,,amorton,,amorton,Low,,,,,,,,,,,,,,,,,,"07/Jan/13 18:46;zznate;Updates Jackson core and asl to latest 1.9.x;;;","08/Jan/13 20:29;amorton;Fixed in CASSANDRA-4183;;;","08/Jan/13 21:01;zznate;Not so - looks like *only* 1.1 has the patch applied. cassandra-1.2 and trunk both still reference older versions.;;;","08/Jan/13 21:16;zznate;So - looks like the jars were updated in the lib dir, but not in the build.xml.

The ramifications of this are that *anything* referencing cassandra on maven central will have the older versions of jackson brought in as dependencies.;;;","08/Jan/13 21:44;amorton;Second version, sets the dependancy to 1.9.2 to match the file in /lib;;;","09/Jan/13 03:06;amorton;Applied to 1.2 and trunk;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Support indexes on composite column components (clustered columns),CASSANDRA-5125,12626383,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,jbellis,jbellis,07/Jan/13 18:33,16/Apr/19 09:32,14/Jul/23 05:53,04/Apr/13 16:33,2.0 beta 1,,,,,,2,,,,,"Given

{code}
CREATE TABLE foo (
  a int,
  b int,
  c int,
  PRIMARY KEY (a, b)
);
{code}

We should support {{CREATE INDEX ON foo(b)}}.",,aleksey,carlyeks,colinkuo,denis.angilella,iflatness,jjordan,liqusha,liqweed,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-8156,,,,CASSANDRA-4511,CASSANDRA-5534,,,,,,,,,,"02/Apr/13 17:42;slebresne;0001-Refactor-aliases-into-column_metadata.txt;https://issues.apache.org/jira/secure/attachment/12576611/0001-Refactor-aliases-into-column_metadata.txt","02/Apr/13 17:42;slebresne;0002-Generalize-CompositeIndex-for-all-column-type.txt;https://issues.apache.org/jira/secure/attachment/12576612/0002-Generalize-CompositeIndex-for-all-column-type.txt","02/Apr/13 17:42;slebresne;0003-Handle-new-type-of-IndexExpression.txt;https://issues.apache.org/jira/secure/attachment/12576613/0003-Handle-new-type-of-IndexExpression.txt","02/Apr/13 17:42;slebresne;0004-Handle-partition-key-indexing.txt;https://issues.apache.org/jira/secure/attachment/12576614/0004-Handle-partition-key-indexing.txt",,,,,,,,,,,,,,,,,4.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302975,,,Tue Oct 21 11:07:08 UTC 2014,,,,,,,,,,"0|i176wn:",250087,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"18/Feb/13 05:25;slebresne;Attaching patches for that (also pushed to https://github.com/pcmanus/cassandra/commits/5125).

The first part of this ticket is about how we store the information that a clustering key column is indexed. Turns out that for ""regular"" columns we use ColumnDefinition and the indexing code also assumes that, so the probably best and simplest approach is to reuse ColumnDefinition for that too.  But then it's easier to always store all primary key columns as ColumnDefinition, pretty much obsoleting the old key_aliases and column_aliases. There is a few related details worth noticing:
# while this obsolete the aliases, those are not removed of the schema by the patch for compatibility sake. Truth is, I'm not sure there is a way to remove a field from the schema without breaking rolling upgrades at this point.
# after this patch, CFDefinition becomes much less useful as CFMetadata + ColumnDefinition holds pretty much the same information in pretty much the same form. So we could slightly simplify things by removing CFDefinition.  However, this is left to later (this won't be a 3 lines patch).

After that, the patch adds a new type of composite indexes to handle indexing clustering keys (which share most code with the existing regular composite index) and update CQL3 to allow adding and querying the new indexes (in particular, it is slighty tricky in SelectStatment to recognize when a clustering key is restricted if 2ndary indexes should be used or not).

The last patch adds support for indexing components of the partition key (we don't allow indexing the first component of the partition key as it makes no sense (it's already primary indexed), but if the partition key is composite, secondary indexing the 2+ parts can be useful).

Lastly, I'll note that the patches only add theses news indexes for non compact tables. We should generalize to compact tables too, but that would require a bit of generalization that I'd rather add in a second phase.
;;;","02/Apr/13 17:42;slebresne;Rebased patches attached.;;;","03/Apr/13 08:31;slebresne;Things move fast on trunk lately, so I've pushed a rebased version at https://github.com/pcmanus/cassandra/commits/5125-2 to avoid rebasing every day.;;;","04/Apr/13 04:16;carlyeks;A few errors when running the test suite:

Testcase: testCli(org.apache.cassandra.cli.CliTest):	Caused an ERROR
java.lang.RuntimeException: org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 4
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1533)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:895)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:918)
	at java.lang.Thread.run(Thread.java:680)
Caused by: org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 4
	at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:69)
	at org.apache.cassandra.db.index.AbstractSimplePerColumnSecondaryIndex.insert(AbstractSimplePerColumnSecondaryIndex.java:121)
	at org.apache.cassandra.db.index.SecondaryIndexManager$PerColumnIndexUpdater.update(SecondaryIndexManager.java:623)
	at org.apache.cassandra.db.AtomicSortedColumns$Holder.addColumn(AtomicSortedColumns.java:313)
	at org.apache.cassandra.db.AtomicSortedColumns.addAllWithSizeDelta(AtomicSortedColumns.java:168)
	at org.apache.cassandra.db.Memtable.resolve(Memtable.java:253)
	at org.apache.cassandra.db.Memtable.put(Memtable.java:169)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:852)
	at org.apache.cassandra.db.Table.apply(Table.java:379)
	at org.apache.cassandra.db.Table.apply(Table.java:342)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:189)
	at org.apache.cassandra.service.StorageProxy$6.runMayThrow(StorageProxy.java:667)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1529)
	... 3 more
	
Testcase: testIndexDeletions(org.apache.cassandra.db.ColumnFamilyStoreTest):	Caused an ERROR
A long is exactly 8 bytes: 4
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 4
	at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:69)
	at org.apache.cassandra.db.index.AbstractSimplePerColumnSecondaryIndex.insert(AbstractSimplePerColumnSecondaryIndex.java:121)
	at org.apache.cassandra.db.index.SecondaryIndexManager$PerColumnIndexUpdater.update(SecondaryIndexManager.java:623)
	at org.apache.cassandra.db.AtomicSortedColumns$Holder.addColumn(AtomicSortedColumns.java:313)
	at org.apache.cassandra.db.AtomicSortedColumns.addAllWithSizeDelta(AtomicSortedColumns.java:168)
	at org.apache.cassandra.db.Memtable.resolve(Memtable.java:253)
	at org.apache.cassandra.db.Memtable.put(Memtable.java:169)
	at org.apache.cassandra.db.ColumnFamilyStore.apply(ColumnFamilyStore.java:852)
	at org.apache.cassandra.db.Table.apply(Table.java:379)
	at org.apache.cassandra.db.Table.apply(Table.java:342)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:189)
	at org.apache.cassandra.db.ColumnFamilyStoreTest.testIndexDeletions(ColumnFamilyStoreTest.java:301)


Testcase: testIndexUpdate(org.apache.cassandra.db.ColumnFamilyStoreTest):	Caused an ERROR
Index: 0, Size: 0
java.lang.IndexOutOfBoundsException: Index: 0, Size: 0
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.cassandra.db.ColumnFamilyStoreTest.testIndexUpdate(ColumnFamilyStoreTest.java:398);;;","04/Apr/13 07:10;slebresne;My bad. That was due to a rebase typo that I had fixed in my CASSANDRA-5417 branch but not on that one. I've push the fix to the same github branch than above (https://github.com/pcmanus/cassandra/commits/5125-2).;;;","04/Apr/13 09:35;aleksey;+1;;;","04/Apr/13 16:33;slebresne;Committed, thanks;;;","20/Oct/14 20:45;denis.angilella;??Lastly, I'll note that the patches only add theses news indexes for non compact tables. We should generalize to compact tables too, but that would require a bit of generalization that I'd rather add in a second phase.??

With 2.1 and *compact* tables it is possible to CREATE INDEX on composite primary key columns, but queries returns no results for the tests below.
Adding this comment for now, can open a new ticket if you prefer.

{code:SQL}
CREATE TABLE users2 (
   userID uuid,
   fname text,
   zip int,
   state text,
  PRIMARY KEY ((userID, fname))
) WITH COMPACT STORAGE;

CREATE INDEX ON users2 (userID);
CREATE INDEX ON users2 (fname);

INSERT INTO users2 (userID, fname, zip, state) VALUES (b3e3bc33-b237-4b55-9337-3d41de9a5649, 'John', 10007, 'NY');

// the following queries returns 0 rows, instead of 1 expected
SELECT * FROM users2 WHERE fname='John'; 
SELECT * FROM users2 WHERE userid=b3e3bc33-b237-4b55-9337-3d41de9a5649;
SELECT * FROM users2 WHERE userid=b3e3bc33-b237-4b55-9337-3d41de9a5649 AND fname='John';

// dropping 2ndary indexes restore normal behavior
{code};;;","21/Oct/14 10:18;slebresne;[~denis.angilella] Correct, the validation during index creation is broken. Do you mind creating a ticket indeed so we track the fix?;;;","21/Oct/14 11:07;denis.angilella;[~slebresne]: I created CASSANDRA-8156 to track the fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Multiget Supercolumns Sometimes Missing Results,CASSANDRA-5123,12626367,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,thobbs,thobbs,07/Jan/13 16:34,16/Apr/19 09:32,14/Jul/23 05:53,16/Jan/13 17:00,2.0 beta 1,,,,,,0,,,,,"Starting approximately with commit c2812f3 (the January 3rd nightly build by DataStax Jenkins, #669), a few of the pycassa unit tests related to multigetting a particular supercolumn started failing periodically.  The nightly build is against Cassandra trunk.

You can reproduce with the pycassa unit tests fairly easily:
{noformat}
nosetests tests/test_columnfamily.py:TestSuperColumnFamily.test_multiget_supercolumn
{noformat}

It should fail within a few runs.

It looks like one of the requested keys isn't being returned at all.",,slebresne,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"14/Jan/13 13:38;slebresne;5123.txt;https://issues.apache.org/jira/secure/attachment/12564702/5123.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302958,,,Wed Jan 16 17:00:13 UTC 2013,,,,,,,,,,"0|i176s7:",250067,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"14/Jan/13 13:38;slebresne;This is the same problem that in CASSANDRA-4928, the slice filter was shared among multiple ReadCommand which is not correct in the current state of things. Attaching patch that remove the sharing.;;;","16/Jan/13 08:26;jbellis;Why is it unsafe to share filter in SelectStatement for NQF?;;;","16/Jan/13 16:40;slebresne;bq. Why is it unsafe to share filter in SelectStatement for NQF?

It's not. Figured the cloneShallow was probably negligible enough to ignore in that case since it yield simpler code. That being said, since NQF is immutable (at least as far as cloning shallow is concerned), I suppose making its cloneShallow just return {{this}} is a valid option. ;;;","16/Jan/13 16:49;jbellis;WFM either way, +1;;;","16/Jan/13 17:00;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Select on composite partition keys are not validated correctly,CASSANDRA-5122,12626335,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,slebresne,slebresne,07/Jan/13 11:32,16/Apr/19 09:32,14/Jul/23 05:53,07/Jan/13 16:23,1.2.1,,,,,,0,,,,,"From Kais Ahmed on the mailing list:
{noformat}
----------------------------------------------------------------------------------------------------------------------
[cqlsh 2.3.0 | Cassandra 1.2.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol
19.35.0]

cqlsh:test> CREATE TABLE foo (   a int,   b text,   c uuid,   PRIMARY KEY
((a, b)) );

cqlsh:test> INSERT INTO foo (a, b , c ) VALUES (  1 , 'aze',
'4d481800-4c5f-11e1-82e0-3f484de45426');
cqlsh:test> INSERT INTO foo (a, b , c ) VALUES (  1 , 'ert',
'693f5800-8acb-11e3-82e0-3f484de45426');
cqlsh:test> INSERT INTO foo (a, b , c ) VALUES (  1 , 'opl',
'd4815800-2d8d-11e0-82e0-3f484de45426');

-----------------------------------------------------------------------------------------------------------------------------------------------------

cqlsh:test> SELECT * FROM foo;

 a | b   | c
---+-----+--------------------------------------
 1 | ert | 693f5800-8acb-11e3-82e0-3f484de45426
 1 | opl | d4815800-2d8d-11e0-82e0-3f484de45426
 1 | aze | 4d481800-4c5f-11e1-82e0-3f484de45426

-----------------------------------------------------------------------------------------------------------------------------------------------------

cqlsh:test> SELECT * FROM foo where a=1;

 a | b   | c
---+-----+--------------------------------------
 1 | ert | 693f5800-8acb-11e3-82e0-3f484de45426
 1 | opl | d4815800-2d8d-11e0-82e0-3f484de45426
{noformat}

The last request should be invalid (since we don't have a good way to do it, at least not with a random partitioner).",,mauzhang,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/13 13:39;slebresne;5122.txt;https://issues.apache.org/jira/secure/attachment/12563565/5122.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302925,,,Mon Jan 07 16:23:37 UTC 2013,,,,,,,,,,"0|i176hz:",250021,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"07/Jan/13 15:43;jbellis;+1;;;","07/Jan/13 16:23;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
system.peers.tokens is empty after node restart,CASSANDRA-5121,12626262,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,slebresne,pchalamet,pchalamet,06/Jan/13 21:32,16/Apr/19 09:32,14/Jul/23 05:53,15/Jan/13 15:26,1.2.1,,,,,,0,,,,,"Using a 2 nodes fresh cluster (127.0.0.1 & 127.0.0.2) running latest 1.2, I’m querying system.peers to get the nodes of the cluster and their respective token. But it seems there is a problem after either node restart.

When both node starts up, querying system.peers seems ok:

{code}
127.0.0.1> select * from system.peers;
+-----------------+------------------------------------------+---------------+-----------+---------------------+-----------------+------------------------------------------+-------------------------------------------+
| data_center     | host_id                                  | peer          | rack      | release_version     | rpc_address     | schema_version                           | tokens                                    |
+=================+==========================================+===============+===========+=====================+=================+==========================================+===========================================+
| datacenter1     | 4819cbb0-9741-4fe0-8d7d-95941b0247bf     | 127.0.0.2     | rack1     | 1.2.0               | 127.0.0.2       | 59adb24e-f3cd-3e02-97f0-5b395827453f     | 56713727820156410577229101238628035242    |
+-----------------+------------------------------------------+---------------+-----------+---------------------+-----------------+------------------------------------------+-------------------------------------------+
{code}

But as soon as one node is restarted (let’s say 127.0.0.2), tokens column is then empty:

{code}
127.0.0.1> select * from system.peers;
+-----------------+------------------------------------------+---------------+-----------+---------------------+-----------------+------------------------------------------+-------------+
| data_center     | host_id                                  | peer          | rack      | release_version     | rpc_address     | schema_version                           | tokens      |
+=================+==========================================+===============+===========+=====================+=================+==========================================+=============+
| datacenter1     | 4819cbb0-9741-4fe0-8d7d-95941b0247bf     | 127.0.0.2     | rack1     | 1.2.0               | 127.0.0.2       | 59adb24e-f3cd-3e02-97f0-5b395827453f     |             |
+-----------------+------------------------------------------+---------------+-----------+---------------------+-----------------+------------------------------------------+-------------+
{code}

{code}
Log server side:
DEBUG 22:08:01,608 Responding: ROWS [peer(system, peers), org.apache.cassandra.db.marshal.InetAddressType][data_center(system, peers), org.apache.cassandra.db.marshal.UTF8Type][host_id(system, peers), org.apache.cassandra.db.marshal.UUIDType][rack(system, peers), org.apache.cassandra.db.marshal.UTF8Type][release_version(system, peers), org.apache.cassandra.db.marshal.UTF8Type][rpc_address(system, peers), org.apache.cassandra.db.marshal.InetAddressType][schema_version(system,
peers), org.apache.cassandra.db.marshal.UUIDType][tokens(system, peers), org.apache.cassandra.db.marshal.SetType(org.apache.cassandra.db.marshal.UTF8Type)]
 | 127.0.0.2 | datacenter1 | 4819cbb0-9741-4fe0-8d7d-95941b0247bf | rack1 | 1.2.0 | 127.0.0.2 | 59adb24e-f3cd-3e02-97f0-5b395827453f | null
{code}

Restarting the other node (127.0.0.1) restore back the tokens column.
",Windows 8 / Java 1.6.0_37-b06,edong,mauzhang,mkjellman,pchalamet,slebresne,soverton,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/13 11:27;slebresne;5121.txt;https://issues.apache.org/jira/secure/attachment/12563554/5121.txt",,,,,,,,,,,,,,,,,,,,1.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302848,,,Sat Jan 19 19:29:37 UTC 2013,,,,,,,,,,"0|i175wv:",249926,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"07/Jan/13 11:01;soverton;In StorageService.handleStateNormal, when we see an endpoint come up which we already knew about: 

{noformat}
else if (endpoint.equals(currentOwner))
{
    // set state back to normal, since the node may have tried to leave, but failed and is now back up
    // no need to persist, token/ip did not change
{noformat}

I think the bug is that then we call 
{noformat}
SystemTable.updateTokens(endpoint, tokensToUpdateInSystemTable);
{noformat}
with an empty collection and SystemTable.updateTokens overwrites the current entry rather than adding to it.

Fix would be
{noformat}
- // no need to persist, token/ip did not change
+ if (!isClientMode)
+    tokensToUpdateInSystemTable.add(token);
{noformat};;;","07/Jan/13 11:27;slebresne;I agree on Sam's analysis but I would suggest the slightly different patch attached, because I think the intend was that if tokensToUpdateInSystemTable is empty, we don't update anything. In particular, in the case where we are relocating, I don't think we want remove the tokens from the system table either (and in the case where there is a token conflict I think the initial intent was also to leave things as they are, even though in that case maybe actually removing the token is not a bad idea?).
;;;","15/Jan/13 15:11;brandon.williams;+1;;;","15/Jan/13 15:26;slebresne;Committed, thanks;;;","19/Jan/13 06:32;edong;Hi, commit ec35427fdfbc46a8adeafc042651f552b9bcc1a0 breaks RelocateTest:

{noformat}
$ ant clean build test -Dtest.name=RelocateTest
...
    [junit] Testsuite: org.apache.cassandra.service.RelocateTest
    [junit] Tests run: 2, Failures: 2, Errors: 0, Time elapsed: 6.215 sec
    [junit] 
    [junit] Testcase: testWriteEndpointsDuringRelocate(org.apache.cassandra.service.RelocateTest):	FAILED
    [junit] removeTokens should be used instead
    [junit] junit.framework.AssertionFailedError: removeTokens should be used instead
    [junit] 	at org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:324)
    [junit] 	at org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTable.java:342)
    [junit] 	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1393)
    [junit] 	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)
    [junit] 	at org.apache.cassandra.service.RelocateTest.createInitialRing(RelocateTest.java:106)
    [junit] 	at org.apache.cassandra.service.RelocateTest.testWriteEndpointsDuringRelocate(RelocateTest.java:128)
    [junit] 
    [junit] 
    [junit] Testcase: testRelocationSuccess(org.apache.cassandra.service.RelocateTest):	FAILED
    [junit] removeTokens should be used instead
    [junit] junit.framework.AssertionFailedError: removeTokens should be used instead
    [junit] 	at org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:324)
    [junit] 	at org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTable.java:342)
    [junit] 	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1393)
    [junit] 	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)
    [junit] 	at org.apache.cassandra.service.RelocateTest.createInitialRing(RelocateTest.java:106)
    [junit] 	at org.apache.cassandra.service.RelocateTest.testRelocationSuccess(RelocateTest.java:177)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.service.RelocateTest FAILED

BUILD FAILED
...
{noformat}


After commit e6b6eaa583e8fc15f03c3e27664bf7fc06b3af0a, testWriteEndpointsDuringRelocate passes but testRelocationSuccess still fails:
{noformat}
$ ant clean build test -Dtest.name=RelocateTest
...
    [junit] Testcase: testRelocationSuccess(org.apache.cassandra.service.RelocateTest):	FAILED
    [junit] removeEndpoint should be used instead
    [junit] junit.framework.AssertionFailedError: removeEndpoint should be used instead
    [junit] 	at org.apache.cassandra.db.SystemTable.updateTokens(SystemTable.java:316)
    [junit] 	at org.apache.cassandra.db.SystemTable.updateLocalTokens(SystemTable.java:334)
    [junit] 	at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1394)
    [junit] 	at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)
    [junit] 	at org.apache.cassandra.service.RelocateTest.testRelocationSuccess(RelocateTest.java:193)
    [junit] 
    [junit] 
    [junit] Test org.apache.cassandra.service.RelocateTest FAILED
...
{noformat}
;;;","19/Jan/13 17:26;brandon.williams;You should update, this was fixed in 17adf8e4f72114d336140fac5157a35e63d1f53a;;;","19/Jan/13 19:29;edong;Updated; test now passes, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
user defined compaction is broken,CASSANDRA-5118,12626146,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,mkjellman,mkjellman,05/Jan/13 01:05,16/Apr/19 09:32,14/Jul/23 05:53,10/Jan/13 17:58,1.1.9,1.2.1,,,,,0,,,,,"currently forceUserDefinedCompaction takes (keyspace, datafile)

cassandra tries to look for ks/ks-cf-hf-80-Data.db when the sstable actually exists at ks/cf/ks-cf-hf-80-Data.db

fix would be for user defined compaction to look for the sstable datafile in the correct location",Ubuntu 12.04,jjordan,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"09/Jan/13 20:03;yukim;5118-1.1.txt;https://issues.apache.org/jira/secure/attachment/12564021/5118-1.1.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302729,,,Sat Jan 26 11:57:27 UTC 2013,,,,,,,,,,"0|i17507:",249779,,,,,,,,,jbellis,,jbellis,Low,,,,,,,,,,,,,,,,,,"09/Jan/13 20:03;yukim;Looks like user defined compaction has not been working since data directory structure change in 1.1.
Patch fixes this by pointing proper directory based on given file names.;;;","09/Jan/13 21:49;mkjellman;+1 verified this patch works against 1.2.0;;;","09/Jan/13 21:52;jbellis;LGTM.  Can you also add a quick test for forceUserDefinedCompaction to catch this regression?

Also created CASSANDRA-5139 for followup.;;;","10/Jan/13 17:58;yukim;Committed with the test added to CompactionsTest.;;;","26/Jan/13 11:57;jjordan;FYI it worked, you just had to put the dir name in the string: so forceUserDefinedCompaction ks cf/ks-cf-hf-80-Data.db
Should put something in NEWS.txt for anyone who figured that out and was using forceUserDefinedCompaction.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CQL 3 ALTER TABLE ... ADD causes OOB,CASSANDRA-5117,12626142,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,khahn,khahn,04/Jan/13 23:38,16/Apr/19 09:32,14/Jul/23 05:53,08/Jan/13 08:49,1.2.1,,,,,,0,,,,,"To reproduce:

./cqlsh
Connected to Test Cluster at localhost:9160.
[cqlsh 2.3.0 | Cassandra 1.2.0-SNAPSHOT | CQL spec 3.0.0 | Thrift protocol 19.35.0]
Use HELP for help.
cqlsh> create keyspace music with replication = {'CLASS' : 
   ... 'SimpleStrategy', 'replication_factor' : 3};
cqlsh> use music
   ... ;
cqlsh:music> CREATE TABLE songs (
         ... id uuid PRIMARY KEY,
         ... title text,
         ... album text,
         ... artist text,
         ... data blob
         ... );
cqlsh:music> insert into songs (id, title, artist, album) values ('a3e64f8f-bd44-4f28-b8d9-6938726e34d4', 'La Grange', 'ZZ Top', 'Tres Hombres');
cqlsh:music> insert into songs (id, title, artist, album) values ('8a172618-b121-4136-bb10-f665cfc469eb', 'Moving in Stereo', 'Fu Manchu', 'We Must Obey');
cqlsh:music> insert into songs (id, title, artist, album) values ('62c36092-82a1-3a00-93d1-46196ee77204', 'Outside Woman Blues', 'Back Door Slam', 'Roll Away');
cqlsh:music> CREATE TABLE song_tags (
         ... id uuid,
         ... tag_name text,
         ... PRIMARY KEY (id, tag_name)
         ... );
cqlsh:music> select * from song_tags;
cqlsh:music> INSERT INTO song_tags (id, tag_name) VALUES ('a3e64f8f-bd44-4f28-b8d9-6938726e34d4', 'blues');
cqlsh:music> INSERT INTO song_tags (id, tag_name) VALUES ('8a172618-b121-4136-bb10-f665cfc469eb', 'covers');
cqlsh:music> INSERT INTO song_tags (id, tag_name) VALUES ('a3e64f8f-bd44-4f28-b8d9-6938726e34d4', '1973');
cqlsh:music> INSERT INTO song_tags (id, tag_name) VALUES ('8a172618-b121-4136-bb10-f665cfc469eb', '2007');
cqlsh:music> select * from song_tags;

 id                                   | tag_name
--------------------------------------+----------
 a3e64f8f-bd44-4f28-b8d9-6938726e34d4 |     1973
 a3e64f8f-bd44-4f28-b8d9-6938726e34d4 |    blues
 8a172618-b121-4136-bb10-f665cfc469eb |     2007
 8a172618-b121-4136-bb10-f665cfc469eb |   covers

cqlsh:music> drop table song_tags;
cqlsh:music> ALTER TABLE songs ADD tags set<text>;
TSocket read 0 bytes
","Mac OSX, DS Java Driver, apache-cassandra-1.2.0-src downloaded from project Jan 2, 2013",aleksey,dbrosius,khahn,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"07/Jan/13 10:28;slebresne;5117-v2.txt;https://issues.apache.org/jira/secure/attachment/12563547/5117-v2.txt","06/Jan/13 01:16;dbrosius@apache.org;5117.txt;https://issues.apache.org/jira/secure/attachment/12563449/5117.txt","05/Jan/13 00:00;khahn;OOB.txt;https://issues.apache.org/jira/secure/attachment/12563380/OOB.txt",,,,,,,,,,,,,,,,,,3.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302725,,,Tue Jan 08 08:49:45 UTC 2013,,,,,,,,,,"0|i174zb:",249775,,,,,,,,,dbrosius,,dbrosius,Normal,,,,,,,,,,,,,,,,,,"04/Jan/13 23:55;aleksey;Looks like it could be related to CASSANDRA-5064.;;;","05/Jan/13 00:00;khahn;system.log;;;","07/Jan/13 10:28;slebresne;I don't think the attached patch is correct (that is, it fixes the fact that componentIndex be set to a negative value, but it's not the correct fix). The problem is that in the case where we add a collection but we had not collection previous, we should not do the {{component--}} at all. Attaching a v2 to fix that.;;;","08/Jan/13 05:34;dbrosius;+1;;;","08/Jan/13 08:49;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
RepairCallback breaks CL guarantees,CASSANDRA-5113,12626061,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,slebresne,slebresne,slebresne,04/Jan/13 16:25,16/Apr/19 09:32,14/Jul/23 05:53,07/Jan/13 10:13,1.2.1,,,,,,0,,,,,"RepairCallback does not validate the consistency level of the query. It seems that this was done on purpose as the comments there says:
{noformat}
/**
 * The main difference between this and ReadCallback is, ReadCallback has a ConsistencyLevel
 * it needs to achieve.  Repair on the other hand is happy to repair whoever replies within the timeout.
 */
{noformat}
Concretely, the get() method of RepairCallback:
* waits for all endpoints, even if there is more than strictly required by the CL.
* if it timeouts, doesn't check it and always return a response.
* for some reason, it returns null unless there is strictly more than 1 response.

All of that seems wrong to me. The result of RepairCallback is what is returned to the client in case of a digest mismatch on the first read. So we must ensure that the CL has been reached. Also, returning null where there is 1 response (or none) seems clearly wrong.

In fact I don't think we need a special callback for this ""read all data"" phase as it is a ""normal"" read (the fact we do a first read with digests is just an ""optimization""). The only difference between the two phases should be how we resolve the responses (in the first case we have digest and in the 2nd we don't) but that's handled by the resolver.

So attaching a patch that removes RepairCallback and use ReadCallback instead.  I'm also attaching a 2nd trivial patch that renames RowRepairResolver to RowDataResolver because I think it describe better what this actually do (i.e.  the main goal is to resolve a full data read to answer the right value to the client, repairing inconsistent nodes is secondary).

The patch is against 1.1, because I think breaking CL guarantees is probably serious enough to warrant pushing this to 1.1.
",,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/13 18:18;slebresne;0001-Always-ensure-CL-after-a-digest-mismatch-1.2.txt;https://issues.apache.org/jira/secure/attachment/12563333/0001-Always-ensure-CL-after-a-digest-mismatch-1.2.txt","04/Jan/13 16:25;slebresne;0001-Always-ensure-CL-after-a-digest-mismatch.txt;https://issues.apache.org/jira/secure/attachment/12563308/0001-Always-ensure-CL-after-a-digest-mismatch.txt","04/Jan/13 18:18;slebresne;0002-Rename-RowRepairResolver-to-RowDataResolver-1.2.txt;https://issues.apache.org/jira/secure/attachment/12563334/0002-Rename-RowRepairResolver-to-RowDataResolver-1.2.txt","04/Jan/13 16:25;slebresne;0002-Rename-RowRepairResolver-to-RowDataResolver.txt;https://issues.apache.org/jira/secure/attachment/12563309/0002-Rename-RowRepairResolver-to-RowDataResolver.txt",,,,,,,,,,,,,,,,,4.0,slebresne,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302644,,,Mon Jan 07 10:13:49 UTC 2013,,,,,,,,,,"0|i174hb:",249694,,,,,,,,,jbellis,,jbellis,Normal,,,,,,,,,,,,,,,,,,"04/Jan/13 17:27;jbellis;+1 on the fix.

-0 on the rename; the only time we do full-data reads is for repairs so I don't really see an improvement there.

-1 on committing to 1.1; this behavior has been present since at least 1.0 and probably longer, and changes to core StorageProxy code like this have a long track record of causing subtle regressions.;;;","04/Jan/13 17:49;slebresne;bq. the only time we do full-data reads is for repairs

You kind of thouch my point, ""the only time we do full-data reads is for repairs"" is imho largely false. We do full-data reads in 2 places: # on a digest mismatch of an initial read (i.e. in SP.fetchRows) # on a digest mismatch in ReadCallback.maybeResolveForRepair, i.e.  asynchronously if we've already responded to the client but have more response due to read_repair_chance kicking in.

While in the 2nd case this is indeed ""for repairs"", in the first case we first and foremost do the full-data read so we can compute the right/up-to-date value to return to the client. That's by far the most important use of the full-data read because that's where correctness is involved. The fact we use that occasion to compute diff and repair inconcistent nodes is secondary as it's after ""an optimization"".

So RowRepairResolver suggests repairing is the most important thing it does, but it's not.

bq. -1 on committing to 1.1; this behavior has been present since at least 1.0

I can buy that. And for the record, the risk of breaking CL guarantees is actually pretty low as you need a digetst mistmatch and have a node die between the digest phase and the full-data read phase.

One thing that has more chance to occurs with the current implementation is to get requests that take rpc_timeout seconds to return sometimes if read_repair_change > 0. Because in that case, and in the even of a mismatch, RepairCallback.get() will wait for all replica (even at QUORUM), so there is a real change that it'll wait until the timeout. But after that timeout it will almost always return the correct value, so it's just an inefficiency.

I'll rebase the patches to 1.2.
;;;","04/Jan/13 18:18;slebresne;Alright, 1.2 rebased version attached (including the rename since as said above I do really like it :));;;","04/Jan/13 19:11;jbellis;+1 on both;;;","07/Jan/13 10:13;slebresne;Committed, thanks;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"NodeCmd misspells ""positives""",CASSANDRA-5110,12625991,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,jalkanen,jalkanen,jalkanen,04/Jan/13 08:58,16/Apr/19 09:32,14/Jul/23 05:53,04/Jan/13 12:41,1.1.9,,,,,,0,,,,,"Running nodetool cfstats speaks of ""Bloom Filter False Postives"". It annoys my OCD to see the misspelling all the time. :-)",Any,jalkanen,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/13 09:02;jalkanen;CASSANDRA-5110.patch;https://issues.apache.org/jira/secure/attachment/12563264/CASSANDRA-5110.patch",,,,,,,,,,,,,,,,,,,,1.0,jalkanen,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302573,,,Fri Jan 04 12:41:03 UTC 2013,,,,,,,,,,"0|i1741j:",249622,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"04/Jan/13 09:02;jalkanen;Patch against cassandra-1.1;;;","04/Jan/13 12:41;brandon.williams;This bothered me too and I seem to recall fixing it, but I guess I missed 1.1.  Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
node fails to start because host id is missing,CASSANDRA-5107,12625922,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,brandon.williams,brandon.williams,03/Jan/13 22:12,16/Apr/19 09:32,14/Jul/23 05:53,22/Feb/13 21:00,1.2.2,,,,,,1,,,,,"I saw this once on dtestbot but couldn't figure it out, but now I've encountered it myself:

{noformat}
ERROR 22:04:45,949 Exception encountered during startup
java.lang.AssertionError
   at org.apache.cassandra.locator.TokenMetadata.updateHostId(TokenMetadata.java:219)
   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:442)
   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:397)
   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:309)
   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:397)
   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:440)
java.lang.AssertionError
   at org.apache.cassandra.locator.TokenMetadata.updateHostId(TokenMetadata.java:219)
   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:442)
   at org.apache.cassandra.service.StorageService.initServer(StorageService.java:397)
   at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:309)
   at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:397)
   at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:440)
Exception encountered during startup: null
{noformat}

Somehow our own hostid is null sometimes.",,blair,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"18/Jan/13 17:30;brandon.williams;0001-Preload-gossip-application-states-before-starting.txt;https://issues.apache.org/jira/secure/attachment/12565509/0001-Preload-gossip-application-states-before-starting.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302504,,,Fri Feb 22 21:00:17 UTC 2013,,,,,,,,,,"0|i173lz:",249552,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"10/Jan/13 23:25;brandon.williams;Another trace that may be related:

{noformat}
ERROR [main] 2013-01-10 17:17:20,150 CassandraDaemon.java (line 387) Exception encountered during startup
java.lang.AssertionError
    at org.apache.cassandra.service.StorageService.getLocalTokens(StorageService.java:1969)
    at org.apache.cassandra.service.StorageService.setTokens(StorageService.java:209)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:740)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:508)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:406)
    at org.apache.cassandra.service.CassandraDaemon.setup(CassandraDaemon.java:282)
    at org.apache.cassandra.service.CassandraDaemon.activate(CassandraDaemon.java:370)
    at org.apache.cassandra.service.CassandraDaemon.main(CassandraDaemon.java:413)
{noformat}

We must be wiping our own tokens out so that there's nothing left to retrieve subsequently.;;;","11/Jan/13 12:50;slebresne;That second trace is CASSANDRA-5121 (which has a patch attached). Maybe the patch there fixes the first trace too, not sure.;;;","17/Jan/13 17:00;brandon.williams;I think CASSANDRA-5121 solved this.;;;","18/Jan/13 15:26;brandon.williams;It didn't :(  Here's another slightly different stacktrace where a remote node's hostId is missing:

{noformat}
 INFO [main] 2013-01-18 09:24:30,576 StorageService.java (line 1288) Node /127.0.0.3 state jump to normal
 INFO [main] 2013-01-18 09:24:30,583 StorageService.java (line 744) Startup completed! Now serving reads.
 WARN [main] 2013-01-18 09:24:30,597 Auth.java (line 131) Skipping default superuser setup: some nodes are not ready
 INFO [GossipStage:1] 2013-01-18 09:24:30,637 Gossiper.java (line 782) Node /127.0.0.1 has restarted, now UP
 INFO [GossipStage:1] 2013-01-18 09:24:30,637 Gossiper.java (line 750) InetAddress /127.0.0.1 is now UP
 INFO [GossipStage:1] 2013-01-18 09:24:30,639 StorageService.java (line 1288) Node /127.0.0.1 state jump to normal
 INFO [GossipStage:2] 2013-01-18 09:24:30,640 Gossiper.java (line 782) Node /127.0.0.2 has restarted, now UP
ERROR [GossipStage:1] 2013-01-18 09:24:30,641 CassandraDaemon.java (line 133) Exception in thread Thread[GossipStage:1,5,main]
java.lang.NullPointerException
    at org.apache.cassandra.gms.Gossiper.getHostId(Gossiper.java:637)
    at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:1304)
    at org.apache.cassandra.service.StorageService.onChange(StorageService.java:1166)
    at org.apache.cassandra.service.StorageService.onJoin(StorageService.java:1899)
    at org.apache.cassandra.gms.Gossiper.handleMajorStateChange(Gossiper.java:802)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:853)
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:43)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:56)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
 INFO [GossipStage:2] 2013-01-18 09:24:30,642 Gossiper.java (line 750) InetAddress /127.0.0.2 is now UP
{noformat};;;","18/Jan/13 16:01;slebresne;Can't that be because we start gossiper before setting the applicationState (including the host_id)? Granted we have a 1 second delay before really starting the gossipTask, but if those stack traces come from the test bot, maybe sometime the machine is slow enough that you can have a whole second interruption of the thread initializing the server.

In any case, this feel dangerous to me. Maybe we could split Gossiper.start into a Gossiper.init(), followed by the addition of the applicationstate that we expect to be always there, and then we start it.;;;","18/Jan/13 17:25;brandon.williams;I'm not convinced this is the cause, but you're right that this seems dangerous regardless.  Patch to preload essential gossip states before starting the gossiper.;;;","18/Jan/13 17:42;slebresne;Nit: the gossip info are not ""pre-loaded"", maybe they good for good measure (splitting start() into init() and start() would have allowed to reuse the gossipSnitchInfo as is :), though I'm fine with duplicating the 2 lines of that function if you prefer). Also, currently the ""gossip snitch infos"" comment are misplaced.

But that's details, +1 in any case.

bq. I'm not convinced this is the cause

I'm not either, but I don't see anything else so far. Besides, I've seen dtest getting to a bit of crawl for some tests that were involving many nodes, so it's far fetched, but not impossible.;;;","18/Jan/13 17:50;brandon.williams;bq. Nit: the gossip info are not ""pre-loaded"", maybe they good for good measure (splitting start() into init() and start() would have allowed to reuse the gossipSnitchInfo as is

The problem with start/init is propagating the passed in generation, and we can't add states until we've added our local state, which is why I added a new start() signature instead.  I get what you're saying about gossipSnitchInfo, but it's also called from PFS.reloadConfiguration, and since addLocalApplicationState does notifications where this 'preload' does not, I didn't want to risk breaking anything there.

bq. Also, currently the ""gossip snitch infos"" comment are misplaced.

Actually I fixed that in a ninja-reupload. :)

Committed 0001, leaving this ticket open to see what happens.;;;","22/Feb/13 21:00;brandon.williams;Closing since I haven't seen this repro since we committed the fix.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
Stock example for using pig throws InvalidRequestException(why:Start token sorts after end token),CASSANDRA-5106,12625917,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jbellis,jdinata,jdinata,03/Jan/13 21:43,16/Apr/19 09:32,14/Jul/23 05:53,10/Jan/13 18:52,1.1.9,1.2.1,,,,,0,,,,,"The setup:
This is from printenv
HADOOP_HOME=/home/Downloads/hadoop-1.1.1
PIG_HOME=/home/Downloads/pig-0.10.0
PIG_INITIAL_ADDRESS=localhost
PIG_RPC_PORT=9160
PIG_PARTITIONER=org.apache.cassandra.dht.Murmur3Partitioner

This is from cassandra-cli
[default@PigTest] describe cluster;
Cluster Information:
   Snitch: org.apache.cassandra.locator.SimpleSnitch
   Partitioner: org.apache.cassandra.dht.Murmur3Partitioner
   Schema versions:
        b5fc9a82-fbdd-3cf5-af16-9c498c9f9a5c: [127.0.0.1]

Running test_storage.pig as
bin/pig_cassandra -x local test/test_storage.pig
after populating the cf as
cassandra-cli --host localhost --port 9160 < populate-cli.txt
throws 
2013-01-03 02:20:47,626 [Thread-4] INFO  org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader - Current spli
t being processed ColumnFamilySplit((-1, '-1728690256123413808] @[localhost.localdomain])
2013-01-03 02:20:47,758 [Thread-4] WARN  org.apache.hadoop.mapred.FileOutputCommitter - Output path is null in cleanup
2013-01-03 02:20:47,760 [Thread-4] WARN  org.apache.hadoop.mapred.LocalJobRunner - job_local_0001
java.lang.RuntimeException: InvalidRequestException(why:Start token sorts after end token)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.maybeInit(ColumnFamilyRecordReader.java:384)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.computeNext(ColumnFamilyRecordReader.java:390)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.computeNext(ColumnFamilyRecordReader.java:313)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:143)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:138)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.nextKeyValue(ColumnFamilyRecordReader.java:184)
        at org.apache.cassandra.hadoop.pig.CassandraStorage.getNext(CassandraStorage.java:226)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigRecordReader.nextKeyValue(PigRecordReader.java:194)
        at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.nextKeyValue(MapTask.java:532)
        at org.apache.hadoop.mapreduce.MapContext.nextKeyValue(MapContext.java:67)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:143)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
        at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:212)
Caused by: InvalidRequestException(why:Start token sorts after end token)
        at org.apache.cassandra.thrift.Cassandra$get_range_slices_result.read(Cassandra.java:12916)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_range_slices(Cassandra.java:734)
        at org.apache.cassandra.thrift.Cassandra$Client.get_range_slices(Cassandra.java:718)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordReader$StaticRowIterator.maybeInit(ColumnFamilyRecordReader.java:346)
        ... 13 more",1 node cluster with default cassandra.yaml,apesternikov,jdinata,mck,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5209,,,,,,,,,,,,,,CASSANDRA-5089,,,,,,"09/Jan/13 22:45;jbellis;5106.txt;https://issues.apache.org/jira/secure/attachment/12564051/5106.txt",,,,,,,,,,,,,,,,,,,,1.0,jbellis,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302499,,,Thu Jan 10 18:52:21 UTC 2013,,,,,,,,,,"0|i173kv:",249547,,,,,,,,,brandon.williams,,brandon.williams,Normal,,,,,,,,,,,,,,,,,,"04/Jan/13 13:45;brandon.williams;You probably have exported PIG_PARTITIONER=org.apache.cassandra.dht.RandomPartitioner, but 1.2 is using the default partitioner.  I've updated the README.txt instructions in examples/pig to use the m3 partitioner instead.;;;","07/Jan/13 23:12;jdinata;Hi,
Still having the same exception thrown even with PIG_PARTITIONER=org.apache.cassandra.dht.Murmur3Partitioner;;;","08/Jan/13 15:23;brandon.williams;Regression from CASSANDRA-5076;;;","08/Jan/13 19:25;yukim;This issue indeed comes from CASSANDRA-5089.
WordCount does fail in latest cassandra-1.1 branch with the same exception.;;;","08/Jan/13 19:31;brandon.williams;Closing in favor of CASSANDRA-5089;;;","08/Jan/13 21:41;jbellis;Reopening to avoid polluting fixver on 5089.;;;","09/Jan/13 22:45;jbellis;patch to re-allow wrapping ranges for start_token/end_token pairing;;;","10/Jan/13 18:09;brandon.williams;+1;;;","10/Jan/13 18:52;jbellis;committed;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
repair -pr throws EOFException,CASSANDRA-5105,12625910,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,yukim,mkjellman,mkjellman,03/Jan/13 21:01,16/Apr/19 09:32,14/Jul/23 05:53,14/Feb/13 20:00,1.2.2,,,,,,1,,,,,"nodetool repair -pr threw an EOF exception

{code:title=node1}
ERROR 12:50:18,723 Exception in thread Thread[Streaming to /10.8.25.113:1,5,main]
java.lang.RuntimeException: java.io.EOFException
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:375)
	at org.apache.cassandra.streaming.FileStreamTask.receiveReply(FileStreamTask.java:193)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:114)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
{code}

{code:title=node2}
 INFO 12:49:45,139 Finished streaming session to /10.8.30.13
ERROR 12:50:18,799 Exception in thread Thread[Thread-4031,5,main]
java.lang.RuntimeException: Last written key DecoratedKey(167625858728826091814875924785363245309, 6634333531356661643161636636373738353431363162353031376164386339) >= current ke
y DecoratedKey(33957321636818582219838207277782228619, 696c2e636f6d200a3c42523e0a3c42523e0a5472656e7420202020202020202020202020202020422e204d697261636c652020202020202020202020202
020202020202020202020202020202020202020202020202020202020200a2020266e62737020266e62737020746d697261636c654073696d6d6f6e736669726d2e636f6d2c206c776f6f74656e4073696d6d6f6e736669726
d2e636f6d203c42523e0a3c42523e0a56616e636520202020202020202020202020202020522e20416e64727573202020202020202020202020202020202020202020202020202020202020202020202020202020202020200
a2020266e62737020266e627370207672614061622d706c632e636f6d203c42523e0a3c42523e0a5665726e6f6e202020202020202020202020202020462e20476c656e6e20202020202020202020202020202020202020202
020202020202020202020202020202020202020202020200a2020266e62737020266e62737020676c656e6e6c6177406c6f77636f756e7472796c61777965722e636f6d203c42523e0a3c42523e0a56696e63656e742020202
0202020202020202020204a2e20446573616c766f2020202020202020202020202020202020202020202020202020202020202020202020202020202020200a2020266e62737020266e6273702076646573616c766f4064657
3616c766f6c61776669726d2e636f6d203c42523e0a3c42523e0a56696e63656e7420202020202020202020202020204a616d65732043617274657220202020202020202020202020202020202020202020202020202020202
0202020202020202020200a2020202020266e62737020266e627370207663617274657240676972617264696b656573652e636f6d2c207479616d6173616b6940676972617264696b656573652e636f6d203c42523e0a0a3c4
2523e0a572e202020202020202020202020202020202020204a616d65732053696e676c65746f6e202020202020202020202020202020202020202020202020202020202020202020202020200a2020202020266e627370202...trunkated...324132393239413134333439413834453531394133373431) writing into /data/cassandra/evidence/fingerprints/evidence-fingerprints-tmp-ia-161-Data.db
        at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:133)
        at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:209)
        at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:226)
        at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:166)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:66)
{code}",Ubuntu 12.04 Java 7,alprema,markncooper,mkjellman,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,CASSANDRA-5146,CASSANDRA-5229,,,,,,,,,,,,,,,,,,,,,,"12/Feb/13 22:29;yukim;0001-add-CompressedInputStream-test.patch;https://issues.apache.org/jira/secure/attachment/12569081/0001-add-CompressedInputStream-test.patch","12/Feb/13 22:29;yukim;0002-fix-compressed-streaming-sends-extra-chunk.patch;https://issues.apache.org/jira/secure/attachment/12569082/0002-fix-compressed-streaming-sends-extra-chunk.patch",,,,,,,,,,,,,,,,,,,2.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302491,,,Wed Feb 27 08:10:27 UTC 2013,,,,,,,,,,"0|i173iv:",249538,,,,,,,,,mkjellman,,mkjellman,Low,,,,,,,,,,,,,,,,,,"03/Jan/13 22:07;yukim;It looks like node1 is streaming wrong part of SSTable file.
With key cache turned on, this may relate to CASSANDRA-4687.;;;","03/Jan/13 22:43;mkjellman;can not reproduce with keycache turned off on node1;;;","04/Jan/13 06:08;mkjellman;i *think* i just reproduced this with key cache turned off..;;;","15/Jan/13 17:08;mkjellman;confirmed that I have seen this with key_cache_size_in_mb: 0 for every node in the cluster;;;","23/Jan/13 13:54;alprema;We just migrated our cluster to 1.2 here and we're having a similar issue when running repairs. Since the call stack mentioned ""CompressedFileStreamTask"" we tried to disable the column family compaction and scrub it. The following repairs worked fine so we decided to re-enable the compression, which made the error come back.
I don't know if it was a coincidence or if there is an issue with the compression, can anyone else reproduce the behavior?;;;","23/Jan/13 17:33;jbellis;Does that help narrow it down, Yuki?;;;","24/Jan/13 14:40;alprema;Update: We moved back the failing CF to sstable_compression='' and it doesn't show errors in the log anymore.
The other CFs are working/repairing fine, but nodetool netstats during a repair shows wrong percentages (156098%...) could it be related?

I'd be happy to run other tests if it can help finding the issue.;;;","24/Jan/13 14:48;yukim;[~alprema] wrong percentage was fixed in CASSANDRA-5130 and will be released in 1.2.1.

I'm trying to reproduce myself but still no luck. Some logs before error happend may help if you can upload.;;;","25/Jan/13 09:45;alprema;[~yukim] Great for the percentages, I'm relieved my other CFs are not impacted by the issue.
For the main problem at hand here are a couple logs (We have a 3 nodes cluster and the problem occurred between 01 and 02, we can see that 02 throws an exception when receiving the stream, which causes a Broken Pipe exception on the 01 side).
{code:title=cassandra02}
 INFO 15:52:16,502 Compacted to [/var/lib/cassandra/data/OpsCenter/rollups60/OpsCenter-rollups60-ia-902-Data.db,].  15,399,190 to 15,439,842 (~100% of original) bytes for 742 keys at 6.415939MB/s.  Time: 2,295ms.
 INFO 15:52:19,121 Compacted to [/var/lib/cassandra/data/MyMetrics/Metrics/MyMetrics-Metrics-ia-652-Data.db,].  34,516,769 to 34,493,377 (~99% of original) bytes for 735 keys at 5.948544MB/s.  Time: 5,530ms.
 INFO 15:52:19,566 Compacted to [/var/lib/cassandra/data/MyTimeouts/RequestTimeoutCommands/MyTimeouts-RequestTimeoutCommands-ia-746-Data.db,].  28,662,815 to 28,722,158 (~100% of original) bytes for 451,077 keys at 4.618375MB/s.  Time: 5,931ms.
 INFO 15:53:26,796 Enqueuing flush of Memtable-MyBusinessHistoryCF@335344726(17286127/177550150 serialized/live bytes, 360751 ops)
 INFO 15:53:26,803 Writing Memtable-MyBusinessHistoryCF@335344726(17286127/177550150 serialized/live bytes, 360751 ops)
 INFO 15:53:27,725 Completed flushing /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4879-Data.db (9746867 bytes) for commitlog position ReplayPosition(segmentId=1358949372386, position=24979117)
 INFO 15:53:36,532 [streaming task #aa70f8e0-656c-11e2-b226-d966287ae7ca] Received task from /10.80.90.51 to stream 7871 ranges to /10.80.90.53
 INFO 15:53:36,533 [streaming task #aa70f8e0-656c-11e2-b226-d966287ae7ca] Performing streaming repair of 7871 ranges with /10.80.90.53
 INFO 15:53:43,216 Stream context metadata [/var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4840-Data.db sections=5086 progress=0/350803667 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4787-Data.db sections=6079 progress=0/1160848303 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4878-Data.db sections=3469 progress=0/45248343 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4783-Data.db sections=6203 progress=0/1189290990 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4782-Data.db sections=3452 progress=0/517261421 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4872-Data.db sections=3518 progress=0/44182720 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4825-Data.db sections=5526 progress=0/571701570 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4879-Data.db sections=3043 progress=0/3086625 - 0%], 11 sstables.
 INFO 15:53:43,217 Streaming to /10.80.90.53
 INFO 15:53:43,325 Beginning transfer to /10.80.90.51
 INFO 15:53:43,362 Flushing memtables for [CFS(Keyspace='MyBusinessKeyspace', ColumnFamily='MyBusinessHistoryCF')]...
 INFO 15:53:43,363 Enqueuing flush of Memtable-MyBusinessHistoryCF@554695962(2424273/24900316 serialized/live bytes, 51289 ops)
 INFO 15:53:43,366 Writing Memtable-MyBusinessHistoryCF@554695962(2424273/24900316 serialized/live bytes, 51289 ops)
 INFO 15:53:43,558 Completed flushing /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4881-Data.db (2083586 bytes) for commitlog position ReplayPosition(segmentId=1358949372387, position=2780498)
 INFO 15:53:51,056 Stream context metadata [/var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4787-Data.db sections=6645 progress=0/1303785158 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4840-Data.db sections=5775 progress=0/392625457 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4878-Data.db sections=4530 progress=0/53236689 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4881-Data.db sections=3917 progress=0/715697 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4783-Data.db sections=6782 progress=0/1316663783 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4782-Data.db sections=3826 progress=0/579082484 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4825-Data.db sections=6223 progress=0/597029084 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4872-Data.db sections=4573 progress=0/52484841 - 0%, /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4879-Data.db sections=4083 progress=0/3265613 - 0%], 12 sstables.
 INFO 15:53:51,057 Streaming to /10.80.90.51
 INFO 15:54:19,013 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4840-Data.db to /10.80.90.53
ERROR 15:54:46,686 Exception in thread Thread[Thread-3087,5,main]
java.lang.RuntimeException: Last written key DecoratedKey(153906576608468125601485890282698016632, 72736b3a67726f75703a73656375726974793a496e74657276656e74696f6e3a496e7472616461793a66306135386333352d353262312d343361642d396430332d6130636630306330306565633a3937383a313330313137) >= current key DecoratedKey(33745288399064288388334698406389712581, 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) writing into /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-tmp-ia-4882-Data.db
	at org.apache.cassandra.io.sstable.SSTableWriter.beforeAppend(SSTableWriter.java:133)
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:209)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:179)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:122)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:226)
	at org.apache.cassandra.net.IncomingTcpConnection.handleStream(IncomingTcpConnection.java:166)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:66)
 INFO 15:55:37,245 CFS(Keyspace='OpsCenter', ColumnFamily='pdps') liveRatio is 2.3633228082745292 (just-counted was 2.3633228082745292).  calculation took 2ms for 1256 columns
 INFO 15:56:30,149 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4787-Data.db to /10.80.90.53
 INFO 15:56:32,401 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4878-Data.db to /10.80.90.53
{code}

{code:title=cassandra01}
 INFO 15:53:43,296 Enqueuing flush of Memtable-MyBusinessHistoryCF@709374075(7410300/48461437 serialized/live bytes, 155356 ops)
 INFO 15:53:43,304 Writing Memtable-MyBusinessHistoryCF@709374075(7410300/48461437 serialized/live bytes, 155356 ops)
 INFO 15:53:43,688 Completed flushing /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4952-Data.db (4474554 bytes) for commitlog position ReplayPosition(segmentId=1358949134910, position=13864741)
 INFO 15:54:09,160 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4922-Data.db to /10.80.90.52
 INFO 15:54:11,605 Successfully sent /var/lib/cassandra/data/MyBusinessKeyspace/MyBusinessHistoryCF/MyBusinessKeyspace-MyBusinessHistoryCF-ia-4922-Data.db to /10.80.90.53
ERROR 15:54:46,682 Exception in thread Thread[Streaming to /10.80.90.52:2,5,main]
java.lang.RuntimeException: java.io.IOException: Broken pipe
	at com.google.common.base.Throwables.propagate(Throwables.java:160)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:32)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)
Caused by: java.io.IOException: Broken pipe
	at sun.nio.ch.FileChannelImpl.transferTo0(Native Method)
	at sun.nio.ch.FileChannelImpl.transferToDirectly(Unknown Source)
	at sun.nio.ch.FileChannelImpl.transferTo(Unknown Source)
	at org.apache.cassandra.streaming.compress.CompressedFileStreamTask.stream(CompressedFileStreamTask.java:90)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:28)
	... 3 more
{code};;;","31/Jan/13 00:31;markncooper;I'm seeing what looks like the same error every time I try and add a new node to my cluster.;;;","07/Feb/13 20:24;mkjellman;i'm wondering if the DecoratedKey RuntimeException is a manifestation but not the real issue here.

After unrelated IOExceptions during the repair while bootstrapping a new node, it appears that the IOException/broken pipe was thrown.

maybe when we retry it starts streaming from the wrong location in the sstable after attempting to recover from the original IOException?;;;","12/Feb/13 22:29;yukim;I found one problem that can send extra chunk to destination which causes reading from wrong position.
This happens when the streaming section of sstable falls into the edge of compression chunks.
Test and fix attached.;;;","14/Feb/13 05:49;mkjellman;+1 patch looks good. verified as fixed.;;;","14/Feb/13 20:00;yukim;Committed, thanks for testing!;;;","27/Feb/13 08:10;alprema;Switched to 1.2.2 and re-enabled compression, repair now works like a charm.
Thanks.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
upgrading from 1.1.7 to 1.2.0 caused upgraded nodes to only know about other 1.2.0 nodes,CASSANDRA-5102,12625867,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Urgent,Fixed,brandon.williams,mkjellman,mkjellman,03/Jan/13 16:14,16/Apr/19 09:32,14/Jul/23 05:53,04/Jan/13 18:15,1.2.1,,,,,,0,,,,,"I upgraded as I have since 0.86 and things didn't go very smoothly.

I did a nodetool drain to my 1.1.7 node and changed my puppet config to use the new merged config. When it came back up (without any errors in the log) a nodetool ring only showed itself. I upgraded another node and sure enough now nodetool ring showed two nodes.


I tried resetting the local schema. The upgraded node happily grabbed the schema again but still only 1.2 nodes were visible in the ring to any upgraded nodes.",,jjordan,mkjellman,rcoli,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"03/Jan/13 22:25;brandon.williams;5102.txt;https://issues.apache.org/jira/secure/attachment/12563177/5102.txt",,,,,,,,,,,,,,,,,,,,1.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302448,,,Fri Mar 22 21:12:01 UTC 2013,,,,,,,,,,"0|i1738n:",249492,,,,,,,,,jbellis,,jbellis,Critical,,,,,,,,,,,,,,,,,,"03/Jan/13 16:50;brandon.williams;Can you add nodetool gossipinfo from one of the new nodes that doesn't see the old ones?  Do the old ones see the new nodes?;;;","03/Jan/13 16:51;mkjellman;i'm using PropertyFileSnitch as my endpoint snitch btw;;;","03/Jan/13 16:56;mkjellman;at this point i just went ahead and upgraded all the nodes (was more worried about getting the cluster back up)

I do notice though that the 1.2.0 nodes show net_version of 6.

as nodes were upgraded to 1.2.0 they didn't show up in the ring on the 1.1.7 side anymore.

gossipinfo on 1.2.0 nodes (ubuntu 12.04) look like:

/10.8.30.14
  RELEASE_VERSION:1.2.0
  NET_VERSION:6
  RPC_ADDRESS:0.0.0.0
  HOST_ID:24647d52-41eb-4df3-993e-51d4f841ca62
  LOAD:2.0129361318E11
  STATUS:NORMAL,70892159775195513221536376548285044050
  DC:DC1
  SCHEMA:da921e0b-4154-3601-9c76-6f61ca5f2872
  RACK:RAC1
  SEVERITY:-3.991605743852711E-11
/10.8.25.101
  RELEASE_VERSION:1.2.0
  RPC_ADDRESS:0.0.0.0
  NET_VERSION:6
  HOST_ID:dd3a40e2-fef1-4574-87b8-e2929fd80235
  LOAD:1.56018171896E11
  STATUS:NORMAL,42535295865117307932921825928971026436
  DC:DC1
  SCHEMA:da921e0b-4154-3601-9c76-6f61ca5f2872
  RACK:RAC2
  SEVERITY:0.019533560597218058 ;;;","03/Jan/13 17:08;brandon.williams;Did they show up on the 1.1 side as being down, or not at all?;;;","03/Jan/13 17:09;mkjellman;not at all.;;;","03/Jan/13 17:14;mkjellman;very possibly unrelated but opscenter seems to be unable to identify the nodes in the cluster either:

2013-01-02 18:39:24-0800 []  WARN: Unable to find a matching cluster for [u'fe80
:0:0:0:ca60:ff:feea:9c03%2', u'10.8.25.114', u'0:0:0:0:0:0:0:1%1', u'127.0.0.1',
 u'127.0.1.1']

maybe the node's identifiers changed with the ipv6 address which caused it to not be a member of the ring?;;;","03/Jan/13 20:34;mkjellman;from one of the last nodes to be upgraded...

{code}
ERROR [GossipStage:906] 2013-01-02 13:51:44,982 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[GossipStage:906,5,main]
java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:89)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.UnknownHostException: addr is of illegal length
        at java.net.InetAddress.getByAddress(InetAddress.java:979)
        at java.net.InetAddress.getByAddress(InetAddress.java:1374)
        at org.apache.cassandra.net.CompactEndpointSerializationHelper.deserialize(CompactEndpointSerializationHelper.java:39)
        at org.apache.cassandra.gms.EndpointStatesSerializationHelper.deserialize(GossipDigestSynMessage.java:117)
        at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:83)
        at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:70)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:60)
        ... 4 more
ERROR [GossipStage:907] 2013-01-02 13:51:45,984 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[GossipStage:907,5,main]
java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:89)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.UnknownHostException: addr is of illegal length
        at java.net.InetAddress.getByAddress(InetAddress.java:979)
        at java.net.InetAddress.getByAddress(InetAddress.java:1374)
        at org.apache.cassandra.net.CompactEndpointSerializationHelper.deserialize(CompactEndpointSerializationHelper.java:39)
        at org.apache.cassandra.gms.EndpointStatesSerializationHelper.deserialize(GossipDigestSynMessage.java:117)
        at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:83)
        at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:70)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:60)
        ... 4 more
ERROR [GossipStage:908] 2013-01-02 13:51:46,988 AbstractCassandraDaemon.java (line 135) Exception in thread Thread[GossipStage:908,5,main]
java.lang.RuntimeException: java.net.UnknownHostException: addr is of illegal length
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:89)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:722)
Caused by: java.net.UnknownHostException: addr is of illegal length
        at java.net.InetAddress.getByAddress(InetAddress.java:979)
        at java.net.InetAddress.getByAddress(InetAddress.java:1374)
        at org.apache.cassandra.net.CompactEndpointSerializationHelper.deserialize(CompactEndpointSerializationHelper.java:39)
        at org.apache.cassandra.gms.EndpointStatesSerializationHelper.deserialize(GossipDigestSynMessage.java:117)
        at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:83)
        at org.apache.cassandra.gms.GossipDigestAckMessageSerializer.deserialize(GossipDigestAckMessage.java:70)
        at org.apache.cassandra.gms.GossipDigestAckVerbHandler.doVerb(GossipDigestAckVerbHandler.java:60)
        ... 4 more
{code};;;","03/Jan/13 22:25;brandon.williams;Here is a sad story of how multiple release cycles ended up causing a regression.

The cause of these exceptions is CASSANDRA-4576.  There, we added checks against VERSION_11 to prevent using the compatible mode with newer node that didn't need it. VERSION_11 has an actual value of 4.  We closed the ticket on Sept 18, and that was that.

Fast forward to November, where we closed CASSANDRA-4880.  To do this, we needed a protocol version bump, and created VERSION_117, which has an actual value of 5.  Unfortunately we used <= comparisons in CASSANDRA-4576, but now had created a version higher than VERSION_11 that still needed the compatibility, and we got our original bug back.

The effect of this is if you upgrade from nodes on 1.1.7 or later to 1.2.0, the 1.2.0 nodes won't be able to gossip with the 1.1.7 nodes and they won't be visible in ring output on the 1.2.0 node until they too are on 1.2.0.  The 1.1.7 nodes will still know about the 1.2.0 node, but they won't be able to successfully gossip with it, and keep it marked down.

Patch attached to go ahead and compare more explicitly against VERSION_12 to fix this, but I think it highlights a deeper problem, which is that if we ever do need to do another protocol bump in a minor, stable branch, we're out of luck because there's no space between VERSION_117 and VERSION_12.;;;","04/Jan/13 08:37;slebresne;bq. if we ever do need to do another protocol bump in a minor, stable branch, we're out of luck

I'm sorry I did not follow CASSANDRA_4880 more closely but hadn't we decided that we should not change the protocol version in a minor version because it breaks streaming and we were only fine doing that for major upgrades?

Now I suppose what's done is done (though I wish some warning in the NEWS file for 1.1.7 had been added with CASSANDRA-4880 to explain that streaming would be broken during pre-1.1.7 to post-1.1.7 upgrades, and since it hadn't we should probably document it now), but as long as protocol bump means breaking streaming then I think we should maintain the rule ""no bump in minor version"" (not that I wouldn't be against lifting the ""protocol bump == break streaming"" limitation if possible but that's a different discussion).;;;","04/Jan/13 12:43;brandon.williams;bq. hadn't we decided that we should not change the protocol version in a minor version because it breaks streaming and we were only fine doing that for major upgrades?

We had, but unfortunately it was the only way to fix what is hopefully the last of our schema problems in 1.1.  The impact from CASSANDRA-4880 is much worse than having to upgrade all nodes before streaming.

bq.  I wish some warning in the NEWS file for 1.1.7 had been added with CASSANDRA-4880 to explain that streaming would be broken during pre-1.1.7 to post-1.1.7 upgrades

Totally agree.;;;","04/Jan/13 17:45;jbellis;The whole thing is a gotcha for assuming that we don't change messaging versions in minor releases.  The futureproof version would have been {{if (!(version >= MessagingService.VERSION_12))}} which looks kind of bizarre and might well have been ""fixed"" later on anyway.

So I'd file this under ""if we have to take a mulligan and add another version mid-release-cycle, make sure we validate usages of the initial major version.""

And if we want to be extra careful we should probably include an unused version before new major releases (i.e. make VERSION_20=8 instead of 7).;;;","04/Jan/13 17:54;jbellis;bq. if (!(version >= MessagingService.VERSION_12))

Don't mind me, of course that simplifies to {{version < VERSION_12}}.

+1 on the patch, and please update NEWS retroactively.;;;","04/Jan/13 18:15;brandon.williams;Committed; also updated 1.1 news to mention 4880;;;","22/Mar/13 21:12;jjordan;Should that also get added to the 1.2.X NEWS.TXT in an UPGRADING section?;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
describe commands fail in cql3 when previously created with cql2,CASSANDRA-5101,12625812,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Low,Fixed,aleksey,mkjellman,mkjellman,03/Jan/13 05:33,16/Apr/19 09:32,14/Jul/23 05:53,04/Jan/13 16:27,1.2.1,,,,,,0,,,,,"column families and keyspaces created with cassandra-cli/cql2 cannot be described with cql3

describe table cfname fails with: ""expected string or buffer""
describe schema fails with ""expected string or buffer"" as well",,aleksey,mkjellman,slebresne,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/13 15:50;aleksey;5101-v2.txt;https://issues.apache.org/jira/secure/attachment/12563297/5101-v2.txt","03/Jan/13 21:42;aleksey;5101.txt;https://issues.apache.org/jira/secure/attachment/12563165/5101.txt",,,,,,,,,,,,,,,,,,,2.0,aleksey,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302358,,,Tue Jan 08 22:22:15 UTC 2013,,,,,,,,,,"0|i170cv:",249025,,,,,,,,,brandon.williams,,brandon.williams,Low,,,,,,,,,,,,,,,,,,"03/Jan/13 05:41;mkjellman;interesting -- if i drop an index in cqlsh in cql2 mode and then recreate it in cql3 describe commands work for that cf. all non-modified cf's post upgrade still fail though.;;;","03/Jan/13 13:32;brandon.williams;From the error it sounds like you're using cqlsh, not the cli?;;;","03/Jan/13 15:51;mkjellman;Using cqlsh in 1.2. Cf were created in cassandra-cli and cql2 describes them just fine in cqlsh in 1.2;;;","03/Jan/13 16:59;aleksey;[~mkjellman]Can you attach the CQL2 CREATE statement you used for the CF that can't be described?;;;","03/Jan/13 17:06;mkjellman;cassandra-cli create statement examples. let me know if you need more:

create keyspace evidence with strategy_class = NetworkTopologyStrategy AND strategy_options:DC1 = 2;
create column family messages with key_validation_class = BytesType AND comparator = BytesType AND compression_options={sstable_compression:SnappyCompressor, chunk_length_kb:64} AND bloom_filter_fp_chance = 0.5 and compaction_strategy = SizeTieredCompactionStrategy;

__

create column family domain_metadata_history with key_validation_class = UTF8Type AND default_validation_class = UTF8Type AND comparator = 'CompositeType(UTF8Type, UTF8Type, IntegerType)' AND compression_options = {sstable_compression:SnappyCompressor, chunk_length_kb:64} WITH compaction_strategy=LeveledCompactionStrategy AND compaction_strategy_options={sstable_size_in_mb: 100};;;;","03/Jan/13 17:32;aleksey;[~mkjellman] Are these the one that fail for you or the ones that work? Because my 1.2.0 cqlsh describes them just fine in both default and legacy modes.

default (CQL3):
{noformat}
cqlsh:evidence> DESC KEYSPACE;

CREATE KEYSPACE evidence WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'DC1': '2'
};

USE evidence;

CREATE TABLE domain_metadata_history (
  key text,
  column1 text,
  column2 text,
  column3 varint,
  value text,
  PRIMARY KEY (key, column1, column2, column3)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.100000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  compaction={'sstable_size_in_mb': '100', 'class': 'LeveledCompactionStrategy'} AND
  compression={'chunk_length_kb': '64', 'sstable_compression': 'SnappyCompressor'};

CREATE TABLE messages (
  key blob,
  column1 blob,
  value blob,
  PRIMARY KEY (key, column1)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.500000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=864000 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'chunk_length_kb': '64', 'sstable_compression': 'SnappyCompressor'};
{noformat}

legacy (CQL2):
{noformat}
cqlsh:evidence> DESC KEYSPACE;

CREATE KEYSPACE evidence WITH strategy_class = 'NetworkTopologyStrategy'
  AND strategy_options:DC1 = '2';

USE evidence;

CREATE TABLE domain_metadata_history (
  KEY text PRIMARY KEY
) WITH
  comment='' AND
  comparator='org.apache.cassandra.db.marshal.CompositeType'<text, text, varint> AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='LeveledCompactionStrategy' AND
  compaction_strategy_options:sstable_size_in_mb='100' AND
  compression_parameters:chunk_length_kb='64' AND
  compression_parameters:sstable_compression='SnappyCompressor';

CREATE TABLE messages (
  KEY blob PRIMARY KEY
) WITH
  comment='' AND
  comparator=blob AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=blob AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:chunk_length_kb='64' AND
  compression_parameters:sstable_compression='SnappyCompressor';
{noformat};;;","03/Jan/13 17:37;mkjellman;Those fail for me.

{code:title=""CQL3""}
[cqlsh 2.3.0 | Cassandra 1.2.0 | CQL spec 3.0.0 | Thrift protocol 19.35.0]
Use HELP for help.
cqlsh> describe schema

CREATE KEYSPACE evidence WITH replication = {
  'class': 'NetworkTopologyStrategy',
  'DC2': '0',
  'DC1': '2'
};

USE evidence;

expected string or buffer
cqlsh>
{code}

{code:title=""CQL2""}
[cqlsh 2.3.0 | Cassandra 0.0.0 | CQL spec 2.0.0 | Thrift protocol 19.35.0]
Use HELP for help.
cqlsh> describe schema

CREATE KEYSPACE evidence WITH strategy_class = 'NetworkTopologyStrategy'
  AND strategy_options:DC2 = '0'
  AND strategy_options:DC1 = '2';

USE evidence;

CREATE TABLE fingerprints (
  KEY blob PRIMARY KEY
) WITH
  comment='' AND
  comparator=blob AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=172800 AND
  default_validation=blob AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  replicate_on_write='true' AND
  compaction_strategy_class='SizeTieredCompactionStrategy' AND
  compression_parameters:sstable_compression='SnappyCompressor';

...and the rest of the schema...
{code};;;","03/Jan/13 17:42;aleksey;[~mkjellman] Created with CQL2 from your example, described with CQL3:

{noformat}
CREATE TABLE fingerprints (
  key blob,
  column1 blob,
  value blob,
  PRIMARY KEY (key, column1)
) WITH COMPACT STORAGE AND
  bloom_filter_fp_chance=0.010000 AND
  caching='KEYS_ONLY' AND
  comment='' AND
  dclocal_read_repair_chance=0.000000 AND
  gc_grace_seconds=172800 AND
  read_repair_chance=0.100000 AND
  replicate_on_write='true' AND
  compaction={'class': 'SizeTieredCompactionStrategy'} AND
  compression={'sstable_compression': 'SnappyCompressor'};
{noformat}

I'm afraid I still can't reproduce :(;;;","03/Jan/13 17:46;mkjellman;are you creating in 1.1.7 though, upgrading and then describing?

edit: i should clarify these column families were created prior to 1.2 on a variety of versions. when i modify the schema for a cf in cql2 on 1.2 describe for that cf works in 1.2. the non modified cf's still fail.;;;","03/Jan/13 17:49;aleksey;Nope. That was never mentioned in the issue, though.;;;","03/Jan/13 17:50;mkjellman;yeah, my bad. sorry. minor detail ;);;;","03/Jan/13 18:52;aleksey;Yep, there is an issue with key_aliases column of system.schema_columnfamilies cf for 1.1 tables. It returns null when it should be returning '[]', and json-loading it fails with 'expected string or buffer' error.;;;","04/Jan/13 07:45;slebresne;For the record, I think I'd prefer fixing cqlsh to be less fragile and to handle both null and [] the same way (it should probably do that for column_aliases too btw). Adding code at startup to rewrite nulls to [] feels highly overkill here, especially since having no key_aliases column is no wrong per se.

I note that the fact we have 2 way to mean ""no key aliases"" is unfortunate but just a downside of using a json-encoded list internally and that'll be fix once we switch to true lists.;;;","04/Jan/13 13:10;aleksey;I'm checking now, but it seems like there is another issue with key aliases for upgraded 1.1 instances - if they have a key alias, it's going to be in the key_alias column, which we are no longer using.

That would make key aliases even less consistent (need to look at key_alias column, need to check key_aliases for null). I think pushing this complexity to the clients is a bad thing (cqlsh is not the only consumer of system.shema_* tables) - the better way would be to convert non-null key_alias to key_aliases singleton list and null key_alias to an empty list on startup, once.

I'm not a fat of either solution, but there is no third one, and I slightly prefer dealing with this in C* and not pushing the complexity to all the clients.;;;","04/Jan/13 13:23;aleksey;Yep, that would be described incorrectly as well.

I think I dislike the overkill-conversion to 1.2-style on startup *slightly* less than I dislike that added extra-logic in every client using schema_columnfamilies metadata.

Another opinion, maybe?;;;","04/Jan/13 13:51;slebresne;Converting things at startup is not as easy as it sound, because the schema is distributed. Typically, removing key_alias on startup on newly upgraded node would potentially propagate and screw up old, not-yet upgraded nodes (and/or new nodes would pull back the ""key_alias"" from older nodes after startup).

I hear you about not pushing the complexity to the clients, but at the same time we have to be extra careful with the schema table, as screwing things there is the one way to screw up a full cluster, and so making those tables convenient to read for clients is not a top priority (which is not saying we should make it hard on purpose but ...).

Also, converting the information from the schema table to a CQL3 definition is already far from trivial and I'm not sure that little details like the one on this issue really make much quantitative difference here. Don't get me wrong, this already existing complexity is not a good thing, and maybe exposing the system tables directly is not the best way to expose the schema for CQL3 clients. But my point is, as far as this ticket is concerned, I'd still rather avoid messing with schema at startup, especially to save a very small amount of complexity to clients library. But I'm all for discussing how we could fix the more general issue that the information exposed in the system schema is hard to make sense of for a CQL3 client.  ;;;","04/Jan/13 13:56;aleksey;You are right.

Will attach a v2 with a cqlsh-fix instead and let's deal with the more general issue later.;;;","04/Jan/13 16:15;brandon.williams;+1;;;","04/Jan/13 16:27;aleksey;Thanks, committed.;;;","08/Jan/13 22:22;mkjellman;confirmed that this commit fixes the issue.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
"Since 1.1, get_count sometimes returns value smaller than actual column count",CASSANDRA-5099,12625743,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,yukim,alienth,alienth,02/Jan/13 22:51,16/Apr/19 09:32,14/Jul/23 05:53,11/Jan/13 17:06,1.1.9,1.2.1,,,,,0,,,,,"We have a CF where rows have thousands of TTLd columns. The columns are continually added at a regular rate, and TTL out after 15 minutes. We continually run a 'get_count' on these keys to get a count of the number of live columns.

Since we upgrade from 1.0 to 1.1.7, ""get_count"" regularly returns much smaller values than are possible. For example, with  roughly 15,000 columns that have well-distributed TTLs, running a get_count 10 times will result in 1 or 2 results that are up to half the actual column count. Using a normal 'get' to count those columns always results in proper values. 

For example:

(all of these counts were ran within a second or less of eachother)
{code}
[default@reddit] count  AccountsActiveBySR['2qh0u'];
13665 columns
[default@reddit] count  AccountsActiveBySR['2qh0u'];
13665 columns
[default@reddit] count  AccountsActiveBySR['2qh0u'];
13666 columns
[default@reddit] count  AccountsActiveBySR['2qh0u'];
3069 columns
[default@reddit] count  AccountsActiveBySR['2qh0u'];
13660 columns
[default@reddit] count  AccountsActiveBySR['2qh0u'];
13661 columns
{code}

I should note that this issue happens much more frequently with larger (>10k columns) rows than smaller rows. It never seems to happen with rows having fewer than 1k columns.

There are no supercolumns in use. The key names and column names are very short, and there are no column values. The CF is LCS, and due to the TTL only hovers around a few MB in size. GC grace is normally at zero, but the problem is consistent with non-zero gc grace times.


It appears that there was an issue (CASSANDRA-4833) fixed in 1.1.7 regarding get_count. Some logic was added to prevent an infinite loop case. Could that change have resulted in this problem somehow? I can't find any other relevant 1.1 changes that might explain this issue.",,alienth,slebresne,thobbs,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"10/Jan/13 00:29;yukim;5099-1.1.txt;https://issues.apache.org/jira/secure/attachment/12564063/5099-1.1.txt",,,,,,,,,,,,,,,,,,,,1.0,yukim,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302294,,,Fri Jan 11 17:06:25 UTC 2013,,,,,,,,,,"0|i16zyn:",248961,,,,,,,,,slebresne,,slebresne,Normal,,,,,,,,,,,,,,,,,,"02/Jan/13 22:56;alienth;Also found CASSANDRA-3798 which altered the get_count logic between 1.0 and 1.1.;;;","05/Jan/13 00:06;alienth;Confirmed that this problem is consistent on both LCS and SizeTiered.;;;","07/Jan/13 08:26;alienth;I've ported the 1.0.11 get_count code from CassandraServer.java to 1.1.7. Doing so resolves this issue.;;;","10/Jan/13 00:29;yukim;Fix for CASSANDRA-4833 assumes that only last page from get_slice has columns less than requested, and if that happens, returns the column count collected.
But in case of ttl, it is possible to have less columns at the middle of the pagenation.

Attached patch changes loop termination check so that it does not depend on column count, which is what we had in 1.0.;;;","10/Jan/13 10:53;slebresne;In fact, I think that this point out to a more serious problem with TTL.

Because even get_count aside, if I do a get_slice with a particular count n, I expect that if I get less than n values in my result, it means there is no more value matching whatever replica I've provided. I'd say that any other behavior is a bug. But with TTL, if a column expires while the query is processed, we may fail that count argument (which is exactly what hits us here).

In other words, I'm not sure reintroducing the inefficiency of always doing one last almost always useless query to make sure the paging is indeed over is the right fix, because I doubt people that do manual row paging using get_slice actually implement that ""do one last query just in case the previous query lied on the count returned"".

What I would suggest instead would be to have CassandraServer save the current time before doing a get_slice, and use that time in thriftifyColumns when deciding if a column is considered expired or not. That way we won't skip an expiring column from the result set if it had been counted as live during the actual internal query.

 
;;;","10/Jan/13 16:36;slebresne;I'll note that the solution I'm suggesting is 1) slightly painful to implement because deserialization change expiring columns into tombstones so we would have have to pass the expireBefore all the way down the stack and 2) this only really work correctly if we actually send that expireBefore to other nodes along with the read request to use as reference. In particular 2) makes such solution pretty much out of scope for 1.1 and even 1.2 (since it's a protocol change). That being said, I do think this is probably the ""right"" solution in the long run (unless someone has a better idea of course).

Note that I don't oppose committing the ""let's do one more request"" to 1.1 to fix get_count if we have nothing better, but that doesn't change the fact that the get_slice count is potentially broken when TTLs are involved. I just don't see a quick fix right off the bat.;;;","10/Jan/13 17:52;jbellis;Sounds to me like we should commit the ""one more request"" fix and open a new ticket to address mid-request ttl expiration for 1.2 only.;;;","10/Jan/13 21:52;yukim;I'm +1 to commit this patch for 1.1.9 and open new ticket for 1.2.
So, is the patch looking good?;;;","11/Jan/13 15:32;slebresne;bq. So, is the patch looking good?

Yes, +1. I've created CASSANDRA-5149 for the follow up.;;;","11/Jan/13 17:06;yukim;Committed, thanks!;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
CassandraStorage doesn't decode name in widerow mode,CASSANDRA-5098,12625705,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,brandon.williams,justen_walker,justen_walker,02/Jan/13 17:08,16/Apr/19 09:32,14/Jul/23 05:53,04/Jan/13 17:13,1.1.9,1.2.1,,,,,0,cassandra,hadoop,pig,,"CassandraStorage doesn't decode name in widerow mode. This causes functions such as FILTER to fail with a ClassCastException, since the key is a bytearray instead of a chararray.

{code:title=test.pig}
DEFINE CassandraStorage org.apache.cassandra.hadoop.pig.CassandraStorage;

A  = LOAD 'cassandra://Metrics/EventEntries?widerows=true' USING CassandraStorage();
-- describe A --> A: {key: chararray,columns: {(name: (),value: chararray)}}

B = FILTER A BY key matches '^user.hit';
-- Throws CCE: org.apache.pig.data.DataByteArray cannot be cast to java.lang.String
{code}","Ubuntu 12.04.1 x64, Cassandra 1.1.8",aleksey,justen_walker,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"04/Jan/13 13:53;brandon.williams;5098.txt;https://issues.apache.org/jira/secure/attachment/12563288/5098.txt","02/Jan/13 17:08;justen_walker;pig.log;https://issues.apache.org/jira/secure/attachment/12562923/pig.log","02/Jan/13 17:26;justen_walker;test_schema.cli;https://issues.apache.org/jira/secure/attachment/12562927/test_schema.cli",,,,,,,,,,,,,,,,,,3.0,brandon.williams,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302253,,,Fri Jan 04 17:13:14 UTC 2013,,,,,,,,,,"0|i16zpj:",248920,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"02/Jan/13 17:08;justen_walker;Add Stack Trace;;;","02/Jan/13 17:26;justen_walker;Attach test_schema;;;","04/Jan/13 13:53;brandon.williams;In widerow mode, we weren't decoding the key since we have to track it to recompose the row into a bag.  Simple patch to decode the key when we add it to the tuple.;;;","04/Jan/13 16:34;aleksey;+1;;;","04/Jan/13 17:13;brandon.williams;Committed.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
cassandra-shuffle fails as system keyspace is not user-modifiable,CASSANDRA-5097,12625687,Bug,Resolved,CASSANDRA,Cassandra,software,mck,"<p>The Cassandra Project is a distributed storage system for managing structured/unstructured data while providing reliability at a massive scale.</p>
<br/>
<p>Please request the <b>creation of a jira account</b> on either the <a href=""https://cassandra.apache.org/_/community.html#discussions"">dev@cassandra.apache.org</a> mailing list or the ASF slack channel <a href=""https://cassandra.apache.org/_/community.html#discussions"">#cassandra-dev</a></p>",http://cassandra.apache.org/,Normal,Fixed,jihartik,jihartik,jihartik,02/Jan/13 15:04,16/Apr/19 09:32,14/Jul/23 05:53,02/Jan/13 18:20,1.2.1,,,Legacy/Tools,,,0,,,,,"cassandra-shuffle tool fails to insert calculated relocations into the system keyspace as it is not user-modifiable. When run, the following exception is thrown after printing out the list of relocations for the first node in ring:

Exception in thread ""main"" java.lang.RuntimeException: InvalidRequestException(why:system keyspace is not user-modifiable.)
        at org.apache.cassandra.tools.Shuffle.executeCqlQuery(Shuffle.java:516)
        at org.apache.cassandra.tools.Shuffle.shuffle(Shuffle.java:359)
        at org.apache.cassandra.tools.Shuffle.main(Shuffle.java:678)
Caused by: InvalidRequestException(why:system keyspace is not user-modifiable.)
        at org.apache.cassandra.thrift.Cassandra$execute_cql3_query_result.read(Cassandra.java:37849)
        at org.apache.thrift.TServiceClient.receiveBase(TServiceClient.java:78)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_execute_cql3_query(Cassandra.java:1562)
        at org.apache.cassandra.thrift.Cassandra$Client.execute_cql3_query(Cassandra.java:1547)
        at org.apache.cassandra.tools.CassandraClient.execute_cql_query(Shuffle.java:733)
        at org.apache.cassandra.tools.Shuffle.executeCqlQuery(Shuffle.java:502)
        ... 2 more

By quickly checking the code it seems that the patch set for CASSANDRA-4874 disallows modifications to system keyspace again (they were previously allowed by CASSANDRA-4664) thus rendering cassandra-shuffle unable to do its job.",,aleksey,jihartik,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,"02/Jan/13 15:52;jihartik;CASSANDRA-5097.patch;https://issues.apache.org/jira/secure/attachment/12562917/CASSANDRA-5097.patch",,,,,,,,,,,,,,,,,,,,1.0,jihartik,,,,,,,,,,,,,,,,,,,,,,false,,,,,,,,,,,,,,,,,302234,,,Wed Jan 02 18:19:28 UTC 2013,,,,,,,,,,"0|i16zlb:",248901,,,,,,,,,aleksey,,aleksey,Normal,,,,,,,,,,,,,,,,,,"02/Jan/13 15:24;jbellis;We should allow modifying system table contents, but not changing the schema.;;;","02/Jan/13 15:53;jihartik;Added patch that fixes the problem by allowing also MODIFY permission for system KS operations (applied against 1.2.0).;;;","02/Jan/13 18:19;aleksey;Can't switch assignee/reviewer for some reason..
anyway, +1 and committed Jouni's patch - thanks, Jouni.;;;",,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,,
