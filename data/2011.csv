Issue Type,Issue key,Issue id,Summary,Description,Assignee,Reporter,Priority,Status,Resolution,Created,Updated
Bug,ZOOKEEPER-973,12494681,bind() could fail on Leader because it does not setReuseAddress on its ServerSocket ,"setReuseAddress(true) should be used below.

    Leader(QuorumPeer self,LeaderZooKeeperServer zk) throws IOException {
        this.self = self;
        try {
            ss = new ServerSocket(self.getQuorumAddress().getPort());
        } catch (BindException e) {
            LOG.error(""Couldn't bind to port ""
                    + self.getQuorumAddress().getPort(), e);
            throw e;
        }
        this.zk=zk;
    }

",qwertymaniac,vishalmlst,Trivial,Resolved,Fixed,05/Jan/11 06:47,24/Jan/12 10:59
Bug,ZOOKEEPER-975,12495589,new peer goes in LEADING state even if ensemble is online,"Scenario:
1. 2 of the 3 ZK nodes are online
2. Third node is attempting to join
3. Third node unnecessarily goes in ""LEADING"" state
4. Then third goes back to LOOKING (no majority of followers) and finally goes to FOLLOWING state.


While going through the logs I noticed that a peer C that is trying to
join an already formed cluster goes in LEADING state. This is because
QuorumCnxManager of A and B sends the entire history of notification
messages to C. C receives the notification messages that were
exchanged between A and B when they were forming the cluster.

In FastLeaderElection.lookForLeader(), due to the following piece of
code, C quits lookForLeader assuming that it is supposed to lead.

740                             //If have received from all nodes, then terminate
741                             if ((self.getVotingView().size() == recvset.size()) &&
742                                     (self.getQuorumVerifier().getWeight(proposedLeader) != 0)){
743                                 self.setPeerState((proposedLeader == self.getId()) ?
744                                         ServerState.LEADING: learningState());
745                                 leaveInstance();
746                                 return new Vote(proposedLeader, proposedZxid);
747
748                             } else if (termPredicate(recvset,


This can cause:
1.  C to unnecessarily go in LEADING state and wait for tickTime * initLimit and then restart the FLE.

2. C waits for 200 ms (finalizeWait) and then considers whatever
notifications it has received to make a decision. C could potentially
decide to follow an old leader, fail to connect to the leader, and
then restart FLE. See code below.

752                             if (termPredicate(recvset,
753                                     new Vote(proposedLeader, proposedZxid,
754                                             logicalclock))) {
755 
756                                 // Verify if there is any change in the proposed leader
757                                 while((n = recvqueue.poll(finalizeWait,
758                                         TimeUnit.MILLISECONDS)) != null){
759                                     if(totalOrderPredicate(n.leader, n.zxid,
760                                             proposedLeader, proposedZxid)){
761                                         recvqueue.put(n);
762                                         break;
763                                     }
764                                 }



In general, this does not affect correctness of FLE since C will
eventually go back to FOLLOWING state (A and B won't vote for
C). However, this delays C from joining the cluster. This can in turn
affect recovery time of an application.


Proposal: A and B should send only the latest notification (most
recent) instead of the entire history. Does this sound reasonable?



",vishalmlst,vishalmlst,Major,Closed,Fixed,14/Jan/11 12:29,23/Nov/11 19:22
Bug,ZOOKEEPER-976,12495845,ZooKeeper startup script doesn't use JAVA_HOME,"From bug filed on CDH: https://issues.cloudera.org/browse/DISTRO-47 - moving it to this jira to address:

------------------------------------------------------
Bug filed by ""grep.alex"" at http://getsatisfaction.com/cloudera/topics/cdh3b3_zookeeper_startup_script_doesnt_use_java_home

On RedHat 5 (using the RPM installer) I was able to install and run all the Hadoop components. The Zookeeper install was fine, but it wouldn't start:

{noformat}
[root@aholmes-desktop init.d]# ./hadoop-zookeeper start 
JMX enabled by default 
Using config: /etc/zookeeper/zoo.cfg 
Starting zookeeper ... 
STARTED 
[root@aholmes-desktop init.d]# Exception in thread ""main"" java.lang.NoSuchMethodError: method java.lang.management.ManagementFactory.getPlatformMBeanServer with signature ()Ljavax.management.MBeanServer; was not found. 
at org.apache.zookeeper.jmx.ManagedUtil.registerLog4jMBeans(ManagedUtil.java:48 
...
{noformat} 

After some digging around I found the cause - the Zookeeper startup script (/usr/lib/zookeeper/bin/zkServer.sh ) uses the java found in the path, whereas the other startup scripts use JAVA_HOME. In my case I had the default RHEL5 1.4 JDK in the path, and the 1.6 JDK RPM's installed under /usr/java, hence the above error, which I'm guessing is a fairly common setup.

In my opinion all the startup scripts should all use the same mechanism to determine where to pick java.",phunt,phunt,Minor,Closed,Fixed,18/Jan/11 01:36,23/Nov/11 19:22
Bug,ZOOKEEPER-981,12497380,Hang in zookeeper_close() in the multi-threaded C client,"I saw a hang once when my C++ application called the zookeeper_close() method of the multi-threaded Zookeeper client library.  The stack trace of the hung thread was the following:

{quote}
Thread 8 (Thread 5644):
#0  0x00007f5d7bb5bbe4 in __lll_lock_wait () from /lib/libpthread.so.0
#1  0x00007f5d7bb59ad0 in pthread_cond_broadcast@@GLIBC_2.3.2 () from /lib/libpthread.so.0
#2  0x00007f5d793628f6 in unlock_completion_list (l=0x32b4d68) at .../zookeeper/src/c/src/mt_adaptor.c:66
#3  0x00007f5d79354d4b in free_completions (zh=0x32b4c80, callCompletion=1, reason=-116) at .../zookeeper/src/c/src/zookeeper.c:1069
#4  0x00007f5d79355008 in cleanup_bufs (zh=0x32b4c80, callCompletion=1, rc=-116) at .../thirdparty/zookeeper/src/c/src/zookeeper.c:1125
#5  0x00007f5d79353200 in destroy (zh=0x32b4c80) at .../thirdparty/zookeeper/src/c/src/zookeeper.c:366
#6  0x00007f5d79358e0e in zookeeper_close (zh=0x32b4c80) at .../zookeeper/src/c/src/zookeeper.c:2326
#7  0x00007f5d79356d18 in api_epilog (zh=0x32b4c80, rc=0) at .../zookeeper/src/c/src/zookeeper.c:1661
#8  0x00007f5d79362f2f in adaptor_finish (zh=0x32b4c80) at .../zookeeper/src/c/src/mt_adaptor.c:205
#9  0x00007f5d79358c8c in zookeeper_close (zh=0x32b4c80) at .../zookeeper/src/c/src/zookeeper.c:2297 
...
{quote}

The omitted part of the stack trace is entirely within my application, and contains no other calls to/from the Zookeeper client.  In particular, I am not calling zookeeper_close() from within a completion handler or any of the library's threads.

I haven't been able to reproduce this, and when I encountered this I wasn't capturing logging from the client library, so unfortunately I don't have any more information at this time.  But I will update this JIRA if I see it again.",strib,strib,Critical,Closed,Fixed,01/Feb/11 20:23,23/Nov/11 19:22
Bug,ZOOKEEPER-983,12497537,running zkServer.sh start remotely using ssh hangs,"If zkServer.sh is run remotely using ssh as follows ssh will ""hang"" - i.e. not complete/return once the server is started. This is even though zkServer.sh starts the java vm in the background.

$ ssh <host> ""zkServer.sh start""

this is due to the following issue:

http://www.slac.stanford.edu/comp/unix/ssh_faq.html#logoff_hangs

",phunt,phunt,Minor,Closed,Fixed,03/Feb/11 05:21,23/Nov/11 19:22
Bug,ZOOKEEPER-985,12498164,Test BookieRecoveryTest fails on trunk.,The unit test fails on trunk on my mac. I think this might be the same on other platforms as well. Ill attach the error logs.,fpj,mahadev,Major,Closed,Fixed,09/Feb/11 19:24,23/Nov/11 19:22
Bug,ZOOKEEPER-994,12498934,"""eclipse"" target in the build script doesnot include libraray required for test classes in the classpath","The ""eclipse"" target in the zoo-keeper build script doesn't include the accessive.jar present in the folder /src/java/libtest in the .classpath file. But the accessive.jar is being referenced from a couple of test classes.
However, the build is successful :)
",mis,mis,Minor,Closed,Fixed,17/Feb/11 19:38,23/Nov/11 19:22
Bug,ZOOKEEPER-1006,12500327,"QuorumPeer ""Address already in use"" -- regression in 3.3.3","CnxManagerTest.testWorkerThreads 

See attachment, this is the first time I've seen this test fail, and it's failed 2 out of the last three test runs.

Notice (attachment) once this happens the port never becomes available.

{noformat}
2011-03-02 15:53:12,425 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11245:NIOServerCnxn$Factory@251] - Accepted socket connection from /172.29.6.162:51441
2011-03-02 15:53:12,430 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11245:NIOServerCnxn@639] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running
2011-03-02 15:53:12,430 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:11245:NIOServerCnxn@1435] - Closed socket connection for client /172.29.6.162:51441 (no session established for client)
2011-03-02 15:53:12,430 - WARN  [QuorumPeer:/0:0:0:0:0:0:0:0:11241:Follower@82] - Exception when following the leader
java.io.EOFException
	at java.io.DataInputStream.readInt(DataInputStream.java:375)
	at org.apache.jute.BinaryInputArchive.readInt(BinaryInputArchive.java:63)
	at org.apache.zookeeper.server.quorum.QuorumPacket.deserialize(QuorumPacket.java:84)
	at org.apache.jute.BinaryInputArchive.readRecord(BinaryInputArchive.java:108)
	at org.apache.zookeeper.server.quorum.Learner.readPacket(Learner.java:148)
	at org.apache.zookeeper.server.quorum.Learner.registerWithLeader(Learner.java:267)
	at org.apache.zookeeper.server.quorum.Follower.followLeader(Follower.java:66)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:645)
2011-03-02 15:53:12,431 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11241:Follower@165] - shutdown called
java.lang.Exception: shutdown Follower
	at org.apache.zookeeper.server.quorum.Follower.shutdown(Follower.java:165)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:649)
2011-03-02 15:53:12,432 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11241:QuorumPeer@621] - LOOKING
2011-03-02 15:53:12,432 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11241:FastLeaderElection@663] - New election. My id =  0, Proposed zxid = 0
2011-03-02 15:53:12,433 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 0 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 0 (n.sid), LOOKING (my state)
2011-03-02 15:53:12,433 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 0 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 0 (n.sid), LOOKING (my state)
2011-03-02 15:53:12,433 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 0 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 0 (n.sid), LOOKING (my state)
2011-03-02 15:53:12,633 - INFO  [WorkerReceiver Thread:FastLeaderElection@496] - Notification: 0 (n.leader), 0 (n.zxid), 2 (n.round), LOOKING (n.state), 0 (n.sid), LOOKING (my state)
2011-03-02 15:53:12,633 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11245:QuorumPeer@655] - LEADING
2011-03-02 15:53:12,636 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11245:Leader@54] - TCP NoDelay set to: true
2011-03-02 15:53:12,638 - INFO  [QuorumPeer:/0:0:0:0:0:0:0:0:11245:ZooKeeperServer@151] - Created server with tickTime 1000 minSessionTimeout 2000 maxSessionTimeout 20000 datadir /var/lib/hudson/workspace/CDH3-ZooKeeper-3.3.3_sles/build/test/tmp/test9001250572426375869.junit.dir/version-2 snapdir /var/lib/hudson/workspace/CDH3-ZooKeeper-3.3.3_sles/build/test/tmp/test9001250572426375869.junit.dir/version-2
2011-03-02 15:53:12,639 - ERROR [QuorumPeer:/0:0:0:0:0:0:0:0:11245:Leader@133] - Couldn't bind to port 11245
java.net.BindException: Address already in use
	at java.net.PlainSocketImpl.socketBind(Native Method)
	at java.net.PlainSocketImpl.bind(PlainSocketImpl.java:365)
	at java.net.ServerSocket.bind(ServerSocket.java:319)
	at java.net.ServerSocket.<init>(ServerSocket.java:185)
	at java.net.ServerSocket.<init>(ServerSocket.java:97)
	at org.apache.zookeeper.server.quorum.Leader.<init>(Leader.java:131)
	at org.apache.zookeeper.server.quorum.QuorumPeer.makeLeader(QuorumPeer.java:512)
	at org.apache.zookeeper.server.quorum.QuorumPeer.run(QuorumPeer.java:657)
{noformat}
",phunt,phunt,Minor,Closed,Fixed,03/Mar/11 17:38,23/Nov/11 19:22
Bug,ZOOKEEPER-1007,12500477,iarchive leak in C client,"On line 1957, zookeeper_process() returns without cleaning up the ""ia"" buffer that was previously allocated.  I don't know how often this code path is taken, but I thought it was worth reporting.  I will attach a simple patch shortly.",strib,strib,Minor,Closed,Fixed,04/Mar/11 21:42,23/Nov/11 19:22
Bug,ZOOKEEPER-1013,12501192,zkServer.sh usage message should mention all startup options,"currently the ""Usage"" message for zkServer shows:

  echo ""Usage: $0 {start|stop|restart|status}"" 

But it seems to me that it should show the other startup options as well, which are currently: start-foreground, upgrade, print-cmd.

",ekoontz,ekoontz,Trivial,Closed,Fixed,11/Mar/11 20:28,23/Nov/11 19:22
Bug,ZOOKEEPER-1027,12502255,chroot not transparent in zoo_create(),"I've recently started to use the chroot functionality (introduced in
3.2.0) as part of my connect string.It mostly works as expected, but
there is one case that is unexpected: when I create a path with
zoo_create() I can retrieve the created path. This is very useful when
you set the ZOO_SEQUENCE flag. Unfortunately the returned path
includes the chroot as part of the path. This was unexpected to me: I
expected that the chroot would be totally transparent. The
documentation for zoo_create() says:
""path_buffer : Buffer which will be filled with the path of the new
node (this might be different than the supplied path because of the
ZOO_SEQUENCE flag).""

This gave me the impression that this flag is the only reason the
returned path is different from the created path, but apparently it's
not. Is this a bug or intended behavior? 
I workaround this issue now by remembering the chroot in
my wrapper code and after a call to zoo_create() i check if the returned
path starts with the chroot. If it does, I remove it.

My use case is to create a path with a sequence number and then delete
this path later. Unfortunately I cannot delete the path because it has
the chroot prepended to it, and thus it will result in two chroots.

I believe this only affects the create functions.",tt,tt,Critical,Closed,Fixed,24/Mar/11 05:06,28/Sep/15 17:33
Bug,ZOOKEEPER-1028,12502320,"In python bindings, zookeeper.set2() should return a stat dict but instead returns None","There is a small bug in the python bindings, specifically with the zookeeper.set2() call. This method should return a stat dictionary, but actually returns None. The fix is a one-character change to zookeeper.c such that the return value is '&stat' rather than 'stat'.",cmedaglia,cmedaglia,Minor,Closed,Fixed,24/Mar/11 20:26,23/Nov/11 19:22
Bug,ZOOKEEPER-1029,12502418,C client bug in zookeeper_init (if bad hostname is given),"If you give invalid hostname to zookeeper_init method, it's not able to resolve it, and it tries to do the cleanup (free buffer/completion lists/etc) . The adaptor_init() is not called for this code path, so the lock,cond variables (for adaptor, completion lists) are not initialized.

As part of the cleanup it's trying to clean up some buffers and acquires locks and unlocks (where the locks have not yet been initialized, so unlocking fails) 
    lock_completion_list(&zh->sent_requests); - pthread_mutex/cond not initialized
    tmp_list = zh->sent_requests;
    zh->sent_requests.head = 0;
    zh->sent_requests.last = 0;
    unlock_completion_list(&zh->sent_requests);   trying to broadcast here on uninitialized cond

It should do error checking to see if locking succeeds before unlocking it. If Locking fails, then appropriate error handling has to be done.",fpj,dheerajagrawal,Blocker,Closed,Fixed,25/Mar/11 20:47,25/Dec/18 09:42
Bug,ZOOKEEPER-1033,12502520,"c client should install includes into INCDIR/zookeeper, not INCDIR/c-client-src","header files are installed into foo/include/c-client-src/, which doesn't indicate a relationship with zookeeper and doesn't correspond to foo/lib/libzookeeper*

header files should be installed into foo/include/zookeeper/ as this is the common practice.",nrh,nrh,Minor,Closed,Fixed,27/Mar/11 20:40,23/Nov/11 19:22
Bug,ZOOKEEPER-1034,12502521,perl bindings should automatically find the zookeeper c-client headers,"Installing Net::ZooKeeper from cpan or the zookeeper distribution tarballs will always fail due to not finding c-client header files.  In conjunction with ZOOKEEPER-1033 update perl bindings to look for c-client header files in INCDIR/zookeeper/

a.k.a. make installs of Net::ZooKeeper via cpan/cpanm/whatever *just work*, assuming you've already got the zookeeper c client installed.",nrh,nrh,Minor,Closed,Fixed,27/Mar/11 20:45,23/Nov/11 19:22
Bug,ZOOKEEPER-1046,12504115,Creating a new sequential node results in a ZNODEEXISTS error,"On several occasions, I've seen a create() with the sequential flag set fail with a ZNODEEXISTS error, and I don't think that should ever be possible.  In past runs, I've been able to closely inspect the state of the system with the command line client, and saw that the parent znode's cversion is smaller than the sequential number of existing children znode under that parent.  In one example:

{noformat}
[zk:<ip:port>(CONNECTED) 3] stat /zkrsm
cZxid = 0x5
ctime = Mon Jan 17 18:28:19 PST 2011
mZxid = 0x5
mtime = Mon Jan 17 18:28:19 PST 2011
pZxid = 0x1d819
cversion = 120710
dataVersion = 0
aclVersion = 0
ephemeralOwner = 0x0
dataLength = 0
numChildren = 2955
{noformat}

However, the znode /zkrsm/000000000000002d_record0000120804 existed on disk.

In a recent run, I was able to capture the Zookeeper logs, and I will attach them to this JIRA.  The logs are named as nodeX.<zxid_prefixes>.log, and each new log represents an application process restart.

Here's the scenario:

# There's a cluster with nodes 1,2,3 using zxid 0x3.
# All three nodes restart, forming a cluster of zxid 0x4.
# Node 3 restarts, leading to a cluster of 0x5.

At this point, it seems like node 1 is the leader of the 0x5 epoch.  In its log (node1.0x4-0x5.log) you can see the first (of many) instances of the following message:

{noformat}
2011-04-11 21:16:12,607 16649 [ProcessThread:-1] INFO org.apache.zookeeper.server.PrepRequestProcessor  - Got user-level KeeperException when processing sessionid:0x512f466bd44e0002 type:create cxid:0x4da376ab zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a Error Path:/zkrsm/00000000000000b2_record0001761440 Error:KeeperErrorCode = NodeExists for /zkrsm/00000000000000b2_record0001761440
{noformat}

This then repeats forever as my application isn't expecting to ever get this error message on a sequential node create, and just continually retries.  The message even transfers over to node3.0x5-0x6.log once the 0x6 epoch comes into play.

I don't see anything terribly fishy in the transition between the epochs; the correct snapshots seem to be getting transferred, etc.  Unfortunately I don't have a ZK snapshot/log that exhibits the problem when starting with a fresh system.

Some oddities you might notice in these logs:
* Between epochs 0x3 and 0x4, the zookeeper IDs of the nodes changed due to a bug in our application code.  (They are assigned randomly, but are supposed to be consistent across restarts.)
* We manage node membership dynamically, and our application restarts the ZooKeeperServer classes whenever a new node wants to join (without restarting the entire application process).  This is why you'll see messages like the following in node1.0x4-0x5.log before a new election begins:
{noformat}
2011-04-11 21:16:00,762 4804 [QuorumPeer:/0.0.0.0:2888] INFO org.apache.zookeeper.server.quorum.Learner  - shutdown called
{noformat}
* There is in fact one of these dynamic membership changes in node1.0x4-0x5.log, just before the 0x4 epoch is formed.  I'm not sure how this would be related though, as no transactions are done during this period.",vishalmlst,strib,Blocker,Closed,Fixed,12/Apr/11 22:24,23/Nov/11 19:22
Bug,ZOOKEEPER-1048,12504153,addauth command does not work in cli_mt/cli_st,"I can not operation a node with ACL by ""addauth"" when using cli_st. I have fixed this bug: 
original：else if (startsWith(line, ""addauth "")) {
      char *ptr;
      line += 8;
      ptr = strchr(line, ' ');
      if (ptr) {
        *ptr = '\0';
        ptr++;
      }
      zoo_add_auth(zh, line, ptr, ptr ? strlen(ptr) -1 : 0, NULL, NULL);
now: zoo_add_auth(zh, line, ptr, ptr ? strlen(ptr) : 0, NULL, NULL);
strlen(ptr) is just ok.",njuicsgz,njuicsgz,Major,Resolved,Fixed,13/Apr/11 09:40,03/Mar/16 01:36
Bug,ZOOKEEPER-1049,12504432,Session expire/close flooding renders heartbeats to delay significantly,"Let's say we have 100 clients (group A) already connected to three-node ZK ensemble with session timeout of 15 second.  And we have 1000 clients (group B) already connected to the same ZK ensemble, all watching several nodes (with 15 second session timeout)

Consider a case in which All clients in group B suddenly hung or deadlocked (JVM OOME) all at the same time. 15 seconds later, all sessions in group B gets expired, creating session closing stampede. Depending on the number of this clients in group B, all request/response ZK ensemble should process get delayed up to 8 seconds (1000 clients we have tested).

This delay causes some clients in group A their sessions expired due to delay in getting heartbeat response. This causes normal servers to drop out of clusters. This is a serious problem in our installation, since some of our services running batch servers or CI servers creating the same scenario as above almost everyday.

I am attaching a graph showing ping response time delay.

I think ordering of creating/closing sessions and ping exchange isn't important (quorum state machine). at least ping request / response should be handle independently (different queue and different thread) to keep realtime-ness of ping.

As a workaround, we are raising session timeout to 50 seconds.
But this causes max. failover of cluster to significantly increased, thus initial QoS we promised cannot be met.







",tru64ufs,tru64ufs,Critical,Closed,Fixed,16/Apr/11 03:42,23/Nov/11 19:22
Bug,ZOOKEEPER-1050,12504838,zooinspector shell scripts do not work,"* zooInspector-dev.sh uses DOS line endings.  Dash at least chokes on this.
* zooInspector.sh has an errant ; in the classpath.

Also there really isn't a reason to hard code the zookeeper version needed in lib. Just use a glob.
",willjohnson3,cburroughs,Trivial,Resolved,Fixed,21/Apr/11 00:46,06/Jan/12 10:57
Bug,ZOOKEEPER-1051,12504889,SIGPIPE in Zookeeper 0.3.* when send'ing after cluster disconnection,"In libzookeeper_mt, if your process is going rather slowly (such as when running it in Valgrind's Memcheck) or you are using gdb with breakpoints, you can occasionally get SIGPIPE when trying to send a message to the cluster. For example:

==12788==
==12788== Process terminating with default action of signal 13 (SIGPIPE)
==12788==    at 0x3F5180DE91: send (in /lib64/libpthread-2.5.so)
==12788==    by 0x7F060AA: ??? (in /usr/lib64/libzookeeper_mt.so.2.0.0)
==12788==    by 0x7F06E5B: zookeeper_process (in /usr/lib64/libzookeeper_mt.so.2.0.0)
==12788==    by 0x7F0D38E: ??? (in /usr/lib64/libzookeeper_mt.so.2.0.0)
==12788==    by 0x3F5180673C: start_thread (in /lib64/libpthread-2.5.so)
==12788==    by 0x3F50CD3F6C: clone (in /lib64/libc-2.5.so)
==12788==

This is probably not the behavior we would like, since we handle server disconnections after a failed call to send. To fix this, there are a few options we could use. For BSD environments, we can tell a socket to never send SIGPIPE with send using setsockopt:

setsockopt(sd, SOL_SOCKET, SO_NOSIGPIPE, (void *)&set, sizeof(int));

For Linux environments, we can add a MSG_NOSIGNAL flag to every send call that says to not send SIGPIPE on a bad file descriptor.

For more information, see: http://stackoverflow.com/questions/108183/how-to-prevent-sigpipes-or-handle-them-properly",tyree731,tyree731,Minor,Closed,Fixed,21/Apr/11 13:56,23/Nov/11 19:21
Bug,ZOOKEEPER-1052,12505075,Findbugs warning in QuorumPeer.ResponderThread.run(),"{noformat}
REC 	Exception is caught when Exception is not thrown in org.apache.zookeeper.server.quorum.QuorumPeer$ResponderThread.run()
{noformat}",fpj,fpj,Major,Closed,Fixed,24/Apr/11 13:45,23/Nov/11 19:22
Bug,ZOOKEEPER-1055,12505247,check for duplicate ACLs in addACL() and create(),"actual result:


[zk: (CONNECTED) 0] create /test2 'test2' digest:test:test:cdrwa,digest:test:test:cdrwa
Created /test2
[zk: (CONNECTED) 1] getAcl /test2
'digest,'test:test
: cdrwa
'digest,'test:test
: cdrwa
[zk: (CONNECTED) 2]

but getAcl should only have a single entry.",ekoontz,ekoontz,Major,Closed,Fixed,26/Apr/11 21:08,23/Nov/11 19:22
Bug,ZOOKEEPER-1057,12506002,"zookeeper c-client, connection to offline server fails to successfully fallback to second zk host","Hello, I'm a contributor for the node.js zookeeper module: https://github.com/yfinkelstein/node-zookeeper
i'm using zk 3.3.3 for the purposes of this issue, but i have validated it fails on 3.3.1 and 3.3.2

i'm having an issue when trying to connect when one of my zookeeper servers is offline.
if the first server attempted is online, all is good.

if the offline server is attempted first, then the client is never able to connect to _any_ server.
inside zookeeper.c a connection loss (-4) is received, the socket is closed and buffers are cleaned up, it then attempts the next server in the list, creates a new socket (which gets the same fd as the previously closed socket) and connecting fails, and it continues to fail seemingly forever.
The nature of this ""fail"" is not that it gets -4 connection loss errors, but that zookeeper_interest doesn't find anything going on on the socket before the user provided timeout kicks things out. I don't want to have to wait 5 minutes, even if i could make myself.

this is the message that follows the connection loss:
2011-04-27 23:18:28,355:13485:ZOO_ERROR@handle_socket_error_msg@1530: Socket [127.0.0.1:5020] zk retcode=-7, errno=60(Operation timed out): connection timed out (exceeded timeout by 3ms)
2011-04-27 23:18:28,355:13485:ZOO_ERROR@yield@213: yield:zookeeper_interest returned error: -7 - operation timeout

While investigating, i decided to comment out close(zh->fd) in handle_error (zookeeper.c#1153)
now everything works (obviously i'm leaking an fd). Connection the the second host works immediately.
this is the behavior i'm looking for, though i clearly don't want to leak the fd, so i'm wondering why the fd re-use is causing this issue.
close() is not returning an error (i checked even though current code assumes success).

i'm on osx 10.6.7
i tried adding a setsockopt so_linger (though i didn't want that to be a solution), it didn't work.

full debug traces are included in issue here: https://github.com/yfinkelstein/node-zookeeper/issues/6
",michim,woody.anderson@gmail.com,Blocker,Closed,Fixed,03/May/11 01:16,13/Mar/14 18:17
Bug,ZOOKEEPER-1058,12506122,fix typo in opToString for getData,fix Request getData to print that instead of getDate,fournc,fournc,Trivial,Closed,Fixed,04/May/11 00:34,23/Nov/11 19:22
Bug,ZOOKEEPER-1059,12506158,stat command isses on non-existing node causes NPE ,"*stat* command issues on non existing zookeeper node,causes NPE to the client.
{noformat}
[zk: localhost:2181(CONNECTED) 2] stat /invalidPath
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.zookeeper.ZooKeeperMain.printStat(ZooKeeperMain.java:131)
        at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:723)
        at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:582)
        at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:354)
        at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:312)
        at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:271)

{noformat}",kam_iitkgp,kam_iitkgp,Major,Closed,Fixed,04/May/11 10:50,23/Nov/11 19:22
Bug,ZOOKEEPER-1060,12506759,QuorumPeer takes a long time to shutdown,"This problem is seen only if you have ZooKeeper embedded in your application. QuorumPeerMain.initializeAndRun() does a quorumPeer.join() before exiting.

QuorumPeer.shutdown() tries to cleanup everything, but it does not interrupt itself. As a result, a if the peer is running FLE, it might be waiting to receive notifications (recvqueue.poll()) in FastLeaderElection. Therefore, quorumPeer.join() will wait until the peer wakes up from poll().

The fix is simple - call this.interrupt() in QuorumPeer.shutdown().",vishalmlst,vishalmlst,Minor,Closed,Fixed,10/May/11 19:32,23/Nov/11 19:22
Bug,ZOOKEEPER-1061,12506765,Zookeeper stop fails if start called twice,"The zkServer.sh script doesn't check properly to see if a previously started
server is still running.  If you call start twice, the second invocation
will over-write the PID file with a process that then fails due to port
occupancy.

This means that stop will subsequently fail.

Here is a reference that describes how init scripts should normally work:

http://refspecs.freestandards.org/LSB_3.1.0/LSB-Core-generic/LSB-Core-generic/iniscrptact.html

",tdunning,tdunning,Major,Closed,Fixed,10/May/11 20:38,30/Mar/17 14:27
Bug,ZOOKEEPER-1062,12507081,Net-ZooKeeper: Net::ZooKeeper consumes 100% cpu on wait,"Reported by a user on the CDH user list (user reports that the listed fix addressed this issue for him): 

""Net::ZooKeeper consumes 100% cpu when ""wait"" is used. At my initial inspection, it seems to be related to implementation mistake in pthread_cond_timedwait.""

https://rt.cpan.org/Public/Bug/Display.html?id=61290
",botond.hejj,phunt,Major,Resolved,Fixed,13/May/11 05:50,20/May/14 11:09
Bug,ZOOKEEPER-1063,12507535,Dubious synchronization in Zookeeper and ClientCnxnSocketNIO classes,"Synchronization around dataWatches, existWatches and childWatches in Zookeeper is incorrect.
Synchronization around outgoingQueue and pendingQueue in ClientCnxnSocketNIO is incorrect.
Synchronization around selector and key sets in ClientCnxnSocketNIO seems odd.",ydufresne,ydufresne,Critical,Closed,Fixed,17/May/11 21:34,23/Nov/11 19:22
Bug,ZOOKEEPER-1068,12508118,Documentation and default config suggest incorrect location for Zookeeper state,"Documentation and default config suggest /var/zookeeper as a value for dataDir. This practice is, strictly speaking, incompatible with UNIX/Linux filesystem layout standards (e.g. http://www.s-gms.ms.edus.si/cgi-bin/man-cgi?filesystem+5 , http://tldp.org/LDP/Linux-Filesystem-Hierarchy/html/index.html  ). 

Even though Zookeeper use is not limited to UNIX-like OSes I'd recommend that we change references to /var/zookeeper to /var/lib/zookeeper",rvs,rvs,Minor,Closed,Fixed,23/May/11 23:08,23/Nov/11 19:22
Bug,ZOOKEEPER-1069,12508120,Calling shutdown() on a QuorumPeer too quickly can lead to a corrupt log,"I've only seen this happen once.  In order to restart Zookeeper with a new set of servers, we have a wrapper class that calls shutdown() on an existing QuorumPeer, and then starts a new one with a new set of servers.  Specifically, our shutdown code looks like this:

{code}
  synchronized(_quorum_peer) {
    _quorum_peer.shutdown();
    FastLeaderElection fle = (FastLeaderElection) _quorum_peer.getElectionAlg();
    fle.shutdown();  // I think this is unnecessary
    try {
      _quorum_peer.getTxnFactory().commit();
    } catch (java.nio.channels.ClosedChannelException e) {
      // ignore
    }
  }
{code}

One time, our wrapper class started one QuorumPeer, and then had to shut it down and start a new one very soon after the QuorumPeer transitioned into a FOLLOWING state.  When the new QuorumPeer tried to read in the latest log from disk, it encountered a bogus magic number of all zeroes:

{noformat}
2011-05-18 22:42:29,823 10467 [pool-1-thread-2] FATAL org.apache.zookeeper.server.quorum.QuorumPeer  - Unable to load database on disk
java.io.IOException: Transaction log: /var/cloudnet/data/zookeeper/version-2/log.700000001 has invalid magic number 0 != 1514884167
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:510)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:527)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:493)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:576)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:479)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:454)
        at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:325)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:126)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398)
...
2011-05-18 22:42:29,823 10467 [pool-1-thread-2] ERROR com.nicira.onix.zookeeper.Zookeeper  - Unexpected exception
java.lang.RuntimeException: Unable to run quorum server 
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:401)
        at com.nicira.onix.zookeeper.Zookeeper.StartZookeeper(Zookeeper.java:198)
        at com.nicira.onix.zookeeper.Zookeeper.RestartZookeeper(Zookeeper.java:277)
        at com.nicira.onix.zookeeper.ZKRPCService.setServers(ZKRPC.java:83)
        at com.nicira.onix.zookeeper.Zkrpc$ZKRPCService.callMethod(Zkrpc.java:8198)
        at com.nicira.onix.rpc.RPC$10.run(RPC.java:534)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Transaction log: /var/cloudnet/data/zookeeper/version-2/log.700000001 has invalid magic number 0 != 1514884167
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.inStreamCreated(FileTxnLog.java:510)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.createInputArchive(FileTxnLog.java:527)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.goToNextLog(FileTxnLog.java:493)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.next(FileTxnLog.java:576)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.init(FileTxnLog.java:479)
        at org.apache.zookeeper.server.persistence.FileTxnLog$FileTxnIterator.<init>(FileTxnLog.java:454)
        at org.apache.zookeeper.server.persistence.FileTxnLog.read(FileTxnLog.java:325)
        at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:126)
        at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:222)
        at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:398)
        ... 8 more
{noformat}

I looked into the code a bit, and I believe the problem comes from the fact that QuorumPeer.shutdown() does not join() on this before returning.  Here's the scenario I think can happen:

# QuorumPeer.run() notices it is in the FOLLOWING state, makes a new Follower, and calls Follower.followLeader(), which starts connecting to the leader.
# In the main program thread, QuorumPeer.shutdown() is called.
# Through a complicated series of calls, this eventually leads to FollowerZooKeeperServer.shutdown() being called.
# This method calls SyncRequestProcess.shutdown(), which joins on this and returns.  However, it's possible that the SyncRequestProcessor thread hasn't yet been started because followLeader() hasn't yet called Learner.syncWithLeader(), which hasn't yet called ZooKeeperServer.startup(), which actually starts the thread.  Thus, the join would have no request, though a requestOfDeath is added to the queued requests list (possibly behind other requests).
# Back in the main thread, FileTxnSnapLog.commit() is called, which doesn't do much because the processor hasn't processed anything yet.
# Finally, ZooKeeperServer.startup is called in the QuorumPeer.run() thread, starting up the SyncRequestProcessor thread.
# That thread appends some request to the log.  The log doesn't exist yet, so it creates a new one, padding it with zeroes.
# Now either the SyncRequestProcessor hits the requestOfDeath or the whole QuorumPeer object is deleted.  It exits that thread without ever committing the log to disk (or the new QuorumPeer tries to read the log before the old thread gets to commit anything), and the log ends up with all zeroes instead of a proper magic number.

I haven't yet looked into whether there's an easy way to join() on the QuorumPeer thread from shutdown(), so that it won't go on to start the processor threads after it's been shutdown.  I wanted to check with the group first and see if anyone else agrees this could be a problem.

I marked this as minor since I think almost no one else uses Zookeeper this way, but it's pretty important to me personally.

I will upload a log file showing this behavior shortly.",vishalmlst,strib,Critical,Closed,Fixed,23/May/11 23:53,23/Nov/11 19:22
Bug,ZOOKEEPER-1073,12508336,address a documentation issue in ZOOKEEPER-1030,"ZOOKEEPER-1030 updated the generated docs, not the source docs. I'll submit a patch to address in the src.",phunt,phunt,Minor,Closed,Fixed,25/May/11 17:52,23/Nov/11 19:22
Bug,ZOOKEEPER-1074,12508341,"zkServer.sh is missing nohup/sleep, which are necessary for remote invocation","zkServer.sh is missing nohup and ""sleep 1"" when starting the background daemon.

This is fine normally, however when running the server remotely via ssh this causes the process to not run successfully (it starts but immediately exits).

I'll be submitting a patch for this shortly.
",phunt,phunt,Major,Closed,Fixed,25/May/11 18:27,23/Nov/11 19:22
Bug,ZOOKEEPER-1076,12508363,some quorum tests are unnecessarily extending QuorumBase,"Some tests are unnecessarily extending QuorumBase. Typically this is not a big issue, but it may cause more servers than necessary to be started (harder to debug a failing test in particular).
",phunt,phunt,Minor,Closed,Fixed,25/May/11 22:19,23/Nov/11 19:22
Bug,ZOOKEEPER-1077,12508402,C client lib doesn't build on Solaris,"Hello,

Some minor trouble with building ZooKeeper C client library on Sun^H^H^HOracle Solaris 5.10.

1. You need to link against ""-lnsl -lsocket""

2. ctime_r needs a buffer size. The signature is: ""char *ctime_r(const time_t *clock, char *buf, int buflen)""

3. In zk_log.c you need to manually cast pid_t to int (-Werror can be cumbersome ;) )

4. getpwuid_r()returns pointer to struct passwd, which works as the last parameter on Linux.

Solaris signature: struct passwd *getpwuid_r(uid_t  uid,  struct  passwd  *pwd, char *buffer, int  buflen); 
Linux signature: int getpwuid_r(uid_t uid, struct passwd *pwd, char *buf, size_t buflen, struct passwd **result);
",cnauroth,tkadlubo,Critical,Closed,Fixed,26/May/11 08:35,21/Jul/16 20:18
Bug,ZOOKEEPER-1083,12508845,Javadoc for WatchedEvent not being generated,See title.,ikelly,ikelly,Major,Closed,Fixed,31/May/11 16:23,23/Nov/11 19:22
Bug,ZOOKEEPER-1086,12508939,zookeeper test jar has non mavenised dependency.,"The zookeeper test jar, (zookeeper-<version>-test.jar) depends on accessive.jar which is not available in maven. This is problematic for projects using the test jar (i.e. hedwig). ",ikelly,ikelly,Major,Closed,Fixed,01/Jun/11 10:52,23/Nov/11 19:22
Bug,ZOOKEEPER-1087,12509385,"ForceSync VM arguement not working when set to ""no""","Cannot use forceSync=no to asynchronously write transaction logs. This is a critical bug, please address it ASAP. More details:

The class org.apache.zookeeper.server.persistence.FileTxnLog initializes forceSync property in a static block. However, the static variable is defined after the static block with a default value of true. Therefore, the value of the variable can never be false. Please move the declaration of the variable before the static block.",nputnam,ankitp129,Blocker,Closed,Fixed,06/Jun/11 20:09,23/Nov/11 19:22
Bug,ZOOKEEPER-1089,12509714,zkServer.sh status does not work due to invalid option of nc,"The nc command used by zkServer.sh does not have the ""-q"" option on some linux versions ( I have checked RedHat/Fedora and FreeBSD).",rvs,billa,Major,Resolved,Fixed,09/Jun/11 13:57,28/Dec/11 10:58
Bug,ZOOKEEPER-1090,12509718,Race condition while taking snapshot can lead to not restoring data tree correctly,"I think I have found a bug in the snapshot mechanism.

The problem occurs because dt.lastProcessedZxid is not synchronized (or rather set before the data tree is modified):

FileTxnSnapLog:
{code}
    public void save(DataTree dataTree,
            ConcurrentHashMap<Long, Integer> sessionsWithTimeouts)
        throws IOException {
        long lastZxid = dataTree.lastProcessedZxid;
        LOG.info(""Snapshotting: "" + Long.toHexString(lastZxid));
        File snapshot=new File(
                snapDir, Util.makeSnapshotName(lastZxid));
        snapLog.serialize(dataTree, sessionsWithTimeouts, snapshot);   <=== the Datatree may not have the modification for lastProcessedZxid
    }
{code}

DataTree:
{code}
    public ProcessTxnResult processTxn(TxnHeader header, Record txn) {
        ProcessTxnResult rc = new ProcessTxnResult();

        String debug = """";
        try {
            rc.clientId = header.getClientId();
            rc.cxid = header.getCxid();
            rc.zxid = header.getZxid();
            rc.type = header.getType();
            rc.err = 0;
            if (rc.zxid > lastProcessedZxid) {
                lastProcessedZxid = rc.zxid;
            }
            [...modify data tree...]           
 }
{code}
The lastProcessedZxid must be set after the modification is done.

As a result, if server crashes after taking the snapshot (and the snapshot does not contain change corresponding to lastProcessedZxid) restore will not restore the data tree correctly:
{code}
public long restore(DataTree dt, Map<Long, Integer> sessions,
            PlayBackListener listener) throws IOException {
        snapLog.deserialize(dt, sessions);
        FileTxnLog txnLog = new FileTxnLog(dataDir);
        TxnIterator itr = txnLog.read(dt.lastProcessedZxid+1); <=== Assumes lastProcessedZxid is deserialized
 }
{code}


I have had offline discussion with Ben and Camille on this. I will be posting the discussion shortly.",vishalmlst,vishalmlst,Critical,Closed,Fixed,09/Jun/11 14:24,23/Nov/11 19:22
Bug,ZOOKEEPER-1097,12510575,Quota is not correctly rehydrated on snapshot reload,traverseNode in DataTree will never actually traverse the limit nodes properly.,fournc,fournc,Blocker,Closed,Fixed,16/Jun/11 14:07,23/Nov/11 19:22
Bug,ZOOKEEPER-1100,12511062,Killed (or missing) SendThread will cause hanging threads,"After investigating an issues with [hanging threads|http://mail-archives.apache.org/mod_mbox/zookeeper-user/201106.mbox/%3Citpgb6$2mi$1@dough.gmane.org%3E] I noticed that any java.lang.Error might silently kill the SendThread. Without a SendThread any thread that wants to send something will hang forever. 

Currently nobody will recognize a SendThread that died. I think at least a state should be flipped (or flag should be set) that causes all further send attempts to fail or to re-spin the connection loop.
",fournc,gunnar,Major,Resolved,Fixed,21/Jun/11 09:24,03/Mar/16 01:37
Bug,ZOOKEEPER-1101,12511111,Upload zookeeper-test maven artifacts to maven repository.,"These are generated by ant package since ZOOKEEPER-1042, they just need to be pushed to a maven repo. Bookkeeper requires this package to build.",phunt,ikelly,Major,Closed,Fixed,21/Jun/11 17:15,23/Nov/11 19:22
Bug,ZOOKEEPER-1105,12511320,c client zookeeper_close not send CLOSE_OP request to server,"in zookeeper_close function,  do adaptor_finish before send CLOSE_OP request to server
so the CLOSE_OP request can not be sent to server

in server zookeeper.log have many
2011-06-22 00:23:02,323 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@634] - EndOfStreamException: Unable to read additional data from client sessionid 0x1305970d66d2224, likely client has closed socket
2011-06-22 00:23:02,324 - INFO  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:2181:NIOServerCnxn@1435] - Closed socket connection for client /10.250.8.123:60257 which had sessionid 0x1305970d66d2224
2011-06-22 00:23:02,325 - ERROR [CommitProcessor:1:NIOServerCnxn@445] - Unexpected Exception:
java.nio.channels.CancelledKeyException
        at sun.nio.ch.SelectionKeyImpl.ensureValid(SelectionKeyImpl.java:55)
        at sun.nio.ch.SelectionKeyImpl.interestOps(SelectionKeyImpl.java:59)
        at org.apache.zookeeper.server.NIOServerCnxn.sendBuffer(NIOServerCnxn.java:418)
        at org.apache.zookeeper.server.NIOServerCnxn.sendResponse(NIOServerCnxn.java:1509)
        at org.apache.zookeeper.server.FinalRequestProcessor.processRequest(FinalRequestProcessor.java:367)
        at org.apache.zookeeper.server.quorum.CommitProcessor.run(CommitProcessor.java:73)

and java client not have this problem",symat,rpggpr,Major,Closed,Fixed,23/Jun/11 06:05,14/Feb/20 15:23
Bug,ZOOKEEPER-1108,12511408,Various bugs in zoo_add_auth in C,"3 issues:
In zoo_add_auth: there is a race condition:
   2940     // [ZOOKEEPER-800] zoo_add_auth should return ZINVALIDSTATE if
   2941     // the connection is closed.
   2942     if (zoo_state(zh) == 0) {
   2943         return ZINVALIDSTATE;
   2944     }
when we do zookeeper_init, the state is initialized to 0 and above we check if state = 0 then throw exception.
There is a race condition where the doIo thread is slow and has not changed the state to CONNECTING, then you end up returning back ZKINVALIDSTATE.
The problem is we use 0 for CLOSED state and UNINITIALIZED state. in case of uninitialized case it should let it go through.

2nd issue:

Another Bug: in send_auth_info, the check is not correct

while (auth->next != NULL) { //--BUG: in cases where there is only one auth in the list, this will never send that auth, as its next will be NULL 
   rc = send_info_packet(zh, auth); 
   auth = auth->next; 
}

FIX IS:
do { 
  rc = send_info_packet(zh, auth); 
  auth = auth->next; 
 } while (auth != NULL); //this will make sure that even if there is one auth ,that will get sent.

3rd issue:
   2965     add_last_auth(&zh->auth_h, authinfo);
   2966     zoo_unlock_auth(zh);
   2967
   2968     if(zh->state == ZOO_CONNECTED_STATE || zh->state == ZOO_ASSOCIATING_STATE)
   2969         return send_last_auth_info(zh);

if it is connected, we only send the last_auth_info, which may be different than the one we added, as we unlocked it before sending it.

",dheerajagrawal,dheerajagrawal,Blocker,Closed,Fixed,23/Jun/11 21:02,23/Nov/11 19:22
Bug,ZOOKEEPER-1109,12511435,Zookeeper service is down when SyncRequestProcessor meets any exception.,"*Problem* Zookeeper is not shut down completely when dataDir disk space is full and ZK Cluster went into unserviceable state.
 

*Scenario*
If the leader zookeeper disk is made full, the zookeeper is trying to shutdown. But it is waiting indefinitely while shutting down the SyncRequestProcessor thread.

*Root Cause* 
this.join() is invoked in the same thread where System.exit(11) has been triggered.

When disk space full happens, It got the exception as follows 'No space left on device' and invoked System.exit(11) from the SyncRequestProcessor thread(The following logs shows the same). Before exiting JVM, ZK will execute the ShutdownHook of QuorumPeerMain and the flow comes to SyncRequestProcessor.shutdown(). Here this.join() is invoked in the same thread where System.exit(11) has been invoked.
",lakshman,lakshman,Critical,Closed,Fixed,24/Jun/11 04:48,23/Nov/11 19:22
Bug,ZOOKEEPER-1111,12512295,JMXEnv uses System.err instead of logging,"As stated in the title, org.apache.zookeeper.test.JMXEnv uses System.err.println to output traces. This makes for a lot of noise on the console when you run the tests. It has a logging object already, so it should use that instead.",ikelly,ikelly,Major,Closed,Fixed,30/Jun/11 08:51,23/Nov/11 19:22
Bug,ZOOKEEPER-1117,12512864,zookeeper 3.3.3 fails to build with gcc >= 4.6.1 on Debian/Ubuntu,"zookeeper 3.3.3 (and 3.3.1) fails to build on Debian and Ubuntu systems with gcc >= 4.6.1:

/bin/bash ./libtool  --tag=CC   --mode=compile gcc -DHAVE_CONFIG_H -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O2 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c -o zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.c
libtool: compile:  gcc -DHAVE_CONFIG_H -I. -I./include -I./tests -I./generated -Wall -Werror -g -O2 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c  -fPIC -DPIC -o .libs/zookeeper.o
src/zookeeper.c: In function 'getaddrs':
src/zookeeper.c:455:13: error: variable 'port' set but not used [-Werror=unused-but-set-variable]
cc1: all warnings being treated as errors

See http://bugs.debian.org/cgi-bin/bugreport.cgi?bug=625441 for more information.",javacruft,javacruft,Minor,Closed,Fixed,05/Jul/11 14:50,23/Nov/11 19:22
Bug,ZOOKEEPER-1119,12513120,zkServer stop command incorrectly reading comment lines in zoo.cfg,"Hello, adding the following commented-out dataDir to the zoo.cfg file (keeping the default one provided active):

{noformat}
# the directory where the snapshot is stored.
# dataDir=test123/data
dataDir=/export/crawlspace/mahadev/zookeeper/server1/data
{noformat}

and then running sh zkServer.sh stop is showing that the program is incorrectly reading the commented-out dataDir:

{noformat}
gmazza@gmazza-work:~/dataExt3/apps/zookeeper-3.3.3/bin$ sh zkServer.sh stop
JMX enabled by default
Using config: /media/NewDriveExt3_/apps/zookeeper-3.3.3/bin/../conf/zoo.cfg
Stopping zookeeper ... 
error: could not find file test123/data
/export/crawlspace/mahadev/zookeeper/server1/data/zookeeper_server.pid
gmazza@gmazza-work:~/dataExt3/apps/zookeeper-3.3.3/bin$ 
{noformat}

If I change the commented-out line in zoo.cfg to ""test123456/data"" and run the stop command again I get:
error: could not find file test123456/data

showing that it's incorrectly doing a run-time read of the commented-out lines.  (Difficult to completely confirm, but this problem  doesn't appear to occur with the start command, only the stop one.)
",phunt,gmazza,Major,Closed,Fixed,07/Jul/11 10:33,23/Nov/11 19:22
Bug,ZOOKEEPER-1124,12513948,Multiop submitted to non-leader always fails due to timeout,"The new Multiop support added under zookeeper-965 fails every single time if the multiop is submitted to a non-leader in quorum mode. In standalone mode it always works properly and this bug only presents itself in quorum mode (with 2 or more nodes). After 12 hours of debugging (*sigh*) it turns out to be a really simple fix. There are a couple of missing case statements inside FollowerRequestProcessor.java and ObserverRequestProcessor.java to ensure that multiop is forwarded to the leader for commit. I've attached a patch that fixes this problem.

It's probably worth nothing that zookeeper-965 has already been committed to trunk. But this is a fatal flaw that will prevent multiop support from working properly and as such needs to get committed to 3.4.0 as well. Is there a way to tie these two cases together in some way?",marshall,marshall,Critical,Closed,Fixed,13/Jul/11 16:17,23/Nov/11 19:22
Bug,ZOOKEEPER-1128,12514611,Recipe wrong for Lock process.,"http://zookeeper.apache.org/doc/trunk/recipes.html
The current recipe for Lock has the wrong process.
Specifically, for the 
""4. The client calls exists( ) with the watch flag set on the path in the lock directory with the next lowest sequence number.""
It shouldn't be the ""the next lowest sequence number"". It should be the ""current lowest path"". 

If you're gonna use ""the next lowest sequence number"", you'll never wait for the lock possession.

The following is the test code:

{code:title=LockTest.java|borderStyle=solid}
        ACL acl = new ACL(Perms.ALL, new Id(""10.0.0.0/8"", ""1""));
        List<ACL> acls = new ArrayList<ACL>();
        acls.add(acl);
        String connectStr = ""localhost:2181"";
        final Semaphore sem = new Semaphore(0);
        ZooKeeper zooKeeper = new ZooKeeper(connectStr, 1000 * 30, new Watcher() {

            @Override
            public void process(WatchedEvent event) {
                System.out.println(""eventType:"" + event.getType());
                System.out.println(""keeperState:"" + event.getState());
                if (event.getType() == Event.EventType.None) {
                    if (event.getState() == Event.KeeperState.SyncConnected) {
                        sem.release();
                    }
                }
            }
        });
        System.out.println(""state:"" + zooKeeper.getState());
        System.out.println(""Waiting for the state to be connected"");
        try {
            sem.acquire();
        } catch (InterruptedException ex) {
            ex.printStackTrace();
        }
        System.out.println(""Now state:"" + zooKeeper.getState());

        String directory = ""/_locknode_"";
        Stat stat = zooKeeper.exists(directory, false);
        if (stat == null) {
            zooKeeper.create(directory, new byte[]{}, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.PERSISTENT);
        }
        String prefix = directory + ""/lock-"";
        String path = zooKeeper.create(prefix, new byte[]{}, ZooDefs.Ids.OPEN_ACL_UNSAFE, CreateMode.EPHEMERAL_SEQUENTIAL);
        System.out.println(""Create the path for "" + path);
        while (true) {
            List<String> children = zooKeeper.getChildren(directory, false);
            Collections.sort(children);
            System.out.println(""The whole lock size is "" + children.size());
            String lowestPath = children.get(0);
            DecimalFormat df = new DecimalFormat(""0000000000"");
            String currentSuffix = lowestPath.substring(""lock-"".length());
            System.out.println(""CurrentSuffix is "" + currentSuffix);
            int intIndex = Integer.parseInt(currentSuffix);

            if (path.equals(directory + ""/"" + lowestPath)) {
                //I've got the lock and release it
                System.out.println(""I've got the lock at "" + new Date());
                System.out.println(""next index is "" + intIndex);
                Thread.sleep(10000);
                System.out.println(""After sleep 3 seconds, I'm gonna release the lock"");
                zooKeeper.delete(path, -1);
                break;
            }
            final Semaphore wakeupSem = new Semaphore(0);
            stat = zooKeeper.exists(directory + ""/"" + lowestPath, new Watcher() {

                @Override
                public void process(WatchedEvent event) {
                    System.out.println(""Event is "" + event.getType());
                    System.out.println(""State is "" + event.getState());
                    if (event.getType() == Event.EventType.NodeDeleted) {
                        wakeupSem.release();
                    }
                }
            });
            if (stat != null) {
                System.out.println(""Waiting for the delete of "");
                wakeupSem.acquire();
            } else {
                System.out.println(""Continue to seek"");
            }
        }
{code} ",yynil,yynil,Major,Resolved,Fixed,19/Jul/11 16:49,03/Mar/16 01:35
Bug,ZOOKEEPER-1134,12515133,ClientCnxnSocket string comparison using == rather than equals,Noticed string comparison using == rather than equals.,phunt,phunt,Critical,Closed,Fixed,22/Jul/11 22:18,23/Nov/11 19:22
Bug,ZOOKEEPER-1136,12515429,NEW_LEADER should be queued not sent to match the Zab 1.0 protocol on the twiki,"the NEW_LEADER message was sent at the beginning of the sync phase in Zab pre1.0, but it must be at the end in Zab 1.0. if the protocol is 1.0 or greater we need to queue rather than send the packet.",breed,breed,Blocker,Closed,Fixed,26/Jul/11 16:58,23/Nov/11 19:22
Bug,ZOOKEEPER-1138,12515570,release audit failing for a number of new files,"I'm seeing a number of problems in the release audit output for 3.4.0, these must be fixed before 3.4.0 release:

{noformat}
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/contrib/ZooInspector/config/defaultConnectionSettings.cfg
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/contrib/ZooInspector/config/defaultNodeVeiwers.cfg
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/contrib/ZooInspector/licences/epl-v10.html
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/Cli.vcproj
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/include/winconfig.h
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/include/winstdint.h
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/zookeeper.sln
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/c/zookeeper.vcproj
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/huebrowser/zkui/src/zkui/static/help/index.html
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/huebrowser/zkui/src/zkui/static/js/package.yml
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/log4j.properties
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/date.format.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.bar.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.dot.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.line.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.pie.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/g.raphael.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/raphael.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/loggraph/web/org/apache/zookeeper/graph/resources/yui-min.js
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/monitoring/JMX-RESOURCES
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/zooinspector/config/defaultConnectionSettings.cfg
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/zooinspector/config/defaultNodeVeiwers.cfg
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/zooinspector/lib/log4j.properties
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/contrib/zooinspector/licences/epl-v10.html
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/java/test/org/apache/zookeeper/MultiTransactionRecordTest.java
[rat:report]  !????? /grid/0/hudson/hudson-slave/workspace/PreCommit-ZOOKEEPER-Build/trunk/build/zookeeper-3.4.0/src/java/test/org/apache/zookeeper/server/quorum/LearnerTest.java
Lines that start with ????? in the release audit report indicate files that do not have an Apache license header.
{noformat}
",phunt,phunt,Blocker,Closed,Fixed,27/Jul/11 20:04,23/Nov/11 19:22
Bug,ZOOKEEPER-1139,12515582,"jenkins is reporting two warnings, fix these","cleanup jenkins report, currently 2 compiler warnings being reported.
",phunt,phunt,Minor,Closed,Fixed,27/Jul/11 21:42,23/Nov/11 19:22
Bug,ZOOKEEPER-1140,12515792,server shutdown is not stopping threads,"Near the end of QuorumZxidSyncTest there are tons of threads running - 115 ""ProcessThread"" threads, similar numbers of SessionTracker.

Also I see ~100 ReadOnlyRequestProcessor - why is this running as a separate thread? (henry/flavio?)

",lakshman,phunt,Blocker,Closed,Fixed,29/Jul/11 16:49,23/Nov/11 19:22
Bug,ZOOKEEPER-1141,12517722,zkpython fails tests under python 2.4,"""ant test"" under python 2.4 is failing due to a small issue in the test code - using a new feature introduced in 2.5.

I have a small patch which addresses this, after which I was able to compile and run the tests successfully under python 2.4.
",phunt,phunt,Major,Closed,Fixed,02/Aug/11 23:31,23/Nov/11 19:22
Bug,ZOOKEEPER-1142,12517728,incorrect stat output,"stat output seems to be missing some end of line:

{noformat}
echo stat |nc c0309 2181
Zookeeper version: 3.4.0--1, built on 08/02/2011 22:25 GMT
Clients:
 /172.29.81.91:33378[0](queued=0,recved=1,sent=0
Latency min/avg/max: 0/28/252
Received: 246844
Sent: 266737
Outstanding: 0
Zxid: 0x4000508c2
Mode: follower
Node count: 4
{noformat}

Multiple clients end up on the same line (missing newline)",phunt,phunt,Blocker,Closed,Fixed,03/Aug/11 00:29,23/Nov/11 19:22
Bug,ZOOKEEPER-1144,12517863,ZooKeeperServer not starting on leader due to a race condition,"I have found one problem that is causing QuorumPeerMainTest:testQuorum to fail. This test uses 2 ZK servers. 

The test is failing because leader is not starting ZooKeeperServer after leader election. so everything halts.

With the new changes, the server is now started in Leader.processAck() which is called from LeaderHandler. processAck() starts ZooKeeperServer if majority have acked NEWLEADER. The leader puts its ack in the the ackSet in Leader.lead(). Since processAck() is called from LearnerHandler it can happen that the learner's ack is processed before the leader is able to put its ack in the ackSet. When LearnerHandler invokes processAck(), the ackSet for newLeaderProposal will not have quorum (in this case 2). As a result, the ZooKeeperServer is never started on the Leader.

The leader needs to ensure that its ack is put in ackSet before starting LearnerCnxAcceptor or invoke processAck() itself after adding to ackSet. I haven't had time to go through the ZAB2 changes so I am not too familiar with the code. Can Ben/Flavio fix this?",vishalmlst,vishalmlst,Blocker,Closed,Fixed,03/Aug/11 22:35,23/Nov/11 19:22
Bug,ZOOKEEPER-1146,12517940,significant regression in client (c/python) performance,"I tried running my latency tester against trunk, in so doing I noticed that the C/Python (not sure which yet) client performance has seriously degraded since 3.3.3.

The first run (below) is with released 3.3.3 client against a 3 server ensemble running released 3.3.3 server code. The second run is the exact same environment (same ensemble), however using trunk c/zkpython client.

Notice:

1) in the first run operations are approx 10ms/write, 0.25ms/read - which is pretty much what's expected.

2) however in the second run we are seeing 50ms/operation regardless of read or write.

{noformat}
[phunt@c0309 zk-smoketest-3.3.3]$ PYTHONPATH=lib.linux-x86_64-2.6/ LD_LIBRARY_PATH=lib.linux-x86_64-2.6/ python26 ./zk-latencies.py --servers ""c0309:2181,c0310:2181,c0311:2181"" --znode_size=100 --znode_count=100 --timeout=5000 --synchronous
Connecting to c0309:2181
Connected in 16 ms, handle is 0
Connecting to c0310:2181
Connected in 16 ms, handle is 1
Connecting to c0311:2181
Connected in 15 ms, handle is 2
Testing latencies on server c0309:2181 using syncronous calls
created     100 permanent znodes  in    959 ms (9.599378 ms/op 104.173415/sec)
set         100           znodes  in    933 ms (9.332101 ms/op 107.157002/sec)
get         100           znodes  in     27 ms (0.270889 ms/op 3691.551589/sec)
deleted     100 permanent znodes  in    881 ms (8.812950 ms/op 113.469388/sec)
created     100 ephemeral znodes  in    956 ms (9.564152 ms/op 104.557103/sec)
watched     100           znodes  in     26 ms (0.264361 ms/op 3782.707587/sec)
deleted     100 ephemeral znodes  in    881 ms (8.819292 ms/op 113.387792/sec)
notif       100           watches in    999 ms (9.994299 ms/op 100.057038/sec)
Testing latencies on server c0310:2181 using syncronous calls
created     100 permanent znodes  in    964 ms (9.640460 ms/op 103.729490/sec)
set         100           znodes  in    933 ms (9.332800 ms/op 107.148981/sec)
get         100           znodes  in     29 ms (0.299308 ms/op 3341.036650/sec)
deleted     100 permanent znodes  in    886 ms (8.864651 ms/op 112.807603/sec)
created     100 ephemeral znodes  in    958 ms (9.585140 ms/op 104.328161/sec)
watched     100           znodes  in     30 ms (0.300801 ms/op 3324.459240/sec)
deleted     100 ephemeral znodes  in    886 ms (8.865030 ms/op 112.802779/sec)
notif       100           watches in   1000 ms (10.000212 ms/op 99.997878/sec)
Testing latencies on server c0311:2181 using syncronous calls
created     100 permanent znodes  in    958 ms (9.582071 ms/op 104.361569/sec)
set         100           znodes  in    935 ms (9.359350 ms/op 106.845024/sec)
get         100           znodes  in     25 ms (0.252700 ms/op 3957.263893/sec)
deleted     100 permanent znodes  in    891 ms (8.913291 ms/op 112.192013/sec)
created     100 ephemeral znodes  in    958 ms (9.584489 ms/op 104.335246/sec)
watched     100           znodes  in     25 ms (0.251091 ms/op 3982.627356/sec)
deleted     100 ephemeral znodes  in    891 ms (8.915379 ms/op 112.165730/sec)
notif       100           watches in   1000 ms (10.000508 ms/op 99.994922/sec)
Latency test complete
[phunt@c0309 zk-smoketest-3.3.3]$ cd ../zk-smoketest-trunk/
[phunt@c0309 zk-smoketest-trunk]$ PYTHONPATH=lib.linux-x86_64-2.6/ LD_LIBRARY_PATH=lib.linux-x86_64-2.6/ python26 ./zk-latencies.py --servers ""c0309:2181,c0310:2181,c0311:2181"" --znode_size=100 --znode_count=100 --timeout=5000 --synchronous
Connecting to c0309:2181
Connected in 31 ms, handle is 0
Connecting to c0310:2181
Connected in 16 ms, handle is 1
Connecting to c0311:2181
Connected in 16 ms, handle is 2
Testing latencies on server c0309:2181 using syncronous calls
created     100 permanent znodes  in   5099 ms (50.999281 ms/op 19.608119/sec)
set         100           znodes  in   5066 ms (50.665429 ms/op 19.737324/sec)
get         100           znodes  in   4009 ms (40.093150 ms/op 24.941916/sec)
deleted     100 permanent znodes  in   5040 ms (50.404449 ms/op 19.839519/sec)
created     100 ephemeral znodes  in   5124 ms (51.249170 ms/op 19.512511/sec)
watched     100           znodes  in   4051 ms (40.514441 ms/op 24.682557/sec)
deleted     100 ephemeral znodes  in   5048 ms (50.484939 ms/op 19.807888/sec)
notif       100           watches in   1000 ms (10.004182 ms/op 99.958199/sec)
Testing latencies on server c0310:2181 using syncronous calls
created     100 permanent znodes  in   5115 ms (51.157510 ms/op 19.547472/sec)
set         100           znodes  in   5056 ms (50.568910 ms/op 19.774996/sec)
get         100           znodes  in   4099 ms (40.999382 ms/op 24.390612/sec)
deleted     100 permanent znodes  in   5041 ms (50.418010 ms/op 19.834182/sec)
created     100 ephemeral znodes  in   5083 ms (50.835850 ms/op 19.671157/sec)
watched     100           znodes  in   4100 ms (41.003261 ms/op 24.388304/sec)
deleted     100 ephemeral znodes  in   5058 ms (50.581930 ms/op 19.769906/sec)
notif       100           watches in   1000 ms (10.005081 ms/op 99.949219/sec)
Testing latencies on server c0311:2181 using syncronous calls
created     100 permanent znodes  in   5099 ms (50.992720 ms/op 19.610642/sec)
set         100           znodes  in   5091 ms (50.916569 ms/op 19.639972/sec)
get         100           znodes  in   4099 ms (40.996401 ms/op 24.392385/sec)
deleted     100 permanent znodes  in   5066 ms (50.669601 ms/op 19.735699/sec)
created     100 ephemeral znodes  in   5124 ms (51.249208 ms/op 19.512496/sec)
watched     100           znodes  in   4099 ms (40.999141 ms/op 24.390755/sec)
deleted     100 ephemeral znodes  in   5049 ms (50.498819 ms/op 19.802443/sec)
notif       100           watches in    999 ms (9.997852 ms/op 100.021486/sec)
Latency test complete
{noformat}",phunt,phunt,Blocker,Closed,Fixed,04/Aug/11 17:21,23/Nov/11 19:22
Bug,ZOOKEEPER-1152,12518738,Exceptions thrown from handleAuthentication can cause buffer corruption issues in NIOServer,"Exceptions thrown by an AuthenticationProvider's handleAuthentication method will not be caught, and can cause the buffers in the NIOServer to not read requests fully or properly. Any exceptions thrown here should be caught and treated as auth failure. ",fournc,fournc,Major,Closed,Fixed,12/Aug/11 20:27,23/Nov/11 19:22
Bug,ZOOKEEPER-1154,12518902,Data inconsistency when the node(s) with the highest zxid is not present at the time of leader election,"If a participant with the highest zxid (lets call it A) isn't present during leader election, a participant with a lower zxid (say B) might be chosen as a leader. When A comes up, it will replay the log with that higher zxid. The change that was in that higher zxid will only be visible to the clients connecting to the participant A, but not to other participants.

I was able to reproduce this problem by
1. connect debugger to B and C and suspend them, so they don't write anything
2. Issue an update to the leader A.
3. After a few seconds, crash all servers (A,B,C)
4. Start B and C, let the leader election take place
5. Start A.
6. You will find that the update done in step 2 is visible on A but not on B,C, hence the inconsistency.

Below is a more detailed analysis of what is happening in the code.


Initial Condition
1.	Lets say there are three nodes in the ensemble A,B,C with A being the leader
2.	The current epoch is 7. 
3.	For simplicity of the example, lets say zxid is a two digit number, with epoch being the first digit.
4.	The zxid is 73
5.	All the nodes have seen the change 73 and have persistently logged it.

Step 1
Request with zxid 74 is issued. The leader A writes it to the log but there is a crash of the entire ensemble and B,C never write the change 74 to their log.

Step 3
B,C restart, A is still down
B,C form the quorum
B is the new leader. Lets say  B minCommitLog is 71 and maxCommitLog is 73
epoch is now 8, zxid is 80
Request with zxid 81 is successful. On B, minCommitLog is now 71, maxCommitLog is 81

Step 4
A starts up. It applies the change in request with zxid 74 to its in-memory data tree
A contacts B to registerAsFollower and provides 74 as its ZxId
Since 71<=74<=81, B decides to send A the diff. B will send to A the proposal 81.


Problem:
The problem with the above sequence is that A's data tree has the update from request 74, which is not correct. Before getting the proposals 81, A should have received a trunc to 73. I don't see that in the code. If the maxCommitLog on B hadn't bumped to 81 but had stayed at 73, that case seems to be fine.
",vishal.k,vishal.k,Blocker,Closed,Fixed,15/Aug/11 17:36,23/Nov/11 19:22
Bug,ZOOKEEPER-1156,12519323,Log truncation truncating log too much - can cause data loss,"The log truncation relies on position calculation for a particular zxid to figure out the new size of the log file. There is a bug in PositionInputStream implementation which skips counting the bytes in the log which have value 0. This can lead to underestimating the actual log size. The log records which should be there can get truncated, leading to data loss on the participant which is executing the trunc.

Clients can see different values depending on whether they connect to the node on which trunc was executed. ",vishal.k,vishal.k,Blocker,Closed,Fixed,18/Aug/11 17:48,23/Nov/11 19:22
Bug,ZOOKEEPER-1163,12520132,Memory leak in zk_hashtable.c:do_insert_watcher_object(),"zk_hashtable.c:do_insert_watcher_object() line number 193 calls add_to_list with clone flag set to 1.  This leaks memory, since the original watcher object was already allocated on the heap by activateWatcher() line 330.

I will upload a patch shortly.  The fix is to set clone flag to 0 in the call to add_to_list().",anupamc,anupamc,Major,Resolved,Fixed,25/Aug/11 17:47,03/Mar/16 01:36
Bug,ZOOKEEPER-1165,12520644,better eclipse support in tests,"The Eclipse test runner tries to run tests from all classes that inherit from TestCase. However, this class is inherited by at least one class (org.apache.zookeeper.test.system.BaseSysTest) that has no test cases as it is used as infrastructure for other real test cases. This patch annotates that class with @Ignore, which causes the class to be Ignored. Also, due to the way annotations are not inherited by default, this patch will not affect classes that inherit from this class.",wturkal,wturkal,Minor,Closed,Fixed,29/Aug/11 22:08,23/Nov/11 19:22
Bug,ZOOKEEPER-1168,12520968,ZooKeeper fails to run with IKVM,"OS: Windows 64-bit
JRE: IKVM 7.0.4258

IKVM 7.0.4258 does not support ManagementFactory.getPlatformMBeanServer(); It will throw a java.lang.Error.
",andrew.finnell,andrew.finnell,Major,Closed,Fixed,31/Aug/11 19:52,23/Nov/11 19:22
Bug,ZOOKEEPER-1171,12521225,fix build for java 7,I tried testing out zk on java 7 (not yet officially supported) but I ran into a road block due to the build failing. Patch coming next.,phunt,phunt,Minor,Closed,Fixed,02/Sep/11 19:35,23/Nov/11 19:22
Bug,ZOOKEEPER-1174,12522301,FD leak when network unreachable,"In the socket connection logic there are several errors that result in bad behavior.  The basic problem is that a socket is registered with a selector unconditionally when there are nuances that should be dealt with.  First, the socket may connect immediately.  Secondly, the connect may throw an exception.  In either of these two cases, I don't think that the socket should be registered.

I will attach a test case that demonstrates the problem.  I have been unable to create a unit test that exhibits the problem because I would have to mock the low level socket libraries to do so.  It would still be good to do so if somebody can figure out a good way.",tdunning,tdunning,Critical,Closed,Fixed,08/Sep/11 18:47,23/Nov/11 19:22
Bug,ZOOKEEPER-1179,12522922,NettyServerCnxn does not properly close socket on 4 letter word requests,"When calling a 4-letter-word to a server configured to use NettyServerCnxnFactory, the factory will not properly cancel all the keys and close the socket after sending the response for the 4lw. The close request will throw this exception, and the thread will not shut down:
2011-09-13 12:14:17,546 - WARN  [New I/O server worker #1-1:NettyServerCnxnFactory$CnxnChannelHandler@117] - Exception caught [id: 0x009300cc, /1.1.1.1:38542 => /139.172.114.138:2181] EXCEPTION: java.io.IOException: A non-blocking socket operation could not be completed immediately
java.io.IOException: A non-blocking socket operation could not be completed immediately
	at sun.nio.ch.SocketDispatcher.close0(Native Method)
	at sun.nio.ch.SocketDispatcher.preClose(SocketDispatcher.java:44)
	at sun.nio.ch.SocketChannelImpl.implCloseSelectableChannel(SocketChannelImpl.java:684)
	at java.nio.channels.spi.AbstractSelectableChannel.implCloseChannel(AbstractSelectableChannel.java:201)
	at java.nio.channels.spi.AbstractInterruptibleChannel.close(AbstractInterruptibleChannel.java:97)
	at org.jboss.netty.channel.socket.nio.NioWorker.close(NioWorker.java:593)
	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.handleAcceptedSocket(NioServerSocketPipelineSink.java:119)
	at org.jboss.netty.channel.socket.nio.NioServerSocketPipelineSink.eventSunk(NioServerSocketPipelineSink.java:76)
	at org.jboss.netty.channel.Channels.close(Channels.java:720)
	at org.jboss.netty.channel.AbstractChannel.close(AbstractChannel.java:208)
	at org.apache.zookeeper.server.NettyServerCnxn.close(NettyServerCnxn.java:116)
	at org.apache.zookeeper.server.NettyServerCnxn.cleanupWriterSocket(NettyServerCnxn.java:241)
	at org.apache.zookeeper.server.NettyServerCnxn.access$0(NettyServerCnxn.java:231)
	at org.apache.zookeeper.server.NettyServerCnxn$CommandThread.run(NettyServerCnxn.java:314)
	at org.apache.zookeeper.server.NettyServerCnxn$CommandThread.start(NettyServerCnxn.java:305)
	at org.apache.zookeeper.server.NettyServerCnxn.checkFourLetterWord(NettyServerCnxn.java:674)
	at org.apache.zookeeper.server.NettyServerCnxn.receiveMessage(NettyServerCnxn.java:791)
	at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.processMessage(NettyServerCnxnFactory.java:217)
	at org.apache.zookeeper.server.NettyServerCnxnFactory$CnxnChannelHandler.messageReceived(NettyServerCnxnFactory.java:141)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:274)
	at org.jboss.netty.channel.Channels.fireMessageReceived(Channels.java:261)
	at org.jboss.netty.channel.socket.nio.NioWorker.read(NioWorker.java:350)
	at org.jboss.netty.channel.socket.nio.NioWorker.processSelectedKeys(NioWorker.java:281)
	at org.jboss.netty.channel.socket.nio.NioWorker.run(NioWorker.java:201)
	at org.jboss.netty.util.internal.IoWorkerRunnable.run(IoWorkerRunnable.java:46)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)",rakeshr,fournc,Critical,Closed,Fixed,13/Sep/11 16:20,13/Mar/14 18:17
Bug,ZOOKEEPER-1181,12522975,Fix problems with Kerberos TGT renewal,"Currently, in Zookeeper trunk, there are two problems with Kerberos TGT renewal:

1. TGTs obtained from a keytab are not refreshed periodically. They should be, just as those from ticket cache are refreshed.

2. Ticket renewal should be retried if it fails. Ticket renewal might fail if two or more separate processes (different JVMs) running as the same user try to renew Kerberos credentials at the same time. 
",ekoontz,ekoontz,Major,Closed,Fixed,13/Sep/11 23:38,02/May/13 02:29
Bug,ZOOKEEPER-1184,12523125,"jute generated files are not being cleaned up via ""ant clean""","The change for ZOOKEEPER-96 has removed the generated files from SVN, it seems that these files should now live under build subdir? If this change is made be sure that the C/contrib/recipes environment is not broken...",thkoch,phunt,Major,Resolved,Fixed,14/Sep/11 22:48,17/Sep/11 10:56
Bug,ZOOKEEPER-1185,12523137,Send AuthFailed event to client if SASL authentication fails,"There are 3 places where ClientCnxn should queue a AuthFailed event if client fails to authenticate. Without sending this event, clients may be stuck watching for a SaslAuthenticated event that will never come (since the client failed to authenticate).

",ekoontz,ekoontz,Major,Closed,Fixed,15/Sep/11 01:11,02/May/13 02:29
Bug,ZOOKEEPER-1189,12523338,For an invalid snapshot file(less than 10bytes size) RandomAccessFile stream is leaking.,"When loading the snapshot, ZooKeeper will consider only the 'snapshots with atleast 10 bytes size'. Otherwsie it will ignore and just return without closing the RandomAccessFile.

{noformat}
Util.isValidSnapshot() having the following logic. 
       // Check for a valid snapshot
        RandomAccessFile raf = new RandomAccessFile(f, ""r"");
        // including the header and the last / bytes
        // the snapshot should be atleast 10 bytes
        if (raf.length() < 10) {
            return false;
        }
{noformat}

Since the snapshot file validation logic is outside try block, it won't go to the finally block and will be leaked.

Suggestion: Move the validation logic to the try/catch block.",rakeshr,rakeshr,Major,Closed,Fixed,16/Sep/11 14:29,23/Nov/11 19:22
Bug,ZOOKEEPER-1190,12523411,ant package is not including many of the bin scripts in the package (zkServer.sh for example),"run ""ant package"" and look in the build/zookeeper-<version>/bin directory. many of the bin scripts are missing.
",eyang,phunt,Blocker,Closed,Fixed,17/Sep/11 00:30,23/Nov/11 19:22
Bug,ZOOKEEPER-1192,12523506,Leader.waitForEpochAck() checks waitingForNewEpoch instead of checking electionFinished,"A follower/leader should block in Leader.waitForEpochAck() until either electingFollowers contains a quorum and electionFinished=true or until a timeout occurs. A timeout means that a quorum of followers didn't ack the epoch on time, which is an error. 

But the check in Leader.waitForEpochAck() is ""if (waitingForNewEpoch) throw..."" and this will never be triggered, even if the wait statement just timed out,  because Leader.getEpochToPropose() completes and sets waitingForNewEpoch to false before Leader.waitForEpochAck() is invoked.

Instead of ""if (waitingForNewEpoch) throw"" the condition in Leader.waitForEpochAck() should be ""if (!electionFinished) throw"".
The guarded block introduced in ZK-1191 should be checking !electionFinished.

",shralex,shralex,Critical,Closed,Fixed,19/Sep/11 02:00,23/Nov/11 19:22
Bug,ZOOKEEPER-1194,12523643,Two possible race conditions during leader establishment,"Leader.getEpochToPropose() and Leader.waitForNewEpoch() act as barriers - they make sure that a leader/follower can return from calling the method only once connectingFollowers (or electingFollowers) contain a quorum. But these methods don't make sure that the leader itself is in connectingFollowers/electingFollowers. So the leader didn't necessarily reach the barrier when followers pass it. This can cause the following problems:

1. If the leader is not in connectingFollowers when a LearnerHandler returns from getEpochToPropose(), then the epoch sent by the leader to the follower might be smaller than the leader's own last accepted epoch.

2. If the leader is not in electingFollowers when LearnerHandler returns from waitForNewEpoch() then the leader will send a NEWLEADER message to followers, and the followers will respond, but it is possible that the NEWLEADER message is not in outstandingProposals when these NEWLEADER  acks arrive, which will cause the NEWLEADER acks to be dropped.


To fix this I propose to explicitly check that the leader is in connectingFollowers/electingFollowers before anyone can pass these barriers.




",shralex,shralex,Major,Closed,Fixed,20/Sep/11 00:10,23/Nov/11 19:22
Bug,ZOOKEEPER-1195,12523731,SASL authorizedID being incorrectly set: should use getHostName() rather than getServiceName(),"Tom Klonikowski writes:

    Hello developers,

    the SaslServerCallbackHandler in trunk changes the principal name
    service/host@REALM to service/service@REALM (i guess unintentionally).

    lines 131-133:
    if (!removeHost() && (kerberosName.getHostName() != null)) {
      userName += ""/"" + kerberosName.getServiceName();
    }

    Server Log:

    SaslServerCallbackHandler@115] - Successfully authenticated client:
    authenticationID=fetcher/ubook@QUINZOO;
    authorizationID=fetcher/ubook@QUINZOO.

    SaslServerCallbackHandler@137] - Setting authorizedID:
    fetcher/fetcher@QUINZOO

",ekoontz,ekoontz,Major,Closed,Fixed,20/Sep/11 15:15,02/May/13 02:29
Bug,ZOOKEEPER-1203,12524484,Zookeeper systest is missing Junit Classes ,"For running these tests, I am following instructions on https://github.com/apache/zookeeper/blob/trunk/src/java/systest/README.txt 

In Step 4, when I try to run java -jar build/contrib/fatjar/zookeeper-<version>-fatjar.jar systest org.apache.zookeeper.test.system.SimpleSysTest , it throws the following error,

Exception in thread ""main"" java.lang.NoClassDefFoundError: junit/framework/TestCase

The problem is that zookeeper-dev-fatjar.jar does not contain the TestCase class.

Patrick Hunt suggested that adding <zipgroupfileset dir=""${zk.root}/build/test/lib"" includes=""*.jar"" /> to fatjar/build.xml should solve the problem and it does.",prashant@cloudera.com,prashant@cloudera.com,Major,Closed,Fixed,23/Sep/11 22:44,23/Nov/11 19:22
Bug,ZOOKEEPER-1206,12524930,Sequential node creation does not use always use digits in node name given certain Locales.,"While I always expect to be able to parse a sequential node by looking for digits, under some locals you end up with non digits - for example: n_००००००००००

It looks like the problem is around line 236 in PrepRequestProcessor:

{code}
                if (createMode.isSequential()) {
                    path = path + String.format(""%010d"", parentCVersion);
                }
{code}

Instead we should pass Locale.ENGLISH to the format call.

{code}
                if (createMode.isSequential()) {
                    path = path + String.format(Locale.ENGLISH, ""%010d"", parentCVersion);
                }
{code}

Lucene/Solr tests with random Locales, and some of my tests that try and inspect the node name and order things expect to find digits - currently my leader election recipe randomly fails when the wrong locale pops up.",markrmiller@gmail.com,markrmiller@gmail.com,Minor,Closed,Fixed,27/Sep/11 16:26,23/Nov/11 19:22
Bug,ZOOKEEPER-1208,12525006,Ephemeral node not removed after the client session is long gone,"Copying from email thread.


We found our ZK server in a state where an ephemeral node still exists after
a client session is long gone. I used the cons command on each ZK host to
list all connections and couldn't find the ephemeralOwner id. We are using
ZK 3.3.3. Has anyone seen this problem?

I got the following information from the logs.

The node that still exists is /kafka-tracking/consumers/UserPerformanceEvent-<host>/owners/UserPerformanceEvent/529-7

I saw that the ephemeral owner is 86167322861045079 which is session id 0x13220b93e610550.

After searching in the transaction log of one of the ZK servers found that session expired

9/22/11 12:17:57 PM PDT session 0x13220b93e610550 cxid 0x74 zxid 0x601bd36f7 closeSession null

On digging further into the logs I found that there were multiple sessions created in quick succession and every session tried to create the same node. But i verified that the sessions were closed and opened in order
9/22/11 12:17:56 PM PDT session 0x13220b93e610550 cxid 0x0 zxid 0x601bd36b5 createSession 6000
9/22/11 12:17:57 PM PDT session 0x13220b93e610550 cxid 0x74 zxid 0x601bd36f7 closeSession null
9/22/11 12:17:58 PM PDT session 0x13220b93e610551 cxid 0x0 zxid 0x601bd36f8 createSession 6000
9/22/11 12:17:59 PM PDT session 0x13220b93e610551 cxid 0x74 zxid 0x601bd373a closeSession null
9/22/11 12:18:00 PM PDT session 0x13220b93e610552 cxid 0x0 zxid 0x601bd373e createSession 6000
9/22/11 12:18:01 PM PDT session 0x13220b93e610552 cxid 0x6c zxid 0x601bd37a0 closeSession null
9/22/11 12:18:02 PM PDT session 0x13220b93e610553 cxid 0x0 zxid 0x601bd37e9 createSession 6000
9/22/11 12:18:03 PM PDT session 0x13220b93e610553 cxid 0x74 zxid 0x601bd382b closeSession null
9/22/11 12:18:04 PM PDT session 0x13220b93e610554 cxid 0x0 zxid 0x601bd383c createSession 6000
9/22/11 12:18:05 PM PDT session 0x13220b93e610554 cxid 0x6a zxid 0x601bd388f closeSession null
9/22/11 12:18:06 PM PDT session 0x13220b93e610555 cxid 0x0 zxid 0x601bd3895 createSession 6000
9/22/11 12:18:07 PM PDT session 0x13220b93e610555 cxid 0x6a zxid 0x601bd38cd closeSession null
9/22/11 12:18:10 PM PDT session 0x13220b93e610556 cxid 0x0 zxid 0x601bd38d1 createSession 6000
9/22/11 12:18:11 PM PDT session 0x13220b93e610557 cxid 0x0 zxid 0x601bd38f2 createSession 6000
9/22/11 12:18:11 PM PDT session 0x13220b93e610557 cxid 0x51 zxid 0x601bd396a closeSession null

Here is the log output for the sessions that tried creating the same node

9/22/11 12:17:54 PM PDT session 0x13220b93e61054f cxid 0x42 zxid 0x601bd366b create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:17:56 PM PDT session 0x13220b93e610550 cxid 0x42 zxid 0x601bd36ce create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:17:58 PM PDT session 0x13220b93e610551 cxid 0x42 zxid 0x601bd3711 create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:18:00 PM PDT session 0x13220b93e610552 cxid 0x42 zxid 0x601bd3777 create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:18:02 PM PDT session 0x13220b93e610553 cxid 0x42 zxid 0x601bd3802 create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:18:05 PM PDT session 0x13220b93e610554 cxid 0x44 zxid 0x601bd385d create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:18:07 PM PDT session 0x13220b93e610555 cxid 0x44 zxid 0x601bd38b0 create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7
9/22/11 12:18:11 PM PDT session 0x13220b93e610557 cxid 0x52 zxid 0x601bd396b create '/kafka-tracking/consumers/UserPerformanceEvent-<hostname>/owners/UserPerformanceEvent/529-7

Let me know if you need additional information.
",phunt,k4j,Blocker,Closed,Fixed,28/Sep/11 04:35,23/Nov/11 19:22
Bug,ZOOKEEPER-1210,12525061,Can't build ZooKeeper RPM with RPM >= 4.6.0 (i.e. on RHEL 6 and Fedora >= 10),"I was trying to build the zookeeper RPM (basically, `ant rpm -Dskip.contrib=1`), using build scripts that were recently merged from the work on the ZOOKEEPER-999 issue.

The final stage, i.e. running rpmbuild failed. From what I understand it mixed BUILD and BUILDROOT subdirectories in /tmp/zookeeper_package_build_tkadlubo/, leaving BUILDROOT empty, and placing everything in BUILD.

The full build log is at http://pastebin.com/0ZvUAKJt (Caution: I cut out long file listings from running tar -xvvf).",tkadlubo,tkadlubo,Minor,Resolved,Fixed,28/Sep/11 14:20,30/Jun/12 11:01
Bug,ZOOKEEPER-1212,12525574,zkServer.sh stop action is not conformat with LSB para 20.2 Init Script Actions,"According to LSB Core para 20.2:
==================================================================================
Otherwise,  the exit  status shall  be non­zero,  as de­fined below. 
In addition to straightforward success, the following situations are
also to be considered successful: 
• restarting a service (instead of reloading it) with the force­reload argument
• running start on a service already running
• running stop on a service already stopped or not running
• running restart on a service already stopped or not running
• running try­restart on a service already stopped or not running
==================================================================================

Yet, zkServer.sh fails on stop if it can't find a PID file:

{noformat}
stop)
    echo -n ""Stopping zookeeper ... ""
    if [ ! -f ""$ZOOPIDFILE"" ]
    then
      echo ""error: could not find file $ZOOPIDFILE""
      exit 1
    else
      $KILL -9 $(cat ""$ZOOPIDFILE"")
      rm ""$ZOOPIDFILE""
      echo STOPPED
      exit 0
    fi
{noformat}",rvs,rvs,Major,Closed,Fixed,03/Oct/11 20:24,23/Nov/11 19:22
Bug,ZOOKEEPER-1214,12525784,QuorumPeer should unregister only its previsously registered MBeans instead of use MBeanRegistry.unregisterAll() method.,"When a QuorumPeer thread dies, it is unregistering *all* ZKMBeanInfo MBeans previously registered on its java process; including those that has not been registered by itself.

It does not cause any side effect in production environment where each server is running on a separate java process; but fails when using ""org.apache.zookeeper.test.QuorumUtil"" to programmatically start up a zookeeper server ensemble and use its provided methods to force Disconnected, SyncConnected or SessionExpired events; in order to perform some basic/functional testing.

Scenario:
* QuorumUtil qU = new QuorumUtil(1); // It creates a 3 servers ensemble.
* qU.startAll(); // Startup all servers: 1 Leader + 2 Followers
* qU.shutdown\(i\); // i is a number from 1 to 3. It shutdown one server.

The last method causes that a QuorumPeer will die, invoking the MBeanRegistry.unregisterAll() method.
As a result, *all* ZKMBeanInfo MBeans are unregistered; including those belonging to the other QuorumPeer instances.

When trying to restart previous server (qU.restart\(i\)) an AssertionError is thrown at MBeanRegistry.register(ZKMBeanInfo bean, ZKMBeanInfo parent) method, causing the QuorumPeer thread dead.

To solve it:
* MBeanRegistry.unregisterAll() method has been removed.
* QuorumPeer only unregister its ZKMBeanInfo MBeans.",calvarez,calvarez,Major,Resolved,Fixed,05/Oct/11 09:56,20/May/14 11:09
Bug,ZOOKEEPER-1220,12526825,./zkCli.sh 'create' command is throwing ArrayIndexOutOfBoundsException,"Few problems while executing create command,
 
If we will give command like 
 
1)[zk: localhost:2181(CONNECTED) 0] create -s -e /node1
{noformat}
       Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 4
	at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:692)
	at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:593)
	at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:365)
	at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:323)
	at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:282)
{noformat}
      but actually it should create emphemeral sequential node.

2)[zk: localhost:2181(CONNECTED) 0] create -s -e
{noformat}
    Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 3
{noformat}
    here it should print the list of commands that is the default behaviour of zkCli for invalid/incomplete commands.

3)[zk: localhost:2181(CONNECTED) 3] create -s -e ""data""
{noformat}
     Exception in thread ""main"" java.lang.ArrayIndexOutOfBoundsException: 4
{noformat}
     here command is wrong so it should print list of commnads. . 

4)[zk: localhost:2181(CONNECTED) 0] create /node1
    zkCli is treating it as a invalid command.because for args.length  check (3)is their but behaviour is 
    if user haven't given any of the option it should create persistent node.
	  {noformat}
	  if (cmd.equals(""create"") && args.length >= 3) {
            int first = 0;
            CreateMode flags = CreateMode.PERSISTENT;
{noformat}",kavita sharma,kavita sharma,Major,Resolved,Fixed,12/Oct/11 10:47,15/Dec/11 11:58
Bug,ZOOKEEPER-1222,12526933,getACL should only call DataTree.copyStat when passed in stat is not null,"getACL(String, Stat) should allow the stat object to be null in the case that the user doesn't care about getting the stat back, as per other methods with similar syntax",michim,fournc,Minor,Resolved,Fixed,12/Oct/11 21:08,08/Jul/14 21:17
Bug,ZOOKEEPER-1236,12527958,Security uses proprietary Sun APIs,"See HADOOP-7211 - Recent kerberos integration resulted in the same issue in ZK.

{noformat}
    [javac] /home/phunt/dev/zookeeper/src/java/main/org/apache/zookeeper/server/auth/KerberosName.java:88: warning: sun.security.krb5.KrbException is Sun proprietary API and may be removed in a future release
    [javac]     } catch (KrbException ke) {
{noformat}",adalbas,phunt,Major,Resolved,Fixed,20/Oct/11 16:05,04/Jul/12 18:25
Bug,ZOOKEEPER-1238,12527976,when the linger time was changed for NIO the patch missed Netty,"from NettyServerCnxn:

bq.         bootstrap.setOption(""child.soLinger"", 2);

See ZOOKEEPER-1049",skye,phunt,Major,Closed,Fixed,20/Oct/11 16:58,13/Mar/14 18:17
Bug,ZOOKEEPER-1241,12528418,Typo in ZooKeeper Recipes and Solutions documentation,"In ""if p is the lowest process node in L, wait on highest process node in P"", ""P"" should be ""L"".",yaojingguo,yaojingguo,Minor,Resolved,Fixed,23/Oct/11 15:10,24/Oct/11 10:53
Bug,ZOOKEEPER-1256,12529020,ClientPortBindTest is failing on Mac OS X,ClientPortBindTest is failing consistently on Mac OS X.,fpj,dferro,Major,Closed,Fixed,27/Oct/11 07:45,18/May/17 03:43
Bug,ZOOKEEPER-1262,12529148,Documentation for Lock recipe has major flaw,"The recipe for Locks documented here: http://zookeeper.apache.org/doc/trunk/recipes.html#sc_recipes_Locks doesn't deal with the problem of create() succeeding but the server crashing before the result is returned. As written, if the server crashes before the result is returned the client can never know what sequential node was created for it. The way to deal with this is to embed the session ID in the node name. The Lock implementation in the ZK distro does this. But, the documentation will lead implementors to write bad code.",randgalt,randgalt,Major,Resolved,Fixed,27/Oct/11 21:46,28/Dec/11 21:18
Bug,ZOOKEEPER-1264,12529192,FollowerResyncConcurrencyTest failing intermittently,"The FollowerResyncConcurrencyTest test is failing intermittently. 

saw the following on 3.4:
{noformat}
junit.framework.AssertionFailedError: Should have same number of
ephemerals in both followers expected:<11741> but was:<14001>
       at org.apache.zookeeper.test.FollowerResyncConcurrencyTest.verifyState(FollowerResyncConcurrencyTest.java:400)
       at org.apache.zookeeper.test.FollowerResyncConcurrencyTest.testResyncBySnapThenDiffAfterFollowerCrashes(FollowerResyncConcurrencyTest.java:196)
       at org.apache.zookeeper.JUnit4ZKTestRunner$LoggedInvokeMethod.evaluate(JUnit4ZKTestRunner.java:52)
{noformat}
",fournc,phunt,Blocker,Closed,Fixed,28/Oct/11 04:23,23/Nov/11 19:22
Bug,ZOOKEEPER-1265,12529249,Normalize switch cases lists on request types,"As discussed on the list, it's probably an error that the ReadOnlyRequestProcessor does not have multi alongside the other write operations.
Adding check to the lists may not make a difference by now since the ZK client does not expose check as a first level request but only encapsulated inside a multi request. However from a logical view, change belongs in these lists.",thkoch,thkoch,Major,Resolved,Fixed,28/Oct/11 13:35,29/Oct/11 10:56
Bug,ZOOKEEPER-1268,12529288,"problems with read only mode, intermittent test failures and ERRORs in the log","I'm having a lot problems testing the 3.4.0 release candidate (0). I'm seeing frequent failures in RO unit tests, also the solaris tests are broken on jenkins, some of which is due to RO mode:
https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper_trunk_solaris/30/#showFailuresLink

I'm also seeing ERROR level messages in the logs during test runs that are a result of attempting to start RO mode.

Given this is a new feature, one that could be very disruptive, I think we need to control whether the feature is enabled or not through a config option (system prop is fine), disabled by default.

I'll look at the RO mode tests to see if I can find the cause of the failures on solaris, but I may also turn off these tests for the time being. (I need to look at this further).


I'm marking this as a blocker for 3.4.0, Mahadev LMK if you feel similarly or whether I should be shooting for 3.4.1 with this. (or perhaps I'm just way off in general).

",phunt,phunt,Blocker,Closed,Fixed,28/Oct/11 18:01,23/Nov/11 19:22
Bug,ZOOKEEPER-1269,12529317,Multi deserialization issues,"From the mailing list:

FileTxnSnapLog.restore contains a code block handling a NODEEXISTS failure during deserialization. The problem is explained there in a code comment. The code block however is only executed for a CREATE txn, not for a multiTxn containing a CREATE.

Even if the mentioned code block would also be executed for multi transactions, it needs adaption for multi transactions. What, if after the first failed transaction in a multi txn during deserialization, there would be subsequent transactions in the same multi that would also have failed?
We don't know, since the first failed transaction hides the information about the remaining transactions.
",fournc,fournc,Major,Closed,Fixed,28/Oct/11 21:34,17/Dec/11 01:33
Bug,ZOOKEEPER-1270,12529326,"testEarlyLeaderAbandonment failing intermittently, quorum formed, no serving.","Looks pretty serious - quorum is formed but no clients can attach. Will attach logs momentarily.

This test was introduced in the following commit (all three jira commit at once):
ZOOKEEPER-335. zookeeper servers should commit the new leader txn to their logs.
ZOOKEEPER-1081. modify leader/follower code to correctly deal with new leader
ZOOKEEPER-1082. modify leader election to correctly take into account current",fpj,phunt,Blocker,Closed,Fixed,28/Oct/11 22:25,23/Nov/11 19:22
Bug,ZOOKEEPER-1271,12529327,testEarlyLeaderAbandonment failing on solaris - clients not retrying connection,"See:
https://builds.apache.org/view/S-Z/view/ZooKeeper/job/ZooKeeper_branch34_solaris/1/testReport/junit/org.apache.zookeeper.server.quorum/QuorumPeerMainTest/testEarlyLeaderAbandonment/

Notice that the clients attempt to connect before the servers have bound, then 30 seconds later, after seemingly no further client activity we see:

2011-10-28 21:40:56,828 [myid:] - INFO  [main-SendThread(localhost:11227):ClientCnxn$SendThread@1057] - Client session timed out, have not heard from server in 30032ms for sessionid 0x0, closing socket connection and attempting reconnect


I believe this is different from ZOOKEEPER-1270 because in the 1270 case it seems like the clients are attempting to connect but the servers are not accepting (notice the stat commands are being dropped due to no server running)",mahadev,phunt,Blocker,Closed,Fixed,28/Oct/11 22:33,23/Nov/11 19:21
Bug,ZOOKEEPER-1273,12529419,Copy'n'pasted unit test,"Probably caused by the usage of a legacy VCS a code duplication happened when you moved from Sourceforge to Apache (ZOOKEEPER-38). The following file can be deleted:
src/java/test/org/apache/zookeeper/server/DataTreeUnitTest.java

src/java/test/org/apache/zookeeper/test/DataTreeTest.java was an exact copy of the above until ZOOKEEPER-1046 added an additional test case only to the latter.

Do I need to upload a patch file for this?",thkoch,thkoch,Trivial,Resolved,Fixed,30/Oct/11 18:33,01/Nov/11 10:57
Bug,ZOOKEEPER-1277,12529896,servers stop serving when lower 32bits of zxid roll over,"When the lower 32bits of a zxid ""roll over"" (zxid is a 64 bit number, however the upper 32 are considered the epoch number) the epoch number (upper 32 bits) are incremented and the lower 32 start at 0 again.

This should work fine, however in the current 3.3 branch the followers see this as a NEWLEADER message, which it's not, and effectively stop serving clients. Attached clients seem to eventually time out given that heartbeats (or any operation) are no longer processed. The follower doesn't recover from this.

I've tested this out on 3.3 branch and confirmed this problem, however I haven't tried it on 3.4/3.5. It may not happen on the newer branches due to ZOOKEEPER-335, however there is certainly an issue with updating the ""acceptedEpoch"" files contained in the datadir. (I'll enter a separate jira for that)

",phunt,phunt,Critical,Resolved,Fixed,02/Nov/11 16:46,15/Sep/20 06:18
Bug,ZOOKEEPER-1283,12529995,building 3.3 branch fails with Ant 1.8.2 (success with 1.7.1 though),"I tried to compile 3.3.3 or the current 3.3 branch head, in both cases using ant 1.8.2 fails, however 1.7.0 is successful

here's the error:
{noformat}
Testsuite: org.apache.zookeeper.VerGenTest
Tests run: 1, Failures: 1, Errors: 0, Time elapsed: 0.009 sec

Testcase: warning took 0.001 sec
	FAILED
Class org.apache.zookeeper.VerGenTest has no public constructor TestCase(String name) or TestCase()
junit.framework.AssertionFailedError: Class org.apache.zookeeper.VerGenTest has no public constructor TestCase(String name) or TestCase()
{noformat}
",phunt,phunt,Blocker,Closed,Fixed,03/Nov/11 05:35,29/Nov/11 17:54
Bug,ZOOKEEPER-1294,12530842,One of the zookeeper server is not accepting any requests,"In zoo.cfg i have configured as
server.1 = XX.XX.XX.XX:65175:65173
server.2 = XX.XX.XX.XX:65185:65183
server.3 = XX.XX.XX.XX:65195:65193
server.4 = XX.XX.XX.XX:65205:65203:observer
server.5 = XX.XX.XX.XX:65215:65213:observer
server.6 = XX.XX.XX.XX:65225:65223:observer

Like above I have configured 3 PARTICIPANTS and 3 OBSERVERS
in the cluster of 6 zookeepers

Steps to reproduce the defect
1. Start all the 3 participant zookeeper
2. Stop all the participant zookeeper
3. Start zookeeper 1(Participant)
4. Start zookeeper 2(Participant)
5. Start zookeeper 4(Observer)
6. Create a persistent node with external client and close it
7. Stop the zookeeper 1(Participant neo quorum is unstable)
8. Create a new client and try to find the node created b4 using exists api (will fail since quorum not statisfied)
9. Start the Zookeeper 1 (Participant stabilise the quorum)

Now check the observer using 4 letter word (Server.4)
linux-216:/home/amith/CI/source/install/zookeeper/zookeeper2/bin # echo stat | netcat localhost 65200
Zookeeper version: 3.3.2-1031432, built on 11/05/2010 05:32 GMT
Clients:
 /127.0.0.1:46370[0](queued=0,recved=1,sent=0)

Latency min/avg/max: 0/0/0
Received: 1
Sent: 0
Outstanding: 0
Zxid: 0x100000003
Mode: observer
Node count: 5

check the participant 2 with 4 letter word

Latency min/avg/max: 22/48/83
Received: 39
Sent: 3
Outstanding: 35
Zxid: 0x100000003
Mode: leader
Node count: 5
linux-216:/home/amith/CI/source/install/zookeeper/zookeeper2/bin #

check the participant 1 with 4 letter word

linux-216:/home/amith/CI/source/install/zookeeper/zookeeper2/bin # echo stat | netcat localhost 65170
This ZooKeeper instance is not currently serving requests

We can see the participant1 logs filled with
2011-11-08 15:49:51,360 - WARN  [NIOServerCxn.Factory:0.0.0.0/0.0.0.0:65170:NIOServerCnxn@642] - Exception causing close of session 0x0 due to java.io.IOException: ZooKeeperServer not running


Problem here is participent1 is not responding / accepting any requests",kavita sharma,amithdk,Major,Resolved,Fixed,09/Nov/11 09:54,25/May/13 00:17
Bug,ZOOKEEPER-1295,12530914,Documentation for jute.maxbuffer is not correct in ZooKeeper Administrator's Guide,"The jute maxbuffer size is documented as being defaulted to 1 megabyte in the administrators guide.  I believe that this is true server side but it is not true client side.  On the client side the default is (at least in 3.3.2) this:

packetLen = Integer.getInteger(""jute.maxbuffer"", 4096 * 1024);

On the server side the documentation looks to be correct:
    private static int determineMaxBuffer() {
        String maxBufferString = System.getProperty(""jute.maxbuffer"");
        try {
            return Integer.parseInt(maxBufferString);
        } catch(Exception e) {
            return 0xfffff;
        }
        
    }

The documentation states this:
jute.maxbuffer:
(Java system property: jute.maxbuffer)

This option can only be set as a Java system property. There is no zookeeper prefix on it. It specifies the maximum size of the data that can be stored in a znode. The default is 0xfffff, or just under 1M. If this option is changed, the system property must be set on all servers and clients otherwise problems will arise. This is really a sanity check. ZooKeeper is designed to store data on the order of kilobytes in size.",arshad.mohammad,dlord,Major,Resolved,Fixed,09/Nov/11 19:23,27/Oct/19 09:28
Bug,ZOOKEEPER-1299,12531579,Add winconfig.h file to ignore in release audit.,We need to add the winconfig.h to ignores in release audits.,mahadev,mahadev,Major,Closed,Fixed,16/Nov/11 06:53,23/Nov/11 19:22
Bug,ZOOKEEPER-1305,12531976,zookeeper.c:prepend_string func can dereference null ptr,"All the callers of the function prepend_string make a call to prepend_string before checking that zhandle_t *zh is not null. At the top of prepend_string, zh is dereferenced without checking for a null ptr:

static char* prepend_string(zhandle_t *zh, const char* client_path) {
    char *ret_str;
    if (zh->chroot == NULL)
        return (char *) client_path;

I propose fixing this by adding the check here in prepend_string:

static char* prepend_string(zhandle_t *zh, const char* client_path) {
    char *ret_str;
    if (zh==NULL || zh->chroot == NULL)
        return (char *) client_path;
",dlescohier,dlescohier,Major,Closed,Fixed,18/Nov/11 18:23,03/May/12 02:06
Bug,ZOOKEEPER-1307,12532441,zkCli.sh is exiting when an Invalid ACL exception is thrown from setACL command through client,"use consoleClient (zkCli.sh) and issue setAcl /temp abc
[zk: XX.XX.XX.XX:XXXX(CONNECTED) 17] setAcl /temp abc
abc does not have the form scheme:id:perm
Exception in thread ""main"" org.apache.zookeeper.KeeperException$InvalidACLException: KeeperErrorCode = InvalidACL
        at org.apache.zookeeper.ZooKeeper.setACL(ZooKeeper.java:1172)
        at org.apache.zookeeper.ZooKeeperMain.processZKCmd(ZooKeeperMain.java:717)
        at org.apache.zookeeper.ZooKeeperMain.processCmd(ZooKeeperMain.java:582)
        at org.apache.zookeeper.ZooKeeperMain.executeLine(ZooKeeperMain.java:354)
        at org.apache.zookeeper.ZooKeeperMain.run(ZooKeeperMain.java:312)
        at org.apache.zookeeper.ZooKeeperMain.main(ZooKeeperMain.java:271)
linux-xxx:/zookeeper1/bin #

if any InvalidACLException is thrown then client is exiting.
client should be able to handle this kind of issues

",kavita sharma,amithdk,Minor,Resolved,Fixed,23/Nov/11 09:07,23/Apr/12 17:17
Bug,ZOOKEEPER-1309,12533050,Creating a new ZooKeeper client can leak file handles,"If there is an IOException thrown by the constructor of ClientCnxn then file handles are leaked because of the initialization of the Selector which is never closed.

    final Selector selector = Selector.open();

If there is an abnormal exit from the constructor then the Selector is not closed and file handles are leaked.  You can easily see this by setting the hosts string to garbage (""qwerty"", ""asdf"", etc.) and then try to open a new ZooKeeper connection.  I've observed the same behavior in production when there were DNS issues where the host names of the ensemble can no longer be resolved and the application servers quickly run out of handles attempting to (re)connect to zookeeper.",dlord,dlord,Critical,Resolved,Fixed,29/Nov/11 02:03,27/Feb/12 00:32
Bug,ZOOKEEPER-1311,12533322,ZooKeeper test jar is broken,"In http://repo1.maven.org/maven2/org/apache/zookeeper/zookeeper/3.4.0/ the test jar cannot be accessed by maven. 

There are two possible solutions to this. 
a) rename zookeeper-3.4.0-test.jar to zookeeper-3.4.0-tests.jar and remove zookeeper-3.4.0-test.pom*
With this, the maven can access the test jar with

{code}
     <dependency>
       <groupId>org.apache.zookeeper</groupId>
       <artifactId>zookeeper</artifactId>
       <version>3.4.0</version>
       <type>test-jar</type>
       <scope>test</scope>
     </dependency>
{code}

b) Alternatively, zookeeper test could be it's own submodule. To do this, it must be deployed in the following layout
{code}
./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.jar
./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.jar.md5
./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.jar.sha1
./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.pom
./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.pom.md5
./org/apache/zookeeper/zookeeper-test/3.4.0-BK-SNAPSHOT/zookeeper-test-3.4.0.pom.sha1
{code}

This can then be accessed by maven with
{code}
     <dependency>
       <groupId>org.apache.zookeeper</groupId>
       <artifactId>zookeeper-test</artifactId>
       <version>3.4.0</version>
       <scope>test</scope>
     </dependency>
{code}


I think a) is the better solution.
",ikelly,ikelly,Blocker,Closed,Fixed,30/Nov/11 18:04,17/Dec/11 01:33
Bug,ZOOKEEPER-1315,12533776,zookeeper_init always reports sessionPasswd=<hidden>,zookeeper_init always reports sessionPasswd=<hidden> even when it's empty.,akitada,akitada,Minor,Closed,Fixed,04/Dec/11 10:31,17/Dec/11 01:33
Bug,ZOOKEEPER-1316,12533781,zookeeper_init leaks memory if chroot is just '/',"zookeeper_init does not free strdup'ed memory when chroot is just '/'.
",akitada,akitada,Minor,Closed,Fixed,04/Dec/11 13:34,17/Dec/11 01:33
Bug,ZOOKEEPER-1317,12533782,Possible segfault in zookeeper_init,"zookeeper_init does not check the return value of strdup(index_chroot).
When it returns NULL, it causes segfault when it try to strlen(zh->chroot).",akitada,akitada,Minor,Closed,Fixed,04/Dec/11 13:48,17/Dec/11 01:33
Bug,ZOOKEEPER-1318,12533800,"In Python binding, get_children (and get and exists, and probably others) with expired session doesn't raise exception properly","In Python binding, get_children (and get and exists, and probably others) with expired session doesn't raise exception properly.


>>> zookeeper.state(h)
-112
>>> zookeeper.get_children(h, '/')
Traceback (most recent call last):
  File ""<console>"", line 1, in <module>
SystemError: error return without exception set

Let me know if you'd like me to work on a patch.",henryr,j1m,Major,Resolved,Fixed,04/Dec/11 19:06,11/May/12 11:00
Bug,ZOOKEEPER-1319,12533982,Missing data after restarting+expanding a cluster,"I've been trying to update to ZK 3.4.0 and have had some issues where some data become inaccessible after adding a node to a cluster.  My use case is a bit strange (as explained before on this list) in that I try to grow the cluster dynamically by having an external program automatically restart Zookeeper servers in a controlled way whenever the list of participating ZK servers needs to change.  This used to work just fine in 3.3.3 (and before), so this represents a regression.

The scenario I see is this:

1) Start up a 1-server ZK cluster (the server has ZK ID 0).
2) A client connects to the server, and makes a bunch of znodes, in particular a znode called ""/membership"".
3) Shut down the cluster.
4) Bring up a 2-server ZK cluster, including the original server 0 with its existing data, and a new server with ZK ID 1.
5) Node 0 has the highest zxid and is elected leader.
6) A client connecting to server 1 tries to ""get /membership"" and gets back a -101 error code (no such znode).
7) The same client then tries to ""create /membership"" and gets back a -110 error code (znode already exists).
8) Clients connecting to server 0 can successfully ""get /membership"".

I will attach a tarball with debug logs for both servers, annotating where steps #1 and #4 happen.  You can see that the election involves a proposal for zxid 110 from server 0, but immediately following the election server 1 has these lines:

2011-12-05 17:18:48,308 9299 [QuorumPeer[myid=1]/127.0.0.1:2901] WARN org.apache.zookeeper.server.quorum.Learner  - Got zxid 0x100000001 expected 0x1
2011-12-05 17:18:48,313 9304 [SyncThread:1] INFO org.apache.zookeeper.server.persistence.FileTxnLog  - Creating new log file: log.100000001

Perhaps that's not relevant, but it struck me as odd.  At the end of server 1's log you can see a repeated cycle of getData->create->getData as the client tries to make sense of the inconsistent responses.

The other piece of information is that if I try to use the on-disk directories for either of the servers to start a new one-node ZK cluster, all the data are accessible.

I haven't tried writing a program outside of my application to reproduce this, but I can do it very easily with some of my app's tests if anyone needs more information.",phunt,strib,Blocker,Closed,Fixed,06/Dec/11 03:06,17/Dec/11 01:33
Bug,ZOOKEEPER-1323,12534419,c client doesn't compile on freebsd,"EAI_NODATA and EAI_ADDRFAMILY have been deprecated in FreeBSD. I'm getting this error:

src/zookeeper.c: In function `getaddrinfo_errno':
src/zookeeper.c:446: error: `EAI_NODATA' undeclared (first use in this function)
src/zookeeper.c:446: error: (Each undeclared identifier is reported only once
src/zookeeper.c:446: error: for each function it appears in.)
src/zookeeper.c: In function `getaddrs':
src/zookeeper.c:581: error: `EAI_ADDRFAMILY' undeclared (first use in this function)

I'll submit a patch.

--Michi",michim,michim,Major,Closed,Fixed,09/Dec/11 01:28,29/Dec/11 23:46
Bug,ZOOKEEPER-1327,12534903,there are still remnants of hadoop urls,there are still hadoop urls and references to zookeeper lists under the hadoop project in the sources.,qwertymaniac,breed,Major,Resolved,Fixed,13/Dec/11 04:25,06/Feb/12 10:58
Bug,ZOOKEEPER-1331,12535659,Typo in docs: acheive -> achieve,Found this typo while reading docs.  Attaching SVN patch,ash211,ash211,Minor,Resolved,Fixed,19/Dec/11 07:10,28/Dec/11 10:58
Bug,ZOOKEEPER-1333,12535772,NPE in FileTxnSnapLog when restarting a cluster,"I think a NPE was created in the fix for https://issues.apache.org/jira/browse/ZOOKEEPER-1269

Looking in DataTree.processTxn(TxnHeader header, Record txn) it seems likely that if rc.err != Code.OK then rc.path will be null. 

I'm currently working on a minimal test case for the bug, I'll attach it to this issue when it's ready.

java.lang.NullPointerException
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.processTransaction(FileTxnSnapLog.java:203)
	at org.apache.zookeeper.server.persistence.FileTxnSnapLog.restore(FileTxnSnapLog.java:150)
	at org.apache.zookeeper.server.ZKDatabase.loadDataBase(ZKDatabase.java:223)
	at org.apache.zookeeper.server.quorum.QuorumPeer.loadDataBase(QuorumPeer.java:418)
	at org.apache.zookeeper.server.quorum.QuorumPeer.start(QuorumPeer.java:410)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.runFromConfig(QuorumPeerMain.java:151)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.initializeAndRun(QuorumPeerMain.java:111)
	at org.apache.zookeeper.server.quorum.QuorumPeerMain.main(QuorumPeerMain.java:78)


",phunt,amcnair,Blocker,Closed,Fixed,20/Dec/11 01:24,29/Dec/11 23:46
Bug,ZOOKEEPER-1334,12535866,Zookeeper 3.4.x is not OSGi compliant - MANIFEST.MF is flawed,"In Zookeeper 3.3.x you use log4j for logging, and the maven dep is

eg from 3.3.4
{code}
    <dependency>
      <groupId>log4j</groupId>
      <artifactId>log4j</artifactId>
      <version>1.2.15</version>
      <scope>compile</scope>
    </dependency>
{code}

Now in 3.4.0 or better you changed to use slf4j also/instead. The maven pom.xml now includes:
{code}
  <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-api</artifactId>
      <version>1.6.1</version>
      <scope>compile</scope>
    </dependency>
    <dependency>
      <groupId>org.slf4j</groupId>
      <artifactId>slf4j-log4j12</artifactId>
      <version>1.6.1</version>
      <scope>compile</scope>
    </dependency>
    <dependency>
      <groupId>log4j</groupId>
      <artifactId>log4j</artifactId>
      <version>1.2.15</version>
      <scope>compile</scope>
    </dependency>
{code}

But the META-INF/MANIFEST.MF file in the distribution did not change to reflect this.

The 3.3.4 MANIFEST.MF, import packages
{code}
Import-Package: javax.management,org.apache.log4j,org.osgi.framework;v
 ersion=""[1.4,2.0)"",org.osgi.util.tracker;version=""[1.1,2.0)""
{code}

And the 3.4.1 MANIFEST.MF, import packages:
{code}
Import-Package: javax.management,org.apache.log4j,org.osgi.framework;v
 ersion=""[1.4,2.0)"",org.osgi.util.tracker;version=""[1.1,2.0)""
{code}

This makes using zookeeper 3.4.x in OSGi environments not possible, as we get NoClassDefFoundException for slf4j classes.",davsclaus,davsclaus,Major,Closed,Fixed,20/Dec/11 16:31,08/Oct/14 15:55
Bug,ZOOKEEPER-1336,12536099,"javadoc for multi is confusing, references functionality that doesn't seem to exist ","There's this in org.apache.zookeeper.ZooKeeper.multi(Iterable<Op>)

{noformat}
     * Executes multiple Zookeeper operations or none of them.  On success, a list of results is returned.
     * On failure, only a single exception is returned.  If you want more details, it may be preferable to
     * use the alternative form of this method that lets you pass a list into which individual results are
     * placed so that you can zero in on exactly which operation failed and why.
{noformat}

What is the ""alternate form of this method"" that's being referred to? Seems like we should add this functionality, or at the very least update the javadoc. (I don't think this is referring to Transaction, although the docs there are pretty thin)

",phunt,phunt,Major,Resolved,Fixed,21/Dec/11 23:24,06/Feb/12 10:58
Bug,ZOOKEEPER-1338,12536106,class cast exceptions may be thrown by multi ErrorResult class (invalid equals),There's a bug in ErrorResult and perhaps some of the other OpResult equals methods in multi.,phunt,phunt,Major,Resolved,Fixed,22/Dec/11 00:30,06/Feb/12 10:58
Bug,ZOOKEEPER-1339,12536141,C clien doesn't build with --enable-debug,"When I'm trying to build 3.4.1 c client with --enable-debug switch I'm getting following error:

{code}
make  all-am
make[1]: Entering directory `/home/jlekstan/zookeeper-3.4.1/src/c'
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF "".deps/zookeeper.Tpo"" -c -o zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.c; \
	then mv -f "".deps/zookeeper.Tpo"" "".deps/zookeeper.Plo""; else rm -f "".deps/zookeeper.Tpo""; exit 1; fi
mkdir .libs
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c  -fPIC -DPIC -o .libs/zookeeper.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zookeeper.lo -MD -MP -MF .deps/zookeeper.Tpo -c src/zookeeper.c -o zookeeper.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT recordio.lo -MD -MP -MF "".deps/recordio.Tpo"" -c -o recordio.lo `test -f 'src/recordio.c' || echo './'`src/recordio.c; \
	then mv -f "".deps/recordio.Tpo"" "".deps/recordio.Plo""; else rm -f "".deps/recordio.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT recordio.lo -MD -MP -MF .deps/recordio.Tpo -c src/recordio.c  -fPIC -DPIC -o .libs/recordio.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT recordio.lo -MD -MP -MF .deps/recordio.Tpo -c src/recordio.c -o recordio.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT zookeeper.jute.lo -MD -MP -MF "".deps/zookeeper.jute.Tpo"" -c -o zookeeper.jute.lo `test -f 'generated/zookeeper.jute.c' || echo './'`generated/zookeeper.jute.c; \
	then mv -f "".deps/zookeeper.jute.Tpo"" "".deps/zookeeper.jute.Plo""; else rm -f "".deps/zookeeper.jute.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zookeeper.jute.lo -MD -MP -MF .deps/zookeeper.jute.Tpo -c generated/zookeeper.jute.c  -fPIC -DPIC -o .libs/zookeeper.jute.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zookeeper.jute.lo -MD -MP -MF .deps/zookeeper.jute.Tpo -c generated/zookeeper.jute.c -o zookeeper.jute.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT zk_log.lo -MD -MP -MF "".deps/zk_log.Tpo"" -c -o zk_log.lo `test -f 'src/zk_log.c' || echo './'`src/zk_log.c; \
	then mv -f "".deps/zk_log.Tpo"" "".deps/zk_log.Plo""; else rm -f "".deps/zk_log.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zk_log.lo -MD -MP -MF .deps/zk_log.Tpo -c src/zk_log.c  -fPIC -DPIC -o .libs/zk_log.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zk_log.lo -MD -MP -MF .deps/zk_log.Tpo -c src/zk_log.c -o zk_log.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT zk_hashtable.lo -MD -MP -MF "".deps/zk_hashtable.Tpo"" -c -o zk_hashtable.lo `test -f 'src/zk_hashtable.c' || echo './'`src/zk_hashtable.c; \
	then mv -f "".deps/zk_hashtable.Tpo"" "".deps/zk_hashtable.Plo""; else rm -f "".deps/zk_hashtable.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zk_hashtable.lo -MD -MP -MF .deps/zk_hashtable.Tpo -c src/zk_hashtable.c  -fPIC -DPIC -o .libs/zk_hashtable.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT zk_hashtable.lo -MD -MP -MF .deps/zk_hashtable.Tpo -c src/zk_hashtable.c -o zk_hashtable.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT st_adaptor.lo -MD -MP -MF "".deps/st_adaptor.Tpo"" -c -o st_adaptor.lo `test -f 'src/st_adaptor.c' || echo './'`src/st_adaptor.c; \
	then mv -f "".deps/st_adaptor.Tpo"" "".deps/st_adaptor.Plo""; else rm -f "".deps/st_adaptor.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT st_adaptor.lo -MD -MP -MF .deps/st_adaptor.Tpo -c src/st_adaptor.c  -fPIC -DPIC -o .libs/st_adaptor.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT st_adaptor.lo -MD -MP -MF .deps/st_adaptor.Tpo -c src/st_adaptor.c -o st_adaptor.o >/dev/null 2>&1
/bin/bash ./libtool --tag=CC --mode=link gcc -Wall -Werror  -g -O0 -D_GNU_SOURCE   -o libzkst.la   zookeeper.lo recordio.lo zookeeper.jute.lo zk_log.lo zk_hashtable.lo st_adaptor.lo -lm 
ar cru .libs/libzkst.a .libs/zookeeper.o .libs/recordio.o .libs/zookeeper.jute.o .libs/zk_log.o .libs/zk_hashtable.o .libs/st_adaptor.o
ranlib .libs/libzkst.a
creating libzkst.la
(cd .libs && rm -f libzkst.la && ln -s ../libzkst.la libzkst.la)
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT hashtable_itr.lo -MD -MP -MF "".deps/hashtable_itr.Tpo"" -c -o hashtable_itr.lo `test -f 'src/hashtable/hashtable_itr.c' || echo './'`src/hashtable/hashtable_itr.c; \
	then mv -f "".deps/hashtable_itr.Tpo"" "".deps/hashtable_itr.Plo""; else rm -f "".deps/hashtable_itr.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT hashtable_itr.lo -MD -MP -MF .deps/hashtable_itr.Tpo -c src/hashtable/hashtable_itr.c  -fPIC -DPIC -o .libs/hashtable_itr.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT hashtable_itr.lo -MD -MP -MF .deps/hashtable_itr.Tpo -c src/hashtable/hashtable_itr.c -o hashtable_itr.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT hashtable.lo -MD -MP -MF "".deps/hashtable.Tpo"" -c -o hashtable.lo `test -f 'src/hashtable/hashtable.c' || echo './'`src/hashtable/hashtable.c; \
	then mv -f "".deps/hashtable.Tpo"" "".deps/hashtable.Plo""; else rm -f "".deps/hashtable.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT hashtable.lo -MD -MP -MF .deps/hashtable.Tpo -c src/hashtable/hashtable.c  -fPIC -DPIC -o .libs/hashtable.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -Wall -Werror -g -O0 -D_GNU_SOURCE -MT hashtable.lo -MD -MP -MF .deps/hashtable.Tpo -c src/hashtable/hashtable.c -o hashtable.o >/dev/null 2>&1
/bin/bash ./libtool --tag=CC --mode=link gcc -Wall -Werror  -g -O0 -D_GNU_SOURCE   -o libhashtable.la   hashtable_itr.lo hashtable.lo  
ar cru .libs/libhashtable.a .libs/hashtable_itr.o .libs/hashtable.o
ranlib .libs/libhashtable.a
creating libhashtable.la
(cd .libs && rm -f libhashtable.la && ln -s ../libhashtable.la libhashtable.la)
/bin/bash ./libtool --tag=CC --mode=link gcc -Wall -Werror  -g -O0 -D_GNU_SOURCE   -o libzookeeper_st.la -rpath /usr/local/lib -no-undefined -version-info 2 -export-symbols-regex '(zoo_|zookeeper_|zhandle|Z|format_log_message|log_message|logLevel|deallocate_|zerror|is_unrecoverable)'  libzkst.la libhashtable.la 
generating symbol list for `libzookeeper_st.la'
/usr/bin/nm -B   ./.libs/libzkst.a ./.libs/libhashtable.a | sed -n -e 's/^.*[ 	]\([ABCDGIRSTW][ABCDGIRSTW]*\)[ 	][ 	]*\([_A-Za-z][_A-Za-z0-9]*\)$/\1 \2 \2/p' | /bin/sed 's/.* //' | sort | uniq > .libs/libzookeeper_st.exp
grep -E -e ""(zoo_|zookeeper_|zhandle|Z|format_log_message|log_message|logLevel|deallocate_|zerror|is_unrecoverable)"" "".libs/libzookeeper_st.exp"" > "".libs/libzookeeper_st.expT""
mv -f "".libs/libzookeeper_st.expT"" "".libs/libzookeeper_st.exp""
echo ""{ global:"" > .libs/libzookeeper_st.ver
 cat .libs/libzookeeper_st.exp | sed -e ""s/\(.*\)/\1;/"" >> .libs/libzookeeper_st.ver
 echo ""local: *; };"" >> .libs/libzookeeper_st.ver
 gcc -shared  -Wl,--whole-archive ./.libs/libzkst.a ./.libs/libhashtable.a -Wl,--no-whole-archive  -lm  -Wl,-soname -Wl,libzookeeper_st.so.2 -Wl,-version-script -Wl,.libs/libzookeeper_st.ver -o .libs/libzookeeper_st.so.2.0.0
(cd .libs && rm -f libzookeeper_st.so.2 && ln -s libzookeeper_st.so.2.0.0 libzookeeper_st.so.2)
(cd .libs && rm -f libzookeeper_st.so && ln -s libzookeeper_st.so.2.0.0 libzookeeper_st.so)
rm -fr .libs/libzookeeper_st.lax
mkdir .libs/libzookeeper_st.lax
rm -fr .libs/libzookeeper_st.lax/libzkst.a
mkdir .libs/libzookeeper_st.lax/libzkst.a
(cd .libs/libzookeeper_st.lax/libzkst.a && ar x /home/jlekstan/zookeeper-3.4.1/src/c/./.libs/libzkst.a)
rm -fr .libs/libzookeeper_st.lax/libhashtable.a
mkdir .libs/libzookeeper_st.lax/libhashtable.a
(cd .libs/libzookeeper_st.lax/libhashtable.a && ar x /home/jlekstan/zookeeper-3.4.1/src/c/./.libs/libhashtable.a)
ar cru .libs/libzookeeper_st.a   .libs/libzookeeper_st.lax/libzkst.a/zookeeper.o .libs/libzookeeper_st.lax/libzkst.a/st_adaptor.o .libs/libzookeeper_st.lax/libzkst.a/recordio.o .libs/libzookeeper_st.lax/libzkst.a/zk_hashtable.o .libs/libzookeeper_st.lax/libzkst.a/zk_log.o .libs/libzookeeper_st.lax/libzkst.a/zookeeper.jute.o  .libs/libzookeeper_st.lax/libhashtable.a/hashtable_itr.o .libs/libzookeeper_st.lax/libhashtable.a/hashtable.o 
ranlib .libs/libzookeeper_st.a
rm -fr .libs/libzookeeper_st.lax
creating libzookeeper_st.la
(cd .libs && rm -f libzookeeper_st.la && ln -s ../libzookeeper_st.la libzookeeper_st.la)
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zookeeper.lo -MD -MP -MF "".deps/libzkmt_la-zookeeper.Tpo"" -c -o libzkmt_la-zookeeper.lo `test -f 'src/zookeeper.c' || echo './'`src/zookeeper.c; \
	then mv -f "".deps/libzkmt_la-zookeeper.Tpo"" "".deps/libzkmt_la-zookeeper.Plo""; else rm -f "".deps/libzkmt_la-zookeeper.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zookeeper.lo -MD -MP -MF .deps/libzkmt_la-zookeeper.Tpo -c src/zookeeper.c  -fPIC -DPIC -o .libs/libzkmt_la-zookeeper.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zookeeper.lo -MD -MP -MF .deps/libzkmt_la-zookeeper.Tpo -c src/zookeeper.c -o libzkmt_la-zookeeper.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-recordio.lo -MD -MP -MF "".deps/libzkmt_la-recordio.Tpo"" -c -o libzkmt_la-recordio.lo `test -f 'src/recordio.c' || echo './'`src/recordio.c; \
	then mv -f "".deps/libzkmt_la-recordio.Tpo"" "".deps/libzkmt_la-recordio.Plo""; else rm -f "".deps/libzkmt_la-recordio.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-recordio.lo -MD -MP -MF .deps/libzkmt_la-recordio.Tpo -c src/recordio.c  -fPIC -DPIC -o .libs/libzkmt_la-recordio.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-recordio.lo -MD -MP -MF .deps/libzkmt_la-recordio.Tpo -c src/recordio.c -o libzkmt_la-recordio.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zookeeper.jute.lo -MD -MP -MF "".deps/libzkmt_la-zookeeper.jute.Tpo"" -c -o libzkmt_la-zookeeper.jute.lo `test -f 'generated/zookeeper.jute.c' || echo './'`generated/zookeeper.jute.c; \
	then mv -f "".deps/libzkmt_la-zookeeper.jute.Tpo"" "".deps/libzkmt_la-zookeeper.jute.Plo""; else rm -f "".deps/libzkmt_la-zookeeper.jute.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zookeeper.jute.lo -MD -MP -MF .deps/libzkmt_la-zookeeper.jute.Tpo -c generated/zookeeper.jute.c  -fPIC -DPIC -o .libs/libzkmt_la-zookeeper.jute.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zookeeper.jute.lo -MD -MP -MF .deps/libzkmt_la-zookeeper.jute.Tpo -c generated/zookeeper.jute.c -o libzkmt_la-zookeeper.jute.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zk_log.lo -MD -MP -MF "".deps/libzkmt_la-zk_log.Tpo"" -c -o libzkmt_la-zk_log.lo `test -f 'src/zk_log.c' || echo './'`src/zk_log.c; \
	then mv -f "".deps/libzkmt_la-zk_log.Tpo"" "".deps/libzkmt_la-zk_log.Plo""; else rm -f "".deps/libzkmt_la-zk_log.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zk_log.lo -MD -MP -MF .deps/libzkmt_la-zk_log.Tpo -c src/zk_log.c  -fPIC -DPIC -o .libs/libzkmt_la-zk_log.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zk_log.lo -MD -MP -MF .deps/libzkmt_la-zk_log.Tpo -c src/zk_log.c -o libzkmt_la-zk_log.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zk_hashtable.lo -MD -MP -MF "".deps/libzkmt_la-zk_hashtable.Tpo"" -c -o libzkmt_la-zk_hashtable.lo `test -f 'src/zk_hashtable.c' || echo './'`src/zk_hashtable.c; \
	then mv -f "".deps/libzkmt_la-zk_hashtable.Tpo"" "".deps/libzkmt_la-zk_hashtable.Plo""; else rm -f "".deps/libzkmt_la-zk_hashtable.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zk_hashtable.lo -MD -MP -MF .deps/libzkmt_la-zk_hashtable.Tpo -c src/zk_hashtable.c  -fPIC -DPIC -o .libs/libzkmt_la-zk_hashtable.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-zk_hashtable.lo -MD -MP -MF .deps/libzkmt_la-zk_hashtable.Tpo -c src/zk_hashtable.c -o libzkmt_la-zk_hashtable.o >/dev/null 2>&1
if /bin/bash ./libtool --tag=CC --mode=compile gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-mt_adaptor.lo -MD -MP -MF "".deps/libzkmt_la-mt_adaptor.Tpo"" -c -o libzkmt_la-mt_adaptor.lo `test -f 'src/mt_adaptor.c' || echo './'`src/mt_adaptor.c; \
	then mv -f "".deps/libzkmt_la-mt_adaptor.Tpo"" "".deps/libzkmt_la-mt_adaptor.Plo""; else rm -f "".deps/libzkmt_la-mt_adaptor.Tpo""; exit 1; fi
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-mt_adaptor.lo -MD -MP -MF .deps/libzkmt_la-mt_adaptor.Tpo -c src/mt_adaptor.c  -fPIC -DPIC -o .libs/libzkmt_la-mt_adaptor.o
 gcc -DHAVE_CONFIG_H -I. -I. -I. -I./include -I./tests -I./generated -DTHREADED -g -O0 -D_GNU_SOURCE -MT libzkmt_la-mt_adaptor.lo -MD -MP -MF .deps/libzkmt_la-mt_adaptor.Tpo -c src/mt_adaptor.c -o libzkmt_la-mt_adaptor.o >/dev/null 2>&1
/bin/bash ./libtool --tag=CC --mode=link gcc -Wall -Werror  -g -O0 -D_GNU_SOURCE   -o libzkmt.la   libzkmt_la-zookeeper.lo libzkmt_la-recordio.lo libzkmt_la-zookeeper.jute.lo libzkmt_la-zk_log.lo libzkmt_la-zk_hashtable.lo libzkmt_la-mt_adaptor.lo -lm 
ar cru .libs/libzkmt.a .libs/libzkmt_la-zookeeper.o .libs/libzkmt_la-recordio.o .libs/libzkmt_la-zookeeper.jute.o .libs/libzkmt_la-zk_log.o .libs/libzkmt_la-zk_hashtable.o .libs/libzkmt_la-mt_adaptor.o
ranlib .libs/libzkmt.a
creating libzkmt.la
(cd .libs && rm -f libzkmt.la && ln -s ../libzkmt.la libzkmt.la)
/bin/bash ./libtool --tag=CC --mode=link gcc -Wall -Werror  -g -O0 -D_GNU_SOURCE   -o libzookeeper_mt.la -rpath /usr/local/lib -no-undefined -version-info 2 -export-symbols-regex '(zoo_|zookeeper_|zhandle|Z|format_log_message|log_message|logLevel|deallocate_|zerror|is_unrecoverable)'  libzkmt.la libhashtable.la -lpthread 
generating symbol list for `libzookeeper_mt.la'
/usr/bin/nm -B   ./.libs/libzkmt.a ./.libs/libhashtable.a | sed -n -e 's/^.*[ 	]\([ABCDGIRSTW][ABCDGIRSTW]*\)[ 	][ 	]*\([_A-Za-z][_A-Za-z0-9]*\)$/\1 \2 \2/p' | /bin/sed 's/.* //' | sort | uniq > .libs/libzookeeper_mt.exp
grep -E -e ""(zoo_|zookeeper_|zhandle|Z|format_log_message|log_message|logLevel|deallocate_|zerror|is_unrecoverable)"" "".libs/libzookeeper_mt.exp"" > "".libs/libzookeeper_mt.expT""
mv -f "".libs/libzookeeper_mt.expT"" "".libs/libzookeeper_mt.exp""
echo ""{ global:"" > .libs/libzookeeper_mt.ver
 cat .libs/libzookeeper_mt.exp | sed -e ""s/\(.*\)/\1;/"" >> .libs/libzookeeper_mt.ver
 echo ""local: *; };"" >> .libs/libzookeeper_mt.ver
 gcc -shared  -Wl,--whole-archive ./.libs/libzkmt.a ./.libs/libhashtable.a -Wl,--no-whole-archive  -lm -lpthread  -Wl,-soname -Wl,libzookeeper_mt.so.2 -Wl,-version-script -Wl,.libs/libzookeeper_mt.ver -o .libs/libzookeeper_mt.so.2.0.0
(cd .libs && rm -f libzookeeper_mt.so.2 && ln -s libzookeeper_mt.so.2.0.0 libzookeeper_mt.so.2)
(cd .libs && rm -f libzookeeper_mt.so && ln -s libzookeeper_mt.so.2.0.0 libzookeeper_mt.so)
rm -fr .libs/libzookeeper_mt.lax
mkdir .libs/libzookeeper_mt.lax
rm -fr .libs/libzookeeper_mt.lax/libzkmt.a
mkdir .libs/libzookeeper_mt.lax/libzkmt.a
(cd .libs/libzookeeper_mt.lax/libzkmt.a && ar x /home/jlekstan/zookeeper-3.4.1/src/c/./.libs/libzkmt.a)
rm -fr .libs/libzookeeper_mt.lax/libhashtable.a
mkdir .libs/libzookeeper_mt.lax/libhashtable.a
(cd .libs/libzookeeper_mt.lax/libhashtable.a && ar x /home/jlekstan/zookeeper-3.4.1/src/c/./.libs/libhashtable.a)
ar cru .libs/libzookeeper_mt.a   .libs/libzookeeper_mt.lax/libzkmt.a/libzkmt_la-zk_hashtable.o .libs/libzookeeper_mt.lax/libzkmt.a/libzkmt_la-zookeeper.o .libs/libzookeeper_mt.lax/libzkmt.a/libzkmt_la-zk_log.o .libs/libzookeeper_mt.lax/libzkmt.a/libzkmt_la-zookeeper.jute.o .libs/libzookeeper_mt.lax/libzkmt.a/libzkmt_la-recordio.o .libs/libzookeeper_mt.lax/libzkmt.a/libzkmt_la-mt_adaptor.o  .libs/libzookeeper_mt.lax/libhashtable.a/hashtable_itr.o .libs/libzookeeper_mt.lax/libhashtable.a/hashtable.o 
ranlib .libs/libzookeeper_mt.a
rm -fr .libs/libzookeeper_mt.lax
creating libzookeeper_mt.la
(cd .libs && rm -f libzookeeper_mt.la && ln -s ../libzookeeper_mt.la libzookeeper_mt.la)
if gcc -DHAVE_CONFIG_H -I. -I. -I.  -I./include -I./tests -I./generated  -Wall -Werror  -g -O0 -D_GNU_SOURCE -MT cli.o -MD -MP -MF "".deps/cli.Tpo"" -c -o cli.o `test -f 'src/cli.c' || echo './'`src/cli.c; \
	then mv -f "".deps/cli.Tpo"" "".deps/cli.Po""; else rm -f "".deps/cli.Tpo""; exit 1; fi
/bin/bash ./libtool --tag=CC --mode=link gcc -Wall -Werror  -g -O0 -D_GNU_SOURCE   -o cli_st  cli.o libzookeeper_st.la 
gcc -Wall -Werror -g -O0 -D_GNU_SOURCE -o .libs/cli_st cli.o  ./.libs/libzookeeper_st.so -lm 
./.libs/libzookeeper_st.so: undefined reference to `hashtable_iterator_value'
./.libs/libzookeeper_st.so: undefined reference to `hashtable_iterator_key'
collect2: ld returned 1 exit status
make[1]: *** [cli_st] Error 1
make[1]: Leaving directory `/home/jlekstan/zookeeper-3.4.1/src/c'
make: *** [all] Error 2
{code}",start2046,kuebk,Major,Resolved,Fixed,22/Dec/11 10:00,08/May/12 18:04
Bug,ZOOKEEPER-1340,12536183,multi problem - typical user operations are generating ERROR level messages in the server,"Multi operations run by users are generating ERROR level messages in the server log even though they are typical user level operations that are not in any way impacting the server, example:

{noformat}
2011-12-22 09:55:06,538 [myid:] - ERROR [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@545] - >>>> Got user-level KeeperException when processing sessionid:0x13466e9828c0000 type:multi cxid:0x3 zxid:0x2 txntype:2 reqpath:n/a Error Path:/nonexisting Error:KeeperErrorCode = NoNode for /nonexisting
2011-12-22 09:55:06,538 [myid:] - ERROR [ProcessThread(sid:0 cport:-1)::PrepRequestProcessor@549] - >>>> ABORTING remaing MultiOp ops
{noformat}

This is misleading. We should demote these messages to INFO level at the highest. (this is what we do for other such user operations, e.g. nonode)
",phunt,phunt,Major,Resolved,Fixed,22/Dec/11 18:03,06/Feb/12 10:58
Bug,ZOOKEEPER-1343,12536279,getEpochToPropose should check if lastAcceptedEpoch is greater or equal than epoch,"The following block in Leader.getEpochToPropose:

{noformat}
if (lastAcceptedEpoch > epoch) {
	epoch = lastAcceptedEpoch+1;
}
{noformat}

needs to be fixed, since it doesn't increment the epoch variable in the case epoch != -1 (initial value) and lastAcceptedEpoch is equal. The fix trivial and corresponds to changing > with >=. ",fpj,fpj,Critical,Resolved,Fixed,23/Dec/11 12:44,09/Jan/12 22:50
Bug,ZOOKEEPER-1344,12536376,ZooKeeper client multi-update command is not considering the Chroot request,"For example: 
I have created a ZooKeeper client with subtree as ""10.18.52.144:2179/apps/X"". Now just generated OP command for the creation of zNode ""/myId"". When the client creates the path ""/myid"", the ZooKeeper server is actually be creating the path as ""/myid"" instead of creating as ""/apps/X/myid""

Expected output: zNode has to be created as ""/apps/X/myid""",rakeshr,rakeshr,Critical,Resolved,Fixed,26/Dec/11 13:36,18/Mar/12 04:54
Bug,ZOOKEEPER-1348,12536716,Zookeeper 3.4.2 C client incorrectly reports string version of 3.4.1,"When running the 3.4.2 C client, it shows the following output:

Client environment:zookeeper.version=zookeeper C client 3.4.1

This should show ""3.4.2"" not ""3.4.1"". The problem looks to be caused by stale autoconf files in the C directory. 

grep -R ""zookeeper C client 3.4.1"" *                                                                                                                                                                     

autom4te.cache/output.0:@%:@ Generated by GNU Autoconf 2.59 for zookeeper C client 3.4.1.
autom4te.cache/output.0:PACKAGE_STRING='zookeeper C client 3.4.1'
autom4te.cache/output.0:\`configure' configures zookeeper C client 3.4.1 to adapt to many kinds of systems.
autom4te.cache/output.0:     short | recursive ) echo ""Configuration of zookeeper C client 3.4.1:"";;
autom4te.cache/output.1:@%:@ Generated by GNU Autoconf 2.59 for zookeeper C client 3.4.1.
autom4te.cache/output.1:PACKAGE_STRING='zookeeper C client 3.4.1'
autom4te.cache/output.1:\`configure' configures zookeeper C client 3.4.1 to adapt to many kinds of systems.
autom4te.cache/output.1:     short | recursive ) echo ""Configuration of zookeeper C client 3.4.1:"";;
config.h:#define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:| #define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.log:PACKAGE_STRING='zookeeper C client 3.4.1'
config.log:#define PACKAGE_STRING ""zookeeper C client 3.4.1""
config.status:s,@PACKAGE_STRING@,zookeeper C client 3.4.1,;t t
config.status:${ac_dA}PACKAGE_STRING${ac_dB}PACKAGE_STRING${ac_dC}""zookeeper C client 3.4.1""${ac_dD}
config.status:${ac_uA}PACKAGE_STRING${ac_uB}PACKAGE_STRING${ac_uC}""zookeeper C client 3.4.1""${ac_uD}
configure:# Generated by GNU Autoconf 2.59 for zookeeper C client 3.4.1.
configure:PACKAGE_STRING='zookeeper C client 3.4.1'
configure:\`configure' configures zookeeper C client 3.4.1 to adapt to many kinds of systems.
configure:     short | recursive ) echo ""Configuration of zookeeper C client 3.4.1:"";;
Binary file libzkmt_la-zookeeper.o matches
Makefile:PACKAGE_STRING = zookeeper C client 3.4.1
",mahadev,marshall,Major,Resolved,Fixed,31/Dec/11 02:58,06/Feb/12 08:20
Bug,MAPREDUCE-2238,12494542,Undeletable build directories ,"The MR hudson job is failing, looks like it's due to a test chmod'ing a build directory so the checkout can't clean the build dir.

https://hudson.apache.org/hudson/job/Hadoop-Mapreduce-trunk/549/console

Building remotely on hadoop7
hudson.util.IOException2: remote file operation failed: /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk at hudson.remoting.Channel@2545938c:hadoop7
	at hudson.FilePath.act(FilePath.java:749)
	at hudson.FilePath.act(FilePath.java:735)
	at hudson.scm.SubversionSCM.checkout(SubversionSCM.java:589)
	at hudson.scm.SubversionSCM.checkout(SubversionSCM.java:537)
	at hudson.model.AbstractProject.checkout(AbstractProject.java:1116)
	at hudson.model.AbstractBuild$AbstractRunner.checkout(AbstractBuild.java:479)
	at hudson.model.AbstractBuild$AbstractRunner.run(AbstractBuild.java:411)
	at hudson.model.Run.run(Run.java:1324)
	at hudson.model.FreeStyleBuild.run(FreeStyleBuild.java:46)
	at hudson.model.ResourceController.execute(ResourceController.java:88)
	at hudson.model.Executor.run(Executor.java:139)
Caused by: java.io.IOException: Unable to delete /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/test/logs/userlogs/job_20101230131139886_0001/attempt_20101230131139886_0001_m_000000_0",tlipcon,eli,Critical,Closed,Fixed,03/Jan/11 22:57,12/Dec/11 06:19
Bug,MAPREDUCE-2251,12495160,Remove mapreduce.job.userhistorylocation config,"Best I can tell, this config parameter is no longer used as of MAPREDUCE-157 but still exists in the code and in mapred-default.xml. We should remove it to avoid user confusion.",qwertymaniac,tlipcon,Major,Closed,Fixed,10/Jan/11 19:34,08/Jul/12 16:18
Bug,MAPREDUCE-2253,12495194,Servlets should specify content type,"HADOOP-7093 will change the default content-type to text/plain. So TaskLogServlet, which outputs HTML, needs to change to specify this content type. I believe the other HTML servlets already correctly specify a content type. The MapOutputServlet appears to specify no content type and work fine without one, but to be ""correct"" we may as well specify application/octet-stream",tlipcon,tlipcon,Critical,Closed,Fixed,11/Jan/11 00:41,12/Dec/11 06:18
Bug,MAPREDUCE-2256,12495277,FairScheduler fairshare preemption from multiple pools may preempt all tasks from one pool causing that pool to go below fairshare.,"Scenarios:
You have a cluster with 600 map slots and 3 pools.  Fairshare for each pool is 200 to start with.  Fairsharepreemption timeout is 5 mins.
1)  Pool1 schedules 300 map tasks first
2)  Pool2 then schedules another 300 map tasks
3)  Pool3 demands 300 map tasks but doesn't get any slot as all slots are taken.
4)  After 5 mins pool3 should preempt 200 map-slots.  Instead of peempting 100 slots each from pool1 and pool2, the bug would cause it to preempt all 200 slots from pool2 (last started) causing it to go below fairshare.  This is happening because the preemptTask method is not reducing the tasks left from a pool while preempting the tasks.  

The above scenario could be an extreme case but some amount of excess preemption would happen because of this bug.

The patch I created was for 0.22.0 but the code fix should work on 0.21  as well as looks like it has the same bug.",priyomustafi,priyomustafi,Major,Closed,Fixed,11/Jan/11 18:05,12/Dec/11 06:19
Bug,MAPREDUCE-2258,12495405,IFile reader closes stream and compressor in wrong order,"In IFile.Reader.close(), we return the decompressor to the pool and then call close() on the input stream. This is backwards and causes a rare race in the case of LzopCodec, since LzopInputStream makes a few calls on the decompressor object inside close(). If another thread pulls the decompressor out of the pool and starts to use it in the meantime, the first thread's close() will cause the second thread to potentially miss pieces of data.",tlipcon,tlipcon,Major,Closed,Fixed,12/Jan/11 19:19,15/Nov/11 00:48
Bug,MAPREDUCE-2262,12495528,Capacity Scheduler unit tests fail with class not found,"Currently the ivy.xml file for the capacity scheduler doesn't include the commons-cli, leading to class not found exceptions.",omalley,omalley,Major,Resolved,Fixed,13/Jan/11 19:12,13/Jan/11 19:35
Bug,MAPREDUCE-2264,12495566,Job status exceeds 100% in some cases ,"I'm looking now at my jobtracker's list of running reduce tasks. One of them is 120.05% complete, the other is 107.28% complete.

I understand that these numbers are estimates, but there is no case in which an estimate of 100% for a non-complete task is better than an estimate of 99.99%, nor is there any case in which an estimate greater than 100% is valid.

I suggest that whatever logic is computing these set 99.99% as a hard maximum.",devaraj,akramer,Major,Closed,Fixed,14/Jan/11 02:27,17/Jul/14 17:55
Bug,MAPREDUCE-2271,12495967,TestSetupTaskScheduling failing in trunk,This test case is failing in trunk after the commit of MAPREDUCE-2207,liangly,tlipcon,Blocker,Closed,Fixed,19/Jan/11 06:07,15/Nov/11 00:49
Bug,MAPREDUCE-2272,12495968,Job ACL file should not be executable,"For some reason the job ACL file is localized with permissions 700. This doesn't make sense, since it's not executable. It should be 600.",qwertymaniac,tlipcon,Trivial,Resolved,Fixed,19/Jan/11 06:22,07/Apr/11 15:40
Bug,MAPREDUCE-2277,12496176,TestCapacitySchedulerWithJobTracker fails sometimes,"Sometimes the testJobTrackerIntegration test fails on my Hudson. It seems the issue is that it doesn't ever wait for the first job to complete before checking its success status. Since the two jobs are in different queues, the first job may complete after the second job.",tlipcon,tlipcon,Minor,Resolved,Fixed,20/Jan/11 21:11,07/Apr/11 15:40
Bug,MAPREDUCE-2278,12496186,DistributedCache shouldn't hold a ref to JobConf,"The reference is unnecessary, leads to a memory leak.",cdouglas,acmurthy,Major,Resolved,Fixed,20/Jan/11 22:04,31/Oct/11 04:56
Bug,MAPREDUCE-2281,12496317,"Fix javac, javadoc, findbugs warnings",Split from HADOOP-6642,pocheung,pocheung,Major,Closed,Fixed,21/Jan/11 19:00,12/Dec/11 06:18
Bug,MAPREDUCE-2282,12496339,MapReduce tests don't compile following HDFS-1561,"TestMRServerPorts depends on TestHDFSServerPorts which was changed by HDFS-1561, resulting in a compilation failure.",shv,tomwhite,Blocker,Closed,Fixed,21/Jan/11 21:47,12/Dec/11 06:19
Bug,MAPREDUCE-2283,12496854,TestBlockFixer hangs initializing MiniMRCluster,TestBlockFixer (a raid contrib test) is hanging the precommit testing on Hudson,rvadali,nidaley,Blocker,Closed,Fixed,26/Jan/11 18:04,12/Dec/11 06:19
Bug,MAPREDUCE-2284,12496969,TestLocalRunner.testMultiMaps times out,This test has timed out in a number of Hudson builds.,tlipcon,tlipcon,Critical,Closed,Fixed,27/Jan/11 18:37,12/Dec/11 06:20
Bug,MAPREDUCE-2285,12496974,MiniMRCluster does not start after ant test-patch,"Any test using MiniMRCluster hangs in the MiniMRCluster constructor after running ant test-patch. Steps to reproduce:
 1. ant -Dpatch.file=<dummy patch to CHANGES.txt>  -Dforrest.home=<path to forrest> -Dfindbugs.home=<path to findbugs> -Dscratch.dir=/tmp/testpatch  -Djava5.home=<path to java5> test-patch
 2. Run any test that creates MiniMRCluster, say ant test -Dtestcase=TestFileArgs (contrib/streaming)

Expected result: Test should succeed
Actual result: Test hangs  in MiniMRCluster.<init>. This does not happen if we run ant clean after ant test-patch

Test output:
{code}
    [junit] 11/01/27 12:11:43 INFO ipc.Server: IPC Server handler 3 on 58675: starting
    [junit] 11/01/27 12:11:43 INFO mapred.TaskTracker: TaskTracker up at: localhost.localdomain/127.0.0.1:58675
    [junit] 11/01/27 12:11:43 INFO mapred.TaskTracker: Starting tracker tracker_host0.foo.com:localhost.localdomain/127.0.0.1:58675
    [junit] 11/01/27 12:11:44 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:0. Already tried 0 time(s).
    [junit] 11/01/27 12:11:45 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:0. Already tried 1 time(s).
    [junit] 11/01/27 12:11:46 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:0. Already tried 2 time(s).
    [junit] 11/01/27 12:11:47 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:0. Already tried 3 time(s).
    [junit] 11/01/27 12:11:48 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:0. Already tried 4 time(s).
    [junit] 11/01/27 12:11:49 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:0. Already tried 5 time(s).
    [junit] 11/01/27 12:11:50 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:0. Already tried 6 time(s).
    [junit] 11/01/27 12:11:51 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:0. Already tried 7 time(s).
    [junit] 11/01/27 12:11:52 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:0. Already tried 8 time(s).
    [junit] 11/01/27 12:11:53 INFO ipc.Client: Retrying connect to server: localhost/127.0.0.1:0. Already tried 9 time(s).
    [junit] 11/01/27 12:11:53 INFO ipc.RPC: Server at localhost/127.0.0.1:0 not available yet, Zzzzz...
{code}

Stack trace: 

{code}
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.ipc.Client$Connection.handleConnectionFailure(Client.java:611)
        at org.apache.hadoop.ipc.Client$Connection.setupConnection(Client.java:429)
        - locked <0x00007f3b8dc08700> (a org.apache.hadoop.ipc.Client$Connection)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:504)
        - locked <0x00007f3b8dc08700> (a org.apache.hadoop.ipc.Client$Connection)
        at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:206)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1164)
        at org.apache.hadoop.ipc.Client.call(Client.java:1008)
        at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:198)
        at org.apache.hadoop.mapred.$Proxy11.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:235)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:275)
        at org.apache.hadoop.ipc.RPC.waitForProxy(RPC.java:206)
        at org.apache.hadoop.ipc.RPC.waitForProxy(RPC.java:185)
        at org.apache.hadoop.ipc.RPC.waitForProxy(RPC.java:169)
        at org.apache.hadoop.mapred.TaskTracker$2.run(TaskTracker.java:699)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1142)
        at org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java:695)
        - locked <0x00007f3b8ccc3870> (a org.apache.hadoop.mapred.TaskTracker)
        at org.apache.hadoop.mapred.TaskTracker.<init>(TaskTracker.java:1391)
        at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.createTaskTracker(MiniMRCluster.java:219)
        at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner$1.run(MiniMRCluster.java:203)
        at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner$1.run(MiniMRCluster.java:201)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1142)
        at org.apache.hadoop.mapred.MiniMRCluster$TaskTrackerRunner.<init>(MiniMRCluster.java:201)
        at org.apache.hadoop.mapred.MiniMRCluster.startTaskTracker(MiniMRCluster.java:716)
        at org.apache.hadoop.mapred.MiniMRCluster.<init>(MiniMRCluster.java:541)
        at org.apache.hadoop.mapred.MiniMRCluster.<init>(MiniMRCluster.java:482)
        at org.apache.hadoop.mapred.MiniMRCluster.<init>(MiniMRCluster.java:474)
        at org.apache.hadoop.mapred.MiniMRCluster.<init>(MiniMRCluster.java:466)
        at org.apache.hadoop.mapred.MiniMRCluster.<init>(MiniMRCluster.java:458)
        at org.apache.hadoop.mapred.MiniMRCluster.<init>(MiniMRCluster.java:448)
        at org.apache.hadoop.mapred.MiniMRCluster.<init>(MiniMRCluster.java:438)
        at org.apache.hadoop.mapred.MiniMRCluster.<init>(MiniMRCluster.java:429)
        at org.apache.hadoop.streaming.TestFileArgs.<init>(TestFileArgs.java:59)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
        at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
        at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
        at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
        at org.junit.runners.BlockJUnit4ClassRunner.createTest(BlockJUnit4ClassRunner.java:202)
        at org.junit.runners.BlockJUnit4ClassRunner$1.runReflectiveCall(BlockJUnit4ClassRunner.java:251)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.BlockJUnit4ClassRunner.methodBlock(BlockJUnit4ClassRunner.java:248)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
        at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
        at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
        at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
        at junit.framework.JUnit4TestAdapter.run(JUnit4TestAdapter.java:39)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.run(JUnitTestRunner.java:422)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.launch(JUnitTestRunner.java:931)
        at org.apache.tools.ant.taskdefs.optional.junit.JUnitTestRunner.main(JUnitTestRunner.java:785)

{code}",tlipcon,rvadali,Blocker,Closed,Fixed,27/Jan/11 20:15,12/Dec/11 06:19
Bug,MAPREDUCE-2289,12497096,Permissions race can make getStagingDir fail on local filesystem,"I've observed the following race condition in TestFairSchedulerSystem which uses a MiniMRCluster on top of RawLocalFileSystem:
- two threads call getStagingDir at the same time
- Thread A checks fs.exists(stagingArea) and sees false
-- Calls mkdirs(stagingArea, JOB_DIR_PERMISSIONS)
--- mkdirs calls the Java mkdir API which makes the file with umask-based permissions
- Thread B runs, checks fs.exists(stagingArea) and sees true
-- checks permissions, sees the default permissions, and throws IOE
- Thread A resumes and sets correct permissions",ahmed.radwan,tlipcon,Major,Closed,Fixed,28/Jan/11 21:04,06/May/13 04:07
Bug,MAPREDUCE-2290,12497126,TestTaskCommit missing getProtocolSignature override,"Fixes an MR compilation error, HADOOP-6904 added a new implementation of getProtocolSignature but TestTaskCommit doesn't override it.",eli,eli,Major,Closed,Fixed,29/Jan/11 06:50,07/Apr/11 15:40
Bug,MAPREDUCE-2294,12497382,Mumak won't compile in MR trunk,"HADOOP-6904 added a required getProtocolSignature() method for protocols, but the mock JT in Mumak doesn't implement this. So, MR trunk is currently failing.",tlipcon,tlipcon,Blocker,Resolved,Fixed,01/Feb/11 20:51,12/Mar/12 05:51
Bug,MAPREDUCE-2296,12497392,Fix references to misspelled message name getProtocolSigature,"HADOOP-7129 fixed the typo, need to update usages in MR.",tlipcon,tlipcon,Trivial,Resolved,Fixed,01/Feb/11 22:26,07/Apr/11 15:40
Bug,MAPREDUCE-2300,12497601,TestUmbilicalProtocolWithJobToken failing,"Testcase: testJobTokenRpc took 0.678 sec
        Caused an ERROR
null
java.lang.NullPointerException
        at org.apache.hadoop.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:241)
        at org.apache.hadoop.ipc.RPC.getProtocolProxy(RPC.java:422)
        at org.apache.hadoop.ipc.RPC.getProtocolProxy(RPC.java:368)
        at org.apache.hadoop.ipc.RPC.getProtocolProxy(RPC.java:333)
        at org.apache.hadoop.ipc.RPC.getProtocolProxy(RPC.java:461)
        at org.apache.hadoop.ipc.RPC.getProxy(RPC.java:442)
        at org.apache.hadoop.mapreduce.security.TestUmbilicalProtocolWithJobToken$1.run(TestUmbilicalProtocolWithJobToken.java:102)
",tlipcon,tlipcon,Blocker,Resolved,Fixed,03/Feb/11 18:38,07/Apr/11 15:40
Bug,MAPREDUCE-2304,12497707,TestMRCLI fails when hostname has a hyphen (-),"TestMRCLI fails with below

Comparator: [RegexpComparator]
Comparision result:   [fail]
Expected output: [mv: Wrong FS: har:/dest/dir0.har/dir0/file0, expected: hdfs://\w+[.a-z]*:[0-9]+]
Actual output:   [mv: Wrong FS: har:/dest/dir0.har/dir0/file0, expected: hdfs://lab-something.host.com:34039
",priyomustafi,priyomustafi,Minor,Resolved,Fixed,04/Feb/11 19:04,06/Aug/11 01:23
Bug,MAPREDUCE-2307,12497986,"Exception thrown in Jobtracker logs, when the Scheduler configured is FairScheduler.","If we try to start the job tracker with fair scheduler using the default configuration, It is giving the below exception.


{code:xml} 
2010-07-03 10:18:27,142 INFO org.apache.hadoop.ipc.Server: IPC Server handler 2 on 9001: starting
2010-07-03 10:18:27,143 INFO org.apache.hadoop.ipc.Server: IPC Server handler 3 on 9001: starting
2010-07-03 10:18:27,143 INFO org.apache.hadoop.ipc.Server: IPC Server handler 4 on 9001: starting
2010-07-03 10:18:27,143 INFO org.apache.hadoop.ipc.Server: IPC Server handler 5 on 9001: starting
2010-07-03 10:18:27,143 INFO org.apache.hadoop.ipc.Server: IPC Server handler 6 on 9001: starting
2010-07-03 10:18:27,143 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 9001: starting
2010-07-03 10:18:27,143 INFO org.apache.hadoop.ipc.Server: IPC Server handler 8 on 9001: starting
2010-07-03 10:18:27,143 INFO org.apache.hadoop.mapred.JobTracker: Starting RUNNING
2010-07-03 10:18:27,143 INFO org.apache.hadoop.ipc.Server: IPC Server handler 9 on 9001: starting
2010-07-03 10:18:28,037 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/linux172.site
2010-07-03 10:18:28,090 INFO org.apache.hadoop.net.NetworkTopology: Adding a new node: /default-rack/linux177.site
2010-07-03 10:18:40,074 ERROR org.apache.hadoop.mapred.PoolManager: Failed to reload allocations file - will use existing allocations.
java.lang.NullPointerException
at java.io.File.<init>(File.java:222)
at org.apache.hadoop.mapred.PoolManager.reloadAllocsIfNecessary(PoolManager.java:127)
at org.apache.hadoop.mapred.FairScheduler.assignTasks(FairScheduler.java:234)
at org.apache.hadoop.mapred.JobTracker.heartbeat(JobTracker.java:2785)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.ipc.RPC$Server.call(RPC.java:513)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:984)
at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:980)
at java.security.AccessController.doPrivileged(Native Method)
at javax.security.auth.Subject.doAs(Subject.java:396)
at org.apache.hadoop.ipc.Server$Handler.run(Server.java:978)
{code} ",devaraj,devaraj,Minor,Closed,Fixed,08/Feb/11 14:57,15/Nov/11 00:49
Bug,MAPREDUCE-2311,12498069,TestFairScheduler failing on trunk,"Most of the test cases in this test are failing on trunk, unclear how long since the contrib tests weren't running while the core tests were failed.",schen,tlipcon,Blocker,Closed,Fixed,09/Feb/11 01:30,15/Nov/11 00:48
Bug,MAPREDUCE-2315,12498195,javadoc is failing in nightly,"Last nightly build failed to publish javadoc because the javadoc build failed:

javadoc:
    [mkdir] Created dir: /grid/0/hudson/hudson-slave/workspace/Hadoop-Mapreduce-22-branch/trunk/build/docs/api
  [javadoc] Generating Javadoc
  [javadoc] Javadoc execution
  [javadoc] 1 error
  [javadoc] javadoc: error - Cannot find doclet class org.apache.hadoop.classification.tools.ExcludePrivateAnnotationsStandardDoclet",tlipcon,tlipcon,Blocker,Closed,Fixed,10/Feb/11 06:00,12/Dec/11 06:19
Bug,MAPREDUCE-2317,12498233,HadoopArchives throwing NullPointerException while creating hadoop archives (.har files),"While we are trying to run hadoop archive tool in widows using this way, it is giving the below exception.

java org.apache.hadoop.tools.HadoopArchives -archiveName temp.har D:/test/in E:/temp

{code:xml} 

java.lang.NullPointerException
	at org.apache.hadoop.tools.HadoopArchives.writeTopLevelDirs(HadoopArchives.java:320)
	at org.apache.hadoop.tools.HadoopArchives.archive(HadoopArchives.java:386)
	at org.apache.hadoop.tools.HadoopArchives.run(HadoopArchives.java:725)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.tools.HadoopArchives.main(HadoopArchives.java:739)

{code} 

I see the code flow to handle this feature in windows also, 

{code:title=Path.java|borderStyle=solid}

/** Returns the parent of a path or null if at root. */
  public Path getParent() {
    String path = uri.getPath();
    int lastSlash = path.lastIndexOf('/');
    int start = hasWindowsDrive(path, true) ? 3 : 0;
    if ((path.length() == start) ||               // empty path
        (lastSlash == start && path.length() == start+1)) { // at root
      return null;
    }
    String parent;
    if (lastSlash==-1) {
      parent = CUR_DIR;
    } else {
      int end = hasWindowsDrive(path, true) ? 3 : 0;
      parent = path.substring(0, lastSlash==end?end+1:lastSlash);
    }
    return new Path(uri.getScheme(), uri.getAuthority(), parent);
  }

{code} ",devaraj,devaraj,Minor,Closed,Fixed,10/Feb/11 12:10,15/Nov/11 00:49
Bug,MAPREDUCE-2324,12498560,Job should fail if a reduce task can't be scheduled anywhere,"If there's a reduce task that needs more disk space than is available on any mapred.local.dir in the cluster, that task will stay pending forever. For example, we produced this in a QA cluster by accidentally running terasort with one reducer - since no mapred.local.dir had 1T free, the job remained in pending state for several days. The reason for the ""stuck"" task wasn't clear from a user perspective until we looked at the JT logs.

Probably better to just fail the job if a reduce task goes through all TTs and finds that there isn't enough space.",revans2,tlipcon,Major,Closed,Fixed,14/Feb/11 18:09,23/Nov/11 18:11
Bug,MAPREDUCE-2327,12498601,MapTask doesn't need to put username information in SpillRecord,"This is an amendment to MAPREDUCE-2096 that's found in Yahoo's 0.20.100 branch.

This bug causes task failures in the following case:
- Cluster is not set up with LinuxTaskController (ie not secured cluster)
- Job submitter is not the same as the user running the TT
- Map output is more than one spill's worth

The issue is that UserGroupInformation's view of the current user is the job submitter, but on disk the spill files will be owned by the TT user. SecureIO will then fail when constructing the spill record.",tlipcon,tlipcon,Blocker,Closed,Fixed,15/Feb/11 01:40,12/Dec/11 06:18
Bug,MAPREDUCE-2336,12498977,Tool-related packages should be in the Tool javadoc group,Some of the tool packages are mistakenly in the general group.,tomwhite,tomwhite,Major,Closed,Fixed,18/Feb/11 00:41,12/Dec/11 06:19
Bug,MAPREDUCE-2356,12500447,A task succeeded even though there were errors on all attempts.,"From Luke Lu:

Here is a summary of why the failed map task was considered ""successful"" (Thanks to Mahadev, Arun and Devaraj
for insightful discussions).

1. The map task was hanging BEFORE being initialized (probably in localization, but it doesn't matter in this case).
Its state is UNASSIGNED.

2. The jt decided to kill it due to timeout and scheduled a cleanup task on the same node.

3. The cleanup task has the same attempt id (by design.) but runs in a different JVM. Its initial state is
FAILED_UNCLEAN.

4. The JVM of the original attempt is getting killed, while proceeding to setupWorkDir and throwed an
IllegalStateException while FileSystem.getLocal, which causes taskFinal.taskCleanup being called in Child, and
triggered the NPE due to the task is not yet initialized (committer is null). Before the NPE, however it sent a
statusUpdate to TT, and in tip.reportProgress, changed the task state (currently FAILED_UNCLEAN) to UNASSIGNED.

5. The cleanup attempt succeeded and report done to TT. In tip.reportDone, the isCleanup() check returned false due to
the UNASSIGNED state and set the task state as SUCCEEDED.
",vicaya,omalley,Major,Closed,Fixed,04/Mar/11 17:03,02/Sep/11 22:13
Bug,MAPREDUCE-2357,12500448,"When extending inputsplit (non-FileSplit), all exceptions are ignored","if you're using a custom RecordReader/InputFormat setup and using an
InputSplit that does NOT extend FileSplit, then any exceptions you throw in your RecordReader.nextKeyValue() function
are silently ignored.",vicaya,omalley,Major,Closed,Fixed,04/Mar/11 17:12,02/Sep/11 22:13
Bug,MAPREDUCE-2358,12500452,MapReduce assumes HDFS as the default filesystem,Mapred assumes hdfs as the default fs even when defined otherwise.,ramach,omalley,Major,Closed,Fixed,04/Mar/11 17:43,02/Sep/11 22:13
Bug,MAPREDUCE-2359,12500679,Distributed cache doesn't use non-default FileSystems correctly,"We are passing fs.deafult.name as viewfs:/// in core site.xml on oozie server.
We have default name node in configuration also viewfs:///

We are using hdfs://path in our path for application.
Its giving following error:

IllegalArgumentException: Wrong FS:
hdfs://nn/user/strat_ci/oozie-oozi/0000002-110217014830452-oozie-oozi-W/hadoop1--map-reduce/map-reduce-launcher.jar,
expected: viewfs:/",ramach,omalley,Major,Closed,Fixed,07/Mar/11 21:24,02/Sep/11 22:13
Bug,MAPREDUCE-2360,12500681,Pig fails when using non-default FileSystem,"The job client strips the file system from the user's job jar, which causes breakage when it isn't the default file system.",,omalley,Major,Closed,Fixed,07/Mar/11 21:34,02/Sep/11 22:13
Bug,MAPREDUCE-2362,12500683,Unit test failures: TestBadRecords and TestTaskTrackerMemoryManager,Fix unit-test failures: TestBadRecords (NPE due to rearranged MapTask code) and TestTaskTrackerMemoryManager (need hostname in output-string pattern).,roelofs,omalley,Major,Closed,Fixed,07/Mar/11 21:44,02/Sep/11 22:13
Bug,MAPREDUCE-2364,12500689,Shouldn't hold lock on rjob while localizing resources.,There is a deadlock while localizing resources on the TaskTracker.,ddas,omalley,Major,Closed,Fixed,07/Mar/11 22:09,26/Apr/12 22:24
Bug,MAPREDUCE-2365,12500691,Add counters for FileInputFormat (BYTES_READ) and FileOutputFormat (BYTES_WRITTEN),"MAP_INPUT_BYTES and MAP_OUTPUT_BYTES will be computed using the difference between FileSystem
counters before and after each next(K,V) and collect/write op.

In case compression is being used, these counters will represent the compressed data sizes. The uncompressed size will
not be available.

This is not a direct back-port of 5710. (Counters will be computed in MapTask instead of in individual RecordReaders).

0.20.100 ->
   New API -> MAP_INPUT_BYTES will be computed using this method
   Old API -> MAP_INPUT_BYTES will remain unchanged.

0.23 ->
   New API -> MAP_INPUT_BYTES will be computed using this method
   Old API -> MAP_INPUT_BYTES likely to use this method
",sseth,omalley,Major,Closed,Fixed,07/Mar/11 22:19,14/Oct/11 09:01
Bug,MAPREDUCE-2366,12500692,TaskTracker can't retrieve stdout and stderr from web UI,"Problem where the task browser UI can't retrieve the stdxxx printouts of streaming jobs that abend in the unix code, in the common case where the containing job doesn't reuse JVM's.",dking,omalley,Major,Closed,Fixed,07/Mar/11 22:27,02/Sep/11 22:13
Bug,MAPREDUCE-2374,12500950,"""Text File Busy"" errors launching MR tasks","Some very small percentage of tasks fail with a ""Text file busy"" error.

The following was the original diagnosis:
{quote}
Our use of PrintWriter in TaskController.writeCommand is unsafe, since that class swallows all IO exceptions. We're not currently checking for errors, which I'm seeing result in occasional task failures with the message ""Text file busy"" - assumedly because the close() call is failing silently for some reason.
{quote}
.. but turned out to be another issue as well (see below)",adi2,tlipcon,Major,Closed,Fixed,09/Mar/11 22:04,07/Feb/13 04:29
Bug,MAPREDUCE-2377,12501196,task-controller fails to parse configuration if it doesn't end in \n,"If the task-controller.cfg file doesn't end in a newline, it fails to parse properly.",benoyantony,tlipcon,Major,Closed,Fixed,11/Mar/11 20:42,17/Oct/12 18:27
Bug,MAPREDUCE-2379,12501215,Distributed cache sizing configurations are missing from mapred-default.xml,"* MAPREDUCE-1538 added {{mapreduce.tasktracker.cache.local.numberdirectories}} which is not documented in mapred-default.xml
* When MAPREDUCE-711 moved DistributedCache into the mapred project, the {{local.cache.size}} parameter was left in core-default.xml instead of moved to mapred-default.xml. It has since been renamed to {{mapreduce.tasktracker.cache.local.size}}",tlipcon,tlipcon,Major,Closed,Fixed,11/Mar/11 23:33,15/Nov/11 00:49
Bug,MAPREDUCE-2392,12501652,TaskTracker shutdown in the tests sometimes take 60s,"There are a lot of the following in the test logs:

{noformat}
2011-03-16 13:47:02,267 INFO  mapred.TaskTracker (TaskTracker.java:shutdown(1275)) - Shutting down StatusHttpServer
2011-03-16 13:48:02,349 ERROR mapred.TaskTracker (TaskTracker.java:offerService(1609)) - Caught exception: java.io.IOException: Call to localhost/127.0.0.1:57512 failed on local exception: java.nio.channels.ClosedByInterruptException
{noformat}

Note there is over one minute between the first line and the second.",tomwhite,tomwhite,Major,Closed,Fixed,17/Mar/11 05:16,12/Dec/11 06:18
Bug,MAPREDUCE-2394,12501711,JUnit output format doesn't propagate into some contrib builds,"Some of the contribs seem to have an issue where the test.junit.output.format property isn't propagating down into their builds. So, Hudson is unable to parse the test output, and we see failed builds with no actual parsed test results showing what failed.

This is at least true for {{contrib/raid}} but maybe others as well.",tlipcon,tlipcon,Blocker,Closed,Fixed,17/Mar/11 17:49,12/Dec/11 06:19
Bug,MAPREDUCE-2395,12501714,TestBlockFixer timing out on trunk,"In recent Hudson builds, TestBlockFixer has been timing out. Not clear how long it has been broken since MAPREDUCE-2394 was hiding the RAID tests from Hudson's test result parsing.",rvadali,tlipcon,Critical,Closed,Fixed,17/Mar/11 18:00,15/Nov/11 00:49
Bug,MAPREDUCE-2398,12501931,MRBench: setting the baseDir parameter has no effect,"The optional {{-baseDir}} parameter lets user specify the base DFS path for output/input of MRBench.

However, the two private variables {{INPUT_DIR}} and {{OUTPUT_DIR}} (MRBench.java) are not updated in the case that the default value of  {{-baseDir}} is actually overwritten by the user. Hence any input and output is always written to the default locations ({{/benchmarks/MRBench/...}}), even though the user-supplied location for {{-baseDir}} is created (and eventually deleted again) on HDFS.

The bug affects at least Hadoop 0.20.2 and the current trunk (r1082703) as of March 21, 2011.",wilfreds,miguno,Minor,Resolved,Fixed,21/Mar/11 09:33,26/Feb/20 05:50
Bug,MAPREDUCE-2409,12502786,Distributed Cache does not differentiate between file /archive for files with the same path,"If a 'global' file is specified as a 'file' by one job - subsequent jobs cannot override this source file to be an 'archive' (until the TT cleans up it's cache or a TT restart).
The other way around as well -> 'archive' to 'file'

In case of an accidental submission using the wrong type - some of the tasks for the second job will end up seeing the source file as an archive, others as a file.",sseth,sseth,Major,Closed,Fixed,30/Mar/11 01:23,02/Sep/11 22:13
Bug,MAPREDUCE-2411,12502933,When you submit a job to a queue with no ACLs you get an inscrutible NPE,"With this patch we'll check for that, and print a message in the logs.  Then at submission time you find out about it.",dking,dking,Minor,Closed,Fixed,31/Mar/11 00:00,02/Sep/11 22:13
Bug,MAPREDUCE-2416,12503105,"In Gridmix, in RoundRobinUserResolver, the list of groups for a user obtained from users-list-file is incorrect","RoundRobinUserResolver.parseUserList() has a bug in obtaining list of groups for each user --- in the sense that the list is not cleared before obtaining groups list for the next user. So if the first line has some groups, then from 2nd
line onwards, the validation of ""whether the users(in the next lines) are also having group names in those lines"" is useless as the list is already nonempty.

For example, users-list-file content as shown below also is valid as per parseUserList():
------------------
user1,group1
user2,
user3,
user4,
------------------",ravidotg,ravidotg,Major,Closed,Fixed,01/Apr/11 09:23,15/Nov/11 00:50
Bug,MAPREDUCE-2417,12503106,"In Gridmix, in RoundRobinUserResolver mode, the testing/proxy users are not associated with unique users in a trace","As per the Gridmix documentation, the testing users should associate with unique user in the trace. However, currently the gridmix impersonate the users based on job irrespective of user.",ravidotg,ravidotg,Major,Closed,Fixed,01/Apr/11 09:31,02/Feb/12 07:45
Bug,MAPREDUCE-2418,12503159,Errors not shown in the JobHistory servlet (specifically Counter Limit Exceeded),"Job error details are not displayed in the JobHistory servlet. e.g. Errors like 'Counter limit exceeded for a job'. 
jobdetails.jsp has 'Failure Info', but this is missing in jobdetailshistory.jsp",sseth,sseth,Minor,Closed,Fixed,01/Apr/11 18:37,02/Sep/11 22:13
Bug,MAPREDUCE-2420,12503362,JobTracker should be able to renew delegation token over HTTP,"in case JobTracker has to talk to a NameNode running a different version (RPC version mismatch), Jobtracker should be able to fall back to HTTP renewal.

Example of the case - running distcp between different versions using hfpt.",boryas,boryas,Major,Closed,Fixed,05/Apr/11 03:27,05/Jun/12 13:53
Bug,MAPREDUCE-2428,12503796,start-mapred.sh script fails if HADOOP_HOME is not set,MapReduce portion of HADOOP-6953,tomwhite,tomwhite,Blocker,Closed,Fixed,08/Apr/11 22:24,02/May/13 02:29
Bug,MAPREDUCE-2429,12503800,Check jvmid during task status report,Currently TT doens't check to ensure jvmid is relevant during communication with the Child via TaskUmbilicalProtocol.,sseth,acmurthy,Major,Closed,Fixed,08/Apr/11 22:37,02/May/13 02:29
Bug,MAPREDUCE-2433,12504119,MR-279: YARNApplicationConstants hard code app master jar version,"YARNApplicationConstants hard code version string in HADOOP_MAPREDUCE_CLIENT_APP_JAR_NAME and consequently YARN_MAPREDUCE_APP_JAR_PATH

This is a blocker.",mahadev,vicaya,Blocker,Closed,Fixed,12/Apr/11 23:33,15/Nov/11 00:49
Bug,MAPREDUCE-2437,12504213,SLive should process only part* files while generating the report.,SliveTest when producing the final report scans all files in the reduce output directory. The directory now may contain {{_SUCCESS}} and {{_logs}} entries. SliveTest should process only files starting with {{part*}}.,shv,shv,Blocker,Closed,Fixed,13/Apr/11 22:03,12/Dec/11 06:19
Bug,MAPREDUCE-2439,12504330,MR-279: Fix YarnRemoteException to give more details.,Fix YarnRemoteException to add more details.,sseth,mahadev,Major,Closed,Fixed,15/Apr/11 00:53,15/Nov/11 00:48
Bug,MAPREDUCE-2440,12504414,MR-279: Name clashes in TypeConverter,"public static TaskTrackerInfo[] fromYarn(List<NodeManagerInfo> nodes) has the same erasure as
public static JobStatus[] fromYarn(List<Application> applications)

Not detected by the current JDK 6 but still wrong according to the JLS 8.4.2.

See also: http://bugs.sun.com/view_bug.do?bug_id=6182950

The patch renames the former signature to fromYarnNodes and the later fromYarnApps.",vicaya,vicaya,Major,Closed,Fixed,15/Apr/11 21:51,15/Nov/11 00:49
Bug,MAPREDUCE-2443,12504619,Fix FI build - broken after MR-2429,"src/test/system/aop/org/apache/hadoop/mapred/TaskAspect.aj:72 [warning] advice defined in org.apache.hadoop.mapred.TaskAspect has not been applied [Xlint:adviceDidNotMatch]

After the fix in MR-2429, the call to ping in TaskAspect needs to be fixed.",sseth,sseth,Minor,Closed,Fixed,19/Apr/11 02:49,02/Sep/11 22:13
Bug,MAPREDUCE-2445,12504943,TestMiniMRWithDFSWithDistinctUsers is very broken,"This test has a number of issues:
- it side steps the normal job submission API for no apparent reason, manually writing splits file and uploading submission files. (but forgets to upload the job jar, so the jobs all fail)
- it doesn't call waitForCompletion, or check job status (so it doesn't notice that the jobs all fail)
- it doesn't verify in any way that the job output is owned by the user who supposedly ran the job
- it shuts down DFS before MR

These all conspire to make it pass, but it isn't actually testing anything.",tlipcon,tlipcon,Major,Closed,Fixed,22/Apr/11 02:35,12/Dec/11 06:20
Bug,MAPREDUCE-2447,12504995,Set JvmContext sooner for a task - MR2429,"TaskTracker.validateJVM() is throwing NPE when setupWorkDir() throws IOException. This is because
taskFinal.setJvmContext() is not executed yet",sseth,sseth,Minor,Closed,Fixed,22/Apr/11 18:27,02/Sep/11 22:13
Bug,MAPREDUCE-2448,12505011,NoSuchMethodError: org.apache.hadoop.hdfs.TestDatanodeBlockScanner.corruptReplica(..),"{noformat}
java.lang.NoSuchMethodError: org.apache.hadoop.hdfs.TestDatanodeBlockScanner.corruptReplica(Ljava/lang/String;I)Z
	at org.apache.hadoop.raid.TestBlockFixer.corruptBlock(TestBlockFixer.java:643)
	at org.apache.hadoop.raid.TestBlockFixer.implBlockFix(TestBlockFixer.java:189)
	at org.apache.hadoop.raid.TestBlockFixer.testBlockFixLocal(TestBlockFixer.java:139)
{noformat}",eli,szetszwo,Minor,Resolved,Fixed,22/Apr/11 20:40,04/May/11 14:57
Bug,MAPREDUCE-2451,12505127,Log the reason string of healthcheck script,The information on why a specific TaskTracker got blacklisted is not stored anywhere. The jobtracker web ui will show the detailed reason string until the TT gets unblacklisted.  After that it is lost.,tgraves,tgraves,Trivial,Closed,Fixed,25/Apr/11 17:35,02/Sep/11 22:13
Bug,MAPREDUCE-2452,12505133,Delegation token cancellation shouldn't hold global JobTracker lock,"Currently, when the JobTracker cancels a job's delegation token (at the end of the job), it holds the global lock. This is not desired.",ddas,ddas,Major,Closed,Fixed,25/Apr/11 18:44,05/Jun/12 13:53
Bug,MAPREDUCE-2457,12505371,job submission should inject group.name (on the JT side),"Until Hadoop 0.20, the JobClient was injecting the property 'group.name' on the JobConf submitted to the JobTracker.

Since Hadoop 0.21, due to security related changes, this is not done anymore.

This breaks backwards compatibility for jobs/components that expect the 'group.name' to be automatically set at submission time.

An example of a component being affected by this change is the FairScheduler where it is common to use the group.name as pool name. Different from other properties, a special characteristic of the group.name is that its value cannot be tampered by a user.

For security reasons this should not be done (as it was done before) in the JobClient side. Instead, it should be done in the JobTracker when the JobConf is received.
",tucu00,tucu00,Critical,Closed,Fixed,28/Apr/11 05:13,12/Dec/11 06:19
Bug,MAPREDUCE-2458,12505442,MR-279: Rename sanitized pom.xml in build directory to work around IDE bug,The sanitized pom.xml in target directory apparently triggered a bug in NetBeans (http://netbeans.org/bugzilla/show_bug.cgi?id=198162) causing it to fail to recognize the generated sources. The work-around is to rename the generated pom.xml to saner-pom.xml,vicaya,vicaya,Major,Closed,Fixed,28/Apr/11 17:51,15/Nov/11 00:50
Bug,MAPREDUCE-2460,12505743,TestFairSchedulerSystem failing on Hudson,Seems to have been failing for a while. For example: https://hudson.apache.org/hudson/job/Hadoop-Mapreduce-trunk/655/testReport/junit/org.apache.hadoop.mapred/TestFairSchedulerSystem/testFairSchedulerSystem/,tlipcon,tlipcon,Blocker,Closed,Fixed,29/Apr/11 02:11,15/Nov/11 00:49
Bug,MAPREDUCE-2461,12505744,Hudson jobs failing because mapred staging directory is full,"All of the tests that submit MR jobs are failing on the h7 build machine. This is because the staging directory is entirely full:

hudson@h7:/tmp/mr/mr$ ls -l /tmp/hadoop-hudson/mapred/staging/ | wc -l
31999

This makes me think that there's some bug where we're leaking things in the staging directory. I will manually clean this for now, but we should investigate.",,tlipcon,Major,Resolved,Fixed,29/Apr/11 02:48,09/Mar/15 20:35
Bug,MAPREDUCE-2463,12505901,Job History files are not moving to done folder when job history location is hdfs location,"If ""mapreduce.jobtracker.jobhistory.location"" is configured as HDFS location then either during initialization of Job Tracker (while moving old job history files) or after completion of the job, history files are not moving to done and giving following exception.

{code:xml} 
2011-04-29 15:27:27,813 ERROR org.apache.hadoop.mapreduce.jobhistory.JobHistory: Unable to move history file to DONE folder.
java.lang.IllegalArgumentException: Wrong FS: hdfs://10.18.52.146:9000/history/job_201104291518_0001_root, expected: file:///
	at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:402)
	at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:58)
	at org.apache.hadoop.fs.RawLocalFileSystem.getFileStatus(RawLocalFileSystem.java:419)
	at org.apache.hadoop.fs.FilterFileSystem.getFileStatus(FilterFileSystem.java:294)
	at org.apache.hadoop.fs.FileUtil.copy(FileUtil.java:215)
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1516)
	at org.apache.hadoop.fs.FileSystem.copyFromLocalFile(FileSystem.java:1492)
	at org.apache.hadoop.fs.FileSystem.moveFromLocalFile(FileSystem.java:1482)
	at org.apache.hadoop.mapreduce.jobhistory.JobHistory.moveToDoneNow(JobHistory.java:348)
	at org.apache.hadoop.mapreduce.jobhistory.JobHistory.access$200(JobHistory.java:61)
	at org.apache.hadoop.mapreduce.jobhistory.JobHistory$1.run(JobHistory.java:439)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)

{code} 
",devaraj,devaraj,Major,Closed,Fixed,02/May/11 06:35,15/Nov/11 00:49
Bug,MAPREDUCE-2466,12505991,TestFileInputFormat.testLocality failing after federation merge,"This test is failing, I believe due to federation merge. It's only finding one location for the test file instead of the expected two.",tlipcon,tlipcon,Blocker,Closed,Fixed,02/May/11 23:47,15/Nov/11 00:49
Bug,MAPREDUCE-2467,12505997,HDFS-1052 changes break the raid contrib module in MapReduce,Raid contrib module requires changes to work with the federation changes made in HDFS-1052.,sureshms,sureshms,Major,Closed,Fixed,03/May/11 00:27,15/Nov/11 00:48
Bug,MAPREDUCE-2470,12506109,Receiving NPE occasionally on RunningJob.getCounters() call,"This is running in a Java daemon that is used as an interface (Thrift) to get information and data from MR Jobs. Using JobClient.getJob(JobID) I successfully get a RunningJob object (I'm checking for NULL), and then rarely I get an NPE when I do RunningJob.getCounters(). This seems to occur after the daemon has been up and running for a while, and in the event of an Exception, I close the JobClient, set it to NULL, and a new one should then be created on the next request for data. Yet, I still seem to be unable to fetch the Counters. Below is the stack trace.


java.lang.NullPointerException
            at org.apache.hadoop.mapred.Counters.downgrade(Counters.java:77)
            at org.apache.hadoop.mapred.JobClient$NetworkedJob.getCounters(JobClient.java:381)
            at com.telescope.HadoopThrift.service.ServiceImpl.getReportResults(ServiceImpl.java:350)
            at com.telescope.HadoopThrift.gen.HadoopThrift$Processor$getReportResults.process(HadoopThrift.java:545)
            at com.telescope.HadoopThrift.gen.HadoopThrift$Processor.process(HadoopThrift.java:421)
            at org.apache.thrift.server.TNonblockingServer$FrameBuffer.invoke(TNonblockingServer.java:697)
            at org.apache.thrift.server.THsHaServer$Invocation.run(THsHaServer.java:317)
            at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
            at java.lang.Thread.run(Thread.java:619)
",revans2,drizzt321,Major,Closed,Fixed,03/May/11 22:08,15/Nov/11 00:48
Bug,MAPREDUCE-2472,12506138,Extra whitespace in mapred.child.java.opts breaks JVM initialization,"When creating taskjvm.sh, we split mapred.child.java.opts on "" "" and then create a quoted argument for each of those results. So, if you have an extra space anywhere in this configuration, you get an argument '' in the child command line, which the JVM interprets as an empty class name. This results in a ClassNotFoundException and the task cannot run.",atm,tlipcon,Major,Closed,Fixed,04/May/11 06:29,12/Dec/11 06:19
Bug,MAPREDUCE-2475,12506343,Disable IPV6 for junit tests,"IPV6 addresses not handles currently in the common library methods. IPV6 can return address as ""0:0:0:0:0:0:port"". Some utility methods such as NetUtils#createSocketAddress(), NetUtils#normalizeHostName(), NetUtils#getHostNameOfIp() to name a few, do not handle IPV6 address and expect address to be of format host:port.

Until IPV6 is formally supported, I propose disabling IPV6 for junit tests to avoid problems seen in HDFS-1891.
",sureshms,sureshms,Major,Closed,Fixed,05/May/11 22:17,15/Nov/11 00:50
Bug,MAPREDUCE-2480,12506662,MR-279: mr app should not depend on hard-coded version of shuffle,"The following commit introduced a dependency of shuffle with hard-coded version for mr app:
{noformat}
commit 6f69742140516be7493c9a9177b81d0516cc9539
Author: Vinod Kumar Vavilapalli <vinodkv@apache.org>
Date:   Wed May 4 06:53:52 2011 +0000

    Adding user log handling for YARN. Making NM put the user-logs on DFS and providing log-dump tools. Contributed by Vinod Kumar Vavilapalli.
{noformat}",vicaya,vicaya,Major,Closed,Fixed,09/May/11 22:53,15/Nov/11 00:48
Bug,MAPREDUCE-2483,12506890,Clean up duplication of dependent jar files,"For trunk, the build and deployment tree look like this:

hadoop-common-0.2x.y
hadoop-hdfs-0.2x.y
hadoop-mapred-0.2x.y

Technically, mapred's the third party dependent jar files should be fetch from hadoop-common and hadoop-hdfs.  However, it is currently fetching from hadoop-mapred/lib only.  It would be nice to eliminate the need to repeat duplicated jar files at build time.

There are two options to manage this dependency list, continue to enhance ant build structure to fetch and filter jar file dependencies using ivy.  On the other hand, it would be a good opportunity to convert the build structure to maven, and use maven to manage the provided jar files.",eyang,eyang,Major,Closed,Fixed,11/May/11 18:15,15/Nov/11 00:49
Bug,MAPREDUCE-2486,12506922,0.22 - snapshot incorrect dependency published in .pom files,"The pom at https://repository.apache.org/content/repositories/snapshots/org/apache/hadoop/hadoop-mapred/0.22.0-SNAPSHOT/ publishes a dependency on hadoop-common version ""0.22.0-dev-SNAPSHOT"" while hadoop-common only publishes ""0.22.0-SNAPSHOT"" (no -dev).",tlipcon,dvryaboy,Blocker,Closed,Fixed,11/May/11 22:59,12/Dec/11 06:18
Bug,MAPREDUCE-2487,12506926,ChainReducer uses MAPPER_BY_VALUE instead of REDUCER_BY_VALUE,"On line 293 of o.a.h.mapred.lib.Chain in setReducer(...):

reducerConf.setBoolean(MAPPER_BY_VALUE, byValue);

this should be REDUCER_BY_VALUE.

http://grepcode.com/file/repository.cloudera.com/content/repositories/releases/com.cloudera.hadoop/hadoop-core/0.20.2-737/org/apache/hadoop/mapred/lib/Chain.java#293",devaraj,forvines,Minor,Resolved,Fixed,11/May/11 23:18,29/Jun/11 14:42
Bug,MAPREDUCE-2489,12507037,Jobsplits with random hostnames can make the queue unusable,"We saw an issue where a custom InputSplit was returning invalid hostnames for the splits that were then causing the JobTracker to attempt to excessively resolve host names.  This caused a major slowdown for the JobTracker.  We should prevent invalid InputSplit hostnames from affecting everyone else.

I propose we implement some verification for the hostnames to try to ensure that we only do DNS lookups on valid hostnames (and fail otherwise).  We could also fail the job after a certain number of failures in the resolve.",naisbitt,naisbitt,Major,Closed,Fixed,12/May/11 19:31,02/May/13 02:29
Bug,MAPREDUCE-2497,12507286,missing spaces in error messages,"Error message(s) are missing spaces.  Here's an example output:
  11/05/15 09:44:10 WARN mapred.JobClient: Error reading task outputhttp://
Generated from this line of source.

./src/mapred/org/apache/hadoop/mapred/JobClient.java:      LOG.warn(""Error reading task output"" + ioe.getMessage()); 

The 1st arg to LOG.warn should end with a ' '.

There may be other instances of this problem in the source base.",eli,rrh,Trivial,Closed,Fixed,15/May/11 17:07,15/Nov/11 00:49
Bug,MAPREDUCE-2500,12507430,MR 279: PB factories are not thread safe,,sseth,sseth,Major,Closed,Fixed,16/May/11 23:46,15/Nov/11 00:49
Bug,MAPREDUCE-2504,12507511,MR 279: race in JobHistoryEventHandler stop ,"The condition to stop the eventHandling thread currently requires it to be 'stopped' AND interrupted. If an interrupt arrives after a take, but before handleEvent is called - the interrupt status ends up being handled by hadoop.util.Shell.runCommand() - which ignores it (and in the process resets the flag).
The eventHandling thread subsequently hangs on eventQueue.take()
This currently randomly fails unit tests - and can hang MR AMs.",sseth,sseth,Major,Closed,Fixed,17/May/11 16:34,15/Nov/11 00:48
Bug,MAPREDUCE-2509,12507549,MR-279: Fix NPE in UI for pending attempts,The task attempts page gets a 500 (and NPE in the AM logs) if the attempt is pending (not running yet).,vicaya,vicaya,Major,Closed,Fixed,18/May/11 01:27,15/Nov/11 00:49
Bug,MAPREDUCE-2510,12507554,TaskTracker throw OutOfMemoryError after upgrade to jetty6,"Our product cluster's TaskTracker sometimes throw OutOfMemoryError after upgrade to jetty6. The exception in TT's log is as follows:
2011-05-17 19:16:40,756 ERROR org.mortbay.log: Error for /mapOutput

java.lang.OutOfMemoryError: Java heap space

        at java.io.BufferedInputStream.<init>(BufferedInputStream.java:178)

        at org.apache.hadoop.fs.BufferedFSInputStream.<init>(BufferedFSInputStream.java:44)

        at org.apache.hadoop.fs.RawLocalFileSystem.open(RawLocalFileSystem.java:176)

        at org.apache.hadoop.fs.FileSystem.open(FileSystem.java:359)

        at org.apache.hadoop.mapred.TaskTracker$MapOutputServlet.doGet(TaskTracker.java:3040)

        at javax.servlet.http.HttpServlet.service(HttpServlet.java:707)

        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)

        at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)

        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)

        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)

        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)

        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)

        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)

        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)

        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)

        at org.mortbay.jetty.Server.handle(Server.java:324)

        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)

        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)

        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)

        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)

        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)

        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)

        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)

Exceptions in .out file:
java.lang.OutOfMemoryError: Java heap space

Exception in thread ""process reaper"" java.lang.OutOfMemoryError: Java heap space

Exception in thread ""pool-1-thread-1"" java.lang.OutOfMemoryError: Java heap space

java.lang.OutOfMemoryError: Java heap space

java.lang.reflect.InvocationTargetException

Exception in thread ""IPC Server handler 6 on 50050""     at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)

        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)

        at java.lang.reflect.Method.invoke(Method.java:597)

        at org.mortbay.log.Slf4jLog.warn(Slf4jLog.java:126)

        at org.mortbay.log.Log.warn(Log.java:181)

        at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:449)

        at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)

        at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)

        at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)

        at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)

        at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)

        at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)

        at org.mortbay.jetty.Server.handle(Server.java:324)

        at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)

        at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)

        at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)

        at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)

        at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)

        at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)

        at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)


",,liangly,Major,Resolved,Fixed,18/May/11 02:14,20/Jul/11 13:57
Bug,MAPREDUCE-2514,12507667,ReinitTrackerAction class name misspelled RenitTrackerAction in task tracker log,,jeagles,jeagles,Trivial,Closed,Fixed,18/May/11 21:03,02/Sep/11 22:13
Bug,MAPREDUCE-2515,12507674,MapReduce references obsolete options,"Option topology.node.switch.mapping.impl has been renamed to net.topology.node.switch.mapping.impl; JT still uses old name. Likewise, JT uses old names for several other since-renamed options.
",asrabkin,asrabkin,Major,Resolved,Fixed,18/May/11 21:45,19/May/11 15:44
Bug,MAPREDUCE-2516,12507688,option to control sensitive web actions,"as per HADOOP-7302, webinterface.private.actions should not be in trunk. But it should be here, and should have a clearer name.",asrabkin,asrabkin,Minor,Resolved,Fixed,18/May/11 23:48,20/May/11 15:39
Bug,MAPREDUCE-2518,12507718,missing t flag in distcp help message '-p[rbugp]',"'t: modification and access times' flag is defined but
missing in distcp help message '-p[rbugp]'. should be
changed to -p[rbugpt].
",weiyj,weiyj,Major,Closed,Fixed,19/May/11 06:53,15/Nov/11 00:50
Bug,MAPREDUCE-2529,12508203,Recognize Jetty bug 1342 and handle it,"We are seeing many instances of the Jetty-1342 (http://jira.codehaus.org/browse/JETTY-1342). The bug doesn't cause Jetty to stop responding altogether, some fetches go through but a lot of them throw exceptions and eventually fail. The only way we have found to get the TT out of this state is to restart the TT.  This jira is to catch this particular exception (or perhaps a configurable regex) and handle it in an automated way to either blacklist or shutdown the TT after seeing it a configurable number of them.
",tgraves,tgraves,Major,Closed,Fixed,24/May/11 15:05,02/Sep/11 22:13
Bug,MAPREDUCE-2531,12508211,org.apache.hadoop.mapred.jobcontrol.getAssignedJobID throw class cast exception ,"When using a combination of the mapred and mapreduce APIs (PIG) it is possible to have the following exception

Caused by: java.lang.ClassCastException: org.apache.hadoop.mapreduce.JobID cannot be cast to
org.apache.hadoop.mapred.JobID
        at org.apache.hadoop.mapred.jobcontrol.Job.getAssignedJobID(Job.java:71)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.MapReduceLauncher.launchPig(MapReduceLauncher.java:239)
        at org.apache.pig.PigServer.launchPlan(PigServer.java:1325)
        ... 29 more

This is because the JobID is just downcast.  It should be calling JobID.downgrade",revans2,revans2,Blocker,Closed,Fixed,24/May/11 16:07,15/Nov/11 00:49
Bug,MAPREDUCE-2534,12508260,MR-279: Fix CI breaking hard coded version in jobclient pom,,vicaya,vicaya,Major,Closed,Fixed,25/May/11 01:02,15/Nov/11 00:49
Bug,MAPREDUCE-2535,12508437,JobClient creates a RunningJob with null status and profile,"Exception occurred because the job was retired and is removed from RetireJobCcahe and CompletedJobStatusStore. But, the
JobClient creates a RunningJob with null status and profile, if getJob(JobID) is called again.
So, Even-though not null check is there in the following user code, it did not help.
466             runningJob = jobClient.getJob(mapRedJobID);
467             if(runningJob != null) {

JobClient.getJob() should return null if status is null.


In trunk this is fixed by validating that the job status is not null every time it is updated, and also verifying that that the profile data is not null when created.",revans2,revans2,Major,Resolved,Fixed,26/May/11 14:17,06/Jun/11 21:33
Bug,MAPREDUCE-2537,12508472,MR-279: The RM writes its log to yarn-mapred-resourcemanager-<RM_Host>.out,,revans2,revans2,Minor,Closed,Fixed,26/May/11 20:28,15/Nov/11 00:49
Bug,MAPREDUCE-2539,12508574,NPE when calling JobClient.getMapTaskReports for retired job,"When calling JobClient.getMapTaskReports for a retired job this results in a NPE.  In the 0.20.* version an empty TaskReport array was returned instead.

Caused by: java.lang.NullPointerException
        at org.apache.hadoop.mapred.JobClient.getMapTaskReports(JobClient.java:588)
        at org.apache.pig.tools.pigstats.JobStats.addMapReduceStatistics(JobStats.java:388)
......",revans2,revans2,Major,Resolved,Fixed,27/May/11 18:42,28/Jan/12 14:04
Bug,MAPREDUCE-2541,12508627,"Race Condition in IndexCache(readIndexFileToCache,removeMap) causes value of totalMemoryUsed corrupt, which may cause TaskTracker continue throw Exception","The race condition goes like this:
Thread1: readIndexFileToCache()  totalMemoryUsed.addAndGet(newInd.getSize())
Thread2: removeMap() totalMemoryUsed.addAndGet(-info.getSize());
When SpillRecord is being read from fileSystem, client kills the job, info.getSize() equals 0, so in fact totalMemoryUsed is not reduced, but after thread1 finished reading SpillRecord, it adds the real index size to totalMemoryUsed, which makes the value of totalMemoryUsed wrong(larger).
When this value(totalMemoryUsed) exceeds totalMemoryAllowed (this usually happens when a vary large job with vary large reduce number is killed by the user, probably because the user sets a wrong reduce number by mistake), and actually indexCache has not cache anything, freeIndexInformation() will throw exception constantly.

A quick fix for this issue is to make removeMap() do nothing, let freeIndexInformation() do this job only.
",decster,decster,Critical,Closed,Fixed,28/May/11 17:34,15/Nov/11 00:48
Bug,MAPREDUCE-2549,12508795,"Potential resource leaks in HadoopServer.java, RunOnHadoopWizard.java and Environment.java",,devaraj,devaraj,Major,Closed,Fixed,31/May/11 05:57,07/Dec/11 11:16
Bug,MAPREDUCE-2550,12508851,bin/mapred no longer works from a source checkout,Developer may want to run hadoop without extracting tarball.  It would be nice if existing method to run mapred scripts from source code is preserved for developers.,eyang,eyang,Blocker,Closed,Fixed,31/May/11 17:21,15/Nov/11 00:48
Bug,MAPREDUCE-2552,12508896,MR 279: NPE when requesting attemptids for completed jobs ,"While constructing a CompletedJob instance on the JobHistory server - successfuleAttempt is not populated. Causes an NPE when listing completed attempts for a job via the CLI.

CLI: hadoop job -list-attempt-ids <job_id> MAP completed",sseth,sseth,Minor,Closed,Fixed,01/Jun/11 01:44,15/Nov/11 00:48
Bug,MAPREDUCE-2555,12509016,JvmInvalidate errors in the gridmix TT logs,"Observing a  lot of jvmValidate exceptions in TT logs for grid mix run



************************
2011-04-28 02:00:37,578 INFO org.apache.hadoop.ipc.Server: IPC Server handler 7 on 46121, call
statusUpdate(attempt_201104270735_5993_m_003305_0, org.apache.hadoop.mapred.MapTaskStatus@1840a9c,
org.apache.hadoop.mapred.JvmContext@1d4ab6b) from 127.0.0.1:50864: error: java.io.IOException: JvmValidate Failed.
Ignoring request from task: attempt_201104270735_5993_m_003305_0, with JvmId:
jvm_201104270735_5993_m_103399012gsbl20430: java.io.IOException: JvmValidate Failed. Ignoring request from task:
attempt_201104270735_5993_m_003305_0, with JvmId: jvm_201104270735_5993_m_103399012gsbl20430: --
      at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1386)
      at java.security.AccessController.doPrivileged(Native Method)
      at javax.security.auth.Subject.doAs(Subject.java:396)
      at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
      at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1384)


*********************

",tgraves,tgraves,Minor,Closed,Fixed,01/Jun/11 20:01,02/May/13 02:29
Bug,MAPREDUCE-2556,12509046,MR 279: NodeStatus.getNodeHealthStatus().setBlah broken,,sseth,sseth,Major,Closed,Fixed,02/Jun/11 00:58,15/Nov/11 00:49
Bug,MAPREDUCE-2559,12509106,ant binary fails due to missing c++ lib dir,"Post MAPRED-2521 ant binary fails without ""-Dcompile.c++=true -Dcompile.native=true"". The bin-package is trying to copy from the c++ lib dir which doesn't exist yet. The binary target should check for the existence of this dir or would also be reasonable to depend on the compile-c++ (since this is the binary target).
",eyang,eyang,Major,Closed,Fixed,02/Jun/11 16:33,15/Nov/11 00:48
Bug,MAPREDUCE-2566,12509249,MR 279: YarnConfiguration should reloadConfiguration if instantiated with a non YarnConfiguration object,YarnConfiguration(conf) uses the ctor Configuration(conf) which is effectively a clone. If the configuration object is created before YarnConfiguration has been loaded - yarn-site.xml will not be available to the configuration.,sseth,sseth,Major,Closed,Fixed,04/Jun/11 01:26,15/Nov/11 00:49
Bug,MAPREDUCE-2569,12509378,MR-279: Restarting resource manager with root capacity not equal to 100 percent should result in error,root.capacity is set to 90% without failure,jeagles,jeagles,Minor,Closed,Fixed,06/Jun/11 18:55,15/Nov/11 00:48
Bug,MAPREDUCE-2571,12509414,CombineFileInputFormat.getSplits throws a java.lang.ArrayStoreException,"The getSplits methods of 
  org.apache.hadoop.mapred.lib.CombineFileInputFormat 
not work.

...mapred.lib.CombineFileInputFormat(0.20-style) is a proxy for ...mapreduce.lib.input.CombineFileInputFormat(0.21-style)

The 0.21-style getSplits returns ArrayList<...mapreduce.lib.input.CombineFileSplit>
and the 0.20-style delegation calls toArray(...mapred.InputSplit[])

The ...mapreduce.lib.input.CombineFileSplit is based on ...mapreduce.InputSplit
and ...mapred.InputSplit is a interface, not a super-class of ...mapreduce.InputSplit

",sinofool,sinofool,Blocker,Closed,Fixed,07/Jun/11 04:16,12/Dec/11 06:19
Bug,MAPREDUCE-2573,12509519,New findbugs warning after MAPREDUCE-2494,"MAPREDUCE-2494 introduced the following findbugs warning in trunk:
TrackerDistributedCacheManager.java:739, SIC_INNER_SHOULD_BE_STATIC, Priority: Low
Should org.apache.hadoop.mapreduce.filecache.TrackerDistributedCacheManager$CacheDir be a _static_ inner class?

This class is an inner class, but does not use its embedded reference to the object which created it.  This reference makes the instances of the class larger, and may keep the reference to the creator object alive longer than necessary.  If possible, the class should be made static.",revans2,tlipcon,Major,Closed,Fixed,07/Jun/11 21:56,15/Nov/11 00:50
Bug,MAPREDUCE-2575,12509524,TestMiniMRDFSCaching fails if test.build.dir is set to something other than build/test,TestMiniMRDFSCaching fails if test.build.dir is set to something other than build/test,tgraves,tgraves,Major,Closed,Fixed,07/Jun/11 22:32,15/Nov/11 00:48
Bug,MAPREDUCE-2576,12509619,Typo in comment in SimulatorLaunchTaskAction.java,"This JIRA is to track a fix to a super-trivial issue of a typo of ""or"" misspelled as ""xor "" in Line 24 of SimulatorLaunchTaskAction.java",tim_s,sherri_chen,Trivial,Closed,Fixed,08/Jun/11 17:49,15/Nov/11 00:49
Bug,MAPREDUCE-2581,12498875,Spelling errors in log messages (MapTask),"Spelling errors in log messages (MapTask) - e.g. search for ""recieve"" (should be ""receive"").  A decent IDE should detect these errors as well.",tim_s,david_syer,Trivial,Closed,Fixed,17/Feb/11 12:17,15/Nov/11 00:48
Bug,MAPREDUCE-2582,12509784,MR 279: Cleanup JobHistory event generation,Generate JobHistoryEvents for the correct transitions. Fix missing / incorrect values being set.,sseth,sseth,Major,Closed,Fixed,10/Jun/11 02:26,15/Nov/11 00:50
Bug,MAPREDUCE-2587,12510102,MR279: Fix RM version in the cluster->about page ,The Resource Manager version in the Cluster->About page always shows 1.0-SNAPSHOT. ,tgraves,tgraves,Minor,Closed,Fixed,13/Jun/11 15:20,15/Nov/11 00:49
Bug,MAPREDUCE-2588,12510123,Raid is not compile after DataTransferProtocol refactoring,Raid is directly using {{DataTransferProtocol}}.  It cannot be compiled after HDFS-2066.,szetszwo,szetszwo,Major,Closed,Fixed,13/Jun/11 18:02,15/Nov/11 00:48
Bug,MAPREDUCE-2595,12510450,MR279: update yarn INSTALL doc,yarn install doc needs to be updated after unsplit: http://svn.apache.org/repos/asf/hadoop/common/branches/MR-279/mapreduce/INSTALL,tgraves,tgraves,Minor,Closed,Fixed,15/Jun/11 14:50,15/Nov/11 00:48
Bug,MAPREDUCE-2598,12510489,"MR 279: miscellaneous UI, NPE fixes for JobHistory, UI",,sseth,sseth,Minor,Closed,Fixed,15/Jun/11 20:07,15/Nov/11 00:49
Bug,MAPREDUCE-2603,12510631,Gridmix system tests are failing due to high ram emulation enable by default for normal mr jobs in the trace which exceeds the solt capacity.,"In Gridmix high ram emulation enable by default.Because of this feature, some of the gridmix system tests are hanging for some time and then failing after timeout. Actually the failure case was occurring whenever reserved slot capacity exceeds the cluster slot capacity.So for fixing the issue by disabling the high ram emulation in the tests which are using the normal mr jobs in the traces.",vinaythota,vinaythota,Major,Closed,Fixed,17/Jun/11 04:26,15/Nov/11 00:48
Bug,MAPREDUCE-2606,12511012,Remove IsolationRunner,"IsolationRunner it seems it has been broken for a while, it gives a NPE when trying to use it.

In addition, it supports only Map tasks; to use it the user must ssh to the node where the task failed; and unless the job has been configured to keep local files, the job must be run again.

Because of this, IMO, the current implementation of IsolationRunner is not of much use.

Any objection to remove it from trunk and if people have the need for such functionality to open another JIRA to build this functionality supported by the JobTracker (ie via the UI console)?",tucu00,tucu00,Major,Closed,Fixed,20/Jun/11 22:35,13/Dec/11 06:22
Bug,MAPREDUCE-2610,12511130,Inconsistent API JobClient.getQueueAclsForCurrentUser,"Client needs access to the current user's queue name.
Public method JobClient.getQueueAclsForCurrentUser() returns QueueAclsInfo[].
The QueueAclsInfo class has default access. A public method should not return a package-private class.

The QueueAclsInfo class, its two constructors, getQueueName, and getOperations methods should be public.",jrottinghuis,jrottinghuis,Major,Closed,Fixed,21/Jun/11 17:57,19/Oct/11 00:26
Bug,MAPREDUCE-2615,12511285,MR 279: KillJob should go through AM whenever possible,KillJob currently goes directly to the RM - which effectively causes the AM and tasks to be killed via a signal. History information is not recorded in this case.,sseth,sseth,Major,Closed,Fixed,22/Jun/11 21:56,15/Nov/11 00:49
Bug,MAPREDUCE-2618,12511374,"MR-279: 0 map, 0 reduce job fails with Null Pointer Exception","A 0 map, 0 reduce job fails with an NPE. This case works fine on hadoop-0.20.x. The job should succeed and run setup/cleanup code - with no tasks.  Below is the stacktrace:

11/06/05 19:35:37 WARN mapred.ClientServiceDelegate:
 StackTrace: java.lang.NullPointerException
        at org.apache.hadoop.mapreduce.v2.app.job.impl.JobImpl.getTaskAttemptCompletionEvents(JobImpl.java:498)
        at
org.apache.hadoop.mapreduce.v2.app.client.MRClientService$MRClientProtocolHandler.getTaskAttemptCompletionEvents(MRClientService.java:290)
        at
org.apache.hadoop.mapreduce.v2.api.impl.pb.service.MRClientProtocolPBServiceImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBServiceImpl.java:139)
        at
org.apache.hadoop.yarn.proto.MRClientProtocol$MRClientProtocolService$2.callBlockingMethod(MRClientProtocol.java:195)
        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$TunnelResponder.call(ProtoOverHadoopRpcEngine.java:168)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:420)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1406)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1402)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1094)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1400)",naisbitt,naisbitt,Major,Closed,Fixed,23/Jun/11 15:46,15/Nov/11 00:50
Bug,MAPREDUCE-2620,12511495,Update RAID for HDFS-2087,DataTransferProtocol was changed by HDFS-2087.  Need to update RAID.,szetszwo,szetszwo,Major,Closed,Fixed,24/Jun/11 17:16,15/Nov/11 00:50
Bug,MAPREDUCE-2621,12511503,"TestCapacityScheduler fails with ""Queue ""q1"" does not exist""","{quote}
Error Message

Queue ""q1"" does not exist

Stacktrace

java.io.IOException: Queue ""q1"" does not exist
	at org.apache.hadoop.mapred.JobInProgress.<init>(JobInProgress.java:354)
	at org.apache.hadoop.mapred.TestCapacityScheduler$FakeJobInProgress.<init>(TestCapacityScheduler.java:172)
	at org.apache.hadoop.mapred.TestCapacityScheduler.submitJob(TestCapacityScheduler.java:794)
	at org.apache.hadoop.mapred.TestCapacityScheduler.submitJob(TestCapacityScheduler.java:818)
	at org.apache.hadoop.mapred.TestCapacityScheduler.submitJobAndInit(TestCapacityScheduler.java:825)
	at org.apache.hadoop.mapred.TestCapacityScheduler.testMultiTaskAssignmentInMultipleQueues(TestCapacityScheduler.java:1109)
{quote}

When queue name is invalid, an exception is thrown now. 

",sherri_chen,sherri_chen,Minor,Closed,Fixed,24/Jun/11 22:19,01/Oct/11 19:30
Bug,MAPREDUCE-2625,12511672,MR-279: Add Node Manager Version to NM info page,Hadoop and YARN versions are missing from the NM info page,jeagles,jeagles,Minor,Closed,Fixed,27/Jun/11 18:52,15/Nov/11 00:50
Bug,MAPREDUCE-2628,12512137,MR-279: Add compiled on date to NM and RM info/about page,"Compiled on dates were present on the JobTracker UI. Bring compiled on dates to resource manager and node
manager UI. 

NM and RM retrieves build version for hadoop and yarn version via the getBuildVersion util api. This function used to
contain the compiled on date, but since has been removed since that function is used to determine hadoop compatible
versions, but was too restrictive with build date being present. Instead, a getDate call should be used to retrieve the
compiled on date.",jeagles,jeagles,Minor,Closed,Fixed,28/Jun/11 21:47,15/Nov/11 00:49
Bug,MAPREDUCE-2630,12512172,MR-279: refreshQueues leads to NPEs when used w/FifoScheduler,"The RM's admin service exposes a method refreshQueues that is used to update the queue configuration when used with the CapacityScheduler, but if it is used with the FifoScheduler, it will set the containerTokenSecretManager/clusterTracker fields on the FifoScheduler to null, which eventually leads to NPE. Since the FifoScheduler only has one queue that cannot be refreshed, the correct behavior is for the refreshQueues call to be a no-op.

I will attach a patch that fixes this by splitting the ResourceScheduler's reinitialize method into separate initialize/updateQueues methods.",jwills,jwills,Minor,Closed,Fixed,29/Jun/11 06:38,15/Nov/11 00:50
Bug,MAPREDUCE-2631,12512282,Potential resource leaks in BinaryProtocol$TeeOutputStream.java,"{code:title=BinaryProtocol$TeeOutputStream.java|borderStyle=solid}

public void close() throws IOException {
      flush();
      file.close();
      out.close();
    }
{code} 

In the above code, if the file.close() throws any exception out will not be closed.
 
",sunilg,raviteja,Major,Resolved,Fixed,30/Jun/11 05:16,06/Jan/17 08:09
Bug,MAPREDUCE-2644,12512915,NodeManager fails to create containers when NM_LOG_DIR is not explicitly set in the Configuration,"If the yarn configuration does not explicitly specify a value for the yarn.server.nodemanager.log.dir property, container allocation will fail on the NodeManager w/an NPE when the LocalDirAllocator goes to create the temp directory. In most of the code, we handle this by defaulting to /tmp/logs, but we cannot do this in the LocalDirAllocator context, so we need to set the default value explicitly in the Configuration.

Marking this as major b/c it's annoying to bump into it when you're getting your first MRv2 cluster up and running. :)",jwills,jwills,Major,Closed,Fixed,06/Jul/11 01:45,15/Nov/11 00:49
Bug,MAPREDUCE-2645,12512916,Updates to MRv2 INSTALL documentation,"There are a few issues w/the current INSTALL document for MRv2 that I came across when I attempted to get it running:

1) Correct the mvn arg for skipping tests,
2) Add a step to start the yarn historyserver,
3) Add instructions to explicitly build the examples jar file and specify the mapreduce.clientfactory.class.name parameter correctly for MRv2.",jwills,jwills,Major,Resolved,Fixed,06/Jul/11 01:51,07/Jul/11 13:46
Bug,MAPREDUCE-2646,12512932,MR-279: AM with same sized maps and reduces hangs in presence of failing maps,Currently AM can assign a container given by RM to any map or reduce. However RM allocates for a particular priority. This leads to AM and RM data structures going out of sync.,sharadag,sharadag,Critical,Closed,Fixed,06/Jul/11 05:40,15/Nov/11 00:48
Bug,MAPREDUCE-2649,12513017,MR279: Fate of finished Applications on RM,"Today RM keeps the references of finished application for ever. Though this is not sustainable long term, it keeps
the user experience saner. Users can revisit RM UI and check the status of their apps.

We need to think of purging old references yet keeping the UX sane.",tgraves,tgraves,Major,Closed,Fixed,06/Jul/11 17:55,15/Nov/11 00:48
Bug,MAPREDUCE-2650,12513039,back-port MAPREDUCE-2238 to 0.20-security,"Dev had seen the attempt directory permission getting set to 000 or 111 in the CI builds and tests run on dev desktops with 0.20-security.
MAPREDUCE-2238 reported and fixed the issue for 0.22.0, back-port to 0.20-security is needed.
",sherri_chen,sherri_chen,Major,Closed,Fixed,06/Jul/11 21:18,19/Oct/11 00:26
Bug,MAPREDUCE-2651,12513047,Race condition in Linux Task Controller for job log directory creation,There is a rare race condition in linux task controller when concurrent task processes tries to create job log directory at the same time. ,bharathm,bharathm,Major,Closed,Fixed,06/Jul/11 23:09,05/Jun/12 13:53
Bug,MAPREDUCE-2652,12513150,MR-279: Cannot run multiple NMs on a single node ,"Currently in MR-279 the Auxiliary services, like ShuffleHandler, have no way to communicate information back to the applications.  Because of this the Map Reduce Application Master has hardcoded in a port of 8080 for shuffle.  This prevents the configuration ""mapreduce.shuffle.port"" form ever being set to anything but 8080.  The code should be updated to allow this information to be returned to the application master.  Also the data needs to be persisted to the task log so that on restart the data is not lost.",revans2,revans2,Major,Closed,Fixed,07/Jul/11 13:53,15/Nov/11 00:50
Bug,MAPREDUCE-2655,12513174,MR279: Audit logs for YARN ,"We need audit logs for YARN components:

ResourceManager:
 - All the refresh* protocol access points - refreshQueues, refreshNodes, refreshProxyUsers,
refreshUserToGroupMappings.
 - All app-submissions, app-kills to RM.
 - Illegal and successful(?) AM registrations.
 - Illegal container allocations/deallocations from AMs
 - Successful container allocations/deallocations from AMs too?

NodeManager:
 - Illegal container launches from AMs
 - Successful container launches from AMs too?

Not sure if we need audit logs from MR AMs.",tgraves,tgraves,Major,Closed,Fixed,07/Jul/11 18:30,11/May/12 16:04
Bug,MAPREDUCE-2661,12513218,MR-279: Accessing MapTaskImpl from TaskImpl,We are directly accessing MapTaskImpl in TaskImpl.InitialScheduleTransition.transition(..). It'll be better to reorganize the code so each subclass can provide its own behavior instead of explicitly checking for the subclass type. ,ahmed.radwan,ahmed.radwan,Minor,Closed,Fixed,08/Jul/11 05:43,15/Nov/11 00:48
Bug,MAPREDUCE-2663,12513246,MR-279: Refactoring StateMachineFactory inner classes,"The code for ApplicableSingleTransition and ApplicableMultipleTransition inner classes is almost identical. For maintainability, it is better to refactor them into a single inner class.",ahmed.radwan,ahmed.radwan,Minor,Closed,Fixed,08/Jul/11 11:20,15/Nov/11 00:49
Bug,MAPREDUCE-2667,12513310,MR279: mapred job -kill leaves application in RUNNING state,"the mapred job -kill command doesn't seem to fully clean up the application.

If you kill a job and run mapred job -list again it still shows up as running:

mapred job -kill job_1310072430717_0003
Killed job job_1310072430717_0003

 mapred job -list
Total jobs:1
JobId   State   StartTime       UserName        Queue   Priority        SchedulingInfo
job_1310072430717_0003  RUNNING 0       tgraves default NORMAL  98.139.92.22:19888/yarn/job/job_1310072430717_3_3

Running kill again will error out.

It also still shows up in the RM Applications UI as running with a note of: Kill Job received from client
job_1310072430717_0003 Job received Kill while in RUNNING state.",tgraves,tgraves,Major,Closed,Fixed,08/Jul/11 21:20,15/Nov/11 00:48
Bug,MAPREDUCE-2668,12513701,MR-279: APPLICATION_STOP is never sent to AuxServices,APPLICATION_STOP is never sent to the AuxServices only APPLICATION_INIT.  This means that all map intermediate data will never be deleted.,tgraves,revans2,Blocker,Closed,Fixed,11/Jul/11 18:10,25/Jun/13 14:26
Bug,MAPREDUCE-2670,12513738,Fixing spelling mistake in FairSchedulerServlet.java,"""Admininstration"" is misspelled.",eli,eli,Trivial,Closed,Fixed,12/Jul/11 01:41,15/Nov/11 00:48
Bug,MAPREDUCE-2671,12513749,"MR-279: Package examples, tools, test jars with the build","Jars such as examples, tools, test, streaming, gridmix has to be packaged as a part of the MR-279 builds https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-MR-279-Build/

",gkesavan,rramya,Minor,Resolved,Fixed,12/Jul/11 02:35,10/Mar/15 01:21
Bug,MAPREDUCE-2677,12513852,MR-279: 404 error while accessing pages from history server,"Accessing the following pages from the history server, causes 404 HTTP error
1. Cluster-> About 
2. Cluster -> Applications
3. Cluster -> Scheduler
4. Application -> About",revans2,rramya,Major,Closed,Fixed,12/Jul/11 21:37,15/Nov/11 00:50
Bug,MAPREDUCE-2678,12513856,MR-279: minimum-user-limit-percent no longer honored,"MR-279: In the capacity-scheduler.xml configuration, the 'minimum-user-limit-percent' property is no longer honored. ",naisbitt,naisbitt,Major,Closed,Fixed,12/Jul/11 21:51,15/Nov/11 00:49
Bug,MAPREDUCE-2687,12514249,Non superusers unable to launch apps in both secure and non-secure cluster,"Apps of non superuser fail to succeed in both secure and non-secure environment. Only the superuser(i.e. one who started/owns the mrv2 cluster) is able to launch apps successfully. However, when a normal user submits a job, the job fails.",mahadev,rramya,Blocker,Closed,Fixed,15/Jul/11 17:12,02/May/13 02:29
Bug,MAPREDUCE-2689,12514262,InvalidStateTransisiton when AM is not assigned to a job,"In cases where an AM is not being assigned to a job, RELEASED at COMPLETED invalid event is observed. This is easily reproducible in cases such as MAPREDUCE-2687.",,rramya,Major,Closed,Fixed,15/Jul/11 18:21,15/Nov/11 00:50
Bug,MAPREDUCE-2690,12514265,Construct the web page for default scheduler,"Currently, the web page for default scheduler reads as ""Under construction"". This is a long known issue, but could not find a tracking ticket. Hence opening one.",epayne,rramya,Major,Closed,Fixed,15/Jul/11 18:45,15/Nov/11 00:48
Bug,MAPREDUCE-2693,12514292,NPE in AM causes it to lose containers which are never returned back to RM,"The following exception in AM of an application at the top of queue causes this. Once this happens, AM keeps obtaining
containers from RM and simply loses them. Eventually on a cluster with multiple jobs, no more scheduling happens
because of these lost containers.

It happens when there are blacklisted nodes at the app level in AM. A bug in AM
(RMContainerRequestor.containerFailedOnHost(hostName)) is causing this - nodes are simply getting removed from the
request-table. We should make sure RM also knows about this update.

========================================================================
11/06/17 06:11:18 INFO rm.RMContainerAllocator: Assigned based on host match 98.138.163.34
11/06/17 06:11:18 INFO rm.RMContainerRequestor: BEFORE decResourceRequest: applicationId=30 priority=20
resourceName=... numContainers=4978 #asks=5
11/06/17 06:11:18 INFO rm.RMContainerRequestor: AFTER decResourceRequest: applicationId=30 priority=20
resourceName=... numContainers=4977 #asks=5
11/06/17 06:11:18 INFO rm.RMContainerRequestor: BEFORE decResourceRequest: applicationId=30 priority=20
resourceName=... numContainers=1540 #asks=5
11/06/17 06:11:18 INFO rm.RMContainerRequestor: AFTER decResourceRequest: applicationId=30 priority=20
resourceName=... numContainers=1539 #asks=6
11/06/17 06:11:18 ERROR rm.RMContainerAllocator: ERROR IN CONTACTING RM. 
java.lang.NullPointerException
        at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.decResourceRequest(RMContainerRequestor.java:246)
        at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerRequestor.decContainerReq(RMContainerRequestor.java:198)
        at
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.assign(RMContainerAllocator.java:523)
        at
org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator$ScheduledRequests.access$200(RMContainerAllocator.java:433)
        at org.apache.hadoop.mapreduce.v2.app.rm.RMContainerAllocator.heartbeat(RMContainerAllocator.java:151)
        at org.apache.hadoop.mapreduce.v2.app.rm.RMCommunicator$1.run(RMCommunicator.java:220)
        at java.lang.Thread.run(Thread.java:619)",hitesh,amolkekre,Critical,Closed,Fixed,15/Jul/11 22:33,15/Nov/11 00:48
Bug,MAPREDUCE-2697,12514298,Enhance CS to cap concurrently running jobs,Enhance CS to cap concurrently running jobs ala 0.20.203,acmurthy,acmurthy,Major,Closed,Fixed,15/Jul/11 23:24,15/Nov/11 00:49
Bug,MAPREDUCE-2705,12514490,tasks localized and launched serially by TaskLauncher - causing other tasks to be delayed,"The current TaskLauncher serially launches new tasks one at a time. During the launch it does the localization and then starts the map/reduce task.  This can cause any other tasks to be blocked waiting for the current task to be localized and started. In some instances we have seen a task that has a large file to localize (1.2MB) block another task for about 40 minutes. This particular task being blocked was a cleanup task which caused the job to be delayed finishing for the 40 minutes.
",tgraves,tgraves,Major,Closed,Fixed,18/Jul/11 19:40,19/Oct/11 00:26
Bug,MAPREDUCE-2706,12514499,MR-279: Submit jobs beyond the max jobs per queue limit no longer gets logged,"Submitting jobs over the queue limits used to print log messages such as these:
hadoop-mapred-jobtracker-HOSTNAME.log. ... INFO
org.apache.hadoop.mapred.CapacityTaskScheduler: default has 10 active tasks for user MYUSER, cannot initialize
job_XXX with 10 tasks since it will exceed limit of 15 active tasks per user for this queue
and
hadoop-mapred-jobtracker-HOSTNAME.log ... INFO org.apache.hadoop.mapred.CapacityTaskScheduler: default already has 2 running jobs and 0 initializing jobs; cannot initialize job_XXX since it will exceeed limit of 2 initialized jobs for this queue

These log messages are useful - especially for QA and testing.  ",naisbitt,naisbitt,Major,Closed,Fixed,18/Jul/11 20:16,15/Nov/11 00:49
Bug,MAPREDUCE-2710,12514593,Update DFSClient.stringifyToken(..) in JobSubmitter.printTokens(..) for HDFS-2161,{{DFSClient.stringifyToken(..)}} was removed by HDFS-2161.  {{JobSubmitter.printTokens(..)}} won't be compiled.,szetszwo,szetszwo,Major,Closed,Fixed,19/Jul/11 14:04,02/May/13 02:29
Bug,MAPREDUCE-2711,12514605,TestBlockPlacementPolicyRaid cannot be compiled,{{TestBlockPlacementPolicyRaid}} access internal {{FSNamesystem}} directly.  It cannot be compiled after HDFS-2147.,szetszwo,szetszwo,Major,Closed,Fixed,19/Jul/11 15:29,16/Mar/15 17:57
Bug,MAPREDUCE-2716,12514832,MR279: MRReliabilityTest job fails because of missing job-file.,"The ApplicationReport should have the jobFile (e.g. hdfs://localhost:9000/tmp/hadoop-<USER>/mapred/staging/<USER>/.staging/job_201107121640_0001/job.xml)


Without it, jobs such as MRReliabilityTest fail with the following error (caused by the fact that jobFile is hardcoded to """" in TypeConverter.java):
e.g. java.lang.IllegalArgumentException: Can not create a Path from an empty string
        at org.apache.hadoop.fs.Path.checkPathArg(Path.java:88)
        at org.apache.hadoop.fs.Path.<init>(Path.java:96)
        at org.apache.hadoop.mapred.JobConf.<init>(JobConf.java:445)
        at org.apache.hadoop.mapreduce.Cluster.getJobs(Cluster.java:104)
        at org.apache.hadoop.mapreduce.Cluster.getAllJobs(Cluster.java:218)
        at org.apache.hadoop.mapred.JobClient.getAllJobs(JobClient.java:757)
        at org.apache.hadoop.mapred.JobClient.jobsToComplete(JobClient.java:741)
        at org.apache.hadoop.mapred.ReliabilityTest.runTest(ReliabilityTest.java:219)
        at org.apache.hadoop.mapred.ReliabilityTest.runSleepJobTest(ReliabilityTest.java:133)
        at org.apache.hadoop.mapred.ReliabilityTest.run(ReliabilityTest.java:116)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.mapred.ReliabilityTest.main(ReliabilityTest.java:504)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
        at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:111)
        at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:118)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:192)",naisbitt,naisbitt,Major,Closed,Fixed,20/Jul/11 21:13,15/Nov/11 00:48
Bug,MAPREDUCE-2722,12515016,Gridmix simulated job's map's hdfsBytesRead counter is wrong when compressed input is used,"When compressed input was used by original job's map task, then the simulated job's map task's hdfsBytesRead counter is wrong if compression emulation is enabled. This issue is because hdfsBytesRead of map task of original job is considered as uncompressed map input size by Gridmix.",ravidotg,ravidotg,Major,Closed,Fixed,22/Jul/11 11:25,03/Sep/14 22:45
Bug,MAPREDUCE-2727,12515290,MR-279: SleepJob throws divide by zero exception when count = 0,"When the count is 0 for mappers or reducers, a divide-by-zero exception is thrown.  There are existing checks to error out when count < 0, which obviously doesn't handle the 0 case.  This is causing the MRReliabilityTest to fail.",naisbitt,naisbitt,Major,Closed,Fixed,25/Jul/11 17:36,15/Nov/11 00:48
Bug,MAPREDUCE-2732,12515478,Some tests using FSNamesystem.LOG cannot be compiled,,szetszwo,szetszwo,Major,Closed,Fixed,27/Jul/11 06:11,15/Nov/11 00:49
Bug,MAPREDUCE-2735,12515529,MR279: finished applications should be added to an application summary log,When an application finishes it should be added to an application summary log for historical purposes.  jira MAPREDUCE-2649 is going to start purging applications from RM when certain limits are hit which makes this more critical. We also need to save the information early enough after the app finishes so we don't lose the info if the RM does get restarted.,tgraves,tgraves,Major,Closed,Fixed,27/Jul/11 13:40,15/Nov/11 00:49
Bug,MAPREDUCE-2737,12515558,Update the progress of jobs on client side,The progress of the jobs are not being correctly updated on the client side. The map progress halts at 66% and both map/reduce progress % does not display 100 when the job completes.,sseth,rramya,Major,Closed,Fixed,27/Jul/11 18:42,15/Nov/11 00:48
Bug,MAPREDUCE-2738,12515562,Missing cluster level stats on the RM UI,"Cluster usage information such as the following are currently not available in the RM UI. 

- Total number of apps submitted so far
- Total number of containers running/total memory usage 
- Total capacity of the cluster (in terms of memory)
- Reserved memory
- Total number of NMs - sorting based on Node IDs is an option but when there are lost NMs or restarted NMs, the node ids does not correspond to the actual value
- Blacklisted NMs - sorting based on health-status and counting manually is not very straight forward
- Excluded NMs
- Handle to the jobhistory server
",revans2,rramya,Blocker,Closed,Fixed,27/Jul/11 19:34,16/Mar/15 17:57
Bug,MAPREDUCE-2739,12515593,MR-279: Update installation docs (remove YarnClientFactory),"Need to remove reference to the YarnClinetFactory in the INSTALL instructions: https://svn.apache.org/repos/asf/hadoop/common/branches/MR-279/mapreduce/INSTALL

The YarnClientFactory class removed (MAPRECUCE-2400 patch). ",bowang,ahmed.radwan,Minor,Closed,Fixed,28/Jul/11 00:18,11/Oct/12 17:48
Bug,MAPREDUCE-2740,12515701,MultipleOutputs in new API creates needless TaskAttemptContexts,"MultipleOutputs.write creates a new TaskAttemptContext, which we've seen to take a significant amount of CPU. The TaskAttemptContext constructor creates a JobConf, gets current UGI, etc. I don't see any reason it needs to do this, instead of just creating a single TaskAttemptContext when the InputFormat is created (or lazily but cached as a member)",tlipcon,tlipcon,Major,Closed,Fixed,28/Jul/11 20:39,15/Nov/11 00:48
Bug,MAPREDUCE-2749,12515740,[MR-279] NM registers with RM even before it starts various servers,"In case NM eventually fails to start the ContainerManager server because of say a port clash, RM will have to wait for expiry to detect the NM crash.

It is desirable to make NM register with RM only after it can start all of its components successfully.",tgraves,vinodkv,Major,Closed,Fixed,29/Jul/11 06:35,15/Nov/11 00:49
Bug,MAPREDUCE-2751,12515769,[MR-279] Lot of local files left on NM after the app finish.,"This ticket is about app-only files which should be cleaned after app-finish.

I see these undeleted after app-finish:
/tmp/nm-local-dir/0/nmPrivate/application_1305091029545_0001/*
/tmp/nm-local-dir/0/nmPrivate/container_1305019205843_0001_000002/*
/tmp/nm-local-dir/0/usercache/nobody/appcache/application_1305091029545_0001/*

We should check for other left-over files too, if any.",sseth,vinodkv,Blocker,Closed,Fixed,29/Jul/11 12:34,15/Nov/11 00:49
Bug,MAPREDUCE-2752,12515793,Build does not pass along properties to contrib builds,"Subant call to compile contribs do not pass along parameters from parent build.
Properties such as hadoop-common.version, asfrepo, offline, etc. are not passed along.
Result is that build not connected to Internet fails, hdfs proxy refuses to build against own recently built common but rather downloads 0.22-SNAPSHOT from apache again.",jrottinghuis,jrottinghuis,Minor,Resolved,Fixed,29/Jul/11 16:59,22/Aug/11 18:35
Bug,MAPREDUCE-2753,12515794,Generated POMs hardcode dependency on hadoop-common version 0.22.0-SNAPSHOT,"The generated poms inject the version of mapred itself, but hardcode the version of hadoop-common they depend on.
When trying to build downstream projects (HBase), then they will require hadoop-common-0.22.0-SNAPSHOT.jar instead of the version they compiled against.

When trying to do an offline build this will fail to resolve as another hadoop-common has been installed in the local maven repo.
Even during online build, it should compile against the hadoop-common that hdfs compiled against.

When versions mismatch one cannot do a coherent build. That is particularly problematic when making simultaneous change in hadoop-common and hadoop-mapreduce and you want to try this locally before committing each.",jrottinghuis,jrottinghuis,Major,Closed,Fixed,29/Jul/11 17:37,12/Dec/11 06:18
Bug,MAPREDUCE-2754,12515799,MR-279: AM logs are incorrectly going to stderr and error messages going incorrectly to stdout,"The log messages for AM container are going into stderr instead of syslog. Also, stderr and stdout roles are reversed.",raviteja,rramya,Blocker,Closed,Fixed,29/Jul/11 19:29,15/Nov/11 00:48
Bug,MAPREDUCE-2756,12515802,JobControl can drop jobs if an error occurs,"If you run a pig job with UDFs that has not been recompiled for MRV2.  There are situations where pig will fail with an error message stating that Hadoop failed and did not give a reason.  There is even the possibility of deadlock if an Error is thrown and the JobControl thread dies.
",revans2,revans2,Minor,Closed,Fixed,29/Jul/11 19:43,15/Nov/11 00:48
Bug,MAPREDUCE-2760,12515934,mapreduce.jobtracker.split.metainfo.maxsize typoed in mapred-default.xml,"The configuration mapreduce.jobtracker.split.metainfo.maxsize is incorrectly included in mapred-default.xml as mapreduce.*job*.split.metainfo.maxsize. It seems that {{jobtracker}} is correct, since this is a JT-wide property rather than a job property.",tlipcon,tlipcon,Minor,Closed,Fixed,01/Aug/11 16:59,26/Mar/13 17:12
Bug,MAPREDUCE-2762,12515939,[MR-279] - Cleanup staging dir after job completion,"The files created under the staging dir have to be deleted after job completion. Currently, all job.* files remain forever in the ${yarn.apps.stagingDir}",mahadev,rramya,Blocker,Closed,Fixed,01/Aug/11 18:08,15/Nov/11 00:48
Bug,MAPREDUCE-2763,12516041,IllegalArgumentException while using the dist cache,"IllegalArgumentException is seen while using distributed cache to cache some files and custom jars in classpath.

A simple way to reproduce this error is by using a streaming job:
hadoop jar hadoop-streaming.jar -libjars file://<path to custom jar> -input <path to input file> -output out -mapper ""cat"" -reducer NONE -cacheFile  hdfs://<path to some file>#linkname

This is a regression introduced and the same command works fine on 0.20.x",,rramya,Major,Closed,Fixed,01/Aug/11 22:31,15/Nov/11 00:48
Bug,MAPREDUCE-2764,12516855,Fix renewal of dfs delegation tokens,"The JT may have issues renewing hftp tokens which disrupt long distcp jobs.  The problem is the JT's delegation token renewal code is built on brittle assumptions.  The token's service field contains only the ""ip:port"" pair.  The renewal process assumes that the scheme must be hdfs.  If that fails due to a {{VersionMismatchException}}, it tries https based on another assumption that it must be hftp if it's not hdfs.  A number of other exceptions, most commonly {{IOExceptions}}, can be generated which fouls up the renewal since it won't fallback to https.",omalley,daryn,Major,Closed,Fixed,02/Aug/11 01:13,09/Mar/15 20:16
Bug,MAPREDUCE-2767,12517699,Remove Linux task-controller from 0.22 branch,"There's a potential security hole in the task-controller as it stands. Based on the discussion on general@, removing task-controller from the 0.22 branch will pave way for 0.22.0 release. (This was done for the 0.21.0 release as well: see MAPREDUCE-2014.) We can roll a 0.22.1 release with the task-controller when it is fixed.",milindb,milindb,Blocker,Closed,Fixed,02/Aug/11 21:10,16/Mar/15 17:49
Bug,MAPREDUCE-2772,12517805,MR-279: mrv2 no longer compiles against trunk after common mavenization.,mrv2 no longer compiles against trunk after common mavenization,revans2,revans2,Major,Closed,Fixed,03/Aug/11 16:38,15/Nov/11 00:49
Bug,MAPREDUCE-2773,12517831,[MR-279] server.api.records.NodeHealthStatus renamed but not updated in client NodeHealthStatus.java,"On the mr279 branch, you can't successfully run the ant target from the mapreduce directory since the checkin of the RM refactor.  

The issue is the NodeHealthStatus rename from org.apache.hadoop.yarn.server.api.records.NodeHealthStatus to org.apache.hadoop.yarn.api.records.NodeHealthStatus but the client mapreduce/src/java/org/apache/hadoop/mapred/NodeHealthStatus.java wasn't updated with the change",tgraves,tgraves,Minor,Closed,Fixed,03/Aug/11 19:44,15/Nov/11 00:48
Bug,MAPREDUCE-2774,12517842,[MR-279] Add a startup msg while starting RM/NM,"Add a startup msg while starting NM/RM indicating the version, build details etc. This will help in easier parsing of logs and debugging.",venug,rramya,Minor,Closed,Fixed,03/Aug/11 21:19,15/Nov/11 00:48
Bug,MAPREDUCE-2775,12517862,[MR-279] Decommissioned node does not shutdown,A Nodemanager which is decommissioned by an admin via refreshnodes does not automatically shutdown. ,devaraj,rramya,Blocker,Closed,Fixed,03/Aug/11 22:24,21/Feb/12 05:14
Bug,MAPREDUCE-2776,12517875,MR 279: Fix some of the yarn findbug warnings,Fix / ignore some of the findbug warnings in the yarn module.,sseth,sseth,Major,Closed,Fixed,04/Aug/11 01:33,15/Nov/11 00:50
Bug,MAPREDUCE-2779,12517985,JobSplitWriter.java can't handle large job.split file,"We use cascading MultiInputFormat. MultiInputFormat sometimes generates big job.split used internally by hadoop, sometimes it can go beyond 2GB.

In JobSplitWriter.java, the function that generates such file uses 32bit signed integer to compute offset into job.split.


writeNewSplits
...
        int prevCount = out.size();
...
        int currCount = out.size();

writeOldSplits
...
      long offset = out.size();
...
      int currLen = out.size();
",mingma,mingma,Major,Closed,Fixed,05/Aug/11 01:32,16/Mar/15 17:42
Bug,MAPREDUCE-2781,12517996,mr279 RM application finishtime not set,The RM Application finishTime isn't being set.  Looks like it got lost in the RM refactor.,tgraves,tgraves,Minor,Closed,Fixed,05/Aug/11 04:24,15/Nov/11 00:49
Bug,MAPREDUCE-2783,12518053,mr279 job history handling after killing application,"The job history/application tracking url handling during kill is not consistent. Currently if you kill a job that was running the tracking url points to job history, but job history server doesn't have the job.  ",epayne,tgraves,Critical,Closed,Fixed,05/Aug/11 17:30,30/Oct/12 23:06
Bug,MAPREDUCE-2784,12518200,[Gridmix] TestGridmixSummary fails with NPE when run in DEBUG mode.,TestGridmixSummary fails with NPE when run in debug mode. JobFactory tries to access the _createReaderThread()_ API of JobStoryProducer which returns null in TestGridmixSummary's FakeJobStoryProducer.,amar_kamat,amar_kamat,Major,Closed,Fixed,08/Aug/11 13:16,10/Mar/15 04:31
Bug,MAPREDUCE-2788,12518264,Normalize requests in FifoScheduler.allocate to prevent NPEs later,The assignContainer() method in org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue can cause the scheduler to crash if the ResourseRequest capability memory == 0 (divide by zero).,ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,09/Aug/11 00:07,15/Nov/11 00:49
Bug,MAPREDUCE-2789,12518352,[MR:279] Update the scheduling info on CLI,"The scheduling information such as number of containers running, memory usage and reservations per job is not available on bin/mapred job -list CLI.",epayne,rramya,Major,Closed,Fixed,09/Aug/11 19:43,15/Nov/11 00:49
Bug,MAPREDUCE-2791,12518358,[MR-279] Missing/incorrect info on job -status CLI ,"There are a couple of details missing/incorrect on the job -status command line output for completed jobs:

1. Incorrect job file
2. map() completion is always 0
3. reduce() completion is always set to 0
4. history URL is empty
5. Missing launched map tasks
6. Missing launched reduce tasks 



",devaraj,rramya,Blocker,Closed,Fixed,09/Aug/11 20:14,15/Nov/11 00:48
Bug,MAPREDUCE-2793,12518381,"[MR-279] Maintain consistency in naming appIDs, jobIDs and attemptIDs ","appIDs, jobIDs and attempt/container ids are not consistently named in the logs, console and UI. For consistency purpose, they all have to follow a common naming convention.

Currently, 
For appID
=========
On the RM UI: app_1308259676864_5 
On the JHS UI: No appID 
Console/logs: No appID
mapred-local dirs are named as: application_1308259676864_0005

For jobID
=========
On the RM UI: job_1308259676864_5_5 
JHS UI: job_1308259676864_5_5 
Console/logs: job_1308259676864_0005
mapred-local dirs are named as: No jobID


For attemptID
============
On the RM UI: attempt_1308259676864_5_5_m_24_0
JHS attempt_1308259676864_5_5_m_24_0
Console/logs: attempt_1308259676864_0005_m_000024_0
mapred-local dirs are named as: container_1308259676864_0005_000024

",bikassaha,rramya,Critical,Resolved,Fixed,09/Aug/11 22:45,25/Feb/12 13:57
Bug,MAPREDUCE-2794,12518388,[MR-279] Incorrect metrics value for AvailableGB per queue per user,"AvailableGB per queue is not the same as AvailableGB per queue per user when the user limit is set to 100%.
i.e. if the total available GB of the cluster is 60, and queue ""default"" has 92% capacity with 100% as the user limit, AvailableGB per queue default = 55 (i.e. 0.92*60) whereas AvailableGB per queue for user ramya is 56 (however it should be 55 = 0.92*60*1) 

Also, unlike the AvailableGB/queue, AvailableGB/queue/user is not decremented when user ramya is running apps on the ""default"" queue.",johnvijoe,rramya,Blocker,Closed,Fixed,10/Aug/11 00:23,15/Nov/11 00:49
Bug,MAPREDUCE-2796,12518392,[MR-279] Start time for all the apps is set to 0,"The start time for all the apps in the output of ""job -list"" is set to 0",devaraj,rramya,Major,Closed,Fixed,10/Aug/11 00:57,15/Nov/11 00:48
Bug,MAPREDUCE-2797,12518418,Some java files cannot be compiled,"Due to the changes in HDFS-2239, the following files cannot be compiled (Thanks Amar for pointing them out.)
1. src/test/mapred/org/apache/hadoop/mapreduce/security/TestTokenCache.java
2. src/test/mapred/org/apache/hadoop/mapreduce/security/TestBinaryTokenFile.java
3. src/test/mapred/org/apache/hadoop/mapreduce/security/TestTokenCacheOldApi.java
4. src/contrib/raid/src/java/org/apache/hadoop/hdfs/server/blockmanagement/BlockPlacementPolicyRaid.java",szetszwo,szetszwo,Major,Closed,Fixed,10/Aug/11 09:36,15/Nov/11 00:50
Bug,MAPREDUCE-2800,12518496,"clockSplits, cpuUsages, vMemKbytes, physMemKbytes is set to -1 in jhist files","clockSplits, cpuUsages, vMemKbytes, physMemKbytes  is set to -1 for all the map tasks for the last 4 progress interval in the jobhistory files.
",sseth,rramya,Major,Closed,Fixed,10/Aug/11 18:28,16/Mar/15 17:57
Bug,MAPREDUCE-2804,12518518,"""Creation of symlink to attempt log dir failed."" message is not useful","In attempting to qualify the 204 RC2 release, my tasktracker logs are filled with the above message.  I'd love to do something about it, but since it doesn't tell me what exactly it is trying to symlink I cannot unless I dig into the source code.
",omalley,aw,Blocker,Closed,Fixed,10/Aug/11 21:58,02/Sep/11 22:13
Bug,MAPREDUCE-2806,12518563,[Gridmix] Load job fails with timeout errors when resource emulation is turned on,"When the Load job's tasks are emulating cpu/memory, the task-tracker kills the emulating task due to lack of status updates. Load job has its own status reporter which dies too soon.",amar_kamat,amar_kamat,Major,Resolved,Fixed,11/Aug/11 07:35,09/Sep/14 21:00
Bug,MAPREDUCE-2808,12518605,pull MAPREDUCE-2797 into mr279 branch,The ant tar command fails in the mapreduce directory on the mr279 branch.  The issue was a change in hdfs and was fixed on trunk with jira MAPREDUCE-2797.  Pull that change into mr279.,tgraves,tgraves,Minor,Closed,Fixed,11/Aug/11 16:19,15/Nov/11 00:48
Bug,MAPREDUCE-2821,12518620,[MR-279] Missing fields in job summary logs ,"The following fields are missing in the job summary logs in mrv2:
- numSlotsPerMap
- numSlotsPerReduce
- clusterCapacity (Earlier known as clusterMapCapacity and clusterReduceCapacity in 0.20.x)

The first two fields are important to know if the job was a High RAM job or not and the last field is important to know the total available resource in the cluster during job execution.
",mahadev,rramya,Blocker,Closed,Fixed,11/Aug/11 18:27,15/Nov/11 00:49
Bug,MAPREDUCE-2837,12518658,MR-279: Bug fixes ported from y-merge,Similar to MAPREDUCE-2679.,,acmurthy,Major,Resolved,Fixed,11/Aug/11 23:47,15/Aug/11 18:01
Bug,MAPREDUCE-2839,12518667,MR Jobs fail on a secure cluster with viewfs,TokenCache needs to use the new FileSystem.getDelegationTokens api for it to work with viewfs.,sseth,sseth,Major,Closed,Fixed,12/Aug/11 05:16,15/Nov/11 00:48
Bug,MAPREDUCE-2840,12518715,mr279 TestUberAM.testSleepJob test fails,"Currently the TestUberAM.testSleepJob  is failing on the mr279 branch. 

snippet of failure:
junit.framework.AssertionFailedError: null
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertTrue(Assert.java:27)
	at org.apache.hadoop.mapreduce.v2.TestMRJobs.testSleepJob(TestMRJobs.java:150)
	at org.apache.hadoop.mapreduce.v2.TestUberAM.testSleepJob(TestUberAM.java:58)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)",jeagles,tgraves,Minor,Closed,Fixed,12/Aug/11 16:05,15/Nov/11 00:48
Bug,MAPREDUCE-2842,12518898,Maven build issues in MR2 ,"* mapreduce has not been rebased on top of trunk

* mapreduce dir/module should be named hadoop-mapreduce (following convention of common, hdfs)

* there is lot of stuff under mapreduce that seems stale (bin/, conf, ivy/, lib/ src/)

* yarn* dirs/modules should be named hadoop-yarn* (following convention of other Hadoop artifacts)

* yarn/bin/ scripts should be under yarn/src/main/bin

* yarn/conf/ scripts should be under yarn/src/main/conf

* JAR POM files do not use hadoop-project POM as parent

* some POM files have version parameterized and this will break things for people consuming JARs from Maven repos

* mapreduce is not using assembly from hadoop-assemblies (the changes introduced by HDFS-2096 make the assembly/packaging reusable across different components)
",,tucu00,Major,Resolved,Fixed,15/Aug/11 16:50,09/Mar/15 21:35
Bug,MAPREDUCE-2843,12518913,[MR-279] Node entries on the RM UI are not sortable,The nodemanager entries on the RM UI is not sortable unlike the other web pages. ,abhijit.shingate,rramya,Major,Closed,Fixed,15/Aug/11 19:24,16/Mar/15 17:57
Bug,MAPREDUCE-2844,12518915,[MR-279] Incorrect node ID info ,The node ID info for the nodemanager entires on the RM UI incorrectly displays the value of $yarn.server.nodemanager.address instead of the ID.,raviteja,rramya,Trivial,Closed,Fixed,15/Aug/11 19:32,15/Nov/11 00:48
Bug,MAPREDUCE-2846,12519020,a small % of all tasks fail with DefaultTaskController,"After upgrading our test 0.20.203 grid to 0.20.204-rc2, we ran terasort to verify operation.  While the job completed successfully, approx 10% of the tasks failed with task runner execution errors and the inability to create symlinks for attempt logs.",omalley,aw,Blocker,Closed,Fixed,16/Aug/11 16:38,05/Sep/11 14:23
Bug,MAPREDUCE-2852,12519203,Jira for YDH bug 2854624 ,"The DefaultTaskController and LinuxTaskController reference Yahoo! internal bug 2854624:

{code}
FileSystem rawFs = FileSystem.getLocal(getConf()).getRaw();
long logSize = 0; //TODO: Ref BUG:2854624
{code}

This jira tracks this TODO. If someone w/ access to Yahoo's bugzilla could update this jira with what the bug is that would be great.",kihwal,eli,Major,Closed,Fixed,17/Aug/11 22:56,19/Oct/11 00:26
Bug,MAPREDUCE-2854,12519307,update INSTALL with config necessary run mapred on yarn,"The following config is needed to run mapreduce on yarn framework.  Document it in the INSTALL doc.

<property>
<name> mapreduce.framework.name</name>
<value>yarn</value>
</property>


The INSTALL doc also still references the old 22 mapred examples jar.",tgraves,tgraves,Major,Closed,Fixed,18/Aug/11 15:45,15/Nov/11 00:48
Bug,MAPREDUCE-2855,12519318,ResourceBundle lookup during counter name resolution takes a lot of time,"Loading a job status page in trunk takes a lot of time, and it seems like most of the time is spent resolving counter names. Looking through the JDK source, ResourceBundle.getBundle(String) ends up calling getClassContext() which is not very efficient. I think if we pass our own classloader manually it will be faster. In Counters.incrAllCounters, we may also be able to avoid setting the counter name if one is already set.",sseth,tlipcon,Major,Resolved,Fixed,18/Aug/11 17:04,18/Aug/12 19:22
Bug,MAPREDUCE-2859,12519353,mapreduce trunk is broken with eclipse plugin contrib,"ant compile with eclipse home fails mapreduce trunk builds.

$ANT_HOME/bin/ant -Dversion=${VERSION} -Declipse.home=$ECLIPSE_HOME compile

compile:
     [echo] contrib: eclipse-plugin 
    [javac] Compiling 45 source files to /home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build/contrib/eclipse-plugin/classes
    [javac] /home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk/trunk/src/contrib/eclipse-plugin/src/java/org/apache/hadoop/eclipse/server/HadoopServer.java:39: cannot find symbol
    [javac] symbol  : class JobClient
    [javac] location: package org.apache.hadoop.mapred
    [javac] import org.apache.hadoop.mapred.JobClient;
    [javac]                                ^


-----




 [javac]     JobConf conf = new JobConf(location.getConfiguration());
    [javac]                        ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 49 errors

BUILD FAILED
/home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk/trunk/build.xml:451: The following error occurred while executing this line:
/home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk/trunk/src/contrib/build.xml:30: The following error occurred while executing this line:
/home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk/trunk/src/contrib/eclipse-plugin/build.xml:62: Compile failed; see the compiler error output for details.
",gkesavan,gkesavan,Major,Closed,Fixed,18/Aug/11 23:53,15/Nov/11 00:48
Bug,MAPREDUCE-2860,12519376,Fix log4j logging in the maven test cases.,At present the logging in the new test cases is broken because surefire isnt able to find the log4j properties file. ,mahadev,mahadev,Major,Closed,Fixed,19/Aug/11 05:45,15/Nov/11 00:48
Bug,MAPREDUCE-2867,12519708,Remove Unused TestApplicaitonCleanup in resourcemanager/applicationsmanager.,TestApplicationCleanup in resourcemanager/applicationsmanager doesnt do anything. There is already a test in resourcemanager/TestApplicationCleanup which tests all the cleanup code for container and applications. We should remove the unused one in the trunk.,mahadev,mahadev,Major,Closed,Fixed,22/Aug/11 16:48,15/Nov/11 00:48
Bug,MAPREDUCE-2868,12519712,ant build broken in hadoop-mapreduce dir,"The ant build target doesn't work in the hadoop-mapreduce directory since the mavenization of hdfs changes were checked in.

Error it gives is:
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve]           ::          UNRESOLVED DEPENDENCIES         ::
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::
[ivy:resolve]           :: org.apache.avro#avro-ipc;working@host: not found
[ivy:resolve]           :: org.apache.hadoop#hadoop-alfredo;working@host: not found
[ivy:resolve]           :: commons-daemon#commons-daemon;working@host: not found
[ivy:resolve]           ::::::::::::::::::::::::::::::::::::::::::::::

Steps I followed:
check out trunk
build common/hdfs: mvn clean install -Pbintar -DskipTests
build yarn/mapred: 
mvn clean install assembly:assembly -DskipTests
ant veryclean tar -Dresolvers=internal  ----> this fails
",mahadev,tgraves,Major,Closed,Fixed,22/Aug/11 17:44,15/Nov/11 00:48
Bug,MAPREDUCE-2874,12519877,ApplicationId printed in 2 different formats and has 2 different toString routines that are used,"Looks like the ApplicationId is now printed in 2 different formats.  ApplicationIdPBImpl.java has a toString routine that prints it in the format: return ""application_"" + this.getClusterTimestamp() + ""_"" + this.getId();

While the webapps use ./hadoop-yarn-common/src/main/java/org/apache/hadoop/yarn/util/Apps.java toString that prints it like:     
return _join(""app"", id.getClusterTimestamp(), id.getId());  ",epayne,tgraves,Major,Closed,Fixed,23/Aug/11 20:46,15/Nov/11 00:48
Bug,MAPREDUCE-2876,12520015,ContainerAllocationExpirer appears to use the incorrect configs,"ContainerAllocationExpirer sets the expiration interval to be RMConfig.CONTAINER_LIVELINESS_MONITORING_INTERVAL but uses AMLIVELINESS_MONITORING_INTERVAL as the interval.  This is very different from what AMLivelinessMonitor does.

There should be two configs RMConfig.CONTAINER_LIVELINESS_MONITORING_INTERVAL for the monitoring interval and RMConfig.CONTAINER_EXPIRY_INTERVAL for the expiry.
",anupamseth,revans2,Critical,Closed,Fixed,24/Aug/11 20:12,15/Nov/11 00:48
Bug,MAPREDUCE-2877,12520073,Add missing Apache license header in some files in MR and also add the rat plugin to the poms.,"Some of the files in MR have a missing Apache header files. We also need to add the apache-rat plugin to be able to run rat automatically via the top level pom. 
",mahadev,mahadev,Major,Closed,Fixed,25/Aug/11 08:20,15/Nov/11 00:50
Bug,MAPREDUCE-2879,12520138,Change mrv2 version to be 0.23.0-SNAPSHOT,"Currently yarn.version and hadoop-mapreduce.version are set to be 1.0, clearly it's 0.23.0. :)

Also, we should stop using ${yarn.version} and ${hadoop-mapreduce.version} in all the poms, maven doesn't like the version substitutions - it complains bitterly! :)",acmurthy,acmurthy,Major,Resolved,Fixed,25/Aug/11 18:32,10/Jan/12 04:44
Bug,MAPREDUCE-2881,12520170,"mapreduce ant compilation fails ""java.lang.IllegalStateException: impossible to get artifacts""","[ivy:resolve] 	found com.cenqua.clover#clover;3.0.2 in fs
[ivy:resolve] 
[ivy:resolve] :: problems summary ::
[ivy:resolve] :::: ERRORS
[ivy:resolve] 	impossible to get artifacts when data has not been loaded. IvyNode = log4j#log4j;1.2.16
[ivy:resolve] 
[ivy:resolve] :: USE VERBOSE OR DEBUG MESSAGE LEVEL FOR MORE DETAILS

BUILD FAILED
/home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/hadoop-mapreduce-project/build.xml:451: The following error occurred while executing this line:
/home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/hadoop-mapreduce-project/src/contrib/build.xml:30: The following error occurred while executing this line:
/home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/hadoop-mapreduce-project/src/contrib/build-contrib.xml:511: impossible to resolve dependencies:
	java.lang.IllegalStateException: impossible to get artifacts when data has not been loaded. IvyNode = log4j#log4j;1.2.16
",gkesavan,gkesavan,Major,Closed,Fixed,25/Aug/11 21:58,15/Nov/11 00:50
Bug,MAPREDUCE-2882,12520173,TestLineRecordReader depends on ant jars,This test is currently importing an ant utility class to read a file - this dependency doesn't work in mavenized land.,tlipcon,tlipcon,Minor,Closed,Fixed,25/Aug/11 22:34,15/Nov/11 00:48
Bug,MAPREDUCE-2885,12520279,mapred-config.sh doesn't look for $HADOOP_COMMON_HOME/libexec/hadoop-config.sh,mapred-config.sh doesn't look for $HADOOP_COMMON_HOME/libexec/hadoop-config.sh and thus fails to find it and errors out.,acmurthy,acmurthy,Blocker,Closed,Fixed,26/Aug/11 17:48,15/Nov/11 00:50
Bug,MAPREDUCE-2886,12520298,Fix Javadoc warnings in MapReduce.,"On the current trunk and 0.23, there are 73 javadoc warnings which is causing the buildbot to -1 every patch in MR. We need to fix this to stabilize the CI precommit builds.",mahadev,mahadev,Critical,Closed,Fixed,26/Aug/11 19:55,15/Nov/11 00:49
Bug,MAPREDUCE-2888,12520313,saveVersion.sh doesn't work when svn copy is staged,"The build fails with an error on the sed command, since saveVersion.sh doesn't correctly grab the URL.",tlipcon,tlipcon,Trivial,Resolved,Fixed,26/Aug/11 21:47,09/Mar/15 21:58
Bug,MAPREDUCE-2903,12520546,Map Tasks graph is throwing XML Parse error when Job is executed with 0 maps,"{code:xml}
XML Parsing Error: no element found
Location: http://10.18.52.170:50030/taskgraph?type=map&jobid=job_201108291536_0001
Line Number 1, Column 1:
^
{code}
",devaraj,devaraj,Major,Closed,Fixed,29/Aug/11 10:28,17/Oct/12 18:27
Bug,MAPREDUCE-2904,12520552,HDFS jars added incorrectly to yarn classpath,,sharadag,sharadag,Major,Closed,Fixed,29/Aug/11 11:40,15/Nov/11 00:49
Bug,MAPREDUCE-2905,12520604,CapBasedLoadManager incorrectly allows assignment when assignMultiple is true (was: assignmultiple per job),"We encountered a situation where in the same cluster, large jobs benefit from mapred.fairscheduler.assignmultiple, but small jobs with small numbers of mappers do not: the mappers all clump to fully occupy just a few nodes, which causes those nodes to saturate and bottleneck. The desired behavior is to spread the job across more nodes so that a relatively small job doesn't saturate any node in the cluster.

Testing has shown that setting mapred.fairscheduler.assignmultiple to false gives the desired behavior for small jobs, but is unnecessary for large jobs. However, since this is a cluster-wide setting, we can't properly tune.

It'd be nice if jobs can set a param similar to mapred.fairscheduler.assignmultiple on submission to better control the task distribution of a particular job.",jwfbean,jwfbean,Major,Closed,Fixed,29/Aug/11 17:20,17/Oct/12 18:27
Bug,MAPREDUCE-2907,12520636,ResourceManager logs filled with [INFO] debug messages from org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue,I see a lot of info messages (probably used for debugging during development),raviprak,raviprak,Major,Closed,Fixed,29/Aug/11 21:06,15/Nov/11 00:48
Bug,MAPREDUCE-2908,12520674,Fix findbugs warnings in Map Reduce.,In the current trunk/0.23 codebase there are 5 findbugs warnings which cause the precommit CI builds to -1 the patches.,vinodkv,mahadev,Critical,Closed,Fixed,30/Aug/11 01:31,15/Nov/11 00:48
Bug,MAPREDUCE-2913,12520756,TestMRJobs.testFailingMapper does not assert the correct thing.,"{code}
    Assert.assertEquals(TaskCompletionEvent.Status.FAILED, 
        events[0].getStatus().FAILED);
    Assert.assertEquals(TaskCompletionEvent.Status.FAILED, 
        events[1].getStatus().FAILED);
{code}

when optimized would be

{code}
    Assert.assertEquals(TaskCompletionEvent.Status.FAILED, 
        TaskCompletionEvent.Status.FAILED);
    Assert.assertEquals(TaskCompletionEvent.Status.FAILED, 
        TaskCompletionEvent.Status.FAILED);
{code}

obviously these assertions will never fail.  If we remove the {code}.FAILED{code} the asserts no longer pass. This could be because MRApp mocks out the task launcher and never actually launches anything.",jeagles,revans2,Critical,Closed,Fixed,30/Aug/11 14:12,16/Mar/15 17:57
Bug,MAPREDUCE-2915,12520863,LinuxTaskController does not work when JniBasedUnixGroupsNetgroupMapping or JniBasedUnixGroupsMapping is enabled,"When a job is submitted, LinuxTaskController launches the native task-controller binary for job initialization. The native program does a series of prep work and call execv() to run JobLocalizer.  It was observed that JobLocalizer does fails to run when JniBasedUnixGroupsNetgroupMapping or JniBasedUnixGroupsMapping is enabled, resulting in 100% job failures.

JobLocalizer normally does not need the native library (libhadoop) for its functioning, but enabling a JNI user-to-group mapping function cause it to load the library. However, JobLocalizer cannot locate the library since ""java.library.path"" is not set.

The proposed solution is to pass the java.library.path property through task-controller. LinuxTaskController already does it when launching the task log truncater.",kihwal,kihwal,Major,Closed,Fixed,31/Aug/11 00:45,25/Oct/11 12:32
Bug,MAPREDUCE-2916,12520872,Ivy build for MRv1 fails with bad organization for common daemon.,This jira is to ignore ivy resolve errors because of bad poms in common daemons.,mahadev,mahadev,Major,Closed,Fixed,31/Aug/11 05:33,15/Nov/11 00:48
Bug,MAPREDUCE-2917,12520880,Corner case in container reservations,"Saw a corner case in container reservations where the node on which the AM is running was reserved, and hence never fulfilled leaving the application hanging.",acmurthy,acmurthy,Major,Closed,Fixed,31/Aug/11 06:44,15/Nov/11 00:49
Bug,MAPREDUCE-2925,12521175,job -status <JOB_ID> is giving continuously info message for completed jobs on the console,"This below message is coming continuously on the console.

{code:xml}
11/09/02 16:00:00 INFO mapred.ClientServiceDelegate: Failed to contact AM for job job_1314955256658_0009  Will retry..
11/09/02 16:00:00 INFO mapred.ClientServiceDelegate: Application state is completed. Redirecting to job history server null
11/09/02 16:00:00 INFO mapred.ClientServiceDelegate: Failed to contact AM for job job_1314955256658_0009  Will retry..
11/09/02 16:00:00 INFO mapred.ClientServiceDelegate: Application state is completed. Redirecting to job history server null
{code}
",devaraj,devaraj,Major,Closed,Fixed,02/Sep/11 12:08,16/Mar/15 17:57
Bug,MAPREDUCE-2932,12521365,Missing instrumentation plugin class shouldn't crash the TT startup per design,"Per the implementation of the TaskTracker instrumentation plugin implementation (from 2008), a ClassNotFoundException during loading up of an configured TaskTracker instrumentation class shouldn't have hampered TT start up at all.

But, there is one class-fetching call outside try/catch, which makes TT fall down with a RuntimeException if there's a class not found. Would be good to include this line into the try/catch itself.

Strace would appear as:

{code}
2011-08-25 11:45:38,470 ERROR org.apache.hadoop.mapred.TaskTracker: Can not start task tracker because java.lang.RuntimeException: java.lang.RuntimeException: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.CustomInstPlugin 
at org.apache.hadoop.conf.Configuration.getClass(Configuration.java) 
at org.apache.hadoop.mapred.TaskTracker.getInstrumentationClass(TaskTracker.java) 
at org.apache.hadoop.mapred.TaskTracker.initialize(TaskTracker.java) 
{code}",qwertymaniac,qwertymaniac,Trivial,Closed,Fixed,05/Sep/11 16:05,17/Oct/12 18:27
Bug,MAPREDUCE-2936,12521428,Contrib Raid compilation broken after HDFS-1620,"After working around MAPREDUCE-2935 by removing TestServiceLevelAuthorization and runing the following:
At the trunk level: mvn clean install package -Dtar -Pdist -Dmaven.test.skip.exec=true
In hadoop-mapreduce-project: ant compile-contrib -Dresolvers=internal

yields 14 errors.",vinodkv,vinodkv,Major,Closed,Fixed,06/Sep/11 09:01,16/Mar/15 17:52
Bug,MAPREDUCE-2937,12521533,Errors in Application failures are not shown in the client trace.,"The client side does not show enough information on why the job failed. Here is step to reproduce it:

1) set the scheduler to be capacity scheduler with queues a, b
2) submit a job to a queue that is not a,b

The job just fails without saying why it failed. We should have enough trace log at the client side to let the user know why it failed.",mahadev,mahadev,Critical,Closed,Fixed,07/Sep/11 03:52,15/Nov/11 00:49
Bug,MAPREDUCE-2938,12521537,Missing log stmt for app submission fail CS,Missing log stmt for app submission fail CS,acmurthy,acmurthy,Trivial,Closed,Fixed,07/Sep/11 05:34,15/Nov/11 00:48
Bug,MAPREDUCE-2940,12521541,Build fails with ant 1.7.0 but works with 1.8.0,"contrib builds fail when using Ant 1.7.

build.xml calls build.xml in contrib, which calls block-forensics build, which in turn uses build-contrib.
The inheritAll=true overrides the basedir in ant 1.7.0 but not in 1.8.0.
",jrottinghuis,jrottinghuis,Major,Closed,Fixed,07/Sep/11 06:03,12/Dec/11 06:19
Bug,MAPREDUCE-2942,12521570,TestNMAuditLogger.testNMAuditLoggerWithIP failing,"This is failing right after the MAPREDUCE-2655 commit, but Jenkins did report a success when that patch was submitted.

{code}
Standard Output

2011-09-07 07:12:52,785 INFO  ipc.Server (Server.java:run(349)) - Starting Socket Reader #1 for port 33000
2011-09-07 07:12:52,787 INFO  ipc.Server (WritableRpcEngine.java:registerProtocolAndImpl(399)) - ProtocolImpl=org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger$MyTestRPCServer protocolClass=org.apache.hadoop.yarn.server.nodemanager.TestNMAuditLogger$MyTestRPCServer version=1
2011-09-07 07:12:52,788 INFO  ipc.Server (Server.java:run(642)) - IPC Server Responder: starting
2011-09-07 07:12:52,788 INFO  ipc.Server (Server.java:run(473)) - IPC Server listener on 33000: starting
2011-09-07 07:12:52,788 INFO  ipc.Server (Server.java:run(1459)) - IPC Server handler 0 on 33000: starting
2011-09-07 07:12:52,798 INFO  ipc.Server (Server.java:run(1497)) - IPC Server handler 0 on 33000, call: ping(), rpc version=2, client version=1, methodsFingerPrint=-1968962669 from 67.195.138.31:33806, error: 
java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.ipc.TestRPC$TestProtocol
	at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)
{code}",tgraves,vinodkv,Critical,Closed,Fixed,07/Sep/11 10:54,10/Mar/15 04:31
Bug,MAPREDUCE-2947,12521628,Sort fails on YARN+MR with lots of task failures,"[~karams](the great man the world hardly knows about) found lots of failing tasks while running sort on a 350 node cluster. The failed tasks eventually failed the job and this happening consistently on the big cluster.
{quote}
Container launch failed for container_1315410418107_0002_01_002511 : RemoteTrace: java.lang.IllegalArgumentException at java.nio.Buffer.position(Buffer.java:218) at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:129) at java.nio.ByteBuffer.get(ByteBuffer.java:675) at com.google.protobuf.ByteString.copyFrom(ByteString.java:108) at com.google.protobuf.ByteString.copyFrom(ByteString.java:117) at org.apache.hadoop.yarn.util.ProtoUtils.convertToProtoFormat(ProtoUtils.java:97) at org.apache.hadoop.yarn.api.records.ProtoBase.convertToProtoFormat(ProtoBase.java:59) at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerResponsePBImpl.access$100(StartContainerResponsePBImpl.java:35) at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerResponsePBImpl$1$1.next(StartContainerResponsePBImpl.java:134) at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerResponsePBImpl$1$1.next(StartContainerResponsePBImpl.java:122) at com.google.protobuf.AbstractMessageLite$Builder.addAll(AbstractMessageLite.java:319) at org.apache.hadoop.yarn.proto.YarnServiceProtos$StartContainerResponseProto$Builder.addAllServiceResponse(YarnServiceProtos.java:12620) at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerResponsePBImpl.addServiceResponseToProto(StartContainerResponsePBImpl.java:144) at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerResponsePBImpl.mergeLocalToBuilder(StartContainerResponsePBImpl.java:60) at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerResponsePBImpl.mergeLocalToProto(StartContainerResponsePBImpl.java:68) at org.apache.hadoop.yarn.api.protocolrecords.impl.pb.StartContainerResponsePBImpl.getProto(StartContainerResponsePBImpl.java:52) at org.apache.hadoop.yarn.api.impl.pb.service.ContainerManagerPBServiceImpl.startContainer(ContainerManagerPBServiceImpl.java:69) at org.apache.hadoop.yarn.proto.ContainerManager$ContainerManagerService$2.callBlockingMethod(ContainerManager.java:83) at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:337) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1496) at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1492) at java.security.AccessController.doPrivileged(Native Method) at javax.security.auth.Subject.doAs(Subject.java:396) at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1490) at LocalTrace: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:151) at $Proxy20.startContainer(Unknown Source) at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:81) at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:215) at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) at java.lang.Thread.run(Thread.java:619) 
{quote}",vinodkv,vinodkv,Major,Closed,Fixed,07/Sep/11 17:23,15/Nov/11 00:48
Bug,MAPREDUCE-2948,12521648,"Hadoop streaming test failure, post MR-2767","After removing LinuxTaskController in MAPREDUCE-2767, one of the tests in contrib/streaming: TestStreamingAsDifferentUser.java is failing since it imports import org.apache.hadoop.mapred.ClusterWithLinuxTaskController. Patch forthcoming.",mahadev,milindb,Major,Closed,Fixed,07/Sep/11 19:43,16/Mar/15 17:49
Bug,MAPREDUCE-2949,12522238,NodeManager in a inconsistent state if a service startup fails.,"When a service startup fails at the Nodemanager, the Nodemanager JVM doesnot exit as the following threads are still running.

Daemon Thread [Timer for 'NodeManager' metrics system] (Running)	
Thread [pool-1-thread-1] (Running)	
Thread [Thread-11] (Running)	
Thread [DestroyJavaVM] (Running).

As a result, the NodeManager keeps running even though no services are started.",raviteja,raviteja,Major,Closed,Fixed,08/Sep/11 10:41,10/Mar/15 04:32
Bug,MAPREDUCE-2950,12522239,[Gridmix] TestUserResolve fails in trunk,TestUserResolve fails in trunk.,ravidotg,amar_kamat,Major,Closed,Fixed,08/Sep/11 10:49,10/Mar/15 04:32
Bug,MAPREDUCE-2952,12522245,Application failure diagnostics are not consumed in a couple of cases,"When Container crashes, the reason for failures isn't propagated because of a bug in _RMAppAttemptImpl.AMContainerCrashedTransition_ which simply discards the diagnostics of the container. Also RMAppAttemptImpl.diagnostics is never consumed.",acmurthy,vinodkv,Blocker,Closed,Fixed,08/Sep/11 11:40,15/Nov/11 00:49
Bug,MAPREDUCE-2953,12522246,"JobClient fails due to a race in RM, removes staged files and in turn crashes MR AM","[~Karams] ran into this multiple times. MR JobClient crashes immediately.

{code}
11/09/08 10:52:35 INFO mapreduce.JobSubmitter: number of splits:2094
11/09/08 10:52:36 INFO mapred.YARNRunner: AppMaster capability = memory: 2048,
11/09/08 10:52:36 INFO mapred.YARNRunner: Command to launch container for ApplicationMaster is : $JAVA_HOME/bin/java -Dhadoop.root.logger=INFO,console -Xmx1536m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1315478927026 1 <FAILCOUNT> 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr
11/09/08 10:52:36 INFO mapred.ResourceMgrDelegate: Submitted application application_1315478927026_1 to ResourceManager
11/09/08 10:52:36 INFO mapreduce.JobSubmitter: Cleaning up the staging area /user/gridperf/.staging/job_1315478927026_0001
RemoteTrace:
 at Local Trace:
        org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: failed to run job
        at org.apache.hadoop.yarn.factories.impl.pb.YarnRemoteExceptionFactoryPBImpl.createYarnRemoteException(YarnRemoteExceptionFactoryPBImpl.java:39)
        at org.apache.hadoop.yarn.ipc.RPCUtil.getRemoteException(RPCUtil.java:47)
        at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:250)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:377)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1072)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1069)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1069)
        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1089)
        at org.apache.hadoop.examples.RandomWriter.run(RandomWriter.java:283)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.examples.RandomWriter.main(RandomWriter.java:294)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:189)
}
{code}

The client crashes due to a race in RM.

Because the client fails, it immediately removes the staged files which in turn makes the MR AM itself to crash due to failed localization on the NM.",tgraves,vinodkv,Major,Closed,Fixed,08/Sep/11 12:02,15/Nov/11 00:48
Bug,MAPREDUCE-2954,12522256,Deadlock in NM with threads racing for ApplicationAttemptId,"Found this:
{code}
Java stack information for the threads listed above:
===================================================
""Thread-45"":
        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.getApplicationId(ApplicationAttemptIdPBImpl.java:101)
        - waiting to lock <0xb6a43ba0> (a org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl)
        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.compareTo(ApplicationAttemptIdPBImpl.java:144)
        - locked <0xb6a443a0> (a org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl)
        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.compareTo(ApplicationAttemptIdPBImpl.java:31)
        at org.apache.hadoop.yarn.api.records.impl.pb.ContainerIdPBImpl.compareTo(ContainerIdPBImpl.java:215)
        at org.apache.hadoop.yarn.api.records.impl.pb.ContainerIdPBImpl.compareTo(ContainerIdPBImpl.java:34)
        at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:797)
        at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1640)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:360)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ContainerEventDispatcher.handle(ContainerManagerImpl.java:355)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:113)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
        at java.lang.Thread.run(Thread.java:619)
""Thread-30"":
        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.getApplicationId(ApplicationAttemptIdPBImpl.java:101)
        - waiting to lock <0xb6a443a0> (a org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl)
        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.compareTo(ApplicationAttemptIdPBImpl.java:144)
        - locked <0xb6a43ba0> (a org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl)
        at org.apache.hadoop.yarn.api.records.impl.pb.ApplicationAttemptIdPBImpl.compareTo(ApplicationAttemptIdPBImpl.java:31)
        at org.apache.hadoop.yarn.api.records.impl.pb.ContainerIdPBImpl.compareTo(ContainerIdPBImpl.java:215)
        at org.apache.hadoop.yarn.api.records.impl.pb.ContainerIdPBImpl.compareTo(ContainerIdPBImpl.java:34)
        at java.util.concurrent.ConcurrentSkipListMap.doRemove(ConcurrentSkipListMap.java:1078)
        at java.util.concurrent.ConcurrentSkipListMap.remove(ConcurrentSkipListMap.java:1673)
        at java.util.concurrent.ConcurrentSkipListMap$Iter.remove(ConcurrentSkipListMap.java:2256)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getNodeStatus(NodeStatusUpdaterImpl.java:223)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.access$300(NodeStatusUpdaterImpl.java:62)
        at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl$1.run(NodeStatusUpdaterImpl.java:262)
Found 1 deadlock.
{code}",sseth,vinodkv,Critical,Closed,Fixed,08/Sep/11 13:28,16/Mar/15 17:57
Bug,MAPREDUCE-2958,12522328,mapred-default.xml not merged from mr279,"I have been running wordcount out of the 23 examples jar.  It says it succeeds but doesn't actually output a file.

hadoop jar examples/hadoop-mapreduce-0.23.0-SNAPSHOT/hadoop-mapreduce-examples-0.23.0-SNAPSHOT.jar wordcount input output2

input file is really basic:
fdksajl
dlkfsajlfljda;j
kldfsjallj
test
one
two
test",acmurthy,tgraves,Critical,Closed,Fixed,08/Sep/11 21:03,15/Nov/11 00:50
Bug,MAPREDUCE-2963,12522350,TestMRJobs hangs waiting to connect to history server.,TestMRJobs is hanging waiting to connect to history server. I will post the logs next.,sseth,mahadev,Critical,Closed,Fixed,09/Sep/11 00:14,10/Mar/15 04:32
Bug,MAPREDUCE-2964,12522368,mapreduce trunk build fails with compile-mapred-test ant target,"{noformat}
compile-mapred-test:
    [mkdir] Created dir: /home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/hadoop-mapreduce-project/build/test/mapred/classes
....
  [javac] found   : org.apache.hadoop.mapred.IFile.Writer
    [javac] required: org.apache.hadoop.mapred.IFile.Writer<java.lang.String,java.lang.Integer>
    [javac]     Writer<String, Integer> mockWriter = mock(Writer.class);
    [javac]                                              ^
    [javac] /home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestCombineOutputCollector.java:128: warning: [unchecked] unchecked conversion
    [javac] found   : org.apache.hadoop.mapred.IFile.Writer
    [javac] required: org.apache.hadoop.mapred.IFile.Writer<java.lang.String,java.lang.Integer>
    [javac]     Writer<String, Integer> mockWriter = mock(Writer.class);
    [javac]                                              ^
    [javac] /home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJvmManager.java:63: unreported exception java.io.IOException; must be caught or declared to be thrown
    [javac]     FileUtil.fullyDelete(TEST_DIR);
    [javac]                         ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 1 error
    [javac] 2 warnings

BUILD FAILED
/home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/hadoop-mapreduce-project/build.xml:538: The following error occurred while executing this line:
/home/jenkins/jenkins-slave/workspace/Hadoop-Mapreduce-trunk-Commit/trunk/hadoop-mapreduce-project/build.xml:615: Compile failed; see the compiler error output for details.

{noformat}",,gkesavan,Major,Resolved,Fixed,09/Sep/11 05:54,13/Sep/11 06:14
Bug,MAPREDUCE-2965,12522372,"Streamline hashCode(), equals(), compareTo() and toString() for all IDs","MAPREDUCE-2954 moved these methods to the record interfaces from the PB impls for ContainerId, ApplicationId and ApplicationAttemptId. This is good as they don't need to be tied to the implementation.

We should do the same for all IDs. In fact some of these are missing for IDs like MR AM JobId, TaskId etc.",sseth,vinodkv,Blocker,Closed,Fixed,09/Sep/11 06:31,16/Mar/15 17:57
Bug,MAPREDUCE-2970,12522425,"Null Pointer Exception while submitting a Job, If mapreduce.framework.name property is not set.","If mapreduce.framework.name property is not set in mapred-site.xml, Null pointer Exception is thrown.

java.lang.NullPointerException
	at org.apache.hadoop.mapreduce.Cluster$1.run(Cluster.java:133)
	at org.apache.hadoop.mapreduce.Cluster$1.run(Cluster.java:1)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
	at org.apache.hadoop.mapreduce.Cluster.getFileSystem(Cluster.java:131)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1067)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1089)
	at org.apache.hadoop.examples.WordCount.main(WordCount.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:189)",venug,venug,Major,Closed,Fixed,09/Sep/11 13:38,16/Mar/15 17:57
Bug,MAPREDUCE-2971,12522432,ant build mapreduce fails  protected access  jc.displayJobList(jobs);,"Running the ant target in the hadoop-mapreduce-project directory fails with:

[jsp-compile] log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
    [javac] /home/tgraves/branch23/branch-0.23/hadoop-mapreduce-project/build.xml:398: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 50 source files to /home/tgraves/branch23/branch-0.23/hadoop-mapreduce-project/build/classes
    [javac] /home/tgraves/branch23/branch-0.23/hadoop-mapreduce-project/src/java/org/apache/hadoop/mapred/JobQueueClient.java:189: displayJobList(org.apache.hadoop.mapreduce.JobStatus[]) has protected access in org.apache.hadoop.mapreduce.tools.CLI
    [javac]       jc.displayJobList(jobs);
    [javac]         ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 1 error",tgraves,tgraves,Blocker,Closed,Fixed,09/Sep/11 14:18,15/Nov/11 00:48
Bug,MAPREDUCE-2975,12522453,ResourceManager Delegate is not getting initialized with yarn-site.xml as default configuration.,MAPREDUCE-2937 accidentally changes ResourceMgrDelegate so that it does not pick up yarn-site.xml as a default resource. Will upload patch.,mahadev,mahadev,Blocker,Closed,Fixed,09/Sep/11 17:17,15/Nov/11 00:48
Bug,MAPREDUCE-2978,12522470,hudson findbugs not reporting properly,"It seems that hudson is not properly reporting findbug failures introduced by jiras. 

Here is an example where hudson gave the jira a +1 for findbugs but it really introduced a bug:
https://issues.apache.org/jira/browse/MAPREDUCE-2937

The actual findbugs report - you'll see there is 1:
https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/662//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-jobclient.html

Note that I had to enter in the extra path of hadoop-mapreduce-project to see the html file so perhaps the path it is using to do the diff is wrong.",tomwhite,tgraves,Major,Resolved,Fixed,09/Sep/11 19:57,12/May/16 18:23
Bug,MAPREDUCE-2979,12522482,Remove ClientProtocolProvider configuration under mapreduce-client-core,"ClientProtocolProvider configuration exists under the job-client and core modules. It's really only required in job-client. The version in core points to JobTrackerClientProtocolProvider which causes

java.util.ServiceConfigurationError: org.apache.hadoop.mapreduce.protocol.ClientProtocolProvider: Provider org.apache.hadoop.mapred.JobTrackerClientProtocolProvider not found
        at java.util.ServiceLoader.fail(ServiceLoader.java:214)
        at java.util.ServiceLoader.access$400(ServiceLoader.java:164)
        at java.util.ServiceLoader$LazyIterator.next(ServiceLoader.java:350)
        at java.util.ServiceLoader$1.next(ServiceLoader.java:421)
        at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:73)
        at org.apache.hadoop.mapreduce.Job.<init>(Job.java:133)
        at org.apache.hadoop.mapreduce.Job.<init>(Job.java:138)
        at org.apache.hadoop.examples.WordCount.main(WordCount.java:75)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:189)",sseth,sseth,Major,Closed,Fixed,09/Sep/11 22:08,16/Mar/15 17:57
Bug,MAPREDUCE-2984,12522756,Throwing NullPointerException when we open the container page,"{code:xml}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.yarn.api.records.ContainerId.compareTo(ContainerId.java:97)
	at org.apache.hadoop.yarn.api.records.ContainerId.compareTo(ContainerId.java:23)
	at java.util.concurrent.ConcurrentSkipListMap.doGet(ConcurrentSkipListMap.java:819)
	at java.util.concurrent.ConcurrentSkipListMap.get(ConcurrentSkipListMap.java:1640)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerPage$ContainerBlock.render(ContainerPage.java:70)
	at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:64)
	at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:74)
	at org.apache.hadoop.yarn.webapp.View.render(View.java:210)
{code}

{code:xml}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.yarn.server.nodemanager.webapp.ContainerPage$ContainerBlock.render(ContainerPage.java:71)
	at org.apache.hadoop.yarn.webapp.view.HtmlBlock.render(HtmlBlock.java:64)
	at org.apache.hadoop.yarn.webapp.view.HtmlBlock.renderPartial(HtmlBlock.java:74)
	at org.apache.hadoop.yarn.webapp.View.render(View.java:210)
	at org.apache.hadoop.yarn.webapp.view.HtmlPage$Page.subView(HtmlPage.java:47)
	at org.apache.hadoop.yarn.webapp.hamlet.HamletImpl$EImp._v(HamletImpl.java:117)
	at org.apache.hadoop.yarn.webapp.hamlet.Hamlet$TD._(Hamlet.java:843)
	at org.apache.hadoop.yarn.webapp.view.TwoColumnLayout.render(TwoColumnLayout.java:54)
	at org.apache.hadoop.yarn.webapp.view.HtmlPage.render(HtmlPage.java:80)
	at org.apache.hadoop.yarn.webapp.Controller.render(Controller.java:210)
	at org.apache.hadoop.yarn.server.nodemanager.webapp.NMController.container(NMController.java:62)
	... 30 more
{code}
",devaraj,devaraj,Minor,Closed,Fixed,12/Sep/11 15:35,15/Nov/11 00:49
Bug,MAPREDUCE-2985,12522776,findbugs error in ResourceLocalizationService.handle(LocalizationEvent),"hudson mapreduce is reporting a findbugs error:
https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/707//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-yarn-server-nodemanager.html

WMI 	Method org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.handle(LocalizationEvent) makes inefficient use of keySet iterator instead of entrySet iterator
	

Bug type WMI_WRONG_MAP_ITERATOR (click for details)
In class org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService
In method org.apache.hadoop.yarn.server.nodemanager.containermanager.localizer.ResourceLocalizationService.handle(LocalizationEvent)
At ResourceLocalizationService.java:[line 295]
Another occurrence at ResourceLocalizationService.java:[line 318]

",tgraves,tgraves,Major,Closed,Fixed,12/Sep/11 18:24,16/Mar/15 17:57
Bug,MAPREDUCE-2987,12522796,RM UI display logged in user as null,"All the pages of the UI, currently show ""Logged in as: null"" instead of the correct username",tgraves,tgraves,Major,Closed,Fixed,12/Sep/11 20:12,15/Nov/11 00:49
Bug,MAPREDUCE-2991,12522823,queueinfo.jsp fails to show queue status if any Capacity scheduler queue name has dash/hiphen in it.,"If any queue name has a dash/hiphen in it, the queueinfo.jsp doesn't show any queue information.  This is happening because the queue name is used to create javascript variables and javascript doesn't allow dash in variable names.",priyomustafi,priyomustafi,Major,Closed,Fixed,12/Sep/11 23:21,16/Mar/15 17:49
Bug,MAPREDUCE-2994,12522859,Parse Error is coming for App ID when we click application link on the RM UI,"{code:xml}
Caused by: org.apache.hadoop.yarn.YarnException: Error parsing app ID: application_1315895242400_1
	at org.apache.hadoop.yarn.util.Apps.throwParseException(Apps.java:60)
	at org.apache.hadoop.yarn.util.Apps.toAppID(Apps.java:43)
	at org.apache.hadoop.yarn.util.Apps.toAppID(Apps.java:38)
	at org.apache.hadoop.yarn.server.resourcemanager.webapp.RmController.app(RmController.java:74)
	... 30 more
{code}",devaraj,devaraj,Major,Closed,Fixed,13/Sep/11 08:21,16/Mar/15 17:57
Bug,MAPREDUCE-2995,12522871,MR AM crashes when a container-launch hangs on a faulty NM,"AM tries to launch containers on a faulty node which blocks several/all of the {{StartContainer}} requests. Eventually, RM expires the container-allocations, informs the AM about container-expiry. But AM crashes with an INTERNAL_ERROR as the event is unexpected.
{code}
11/09/12 14:11:38 ERROR impl.TaskAttemptImpl: Can't handle this event at current state
org.apache.hadoop.yarn.state.InvalidStateTransitonException: Invalid event: TA_CONTAINER_COMPLETED at ASSIGNED
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:297)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:39)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:439)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:903)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:127)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:543)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:536)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:113)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
        at java.lang.Thread.run(Thread.java:619)
{code}

Found this on a big cluster where [~karams] was trying to get sort running.",vinodkv,vinodkv,Major,Closed,Fixed,13/Sep/11 09:49,15/Nov/11 00:50
Bug,MAPREDUCE-2996,12522877,Log uberized information into JobHistory and use the same via CompletedJob,We always print the uberized info on the UI to be false irrespective of whether it is uberized or not.,jeagles,vinodkv,Blocker,Closed,Fixed,13/Sep/11 10:19,15/Nov/11 00:49
Bug,MAPREDUCE-2997,12522880,MR task fails before launch itself with an NPE in ContainerLauncher,"Exception found on the AM web UI while the application is running:
{code}
Container launch failed for container_1315908079531_0002_01_000387 : java.lang.NullPointerException
  at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.getCMProxy(ContainerLauncherImpl.java:162)
  at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:204)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:619) 
{code}",vinodkv,vinodkv,Major,Closed,Fixed,13/Sep/11 10:38,15/Nov/11 00:48
Bug,MAPREDUCE-2998,12522915,Failing to contact Am/History for jobs: java.io.EOFException in DataInputStream,"I am getting an exception frequently when running my jobs on a single-node cluster.  It happens with basically any job I run: sometimes the job will work, but most of the time I get this exception (in this case, I was running a simple wordcount from the examples jar - where I got the exception 4 times in a row, and then the job worked the fifth time I submitted it). 
Sometimes restarting the namenode, resourcemanager, and historyserver helps - but not always.  Several other developers have seen this problem.


11/09/12 17:17:50 INFO mapred.YARNRunner: AppMaster capability = memory: 2048, 
11/09/12 17:17:51 INFO mapred.YARNRunner: Command to launch container for ApplicationMaster is : $JAVA_HOME/bin/java -Dhadoop.root.logger=DEBUG,console -Xmx1536m org.apache.hadoop.mapreduce.v2.app.MRAppMaster 1315847180566 6 <FAILCOUNT> 1><LOG_DIR>/stdout 2><LOG_DIR>/stderr 
11/09/12 17:17:51 INFO mapred.ResourceMgrDelegate: Submitted application application_1315847180566_6 to ResourceManager
11/09/12 17:17:51 INFO mapred.ClientCache: Connecting to HistoryServer at: 0.0.0.0:10020
11/09/12 17:17:51 INFO ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
11/09/12 17:17:51 INFO mapred.ClientCache: Connected to HistoryServer at: 0.0.0.0:10020
11/09/12 17:17:51 INFO ipc.HadoopYarnRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.mapreduce.v2.api.MRClientProtocol
11/09/12 17:17:51 INFO mapreduce.Job: Running job: job_1315847180566_0006
11/09/12 17:17:52 INFO mapreduce.Job:  map 0% reduce 0%
11/09/12 17:18:00 INFO mapred.ClientServiceDelegate: Tracking Url of JOB is <IP-ADDRESS>:55361
11/09/12 17:18:00 INFO mapred.ClientServiceDelegate: Connecting to <IP-ADDRESS>:43465
11/09/12 17:18:00 INFO ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
11/09/12 17:18:00 INFO ipc.HadoopYarnRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.mapreduce.v2.api.MRClientProtocol
11/09/12 17:18:01 INFO mapred.ClientServiceDelegate: Failed to contact AM/History for job job_1315847180566_0006  Will retry..
java.lang.reflect.UndeclaredThrowableException
    at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBClientImpl.java:179)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:237)
    at org.apache.hadoop.mapred.ClientServiceDelegate.getTaskCompletionEvents(ClientServiceDelegate.java:276)
    at org.apache.hadoop.mapred.YARNRunner.getTaskCompletionEvents(YARNRunner.java:547)
    at org.apache.hadoop.mapreduce.Job.getTaskCompletionEvents(Job.java:540)
    at org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1144)
    at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1092)
    at org.apache.hadoop.examples.WordCount.main(WordCount.java:84)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
    at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
    at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.hadoop.util.RunJar.main(RunJar.java:189)
Caused by: com.google.protobuf.ServiceException: java.io.IOException: Call to /<IP-ADDRESS>:43465 failed on local exception: java.io.EOFException
    at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)
    at $Proxy8.getTaskAttemptCompletionEvents(Unknown Source)
    at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBClientImpl.java:172)
    ... 23 more
Caused by: java.io.IOException: Call to /<IP-ADDRESS>:43465 failed on local exception: java.io.EOFException
    at org.apache.hadoop.ipc.Client.wrapException(Client.java:1119)
    at org.apache.hadoop.ipc.Client.call(Client.java:1087)
    at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)
    ... 25 more
Caused by: java.io.EOFException
    at java.io.DataInputStream.readInt(DataInputStream.java:375)
    at org.apache.hadoop.ipc.Client$Connection.receiveResponse(Client.java:816)
    at org.apache.hadoop.ipc.Client$Connection.run(Client.java:754)
11/09/12 17:18:01 INFO mapreduce.Job: Job job_1315847180566_0006 failed with state FAILED
11/09/12 17:18:01 INFO mapreduce.Job: Counters: 0 

",vinodkv,naisbitt,Critical,Closed,Fixed,13/Sep/11 15:57,16/Mar/15 17:57
Bug,MAPREDUCE-2999,12522924,hadoop.http.filter.initializers not working properly on yarn UI,"Currently httpserver only has *.html"", ""*.jsp as user facing urls when you add a filter. For the new web framework in yarn, the pages no longer have the *.html or *.jsp and thus they are not properly being filtered.",tgraves,tgraves,Critical,Closed,Fixed,13/Sep/11 17:17,02/May/13 02:29
Bug,MAPREDUCE-3003,12522963,Publish MR JARs to Maven snapshot repository,"Currently this is failing since no distribution management section is defined in the POM.

https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Common-trunk-Commit/883/consoleFull",tucu00,tomwhite,Major,Closed,Fixed,13/Sep/11 21:47,16/Mar/15 17:57
Bug,MAPREDUCE-3004,12522983,sort example fails in shuffle/reduce stage as it assumes a local job by default ,"Log trace when running sort on a single node setup:

11/09/13 17:01:06 INFO mapreduce.Job:  map 100% reduce 0%
11/09/13 17:01:10 INFO mapreduce.Job: Task Id : attempt_1315949787252_0009_r_000000_0, Status : FAILED
java.lang.UnsupportedOperationException: Incompatible with LocalRunner
	at org.apache.hadoop.mapred.YarnOutputFiles.getInputFile(YarnOutputFiles.java:200)
	at org.apache.hadoop.mapred.ReduceTask.getMapFiles(ReduceTask.java:183)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:365)
	at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:148)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
	at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:143)
",hitesh,hitesh,Minor,Closed,Fixed,14/Sep/11 01:21,15/Nov/11 00:49
Bug,MAPREDUCE-3005,12522994,MR app hangs because of a NPE in ResourceManager,"The app hangs and it turns out to be a NPE in ResourceManager. This happened two of five times on [~karams]'s sort runs on a big cluster.
{code}
2011-09-12 15:02:33,715 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event type NODE_UPDATE to the scheduler
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocateNodeLocal(AppSchedulingInfo.java:244)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.AppSchedulingInfo.allocate(AppSchedulingInfo.java:206)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.allocate(SchedulerApp.java:230)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1120)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:961)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:933)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:725)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:577)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:509)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:579)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:620)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:75)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:266)
        at java.lang.Thread.run(Thread.java:619)
{code}",acmurthy,vinodkv,Major,Closed,Fixed,14/Sep/11 04:29,15/Nov/11 00:49
Bug,MAPREDUCE-3006,12523029,MapReduce AM exits prematurely before completely writing and closing the JobHistory file,"[~Karams] was executing a sleep job with 100,000 tasks on a 350 node cluster to test MR AM's scalability and ran into this. The job ran successfully but the history was not available.

I debugged around and figured that the job is finishing prematurely before the JobHistory is written. In most of the cases, we don't see this bug as we have a 5 seconds sleep in AM towards the end.",vinodkv,vinodkv,Major,Closed,Fixed,14/Sep/11 11:08,15/Nov/11 00:50
Bug,MAPREDUCE-3009,12523055,RM UI -> Applications -> Application(Job History) -> Map Tasks -> Task ID -> Node link is not working,"RM UI -> Applications -> Application(Job History) -> Map Tasks -> Task ID -> Node link is not working. The URL contains extra '/' which is causing the problem. Please find in the attached screen shots.
",chaku88,chaku88,Major,Resolved,Fixed,14/Sep/11 14:04,06/Mar/12 13:30
Bug,MAPREDUCE-3017,12523274,The Web UI shows FINISHED for killed/successful/failed jobs.,The RM web ui shows FINISHED status for all the jobs even if they failed/killed or were successful. This should be fixed. Only the jobs where the AM crashes are marked as Failed.  ,mahadev,mahadev,Blocker,Closed,Fixed,15/Sep/11 23:59,15/Nov/11 00:50
Bug,MAPREDUCE-3018,12523276,Streaming jobs with -file option fail to run.,"Streaming jobs fail to run with the -file option.
hadoop jar streaming.jar -input input.txt -output Out -mapper ""mapper.sh"" -reducer NONE -file path_to_mapper.sh

fails to run.",mahadev,mahadev,Blocker,Closed,Fixed,16/Sep/11 00:03,15/Nov/11 00:48
Bug,MAPREDUCE-3020,12523303,Node link in reduce task attempt page is not working [Job History Page],"RM UI -> Applications -> Application(Job History) -> Reduce Tasks -> Task ID -> Node link is not working
hostname for ReduceAttemptFinishedEvent is coming wrong when loading from history file.
",chaku88,chaku88,Major,Closed,Fixed,16/Sep/11 09:20,16/Mar/15 17:57
Bug,MAPREDUCE-3021,12523341,"all yarn webapps use same base name of ""yarn/""","All of the yarn webapps (resource manager, node manager, app master, job history) use the same base url of /yarn/.  This doesn't lend itself very well to filters be able to differentiate them to say allow some to be not authenticated and other to be authenticated.  Perhaps we should rename them based on component.

There are also things in the code that hardcode paths to ""/yarn"" that should be fixed up.

",tgraves,tgraves,Major,Closed,Fixed,16/Sep/11 14:42,17/Jul/13 22:56
Bug,MAPREDUCE-3022,12523342,Some Web UI links to other components don't specify path,"Some of the links to other components in the web ui just specify host:port and don't add on the path.  For instance in the RM UI - the nodes page.  Each node is just listed as IP:port.  The actual path to those pages are IP:port/yarn/*.  I think the links should be what that components webapp is registered at.  

There may be other places too so we should search for them.",,tgraves,Major,Resolved,Fixed,16/Sep/11 14:47,10/Mar/15 01:22
Bug,MAPREDUCE-3023,12523353,Queue state is not being translated properly (is always assumed to be running),"During translation of QueueInfo, 

bq. TypeConverter.java:435 : queueInfo.toString(), QueueState.RUNNING,
ought to be 
bq. queueInfo.toString(), QueueState.getState(queueInfo.getQueueState().toString().toLowerCase()),",raviprak,raviprak,Major,Closed,Fixed,16/Sep/11 16:18,15/Nov/11 00:50
Bug,MAPREDUCE-3025,12523377,Contribs not building,"Contribs are not getting built.
Snippet from Jenkins:

compile:
[subant] No sub-builds to iterate on
",jrottinghuis,jrottinghuis,Blocker,Closed,Fixed,16/Sep/11 18:28,12/Dec/11 06:18
Bug,MAPREDUCE-3026,12523404,"When user adds hierarchical queues to the cluster, mapred queue -list returns NULL Pointer Exception","When User adds the hierarchical queues, and try to see them from the command line using 
mapred queue -list 
It returns Null Pointer Exception.",mayank_bansal,mayank_bansal,Major,Resolved,Fixed,16/Sep/11 23:24,23/Nov/11 05:39
Bug,MAPREDUCE-3028,12523414,Support job end notification in .next /0.23,"Oozie primarily depends on  the job end notification to determine when the job finishes. In the current version,  job end notification is implemented in job tracker. Since job tracker will be removed in the upcoming hadoop release (.next), we wander where this support will move. I think this best effort notification could be implemented in the new Application Manager as one of the last step of job completion.

Whatever implementation will it be, Oozie badly needs this feature to be continued in next releases as well.

 

",raviprak,kamrul,Blocker,Closed,Fixed,17/Sep/11 02:07,06/Sep/12 11:44
Bug,MAPREDUCE-3030,12523541,RM is not processing heartbeat and continuously giving the message 'Node not found rebooting',"{code:title=Node Manager Logs|borderStyle=solid}
2011-09-19 13:39:29,816 INFO  webapp.WebApps (WebApps.java:start(162)) - Registered webapp guice modules
2011-09-19 13:39:29,817 INFO  service.AbstractService (AbstractService.java:start(61)) - Service:org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer is started.
2011-09-19 13:39:29,818 INFO  service.AbstractService (AbstractService.java:start(61)) - Service:Dispatcher is started.
2011-09-19 13:39:29,819 INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:start(133)) - Configured ContainerManager Address is 10.18.52.124:45454
2011-09-19 13:39:29,819 INFO  ipc.YarnRPC (YarnRPC.java:create(47)) - Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
2011-09-19 13:39:29,822 INFO  ipc.HadoopYarnRPC (HadoopYarnProtoRPC.java:getProxy(49)) - Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.server.api.ResourceTracker
2011-09-19 13:39:29,862 INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:registerWithRM(165)) - Connected to ResourceManager at 0.0.0.0:8025
2011-09-19 13:39:30,369 INFO  nodemanager.NodeStatusUpdaterImpl (NodeStatusUpdaterImpl.java:registerWithRM(189)) - Registered with ResourceManager as 10.18.52.124:45454 with total resource of memory: 8192, 
2011-09-19 13:39:30,369 INFO  service.AbstractService (AbstractService.java:start(61)) - Service:org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl is started.
2011-09-19 13:39:30,371 INFO  service.AbstractService (AbstractService.java:start(61)) - Service:org.apache.hadoop.yarn.server.nodemanager.NodeManager is started.
{code}



{code:title=Resource Manager Logs|borderStyle=solid}
2011-09-19 14:01:03,238 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:nodeHeartbeat(201)) - Node not found rebooting 10.18.52.124:45454
Call: protocol=org.apache.hadoop.yarn.proto.ResourceTracker$ResourceTrackerService$BlockingInterface, method=nodeHeartbeat
2011-09-19 14:01:04,240 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:nodeHeartbeat(201)) - Node not found rebooting 10.18.52.124:45454
Call: protocol=org.apache.hadoop.yarn.proto.ResourceTracker$ResourceTrackerService$BlockingInterface, method=nodeHeartbeat
2011-09-19 14:01:05,242 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:nodeHeartbeat(201)) - Node not found rebooting 10.18.52.124:45454
Call: protocol=org.apache.hadoop.yarn.proto.ResourceTracker$ResourceTrackerService$BlockingInterface, method=nodeHeartbeat
2011-09-19 14:01:06,244 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:nodeHeartbeat(201)) - Node not found rebooting 10.18.52.124:45454
Call: protocol=org.apache.hadoop.yarn.proto.ResourceTracker$ResourceTrackerService$BlockingInterface, method=nodeHeartbeat
2011-09-19 14:01:07,246 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:nodeHeartbeat(201)) - Node not found rebooting 10.18.52.124:45454
Call: protocol=org.apache.hadoop.yarn.proto.ResourceTracker$ResourceTrackerService$BlockingInterface, method=nodeHeartbeat
2011-09-19 14:01:08,247 INFO  resourcemanager.ResourceTrackerService (ResourceTrackerService.java:nodeHeartbeat(201)) - Node not found rebooting 10.18.52.124:45454
{code}

Node Manager is registered with Resource manager and the for every heartbeat, it is printing the above message.",devaraj,devaraj,Blocker,Closed,Fixed,19/Sep/11 10:15,16/Mar/15 17:57
Bug,MAPREDUCE-3031,12523552,Job Client goes into infinite loop when we kill AM,"Started a cluster. Submitted a sleep job with around 10000 maps and 1000 reduces.
Killed AM with kill -9 by which time already 7000 thousands maps got completed.

On the RM webUI, Application is stuck in Application.RUNNING state. And JobClient goes into an infinite loop as RM keeps telling the client that the application is running.",sseth,karams,Blocker,Closed,Fixed,19/Sep/11 13:13,15/Nov/11 00:50
Bug,MAPREDUCE-3032,12523553,JobHistory doesn't have error information from failed tasks,,devaraj,vinodkv,Blocker,Closed,Fixed,19/Sep/11 13:15,11/Apr/14 16:28
Bug,MAPREDUCE-3033,12523555,JobClient requires mapreduce.jobtracker.address config even when mapreduce.framework.name is set to yarn,"If mapreduce.jobtracker.address is not set in mapred-site.xml and mapreduce.framework.name is set yarn, job submission fails :

Tried to submit sleep job with maps 1 task. Job submission failed with following exception -:
{code}
11/09/19 13:19:20 INFO ipc.YarnRPC: Creating YarnRPC for org.apache.hadoop.yarn.ipc.HadoopYarnProtoRPC
11/09/19 13:19:20 INFO mapred.ResourceMgrDelegate: Connecting to ResourceManager at <RMHost>:8040
11/09/19 13:19:20 INFO ipc.HadoopYarnRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.yarn.api.ClientRMProtocol
11/09/19 13:19:20 INFO mapred.ResourceMgrDelegate: Connected to ResourceManager at <RMHost>:8040
11/09/19 13:19:21 INFO mapred.ResourceMgrDelegate: DEBUG --- getStagingAreaDir: dir=/user/<username>/.staging
11/09/19 13:19:21 INFO mapreduce.JobSubmitter: Cleaning up the staging area /user/<username>/.staging/job_1316435926198_0004
java.lang.RuntimeException: Not a host:port pair: local
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)
	at org.apache.hadoop.mapred.Master.getMasterAddress(Master.java:42)
	at org.apache.hadoop.mapred.Master.getMasterPrincipal(Master.java:47)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:104)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:90)
	at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:83)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:346)
	at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1072)
	at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1069)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1069)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1089)
	at org.apache.hadoop.mapreduce.SleepJob.run(SleepJob.java:262)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
	at org.apache.hadoop.mapreduce.SleepJob.main(SleepJob.java:194)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
	at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:111)
	at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:118)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:189)
{code}",hitesh,karams,Blocker,Closed,Fixed,19/Sep/11 13:25,15/Nov/11 00:49
Bug,MAPREDUCE-3034,12523557,NM should act on a REBOOT command from RM,"RM sends a reboot command to NM in some cases, like when it gets lost and rejoins back. In such a case, NM should act on the command and reboot/reinitalize itself.

This is akin to TT reinitialize on order from JT. We will need to shutdown all the services properly and reinitialize - this should automatically take care of killing of containers, cleaning up local temporary files etc.",devaraj,vinodkv,Critical,Resolved,Fixed,19/Sep/11 13:57,06/Mar/12 13:30
Bug,MAPREDUCE-3035,12523570,MR V2 jobhistory does not contain rack information,"When topology.node.switch.mapping.impl is set to enable rack-locality resolution via the topology script, from the RM web-UI, we can see the rack information for each node. Running a job also reveals the information about rack-local map tasks launched at end of job completion on the client side.

But the hostname field for attempts in the JobHistory does not contain this rack information.

In case of hadoop-0.20 securiy or MRV1, hostname field of job history does contain rackid/hostname whereas in MRV2, hostname field only contains the hostIP. Thus this is a regression.",chaku88,karams,Critical,Closed,Fixed,19/Sep/11 15:21,15/Nov/11 00:49
Bug,MAPREDUCE-3036,12523572,Some of the Resource Manager memory metrics go negative.,"ReservedGB seems to always be decremented when a container is released, even though the container never reserved any memory.
AvailableGB also seems to be able to go negative in a few situations.",revans2,revans2,Blocker,Closed,Fixed,19/Sep/11 16:30,16/Mar/15 17:57
Bug,MAPREDUCE-3038,12523602,job history server not starting because conf() missing HsController,"Exception starting history server.


Sep 19, 2011 6:51:53 PM com.google.inject.MessageProcessor visit
INFO: An exception was caught and reported. Message: org.apache.hadoop.yarn.webapp.WebAppException: conf() not found in class org.apache.hadoop.mapreduce.v2.hs.webapp.HsController                                                                                 org.apache.hadoop.yarn.webapp.WebAppException: conf() not found in class org.apache.hadoop.mapreduce.v2.hs.webapp.HsController
    at org.apache.hadoop.yarn.webapp.Router.addController(Router.java:107)
    at org.apache.hadoop.yarn.webapp.Router.add(Router.java:83)
    at org.apache.hadoop.yarn.webapp.WebApp.route(WebApp.java:140)
    at org.apache.hadoop.yarn.webapp.WebApp.route(WebApp.java:146)
    at org.apache.hadoop.mapreduce.v2.hs.webapp.HsWebApp.setup(HsWebApp.java:42)
    at org.apache.hadoop.yarn.webapp.WebApp.configureServlets(WebApp.java:121)
    at com.google.inject.servlet.ServletModule.configure(ServletModule.java:45)
    at com.google.inject.AbstractModule.configure(AbstractModule.java:59)
    at com.google.inject.spi.Elements$RecordingBinder.install(Elements.java:223)
    at com.google.inject.spi.Elements.getElements(Elements.java:101)
    at com.google.inject.InjectorShell$Builder.build(InjectorShell.java:135)
    at com.google.inject.InjectorBuilder.build(InjectorBuilder.java:102)
    at com.google.inject.Guice.createInjector(Guice.java:92)
    at com.google.inject.Guice.createInjector(Guice.java:69)
    at com.google.inject.Guice.createInjector(Guice.java:59)
    at org.apache.hadoop.yarn.webapp.WebApps$Builder.start(WebApps.java:166)
    at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService.initializeWebApp(HistoryClientService.java:138)
    at org.apache.hadoop.mapreduce.v2.hs.HistoryClientService.start(HistoryClientService.java:109)
    at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)
    at org.apache.hadoop.mapreduce.v2.hs.JobHistoryServer.main(JobHistoryServer.java:83)",naisbitt,tgraves,Blocker,Closed,Fixed,19/Sep/11 18:59,16/Mar/15 17:57
Bug,MAPREDUCE-3039,12523605,Make mapreduce use same version of avro as HBase,"HBase depends on avro 1.5.3 whereas hadoop-common depends on 1.3.2.
When building HBase on top of hadoop, this should be consistent.
Moreover, this should be consistent between common, hdfs, and mapreduce.

Contribs seem to have declared a dependency on avro but are not in fact depending on it.",jrottinghuis,jrottinghuis,Major,Closed,Fixed,19/Sep/11 19:29,12/Dec/11 06:19
Bug,MAPREDUCE-3040,12523608,"TestMRJobs, TestMRJobsWithHistoryService, TestMROldApiJobs fail","Running org.apache.hadoop.mapreduce.v2.TestMRJobs
Tests run: 4, Failures: 0, Errors: 4, Skipped: 0, Time elapsed: 6.229 sec <<< FAILURE!
Running org.apache.hadoop.mapreduce.v2.TestMRJobsWithHistoryService
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.887 sec <<< FAILURE!
Running org.apache.hadoop.mapreduce.v2.TestMROldApiJobs
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 6.067 sec <<< FAILURE!

All of them have the exception:


java.lang.NullPointerException
        at org.apache.hadoop.mapreduce.v2.util.MRApps.parseDistributedCacheArtifacts(MRApps.java:300)
        at org.apache.hadoop.mapreduce.v2.util.MRApps.setupDistributedCache(MRApps.java:277)
        at org.apache.hadoop.mapred.YARNRunner.createApplicationSubmissionContext(YARNRunner.java:349)
        at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:227)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:376)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1161)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1158)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1158)
        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1178)
        at org.apache.hadoop.mapreduce.v2.TestMRJobs.testSleepJob(TestMRJobs.java:147)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.runners.BlockJUnit4ClassRunner.runNotIgnored(BlockJUnit4ClassRunner.java:79)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:71)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:49)
        at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
        at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
        at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)",acmurthy,tgraves,Major,Closed,Fixed,19/Sep/11 20:06,15/Nov/11 00:48
Bug,MAPREDUCE-3041,12523612,Enhance YARN Client-RM protocol to provide access to information such as cluster's Min/Max Resource capabilities similar to that of AM-RM protocol,"To request a container to launch an application master, the client needs to know the min/max resource capabilities so as to be able to make a proper resource request when submitting a new application.
",hitesh,hitesh,Blocker,Closed,Fixed,19/Sep/11 20:17,15/Nov/11 00:48
Bug,MAPREDUCE-3042,12523622,YARN RM fails to start,"When I build and run YARN's RM, I get an invalid host:port exception.

Looks like there's a typo in the ResourceTrackerService.",criccomini,criccomini,Major,Closed,Fixed,19/Sep/11 21:17,15/Nov/11 00:49
Bug,MAPREDUCE-3043,12523631,Missing containers info on the nodes page,The containers info on the nodes page on the RM seems to be missing. This was useful in understanding the usage on each of the nodemanagers.,subrotosanyal,rramya,Major,Resolved,Fixed,19/Sep/11 22:29,14/Oct/19 15:37
Bug,MAPREDUCE-3044,12523638,Pipes jobs stuck without making progress,A simple example pipes job gets stuck without making any progress. The AM is launched but the maps do not make any progress.,mahadev,rramya,Blocker,Closed,Fixed,19/Sep/11 23:34,15/Nov/11 00:49
Bug,MAPREDUCE-3045,12523640,Elapsed time filter on jobhistory server displays incorrect table entries,"The elapsed time filter on the jobhistory server filters incorrect information. 
For e.g. on a cluster where the elapsed time of all the tasks is either 7 or 8sec, the filter displays non null table entries for 1sec or 3sec",jeagles,rramya,Minor,Closed,Fixed,19/Sep/11 23:42,05/Mar/12 02:49
Bug,MAPREDUCE-3048,12523665,"Fix test-patch to run tests via ""mvn clean install test""","Some tests like the ones failing at MAPREDUCE-3040 depend on the generated jars. TestMRJobs for e.g. won't run if we simply run ""mvn clean test"".

I propose that we change test-patch to run tests using ""mvn clean install test"".",vinodkv,vinodkv,Major,Closed,Fixed,20/Sep/11 07:47,15/Nov/11 00:49
Bug,MAPREDUCE-3050,12523752,YarnScheduler needs to expose Resource Usage Information,"Before the recent refactor The nodes had information in them about how much resources they were using.  This information is not hidden inside SchedulerNode.  Similarly resource usage information about an application, or in aggregate is only available through the Scheduler and there is not interface to pull it out.

We need to expose APIs to get Resource and Container information from the scheduler, in aggregate across the entire cluster, per application, per node, and ideally also per queue if applicable (although there are no JIRAs I am aware of that need this right now).",revans2,revans2,Blocker,Closed,Fixed,20/Sep/11 17:05,10/Mar/15 04:31
Bug,MAPREDUCE-3053,12523791,YARN Protobuf RPC Failures in RM,"When I try to register my ApplicationMaster with YARN's RM, it fails.

In my ApplicationMaster's logs:

Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
	at org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:108)
	at kafka.yarn.util.ApplicationMasterHelper.registerWithResourceManager(YarnHelper.scala:48)
	at kafka.yarn.ApplicationMaster$.main(ApplicationMaster.scala:32)
	at kafka.yarn.ApplicationMaster.main(ApplicationMaster.scala)
Caused by: com.google.protobuf.ServiceException: java.lang.NullPointerException: java.lang.NullPointerException
	at org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)
	at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)

	at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:130)
	at $Proxy6.registerApplicationMaster(Unknown Source)
	at org.apache.hadoop.yarn.api.impl.pb.client.AMRMProtocolPBClientImpl.registerApplicationMaster(AMRMProtocolPBClientImpl.java:101)
	... 3 more
Caused by: java.lang.NullPointerException: java.lang.NullPointerException
	at org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)
	at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)

	at org.apache.hadoop.ipc.Client.call(Client.java:1084)
	at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:127)
	... 5 more


In the ResourceManager's logs:

2011-09-20 15:11:20,973 INFO  ipc.Server (Server.java:run(1497)) - IPC Server handler 2 on 8040, call: org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$ProtoSpecificRequestWritable@455dd32a from 127.0.0.1:33793, error: 
java.lang.NullPointerException
	at org.apache.hadoop.yarn.proto.ClientRMProtocol$ClientRMProtocolService$2.getRequestPrototype(ClientRMProtocol.java:186)
	at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Server.call(ProtoOverHadoopRpcEngine.java:323)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)

My registration code:

    val appId = args(0).toInt
    val attemptId = args(1).toInt
    val timestamp = args(2).toLong

    // these are our application master's parameters
    val streamerClass = args(3)
    val tasks = args(4).toInt

    // TODO log params here

    // start the application master helper
    val conf = new Configuration
    val applicationMasterHelper = new ApplicationMasterHelper(appId, attemptId, timestamp, conf)
      .registerWithResourceManager

  .....

  val rpc = YarnRPC.create(conf)
  val appId = Records.newRecord(classOf[ApplicationId])
  val appAttemptId = Records.newRecord(classOf[ApplicationAttemptId])
  val rmAddress = NetUtils.createSocketAddr(conf.get(YarnConfiguration.RM_ADDRESS, YarnConfiguration.DEFAULT_RM_ADDRESS))
  val resourceManager = rpc.getProxy(classOf[AMRMProtocol], rmAddress, conf).asInstanceOf[AMRMProtocol]
  var requestId = 0

  appId.setClusterTimestamp(lTimestamp)
  appId.setId(iAppId)
  appAttemptId.setApplicationId(appId)
  appAttemptId.setAttemptId(iAppAttemptId)

  def registerWithResourceManager(): ApplicationMasterHelper = {
    val req = Records.newRecord(classOf[RegisterApplicationMasterRequest])
    req.setApplicationAttemptId(appAttemptId)
    // TODO not sure why these are blank- This is how spark does it
    req.setHost("""")
    req.setRpcPort(1)
    req.setTrackingUrl("""")
    resourceManager.registerApplicationMaster(req)
    this
  }

My params are receiving the proper app/attempt/cluster timestamps:

app - 1
attempt - 1
timestamp - 1316556657998
",vinodkv,criccomini,Major,Closed,Fixed,20/Sep/11 22:32,15/Nov/11 00:49
Bug,MAPREDUCE-3054,12523798,Unable to kill submitted jobs,"Found by Philip Su

The ""mapred job -kill"" command
appears to succeed, but listing the jobs again shows that the job supposedly killed is still there. 

{code}
mapred job -list
Total jobs:2
JobId   State   StartTime       UserName        Queue   Priority        SchedulingInfo
job_1316203984216_0002  PREP    1316204924937   hadoopqa        default NORMAL
job_1316203984216_0001  PREP    1316204031206   hadoopqa        default NORMAL

mapred job -kill job_1316203984216_0002
Killed job job_1316203984216_0002

mapred job -list
Total jobs:2
JobId   State   StartTime       UserName        Queue   Priority        SchedulingInfo
job_1316203984216_0002  PREP    1316204924937   hadoopqa        default NORMAL
job_1316203984216_0001  PREP    1316204031206   hadoopqa        default NORMAL
{code}",mahadev,sseth,Blocker,Closed,Fixed,21/Sep/11 00:09,12/Oct/13 08:20
Bug,MAPREDUCE-3055,12523803,"Simplify parameter passing to Application Master from Client. SImplify approach to pass info such  appId, ClusterTimestamp and failcount required by App Master.","The Application master needs the application attempt id to register with the Applications Manager. To create an appAttemptId object, the appId object(needs cluster timestamp and app id) and failCount are needed.

Currently, all clients need to pass in the appId, cluster timestamp and fail count to the app master for the required objects to be constructed. 

We could look at simplifying this by providing either placeholders that would have values replaced by the app master launcher or setting it  into the environment ( although that requires a set of whitelisted env vars that can only be set by the yarn framework ). ",vinodkv,hitesh,Minor,Closed,Fixed,21/Sep/11 00:43,15/Nov/11 00:50
Bug,MAPREDUCE-3056,12523833,Jobs are failing when those are submitted by other users,"MR cluster is started by the user 'root'. If any other users other than 'root' submit a job, it is failing always.

Find the conatiner logs in the comments section.",devaraj,devaraj,Blocker,Closed,Fixed,21/Sep/11 09:57,13/May/15 09:54
Bug,MAPREDUCE-3057,12523853,Job History Server goes of OutOfMemory with 1200 Jobs and Heap Size set to 10 GB,"History server was started with -Xmx10000m
Ran GridMix V3 with 1200 Jobs trace in STRESS mode on 350 nodes with each node 4 NMS.
All jobs finished as reported by RM Web UI and HADOOP_MAPRED_HOME/bin/mapred job -list all
But found that GridMix job client was stuck while trying connect to HistoryServer
Then tried to do HADOOP_MAPRED_HOME/bin/mapred job -status jobid
JobClient also got stuck while looking for token to connect to History server
Then looked at History Server logs and found History is trowing ""java.lang.OutOfMemoryError: GC overhead limit exceeded"" error.

With 10GB of Heap space and 1200 Jobs, History Server should not go out of memory .
No matter what are the type of jobs.



",epayne,karams,Blocker,Closed,Fixed,21/Sep/11 12:23,15/Nov/11 00:48
Bug,MAPREDUCE-3058,12523856,Sometimes task keeps on running while its Syslog says that it is shutdown,"While running GridMixV3, one of the jobs got stuck for 15 hrs. After clicking on the Job-page, found one of its reduces to be stuck. Looking at syslog of the stuck reducer, found this:
Task-logs' head:

{code}
2011-09-19 17:57:22,002 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Scheduled snapshot period at 10 second(s).
2011-09-19 17:57:22,002 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system started
{code}

Task-logs' tail:
{code}
2011-09-19 18:06:49,818 INFO org.apache.hadoop.hdfs.DFSClient: Exception in createBlockOutputStream java.io.IOException: Bad connect ack with firstBadLink as <DATANODE1>
2011-09-19 18:06:49,818 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block BP-1405370709-<NAMENODE>-1316452621953:blk_-7004355226367468317_79871 in pipeline  <DATANODE2>,  <DATANODE1>: bad datanode  <DATANODE1>
2011-09-19 18:06:49,818 DEBUG org.apache.hadoop.hdfs.protocol.datatransfer.DataTransferProtocol: lastAckedSeqno = 26870
2011-09-19 18:06:49,820 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <NAMENODE> from gridperf sending #454
2011-09-19 18:06:49,826 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <<NAMENODE> from gridperf got value #454
2011-09-19 18:06:49,827 DEBUG org.apache.hadoop.ipc.RPC: Call: getAdditionalDatanode 8
2011-09-19 18:06:49,827 DEBUG org.apache.hadoop.hdfs.DFSClient: Connecting to datanode <DATANODE2>
2011-09-19 18:06:49,827 DEBUG org.apache.hadoop.hdfs.DFSClient: Send buf size 131071
2011-09-19 18:06:49,833 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception
java.io.EOFException: Premature EOF: no length prefix available
        at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)
2011-09-19 18:06:49,837 WARN org.apache.hadoop.mapred.YarnChild: Exception running child : java.io.EOFException: Premature EOF: no length prefix available
        at org.apache.hadoop.hdfs.protocol.HdfsProtoUtil.vintPrefixed(HdfsProtoUtil.java:158)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.transfer(DFSOutputStream.java:860)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.addDatanode2ExistingPipeline(DFSOutputStream.java:838)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.setupPipelineForAppendOrRecovery(DFSOutputStream.java:929)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.processDatanodeError(DFSOutputStream.java:740)
        at org.apache.hadoop.hdfs.DFSOutputStream$DataStreamer.run(DFSOutputStream.java:415)

2011-09-19 18:06:49,837 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <APPMASTER> from job_1316452677984_0862 sending #455
2011-09-19 18:06:49,839 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <APPMASTER> from job_1316452677984_0862 got value #455
2011-09-19 18:06:49,840 DEBUG org.apache.hadoop.ipc.RPC: Call: statusUpdate 3
2011-09-19 18:06:49,840 INFO org.apache.hadoop.mapred.Task: Runnning cleanup for the task
2011-09-19 18:06:49,840 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <NAMENODE> from gridperf sending #456
2011-09-19 18:06:49,858 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <NAMENODE> from gridperf got value #456
2011-09-19 18:06:49,858 DEBUG org.apache.hadoop.ipc.RPC: Call: delete 18
2011-09-19 18:06:49,858 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <APPMASTER> from job_1316452677984_0862 sending #457
2011-09-19 18:06:49,859 DEBUG org.apache.hadoop.ipc.Client: IPC Client (26613121) connection to <APPMASTER> from job_1316452677984_0862 got value #457
2011-09-19 18:06:49,859 DEBUG org.apache.hadoop.ipc.RPC: Call: reportDiagnosticInfo 1
2011-09-19 18:06:49,859 DEBUG org.apache.hadoop.metrics2.impl.MetricsSystemImpl: refCount=1
2011-09-19 18:06:49,859 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping ReduceTask metrics system...
2011-09-19 18:06:49,859 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping metrics source UgiMetrics
2011-09-19 18:06:49,859 DEBUG org.apache.hadoop.metrics2.impl.MetricsSystemImpl: class org.apache.hadoop.metrics2.lib.MetricsSourceBuilder$1
2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.util.MBeans: Unregistering Hadoop:service=ReduceTask,name=UgiMetrics
2011-09-19 18:06:49,860 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: Stopping metrics source JvmMetrics
2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.impl.MetricsSystemImpl: class org.apache.hadoop.metrics2.source.JvmMetrics
2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.util.MBeans: Unregistering Hadoop:service=ReduceTask,name=JvmMetrics
2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.util.MBeans: Unregistering Hadoop:service=ReduceTask,name=MetricsSystem,sub=Stats
2011-09-19 18:06:49,860 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system stopped.
2011-09-19 18:06:49,860 DEBUG org.apache.hadoop.metrics2.util.MBeans: Unregistering Hadoop:service=ReduceTask,name=MetricsSystem,sub=Control
2011-09-19 18:06:49,860 INFO org.apache.hadoop.metrics2.impl.MetricsSystemImpl: ReduceTask metrics system shutdown complete.
{code}

Which means that tasks is supposed to have stopped within 20 secs, whereas the process itself is stuck for more than 15 hours. From AM log, also found that this task was sending its update regularly. ps -ef | grep java was also showing that process is still alive.
",vinodkv,karams,Critical,Closed,Fixed,21/Sep/11 12:53,15/Nov/11 00:48
Bug,MAPREDUCE-3059,12523864,QueueMetrics do not have metrics for aggregate containers-allocated and aggregate containers-released,"QueueMetrics for ResourceManager do not have any metrics for aggregate containers-allocated and containers-released.

We need the aggregates of containers-allocated and containers-released to figure out the rate at which RM is dishing out containers. NodeManager do have containers-launched and container-released metrics, but this is not across all nodes; so to get the cluster level aggregate, we need to preprocess NM metrics from all nodes - which is troublesome.

Currently, we do have AllocatedContainers and PendingContainers which reflect the running containers given out to AMs, and containers waiting for allocation respectively.",devaraj,karams,Blocker,Closed,Fixed,21/Sep/11 13:28,15/Nov/11 00:50
Bug,MAPREDUCE-3062,12523890,YARN NM/RM fail to start,"2011-09-21 10:21:41,932 FATAL resourcemanager.ResourceManager (ResourceManager.java:main(502)) - Error starting ResourceManager
java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.admin.address
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)
	at org.apache.hadoop.yarn.server.resourcemanager.AdminService.init(AdminService.java:88)
	at org.apache.hadoop.yarn.service.CompositeService.init(CompositeService.java:58)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.init(ResourceManager.java:191)
	at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager.main(ResourceManager.java:497)

Another copy and paste issue. Similar to https://issues.apache.org/jira/browse/MAPREDUCE-3042.",criccomini,criccomini,Major,Closed,Fixed,21/Sep/11 17:31,15/Nov/11 00:49
Bug,MAPREDUCE-3064,12523894,"27 unit test failures with  Invalid ""mapreduce.jobtracker.address"" configuration value for JobTracker: ""local""","unit test failure here: https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Mapreduce-trunk-Commit/946/

	Test Result (27 failures / +27)

    org.apache.hadoop.mapred.TestCollect.testCollect
    org.apache.hadoop.mapred.TestComparators.testDefaultMRComparator
    org.apache.hadoop.mapred.TestComparators.testUserMRComparator
    org.apache.hadoop.mapred.TestComparators.testUserValueGroupingComparator
    org.apache.hadoop.mapred.TestComparators.testAllUserComparators
    org.apache.hadoop.mapred.TestFileOutputFormat.testCustomFile
    org.apache.hadoop.mapred.TestJavaSerialization.testMapReduceJob
    org.apache.hadoop.mapred.TestJavaSerialization.testWriteToSequencefile
    org.apache.hadoop.mapred.TestMapOutputType.testKeyMismatch
    org.apache.hadoop.mapred.TestMapOutputType.testValueMismatch
    org.apache.hadoop.mapred.TestMapOutputType.testNoMismatch
    org.apache.hadoop.mapred.TestMapRed.testMapred
    org.apache.hadoop.mapred.TestMapRed.testNullKeys
    org.apache.hadoop.mapred.TestMapRed.testCompression
    org.apache.hadoop.mapred.TestMapRed.testSmallInput
    org.apache.hadoop.mapred.TestMapRed.testBiggerInput
    org.apache.hadoop.mapreduce.TestMapCollection.testValLastByte
    org.apache.hadoop.mapreduce.TestMapCollection.testLargeRecords
    org.apache.hadoop.mapreduce.TestMapCollection.testSpillPer2B
    org.apache.hadoop.mapreduce.TestMapCollection.testZeroVal
    org.apache.hadoop.mapreduce.TestMapCollection.testSingleRecord
    org.apache.hadoop.mapreduce.TestMapCollection.testLowSpill
    org.apache.hadoop.mapreduce.TestMapCollection.testSplitMetaSpill
    org.apache.hadoop.mapreduce.TestMapCollection.testPostSpillMeta
    org.apache.hadoop.mapreduce.TestMapCollection.testLargeRecConcurrent
    org.apache.hadoop.mapreduce.TestMapCollection.testRandom
    org.apache.hadoop.mapreduce.TestMapCollection.testRandomCompress


All of them have similar stack traces of:

java.io.IOException: Invalid ""mapreduce.jobtracker.address"" configuration value for JobTracker: ""local""
	at org.apache.hadoop.mapred.JobTrackerClientProtocolProvider.create(JobTrackerClientProtocolProvider.java:47)
	at org.apache.hadoop.mapreduce.Cluster.<init>(Cluster.java:74)
	at org.apache.hadoop.mapred.JobClient.init(JobClient.java:459)
	at org.apache.hadoop.mapred.JobClient.<init>(JobClient.java:438)
	at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:777)
	at org.apache.hadoop.mapred.TestCollect.testCollect(TestCollect.java:133)",venug,tgraves,Blocker,Closed,Fixed,21/Sep/11 17:49,15/Nov/11 00:49
Bug,MAPREDUCE-3066,12524218,YARN NM fails to start,"Please check conf.get() calls. Every time I svn up, I get one of these.


2011-09-21 15:36:33,534 INFO  service.AbstractService (AbstractService.java:stop(71)) - Service:org.apache.hadoop.yarn.server.nodemanager.DeletionService is stopped.
2011-09-21 15:36:33,534 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager
org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager
	at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)
Caused by: org.apache.avro.AvroRuntimeException: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)
	at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)
	... 2 more
Caused by: java.lang.RuntimeException: Not a host:port pair: yarn.resourcemanager.resource-tracker.address
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:148)
	at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:132)
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.getRMClient(NodeStatusUpdaterImpl.java:154)
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:164)
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)
	... 3 more
2011-09-21 15:36:33,535 INFO  service.CompositeService (CompositeService.java:stop(97)) - Error stopping org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl
java.lang.IllegalStateException: For this operation, current State must be STARTED instead of INITED
	at org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState(AbstractService.java:101)
	at org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:69)
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.stop(NodeStatusUpdaterImpl.java:149)
	at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:95)
	at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:85)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:158)
	at org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:118)
2011-09-21 15:36:33,535 INFO  nodemanager.NodeManager (StringUtils.java:run(605)) - SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NodeManager at criccomi-ld/127.0.0.1
************************************************************/
2011-09-21 15:36:33,536 INFO  ipc.Server (Server.java:stop(1708)) - Stopping server on 45454
2011-09-21 15:36:33,536 INFO  logaggregation.LogAggregationService (LogAggregationService.java:stop(116)) - org.apache.hadoop.yarn.server.nodemanager.containermanager.logaggregation.LogAggregationService waiting for pending aggregation during exit
2011-09-21 15:36:33,536 INFO  ipc.Server (Server.java:stop(1708)) - Stopping server on 4344
2011-09-21 15:36:33,536 INFO  service.CompositeService (CompositeService.java:run(120)) - Error stopping org.apache.hadoop.yarn.server.nodemanager.NodeManager
java.lang.IllegalStateException: For this operation, current State must be STARTED instead of INITED
	at org.apache.hadoop.yarn.service.AbstractService.ensureCurrentState(AbstractService.java:101)
	at org.apache.hadoop.yarn.service.AbstractService.stop(AbstractService.java:69)
	at org.apache.hadoop.yarn.service.CompositeService.stop(CompositeService.java:87)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.stop(NodeManager.java:158)
	at org.apache.hadoop.yarn.service.CompositeService$CompositeServiceShutdownHook.run(CompositeService.java:118)
",criccomini,criccomini,Major,Closed,Fixed,21/Sep/11 22:42,15/Nov/11 00:48
Bug,MAPREDUCE-3067,12524235,Container exit status not set properly to launched process's exit code on successful completion of process,"When testing the distributed shell sample app master, the container exit status was being returned incorrectly. 

11/09/21 11:32:58 INFO DistributedShell.ApplicationMaster: Got container status for containerID= container_1316629955324_0001_01_000002, state=COMPLETE, exitStatus=-1000, diagnostics=",hitesh,hitesh,Blocker,Closed,Fixed,22/Sep/11 04:57,15/Nov/11 00:48
Bug,MAPREDUCE-3068,12524237,Should set MALLOC_ARENA_MAX for all YARN daemons and AMs/Containers,"This is same as HADOOP-7154 but for yarn. RM, NM, AM and containers should all have this.",criccomini,vinodkv,Blocker,Closed,Fixed,22/Sep/11 05:59,13/Nov/15 18:51
Bug,MAPREDUCE-3070,12524292,NM not able to register with RM after NM restart,"After stopping NM gracefully then starting NM, NM registration fails with RM with Duplicate registration from the node! error.


{noformat} 
2011-09-23 01:50:46,705 FATAL nodemanager.NodeManager (NodeManager.java:main(204)) - Error starting NodeManager
org.apache.hadoop.yarn.YarnException: Failed to Start org.apache.hadoop.yarn.server.nodemanager.NodeManager
	at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:78)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.start(NodeManager.java:153)
	at org.apache.hadoop.yarn.server.nodemanager.NodeManager.main(NodeManager.java:202)
Caused by: org.apache.avro.AvroRuntimeException: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:141)
	at org.apache.hadoop.yarn.service.CompositeService.start(CompositeService.java:68)
	... 2 more
Caused by: org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Duplicate registration from the node!
	at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)
	at $Proxy13.registerNodeManager(Unknown Source)
	at org.apache.hadoop.yarn.server.api.impl.pb.client.ResourceTrackerPBClientImpl.registerNodeManager(ResourceTrackerPBClientImpl.java:59)
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.registerWithRM(NodeStatusUpdaterImpl.java:175)
	at org.apache.hadoop.yarn.server.nodemanager.NodeStatusUpdaterImpl.start(NodeStatusUpdaterImpl.java:137)
	... 3 more
{noformat} 
",devaraj,raviteja,Blocker,Closed,Fixed,22/Sep/11 14:33,25/Jun/19 06:00
Bug,MAPREDUCE-3071,12524297,app master configuration web UI link under the Job menu opens up application menu,"If you go to the app master web UI for a particular job. The job menu on the left side displays links for overview, counters, configuration, etc..

If you click on the configuration one, it closes the job menu and opens the application menu on that left side. It shouldn't do this. It should leave the job menu open.
",tgraves,tgraves,Major,Closed,Fixed,22/Sep/11 15:17,15/Nov/11 00:48
Bug,MAPREDUCE-3073,12524311,Build failure for MRv1 caused due to changes to MRConstants.,"When runnning ant -Dresolvers=internal binary, the build seems to be failing with:

  [javac] public class JobTracker implements MRConstants,
InterTrackerProtocol,
   [javac]                                    ^
   [javac] 
/home/y/var/builds/thread2/workspace/Cloud-Yarn-0.23-Secondary/hadoop-mapred
uce-project/src/java/org/apache/hadoop/mapred/TaskTracker.java:131:
interface expected here
   [javac]     implements MRConstants, TaskUmbilicalProtocol, Runnable,
TTConfig {
   [javac]                ^
   [javac] 
/home/y/var/builds/thread2/workspace/Cloud-Yarn-0.23-Secondary/hadoop-mapred
uce-project/src/java/org/apache/hadoop/mapred/TaskTracker.java:552: cannot
find symbol
   [javac] symbol  : variable WORKDIR
   [javac] location: class org.apache.hadoop.mapred.MRConstants
   [javac]     return getLocalJobDir(user, jobid) + Path.SEPARATOR +
MRConstants.WORKDIR;
   [javac]        
^
",mahadev,mahadev,Blocker,Closed,Fixed,22/Sep/11 17:50,15/Nov/11 00:49
Bug,MAPREDUCE-3076,12524359,TestSleepJob fails ,"TestSleepJob fails, it was intended to be used in other tests for MAPREDUCE-2981.",acmurthy,acmurthy,Blocker,Closed,Fixed,23/Sep/11 00:47,12/Sep/12 02:27
Bug,MAPREDUCE-3078,12524392,Application's progress isn't updated from AM to RM.,"It helps to be able to monitor the application-progress from the RM UI itself.

Bits of it is already there, even the AM-RM API (in AllocateRequest). We just need to make sure the progress is produced and consumed properly.",vinodkv,vinodkv,Blocker,Closed,Fixed,23/Sep/11 08:47,15/Nov/11 00:48
Bug,MAPREDUCE-3081,12524475,Change the name format for hadoop core and vaidya jar to be hadoop-{core/vaidya}-{version}.jar in vaidya.sh,Vaidya script is broken due to change in the naming convention for hadoop core jar and vaidya jar. ,,vitthal_gogate,Major,Closed,Fixed,23/Sep/11 21:09,19/Oct/11 00:26
Bug,MAPREDUCE-3082,12524492,archive command take wrong path for input file with current directory,"$hadoop dfs -copyFromLocal /etc/passwd .
$hadoop dfs -lsr .
-rw-------   3 hadoopqa hdfs       6883 2011-09-23 22:37 /user/hadoopqa/passwd
$hadoop archive -archiveName test1.har -p .  passwd .
11/09/23 22:39:22 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 4 for hadoopqa
11/09/23 22:39:22 INFO security.TokenCache: Got dt for
hdfs://<NN host>/user/hadoopqa/.staging/job_201109232234_0004;uri=<NN IP>:8020;t.service=<NN IP>:8020
11/09/23 22:39:22 INFO mapred.JobClient: Running job: job_201109232234_0004
11/09/23 22:39:23 INFO mapred.JobClient:  map 0% reduce 0%
11/09/23 22:39:34 INFO mapred.JobClient: Task Id : attempt_201109232234_0004_m_000000_0, Status : FAILED
java.io.FileNotFoundException: File does not exist: hdfs://<NN host>/user/hadoopqa/hadoopqa/passwd
        at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:525)
        at org.apache.hadoop.tools.HadoopArchives$HArchivesMapper.map(HadoopArchives.java:697)
        at org.apache.hadoop.tools.HadoopArchives$HArchivesMapper.map(HadoopArchives.java:587)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:50)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:261)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
        at org.apache.hadoop.mapred.Child.main(Child.java:255)

So Archiving is failing as it was finding input file at /user/hadoopqa/hadoopqa/passwd , whereas it should look for /user/hadoopqa/passwd",johnvijoe,rajsaha,Major,Closed,Fixed,24/Sep/11 00:01,07/Sep/12 21:03
Bug,MAPREDUCE-3087,12524569,CLASSPATH not the same after MAPREDUCE-2880,"After MAPREDUCE-2880, my classpath was missing key jar files. ",raviprak,raviprak,Critical,Closed,Fixed,25/Sep/11 17:00,20/Feb/12 09:55
Bug,MAPREDUCE-3088,12524587,Clover 2.4.3 breaks build for 0.22 branch,Due to known bug in Clover 2.4.3 build for 0.22 branch is broken.,shv,shv,Major,Closed,Fixed,26/Sep/11 00:16,12/Dec/11 06:19
Bug,MAPREDUCE-3092,12524640,Remove JOB_ID_COMPARATOR usage in JobHistory.java,"As part of the defect MAPREDUCE-2965, JobId.compareTo() has been implemented. Usage of JOB_ID_COMPARATOR in JobHistory.java can be removed because comparison is handling by JobId itself. 
 

",devaraj,devaraj,Minor,Closed,Fixed,26/Sep/11 14:53,16/Mar/15 17:57
Bug,MAPREDUCE-3095,12524730,fairscheduler ivy including wrong version for hdfs,fairscheduler ivy.xml includes the common version for hdfs dependency. This could break builds that have different common and hdfs version numbers. The reason we dont see it on the jenkins build is because we use the same version number for common and hdfs.,johnvijoe,johnvijoe,Major,Closed,Fixed,26/Sep/11 20:24,15/Nov/11 00:48
Bug,MAPREDUCE-3101,12524796,[Umbrella] Security issues in YARN,Most of the chassis for security in YARN is set up and is working. There are known bugs and security holes though. This JIRA is an umbrella ticket for tracking those.,,vinodkv,Major,Resolved,Fixed,27/Sep/11 05:15,08/May/15 18:17
Bug,MAPREDUCE-3110,12524856,TestRPC.testUnknownCall() is failing,"{code:xml}
Failed tests: 
  testUnknownCall(org.apache.hadoop.yarn.TestRPC): null expected:<...icationId called on []org.apache.hadoop.ya...> but was:<...icationId called on [interface ]org.apache.hadoop.ya...>

Tests run: 65, Failures: 1, Errors: 0, Skipped: 0

{code}",vinodkv,devaraj,Major,Closed,Fixed,27/Sep/11 13:15,15/Nov/11 00:50
Bug,MAPREDUCE-3112,12525012,Calling hadoop cli inside mapreduce job leads to errors,"When running a streaming job with mapper

bin/hadoop --config /etc/hadoop/ jar contrib/streaming/hadoop-streaming-0.20.205.0.jar -mapper ""hadoop --config /etc/hadoop/ dfs -help"" -reducer NONE -input ""/tmp/input.txt"" -output NONE

Task log shows:

{noformat}
Exception in thread ""main"" java.lang.ExceptionInInitializerError
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:57)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.fs.FsShell.main(FsShell.java:1895)
Caused by: org.apache.commons.logging.LogConfigurationException: User-specified log class 'org.apache.commons.logging.impl.Log4JLogger' cannot be found or is not useable.
	at org.apache.commons.logging.impl.LogFactoryImpl.discoverLogImplementation(LogFactoryImpl.java:874)
	at org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:604)
	at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:336)
	at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:310)
	at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:685)
	at org.apache.hadoop.conf.Configuration.<clinit>(Configuration.java:142)
	... 3 more
java.lang.RuntimeException: PipeMapRed.waitOutputThreads(): subprocess failed with code 1
	at org.apache.hadoop.streaming.PipeMapRed.waitOutputThreads(PipeMapRed.java:311)
	at org.apache.hadoop.streaming.PipeMapRed.mapRedFinished(PipeMapRed.java:545)
	at org.apache.hadoop.streaming.PipeMapper.close(PipeMapper.java:132)
	at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
	at org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:36)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:261)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.Child.main(Child.java:255)
{noformat}

Upon inspection, there are two problems in the inherited from environment which prevent the logger initialization to work properly.  In hadoop-env.sh, the HADOOP_OPTS is inherited from the parent process.  This configuration was requested by user to have a way to override HADOOP environment in the configuration template:

{noformat}
export HADOOP_OPTS=""-Djava.net.preferIPv4Stack=true $HADOOP_OPTS""
{noformat}

-Dhadoop.log.dir=$HADOOP_LOG_DIR/task_tracker_user is injected into HADOOP_OPTS in the tasktracker environment.  Hence, the running task would inherit the wrong logging directory, which the end user might not have sufficient access to write.  Second, $HADOOP_ROOT_LOGGER is override to: -Dhadoop.root.logger=INFO,TLA by the task controller, therefore, the bin/hadoop script will attempt to use hadoop.root.logger=INFO,TLA, but fail to initialize.  ",eyang,eyang,Major,Closed,Fixed,28/Sep/11 05:42,19/Oct/11 00:26
Bug,MAPREDUCE-3114,12525054,Invalid ApplicationMaster URL in Applications Page,"When the Application is in Accepted state and user tries to click the ApplicationMaster URL in Applications Page, it ends up in Invalid HTTP URL. 
The screenshot attached with this Issue makes it more clear.

The HTTP url formed is: http://n/A",subrotosanyal,subrotosanyal,Major,Closed,Fixed,28/Sep/11 13:25,15/Nov/11 00:48
Bug,MAPREDUCE-3120,12525171,"JobHistory is not providing correct count failed,killed task","Please refer the attachment JobFail.PNG.
Here the Job (WordCount) Failed as all Map Attempts were killed(intensionally) but, still the Table in UI shows 0 Killed Attempts and no reason for Failure is also available.",subrotosanyal,subrotosanyal,Major,Resolved,Fixed,29/Sep/11 11:17,16/Mar/15 20:08
Bug,MAPREDUCE-3121,12525190,DFIP aka 'NodeManager should handle Disk-Failures In Place',"This is akin to MAPREDUCE-2413 but for YARN's NodeManager. We want to minimize the impact of transient/permanent disk failures on containers. With larger number of disks per node, the ability to continue to run containers on other disks is crucial.",ravidotg,vinodkv,Blocker,Closed,Fixed,29/Sep/11 14:34,05/Mar/12 02:49
Bug,MAPREDUCE-3123,12525234,Symbolic links with special chars causing container/task.sh to fail,"the following job throws an exception when you have the special characters in it.

hadoop jar hadoop-streaming.jar -Dmapreduce.job.acl-view-job=* -Dmapreduce.job.queuename=queue1 -files file:///homes/user/hadoop/Streaming/data/streaming-980//InputDir#testlink!@$&*()-_+= -input Streaming/streaming-980/input.txt  -mapper 'xargs cat' -reducer cat -output Streaming/streaming-980/Output -jobconf mapred.job.name=streamingTest-980 -jobconf mapreduce.job.acl-view-job=*

Exception:
2011-09-27 20:58:48,903 INFO org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: launchContainer:
[container-executor, hadoopuser, 1, application_1317077272567_0239,
container_1317077272567 0239_01_000001,
tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001,
tmp/mapred-local/nmPrivate/application_1317077272567_0239/container_1317077272567_0239_01 000001/task.sh,
tmp/mapred-local/nmPrivate/container_1317077272567_0239_01_000001/container_1317077272567_0239_01_000001.tokens]1109221111-tests.jar:hadoop-mapreduce-p2011-09-27
20:58:48,944 WARN org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Exit code from container is : 2    
                                                                                                           2011-09-27
20:58:48,946 WARN org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor: Exception from container-launch :  
                                                                                                          
org.apache.hadoop.util.Shell$ExitCodeException:
/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh:
line 26: syntax error near unexpected token `-_+='       
/tmp/mapred-local/usercache/hadoopuser/appcache/application_1317077272567_0239/container_1317077272567_0239_01_000001/task.sh:
line 26: `ln -sf /tmp/mapred-local/usercache/hadoopqa/filecache/-1888139433818483070/InputDir test
ink!@$&*()-_+='kson-jaxrs-1.7.1.jar:/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.7.3/jackson-mapper-asl-1.7.3.jar:.m2/repository/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar:

     at org.apache.hadoop.util.Shell.runCommand(Shell.java:261)                                                        
                                                                                                                       
   at org.apache.hadoop.util.Shell.run(Shell.java:188)                                                                 
                                                                                                                       
 at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:381)                                          
                                                                                                                      
at org.apache.hadoop.yarn.server.nodemanager.LinuxContainerExecutor.launchContainer(LinuxContainerExecutor.java:174)   
                                                                                                                     
at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:197)  
                                                                                                                     
at org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch.call(ContainerLaunch.java:62)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
2011-09-27 20:58:48,951 INFO org.apache.hadoop.yarn.server.nodemanager.ContainerExecutor:
2011-09-27 20:58:48,951 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.container.Container: Processing
container_1317077272567_0239_01_000001 of type UPDATE_DIAGNOSTICS_MSG
2011-09-27 20:58:48,951 WARN org.apache.hadoop.yarn.server.nodemanager.containermanager.launcher.ContainerLaunch:
Container exited with a non-zero exit code 2


",hitesh,tgraves,Blocker,Closed,Fixed,29/Sep/11 19:03,15/Nov/11 00:49
Bug,MAPREDUCE-3124,12525245,mapper failed with failed to load native libs,"hadoop jar hadoop-mapreduce-examples-*.jar sort -Dmapreduce.job.acl-view
-job=* -Dmapreduce.map.output.compress=true 
-Dmapreduce.map.output.compress.codec=org.apache.hadoop.io.compress.GzipCodec 
-Dmapreduce.output.fileoutputformat.compress=true  -Dmapreduce.output.fileoutputformat.compression.type=NONE -Dmap
reduce.output.fileoutputformat.compression.codec=org.apache.hadoop.io.compress.GzipCodec  -outKey
org.apache.hadoop.io.Text -outValue org.apache.hadoop.io.Text  Compression/textinput Compression/textoutput-1317315994

This will fail with native libs not found error unless -Dmapred.child.java.opts='-Djava.library.path=${HADOOP_COMMON_HOME}/lib/native/Linux-i386-32' is added.


The error in container log:


2011-09-29 17:06:56,787 DEBUG org.apache.hadoop.util.NativeCodeLoader: Trying to load the custom-built native-hadoop
library...2011-09-29 17:06:56,787 DEBUG org.apache.hadoop.util.NativeCodeLoader: Failed to load native-hadoop with
error: java.lang.UnsatisfiedLinkError: no hadoop in java.library.path2011-09-29 17:06:56,787 DEBUG
org.apache.hadoop.util.NativeCodeLoader:
java.library.path=/share/gridjdk-1.6.0_21/jre/lib/i386/server:/share/gridjdk-1.6.0_21/jre/lib/i386:/share/gridjdk-1.6.0_21/jre/../lib/i386:/tmp/mapred-local/usercache/hadoopqa/appcache/application_1317314754104_0012/container_1317314754104_0012_01_000002:/current/lib:/usr/java/packages/lib/i386:/lib:/usr/lib2011-09-29
17:06:56,787 WARN org.apache.hadoop.util.NativeCodeLoader: Unable to load native-hadoop library for your platform...
using builtin-java classes where applicable


Also note that the error that shows up at the application master for this is terrible:

Container killed by the ApplicationMaster. Container killed on request. Exit code is 137 Too Many fetch failures.Failing the attempt 
",johnvijoe,tgraves,Blocker,Closed,Fixed,29/Sep/11 22:13,10/Mar/15 04:31
Bug,MAPREDUCE-3125,12525248,app master web UI shows reduce task progress 100% even though reducers not complete and state running/scheduled,ran same command as MAPREDUCE-3124. The app master web ui was displaying the reduce task progress as 100% even though the states were still running/scheduled.  Each of those reduce tasks had attempts that failed or killed and another one unassigned. Attaching screenshots.,hitesh,tgraves,Critical,Closed,Fixed,29/Sep/11 22:29,15/Nov/11 00:50
Bug,MAPREDUCE-3126,12525252,mr job stuck because reducers using all slots and mapper isn't scheduled,"The command in MAPREDUCE-3124 run and this job got hung with 1 Map task waiting for resources and 7 Reducers running (2 waiting).  The mapper got scheduler, then AM scheduled the reducers, the map task failed and tried to start a new attempt but reducers were using all the slots.   

I will try to add some more info from the logs.",acmurthy,tgraves,Blocker,Closed,Fixed,29/Sep/11 22:52,15/Nov/11 00:48
Bug,MAPREDUCE-3138,12525600,Allow for applications to deal with MAPREDUCE-954,MAPREDUCE-954 changed the context-objs api to interfaces. This breaks Pig. We need a bridge for them to move to 0.23.,omalley,acmurthy,Blocker,Closed,Fixed,03/Oct/11 23:29,15/Nov/11 00:49
Bug,MAPREDUCE-3139,12525614,SlivePartitioner generates negative partitions,"{{SlivePartitioner.getPartition()}} returns negative partition numbers on some occasions, which is illegal.",jghoman,shv,Major,Closed,Fixed,04/Oct/11 01:41,08/Feb/12 02:49
Bug,MAPREDUCE-3140,12525652,Invalid JobHistory URL for failed applications,"After completion of the applications execution (application has failed though), to verify the job history, I clicked on the JobHistory hyper-link displayed as part of the application details.In this case, it is displaying [http://n/A].",subrotosanyal,kam_iitkgp,Major,Closed,Fixed,04/Oct/11 12:01,15/Nov/11 00:49
Bug,MAPREDUCE-3143,12525806,Complete aggregation of user-logs spit out by containers onto DFS,"Already implemented the feature for handling user-logs spit out by containers in NodeManager. But the feature is currently disabled due to user-interface issues.

This is the umbrella ticket for tracking the pending bugs w.r.t putting container-logs on DFS.",,vinodkv,Major,Closed,Fixed,05/Oct/11 12:30,01/Sep/13 23:28
Bug,MAPREDUCE-3149,12526091,add a test to verify that buildDTAuthority works for cases with no authority.,Add a test to verify that buildDTAuthority works for cases with no Authority.,johnvijoe,johnvijoe,Major,Resolved,Fixed,06/Oct/11 18:24,13/May/16 05:14
Bug,MAPREDUCE-3151,12526145,Contrib tests failing,"Jenkins builds fail:
https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Mapreduce-22-branch/80/console
",jrottinghuis,jrottinghuis,Major,Closed,Fixed,07/Oct/11 03:38,12/Dec/11 06:19
Bug,MAPREDUCE-3153,12526195,TestFileOutputCommitter.testFailAbort() is failing on trunk on Jenkins,This mostly is caused by MAPREDUCE-2702.,mahadev,vinodkv,Major,Closed,Fixed,07/Oct/11 12:45,15/Nov/11 00:49
Bug,MAPREDUCE-3157,12526422,Rumen TraceBuilder is skipping analyzing 0.20 history files,Rumen TraceBuilder is assuming the Pre21 history file name format to be JTIdentifier_jobId_<something>. But it can be jobId_<something> also as it is now in latest 0.20.x version. This also needs to be understood by TraceBuilder.,ravidotg,ravidotg,Major,Closed,Fixed,10/Oct/11 10:12,15/Nov/11 00:50
Bug,MAPREDUCE-3158,12526488,Fix trunk build failures,"https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Mapreduce-trunk-Commit/1060/

",hitesh,hitesh,Major,Closed,Fixed,10/Oct/11 18:32,15/Nov/11 00:48
Bug,MAPREDUCE-3159,12526505,DefaultContainerExecutor removes appcache dir on every localization,The DefaultContainerExecutor currently has code that removes the application dir from appcache/ in the local directories on every task localization. This causes any concurrent executing tasks from the same job to fail.,tlipcon,tlipcon,Blocker,Closed,Fixed,10/Oct/11 20:08,15/Nov/11 00:49
Bug,MAPREDUCE-3163,12526554,JobClient spews errors when killing MR2 job,"When I used the ""hadoop job"" command line to kill a running MR2 job, I got a bunch of error spew on the console, despite the kill actually taking effect.",mahadev,tlipcon,Blocker,Closed,Fixed,11/Oct/11 03:35,15/Nov/11 00:48
Bug,MAPREDUCE-3165,12526561,Ensure logging option is set on child command line,Currently the logging config is set in env in MapReduceChildJVM - we need to set it on command line.,tlipcon,acmurthy,Blocker,Closed,Fixed,11/Oct/11 05:45,15/Nov/11 00:48
Bug,MAPREDUCE-3166,12526565,Make Rumen use job history api instead of relying on current history file name format,"Rumen should not depend on the regular expression of job history file name format and should use the newly added api like isValidJobHistoryFileName(), getJobIDFromHistoryFilePath().",ravidotg,ravidotg,Major,Closed,Fixed,11/Oct/11 06:25,15/Nov/11 00:49
Bug,MAPREDUCE-3167,12526566,container-executor is not being packaged with the assembly target.,Looks like MAPREDUCE-2988 broke this. This is a temporary fix until we get a full fledged maven dist tar working. Trivial fix.,mahadev,mahadev,Minor,Closed,Fixed,11/Oct/11 06:33,15/Nov/11 00:50
Bug,MAPREDUCE-3170,12526783,Trunk nightly commit builds are failing.,Looks like the trunk commit builds are failing after MAPREDUCE-3148 and MAPREDUCE-3126  were committed. I suspect its MAPREDUCE-3148.,hitesh,mahadev,Critical,Closed,Fixed,12/Oct/11 00:55,15/Nov/11 00:48
Bug,MAPREDUCE-3173,12526824,MRV2 UI doesn't work properly without internet,"When we try access the MRV2 UI, it is always giving the below message in the UI even if the java script enabled in the browser.
{code:xml}
This page works best with javascript enabled. 
{code}

It is trying to download these below css/js files from internet and finally ending up with the above message. For loading the page also it is taking long time.

{code:title=JQueryUI.java|borderStyle=solid}
  html.
      link(join(""https://ajax.googleapis.com/ajax/libs/jqueryui/1.8.9/themes/"",
                getTheme(), ""/jquery-ui.css"")).
      link(""/static/dt-1.7.5/css/jui-dt.css"").
      script(""https://ajax.googleapis.com/ajax/libs/jquery/1.4.4/jquery.min.js"").
      script(""https://ajax.googleapis.com/ajax/libs/jqueryui/1.8.9/jquery-ui.min.js"").
{code} ",devaraj,devaraj,Critical,Closed,Fixed,12/Oct/11 10:21,12/May/16 18:22
Bug,MAPREDUCE-3176,12526924,ant mapreduce tests are timing out,Secondary YARN builds started taking inordinately long and lots of tests started failing. Usually the secondary build would take ~ 2 hours. But recently even after 7 hours it wasn't done. ,hitesh,raviprak,Blocker,Closed,Fixed,12/Oct/11 20:22,15/Nov/11 00:48
Bug,MAPREDUCE-3179,12527042,Incorrect exit code for hadoop-mapreduce-test tests when exception thrown,"Exit code for test jar is 0 despite exception thrown

hadoop jar hadoop-mapreduce-test-0.23.0-SNAPSHOT.jar loadgen -Dmapreduce.job.acl-view -m 18 -r 0 -outKey org.apache.hadoop.io.Text -outValue org.apache.hadoop.io.Text -indir nonexistentdir

Loadgen output snippet
org.apache.hadoop.mapred.InvalidInputException: Input path does not exist: hdfs://machine.name.example.com:9000/user/exampleuser/nonexistentdir
        at org.apache.hadoop.mapred.FileInputFormat.listStatus(FileInputFormat.java:234)
        at org.apache.hadoop.mapred.FileInputFormat.getSplits(FileInputFormat.java:254)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeOldSplits(JobSubmitter.java:470)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:462)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:358)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1159)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1156)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1156)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:539)
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:778)
        at org.apache.hadoop.mapred.GenericMRLoadGenerator.run(GenericMRLoadGenerator.java:200)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.mapred.GenericMRLoadGenerator.main(GenericMRLoadGenerator.java:214)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
        at org.apache.hadoop.test.MapredTestDriver.run(MapredTestDriver.java:111)
        at org.apache.hadoop.test.MapredTestDriver.main(MapredTestDriver.java:118)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:189)
-bash-3.2$ echo $?
0

This differs from example jar which correctly returns the correct exit code

hadoop jar hadoop-mapreduce-examples-0.23.0-SNAPSHOT.jar wordcount nonexistentdir /outputdir

wordcount output snippet
org.apache.hadoop.mapreduce.lib.input.InvalidInputException: Input path does not exist: hdfs://machine.name.example.com:9000/user/exampleuser/nonexistentdir
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:243)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:269)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeNewSplits(JobSubmitter.java:443)
        at org.apache.hadoop.mapreduce.JobSubmitter.writeSplits(JobSubmitter.java:460)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:358)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1159)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1156)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1156)
        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1176)
        at org.apache.hadoop.examples.WordCount.main(WordCount.java:84)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:189)
-bash-3.2$ echo $?
255

",jeagles,jeagles,Major,Closed,Fixed,13/Oct/11 16:22,10/Mar/15 04:32
Bug,MAPREDUCE-3180,12527099,TaskTracker.java.orig accidentally checked in to 0.20-security-205,"The file src/mapred/org/apache/hadoop/mapred/TaskTracker.java.orig was accidentally checked in as part of r1179465.  It is only in 0.20-security-205, not 0.20-security.  If there is a 0.20.205.1, remove it then.",mattf,mattf,Trivial,Closed,Fixed,13/Oct/11 21:12,28/Nov/11 09:17
Bug,MAPREDUCE-3181,12527101,Terasort fails with Kerberos exception on secure cluster,"We are seeing the following Kerberos exception upon trying to run terasort on secure single and multi-node clusters using the latest build from branch 0.23.

java.io.IOException: Can't get JobTracker Kerberos principal for use as renewer
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:106)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:90)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:83)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.listStatus(FileInputFormat.java:205)
        at org.apache.hadoop.mapreduce.lib.input.FileInputFormat.getSplits(FileInputFormat.java:269)
        at org.apache.hadoop.examples.terasort.TeraInputFormat.getSplits(TeraInputFormat.java:318)
        at org.apache.hadoop.examples.terasort.TeraInputFormat.writePartitionFile(TeraInputFormat.java:169)
        at org.apache.hadoop.examples.terasort.TeraSort.run(TeraSort.java:306)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.examples.terasort.TeraSort.main(TeraSort.java:325)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
        at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
        at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:189)

Adding debug output shows that the job configuration is not loading up yarn-site.xml causing the above failure to happen.
",acmurthy,anupamseth,Blocker,Closed,Fixed,13/Oct/11 21:16,15/Nov/11 00:48
Bug,MAPREDUCE-3183,12527118,hadoop-assemblies/src/main/resources/assemblies/hadoop-mapreduce-dist.xml missing license header,Re-assigning as this is part of the mavenization related changes and requires a delayed merge to the 23 branch. ,hitesh,hitesh,Trivial,Closed,Fixed,13/Oct/11 22:28,16/Mar/15 17:57
Bug,MAPREDUCE-3185,12527133,RM Web UI does not sort the columns in some cases.,"While running lots of jobs on a MRv2 cluster the RM web UI shows this error on loading the RM web UI:

""DataTables warning (table id = 'apps'): Added data (size 8) does not match known number of columns (9)""

After ignoring the error, the column sorting on Web UI stops working.",jeagles,mahadev,Critical,Closed,Fixed,14/Oct/11 01:09,15/Nov/11 00:50
Bug,MAPREDUCE-3186,12527162,User jobs are getting hanged if the Resource manager process goes down and comes up while job is getting executed.,"If the resource manager is restarted while the job execution is in progress, the job is getting hanged.
UI shows the job as running.
In the RM log, it is throwing an error ""ERROR org.apache.hadoop.yarn.server.resourcemanager.ApplicationMasterService: AppAttemptId doesnt exist in cache appattempt_1318579738195_0004_000001""
In the console MRAppMaster and Runjar processes are not getting killed",epayne,ramgopalnaali,Blocker,Closed,Fixed,14/Oct/11 08:59,25/Jul/18 10:35
Bug,MAPREDUCE-3188,12527238,Lots of errors in logs when daemon startup fails,"Since the MR2 daemons are made up of lots of component services, if one of those components fails to start, it will cause the others to shut down as well, even if they haven't fully finished starting up. Currently, this causes the error output to have a bunch of NullPointerExceptions, IllegalStateExceptions, etc, which mask the actual root cause error at the top.",tlipcon,tlipcon,Major,Closed,Fixed,14/Oct/11 17:59,15/Nov/11 00:48
Bug,MAPREDUCE-3191,12527247,docs for map output compression incorrectly reference SequenceFile,"The documentation currently says that map output compression uses SequenceFile compression. This hasn't been true in several years, since we use IFile for intermediate data now.",airbots,tlipcon,Trivial,Closed,Fixed,14/Oct/11 18:30,09/Sep/14 20:26
Bug,MAPREDUCE-3192,12527252,Fix Javadoc warning in JobClient.java and Cluster.java,Javadoc warnings in JobClient.java and Cluster.java need to be fixed.,jnp,jnp,Major,Closed,Fixed,14/Oct/11 18:57,15/Nov/11 00:49
Bug,MAPREDUCE-3193,12527413,FileInputFormat doesn't read files recursively in the input path dir,"java.io.FileNotFoundException is thrown,if input file is more than one folder level deep and the job is getting failed.
Example:Input file is /r1/r2/input.txt
",devaraj,ramgopalnaali,Major,Closed,Fixed,17/Oct/11 10:00,12/May/16 18:24
Bug,MAPREDUCE-3194,12527467,"""mapred mradmin"" command is broken in mrv2","$mapred  mradmin  
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/mapred/tools/MRAdmin
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.tools.MRAdmin
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.hadoop.mapred.tools.MRAdmin.  Program will exit.",jlowe,sseth,Major,Closed,Fixed,17/Oct/11 16:17,05/Mar/12 02:49
Bug,MAPREDUCE-3196,12527511,TestLinuxContainerExecutorWithMocks fails on Mac OSX,TestLinuxContainerExecutorWithMocks uses /bin/true which isn't present. ,acmurthy,acmurthy,Major,Closed,Fixed,17/Oct/11 22:27,15/Nov/11 00:49
Bug,MAPREDUCE-3197,12527512,TestMRClientService failing on building clean checkout of branch 0.23,"A clean checkout of the branch 0.23 source tree does not pass TestMRClientService#test(), which fails with the error message ""Num diagnostics is not correct expected <2> but was:<1> upon running ""mvn clean install assembly:assembly"" inside MR directory.

",mahadev,anupamseth,Major,Closed,Fixed,17/Oct/11 22:28,15/Nov/11 00:49
Bug,MAPREDUCE-3198,12527525,Change mode for hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/test/resources/mock-container-executor to 755 ,The file is checked in with 644 permissions. TestLinuxContainerExecutorWithMocks changes the file mode to add executable permission if needed resulting in a modified file for 'git/svn status' when tests are run. ,acmurthy,hitesh,Trivial,Closed,Fixed,18/Oct/11 00:51,15/Nov/11 00:48
Bug,MAPREDUCE-3199,12527563,TestJobMonitorAndPrint is broken on trunk,I bisected this down to MAPREDUCE-3003 changes. The parent project for client-core changed to hadoop-project which doesn't have the log4j configuration unlike the previous parent hadoop-mapreduce-client.,vinodkv,vinodkv,Major,Closed,Fixed,18/Oct/11 10:31,15/Nov/11 00:50
Bug,MAPREDUCE-3201,12527570,"Even though jobs are getting failed on particular NM, it is not getting blacklisted","{code:xml}
The yarnchild process on a particular NM are getting killed continuosly. 
Still the NM is not getting blacklisted
{code}",,ramgopalnaali,Minor,Resolved,Fixed,18/Oct/11 11:04,20/Mar/15 00:49
Bug,MAPREDUCE-3203,12527668,Fix some javac warnings in MRAppMaster.,MAPREDUCE-2762 accidentally introduced a couple of javac warning. This jira is to fix some of them in MRAppMaster. We have plenty more to fix but I dont intend to fix them all here. This is just so that the hudson bot does not -1 other patches with javac warnings.,mahadev,mahadev,Major,Closed,Fixed,18/Oct/11 22:12,15/Nov/11 00:49
Bug,MAPREDUCE-3204,12527670,mvn site:site fails on MapReduce,This problem does not happen on 0.23. See details in the next comment.,tucu00,sureshms,Major,Closed,Fixed,18/Oct/11 22:36,16/Mar/15 17:57
Bug,MAPREDUCE-3208,12527693,NPE while flushing TaskLogAppender,"NPE will be throwed out while calling flush() of TaskLogAppender,if the QuietWriter isn't initialized in advance.",liangzhwa,liangzhwa,Minor,Closed,Fixed,19/Oct/11 02:55,10/Mar/15 04:32
Bug,MAPREDUCE-3209,12527700,Jenkins reports 160 FindBugs warnings,"See
https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1055//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-common.html
https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1055//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-app.html
https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1055//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-client-core.html",vinodkv,vinodkv,Major,Closed,Fixed,19/Oct/11 04:39,15/Nov/11 00:49
Bug,MAPREDUCE-3212,12527745,Message displays while executing yarn command should be proper,"execute yarn command without any arguments. It displays
{noformat}Usage: hadoop [--config confdir] COMMAND {noformat}.
Rather the message should be
{noformat}Usage: yarn [--config confdir] COMMAND{noformat}
",kamesh,kam_iitkgp,Minor,Closed,Fixed,19/Oct/11 13:36,15/Nov/11 00:49
Bug,MAPREDUCE-3214,12527822,ant mapreduce tests failing,Umbrella jira for various test failures,,hitesh,Major,Resolved,Fixed,19/Oct/11 20:41,09/Mar/15 22:47
Bug,MAPREDUCE-3223,12527850,Remove MR1 configs from mapred-default.xml,"All of the MRv1 configs are still in mapred-default.xml. This is confusing when trying to make config changes. Since a lot of the input/output format tests still depend on MR1, I'd like to move these to src/test/mapred-site.xml for now, and once that dependency is broken, we can remove them entirely.",tlipcon,tlipcon,Major,Resolved,Fixed,19/Oct/11 23:51,12/May/16 18:22
Bug,MAPREDUCE-3226,12527880,Few reduce tasks hanging in a gridmix-run,"In a gridmix run with ~1000 jobs, one job is getting stuck because of 2-3 hanging reducers. All of the them are stuck after downloading all map outputs and have the following thread dump.

{code}
""EventFetcher for fetching Map Completion Events"" daemon prio=10 tid=0xa325fc00 nid=0x1ca4 waiting on condition [0xa315c000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at org.apache.hadoop.mapreduce.task.reduce.EventFetcher.run(EventFetcher.java:71)

""main"" prio=10 tid=0x080ed400 nid=0x1c71 in Object.wait() [0xf73a2000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)
        at java.lang.Thread.join(Thread.java:1143)
        - locked <0xa94b23d8> (a org.apache.hadoop.mapreduce.task.reduce.EventFetcher)
        at java.lang.Thread.join(Thread.java:1196)
        at org.apache.hadoop.mapreduce.task.reduce.Shuffle.run(Shuffle.java:135)
        at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:367)
        at org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:147)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
        at org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:142)
{code}

Thanks to [~karams] for helping track this down.",vinodkv,vinodkv,Blocker,Closed,Fixed,20/Oct/11 06:38,15/Nov/11 00:49
Bug,MAPREDUCE-3228,12527911,MR AM hangs when one node goes bad,"Found this on one of the gridmix runs, again. One of the nodes went real bad, the job had three containers running on the node. Eventually, AM marked the tasks as timedout and initiated cleanup of the failed containers via {{stopContainer()}}. The later got stuck at the faulty node, the tasks are stuck in FAIL_CONTAINER_CLEANUP stage and the job lies in there waiting for ever.

Thanks to [~Karams] for helping with this.",vinodkv,vinodkv,Blocker,Closed,Fixed,20/Oct/11 12:12,15/Nov/11 00:48
Bug,MAPREDUCE-3240,12528282,NM should send a SIGKILL for completed containers also,"This is to address the containers which exit properly after spawning sub-processes themselves. We don't want to leave these sub-process-tree or else they can pillage the NM's resources.

Today, we already have code to send SIGKILL to the whole process-trees (because of single sessionId resulting from  setsid) when the container is alive. We need to obtain the PID of the containers when they start and use that PID to send signal for completed containers' case also.",hitesh,vinodkv,Blocker,Closed,Fixed,21/Oct/11 14:38,15/Nov/11 00:48
Bug,MAPREDUCE-3241,12528296,(Rumen)TraceBuilder throws IllegalArgumentException,"When we run the TraceBuilder, we get this exception. Output of the TraceBuilder doesn't contain the map and reduce task information.

{code}
2011-10-21 22:07:17,268 WARN  rumen.TraceBuilder (TraceBuilder.java:run(272)) - TraceBuilder got an error while processing the [possibly virtual] file job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist within Path hdfs://10.18.52.57:9000/user/root/null/history/done_intermediate/root/job_1319214405771_0002-1319214846458-root-word+count-1319214871038-1-1-SUCCEEDED.jhist
java.lang.IllegalArgumentException: JobBuilder.process(HistoryEvent): unknown event type
        at org.apache.hadoop.tools.rumen.JobBuilder.process(JobBuilder.java:165)
        at org.apache.hadoop.tools.rumen.TraceBuilder.processJobHistory(TraceBuilder.java:304)
        at org.apache.hadoop.tools.rumen.TraceBuilder.run(TraceBuilder.java:258)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
        at org.apache.hadoop.tools.rumen.TraceBuilder.main(TraceBuilder.java:185)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:189)

{code}",amar_kamat,devaraj,Major,Resolved,Fixed,21/Oct/11 16:25,10/Mar/15 04:32
Bug,MAPREDUCE-3242,12528329,Trunk compilation broken with bad interaction from MAPREDUCE-3070 and MAPREDUCE-3239.,Looks like patch command threw away some of the changes when I committed MAPREDUCE-3239 after MAPREDUCE-3070.,mahadev,mahadev,Major,Closed,Fixed,21/Oct/11 22:15,15/Nov/11 00:49
Bug,MAPREDUCE-3243,12528339,Invalid tracking URL for streaming jobs,"The tracking URL for streaming jobs currently display ""http://N/A""

{noformat}
INFO streaming.StreamJob: To kill this job, run:
INFO streaming.StreamJob: hadoop job -kill <jobID>
INFO streaming.StreamJob: Tracking URL: http://N/A
INFO mapreduce.Job: Running job: <jobID>
INFO mapreduce.Job:  map 0% reduce 0%
INFO mapred.ClientServiceDelegate: Tracking Url of JOB is <host:port>

{noformat}
",jeagles,rramya,Major,Closed,Fixed,22/Oct/11 00:46,05/Mar/12 02:49
Bug,MAPREDUCE-3248,12528438,Log4j logs from unit tests are lost,"Can't find log4j logs in tests, all of them complain:

{noformat}
log4j:WARN No appenders could be found for logger (org.apache.hadoop.yarn.server.resourcemanager.security.TestDelegationTokenRenewer).
log4j:WARN Please initialize the log4j system properly.
{noformat}

I suspect MAPREDUCE-3199.",vinodkv,acmurthy,Blocker,Resolved,Fixed,24/Oct/11 05:20,10/Mar/15 04:32
Bug,MAPREDUCE-3252,12528555,MR2: Map tasks rewrite data once even if output fits in sort buffer,"I found that, even if the output of a map task fits entirely in its sort buffer, it was rewriting the output entirely rather than just renaming the first spill into place. This is due to RawLocalFileSystem.rename() falling back to a copy if renameTo() fails. The first rename attempt was failing because no one has called mkdir for the output directory yet.",tlipcon,tlipcon,Critical,Closed,Fixed,24/Oct/11 20:50,15/Nov/11 00:48
Bug,MAPREDUCE-3253,12528557,ContextFactory throw NoSuchFieldException,"I see exceptions from ContextFactory when I am running Pig unit test:
Caused by: java.lang.IllegalArgumentException: Can't find field
        at org.apache.hadoop.mapreduce.ContextFactory.<clinit>(ContextFactory.java:139)
Caused by: java.lang.NoSuchFieldException: reporter
        at java.lang.Class.getDeclaredField(Class.java:1882)
        at org.apache.hadoop.mapreduce.ContextFactory.<clinit>(ContextFactory.java:126)",acmurthy,daijy,Blocker,Closed,Fixed,24/Oct/11 20:59,15/Nov/11 00:49
Bug,MAPREDUCE-3254,12528567,Streaming jobs failing with PipeMapRunner ClassNotFoundException,"ClassNotFoundException: org.apache.hadoop.streaming.PipeMapRunner encountered while running streaming jobs. Stack trace in the next comment.
",acmurthy,rramya,Blocker,Closed,Fixed,24/Oct/11 22:03,15/Nov/11 00:49
Bug,MAPREDUCE-3258,12528631,Job counters missing from AM and history UI,,sseth,sseth,Blocker,Closed,Fixed,25/Oct/11 06:57,15/Nov/11 00:48
Bug,MAPREDUCE-3259,12528650,ContainerLocalizer should get the proper java.library.path from LinuxContainerExecutor,"As seen in MAPREDUCE-2915, java.library.path is not being passed when the LCE spawns a JVM for ContainerLocalizer. 

However, unlike branch-0.20-security, the task runtime in 0.23 is unaffected by this. This is because tasks' run-time environment is specified in the launch script by client. Setting LD_LIBRARY_PATH is the primary way of specifying the locations of required native library in this case. The config property, mapreduce.admin.user.env is always set in the job environment and the default value is to add the path to the hadoop native library to LD_LABRARY_PATH.

For JVM's being launched by the hadoop system scripts, java.library.path is set.",kihwal,kihwal,Blocker,Closed,Fixed,25/Oct/11 10:14,16/Mar/15 17:57
Bug,MAPREDUCE-3261,12528719,AM unable to release containers,"I'm probably doing something wrong here, but I can't figure it out.

My ApplicationMaster is sending an AllocateRequest with ContainerIds to release. My ResourceManager logs say:

2011-10-25 10:02:52,236 WARN  resourcemanager.RMAuditLogger (RMAuditLogger.java:logFailure(207)) - USER=criccomi	IP=127.0.0.1	OPERATION=AM Released Container	TARGET=FifoScheduler	RESULT=FAILURE	DESCRIPTION=Trying to release container not owned by app or with invalid id	PERMISSIONS=Unauthorized access or invalid container	APPID=application_1319485153554_0028	CONTAINERID=container_1319485153554_0028_01_000003

The container ID is valid, as is the app id:

[criccomi@criccomi-ld logs]$ pwd
/tmp/logs
[criccomi@criccomi-ld logs]$ find .
.
./application_1319485153554_0028
./application_1319485153554_0028/container_1319485153554_0028_01_000002
./application_1319485153554_0028/container_1319485153554_0028_01_000002/stderr
./application_1319485153554_0028/container_1319485153554_0028_01_000002/stdout
./application_1319485153554_0028/container_1319485153554_0028_01_000001
./application_1319485153554_0028/container_1319485153554_0028_01_000001/stderr
./application_1319485153554_0028/container_1319485153554_0028_01_000001/stdout
./application_1319485153554_0028/container_1319485153554_0028_01_000003
./application_1319485153554_0028/container_1319485153554_0028_01_000003/stderr
./application_1319485153554_0028/container_1319485153554_0028_01_000003/stdout
./application_1319485153554_0028/container_1319485153554_0028_01_000006
./application_1319485153554_0028/container_1319485153554_0028_01_000006/stderr
./application_1319485153554_0028/container_1319485153554_0028_01_000006/stdout

The containers are still running.

My code to start a container, and then to release it:
{code}
  // ugi = UserGroupInformation.getCurrentUser
  // security is not enabled
  def startContainer(packagePath: Path, container: Container, ugi: UserGroupInformation, env: Map[String, String], cmds: String*) {
    info(""%s starting container %s %s %s %s %s"" format (appAttemptId, packagePath, container, ugi, env, cmds))
    // connect to container manager (based on similar code in the ContainerLauncher in Hadoop MapReduce)
    val contToken = container.getContainerToken
    val address = container.getNodeId.getHost + "":"" + container.getNodeId.getPort
    var user = ugi

    if (UserGroupInformation.isSecurityEnabled) {
      debug(""%s security is enabled"" format (appAttemptId))
      val hadoopToken = new Token[ContainerTokenIdentifier](contToken.getIdentifier.array, contToken.getPassword.array, new Text(contToken.getKind), new Text(contToken.getService))
      user = UserGroupInformation.createRemoteUser(address)
      user.addToken(hadoopToken)
      info(""%s changed user to %s"" format (appAttemptId, user))
    }

    val containerManager = user.doAs(new PrivilegedAction[ContainerManager] {
      def run(): ContainerManager = {
        return YarnRPC.create(conf).getProxy(classOf[ContainerManager], NetUtils.createSocketAddr(address), conf).asInstanceOf[ContainerManager]
      }
    })

    // set the local package so that the containers and app master are provisioned with it
    val packageResource = Records.newRecord(classOf[LocalResource])
    val packageUrl = ConverterUtils.getYarnUrlFromPath(packagePath)
    val fileStatus = packagePath.getFileSystem(conf).getFileStatus(packagePath)

    packageResource.setResource(packageUrl)
    packageResource.setSize(fileStatus.getLen)
    packageResource.setTimestamp(fileStatus.getModificationTime)
    packageResource.setType(LocalResourceType.ARCHIVE)
    packageResource.setVisibility(LocalResourceVisibility.APPLICATION)

    // start the container
    val ctx = Records.newRecord(classOf[ContainerLaunchContext])
    ctx.setEnvironment(env)
    ctx.setContainerId(container.getId())
    ctx.setResource(container.getResource())
    ctx.setUser(user.getShortUserName())
    ctx.setCommands(cmds.toList)
    ctx.setLocalResources(Collections.singletonMap(""package"", packageResource))

    debug(""%s setting package to %s"" format (appAttemptId, packageResource))
    debug(""%s setting context to %s"" format (appAttemptId, ctx))

    val startContainerRequest = Records.newRecord(classOf[StartContainerRequest])
    startContainerRequest.setContainerLaunchContext(ctx)
    containerManager.startContainer(startContainerRequest)
  }
{code}
-----
{code}
  def sendResourceRequest(requests: List[ResourceRequest], release: List[ContainerId]): AMResponse = {
    info(""%s sending resource request %s %s"" format (appAttemptId, requests, release))
    val req = Records.newRecord(classOf[AllocateRequest])
    req.setResponseId(requestId)
    req.setApplicationAttemptId(appAttemptId)
    req.addAllAsks(requests)
    req.addAllReleases(release)
    requestId += 1
    debug(""%s RM resource request %s"" format (appAttemptId, req))
    resourceManager.allocate(req).getAMResponse
  }
{code}

I have double checked that my ContainerIds are accurate, and they are.

Any idea what I'm doing wrong here?",,criccomini,Major,Closed,Fixed,25/Oct/11 17:11,16/Mar/15 17:57
Bug,MAPREDUCE-3262,12528722,A few events are not handled by the NodeManager in failure scenarios,"Need to handle kill container event in localization failed state. 
Need to handle resource localized in localization failed state. ",hitesh,hitesh,Critical,Closed,Fixed,25/Oct/11 17:42,15/Nov/11 00:49
Bug,MAPREDUCE-3263,12528725,compile-mapred-test target fails,"Compile mapred test target is broken due to which the builds are not archiving the test jars.

",hitesh,rramya,Blocker,Closed,Fixed,25/Oct/11 17:52,15/Nov/11 00:48
Bug,MAPREDUCE-3264,12528737,mapreduce.job.user.name needs to be set automatically,"Currently in MR2 I have to manually specify mapreduce.job.user.name for each job. It's not picking it up from the security infrastructure, at least when running with DefaultContainerExecutor. This is obviously incorrect.",acmurthy,tlipcon,Blocker,Closed,Fixed,25/Oct/11 19:09,15/Nov/11 00:49
Bug,MAPREDUCE-3269,12528759,Jobsummary logs not being moved to a separate file,"The jobsummary logs are not being moved to a separate file. Below is the configuration in log4j.properties:

{noformat}
mapred.jobsummary.logger=INFO,console
log4j.logger.org.apache.hadoop.mapreduce.jobhistory.JobSummary=${mapred.jobsummary.logger}
log4j.additivity.org.apache.hadoop.mapreduce.jobhistory.JobSummary=false
log4j.appender.JSA=org.apache.log4j.DailyRollingFileAppender
log4j.appender.JSA.File=${hadoop.log.dir}/mapred-jobsummary.log
log4j.appender.JSA.layout=org.apache.log4j.PatternLayout
log4j.appender.JSA.layout.ConversionPattern=%d{ISO8601} %p %c{2}: %m%n
log4j.appender.JSA.DatePattern=.yyyy-MM-dd
{noformat}",mahadev,rramya,Blocker,Closed,Fixed,25/Oct/11 21:07,15/Nov/11 00:49
Bug,MAPREDUCE-3274,12528959,Race condition in MR App Master Preemtion can cause a dead lock,There appears to be a race condition in the MR App Master in relation to preempting reducers to let a mapper run.  In the particular case that I have been debugging a reducer was selected for preemption that did not have a container assigned to it yet. When the container became available that reduce started running and the previous TA_KILL event appears to have been ignored.,revans2,revans2,Blocker,Closed,Fixed,26/Oct/11 20:48,16/Mar/15 17:57
Bug,MAPREDUCE-3279,12529022,TestJobHistoryParsing broken,"Broken after 3264, the test was verifying against the default user.",sseth,sseth,Major,Closed,Fixed,27/Oct/11 07:59,15/Nov/11 00:49
Bug,MAPREDUCE-3280,12529031,MR AM should not read the username from configuration,"MR AM reads the value for mapreduce.job.user.name from the configuration in several places. It should instead get the app-submitter name from the RM.

Once that is done, we can remove the default value for mapreduce.job.user.name from mapred-default.xml",vinodkv,vinodkv,Major,Closed,Fixed,27/Oct/11 10:31,05/Mar/12 02:48
Bug,MAPREDUCE-3281,12529063,TestLinuxContainerExecutorWithMocks failing on trunk.,,vinodkv,vinodkv,Blocker,Closed,Fixed,27/Oct/11 15:04,15/Nov/11 00:49
Bug,MAPREDUCE-3282,12529100,bin/mapred job -list throws exception,"bin/mapred job -list throws exception when mapreduce.framework.name is set to ""yarn""
",acmurthy,rramya,Critical,Closed,Fixed,27/Oct/11 18:10,15/Nov/11 00:48
Bug,MAPREDUCE-3283,12529104,mapred classpath CLI does not display the complete classpath,"bin/yarn classpath does not display the complete classpath. Below is how the classpath looks like:
{noformat}
$HADOOP_CONF_DIR:$HADOOP_CONF_DIR::$TOOLS_JAR:$HADOOP_COMMON_HOME/*:$HADOOP_COMMON_HOME/lib/*:$HADOOP_HDFS_HOME/*:$HADOOP_HDFS_HOME/lib/*:
$HADOOP_MAPRED_HOME/bin/../modules/*:$HADOOP_MAPRED_HOME/bin/../lib/*
{noformat}

""*"" has to be substituted with the actual jars. Also, $HADOOP_CONF_DIR appears twice in the classpath",varun_saxena,rramya,Minor,Closed,Fixed,27/Oct/11 18:18,10/Apr/15 20:19
Bug,MAPREDUCE-3284,12529117,bin/mapred queue fails with JobQueueClient ClassNotFoundException,"bin/mapred queue fails with the following exception:

{code}

-bash$ bin/mapred queue
Exception in thread ""main"" java.lang.NoClassDefFoundError: org/apache/hadoop/mapred/JobQueueClient
Caused by: java.lang.ClassNotFoundException: org.apache.hadoop.mapred.JobQueueClient
        at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
Could not find the main class: org.apache.hadoop.mapred.JobQueueClient.  Program will exit.

{code}",acmurthy,rramya,Major,Closed,Fixed,27/Oct/11 18:39,15/Nov/11 00:49
Bug,MAPREDUCE-3285,12529119,Tests on branch-0.23 failing ,"Most are failing with some kerberos login exception:

Running org.apache.hadoop.yarn.server.nodemanager.TestLinuxContainerExecutorWithMocks
Tests run: 3, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 0.548 sec <<< FAILURE!
--
Running org.apache.hadoop.yarn.server.resourcemanager.TestAppManager
Tests run: 8, Failures: 0, Errors: 6, Skipped: 0, Time elapsed: 0.125 sec <<< FAILURE!
Running org.apache.hadoop.yarn.server.resourcemanager.TestRMAuditLogger
Tests run: 3, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.065 sec <<< FAILURE!
--
Running org.apache.hadoop.yarn.server.resourcemanager.TestApplicationCleanup
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.033 sec <<< FAILURE!
--
Running org.apache.hadoop.yarn.server.resourcemanager.applicationsmanager.TestAMRMRPCResponseId
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.024 sec <<< FAILURE!
--
Running org.apache.hadoop.yarn.server.resourcemanager.TestRM
Tests run: 3, Failures: 0, Errors: 3, Skipped: 0, Time elapsed: 0.072 sec <<< FAILURE!
Running org.apache.hadoop.yarn.server.resourcemanager.TestApplicationACLs
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 90.167 sec <<< FAILURE!
Running org.apache.hadoop.yarn.server.resourcemanager.TestFifoScheduler
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.056 sec <<< FAILURE!

TestLinuxContainerExecutorWithMocks is tracked via MAPREDUCE-3281",sseth,acmurthy,Blocker,Closed,Fixed,27/Oct/11 18:53,15/Nov/11 00:49
Bug,MAPREDUCE-3288,12529165,Mapreduce 23 builds failing,Hadoop mapreduce 0.23 builds are failing.,mahadev,rramya,Blocker,Closed,Fixed,27/Oct/11 23:29,15/Nov/11 00:50
Bug,MAPREDUCE-3290,12529175,list-active-trackers throws NPE,"bin/mapred -list-active-trackers throws NPE in mrV2. Trace in the next comment.

",acmurthy,rramya,Major,Closed,Fixed,28/Oct/11 01:09,15/Nov/11 00:49
Bug,MAPREDUCE-3291,12529178,App fail to launch due to delegation token not found in cache,"In secure mode, saw an app failure due to ""org.apache.hadoop.security.token.SecretManager$InvalidToken: token (HDFS_DELEGATION_TOKEN token <id> for <user>) can't be found in cache"" Exception in the next comment.",revans2,rramya,Blocker,Closed,Fixed,28/Oct/11 01:55,11/Jan/12 05:54
Bug,MAPREDUCE-3292,12529180,In secure mode job submission fails with Provider org.apache.hadoop.mapreduce.security.token.JobTokenIndentifier$Renewer not found.,"This happens when you submit a job to a secure cluster. Also, its only the first time the error shows up. On the next submission of the job, the job passes.",mahadev,mahadev,Critical,Closed,Fixed,28/Oct/11 02:00,15/Nov/11 00:50
Bug,MAPREDUCE-3295,12529194,TestAMAuthorization failing on branch 0.23.,The test seems to fail both on Mac and linux. Trace in the next comment.,,mahadev,Critical,Closed,Fixed,28/Oct/11 04:35,15/Nov/11 00:48
Bug,MAPREDUCE-3296,12529197,Pending(9) findBugs warnings,,vinodkv,vinodkv,Major,Closed,Fixed,28/Oct/11 05:34,15/Nov/11 00:48
Bug,MAPREDUCE-3304,12529262,TestRMContainerAllocator#testBlackListedNodes fails intermittently,"Thanks to Hitesh for verifying!

bq. The heartbeat event should be drained before the schedule call.
bq. -- Hitesh

I can see this test fail intermittently on my Mac OSX 10.5 and Fedora 14 machines. ",raviprak,raviprak,Major,Closed,Fixed,28/Oct/11 14:46,15/Nov/11 00:48
Bug,MAPREDUCE-3306,12529285,Cannot run apps after MAPREDUCE-2989,"Seeing this in NM logs when trying to run jobs.
{code}
2011-10-28 21:40:21,263 INFO org.apache.hadoop.yarn.server.nodemanager.containermanager.application.Application: Processing application_1319818154209_0001 of type APPLICATION_INITED
2011-10-28 21:40:21,264 FATAL org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread. Exiting..
java.util.NoSuchElementException
        at java.util.HashMap$HashIterator.nextEntry(HashMap.java:796)
        at java.util.HashMap$ValueIterator.next(HashMap.java:822)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:251)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl$AppInitDoneTransition.transition(ApplicationImpl.java:245)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:385)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.application.ApplicationImpl.handle(ApplicationImpl.java:58)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:407)
        at org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl$ApplicationEventDispatcher.handle(ContainerManagerImpl.java:399)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
        at java.lang.Thread.run(Thread.java:662)
{code}",vinodkv,vinodkv,Blocker,Closed,Fixed,28/Oct/11 16:59,15/Nov/11 00:48
Bug,MAPREDUCE-3312,12529389,Make MR AM not send a stopContainer w/o corresponding start container,"This is a follow on to MAPREDUCE-3274.  It is possible, although rare, for the MR AM to send a stop container before it sends a start container.  This needs to stop that from happening.  If a stop is found first it should prevent the start from being sent.  It tries to do this, but only if the stop is currently pending.",revans2,revans2,Major,Closed,Fixed,30/Oct/11 00:21,10/Mar/15 04:32
Bug,MAPREDUCE-3313,12529414,TestResourceTrackerService failing in trunk some times,"TestResourceTrackerService is failing in trunk sometimes with the following error:

testDecommissionWithIncludeHosts(org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService)  Time elapsed: 0.876 sec  <<< ERROR!
java.lang.NullPointerException
  at org.apache.hadoop.yarn.server.resourcemanager.ClusterMetrics.getNumDecommisionedNMs(ClusterMetrics.java:78)
  at org.apache.hadoop.yarn.server.resourcemanager.TestResourceTrackerService.testDecommissionWithIncludeHosts(TestResourceTrackerService.java:70)",hitesh,ravidotg,Blocker,Closed,Fixed,30/Oct/11 15:51,15/Nov/11 00:49
Bug,MAPREDUCE-3316,12529474,Rebooted link is not working properly,"While clicking on the *Rebooted Nodes* link, it is showing the following error message
             {color:red}Sorry, got error 500{color}",kamesh,kam_iitkgp,Major,Closed,Fixed,31/Oct/11 12:42,15/Nov/11 00:48
Bug,MAPREDUCE-3317,12529521,Rumen TraceBuilder is emiting null as hostname,Trace generated by Rumen TraceBuilder contains null as hostname even though hostName and rackName are seen in history file. This is after MAPREDUCE-3035.,ravidotg,ravidotg,Major,Closed,Fixed,31/Oct/11 17:48,15/Nov/11 00:49
Bug,MAPREDUCE-3319,12529564,multifilewc from hadoop examples seems to be broken in 0.20.205.0,"{noformat}
/usr/lib/hadoop/bin/hadoop jar /usr/lib/hadoop/hadoop-examples-0.20.205.0.22.jar multifilewc  examples/text examples-output/multifilewc
11/10/31 16:50:26 INFO mapred.FileInputFormat: Total input paths to process : 2
11/10/31 16:50:26 INFO mapred.JobClient: Running job: job_201110311350_0220
11/10/31 16:50:27 INFO mapred.JobClient:  map 0% reduce 0%
11/10/31 16:50:42 INFO mapred.JobClient: Task Id : attempt_201110311350_0220_m_000000_0, Status : FAILED
java.lang.ClassCastException: org.apache.hadoop.io.IntWritable cannot be cast to org.apache.hadoop.io.LongWritable
	at org.apache.hadoop.mapred.lib.LongSumReducer.reduce(LongSumReducer.java:44)
	at org.apache.hadoop.mapred.Task$OldCombinerRunner.combine(Task.java:1431)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.sortAndSpill(MapTask.java:1436)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.flush(MapTask.java:1298)
	at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:437)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
{noformat}",subrotosanyal,rvs,Blocker,Closed,Fixed,31/Oct/11 21:41,28/Dec/11 10:03
Bug,MAPREDUCE-3321,12529601,Disable some failing legacy tests for MRv2 builds to go through,By-product of MR-3214. Disable tests for the short term until fixes are available for all tests.,hitesh,hitesh,Minor,Closed,Fixed,01/Nov/11 00:52,15/Nov/11 00:49
Bug,MAPREDUCE-3324,12529696,"Not All HttpServer tools links (stacks,logs,config,metrics) are accessible through all UI servers","Nodemanager has no tools listed under tools UI.
Jobhistory server has no logs tool listed under tools UI.",jeagles,jeagles,Critical,Closed,Fixed,01/Nov/11 16:52,05/Mar/12 02:49
Bug,MAPREDUCE-3326,12529706,RM web UI scheduler link not as useful as should be,"The resource manager web ui page for scheduler doesn't have all the information about the configuration like the jobtracker page used to have.  The things it seems to show you are the current queues - each queues used, set, and max percent and then what apps are running in that queue.  

It doesn't list any of yarn.scheduler.capacity.maximum-applications, yarn.scheduler.capacity.maximum-am-resource-percent, yarn.scheduler.capacity.<queue-path>.user-limit-factor, yarn.scheduler.capacity.<queue-path>.minimum-user-limit-percent, queue state, active users and percent used by user ",jlowe,tgraves,Critical,Closed,Fixed,01/Nov/11 17:45,10/Mar/15 04:31
Bug,MAPREDUCE-3327,12529733,RM web ui scheduler link doesn't show correct max value for queues,"Configure a cluster to use the capacity scheduler and then specifying a maximum-capacity < 100% for a queue.  If you go to the RM Web UI and hover over the queue, it always shows the max at 100%.",anupamseth,tgraves,Critical,Closed,Fixed,01/Nov/11 20:20,05/Mar/12 02:48
Bug,MAPREDUCE-3328,12529742,mapred queue -list output inconsistent and missing child queues,"When running mapred queue -list on a 0.23.0 cluster with capacity scheduler configured with child queues.  In my case I have queues default, test1, and test2.  test1 has subqueues of a1, a2.  test2 has subqueues of a3 and a4.

- the child queues do not show up
- The output of maximum capacity doesn't match the format of the current capacity and capacity.  the latter two use float while the maximum is specified as int:

Queue Name : default 
Queue State : running 
Scheduling Info : queueName: ""default"", capacity: 0.7, maximumCapacity: 90.0, currentCapacity: 0.0, state: Q_RUNNING,  
======================
Queue Name : test 
Queue State : running 
Scheduling Info : queueName: ""test"", capacity: 0.2, maximumCapacity: -1.0, currentCapacity: 0.0, state: Q_RUNNING,  
======================
Queue Name : test2 
Queue State : running 
Scheduling Info : queueName: ""test2"", capacity: 0.1, maximumCapacity: 5.0, currentCapacity: 0.0, state: Q_RUNNING,  
======================


here default is configured to have capacity=70% and maximum capacity = 90%",raviprak,tgraves,Critical,Closed,Fixed,01/Nov/11 20:35,05/Mar/12 02:49
Bug,MAPREDUCE-3329,12529746,capacity schedule maximum-capacity allowed to be less then capacity,"When configuring the capacity scheduler capacity and maximum-capacity, it allows the maximum-capacity to be less then the capacity.  I did not test to see what true limit is, I assume maximum capacity.

output from mapred queue -list where capacity = 10%, max capacity = 5%.

Queue Name : test2 
Queue State : running 
Scheduling Info : queueName: ""test2"", capacity: 0.1, maximumCapacity: 5.0, currentCapacity: 0.0, state: Q_RUNNING,  
",acmurthy,tgraves,Blocker,Closed,Fixed,01/Nov/11 20:41,05/Mar/12 02:49
Bug,MAPREDUCE-3332,12529772,contrib/raid compile breaks due to changes in hdfs/protocol/datatransfer/Sender#writeBlock related to checksum handling ,"    [javac] /Users/Hitesh/dev/hadoop-common/hadoop-mapreduce-project/src/contrib/raid/src/java/org/apache/hadoop/raid/BlockFixer.java:783: writeBlock(org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token<org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier>,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],org.apache.hadoop.hdfs.protocol.DatanodeInfo,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long,org.apache.hadoop.util.DataChecksum) in org.apache.hadoop.hdfs.protocol.datatransfer.Sender cannot be applied to (org.apache.hadoop.hdfs.protocol.ExtendedBlock,org.apache.hadoop.security.token.Token<org.apache.hadoop.hdfs.security.token.block.BlockTokenIdentifier>,java.lang.String,org.apache.hadoop.hdfs.protocol.DatanodeInfo[],<nulltype>,org.apache.hadoop.hdfs.protocol.datatransfer.BlockConstructionStage,int,long,long,long)
    [javac]         new Sender(out).writeBlock(block.getBlock(), block.getBlockToken(), """",
    [javac]                        ^
",hitesh,hitesh,Trivial,Closed,Fixed,01/Nov/11 22:49,15/Nov/11 00:48
Bug,MAPREDUCE-3333,12529861,MR AM for sort-job going out of memory,"[~Karams] just found this. The usual sort job on a 350 node cluster hung due to OutOfMemory and eventually failed after an hour instead of the usual odd 20 minutes.
{code}
2011-11-02 11:40:36,438 ERROR [ContainerLauncher #258] org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl: Container launch failed for container_1320233407485_0002
_01_001434 : java.lang.reflect.UndeclaredThrowableException
        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:88)
        at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:290)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: com.google.protobuf.ServiceException: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: ""gsbl91281.blue.ygrid.yahoo.com/98.137.101.189""; destination host is: """"gsbl91525.blue.ygrid.yahoo.com"":45450; 
        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:139)
        at $Proxy20.startContainer(Unknown Source)
        at org.apache.hadoop.yarn.api.impl.pb.client.ContainerManagerPBClientImpl.startContainer(ContainerManagerPBClientImpl.java:81)
        ... 4 more
Caused by: java.io.IOException: Failed on local exception: java.io.IOException: Couldn't set up IO streams; Host Details : local host is: ""gsbl91281.blue.ygrid.yahoo.com/98.137.101.189""; destination host is: """"gsbl91525.blue.ygrid.yahoo.com"":45450; 
        at org.apache.hadoop.net.NetUtils.wrapException(NetUtils.java:655)
        at org.apache.hadoop.ipc.Client.call(Client.java:1089)
        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:136)
        ... 6 more
Caused by: java.io.IOException: Couldn't set up IO streams
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:621)
        at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:205)
        at org.apache.hadoop.ipc.Client.getConnection(Client.java:1195)
        at org.apache.hadoop.ipc.Client.call(Client.java:1065)
        ... 7 more
Caused by: java.lang.OutOfMemoryError: unable to create new native thread
        at java.lang.Thread.start0(Native Method)
        at java.lang.Thread.start(Thread.java:597)
        at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:614)
        ... 10 more
{code}",vinodkv,vinodkv,Blocker,Closed,Fixed,02/Nov/11 14:00,18/Jul/14 14:05
Bug,MAPREDUCE-3336,12529953,com.google.inject.internal.Preconditions not public api - shouldn't be using it,"com.google.inject.internal.Preconditions does not exist in guice 3.0 and from in guice 2.0 it was an internal api and shouldn't have been used.   We should use com.google.common.base.Preconditions instead.

This is currently being used in hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/src/main/java/org/apache/hadoop/yarn/server/nodemanager/containermanager/monitor/ContainersMonitorImpl.java.

",tgraves,tgraves,Critical,Closed,Fixed,02/Nov/11 21:35,05/Mar/12 02:48
Bug,MAPREDUCE-3337,12529999,Missing license headers for some files,Missing apache license headers for some files,acmurthy,acmurthy,Blocker,Closed,Fixed,03/Nov/11 06:37,15/Nov/11 00:50
Bug,MAPREDUCE-3339,12530026,"Job is getting hanged indefinitely,if the child processes are killed on the NM.  KILL_CONTAINER eventtype is continuosly sent to the containers that are not existing","I have only one NM running.
I have submitted a job and all the child processes on the NM got killed continuosly.This made the Job to hang indefinitely.

In the NM logs it is logging WARN message :org.apache.hadoop.yarn.server.nodemanager.containermanager.ContainerManagerImpl: Event EventType: KILL_CONTAINER sent to absent container container_1320301910500_0004_01_001359 ",sseth,ramgopalnaali,Blocker,Closed,Fixed,03/Nov/11 10:03,03/Oct/13 20:44
Bug,MAPREDUCE-3342,12530120,JobHistoryServer doesn't show job queue,"The job history server doesn't show the queue the jobwas run in.  It is inserted into the job history file.

It seems like this should be part of the Job interface.

JobImpl current gets it from the job config to insert into the history.  ",jeagles,tgraves,Critical,Closed,Fixed,03/Nov/11 21:52,05/Mar/12 02:49
Bug,MAPREDUCE-3343,12530133,TaskTracker Out of Memory because of distributed cache,"This Out of Memory happens when you run large number of jobs (using the distributed cache) on a TaskTracker. 

Seems the basic issue is with the distributedCacheManager (instance of TrackerDistributedCacheManager in TaskTracker.java), this gets created during TaskTracker.initialize(), and it keeps references to TaskDistributedCacheManager for every submitted job via the jobArchives Map, also references to CacheStatus via cachedArchives map. I am not seeing these cleaned up between jobs, so this can out of memory problems after really large number of jobs are submitted. We have seen this issue in a number of cases.",zhaoyunjiong,ahmed.radwan,Major,Closed,Fixed,03/Nov/11 23:42,18/Mar/12 06:12
Bug,MAPREDUCE-3344,12530145,o.a.h.mapreduce.Reducer since 0.21 blindly casts to ReduceContext.ValueIterator,"0.21 mapreduce.Reducer introduced a blind cast to ReduceContext.ValueIterator. There should an instanceof check around this block to ensure we don't throw a CastClassException:
{code}
       // If a back up store is used, reset it
      ((ReduceContext.ValueIterator)
          (context.getValues().iterator())).resetBackupStore();
{code}",brocknoland,brocknoland,Major,Closed,Fixed,04/Nov/11 02:01,10/Mar/15 04:32
Bug,MAPREDUCE-3345,12530240,Race condition in ResourceManager causing TestContainerManagerSecurity to fail sometimes,See https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1247//testReport/org.apache.hadoop.yarn.server/TestContainerManagerSecurity/testUnauthorizedUser/,hitesh,vinodkv,Major,Closed,Fixed,04/Nov/11 09:05,05/Mar/12 02:49
Bug,MAPREDUCE-3346,12530245,Rumen LoggedTaskAttempt  getHostName call returns hostname as null,"After MAPREDUCE-3035 and MAPREDUCE-3317
Now MRV2 job history contains hostName and rackName.
when rumen trace builder is ran on jobhistory, its generated trace contains hostname in form of 
hostName : /raclname/hostname

But getHostName for LoggedTaskAttempt returns hostname as null
Seems that TraceBuilder is setting hostName properly but JobTraceReader is not able read it.",amar_kamat,karams,Blocker,Closed,Fixed,04/Nov/11 10:01,05/Mar/12 02:48
Bug,MAPREDUCE-3348,12530261,mapred job -status fails to give info even if the job is present in History,"It is trying to get the app report from the RM  for the job, RM throws exception when it doesn't find and then it is giving the same exception without trying from History Server.

{code}
11/11/03 08:47:27 INFO ipc.HadoopYarnRPC: Creating a HadoopYarnProtoRpc proxy for protocol interface org.apache.hadoop.mapred                                                                                  uce.v2.api.MRClientProtocol
11/11/03 08:47:28 WARN mapred.ClientServiceDelegate: Exception thrown by remote end.
RemoteTrace:
 at LocalTrace:
        org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Trying to get information for an absent applicat                                                                                  ion application_1320278804241_0002
        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)
        at $Proxy6.getApplicationReport(Unknown Source)
        at org.apache.hadoop.yarn.api.impl.pb.client.ClientRMProtocolPBClientImpl.getApplicationReport(ClientRMProtocolPBClie                                                                                  ntImpl.java:111)
        at org.apache.hadoop.mapred.ResourceMgrDelegate.getApplicationReport(ResourceMgrDelegate.java:321)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getProxy(ClientServiceDelegate.java:137)
        at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:273)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:353)
        at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:429)
        at org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:186)
        at org.apache.hadoop.mapreduce.tools.CLI.run(CLI.java:240)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
        at org.apache.hadoop.mapred.JobClient.main(JobClient.java:1106)
Exception in thread ""main"" RemoteTrace:
 at Local Trace:
        org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Trying to get information for an absent applicat                                                                                  ion application_1320278804241_0002
        at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:142)
        at $Proxy6.getApplicationReport(Unknown Source)
        at org.apache.hadoop.yarn.api.impl.pb.client.ClientRMProtocolPBClientImpl.getApplicationReport(ClientRMProtocolPBClie                                                                                  ntImpl.java:111)
        at org.apache.hadoop.mapred.ResourceMgrDelegate.getApplicationReport(ResourceMgrDelegate.java:321)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getProxy(ClientServiceDelegate.java:137)
        at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:273)
        at org.apache.hadoop.mapred.ClientServiceDelegate.getJobStatus(ClientServiceDelegate.java:353)
        at org.apache.hadoop.mapred.YARNRunner.getJobStatus(YARNRunner.java:429)
        at org.apache.hadoop.mapreduce.Cluster.getJob(Cluster.java:186)
        at org.apache.hadoop.mapreduce.tools.CLI.run(CLI.java:240)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
        at org.apache.hadoop.mapred.JobClient.main(JobClient.java:1106)

{code}",devaraj,devaraj,Major,Closed,Fixed,04/Nov/11 12:29,11/Oct/12 17:48
Bug,MAPREDUCE-3349,12530263,No rack-name logged in JobHistory for unsuccessful tasks,"Found this while running jobs on a cluster with [~Karams].

This is because TaskAttemptUnsuccessfulCompletionEvent history record doesn't have a rack field.",amar_kamat,vinodkv,Blocker,Closed,Fixed,04/Nov/11 12:39,05/Mar/12 02:49
Bug,MAPREDUCE-3350,12530266,Per-app RM page should have the list of application-attempts like on the app JHS page,,jeagles,vinodkv,Critical,Closed,Fixed,04/Nov/11 12:48,11/Oct/12 17:48
Bug,MAPREDUCE-3353,12530275,Need a RM->AM channel to inform AMs about faulty/unhealthy/lost nodes,"When a node gets lost or turns faulty, AM needs to know about that event so that it can take some action like for e.g. re-executing map tasks whose intermediate output live on that faulty node.",bikassaha,vinodkv,Major,Closed,Fixed,04/Nov/11 13:14,10/Mar/15 04:31
Bug,MAPREDUCE-3354,12530277,JobHistoryServer should be started by bin/mapred and not by bin/yarn,JobHistoryServer belongs to mapreduce land.,jeagles,vinodkv,Blocker,Closed,Fixed,04/Nov/11 13:20,05/Mar/12 02:48
Bug,MAPREDUCE-3355,12530282,AM scheduling hangs frequently with sort job on 350 nodes,"Another collaboration with [~karams]. Sort job hangs not so rarely on a 350 node cluster. Found this in AM logs:
{code}

Exception in thread ""ContainerLauncher #60"" org.apache.hadoop.yarn.YarnException: java.lang.InterruptedException
            at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:170)
            at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:379)
            at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
            at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.InterruptedException
            at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1199)
            at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:312)
            at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:294)
            at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:168)
            ... 4 more

Exception in thread ""ContainerLauncher #53"" org.apache.hadoop.yarn.YarnException: java.lang.InterruptedException
            at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:170)
            at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl.sendContainerLaunchFailedMsg(ContainerLauncherImpl.java:405)
            at org.apache.hadoop.mapreduce.v2.app.launcher.ContainerLauncherImpl$EventProcessor.run(ContainerLauncherImpl.java:330)
            at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
            at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
            at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.InterruptedException
            at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireInterruptibly(AbstractQueuedSynchronizer.java:1199)
            at java.util.concurrent.locks.ReentrantLock.lockInterruptibly(ReentrantLock.java:312)
            at java.util.concurrent.LinkedBlockingQueue.put(LinkedBlockingQueue.java:294)
            at org.apache.hadoop.yarn.event.AsyncDispatcher$GenericEventHandler.handle(AsyncDispatcher.java:168)
            ... 5 more
{code}",vinodkv,vinodkv,Blocker,Closed,Fixed,04/Nov/11 13:48,05/Mar/12 02:48
Bug,MAPREDUCE-3366,12530596,Mapreduce component should use consistent directory structure layout as HDFS/common,"Directory structure for MRv2 layout looks like:

{noformat}
hadoop-mapreduce-0.23.0-SNAPSHOT/bin
                                /conf
                                /lib
                                /modules
{noformat}

The directory structure layout should be updated to reflect changes implemented in HADOOP-6255.

{noformat}
hadoop-mapreduce-0.23.0-SNAPSHOT/bin
                                /etc/hadoop
                                /lib
                                /libexec
                                /sbin
                                /share/hadoop
                                /share/hadoop/lib
{noformat}",eyang,eyang,Major,Closed,Fixed,07/Nov/11 16:59,02/May/13 02:29
Bug,MAPREDUCE-3368,12530610,compile-mapred-test fails,"compile-mapred-test target is failing once again.
Details: https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Mapreduce-0.23-Build/83/consoleFull",hitesh,rramya,Critical,Closed,Fixed,07/Nov/11 18:48,05/Mar/12 02:49
Bug,MAPREDUCE-3370,12530629,MiniMRYarnCluster uses a hard coded path location for the MapReduce application jar,MiniMRYarnCluster uses a hard coded relative path location for the MapReduce application jar. It is better to have this location as a system property so tests can pick the application jar regardless of their working directory.,ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,07/Nov/11 20:49,02/May/13 02:30
Bug,MAPREDUCE-3372,12530641,HADOOP_PREFIX cannot be overriden,"hadoop-config.sh forces HADOOP_prefix to a specific value:
export HADOOP_PREFIX=`dirname ""$this""`/..

It would be nice to make this overridable.
",bmahe,bmahe,Major,Closed,Fixed,07/Nov/11 22:28,05/Mar/12 02:48
Bug,MAPREDUCE-3373,12530654,"Hadoop scripts unconditionally source ""$bin""/../libexec/hadoop-config.sh.",It would be nice to be able to specify some other location for hadoop-config.sh,bmahe,bmahe,Major,Closed,Fixed,08/Nov/11 00:05,05/Mar/12 02:49
Bug,MAPREDUCE-3374,12530662,src/c++/task-controller/configure is not set executable in the tarball and that prevents task-controller from rebuilding,ant task-controller fails because src/c++/task-controller/configure is not set executable,,rvs,Major,Closed,Fixed,08/Nov/11 00:47,28/Dec/11 10:03
Bug,MAPREDUCE-3376,12530733,Old mapred API combiner uses NULL reporter,"The OldCombinerRunner class inside Task.java uses a NULL Reporter.  If the combiner code runs for an extended period of time, even with reporting progress as it should, the map task can timeout and be killed.  It appears that the NewCombinerRunner class uses a valid reporter and as such is not impacted by this bug.",subrotosanyal,revans2,Major,Closed,Fixed,08/Nov/11 16:25,10/Mar/15 04:31
Bug,MAPREDUCE-3377,12530760,Compatibility issue with 0.20.203.,"I have an OutputFormat which implements Configurable.  I set new config entries to a job configuration during checkOutputSpec() so that the tasks will get the config entries through the job configuration.  This works fine in 0.20.2, but stopped working starting from 0.20.203.  With 0.20.203, my OutputFormat still has the configuration set, but the copy a task gets does not have the new entries that are set as part of checkOutputSpec().  

I believe that the problem is with JobClient.  The job configuration needs to wait till checkOutputSpec() is returned before being cloned and submitted.",jxchen,jxchen,Major,Closed,Fixed,08/Nov/11 19:47,29/May/12 06:33
Bug,MAPREDUCE-3379,12530787,LocalResourceTracker should not tracking deleted cache entries,,sseth,sseth,Major,Closed,Fixed,08/Nov/11 23:37,05/Mar/12 02:49
Bug,MAPREDUCE-3382,12530815,Network ACLs can prevent AMs to ping the Job-end notification URL,MAPREDUCE-3028 added support for job-end notification from MR AMs after the job finishes. Network ACLs can have an implication on this one - outgoing connections from the compute nodes may be restricted in some settings and so job-end notification( that can originate from the AMs which may run on random nodes in the cluster) may have issues.,raviprak,vinodkv,Critical,Closed,Fixed,09/Nov/11 03:43,05/Mar/12 02:49
Bug,MAPREDUCE-3383,12530830,Duplicate job.getOutputValueGroupingComparator() in ReduceTask,"This is probably just a small error by mistake.
",decster,decster,Major,Resolved,Fixed,09/Nov/11 08:08,30/Aug/16 01:20
Bug,MAPREDUCE-3387,12530887,A tracking URL of N/A before the app master is launched breaks oozie,"When oozie launches a map/reduce job it retrieves the tracking URL of that job to display in its own UI.  Previously this tracking URL did not change except when the job completed.  Currently the URL starts out as N/A and then changes to the real URL once the AppMaster launches.  This breaks oozie, as oozie expects to be able to get that information quickly and get back to processing other requests.

Because the web app proxy is now available we can maintain backwards compatibility by instead of returning N/A we can return the URL of the proxy for that application.  This relies on the fact that the application master's UI will display the correct thing for the URI http://<host>:<port>/, which MR and others now do.",revans2,revans2,Critical,Closed,Fixed,09/Nov/11 15:21,10/Mar/15 04:32
Bug,MAPREDUCE-3389,12530933,MRApps loads the 'mrapp-generated-classpath' file with classpath from the build machine,"The 'mrapp-generated-classpath' file contains the classpath from where Hadoop was build. This classpath is not useful under any circumstances.

For example the content of the 'mrapp-generated-classpath' in my dev environment is:

/Users/tucu/.m2/repository/aopalliance/aopalliance/1.0/aopalliance-1.0.jar:/Users/tucu/.m2/repository/asm/asm/3.2/asm-3.2.jar:/Users/tucu/.m2/repository/com/cenqua/clover/clover/3.0.2/clover-3.0.2.jar:/Users/tucu/.m2/repository/com/google/guava/guava/r09/guava-r09.jar:/Users/tucu/.m2/repository/com/google/inject/guice/2.0/guice-2.0.jar:/Users/tucu/.m2/repository/com/google/inject/extensions/guice-servlet/2.0/guice-servlet-2.0.jar:/Users/tucu/.m2/repository/com/google/protobuf/protobuf-java/2.4.0a/protobuf-java-2.4.0a.jar:/Users/tucu/.m2/repository/com/sun/jersey/jersey-core/1.8/jersey-core-1.8.jar:/Users/tucu/.m2/repository/com/sun/jersey/jersey-json/1.8/jersey-json-1.8.jar:/Users/tucu/.m2/repository/com/sun/jersey/jersey-server/1.8/jersey-server-1.8.jar:/Users/tucu/.m2/repository/com/sun/xml/bind/jaxb-impl/2.2.3-1/jaxb-impl-2.2.3-1.jar:/Users/tucu/.m2/repository/com/thoughtworks/paranamer/paranamer/2.3/paranamer-2.3.jar:/Users/tucu/.m2/repository/commons-beanutils/commons-beanutils/1.7.0/commons-beanutils-1.7.0.jar:/Users/tucu/.m2/repository/commons-beanutils/commons-beanutils-core/1.8.0/commons-beanutils-core-1.8.0.jar:/Users/tucu/.m2/repository/commons-cli/commons-cli/1.2/commons-cli-1.2.jar:/Users/tucu/.m2/repository/commons-codec/commons-codec/1.4/commons-codec-1.4.jar:/Users/tucu/.m2/repository/commons-collections/commons-collections/3.2.1/commons-collections-3.2.1.jar:/Users/tucu/.m2/repository/commons-configuration/commons-configuration/1.6/commons-configuration-1.6.jar:/Users/tucu/.m2/repository/commons-daemon/commons-daemon/1.0.3/commons-daemon-1.0.3.jar:/Users/tucu/.m2/repository/commons-digester/commons-digester/1.8/commons-digester-1.8.jar:/Users/tucu/.m2/repository/commons-el/commons-el/1.0/commons-el-1.0.jar:/Users/tucu/.m2/repository/commons-httpclient/commons-httpclient/3.1/commons-httpclient-3.1.jar:/Users/tucu/.m2/repository/commons-io/commons-io/2.1/commons-io-2.1.jar:/Users/tucu/.m2/repository/commons-lang/commons-lang/2.5/commons-lang-2.5.jar:/Users/tucu/.m2/repository/commons-logging/commons-logging/1.1.1/commons-logging-1.1.1.jar:/Users/tucu/.m2/repository/commons-logging/commons-logging-api/1.1/commons-logging-api-1.1.jar:/Users/tucu/.m2/repository/commons-net/commons-net/1.4.1/commons-net-1.4.1.jar:/Users/tucu/.m2/repository/hsqldb/hsqldb/1.8.0.7/hsqldb-1.8.0.7.jar:/Users/tucu/.m2/repository/javax/activation/activation/1.1/activation-1.1.jar:/Users/tucu/.m2/repository/javax/servlet/servlet-api/2.5/servlet-api-2.5.jar:/Users/tucu/.m2/repository/javax/servlet/jsp/jsp-api/2.1/jsp-api-2.1.jar:/Users/tucu/.m2/repository/javax/xml/bind/jaxb-api/2.2.2/jaxb-api-2.2.2.jar:/Users/tucu/.m2/repository/javax/xml/stream/stax-api/1.0-2/stax-api-1.0-2.jar:/Users/tucu/.m2/repository/jdiff/jdiff/1.0.9/jdiff-1.0.9.jar:/Users/tucu/.m2/repository/jline/jline/0.9.94/jline-0.9.94.jar:/Users/tucu/.m2/repository/junit/junit/4.8.2/junit-4.8.2.jar:/Users/tucu/.m2/repository/log4j/log4j/1.2.15/log4j-1.2.15.jar:/Users/tucu/.m2/repository/net/java/dev/jets3t/jets3t/0.6.1/jets3t-0.6.1.jar:/Users/tucu/.m2/repository/net/sf/kosmosfs/kfs/0.3/kfs-0.3.jar:/Users/tucu/.m2/repository/org/apache/avro/avro/1.5.3/avro-1.5.3.jar:/Users/tucu/.m2/repository/org/apache/avro/avro-ipc/1.5.3/avro-ipc-1.5.3.jar:/Users/tucu/.m2/repository/org/apache/commons/commons-math/2.1/commons-math-2.1.jar:/Users/tucu/src/apache/hadoop/git/hadoop-common-project/hadoop-annotations/target/hadoop-annotations-0.24.0-SNAPSHOT.jar:/Users/tucu/src/apache/hadoop/git/hadoop-common-project/hadoop-auth/target/hadoop-auth-0.24.0-SNAPSHOT.jar:/Users/tucu/src/apache/hadoop/git/hadoop-common-project/hadoop-common/target/hadoop-common-0.24.0-SNAPSHOT.jar:/Users/tucu/src/apache/hadoop/git/hadoop-common-project/hadoop-common/target/hadoop-common-0.24.0-SNAPSHOT-tests.jar:/Users/tucu/src/apache/hadoop/git/hadoop-hdfs-project/hadoop-hdfs/target/hadoop-hdfs-0.24.0-SNAPSHOT.jar:/Users/tucu/src/apache/hadoop/git/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-common/target/hadoop-mapreduce-client-common-0.24.0-SNAPSHOT.jar:/Users/tucu/src/apache/hadoop/git/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-core/target/hadoop-mapreduce-client-core-0.24.0-SNAPSHOT.jar:/Users/tucu/src/apache/hadoop/git/hadoop-mapreduce-project/hadoop-mapreduce-client/hadoop-mapreduce-client-shuffle/target/hadoop-mapreduce-client-shuffle-0.24.0-SNAPSHOT.jar:/Users/tucu/src/apache/hadoop/git/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-api/target/hadoop-yarn-api-0.24.0-SNAPSHOT.jar:/Users/tucu/src/apache/hadoop/git/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/target/hadoop-yarn-common-0.24.0-SNAPSHOT.jar:/Users/tucu/src/apache/hadoop/git/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/target/hadoop-yarn-common-0.24.0-SNAPSHOT-tests.jar:/Users/tucu/src/apache/hadoop/git/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/target/hadoop-yarn-server-common-0.24.0-SNAPSHOT.jar:/Users/tucu/src/apache/hadoop/git/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/hadoop-yarn-server-nodemanager-0.24.0-SNAPSHOT.jar:/Users/tucu/src/apache/hadoop/git/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/target/hadoop-yarn-server-resourcemanager-0.24.0-SNAPSHOT.jar:/Users/tucu/src/apache/hadoop/git/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-resourcemanager/target/hadoop-yarn-server-resourcemanager-0.24.0-SNAPSHOT-tests.jar:/Users/tucu/src/apache/hadoop/git/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-web-proxy/target/hadoop-yarn-server-web-proxy-0.24.0-SNAPSHOT.jar:/Users/tucu/.m2/repository/org/apache/velocity/velocity/1.7/velocity-1.7.jar:/Users/tucu/.m2/repository/org/apache/zookeeper/zookeeper/3.3.1/zookeeper-3.3.1.jar:/Users/tucu/.m2/repository/org/aspectj/aspectjrt/1.6.5/aspectjrt-1.6.5.jar:/Users/tucu/.m2/repository/org/codehaus/jackson/jackson-core-asl/1.7.1/jackson-core-asl-1.7.1.jar:/Users/tucu/.m2/repository/org/codehaus/jackson/jackson-jaxrs/1.7.1/jackson-jaxrs-1.7.1.jar:/Users/tucu/.m2/repository/org/codehaus/jackson/jackson-mapper-asl/1.7.1/jackson-mapper-asl-1.7.1.jar:/Users/tucu/.m2/repository/org/codehaus/jackson/jackson-xc/1.7.1/jackson-xc-1.7.1.jar:/Users/tucu/.m2/repository/org/codehaus/jettison/jettison/1.1/jettison-1.1.jar:/Users/tucu/.m2/repository/org/eclipse/jdt/core/3.1.1/core-3.1.1.jar:/Users/tucu/.m2/repository/org/jboss/netty/netty/3.2.3.Final/netty-3.2.3.Final.jar:/Users/tucu/.m2/repository/org/mockito/mockito-all/1.8.5/mockito-all-1.8.5.jar:/Users/tucu/.m2/repository/org/mortbay/jetty/jetty/6.1.26/jetty-6.1.26.jar:/Users/tucu/.m2/repository/org/mortbay/jetty/jetty-util/6.1.26/jetty-util-6.1.26.jar:/Users/tucu/.m2/repository/org/slf4j/slf4j-api/1.6.1/slf4j-api-1.6.1.jar:/Users/tucu/.m2/repository/org/slf4j/slf4j-log4j12/1.6.1/slf4j-log4j12-1.6.1.jar:/Users/tucu/.m2/repository/org/xerial/snappy/snappy-java/1.0.3.2/snappy-java-1.0.3.2.jar:/Users/tucu/.m2/repository/oro/oro/2.0.8/oro-2.0.8.jar:/Users/tucu/.m2/repository/stax/stax-api/1.0.1/stax-api-1.0.1.jar:/Users/tucu/.m2/repository/tomcat/jasper-compiler/5.5.23/jasper-compiler-5.5.23.jar:/Users/tucu/.m2/repository/tomcat/jasper-runtime/5.5.23/jasper-runtime-5.5.23.jar:/Users/tucu/.m2/repository/xmlenc/xmlenc/0.52/xmlenc-0.52.jar
",tucu00,tucu00,Critical,Closed,Fixed,09/Nov/11 22:30,10/Mar/15 04:32
Bug,MAPREDUCE-3390,12530960,NPE while submitting job,"Caused by: java.lang.NullPointerException
        at java.io.Reader.<init>(Reader.java:61)
        at java.io.InputStreamReader.<init>(InputStreamReader.java:55)
        at org.apache.hadoop.mapreduce.v2.util.MRApps.setMRFrameworkClasspath(MRApps.java:183)
        at org.apache.hadoop.mapreduce.v2.util.MRApps.setClasspath(MRApps.java:220)
        at org.apache.hadoop.mapred.YARNRunner.createApplicationSubmissionContext(YARNRunner.java:360)
        at org.apache.hadoop.mapred.YARNRunner.submitJob(YARNRunner.java:237)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:377)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1159)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1156)
        at java.security.AccessController.doPrivileged(Native Method)        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1152)        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1156)
        at org.apache.hadoop.mapred.JobClient.submitJob(JobClient.java:571)",johnvijoe,johnvijoe,Minor,Resolved,Fixed,10/Nov/11 03:52,09/Mar/15 21:51
Bug,MAPREDUCE-3391,12530963,Connecting to CM is logged as Connecting to RM,"In class *org.apache.hadoop.yarn.applications.distributedshell.ApplicationMaster*
{code}
private void connectToCM() {
      String cmIpPortStr = container.getNodeId().getHost() + "":"" 
          + container.getNodeId().getPort();		
      InetSocketAddress cmAddress = NetUtils.createSocketAddr(cmIpPortStr);		
      LOG.info(""Connecting to ResourceManager at "" + cmIpPortStr);
      this.cm = ((ContainerManager) rpc.getProxy(ContainerManager.class, cmAddress, conf));
    }
{code}",subrotosanyal,subrotosanyal,Minor,Closed,Fixed,10/Nov/11 06:45,05/Mar/12 02:49
Bug,MAPREDUCE-3398,12531410,Log Aggregation broken in Secure Mode,"Log aggregation in secure mode does not work with MAPREDUCE-2977. The nodemanager relies on the users credentials to write out logs to HDFS. These credentials are currently cancelled once a job completes, before the NM can write out the logs.",sseth,sseth,Blocker,Closed,Fixed,15/Nov/11 05:20,05/Mar/12 02:49
Bug,MAPREDUCE-3404,12531521,Speculative Execution: speculative map tasks launched even if -Dmapreduce.map.speculative=false,"When forcing a mapper to take significantly longer than other map tasks, speculative map tasks are
launched even if the mapreduce.job.maps.speculative.execution parameter is set to 'false'.

Testcase: ran default WordCount job with spec execution set to false for both map and reduce but still saw a fifth mapper
task launch, ran job as follows:

hadoop --config <config>  jar   /tmp/testphw/wordcount.jar   WordCount  
-Dmapreduce.job.maps.speculative.execution=false  -Dmapreduce.job.reduces.speculative.execution=false 
/tmp/test_file_of_words* /tmp/file_of_words.out

Input data was 4 text files >hdfs blocksize, with same word pattern plus one diff text line in each file, fourth
file was 4 times as large as others:

hadoop --config <config>  fs -ls  /tmp
Found 5 items
drwxr-xr-x   - user hdfs          0 2011-10-20 16:17 /tmp/file_of_words.out
-rw-r--r--   3 user hdfs   62800021 2011-10-20 14:45 /tmp/test_file_of_words1
-rw-r--r--   3 user hdfs   62800024 2011-10-20 14:46 /tmp/test_file_of_words2
-rw-r--r--   3 user hdfs   62800024 2011-10-20 14:46 /tmp/test_file_of_words3
-rw-r--r--   3 user hdfs  271708312 2011-10-20 15:50 /tmp/test_file_of_words4

Job launched 5 mappers despite spec exec set to false, output snippet:

        org.apache.hadoop.mapreduce.JobCounter
                NUM_FAILED_MAPS=1
                TOTAL_LAUNCHED_MAPS=5
                TOTAL_LAUNCHED_REDUCES=1
                RACK_LOCAL_MAPS=5
                SLOTS_MILLIS_MAPS=273540
                SLOTS_MILLIS_REDUCES=212876


Reran same case as above only set both spec exec params to 'true', same results only this time the fifth task being
launched is expected since spec exec = true.

job run:

hadoop --config <config>  jar   /tmp/testphw/wordcount.jar   WordCount  
-Dmapreduce.job.maps.speculative.execution=true  -Dmapreduce.job.reduces.speculative.execution=true 
/tmp/test_file_of_words* /tmp/file_of_words.out

output snippet:

        org.apache.hadoop.mapreduce.JobCounter
                NUM_FAILED_MAPS=1
                TOTAL_LAUNCHED_MAPS=5
                TOTAL_LAUNCHED_REDUCES=1
                RACK_LOCAL_MAPS=5
                SLOTS_MILLIS_MAPS=279653
                SLOTS_MILLIS_REDUCES=211474",epayne,patwhitey2007,Critical,Closed,Fixed,15/Nov/11 19:18,05/Mar/12 02:49
Bug,MAPREDUCE-3405,12531535,MAPREDUCE-3015 broke compilation of contrib scheduler tests,"MAPREDUCE-3015 added a new argument to the TaskTrackerStatus constructor, which is used by a few of the scheduler tests, but didn't update those tests. So, the contrib test build is now failing on 0.20-security",tlipcon,tlipcon,Critical,Closed,Fixed,15/Nov/11 21:39,17/Oct/12 18:27
Bug,MAPREDUCE-3407,12531555,Wrong jar getting used in TestMR*Jobs* for MiniMRYarnCluster,pom for mapreduce-client-jobclient sets system property to incorrect jar name. ,hitesh,hitesh,Minor,Closed,Fixed,15/Nov/11 23:40,10/Mar/15 04:32
Bug,MAPREDUCE-3408,12531565,yarn-daemon.sh unconditionnaly sets yarn.root.logger,"yarn-daemon.sh unconditionnaly sets yarn.root.logger which then prevent any override from happening.
From ./hadoop-mapreduce-project/hadoop-yarn/bin/yarn-daemon.sh:
> export YARN_ROOT_LOGGER=""INFO,DRFA""
> export YARN_JHS_LOGGER=""INFO,JSA""

and then yarn-daemon.sh will call ""$YARN_HOME""/bin/yarn which does the following:
> YARN_OPTS=""$YARN_OPTS -Dhadoop.root.logger=${YARN_ROOT_LOGGER:-INFO,console}""
> YARN_OPTS=""$YARN_OPTS -Dyarn.root.logger=${YARN_ROOT_LOGGER:-INFO,console}""

This has at least 2 issues:
* I cannot override hadoop.root.logger when using the yarn-daemon.sh script
* I cannot have different values for hadoop.root.logger and yarn.root.logger

I currently see two different ways to proceed forward:
1/ Make the script yarn-daemon.sh only sets a default value for YARN_ROOT_LOGGER if this variable is not defined
2/ Remove the quoted code from yarn-daemon.sh since yarn already does something similar
3/ Entirely remove that chunk and let people define their logging however they want through some properties files (see log4j.properties in the conf directories for instance)

I would also use the variable HADOOP_ROOT_LOGGER for hadoop.root.logger if either option 1/ or 2/ would be taken.

I don't really have any preference toward any of these solutions. What would you recommend? What is the Apache Hadoop way for this matter?

Note: This is probably happening as well for the other daemons, and I will take a look at it once this issue is resolved.",bmahe,bmahe,Major,Closed,Fixed,16/Nov/11 01:35,05/Mar/12 02:48
Bug,MAPREDUCE-3412,12531665,'ant docs' is broken,'ant docs' no longer work.,amar_kamat,amar_kamat,Major,Closed,Fixed,16/Nov/11 18:27,10/Mar/15 04:32
Bug,MAPREDUCE-3413,12531671,RM web ui applications not sorted in any order by default,,jeagles,jeagles,Minor,Closed,Fixed,16/Nov/11 19:27,10/Mar/15 04:32
Bug,MAPREDUCE-3417,12531692,job access controls not working app master and job history UI's,"tested with security on, no filters defined for httpserver, job acls set so that only I could view/modify the job.  Then went to the web ui to app master and job history server and both allowed me to view the job details.  The webui shows the user ""webuser"".   The RM properly rejected my request although it was using user ""Dr.Who"".    


The exception shown in the log is:
11/11/16 18:58:53 INFO mapred.JobACLsManager: job checkAccess user is: webuser
11/11/16 18:58:53 WARN security.ShellBasedUnixGroupsMapping: got exception trying to get groups for user webuser
org.apache.hadoop.util.Shell$ExitCodeException: id: webuser: No such user

        at org.apache.hadoop.util.Shell.runCommand(Shell.java:261)
        at org.apache.hadoop.util.Shell.run(Shell.java:188)
        at org.apache.hadoop.util.Shell$ShellCommandExecutor.execute(Shell.java:381)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:467)
        at org.apache.hadoop.util.Shell.execCommand(Shell.java:450)
        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getUnixGroups(ShellBasedUnixGroupsMapping.java:86)
        at org.apache.hadoop.security.ShellBasedUnixGroupsMapping.getGroups(ShellBasedUnixGroupsMapping.java:55)
        at org.apache.hadoop.security.Groups.getGroups(Groups.java:88)
        at org.apache.hadoop.security.UserGroupInformation.getGroupNames(UserGroupInformation.java:1043)
        at org.apache.hadoop.security.authorize.AccessControlList.isUserAllowed(AccessControlList.java:221)
        at org.apache.hadoop.mapred.JobACLsManager.checkAccess(JobACLsManager.java:103)
        at org.apache.hadoop.mapreduce.v2.hs.CompletedJob.checkAccess(CompletedJob.java:325)
        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.checkAccess(AppController.java:292)
        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.requireJob(AppController.java:313)
        at org.apache.hadoop.mapreduce.v2.app.webapp.AppController.job(AppController.java:97)",jeagles,tgraves,Blocker,Closed,Fixed,16/Nov/11 21:00,05/Mar/12 02:49
Bug,MAPREDUCE-3419,12531702,Don't mark exited TT threads as dead in MiniMRCluster  ,"MAPREDUCE-2850 flagged all TT threads that exited in the MiniMRCluster as dead, this breaks a number of the other tests that use MiniMRCluster across restart.",eli,eli,Major,Closed,Fixed,16/Nov/11 22:14,17/Oct/12 18:27
Bug,MAPREDUCE-3420,12531719,[Umbrella ticket] Make uber jobs functional,Umbrella jira for getting uber jobs to work correctly with YARN/MRv2,,hitesh,Major,Closed,Fixed,17/Nov/11 00:38,10/Mar/15 04:32
Bug,MAPREDUCE-3422,12531752,Counter display names are not being picked up,"When running a job I see ""MAP_INPUT_RECORDS"" rather than ""Map input records"" for the counter name. To fix this the resource bundle properties files need to be moved to the src/main/resources tree. ",jeagles,tomwhite,Major,Closed,Fixed,17/Nov/11 06:18,10/Mar/15 04:32
Bug,MAPREDUCE-3427,12531888,streaming tests fail with MR2,"After Mavenizing streaming and getting its testcases to use the MiniMRCluster wrapper (MAPREDUCE-3169), 4 testcases fail to pass.

Following is an assessment of those failures. Note that the testcases have been tweaked only to set the streaming JAR and yarn as the  framework.
 
(If these issues are unrelated we should create sub-tasks for each one of them).

*TestStreamingCombiner*, fails because returned counters don't match assertion. However, counters printed in the test output indicate values that would satisfy the assertion. As Tom has indicated it seems MR/YARN are not passing back counter information to the client API.

*TestStreamingBadRecords*, the job is failing with the following exception

{code}
Application application_1321575850006_0001 failed 1 times due to AM Container for 
appattempt_1321575850006_0001_000001 exited with  exitCode: 127 due to: 
.Failing this attempt.. Failing the application.
{code}

Difficult to troubleshoot because there are not task logs from Mini MR/YARN  run.

*TestStreamingStatus* fails in validateTaskStatus() in the following assertion

{code}
expected:<[before consuming input > sort]> but was:<[SUCCEEDED]>
{code}

*TestUlimit* fails with

{code}
org.junit.ComparisonFailure: output is wrong expected:<[786432]> but was:<[unlimited]>
{code}
",hitesh,tucu00,Blocker,Closed,Fixed,18/Nov/11 04:41,10/Mar/15 04:31
Bug,MAPREDUCE-3429,12531961,Few contrib tests are failing because of the missing commons-lang dependency,As the result of MAPREDUCE-3311 fix a transient {{commons-lang}} isn't available anymore to contrib tests. This causing silent failure with timeout. The problem is only seeing if tests are ran with {{-Dtest.output=yes}},cos,cos,Major,Closed,Fixed,18/Nov/11 15:54,12/Dec/11 06:19
Bug,MAPREDUCE-3431,12531963,NPE in Resource Manager shutdown,bringing up a resource manager failed; shutdown triggered an NPE,stevel@apache.org,stevel@apache.org,Minor,Closed,Fixed,18/Nov/11 16:24,10/Mar/15 04:31
Bug,MAPREDUCE-3434,12532012,Nightly build broken ,"https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Mapreduce-trunk/901/

Results :

Failed tests:   testSleepJob(org.apache.hadoop.mapreduce.v2.TestMRJobs)
  testRandomWriter(org.apache.hadoop.mapreduce.v2.TestMRJobs)
  testDistributedCache(org.apache.hadoop.mapreduce.v2.TestMRJobs)

Tests in error: 
  org.apache.hadoop.mapreduce.v2.TestMROldApiJobs: Failed to Start org.apache.hadoop.mapreduce.v2.TestMROldApiJobs
  org.apache.hadoop.mapreduce.v2.TestUberAM: Failed to Start org.apache.hadoop.mapreduce.v2.TestMRJobs

Likely due to either of:
MAPREDUCE-3415. improve MiniMRYarnCluster & DistributedShell JAR resolution (tucu)
MAPREDUCE-3169. Create a new MiniMRCluster equivalent which only provides client APIs cross MR1 and MR2. (Ahmed via tucu)
",hitesh,hitesh,Blocker,Closed,Fixed,18/Nov/11 23:13,10/Mar/15 04:32
Bug,MAPREDUCE-3436,12532032,JobHistory webapp address should use the host from the jobhistory address,"On the following page : http://<RESOURCE_MANAGER>:8088/cluster/apps
There are links to the history for each application. None of them can be reached since they all point to the ip 0.0.0.0. For instance:
http://0.0.0.0:8088/proxy/application_1321658790349_0002/jobhistory/job/job_1321658790349_2_2

Am I missing something?

[root@bigtop-fedora-15 ~]# jps
9968 ResourceManager
1495 NameNode
1645 DataNode
12935 Jps
11140 -- process information unavailable
5309 JobHistoryServer
10237 NodeManager

[root@bigtop-fedora-15 ~]# netstat -tlpn | grep 8088
tcp        0      0 :::8088                     :::*                        LISTEN      9968/java    

For reference, here is my configuration:
root@bigtop-fedora-15 ~]# cat /etc/yarn/conf/yarn-site.xml 
<?xml version=""1.0""?>
<configuration>

<!-- Site specific YARN configuration properties -->

   <property>
      <name>yarn.nodemanager.aux-services</name>
      <value>mapreduce.shuffle</value>
    </property>
    <property>
      <name>yarn.nodemanager.aux-services.mapreduce.shuffle.class</name>
      <value>org.apache.hadoop.mapred.ShuffleHandler</value>
    </property>
    <property>
      <name>mapreduce.admin.user.env</name>
      <value>CLASSPATH=/etc/hadoop/conf/*:/usr/lib/hadoop/*:/usr/lib/hadoop/lib/*</value>
    </property>

</configuration>


[root@bigtop-fedora-15 ~]# cat /etc/hadoop/conf/hdfs-site.xml 
<?xml version=""1.0""?>

<configuration>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
  </property>
  <property>
     <name>dfs.permissions</name>
     <value>false</value>
  </property>
  <property>
     <!-- specify this so that running 'hadoop namenode -format' formats the right dir -->
     <name>dfs.name.dir</name>
     <value>/var/lib/hadoop/cache/hadoop/dfs/name</value>
  </property>
</configuration>

[root@bigtop-fedora-15 ~]# cat /etc/hadoop/conf/core-site.xml 
<?xml version=""1.0""?>
<configuration>
  <property>
    <name>fs.default.name</name>
    <value>hdfs://localhost:8020</value>
  </property>

  <property>
     <name>hadoop.tmp.dir</name>
     <value>/var/lib/hadoop/cache/${user.name}</value>
  </property>

  <!-- OOZIE proxy user setting -->
  <property>
    <name>hadoop.proxyuser.oozie.hosts</name>
    <value>*</value>
  </property>
  <property>
    <name>hadoop.proxyuser.oozie.groups</name>
    <value>*</value>
  </property>

</configuration>
",ahmed.radwan,bmahe,Major,Closed,Fixed,19/Nov/11 02:29,05/Mar/12 02:48
Bug,MAPREDUCE-3437,12532037,Branch 23 fails to build with Failure to find org.apache.hadoop:hadoop-project:pom:0.24.0-SNAPSHOT,"[INFO] Scanning for projects...
[ERROR] The build could not read 1 project -> [Help 1]
[ERROR]   
[ERROR]   The project org.apache.hadoop:hadoop-mapreduce-examples:0.24.0-SNAPSHOT (/home/jeagles/hadoop/trunk/hadoop-mapreduce-project/hadoop-mapreduce-examples/pom.xml) has 1 error
[ERROR]     Non-resolvable parent POM: Failure to find org.apache.hadoop:hadoop-project:pom:0.24.0-SNAPSHOT in http://stormwalk.champ.corp.yahoo.com:8081/nexus/content/groups/public was cached in the local repository, resolution will not be reattempted until the update interval of nexus has elapsed or updates are forced and 'parent.relativePath' points at wrong local POM @ line 17, column 11 -> [Help 2]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/ProjectBuildingException
[ERROR] [Help 2] http://cwiki.apache.org/confluence/display/MAVEN/UnresolvableModelException
",jeagles,jeagles,Blocker,Closed,Fixed,19/Nov/11 06:06,05/Mar/12 02:49
Bug,MAPREDUCE-3438,12532107,"TestRaidNode fails because of ""Too many open files""",TestRaidNode fails because it opens many connections.,rvadali,shv,Major,Closed,Fixed,21/Nov/11 02:36,12/Dec/11 06:20
Bug,MAPREDUCE-3443,12532180,Oozie jobs are running as oozie user even though they create the jobclient as doAs.,"Oozie is having issues with job submission, since it does the following:

{code}
doAs(userwhosubmittedjob) {
 jobclient = new JobClient(jobconf);
}

jobclient.submitjob()

{code}

In 0.20.2** this works because the JT proxy is created as soon as we call new JobClient(). But in 0.23 this is no longer true since the client has to talk to multiple servers (AM/RM/JHS). To keep this behavior we will have to store the ugi in new JobClient() and make sure all the calls are run with a doAs() inside the jobclient.",mahadev,mahadev,Blocker,Closed,Fixed,21/Nov/11 17:39,05/Mar/12 02:49
Bug,MAPREDUCE-3444,12532190,trunk/0.23 builds broken ,"https://builds.apache.org/job/Hadoop-Mapreduce-0.23-Commit/208/ 
https://builds.apache.org/job/Hadoop-Mapreduce-trunk-Commit/1310/
",hitesh,hitesh,Blocker,Closed,Fixed,21/Nov/11 18:49,10/Mar/15 04:32
Bug,MAPREDUCE-3447,12532193,mapreduce examples not working,"Since the mavenization went in the mapreduce examples jar no longer works.  

$ hadoop jar ./hadoop-0.23.0-SNAPSHOT/modules/hadoop-mapreduce-examples-0.23.0-SNAPSHOT.jar  wordcount input output
Exception in thread ""main"" java.lang.ClassNotFoundException: wordcount
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:307)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:252)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:320)
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:247)
        at org.apache.hadoop.util.RunJar.main(RunJar.java:193)


",mahadev,tgraves,Blocker,Closed,Fixed,21/Nov/11 19:02,05/Mar/12 02:48
Bug,MAPREDUCE-3448,12532215,TestCombineOutputCollector javac unchecked warning on mocked generics,"  [javac] found   : org.apache.hadoop.mapred.IFile.Writer
    [javac] required: org.apache.hadoop.mapred.IFile.Writer<java.lang.String,java.lang.Integer>
    [javac]     Writer<String, Integer> mockWriter = mock(Writer.class);
    [javac]                                              ^
    [javac] /home/jeagles/hadoop/trunk/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestCombineOutputCollector.java:125: warning: [unchecked] unchecked conversion
    [javac] found   : org.apache.hadoop.mapred.IFile.Writer
    [javac] required: org.apache.hadoop.mapred.IFile.Writer<java.lang.String,java.lang.Integer>
    [javac]     Writer<String, Integer> mockWriter = mock(Writer.class);
    [javac]                                              ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] 2 warnings
",jeagles,jeagles,Minor,Closed,Fixed,21/Nov/11 20:58,10/Mar/15 04:32
Bug,MAPREDUCE-3450,12532260,NM port info no longer available in JobHistory,The NM RPC port used to be part of the hostname field in JobHistory. That seems to have gone missing. Required for the task log link on the history server.,sseth,sseth,Major,Closed,Fixed,22/Nov/11 03:26,05/Mar/12 02:49
Bug,MAPREDUCE-3452,12532276,fifoscheduler web ui page always shows 0% used for the queue,"When the fifo scheduler is configured to be on, go to the RM web ui page and click the scheduler link.  Hover over the default queue to see the used%.  It always shows used% as 0.0% even when jobs are running.",jeagles,tgraves,Major,Closed,Fixed,22/Nov/11 06:18,10/Mar/15 04:32
Bug,MAPREDUCE-3453,12532280,RM web ui application details page shows RM cluster about information,"Go to the RM Web ui page.  Click on the Applications link, then click on a particular application. The applications details page inadvertently includes the RM about page information after the application details:

Cluster ID: 	1321943597242
ResourceManager state: 	STARTED
ResourceManager started on: 	22-Nov-2011 06:33:17
ResourceManager version: 	0.23.0-SNAPSHOT from 1203458 by user source checksum 0c288fc0971ed28c970272a62f547eae on Tue Nov 22 06:31:09 UTC 2011
Hadoop version: 	0.23.0-SNAPSHOT from 1204629 by user source checksum 421c41e5cfbed4a9d473b123425ad94f on Tue Nov 22 06:29:17 UTC 2011 ",jeagles,tgraves,Major,Closed,Fixed,22/Nov/11 06:56,05/Mar/12 02:49
Bug,MAPREDUCE-3454,12532292,[Gridmix] TestDistCacheEmulation is broken,TestDistCacheEmulation is broken as 'MapReduceTestUtil' no longer exists.,hitesh,amar_kamat,Major,Closed,Fixed,22/Nov/11 09:01,10/Mar/15 04:32
Bug,MAPREDUCE-3456,12532409,$HADOOP_PREFIX/bin/yarn should set defaults for $HADOOP_*_HOME,"If the $HADOOP_PREFIX/hadoop-dist/target/hadoop-0.23.0-SNAPSHOT.tar.gz tarball is used to distribute hadoop, all of the HADOOP components (HDFS, MAPRED, COMMON) are all under one directory. In this use case, HADOOP_PREFIX should be set and should point to the root directory for all components, and it should not be necessary to set HADOOP_HDFS_HOME, HADOOP_COMMON_HOME, and HADOOP_MAPRED_HOME. However, the $HADOOP_PREFIX/bin/yarn script requires these 3 to be set explicitly in the calling environment or it won't run.

$HADOOP_PREFIX/bin/yarn should check if $HADOOP_PREFIX is set and, if it is, use that value for the 3 other HADOOP_*_HOME variables.",epayne,epayne,Blocker,Closed,Fixed,22/Nov/11 23:31,05/Mar/12 02:49
Bug,MAPREDUCE-3458,12532421,Fix findbugs warnings in hadoop-examples,"I see 12 findbugs warnings in hadoop-examples: 
https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1336//artifact/trunk/hadoop-mapreduce-project/patchprocess/newPatchFindbugsWarningshadoop-mapreduce-examples.html",devaraj,acmurthy,Major,Closed,Fixed,23/Nov/11 01:47,05/Mar/12 02:49
Bug,MAPREDUCE-3460,12532429,MR AM can hang if containers are allocated on a node blacklisted by the AM,"When an AM is assigned a FAILED_MAP (priority = 5) container on a nodemanager which it has blacklisted - it tries to
find a corresponding container request.
This uses the hostname to find the matching container request - and can end up returning any of the ContainerRequests which may have requested a container on this node. This container request is cleaned to remove the bad node - and then added back to the RM 'ask' list.
The AM cleans the 'ask' list after each heartbeat - The RM Allocator is still aware of the priority=5 container (in 'remoteRequestsTable') - but this never gets added back to the 'ask' set - which is what is sent to the RM.",revans2,sseth,Blocker,Closed,Fixed,23/Nov/11 04:16,10/Mar/15 04:31
Bug,MAPREDUCE-3462,12532440,Job submission failing in JUnit tests,"When I run JUnit tests (e.g. TestDistCacheEmulation, TestSleepJob and TestCompressionEmulationUtils), I see job submission failing with the following error:
{noformat}
java.lang.IllegalStateException: Variable substitution depth too large: 20 ${fs.default.name}
        at org.apache.hadoop.conf.Configuration.substituteVars(Configuration.java:551)
        at org.apache.hadoop.conf.Configuration.get(Configuration.java:569)
        at org.apache.hadoop.conf.Configuration.getStrings(Configuration.java:1020)
        at org.apache.hadoop.mapreduce.JobSubmitter.populateTokenCache(JobSubmitter.java:564)
        at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:353)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1159)
        at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1156)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1152)
        at org.apache.hadoop.mapreduce.Job.submit(Job.java:1156)
        at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1176)
        at org.apache.hadoop.mapred.gridmix.Gridmix.launchGridmixJob(Gridmix.java:190)
        at org.apache.hadoop.mapred.gridmix.Gridmix.writeInputData(Gridmix.java:150)
        at org.apache.hadoop.mapred.gridmix.Gridmix.start(Gridmix.java:425)
        at org.apache.hadoop.mapred.gridmix.Gridmix.runJob(Gridmix.java:380)
        at org.apache.hadoop.mapred.gridmix.Gridmix.access$000(Gridmix.java:56)
        at org.apache.hadoop.mapred.gridmix.Gridmix$1.run(Gridmix.java:313)
        at org.apache.hadoop.mapred.gridmix.Gridmix$1.run(Gridmix.java:311)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1152)
        at org.apache.hadoop.mapred.gridmix.Gridmix.run(Gridmix.java:311)
{noformat}",raviprak,amar_kamat,Blocker,Closed,Fixed,23/Nov/11 09:06,05/Mar/12 02:48
Bug,MAPREDUCE-3463,12532471,Second AM fails to recover properly when first AM is killed with java.lang.IllegalArgumentException causing lost job,"Set yarn.resourcemanager.am.max-retries=5 in yarn-site.xml. Started yarn 4 Node cluster.
First Ran Randowriter/Sort/Sort-validate successfully
Then again sort, when job was 50% complete
Login node running AppMaster, and killed AppMaster with kill -9
On Client side failed with following:
{code}
11/11/23 10:57:27 INFO mapreduce.Job:  map 58% reduce 8%
11/11/23 10:57:27 INFO mapred.ClientServiceDelegate: Failed to contact AM/History for job job_1322040898409_0005 retrying..
11/11/23 10:57:28 INFO mapreduce.Job:  map 0% reduce 0%
11/11/23 10:57:37 INFO mapred.ClientServiceDelegate: Application state is completed. FinalApplicationStatus=UNDEFINED. Redirecting to job history server
11/11/23 10:57:37 INFO client.ClientTokenSelector: Looking for a token with service <RM Host>:Port
11/11/23 10:57:37 INFO client.ClientTokenSelector: Token kind is YARN_CLIENT_TOKEN and the token's service name is <New AM Host>:Port
11/11/23 10:57:38 WARN mapred.ClientServiceDelegate: Error from remote end: Unknown job job_1322040898409_0005
RemoteTrace: 
 at Local Trace: 
	org.apache.hadoop.yarn.exceptions.impl.pb.YarnRemoteExceptionPBImpl: Unknown job job_1322040898409_0005
	at org.apache.hadoop.yarn.ipc.ProtoOverHadoopRpcEngine$Invoker.invoke(ProtoOverHadoopRpcEngine.java:151)
	at $Proxy10.getTaskAttemptCompletionEvents(Unknown Source)
	at org.apache.hadoop.mapreduce.v2.api.impl.pb.client.MRClientProtocolPBClientImpl.getTaskAttemptCompletionEvents(MRClientProtocolPBClientImpl.java:172)
	at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.mapred.ClientServiceDelegate.invoke(ClientServiceDelegate.java:273)
	at org.apache.hadoop.mapred.ClientServiceDelegate.getTaskCompletionEvents(ClientServiceDelegate.java:320)
	at org.apache.hadoop.mapred.YARNRunner.getTaskCompletionEvents(YARNRunner.java:438)
	at org.apache.hadoop.mapreduce.Job.getTaskCompletionEvents(Job.java:621)
	at org.apache.hadoop.mapreduce.Job.monitorAndPrintJob(Job.java:1231)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1179)
	at org.apache.hadoop.examples.Sort.run(Sort.java:181)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
	at org.apache.hadoop.examples.Sort.main(Sort.java:192)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.ProgramDriver$ProgramDescription.invoke(ProgramDriver.java:72)
	at org.apache.hadoop.util.ProgramDriver.driver(ProgramDriver.java:144)
	at org.apache.hadoop.examples.ExampleDriver.main(ExampleDriver.java:68)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:189)
{code}

On lookig RM logs found second AM was also lauched, it was saying -:
{code}
011-11-23 10:57:37,737 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.attempt.RMAppAttemptImpl: appattempt_1322040898409_0005_000002 State change from RUNNING to FINISHED
2011-11-23 10:57:37,737 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: Processing event for application_1322040898409_0005 of type ATTEMPT_FINISHED
2011-11-23 10:57:37,737 INFO org.apache.hadoop.yarn.server.resourcemanager.rmapp.RMAppImpl: application_1322040898409_0005 State change from RUNNING to FINISHED
2011-11-23 10:57:37,737 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: Application appattempt_1322040898409_0005_000002 is done. finalState=FINISHED
{code}

Now looking at AM logs and found Second AM was shutdown gracefully due to :-
{code}
2011-11-23 10:57:37,640 INFO [AsyncDispatcher event handler] org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService: Sending assigned event to attempt_1322040898409_0005_m_000000_0
2011-11-23 10:57:37,641 FATAL [AsyncDispatcher event handler] org.apache.hadoop.yarn.event.AsyncDispatcher: Error in dispatcher thread. Exiting..
java.lang.IllegalArgumentException: Invalid NodeId [<NMHostName>]. Expected host:port
        at org.apache.hadoop.yarn.util.ConverterUtils.toNodeId(ConverterUtils.java:144)
        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.sendAssignedEvent(RecoveryService.java:410)
        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$InterceptingEventHandler.handle(RecoveryService.java:314)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:1010)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl$RequestContainerTransition.transition(TaskAttemptImpl.java:985)
        at org.apache.hadoop.yarn.state.StateMachineFactory$SingleInternalArc.doTransition(StateMachineFactory.java:357)
        at org.apache.hadoop.yarn.state.StateMachineFactory.doTransition(StateMachineFactory.java:298)
        at org.apache.hadoop.yarn.state.StateMachineFactory.access$300(StateMachineFactory.java:43)
        at org.apache.hadoop.yarn.state.StateMachineFactory$InternalStateMachine.doTransition(StateMachineFactory.java:443)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:851)
        at org.apache.hadoop.mapreduce.v2.app.job.impl.TaskAttemptImpl.handle(TaskAttemptImpl.java:128)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:853)
        at org.apache.hadoop.mapreduce.v2.app.MRAppMaster$TaskAttemptEventDispatcher.handle(MRAppMaster.java:845)
        at org.apache.hadoop.yarn.event.AsyncDispatcher.dispatch(AsyncDispatcher.java:116)
        at org.apache.hadoop.mapreduce.v2.app.recover.RecoveryService$RecoveryDispatcher.dispatch(RecoveryService.java:270)
        at org.apache.hadoop.yarn.event.AsyncDispatcher$1.run(AsyncDispatcher.java:75)
        at java.lang.Thread.run(Thread.java:619)
2011-11-23 10:57:37,642 INFO [CompositeServiceShutdownHook for org.apache.hadoop.mapreduce.v2.app.MRAppMaster] org.apache.hadoop.mapreduce.jobhistory.JobHistoryEventHandler: Stopping JobHistoryEventHandler
{code}",sseth,karams,Blocker,Closed,Fixed,23/Nov/11 13:45,05/Mar/12 02:49
Bug,MAPREDUCE-3464,12532508,mapreduce jsp pages missing DOCTYPE [post-split branches],"Some jsp pages in the UI are missing a DOCTYPE declaration. This causes the pages to render incorrectly on some browsers, such as IE9. Please see parent bug HADOOP-7827 for details and patch.",davevr,davevr,Trivial,Closed,Fixed,23/Nov/11 18:07,05/Mar/12 02:48
Bug,MAPREDUCE-3465,12532517,org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin fails on 0.23 ,"Running org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin
Tests run: 2, Failures: 0, Errors: 2, Skipped: 0, Time elapsed: 0.121 sec <<< FAILURE!
Tests in error: 
  testParsingProcStatAndCpuFile(org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin): /homes/hortonhi/dev/hadoop-common/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/target/test-dir/CPUINFO_943711651 (No such file or directory)
  testParsingProcMemFile(org.apache.hadoop.yarn.util.TestLinuxResourceCalculatorPlugin): /homes/hortonhi/dev/hadoop-common/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-common/target/test-dir/MEMINFO_943711651 (No such file or directory)
",hitesh,hitesh,Minor,Closed,Fixed,23/Nov/11 19:12,05/Mar/12 02:48
Bug,MAPREDUCE-3475,12532927,JT can't renew its own tokens,"When external systems submit jobs whose tasks need to submit additional jobs (such as oozie/pig), they include their own MR token used to submit the job.  The token's renewer may not allow the JT to renew the token.  The JT log will include very long SASL/GSSAPI exceptions when the job is submitted.  It is also dubious for the JT to renew its token because it renders the expiry as meaningless since the JT will renew its own token until the max lifetime is exceeded.

After speaking with Owen & Jitendra, the immediate solution is for the JT to not attempt to renew its own tokens.",daryn,daryn,Major,Resolved,Fixed,28/Nov/11 16:04,10/Mar/15 02:50
Bug,MAPREDUCE-3477,12532982,Hadoop site documentation cannot be built anymore on trunk and branch-0.23,"Maven fails and here is the issue I get:

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-site-plugin:3.0:site (default-site) on project hadoop-yarn-site: Error during page generation: Error parsing '/home/bruno/freesoftware/bigtop/build/hadoop/rpm/BUILD/apache-hadoop-common-e127450/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-site/src/site/apt/SingleCluster.apt.vm': line [23] Unable to execute macro in the APT document: ParseException: expected SECTION2, found SECTION3 -> [Help 1]
[ERROR] 
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e switch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR] 
[ERROR] For more information about the errors and possible solutions, please read the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoExecutionException
[ERROR] 
[ERROR] After correcting the problems, you can resume the build with the command
[ERROR]   mvn <goals> -rf :hadoop-yarn-site
error: Bad exit status from /var/tmp/rpm-tmp.OFegWv (%build)

",jeagles,bmahe,Major,Closed,Fixed,28/Nov/11 20:44,10/Mar/15 04:31
Bug,MAPREDUCE-3478,12533038,Cannot build against ZooKeeper 3.4.0,"I tried to see if one could build Hadoop 0.23.0 against ZooKeeper 3.4.0, rather than 3.3.1 (3.3.3 does work, fwiw) and hit compilation errors:

{quote}
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.3.2:testCompile (default-testCompile) on project hadoop-yarn-server-common: Compilation failure: Compilation failure:
[ERROR] /Volumes/EssEssDee/abayer/src/asf-git/hadoop-common/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/lib/TestZKClient.java:[48,25] cannot find symbol
[ERROR] symbol  : class Factory
[ERROR] location: class org.apache.zookeeper.server.NIOServerCnxn
[ERROR] /Volumes/EssEssDee/abayer/src/asf-git/hadoop-common/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-common/src/test/java/org/apache/hadoop/yarn/lib/TestZKClient.java:[150,33] cannot find symbol
[ERROR] symbol  : class Factory
[ERROR] location: class org.apache.zookeeper.server.NIOServerCnxn
[ERROR] -> [Help 1]
{quote}

Presumably, Yarn needs to build against newer ZK releases eventually, hence this bug. =)",tomwhite,abayer,Minor,Closed,Fixed,29/Nov/11 01:03,05/Mar/12 02:49
Bug,MAPREDUCE-3479,12533041,JobClient#getJob cannot find local jobs,"The problem is that JobClient#submitJob doesn't pass the Cluster object to Job for the submission process, which means that two Cluster objects and two LocalJobRunner objects are created. LocalJobRunner keeps an instance map of job IDs to Jobs, and when JobClient#getJob is called the LocalJobRunner with the unpopulated map is used which results in the job not being found.",tomwhite,tomwhite,Major,Closed,Fixed,29/Nov/11 01:10,05/Mar/12 02:49
Bug,MAPREDUCE-3480,12533079,TestJvmReuse fails in 1.0,"TestJvmReuse is failing in apache builds, although it passes in my local machine.",jnp,jnp,Major,Closed,Fixed,29/Nov/11 08:08,06/Feb/13 04:12
Bug,MAPREDUCE-3484,12533204,JobEndNotifier is getting interrupted before completing all its retries.,"We noticed JobEndNotifier was getting an InterruptedException before completing all its retries.

To fix this, Job end notification method should be called before stop() in handle(JobFinishEvent).",raviprak,raviprak,Major,Closed,Fixed,29/Nov/11 22:04,05/Mar/12 02:49
Bug,MAPREDUCE-3487,12533330,jobhistory web ui task counters no longer links to singletakecounter page,"The task counters on the job history task counter web ui page ( ie host:19888/jobhistory/taskcounters/task_1322451030861_9_9_m_0) are no longer links. They are supposed to be links that take you to the singletaskcounter page and show you the task attempts that affected the counter.

Looks like MAPREDUCE-3258  changed CounterBlock.java so it doesn't show the link on the counter:
 if (mg == null && rg == null) {
           groupRow.td().$title(counter.getName())._(counter.getDisplayName()).
            _();
          } else {


",jlowe,tgraves,Critical,Closed,Fixed,30/Nov/11 19:01,05/Mar/12 02:49
Bug,MAPREDUCE-3488,12533337,Streaming jobs are failing because the main class isnt set in the pom files.,Streaming jobs are failing since the main MANIFEST file isnt being set in the pom files.,mahadev,mahadev,Blocker,Closed,Fixed,30/Nov/11 19:48,05/Mar/12 02:49
Bug,MAPREDUCE-3490,12533389,RMContainerAllocator counts failed maps towards Reduce ramp up,"The RMContainerAllocator does not differentiate between failed and successful maps while calculating whether reduce tasks are ready to launch. Failed tasks are also counted towards total completed tasks. 
Example. 4 failed maps, 10 total maps. Map%complete = 4/14 * 100 instead of being 0.",sharadag,sseth,Blocker,Closed,Fixed,01/Dec/11 01:17,05/Mar/12 02:49
Bug,MAPREDUCE-3491,12533404,TestContainerManagerWithLCE is failing,"$ mvn test -Dtest=TestContainerManagerWithLCE -Dapplication.submitter=nobody -Dyarn.nodemanager.linux-container-executor.path=<path of container-executor binary>


TestContainerManagerWithLCE is failing with the error:

Test set: org.apache.hadoop.yarn.server.nodemanager.TestContainerManagerWithLCE
-------------------------------------------------------------------------------
Tests run: 6, Failures: 5, Errors: 0, Skipped: 0, Time elapsed: 26.219 sec <<< FAILURE!
testContainerSetup(org.apache.hadoop.yarn.server.nodemanager.TestContainerManagerWithLCE)  Time elapsed: 2.476 sec  <<< FAILURE!
junit.framework.AssertionFailedError: workspace/gitTrunk/hadoop-common/hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-server/hadoop-yarn-server-nodemanager/target/org.apache.hadoop.yarn.server.nodemanager.TestContainerManagerWithLCE-localDir/usercache/nobody/appcache/application_0_0000 doesn't exist!!
  at junit.framework.Assert.fail(Assert.java:47)
  at junit.framework.Assert.assertTrue(Assert.java:20)
  at org.apache.hadoop.yarn.server.nodemanager.containermanager.TestContainerManager.testContainerSetup(TestContainerManager.java:179)
  at org.apache.hadoop.yarn.server.nodemanager.TestContainerManagerWithLCE.testContainerSetup(TestContainerManagerWithLCE.java:83)",,ravidotg,Major,Resolved,Fixed,01/Dec/11 06:08,10/Mar/15 04:31
Bug,MAPREDUCE-3493,12533415,Add the default mapreduce.shuffle.port property to mapred-default.xml,"I faced this issue when trying to run multiple Hadoop MR2 instances on the same node. The default value for this property is hardcoded in the ShuffleHandler.java class so it results in port conflicts. The issue is resolved if you set the property value in your conf files. But the absence of this property from *-default.xml files is confusing. So It'll be cleaner to move this property to mapred-default.xml, so its default value can be easily identified and changed if needed. ",,ahmed.radwan,Minor,Closed,Fixed,01/Dec/11 09:35,11/Oct/12 17:48
Bug,MAPREDUCE-3496,12533481,Yarn initializes ACL operations from capacity scheduler config in a non-deterministic order,'mapred queue -showacls' does not output put acls in a predictable manner. This is a regression from previous versions.,jeagles,jeagles,Major,Closed,Fixed,01/Dec/11 18:01,10/Mar/15 04:31
Bug,MAPREDUCE-3497,12533523,missing documentation for yarn cli and subcommands - similar to commands_manual.html,the yarn cli and sub-commands aren't documented anywhere.  Should have documentation similar to the commands_manual.html,tgraves,tgraves,Major,Resolved,Fixed,01/Dec/11 22:02,05/Mar/12 13:25
Bug,MAPREDUCE-3499,12533548,"New MiniMR does not setup proxyuser configuration correctly, thus tests using doAs do not work","The new MiniMR implementation is not taking proxyuser settings.

Because of this, testcases using/testing doAs functionality fail.

This affects all Oozie testcases that use MiniMR.",johnvijoe,tucu00,Blocker,Closed,Fixed,02/Dec/11 00:04,10/Mar/15 04:33
Bug,MAPREDUCE-3500,12533554,MRJobConfig creates an LD_LIBRARY_PATH using the platform ARCH,"With HADOOP-7874 we are removing the arch from the java.library.path.

The LD_LIBRARY_PATH being set should not include the ARCH.

{code}
  public static final String DEFAULT_MAPRED_ADMIN_USER_ENV =
      ""LD_LIBRARY_PATH=$HADOOP_COMMON_HOME/lib/native/"" + PlatformName.getPlatformName();
{code}

",tucu00,tucu00,Major,Closed,Fixed,02/Dec/11 00:51,10/Mar/15 04:33
Bug,MAPREDUCE-3505,12533704,yarn APPLICATION_CLASSPATH needs to be overridable,"Right now MRApps sets the classpath to just being mrapp-generated-classpath, its content and a hardcoded list of directories.
If I understand correctly mrapp-generated-classpath is only there for testing and may change or disappear at any time

The list of hardcoded directories is defined in hadoop-mapreduce-project/hadoop-yarn/hadoop-yarn-api/src/main/java/org/apache/hadoop/yarn/api/ApplicationConstants.java at line 92.
For convenience, here is its current content:
{noformat}
  /**
   * Classpath for typical applications.
   */
  public static final String[] APPLICATION_CLASSPATH =
      new String[] {
        ""$HADOOP_CONF_DIR"",
        ""$HADOOP_COMMON_HOME/share/hadoop/common/*"",
        ""$HADOOP_COMMON_HOME/share/hadoop/common/lib/*"",
        ""$HADOOP_HDFS_HOME/share/hadoop/hdfs/*"",
        ""$HADOOP_HDFS_HOME/share/hadoop/hdfs/lib/*"",
        ""$YARN_HOME/modules/*"",
        ""$YARN_HOME/lib/*""
      };
{noformat}

Not all deployment scenarii fit in this layout and therefore we need a standardized way to customize this class path.
",ahmed.radwan,bmahe,Major,Closed,Fixed,02/Dec/11 23:59,05/Mar/12 02:49
Bug,MAPREDUCE-3506,12533748,Calling getPriority on JobInfo after parsing a history log with JobHistoryParser throws a NullPointerException,Somehow the priority field under JobHistoryParser.JobInfo is not set. Calling getPriority on Jobinfo after parsing a Job hisotry log throws NPE,jlowe,rdsr,Minor,Closed,Fixed,03/Dec/11 20:51,11/Oct/12 17:48
Bug,MAPREDUCE-3510,12533942,Capacity Scheduler inherited ACLs not displayed by mapred queue -showacls,mapred queue -showacls does not show inherited acls,jeagles,jeagles,Major,Closed,Fixed,05/Dec/11 21:20,10/Mar/15 04:32
Bug,MAPREDUCE-3513,12533973,Capacity Scheduler web UI has a spelling mistake for Memory.,"The web page for capacity scheduler has a column named ""Memopry Total"", a spelling mistake which needs to be fixed.",chaku88,mahadev,Trivial,Closed,Fixed,06/Dec/11 01:46,05/Mar/12 02:48
Bug,MAPREDUCE-3518,12534270,mapred queue -info <queue> -showJobs throws NPE,"mapred queue -info default -showJobs

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.hadoop.mapreduce.tools.CLI.displayJobList(CLI.java:572)
        at org.apache.hadoop.mapred.JobQueueClient.displayQueueInfo(JobQueueClient.java:190)
        at org.apache.hadoop.mapred.JobQueueClient.run(JobQueueClient.java:103)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
        at org.apache.hadoop.mapred.JobQueueClient.main(JobQueueClient.java:234)
",jeagles,jeagles,Critical,Closed,Fixed,07/Dec/11 22:49,10/Mar/15 04:31
Bug,MAPREDUCE-3521,12534410,Hadoop Streaming ignores unknown parameters,"The hadoop streaming command will ignore any command line arguments to it.

{code}
hadoop jar streaming.jar -input input -output output -mapper cat -reducer cat ThisIsABadArgument
{code}

Works just fine.  This can mask issues where quotes were mistakenly missed like

{code}
hadoop jar streaming.jar -input input -output output -mapper xargs cat -reducer cat -archive someArchive.tgz
{code}

Streaming should fail if it encounters an unexpected command line parameter",revans2,revans2,Minor,Closed,Fixed,08/Dec/11 22:12,05/Mar/12 02:49
Bug,MAPREDUCE-3522,12534413,Capacity Scheduler ACLs not inherited by default,"Hierarchical Queues do not inherit parent ACLs correctly by default. Instead, if no value is specified for submit or administer acls, then all access is granted.",jeagles,jeagles,Major,Closed,Fixed,08/Dec/11 23:15,05/Mar/12 02:48
Bug,MAPREDUCE-3527,12534572,Fix minor API incompatibilities between 1.0 and 0.23,There are a few minor incompatibilities that were found in HADOOP-7738 and are straightforward to fix.,tomwhite,tomwhite,Major,Closed,Fixed,10/Dec/11 01:26,05/Mar/12 02:48
Bug,MAPREDUCE-3528,12534583,The task timeout check interval should be configurable independent of mapreduce.task.timeout,TaskHeartbeatHandler sleeps for 'mapreduce.task.timeout' - between each check. If a task/NM goes bad immediately after starting a task - the timeout is detected in ~2x the configured timeout interval.,sseth,sseth,Major,Closed,Fixed,10/Dec/11 03:31,05/Mar/12 02:49
Bug,MAPREDUCE-3529,12534584,TokenCache does not cache viewfs credentials correctly,"viewfs returns a list of delegation tokens for the actual namenodes. TokenCache caches these based on the actual service name - subsequent calls to TokenCache end up trying to get a new set of tokens.

Tasks which happen to access TokenCache fail when using viewfs - since they end up trying to get a new set of tokens even though the tokens are already available.

{noformat}
Error: java.io.IOException: Delegation Token can be issued only with kerberos or web authentication
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getDelegationToken(FSNamesystem.java:4027)
        at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getDelegationToken(NameNodeRpcServer.java:281)
        at sun.reflect.GeneratedMethodAccessor38.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:365)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1490)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1486)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1152)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1484)

        at org.apache.hadoop.ipc.Client.call(Client.java:1085)
        at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:193)
        at $Proxy8.getDelegationToken(Unknown Source)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:100)
        at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:65)
        at $Proxy8.getDelegationToken(Unknown Source)
        at org.apache.hadoop.hdfs.DFSClient.getDelegationToken(DFSClient.java:456)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationToken(DistributedFileSystem.java:812)
        at org.apache.hadoop.hdfs.DistributedFileSystem.getDelegationTokens(DistributedFileSystem.java:839)
        at org.apache.hadoop.fs.viewfs.ChRootedFileSystem.getDelegationTokens(ChRootedFileSystem.java:311)
        at org.apache.hadoop.fs.viewfs.ViewFileSystem.getDelegationTokens(ViewFileSystem.java:490)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:144)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodesInternal(TokenCache.java:91)
        at org.apache.hadoop.mapreduce.security.TokenCache.obtainTokensForNamenodes(TokenCache.java:84)
{noformat}


This will likely require some changes in viewfs/hdfs - will open a Jira with details.",sseth,sseth,Critical,Closed,Fixed,10/Dec/11 03:46,05/Mar/12 02:49
Bug,MAPREDUCE-3530,12534740,Sometimes NODE_UPDATE to the scheduler throws an NPE causing the scheduling to stop,"Sometimes NODE_UPDATE to the scheduler throws NPE causes scheduling to stop but ResourceManager keeps on running.
I have been observing intermitently for last 3 weeks.
But with latest svn code. I tried to run sort twice and both times Job got stuck due to NPE.
{code}
java.lang.NullPointerException
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.SchedulerApp.containerLaunchedOnNode(SchedulerApp.java:181)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.containerLaunchedOnNode(CapacityScheduler.java:596)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:539)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:617)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:77)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:294)
        at java.lang.Thread.run(Thread.java:619)
{code}",acmurthy,karams,Blocker,Closed,Fixed,12/Dec/11 05:36,05/Mar/12 02:49
Bug,MAPREDUCE-3531,12534741,Sometimes java.lang.IllegalArgumentException: Invalid key to HMAC computation in NODE_UPDATE also causing RM to stop scheduling ,"Filling this Jira a bit late
Started 350 cluster
sbummited large sleep job.
Foud that job was not running as RM has not allocated resouces to it.
{code}
2011-12-01 11:56:25,200 INFO org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler: nodeUpdate: <NMHost>:48490 clusterResources: memory: 3225600
2011-12-01 11:56:25,202 ERROR org.apache.hadoop.yarn.server.resourcemanager.ResourceManager: Error in handling event
type NODE_UPDATE to the scheduler
java.lang.IllegalArgumentException: Invalid key to HMAC computation
        at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:141)
        at org.apache.hadoop.yarn.server.security.ContainerTokenSecretManager.createPassword(ContainerTokenSecretManager.java:61)
        atorg.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.createContainer(LeafQueue.java:1108)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.getContainer(LeafQueue.java:1091)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainer(LeafQueue.java:1137)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignNodeLocalContainers(LeafQueue.java:1001)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainersOnNode(LeafQueue.java:973)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.LeafQueue.assignContainers(LeafQueue.java:760)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainersToChildQueues(ParentQueue.java:583)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.ParentQueue.assignContainers(ParentQueue.java:513)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.nodeUpdate(CapacityScheduler.java:569)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:611)
        at org.apache.hadoop.yarn.server.resourcemanager.scheduler.capacity.CapacityScheduler.handle(CapacityScheduler.java:77)
        at org.apache.hadoop.yarn.server.resourcemanager.ResourceManager$SchedulerEventDispatcher$EventProcessor.run(ResourceManager.java:294)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.security.InvalidKeyException: Secret key expected
        at com.sun.crypto.provider.HmacCore.a(DashoA13*..)
        at com.sun.crypto.provider.HmacSHA1.engineInit(DashoA13*..)
        at javax.crypto.Mac.init(DashoA13*..)
        at org.apache.hadoop.security.token.SecretManager.createPassword(SecretManager.java:139)
        ... 14 more
{code}
As this stack is from 30 Nov checkou line number may be different",revans2,karams,Blocker,Closed,Fixed,12/Dec/11 05:42,05/Mar/12 02:49
Bug,MAPREDUCE-3532,12534743,"When 0 is provided as port number in yarn.nodemanager.webapp.address, NMs webserver component picks up random port, NM keeps on Reporting 0 port to RM","I tried following -:
yarn.nodemanager.address=0.0.0.0:0
yarn.nodemanager.webapp.address=0.0.0.0:0
yarn.nodemanager.localizer.address=0.0.0.0:0
mapreduce.shuffle.port=0

When 0 is provided as number in yarn.nodemanager.webapp.address. 
NM instantiate WebServer as 0 piort e.g.
{code}
2011-12-08 11:33:02,467 INFO org.apache.hadoop.yarn.server.nodemanager.webapp.WebServer: Instantiating NMWebApp at 0.0.0.0:0
{code}

After that WebServer pick up some random port e.g.
{code}
2011-12-08 11:33:02,562 INFO org.apache.hadoop.http.HttpServer: Jetty bound to port 36272
2011-12-08 11:33:02,562 INFO org.mortbay.log: jetty-6.1.26
2011-12-08 11:33:02,831 INFO org.mortbay.log: Started SelectChannelConnector@0.0.0.0:36272
2011-12-08 11:33:02,831 INFO org.apache.hadoop.yarn.webapp.WebApps: Web app /node started at 36272
{code}

And NM WebServer responds correctly but
 RM's cluster/Nodes page shows the following -:
{code}
/Rack RUNNING NM:57963 NM:0 Healthy 8-Dec-2011 11:33:01 Healthy 8 12 GB 0 KB
{code}
Whereas NM:0 is not clickable.
Seems even NM's webserver pick random port but it never gets updated and so NM report 0 as HTTP port to RM causing NM Hyperlinks un-clickable
But verified that MR job runs successfully with random.
",kamesh,karams,Critical,Closed,Fixed,12/Dec/11 05:59,05/Mar/12 02:49
Bug,MAPREDUCE-3537,12534890,DefaultContainerExecutor has a race condn. with multiple concurrent containers,"DCE relies cwd before calling ContainerLocalizer.runLocalization. However, with multiple containers setting cwd on same localFS reference leads to race. ",acmurthy,acmurthy,Blocker,Closed,Fixed,13/Dec/11 01:25,05/Mar/12 02:48
Bug,MAPREDUCE-3541,12534981,Fix broken TestJobQueueClient test,"Ant build complains 
    [javac] /hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/mapred/TestJobQueueClient.java>:80: printJobQueueInfo(org.apache.hadoop.mapred.JobQueueInfo,java.io.Writer,java.lang.String) in org.apache.hadoop.mapred.JobQueueClient cannot be applied to (org.apache.hadoop.mapred.JobQueueInfo,java.io.StringWriter)
    [javac]     client.printJobQueueInfo(root, writer);
",raviprak,raviprak,Blocker,Closed,Fixed,13/Dec/11 16:53,05/Mar/12 02:49
Bug,MAPREDUCE-3542,12535038,"Support ""FileSystemCounter"" legacy counter group name for compatibility","The group name changed from ""FileSystemCounter"" to ""org.apache.hadoop.mapreduce.FileSystemCounter"", but we should support the old one for compatibility's sake. This came up in PIG-2347. ",tomwhite,tomwhite,Major,Closed,Fixed,13/Dec/11 21:39,13/Aug/12 08:10
Bug,MAPREDUCE-3543,12535055,Mavenize Gridmix.,Gridmix codebase still resides in src/contrib and needs to be compiled via ant. We should move it to maven.,tgraves,mahadev,Critical,Closed,Fixed,13/Dec/11 23:13,02/May/13 02:29
Bug,MAPREDUCE-3544,12535065,"gridmix build is broken, requires hadoop-archives to be added as ivy dependency","Having moved HAR/HadoopArchives to common/tools makes gridmix to fail as HadoopArchives is not in the mr1 classpath anymore.

hadoop-archives artifact should be added to gridmix dependencies
",tucu00,tucu00,Major,Closed,Fixed,14/Dec/11 00:12,10/Mar/15 04:32
Bug,MAPREDUCE-3545,12535076,Remove Avro RPC,"Please see the discussion in HDFS-2660 for more details. I have created a branch HADOOP-6659 to save the Avro work, if in the future some one wants to use the work that existed to add support for Avro RPC.",sureshms,sureshms,Major,Closed,Fixed,14/Dec/11 02:11,02/May/13 02:29
Bug,MAPREDUCE-3549,12535081,"write api documentation for web service apis for RM, NM, mapreduce app master, and job history server","write api documentation for web service apis for RM, NM, mapreduce app master, and job history server. web services were added in MAPREDUCE-2863",tgraves,tgraves,Blocker,Closed,Fixed,14/Dec/11 02:44,02/May/13 02:29
Bug,MAPREDUCE-3551,12535084,web proxy returns internal server error when application invalid,"querying for an absent app i.e. say http://RM:8088/proxy/application_1323054018624_0004/ws/v1/mapreduce/info - returns a 500 with a stack trace instead of a 404. From an api spec point of view, this is not really an error but just a not found.  This occurs with web services as well as any web ui page.",jeagles,tgraves,Major,Resolved,Fixed,14/Dec/11 02:47,15/Apr/14 20:41
Bug,MAPREDUCE-3557,12535168,MR1 test fail to compile because of missing hadoop-archives dependency,"MAPREDUCE-3544 added hadoop-archives as dependency to gridmix and raid, but missed to add it to the main ivy.xml for the MR1 testcases thus the ant target 'compile-mapred-test' fails.

I was under the impression that this stuff was not used anymore but trunk is failing on that target.",tucu00,tucu00,Major,Closed,Fixed,14/Dec/11 14:47,10/Mar/15 04:31
Bug,MAPREDUCE-3560,12535233,TestRMNodeTransitions is failing on trunk,"Apparently Jenkins is screwed up. It is happily blessing patches, even though tests are failing.

Link to logs: https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1454//testReport/org.apache.hadoop.yarn.server.resourcemanager/TestRMNodeTransitions/testExpiredContainer/",sseth,vinodkv,Blocker,Closed,Fixed,15/Dec/11 00:12,05/Mar/12 02:49
Bug,MAPREDUCE-3561,12535237,[Umbrella ticket] Performance issues in YARN+MR,"Been working on measuring performance of YARN+MR relative to the 0.20.xx release line together with [~karams].

This is an umbrella ticket to track all the issues related to performance.",vinodkv,vinodkv,Major,Resolved,Fixed,15/Dec/11 01:09,08/May/15 18:11
Bug,MAPREDUCE-3563,12535267,LocalJobRunner doesn't handle Jobs using o.a.h.mapreduce.OutputCommitter,"LocalJobRunner doesn't handle Jobs using o.a.h.mapreduce.OutputCommitter, ran into this debugging PIG-2347.",acmurthy,acmurthy,Major,Closed,Fixed,15/Dec/11 08:21,12/Feb/15 10:15
Bug,MAPREDUCE-3564,12535277,TestStagingCleanup and TestJobEndNotifier are failing on trunk.,"From recent jenkins test runs:


-1 core tests. The patch failed these unit tests:
org.apache.hadoop.mapreduce.v2.app.TestStagingCleanup
org.apache.hadoop.mapreduce.v2.app.TestJobEndNotifier

https://builds.apache.org/job/PreCommit-MAPREDUCE-Build/1457//testReport/
",sseth,mahadev,Blocker,Closed,Fixed,15/Dec/11 09:07,05/Mar/12 02:48
Bug,MAPREDUCE-3578,12535550,"starting nodemanager as 'root' gives ""Unknown -jvm option""","
running ""sudo HADOOP_ROOT/bin/yarn-daemon.sh start nodemanager"" I get ""unknown -jvm option"" (jdk version 1.6.0.26). The problem seems to be with line 204 in yarn:

elif [ ""$COMMAND"" = ""nodemanager"" ] ; then
CLASSPATH=${CLASSPATH}:$YARN_CONF_DIR/nm-config/log4j.properties
CLASS='org.apache.hadoop.yarn.server.nodemanager.NodeManager'
if [[ $EUID -eq 0 ]]; then
YARN_OPTS=""$YARN_OPTS -jvm server $YARN_NODEMANAGER_OPTS""
else
YARN_OPTS=""$YARN_OPTS -server $YARN_NODEMANAGER_OPTS""
fi

using -server seems to solve the problem for me.

I tested using build 929 from https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Mapreduce-trunk/.

",tomwhite,giladwolff,Major,Closed,Fixed,17/Dec/11 01:45,23/May/12 20:28
Bug,MAPREDUCE-3579,12535582,ConverterUtils should not include a port in a path for a URL with no port,"In {{ConverterUtils#getPathFromYarnURL}}, it's incorrectly assumed that if a URL includes a valid host it must also include a valid port.",atm,atm,Major,Closed,Fixed,17/Dec/11 16:49,10/Mar/15 04:32
Bug,MAPREDUCE-3582,12535831,Move successfully passing MR1 tests to MR2 maven tree.,This ticket will track moving mr1 tests that are passing successfully to mr2 maven tree.,ahmed.radwan,ahmed.radwan,Major,Closed,Fixed,20/Dec/11 11:53,02/May/13 02:29
Bug,MAPREDUCE-3583,12535850,ProcfsBasedProcessTree#constructProcessInfo() may throw NumberFormatException,"HBase PreCommit builds frequently gave us NumberFormatException.

From https://builds.apache.org/job/PreCommit-HBASE-Build/553//testReport/org.apache.hadoop.hbase.mapreduce/TestHFileOutputFormat/testMRIncrementalLoad/:
{code}
2011-12-20 01:44:01,180 WARN  [main] mapred.JobClient(784): No job jar file set.  User classes may not be found. See JobConf(Class) or JobConf#setJar(String).
java.lang.NumberFormatException: For input string: ""18446743988060683582""
	at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
	at java.lang.Long.parseLong(Long.java:422)
	at java.lang.Long.parseLong(Long.java:468)
	at org.apache.hadoop.util.ProcfsBasedProcessTree.constructProcessInfo(ProcfsBasedProcessTree.java:413)
	at org.apache.hadoop.util.ProcfsBasedProcessTree.getProcessTree(ProcfsBasedProcessTree.java:148)
	at org.apache.hadoop.util.LinuxResourceCalculatorPlugin.getProcResourceValues(LinuxResourceCalculatorPlugin.java:401)
	at org.apache.hadoop.mapred.Task.initialize(Task.java:536)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:353)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1083)
	at org.apache.hadoop.mapred.Child.main(Child.java:249)
{code}
From hadoop 0.20.205 source code, looks like ppid was 18446743988060683582, causing NFE:
{code}
        // Set (name) (ppid) (pgrpId) (session) (utime) (stime) (vsize) (rss)
         pinfo.updateProcessInfo(m.group(2), Integer.parseInt(m.group(3)),
{code}
You can find information on the OS at the beginning of https://builds.apache.org/job/PreCommit-HBASE-Build/553/console:
{code}
asf011.sp2.ygridcore.net
Linux asf011.sp2.ygridcore.net 2.6.32-33-server #71-Ubuntu SMP Wed Jul 20 17:42:25 UTC 2011 x86_64 GNU/Linux
core file size          (blocks, -c) 0
data seg size           (kbytes, -d) unlimited
scheduling priority             (-e) 20
file size               (blocks, -f) unlimited
pending signals                 (-i) 16382
max locked memory       (kbytes, -l) 64
max memory size         (kbytes, -m) unlimited
open files                      (-n) 60000
pipe size            (512 bytes, -p) 8
POSIX message queues     (bytes, -q) 819200
real-time priority              (-r) 0
stack size              (kbytes, -s) 8192
cpu time               (seconds, -t) unlimited
max user processes              (-u) 2048
virtual memory          (kbytes, -v) unlimited
file locks                      (-x) unlimited
60000
Running in Jenkins mode
{code}

From Nicolas Sze:
{noformat}
It looks like that the ppid is a 64-bit positive integer but Java long is signed and so only works with 63-bit positive integers.  In your case,

  2^64 > 18446743988060683582 > 2^63.

Therefore, there is a NFE. 
{noformat}

I propose changing allProcessInfo to Map<String, ProcessInfo> so that we don't encounter this problem by avoiding parsing large integer.",zhihyu@ebaysf.com,zhihyu@ebaysf.com,Critical,Closed,Fixed,20/Dec/11 15:07,09/Jan/14 17:22
Bug,MAPREDUCE-3586,12535891,Lots of AMs hanging around in PIG testing,"[~daijy] found this. Here's what he says:
bq. I see hundreds of MRAppMaster process on my machine, and lots of tests fail for ""Too many open files"".",vinodkv,vinodkv,Blocker,Closed,Fixed,20/Dec/11 20:01,05/Mar/12 02:49
Bug,MAPREDUCE-3587,12535913,The deployment tarball should have different directories for yarn jars and mapreduce jars.,Currently all the jars in the mr tarball go to share/hadoop/mapreduce. The jars should be split into: share/hadoop/yarn and share/hadoop/mapreduce for clear seperation between yarn framework and mr.,,mahadev,Major,Resolved,Fixed,20/Dec/11 22:07,09/Mar/15 21:55
Bug,MAPREDUCE-3588,12535925,bin/yarn broken after MAPREDUCE-3366,"bin/yarn broken after MAPREDUCE-3366, doesn't add yarn jars to classpath. As a result no servers can be started.",acmurthy,acmurthy,Blocker,Closed,Fixed,20/Dec/11 23:27,05/Mar/12 02:49
Bug,MAPREDUCE-3593,12536089,MAPREDUCE Impersonation is not working in 22,,mayank_bansal,mayank_bansal,Major,Resolved,Fixed,21/Dec/11 21:49,11/Jan/12 01:20
Bug,MAPREDUCE-3596,12536228,Sort benchmark got hang after completion of 99% map phase,"Courtesy [~vinaythota]
{quote}
Ran sort benchmark couple of times and every time the job got hang after completion 99% map phase. There are some map tasks failed. Also it's not scheduled some of the pending map tasks.
Cluster size is 350 nodes.

Build Details:
==============

Compiled:       Fri Dec 9 16:25:27 PST 2011 by someone from branches/branch-0.23/hadoop-common-project/hadoop-common 
ResourceManager version:        revision 1212681 by someone source checksum on Fri Dec 9 16:52:07 PST 2011
Hadoop version:         revision 1212592 by someone Fri Dec 9 16:25:27 PST 2011
{quote}


",vinodkv,raviprak,Blocker,Closed,Fixed,22/Dec/11 23:52,05/Mar/12 02:49
Bug,MAPREDUCE-3604,12536476,Streaming's check for local mode is broken,Streaming isn't checking for mapreduce.framework.name as part of check for 'local' mode.,acmurthy,acmurthy,Blocker,Closed,Fixed,28/Dec/11 06:38,05/Mar/12 02:48
Bug,MAPREDUCE-3608,12536632,MAPREDUCE-3522 commit causes compilation to fail,There are compilation errors after MAPREDUCE-3522 was committed. Some more changes were need to webapps to fix the compilation issue.,mahadev,mahadev,Major,Closed,Fixed,30/Dec/11 00:32,05/Mar/12 02:49
Bug,KAFKA-4,12514641,Confusing Error mesage from producer when no kafka brokers are available,"If no kafka brokers are available the producer gives the following error: 

Exception in thread ""main"" kafka.common.InvalidPartitionException: Invalid number of partitions: 0 
Valid values are > 0 
at kafka.producer.Producer.kafka$producer$Producer$$getPartition(Producer.scala:144) 
at kafka.producer.Producer$$anonfun$3.apply(Producer.scala:112) 
at kafka.producer.Producer$$anonfun$3.apply(Producer.scala:102) 
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) 
at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206) 
at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34) 
at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32) 
at scala.collection.TraversableLike$class.map(TraversableLike.scala:206) 
at scala.collection.mutable.WrappedArray.map(WrappedArray.scala:32) 
at kafka.producer.Producer.send(Producer.scala:102) 
at kafka.javaapi.producer.Producer.send(Producer.scala:101) 
at com.linkedin.nusviewer.PublishTestMessage.main(PublishTestMessage.java:45) 

This is confusing. The problem is that no brokers are available, we should make this more clear.",,,Minor,Resolved,Fixed,19/Jul/11 21:32,16/Nov/17 09:11
Bug,KAFKA-9,12514646,Consumer logs ERROR during close,"When closing the consumer, we sometimes get the following error: 

[2011-05-16 13:24:39,203] ERROR consumed offset: 862812384944 doesn't match fetch offset: 862813943889 for PageViewEvent:2-0; consumer may lose data (kafka.consumer.ConsumerIterator)",,,Major,Closed,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-11,12514648,The zookeeper based Producer doesn't remove a dead broker from its list while serving a produce request,"The producer registers a watcher on /brokers/ids to detect the new set of brokers in the cluster. It uses that to keep the producer pool connections updated. But, this watcher callback should also remove the dead brokers, if any, from its in memory data structure. This is important so that we don't accidentally pick a dead broker to serve a produce request. T",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-12,12514649,Improve EventHandler in AsyncProducer,"There are a few issues with the current EventHandler. 
1. If an EventHandler is specified in a config file, the instantiator requires that the EventHandler has an empty constructor. 
2. Today, a user has to pass in the Encoder twice, once through the Producer and another through the EventHandler. 
3. The default EventHandler is not set (say, for events of String type).",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-13,12514650,ZKBrokerPartitionInfo doesn't allow load balancing on a new topic,"The problem is that initially no broker has registered for a topic in ZK. Once the producer sends a message to a broker, that broker is registered in ZK. After that, the producer sticks with that broker.",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-14,12514651,ZK exception in consumer when no broker has registered,The consumer will get a ZKNoNodeException if no broker has ever registered in ZK.,,,Major,Closed,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-15,12514652,SBT release-zip target doesn't include bin and config directories anymore,"SBT release-zip target is responsible for creating a fully deployable release zip containing all the package jars, scripts in the bin directory and config property files. 
Currently, it packages the kafka jar and the lib directories correctly.",junrao,junrao,Major,Closed,Fixed,19/Jul/11 21:32,13/Jun/13 16:27
Bug,KAFKA-17,12514654,ZookeeperConsumerConnectorMBean needs to close SimpleConsumer when done,,,,Major,Closed,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-21,12514658,Increase sleep timeout in AutoOffsetResetTest,"The auto offset reset integration tests in AutoOffsetResetTest.scala, the timeout of 1100 is not enough for some Linux machines. 

Increasing that timeout to 2seconds fixes the issue.",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-22,12514659,Unrequired explicit dependency on specific versions of certain libraries,"Currently, kafka explicitly depends on a certain version of cglib - 2.1_3. On linux, this version causes NoSuchMethodErrors. 

Also, the dependency on asm is not required. The correct set of dependencies are - 

scalatest 
junit 
easymock 3.0 (this pulls in the correct versions of cglib and objenesis)",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-31,12514668,Handle ZK exception properly in consumer,"Occasionally, during rebalance, we may hit a ZK exception because a ZK node that we thought is there suddenly disappear. When this happens, we need to reset the consumer state before the next rebalance happens.",,,Major,Closed,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-32,12514669,consumer picks up wrong offset during rebalance,"Occasionally, we saw a consumer picks up wrong offset during rebalance.",,,Major,Closed,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-34,12514671,NotCompliantMBeanException while creating kafka.javaapi.consumer.ZookeeperConsumerConnector,"When you try to create a zk consumer through the java api - 

javax.management.NotCompliantMBeanException: MBean class kafka.javaapi.consumer.ZookeeperConsumerConnector does not implement DynamicMBean, neither follows the Standard MBean conventions (javax.management.NotCompliantMBeanException: Class kafka.javaapi.consumer.ZookeeperConsumerConnector is not a JMX compliant Standard MBean) nor the MXBean conventions (javax.management.NotCompliantMBeanException: kafka.javaapi.consumer.ZookeeperConsumerConnector: Class kafka.javaapi.consumer.ZookeeperConsumerConnector is not a JMX compliant MXBean) 
at com.sun.jmx.mbeanserver.Introspector.checkCompliance(Introspector.java:160) 
at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:305) 
at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-37,12514674,Quick start still references SimpleProducer,"The quickstart kafka page still references ""SimpleProducer"". As of the latest github checkout, this no longer exists.",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-41,12514678,multi-produce and multi-fetch support with replication,We need to figure out how to support multi-produce and multi-fetch smoothly with the replication support. The client has to figure out which partitions are collocated on the same broker and adjust accordingly when some partitions are moved.,,junrao,Major,Resolved,Fixed,19/Jul/11 21:32,06/Apr/12 17:53
Bug,KAFKA-42,12514679,Support rebalancing the partitions with replication,"As new brokers are added, we need to support moving partition replicas from one set of brokers to another, online.",nehanarkhede,junrao,Blocker,Closed,Fixed,19/Jul/11 21:32,30/Dec/14 00:36
Bug,KAFKA-43,12514680,Rebalance to preferred broke with intra-cluster replication support,"We need to allow the leader to be moved to the preferred broker, for better load balancing.",nehanarkhede,junrao,Blocker,Closed,Fixed,19/Jul/11 21:32,13/Oct/12 00:45
Bug,KAFKA-44,12514681,Various ZK listeners to support intra-cluster replication,We need to implement the new ZK listeners for the new paths registered in ZK.,nehanarkhede,junrao,Major,Resolved,Fixed,19/Jul/11 21:32,10/Jun/12 20:42
Bug,KAFKA-45,12514682,"Broker startup, leader election, becoming a leader/follower for intra-cluster replication","We need to implement the logic for starting a broker with replicated partitions, the leader election logic and how to become a leader and a follower.",nehanarkhede,junrao,Major,Resolved,Fixed,19/Jul/11 21:32,10/Jun/12 20:40
Bug,KAFKA-46,12514683,"Commit thread, ReplicaFetcherThread for intra-cluster replication",We need to implement the commit thread at the leader and the fetcher thread at the follower for replication the data from the leader.,nehanarkhede,junrao,Major,Resolved,Fixed,19/Jul/11 21:32,01/Jun/12 20:58
Bug,KAFKA-47,12514684,Create topic support and new ZK data structures for intra-cluster replication,"We need the DDL syntax for creating new topics. May need to use things like javaCC. Also, we need to register new data structures in ZK accordingly.",,junrao,Major,Resolved,Fixed,19/Jul/11 21:32,07/Sep/17 18:20
Bug,KAFKA-48,12514685,"Implement optional ""long poll"" support in fetch request","Currently, the fetch request is non-blocking. If there is nothing on the broker for the consumer to retrieve, the broker simply returns an empty set to the consumer. This can be inefficient, if you want to ensure low-latency because you keep polling over and over. We should make a blocking version of the fetch request so that the fetch request is not returned until the broker has at least one message for the fetcher or some timeout passes.",jkreps,junrao,Major,Resolved,Fixed,19/Jul/11 21:32,30/Apr/12 21:42
Bug,KAFKA-49,12514686,Add acknowledgement to the produce request.,"Currently, the produce request doesn't get acknowledged. We need to have a broker send a response to the producer and have the producer wait for the response before sending the next request.",prashanth.menon,junrao,Major,Resolved,Fixed,19/Jul/11 21:32,02/May/13 02:29
Bug,KAFKA-51,12514688,getOffsetsBefore() returns wrong offset when given a specific timestamp,"When a specific timestamp is specified, getOffsetsBefore() always returns the current HW, which is incorrect.",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-52,12514689,Consumer Code documentation,"The example code for the ""Consumer Code"" section on http://sna-projects.com/kafka/quickstart.php seems to contain a couple of errors. 

Here's the working code: 

{code} 
// specify some consumer properties 
Properties props = new Properties(); 
props.put(""zk.connect"", ""localhost:2181""); 
props.put(""zk.connectiontimeout.ms"", ""1000000""); 
props.put(""groupid"", ""test_group""); 

// Create the connection to the cluster 
ConsumerConfig consumerConfig = new ConsumerConfig(props); 
ConsumerConnector consumerConnector = Consumer.create(consumerConfig); 

// create 4 partitions of the stream for topic ""test"", to allow 4 threads to consume 
Map<String, List<KafkaMessageStream>> topicMessageStreams = 
consumerConnector.createMessageStreams(ImmutableMap.of(""test"", 4)); 
// create list of 4 threads to consume from each of the partitions 
List<KafkaMessageStream> streams = topicMessageStreams.get(""test""); 
ExecutorService executor = Executors.newFixedThreadPool(4); 

// consume the messages in the threads 
for (final KafkaMessageStream stream : streams) { 
executor.submit(new Runnable() { 
//final KafkaMessageStream stream = topicStream.getValue(); 
public void run() { 
for (Message message : stream) { 
// process message 
} 
} 
}); 
} 
{code} 

It might also be worth specifying the imports: 

{code} 
import kafka.consumer.*; 
import kafka.message.Message; 

import java.util.Properties; 
import java.util.Map; 
import java.util.List; 
import java.util.concurrent.ExecutorService; 
import java.util.concurrent.Executors; 

import com.google.common.collect.ImmutableMap; 
{code}",,,Minor,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-53,12514690,close() in SimpleConsumer should be synchronized,"Similar to KAFKA-12, the close method in SimpleConsumer doesn't hold the lock while closing the channel and setting it to null, potentially creating a race condition if a message is being received.",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-54,12514691,Propagate server all exceptions to consumer,"Currently, we only propagate a few known exceptions to the consumer. We should propagate all exceptions to the consumer.",,,Major,Closed,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-55,12514692,shutdown kafka when there is any disk IO error,"Currently, if we encounter any IO error while writing to a kafka log, we simply log the error and continue. However, this kind of errors could leave the log in a corrupted state (e.g., only part of a message is added to the log). When this happens, we should stop accepting new requests and force kafka to shutdown. Once kafka is restarted, log recovery can clean up any corrupted log.",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-59,12514696,ByteBufferMessageSet logs error about fetch size,"Not sure how this happened, but someone added an error message about fetch size being too small in ByteBufferMessageSet. This obviously makes no sense since this class is used in the producer and broker as well as in the consumer, neither of which have a fetch size. This error needs to be properly handled (say by throwing an error), and each user needs to be modified to handle it appropriately.",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-60,12514697,"If the producer sends an invalid MessageSet the broker will append it, corrupting the log","It appears our producer request handling is actually a little buggy. We allow messagesets to be ragged on the right (i.e. contain a trailing partial message), but when we append() we ultimately do 
messages.writeTo(channel, 0, messages.sizeInBytes) 
which i believe would append not just the valid messages, but also the partial message, thus corrupting the log. 

We need to set limit() on the ByteBuffer to truncate off invalid trailing messages before writing, or something like that.",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-61,12514698,SimpleProducer lose messages when socket gets an io exception,"Currently, if we get any io exception while sending a message, SimpleProducer reestablishes the socket connection without resending the message. Thus the message is lost. 

One way to fix this is to only reset socket channel to null when there is io exception during send and throw the exception back to the caller. The caller can capture the exception and resend the message.",,,Major,Resolved,Fixed,19/Jul/11 21:32,19/Jul/11 21:32
Bug,KAFKA-81,12517818,wrong path in bin/kafka-run-class.sh ,"https://github.com/kafka-dev/kafka/issues/28

{{monospace}}
I just downloaded the official 0.6 archive:
https://github.com/downloads/kafka-dev/kafka/kafka-0.6.zip

and tried starting zookeeper / kafka.

The above archive will extract the deps into a dir called ""libs"", but in bin/kafka-run-class.sh there's a loop to add the jars in ""lib"" to the classpath:

for file in $base_dir/lib/*.jar;
do
  CLASSPATH=$CLASSPATH:$file
done

It's a little more complicated then that. The tarball also places kafka-0.6.jar in the root of the directory, where no scripts look. config does not seem to have the log4j properties files, which makes zookeeper sad.
{{monospace}}
",,cburroughs,Major,Resolved,Fixed,03/Aug/11 18:07,14/Jun/13 03:56
Bug,KAFKA-83,12517962,Options in SyncProducerConfig and AsyncProducerConfig can leak,"There is the high-level producer api, and then there is the sync producer and then there is the async producer. Some config options are shared across these which leads to a small degree of confusion. I have a diagram lying around that I can attach to this jira. Anyway, due to this sharing the ProducerPool code copies around properties which is unsafe, because a developer who adds a new option may forget to copy it over. The fix is simple and should make the preceding summary a bit more clear.",,jjkoshy,Minor,Resolved,Fixed,04/Aug/11 20:31,12/Nov/11 21:13
Bug,KAFKA-86,12518124,"the ""design"" link on the home page 404's","The link for ""design"" in See our design page for more details. is broken (http://incubator.apache.org/kafka/design.php)",jkreps,rberger,Minor,Resolved,Fixed,06/Aug/11 21:32,07/Aug/11 01:37
Bug,KAFKA-88,12518225,Producer perf test fails against localhost with > 10 threads,"The perf test starts producing errors when it is run with --threads 11 (or higher). The cause is that we create a zookeeper connection per thread, and zookeeper recently added a feature which limits the number of connections per ip in ZOOKEEPER-336. This setting is set to 10 by default. I recommend we bump this up in our packaged zk config, since it is hard to figure this out and makes it look like the client itself is having issues.

",jkreps,jkreps,Minor,Resolved,Fixed,08/Aug/11 17:36,08/Aug/11 19:09
Bug,KAFKA-91,12518343,zkclient does not show up in pom,"The pom from created by `make-pom`. Does not include zkclient, which is  of course a key dependency.  Not sure yet how to pull in zkclient while excluding sbt itself.

$ cat core/target/scala_2.8.0/kafka-0.7.pom  | grep -i zkclient | wc -l
0
",cburroughs,cburroughs,Minor,Resolved,Fixed,09/Aug/11 18:27,30/Oct/11 21:42
Bug,KAFKA-94,12518663,KafkaServer can throw a NullPointerException during startup if zookeeper is down,"Starting up KafkaServer when zookeeper is down can lead to a NullPointerException as shown below. The LogManager throws a ZkTimeoutException from startup if zookeeper is down. This is caught and we call shutdown. socketServer is uninitialized at this point, and hence the NPE. Will upload patch in a bit.

2011/08/12 00:22:36.194 FATAL [KafkaServer] [pool-2-thread-1] [kafka] java.lang.NullPointerException
	at kafka.server.KafkaServer.shutdown(KafkaServer.scala:98)
	at kafka.server.KafkaServer.startup(KafkaServer.scala:84)
	at kafka.server.KafkaServerStartable.startup(KafkaServerStartable.scala:40)
	at com.linkedin.kafka.KafkaStartable.start(KafkaStartable.java:49)
	at com.linkedin.spring.servlet.ComponentsContextLoaderListener.setServletContextAttributes(ComponentsContextLoaderListener.java:113)
	at com.linkedin.spring.servlet.ComponentsContextLoaderListener.contextInitialized(ComponentsContextLoaderListener.java:50)
	at org.mortbay.jetty.handler.ContextHandler.startContext(ContextHandler.java:549)
	at org.mortbay.jetty.servlet.Context.startContext(Context.java:136)
	at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1282)
	at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:518)
	at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:499)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at com.linkedin.emweb.ContextBasedHandlerImpl.doStart(ContextBasedHandlerImpl.java:123)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at com.linkedin.emweb.WebappDeployerImpl.start(WebappDeployerImpl.java:324)
	at com.linkedin.emweb.WebappDeployerImpl.deploy(WebappDeployerImpl.java:178)
	at com.linkedin.emweb.StateKeeperWebappDeployer.deploy(StateKeeperWebappDeployer.java:73)
	at com.linkedin.emweb.mbeans.WebappDeployerAdmin.deploy(WebappDeployerAdmin.java:86)
	at com.linkedin.emweb.mbeans.WebappDeployerAdmin$1.call(WebappDeployerAdmin.java:130)
	at com.linkedin.emweb.mbeans.WebappDeployerAdmin$1.call(WebappDeployerAdmin.java:127)
	at com.linkedin.emweb.mbeans.WebappDeployerAdmin$3.call(WebappDeployerAdmin.java:171)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
",,jjkoshy,Minor,Resolved,Fixed,12/Aug/11 01:17,18/Jul/12 00:00
Bug,KAFKA-98,12518808,Unit tests hard code ports,This doesn't work on a test server or on your own box if you have kafka running. Patch removes this.,jkreps,jkreps,Minor,Resolved,Fixed,14/Aug/11 05:36,15/Aug/11 17:34
Bug,KAFKA-99,12518811,SocketServer.scala does not enforce a maximum request size,"The socket server should enforce a max request size to avoid running out of memory if a large request is sent. I see code in BoundedByteBufferReceive and in KafkaConfig to specify this, but it doesn't seem to be getting used. I added this in and added a stand-alone test for the socket server to start handling some of this stuff.",jkreps,jkreps,Major,Resolved,Fixed,14/Aug/11 07:30,20/Aug/11 04:08
Bug,KAFKA-102,12518896,"Shutting down Kafka should be FATAL, not ERROR","When Kafka encounters an unrecoverable error it generates an error level log record and then calls Runtime.getRuntime.halt(1). This should really be fatal, not an error.",,bmatheny,Minor,Resolved,Fixed,15/Aug/11 16:45,15/Aug/11 17:32
Bug,KAFKA-107,12519065,Bug in serialize and collate logic in the DefaultEventHandler,"There is a bug in the serialize and collate in the DefaultEventHandler, that uses the map() API on a hashmap to convert a sequence of messages to a ByteBufferMessageSet, based on the compression configs. The usage of the zip() API after the map() API on a hashmap, has the side effect of reordering the mapping between the keys and the values. ",,nehanarkhede,Major,Resolved,Fixed,17/Aug/11 03:02,13/Sep/11 01:27
Bug,KAFKA-109,12519264,CompressionUtils introduces a GZIP header while compressing empty message sets,"The CompressionUtils helper class takes in a sequence of messages and compresses those, using the appropriate codec. But even if it receives an empty sequence, it still ends up adding a GZIP compression header to the data, efffectively ""adding"" data to the resulting ByteBuffer. This doesn't match with the behavior for uncompressed empty message sets. CompressionUtils should be fixed by removing this side-effect.",,nehanarkhede,Major,Resolved,Fixed,18/Aug/11 09:10,13/Sep/11 01:27
Bug,KAFKA-110,12519275,Bug in the collate logic of the DefaultEventHandler dispatches empty list of messages using the producer,"The collate logic in the DefaultEventHandler is designed to batch together requests for a single topic and partition in order to send it to the server in a single request. In this collate logic, the use of the partition API might give back an empty sequence of data for a particular topic,partition pair. It is useless to add it to the list of data to be sent, and it should avoid making network requests.",,nehanarkhede,Major,Resolved,Fixed,18/Aug/11 10:39,13/Sep/11 01:27
Bug,KAFKA-111,12519279,A bug in the iterator of the ByteBufferMessageSet returns incorrect offsets when it encounters a compressed empty message set,"The deep iterator logic in the ByteBufferMessageSet returns incorrect offsets when it encounters empty compressed data. Ideally, it should be able to decompress the data, figure out that it is somehow empty, skip it and proceed to decoding rest of the data. To make this possible, the manner in which we update the offset to be returned by the iterator, needs to be tweaked.",,nehanarkhede,Major,Resolved,Fixed,18/Aug/11 10:56,13/Sep/11 01:28
Bug,KAFKA-115,12519771,Kafka server access log does not log request details coming from a MultiProduceRequest,"the access logger logic on the kafka server has a bug, that doesn't log the individual produce request that are part of a MultiProduceRequest. ",,nehanarkhede,Major,Resolved,Fixed,23/Aug/11 01:51,13/Sep/11 01:30
Bug,KAFKA-116,12519801,AsyncProducer shutdown logic causes data loss,"The current shutdown logic of the AsyncProducer allows adding events to the queue, after adding the shutdown command to it. The ProducerSendThread drains all the data in the queue until it hits the shutdown command. Hence, all data added after the shutdown command is lost.",,nehanarkhede,Major,Resolved,Fixed,23/Aug/11 07:55,13/Sep/11 01:30
Bug,KAFKA-117,12519882,The FetcherRunnable busy waits on empty fetch requests ,The FetcherRunnable busy waits on empty fetch requests by skipping the backoff logic. This can fill up the disk space due to the public access log being filled up. Also the CPU usage shoots up to 100%. ,,nehanarkhede,Major,Resolved,Fixed,23/Aug/11 21:42,13/Sep/11 01:30
Bug,KAFKA-124,12520315,Console consumer does not exit if consuming process dies,Running the kafka console consumer it should be the case that if the consuming subprocess dies the java process dies as well. Instead it continues consuming messages even though there is no one to give them to.,jkreps,jkreps,Major,Resolved,Fixed,26/Aug/11 22:40,01/Sep/11 05:48
Bug,KAFKA-125,12520945,tooBigRequestIsRejected fails with unexpected Exceptoin,"Commit: http://svn.apache.org/viewvc?view=revision&revision=1159837

[info] Test Starting: tooBigRequestIsRejected
[error] Test Failed: tooBigRequestIsRejected
java.lang.Exception: Unexpected exception, expected<java.io.EOFException> but was<java.net.SocketException>
	at org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:91)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)
	at org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)
	at org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)
	at org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)
	at org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)
	at org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:29)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:121)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:100)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:91)
	at org.scalatest.junit.JUnitSuite$class.run(JUnitSuite.scala:261)
	at kafka.network.SocketServerTest.run(SocketServerTest.scala:32)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketInputStream.read(SocketInputStream.java:168)
	at java.net.SocketInputStream.read(SocketInputStream.java:182)
	at java.io.DataInputStream.readInt(DataInputStream.java:370)
	at kafka.network.SocketServerTest.sendRequest(SocketServerTest.scala:56)
	at kafka.network.SocketServerTest.tooBigRequestIsRejected(SocketServerTest.scala:78)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.internal.runners.TestMethodRunner.executeMethodBody(TestMethodRunner.java:99)
	at org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:81)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)
	at org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)
	at org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)
	at org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)
	at org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)
	at org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:29)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:121)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:100)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:91)
	at org.scalatest.junit.JUnitSuite$class.run(JUnitSuite.scala:261)
	at kafka.network.SocketServerTest.run(SocketServerTest.scala:32)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
",jkreps,cburroughs,Minor,Resolved,Fixed,31/Aug/11 17:04,03/Oct/11 20:12
Bug,KAFKA-126,12520992,Log flush should complete upon broker shutdown,"Broker shutdown currently forces the flush scheduler to shutdown(Now). This leads to an unclean shutdown. cleanupLogs may be affected by a similar scenario.

2011/08/31 09:45:34.833 ERROR [LogManager] [kafka-logflusher-0] [kafka] Error flushing topic MyTopic
java.nio.channels.ClosedByInterruptException
        at java.nio.channels.spi.AbstractInterruptibleChannel.end(AbstractInterruptibleChannel.java:184)
        at sun.nio.ch.FileChannelImpl.force(FileChannelImpl.java:362)
        at kafka.message.FileMessageSet.flush(FileMessageSet.scala:174)
        at kafka.log.Log.flush(Log.scala:306)
        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushAllLogs$1.apply(LogManager.scala:274)
        at kafka.log.LogManager$$anonfun$kafka$log$LogManager$$flushAllLogs$1.apply(LogManager.scala:263)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)
        at kafka.log.LogManager.kafka$log$LogManager$$flushAllLogs(LogManager.scala:263)
        at kafka.log.LogManager$$anonfun$startup$1.apply$mcV$sp(LogManager.scala:129)
        at kafka.utils.Utils$$anon$2.run(Utils.scala:58)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

A possible fix this would be to use shutdown() instead of shutdownNow() in the scheduler.
",,jjkoshy,Major,Resolved,Fixed,31/Aug/11 23:58,05/Oct/11 23:05
Bug,KAFKA-128,12521003,DumpLogSegments outputs wrong offsets,,junrao,junrao,Major,Resolved,Fixed,01/Sep/11 01:24,01/Sep/11 07:50
Bug,KAFKA-129,12521006,ZK-based producer can throw an unexpceted exception when sending a message,"Here is a log trace when that happens.

2011/08/26 11:25:20.104 FATAL [EmbeddedConsumer] [kafka-embedded-consumer-firehoseActivity-0] [kafka] java.util.NoSuchElementException: None.getjava.util.NoSuchElementException: None.get
        at scala.None$.get(Option.scala:185)
        at scala.None$.get(Option.scala:183)
        at kafka.producer.Producer$$anonfun$3.apply(Producer.scala:115)
        at kafka.producer.Producer$$anonfun$3.apply(Producer.scala:101)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.TraversableLike$$anonfun$map$1.apply(TraversableLike.scala:206)
        at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:34)
        at scala.collection.mutable.WrappedArray.foreach(WrappedArray.scala:32)
        at scala.collection.TraversableLike$class.map(TraversableLike.scala:206)
        at scala.collection.mutable.WrappedArray.map(WrappedArray.scala:32)
        at kafka.producer.Producer.send(Producer.scala:101)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1$$anonfun$run$1.apply(KafkaServerStartable.scala:136)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1$$anonfun$run$1.apply(KafkaServerStartable.scala:134)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)
        at scala.collection.IterableLike$class.foreach(IterableLike.scala:79)
        at kafka.consumer.KafkaMessageStream.foreach(KafkaMessageStream.scala:29)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1.run(KafkaServerStartable.scala:134)
        at java.lang.Thread.run(Thread.java:619)
",,junrao,Major,Resolved,Fixed,01/Sep/11 01:36,04/Oct/11 23:05
Bug,KAFKA-131,12521223,Hadoop Consumer goes into an infinite loop when  kafka.request.limit is set to -1,There is a bug in  KafkaETLContext.java  where in a new Iterator instance is being created every time. This causes endless loops.,,sampd,Major,Resolved,Fixed,02/Sep/11 19:20,29/Sep/11 18:39
Bug,KAFKA-135,12523754,the ruby kafka gem is not functional,"The gem spec is missing a file declaration, the resulting gem is thus unusable",pyritschard,pyritschard,Major,Closed,Fixed,20/Sep/11 17:10,04/Mar/13 17:02
Bug,KAFKA-138,12524582,Bug in the queue timeout logic of the async producer,There is a bug in the queue timeout logic of the async producer. This bug shows up when the producer is very low throughput. The behavior observed by such very low throughput producers is delayed dispatching of the events. There is no observed data loss though.,,nehanarkhede,Major,Resolved,Fixed,25/Sep/11 22:59,28/Sep/11 00:52
Bug,KAFKA-145,12525599,Kafka server mirror shutdown bug,"When a machine that is mirroring data off of another Kafka broker is shutdown, it runs into the following exception, effectively dropping data. The shutdown API needs to be fixed to first shutdown the consumer threads, drain all the data to the producer, and only then shutdown the producer. 

FATAL kafka.server.EmbeddedConsumer  - kafka.producer.async.QueueClosedException: Attempt to add event to a closed queue.kafka.producer.async.QueueClosedException: Attempt to add event to a closed queue.
        at kafka.producer.async.AsyncProducer.send(AsyncProducer.scala:87)
        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$2.apply(ProducerPool.scala:131)
        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1$$anonfun$apply$2.apply(ProducerPool.scala:131)
        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
        at scala.collection.immutable.List.foreach(List.scala:45)
        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1.apply(ProducerPool.scala:131)
        at kafka.producer.ProducerPool$$anonfun$send$1$$anonfun$apply$mcVI$sp$1.apply(ProducerPool.scala:130)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply$mcVI$sp(ProducerPool.scala:130)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:102)
        at kafka.producer.ProducerPool$$anonfun$send$1.apply(ProducerPool.scala:102)
        at scala.collection.mutable.ResizableArray$class.foreach(ResizableArray.scala:57)
        at scala.collection.mutable.ArrayBuffer.foreach(ArrayBuffer.scala:43)
        at kafka.producer.ProducerPool.send(ProducerPool.scala:102)
        at kafka.producer.Producer.zkSend(Producer.scala:144)
        at kafka.producer.Producer.send(Producer.scala:106)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1$$anonfun$run$1.apply(KafkaServerStartable.scala:136)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$1$$anonfun$apply$1$$anon$1$$anonfun$run$1.apply(KafkaServerStartable.scala:134)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)
        at kafka.utils.IteratorTemplate.foreach(IteratorTemplate.scala:30)
",nehanarkhede,nehanarkhede,Major,Resolved,Fixed,03/Oct/11 23:25,05/Oct/11 23:03
Bug,KAFKA-146,12525615,testUnreachableServer sporadically fails,"(If anyone can tell me how to convince Jira to do verbatim output,  I would be grateful)

This seems to fail about 50% of the time on builds.apache.org, also reported by Bao Thai Ngo on the -dev list.  I have not had success reproducing it locally on my Ubuntu laptop.

[0m[[0minfo[0m] [34m[0m
[0m[[0minfo[0m] [34m== core-kafka / kafka.javaapi.producer.SyncProducerTest ==[0m
[0m[[0minfo[0m] [0mTest Starting: testUnreachableServer[0m
First message send retries took 365 ms
[0m[[31merror[0m] [0mTest Failed: testUnreachableServer[0m
junit.framework.AssertionFailedError: null
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.assertTrue(Assert.java:20)
	at junit.framework.Assert.assertTrue(Assert.java:27)
	at kafka.javaapi.producer.SyncProducerTest.testUnreachableServer(SyncProducerTest.scala:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.internal.runners.TestMethodRunner.executeMethodBody(TestMethodRunner.java:99)
	at org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:81)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)
	at org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)
	at org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)
	at org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)
	at org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)
	at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
	at org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)
	at org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:29)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:121)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:100)
	at org.junit.runner.JUnitCore.run(JUnitCore.java:91)
	at org.scalatest.junit.JUnitSuite$class.run(JUnitSuite.scala:261)
	at kafka.javaapi.producer.SyncProducerTest.run(SyncProducerTest.scala:33)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
",nehanarkhede,cburroughs,Minor,Resolved,Fixed,04/Oct/11 02:09,05/Oct/11 20:29
Bug,KAFKA-147,12525755,kafka integration tests fail on a fresh checkout,"On a fresh checkout and with an empty .ivy2 and .m2 cache, if you execute ./sbt update test, the integration tests will fail with this error - 

java.lang.NoSuchMethodError: junit.framework.TestSuite.<init>([Ljava/lang/Class;)V
	at org.scalatest.junit.JUnit3Suite.run(JUnit3Suite.scala:309)
	at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
	at sbt.TestRunner.run(TestFramework.scala:53)
	at sbt.TestRunner.runTest$1(TestFramework.scala:67)
	at sbt.TestRunner.run(TestFramework.scala:76)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
	at sbt.NamedTestTask.run(TestFramework.scala:92)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
	at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
	at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
	at sbt.impl.RunTask.runTask(RunTask.scala:85)
	at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
	at sbt.Control$.trapUnit(Control.scala:19)
	at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)

The reason being 2 versions of the junit jar on the test classpath that SBT uses to run the ""test"" command. The KafkaProject.scala file corrects defines one of the test dependencies to be junit-4.1, since it uses a JUnit api in some of the tests. The problem is that there is another junit jar (v3.8.1) which gets downloaded as a transitive dependency on Scala 2.8.0. The cause of the above error is an incorrect test classpath, that includes both v4.1 as well as v3.8.1.

One of the possible fixes is to override the ""testClasspath"" variable in SBT to explicitly exclude junit from directories other than core/lib_managed/test
",nehanarkhede,nehanarkhede,Blocker,Resolved,Fixed,05/Oct/11 01:28,06/Oct/11 01:59
Bug,KAFKA-149,12526092,Current perf directory has buggy perf tests,"The scripts in the current perf directory are buggy and not useful to run any reliable Kafka performance tests. The performance tools that work correctly are -

ProducerPerformance.scala
SimpleConsumerPerformance.scala
ConsumerPerformance.scala

Currently, the above are in the tools directory. Ideally, a Kafka performance suite should repackage these tools with some sample performance load and output data in csv format that can be graphed. 

I suggest deleting the perf directory and redoing this cleanly.",,nehanarkhede,Major,Resolved,Fixed,06/Oct/11 18:25,09/Oct/11 21:53
Bug,KAFKA-154,12526693,ZK consumer may lose a chunk worth of message during rebalance in some rare cases,"Occasionally, we have see errors with the following message in the consumer log after a rebalance happens.
   consumed offset: xxx doesn't match fetch offset: yyy for topicz

The consumer offset xxx should always match the fetch offset yyy.",junrao,junrao,Major,Resolved,Fixed,11/Oct/11 17:34,15/Oct/11 07:33
Bug,KAFKA-157,12527296,Message may be delivered to incorrect partition incase of semantic partitioning,"Incase the broker hosting the partition is down, messages are currently repartitioned with the number of available brokers. This may lead to void the partitioning contract.

http://bit.ly/oEz2fT",,sharadag,Major,Resolved,Fixed,15/Oct/11 14:38,07/Feb/15 23:49
Bug,KAFKA-158,12527450,go consumer & producer to support compression,"As related to KAFKA-79, the go consumer and producer needs to support the compression attribute per https://cwiki.apache.org/confluence/display/KAFKA/Compression.

Can someone assign this to me, i'll add support and create a patch.

thanks ",,jdamick,Minor,Resolved,Fixed,17/Oct/11 13:52,27/Oct/11 14:23
Bug,KAFKA-159,12527543,Php Client support for compression attribute,"The php client didn't support the new compression attribute 

https://cwiki.apache.org/confluence/display/KAFKA/Compression",,araddon,Minor,Resolved,Fixed,18/Oct/11 04:44,18/Oct/11 17:57
Bug,KAFKA-160,12527622,ZK consumer gets into infinite loop if a message is larger than fetch size,,junrao,junrao,Blocker,Resolved,Fixed,18/Oct/11 17:11,19/Oct/11 23:53
Bug,KAFKA-161,12527665,Producer using broker list does not load balance requests across multiple partitions on a broker,"https://issues.apache.org/jira/browse/KAFKA-129 introduced a bug in the load balancing logic of the Producer using broker.list.
Since the broker.list doesn't specify the number of partitions in total, it should ideally pick a broker randomly, and then send the produce request with partition id -1, so that the EventHandler routes the request to a random partition.
Instead of that, it defaults to 1 partition on each broker and ends up using the Partitioner to pick a partitions amongst the available ones.",nehanarkhede,nehanarkhede,Major,Resolved,Fixed,18/Oct/11 21:58,25/Oct/11 04:59
Bug,KAFKA-171,12528928,Kafka producer should do a single write to send message sets,"From email thread: 
http://mail-archives.apache.org/mod_mbox/incubator-kafka-dev/201110.mbox/%3cCAFbh0Q1PYUj32thBaYQ29E6J4wT_mrG5SuUsfdeGWj6rmEx9Gw@mail.gmail.com%3e
> Before sending an actual message, kafka producer do send a (control) message of 4 bytes to the server. Kafka producer always does this action before send some message to the server.

I think this is because in BoundedByteBufferSend.scala we do essentially
 channel.write(sizeBuffer)
 channel.write(dataBuffer)

The correct solution is to use vector I/O and instead do
 channel.write(Array(sizeBuffer, dataBuffer))",jkreps,jkreps,Major,Resolved,Fixed,26/Oct/11 17:52,23/Nov/11 07:30
Bug,KAFKA-180,12529619,Clean up shell scripts,"Currently it is a bit of a mess:
jkreps-mn:kafka-git jkreps$ ls bin
kafka-console-consumer-log4j.properties	kafka-producer-perf-test.sh		kafka-server-stop.sh			zookeeper-server-stop.sh
kafka-console-consumer.sh		kafka-producer-shell.sh			kafka-simple-consumer-perf-test.sh	zookeeper-shell.sh
kafka-console-producer.sh		kafka-replay-log-producer.sh		kafka-simple-consumer-shell.sh
kafka-consumer-perf-test.sh		kafka-run-class.sh			run-rat.sh
kafka-consumer-shell.sh			kafka-server-start.sh			zookeeper-server-start.sh

I think all the *-shell.sh scripts and all the *-simple-perf-test.sh scripts should die. If anyone has a use for these test classes we can keep them around and use the via kafka-run-class, but they are clearly not made for normal people to use. The *-shell.sh scripts are obsolete now that we have the *-console-*.sh scripts, since these do everything the old scripts did and more. I recommend we also delete the code for these.

I would like to change each tool so that it produces a usage line explaining what it does when run without arguments. Currently I actually had to go read the code to figure out what some of these are.

I would like to clean up places where the arguments are non-standard. Argument names should be the same across all the tools.

I would also like to rename kafka-replay-log-producer.sh to kafka-copy-topic.sh. I think this tool should also accept two zookeeper urls, the url of the input cluster and the url of the output cluster so this tool can be used to copy between clusters. I think we can have a --zookeeper a --input-zookeeper and a --output-zookeeper where --zookeeper is equivalent to setting both the input and the output zookeeper. Also confused why the options for this list --brokerinfo which can be either a zk url or brokerlist AND also --zookeeper which must be a zk url.

Any objections to all this? Any other gripes people have while I am in there?",jkreps,jkreps,Major,Resolved,Fixed,01/Nov/11 04:08,09/Feb/14 23:51
Bug,KAFKA-182,12529732,Set a TCP connection timeout for the SimpleConsumer,"Currently we use SocketChannel.open which I *think* can block for a long time. We should make this configurable, and we may have to create the socket in a different way to enable this.",,jkreps,Major,Resolved,Fixed,01/Nov/11 20:19,07/Feb/15 23:46
Bug,KAFKA-184,12529809,Log retention size and file size should be a long,"Realized this in a local set up: the log.retention.size config option should be a long, or we're limited to 2GB. Also, the name can be improved to log.retention.size.bytes or Mbytes as appropriate. Same comments for log.file.size. If we rename the configs, it would be better to resolve KAFKA-181 first.
",,jjkoshy,Minor,Resolved,Fixed,02/Nov/11 05:37,03/Jul/13 22:08
Bug,KAFKA-186,12530354,no clean way to getCompressionCodec from Java-the-language,"The obvious thing fails:

CompressionCodec.getCompressionCodec(1) results in cannot find symbol
symbol  : method getCompressionCodec(int)
location: interface kafka.message.CompressionCodec

Writing a switch statement with  kafka.message.NoCompressionCodec$.MODULE$ and duplicating the logic in CompressionCodec.getCompressionCodec is no fun, nor is creating a Hashtable just to call Utils.getCompressionCodec.  I'm not sure if there is a magic keyword to make it easy for javac to understand which CompressionCodec I'm referring to.

",,cburroughs,Major,Resolved,Fixed,04/Nov/11 17:26,17/Aug/17 11:41
Bug,KAFKA-192,12530476,CompressionUtilTest does not run and fails when it does,"CompressionUtilTest does not run the functions inside of it during ./sbt test

if you change CompressionUtilTest to extend JUnitSuite then the existing functions run (once you adorne them with @Test) but then fail ...

I suspect the TestUtils.checkEquals(messages.iterator, decompressedMessages.iterator) is failing in testSimpleCompressDecompress because all of the messages are serialized into byte arrays and the entire set of messages compressed and that new compressed messages is what is returned as one message instead of the List[Message] and therefor are not interpreted within TestUtil.checkEquals to see this nuance.

e.g.

[error] Test Failed: testSimpleCompressDecompress
junit.framework.AssertionFailedError: expected:<message(magic = 1, attributes = 0, crc = 3819140844, payload = java.nio.HeapByteBuffer[pos=0 lim=8 cap=8])> but was:<MessageAndOffset(message(magic = 1, attributes = 0, crc = 3819140844, payload = java.nio.HeapByteBuffer[pos=0 lim=8 cap=8]),18)>

and

[error] Test Failed: testComplexCompressDecompress
junit.framework.AssertionFailedError: expected:<2> but was:<3>
",joestein,joestein,Major,Resolved,Fixed,06/Nov/11 06:51,14/Nov/11 21:55
Bug,KAFKA-196,12530541,Topic creation fails on large values,"Since topic logs are stored in a directory holding the topic's name, creation of the directory might fail for large strings.
This is not a problem per-se but the exception thrown is rather cryptic and hard to figure out for operations.

I propose fixing this temporarily with a hard limit of 200 chars for topic names, it would also be possible to hash the topic name.

Another concern is that the exception raised stops the broker, effectively creating  a simple DoS vector, I'm concerned about how tests or wrong client library usage can take down the whole broker.",,pyritschard,Major,Resolved,Fixed,07/Nov/11 09:08,17/Aug/17 11:38
Bug,KAFKA-197,12530895,Embedded consumer doesn't shut down if the server can't start,"If a broker embeds a consumer and the broker itself doesn't start (e.g., conflicting broker id in ZK), the embedded consumer is still running. In this case, we should probably shut down the embedded consumer too.

To do this, we need to either throw an exception or return an error in KafkaServer.startup and act accordingly in KafkaServerStartable.startup.",junrao,junrao,Major,Resolved,Fixed,09/Nov/11 16:45,13/Dec/11 03:04
Bug,KAFKA-204,12531486,BoundedByteBufferReceive hides OutOfMemoryError,"  private def byteBufferAllocate(size: Int): ByteBuffer = {
    var buffer: ByteBuffer = null
    try {
      buffer = ByteBuffer.allocate(size)
    }
    catch {
      case e: OutOfMemoryError =>
        throw new RuntimeException(""OOME with size "" + size, e)
      case e2 =>
        throw e2
    }
    buffer
  }

This hides the fact that an Error occurred, and will likely result in some log handler printing a message, instead of exiting with non-zero status.  Knowing how large the allocation was that caused an OOM is really nice, so I'd suggest logging in byteBufferAllocate and then re-throwing OutOfMemoryError",cburroughs,cburroughs,Critical,Resolved,Fixed,15/Nov/11 16:31,23/Nov/11 23:46
Bug,KAFKA-206,12531622,"no DISCLAIMER, NOTICE needs cleanup","Followup from the incubator vote.  We need a DISCLAIMER and to clean up the notice.

http://mail-archives.apache.org/mod_mbox/incubator-general/201111.mbox/%3CCAOGo0VbZd23mxtVFMCuMkHN9fVhiekBUg1x7mxtH7oAzqgp9mQ%40mail.gmail.com%3E",cburroughs,cburroughs,Major,Resolved,Fixed,16/Nov/11 14:18,01/Dec/11 23:36
Bug,KAFKA-207,12531689,AsyncProducerStats is not a singleton,"AsyncProducerStats is not a singleton. This means that if a client instantiates multiple producers, the stat is collected for only 1 instance, instead of all instances.",junrao,junrao,Major,Resolved,Fixed,16/Nov/11 20:52,22/Nov/11 00:58
Bug,KAFKA-212,12532524,IllegalThreadStateException in topic watcher for Kafka mirroring,"If the kafka mirroring embedded consumer receives a new topic watcher notification, it runs into the following exception 

[2011-11-23 02:49:15,612] FATAL java.lang.IllegalThreadStateException (kafka.consumer.ZookeeperTopicEventWatcher)
[2011-11-23 02:49:15,612] FATAL java.lang.IllegalThreadStateException
        at java.lang.Thread.start(Thread.java:595)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$3.apply(KafkaServerStartable.scala:142)
        at kafka.server.EmbeddedConsumer$$anonfun$startNewConsumerThreads$3.apply(KafkaServerStartable.scala:142)
        at scala.collection.LinearSeqOptimized$class.foreach(LinearSeqOptimized.scala:61)
        at scala.collection.immutable.List.foreach(List.scala:45)
        at kafka.server.EmbeddedConsumer.startNewConsumerThreads(KafkaServerStartable.scala:142)
        at kafka.server.EmbeddedConsumer.handleTopicEvent(KafkaServerStartable.scala:109)
        at kafka.consumer.ZookeeperTopicEventWatcher$ZkTopicEventListener.liftedTree2$1(ZookeeperTopicEventWatcher.scala:83)
        at kafka.consumer.ZookeeperTopicEventWatcher$ZkTopicEventListener.handleChildChange(ZookeeperTopicEventWatcher.scala:78)
        at org.I0Itec.zkclient.ZkClient$7.run(ZkClient.java:568)
        at org.I0Itec.zkclient.ZkEventThread.run(ZkEventThread.java:71)
 (kafka.consumer.ZookeeperTopicEventWatcher)

This happens since it tries to start a thread which has finished executing",nehanarkhede,nehanarkhede,Major,Resolved,Fixed,23/Nov/11 19:43,01/Dec/11 01:04
Bug,KAFKA-216,12533056,Add nunit license to the NOTICE file,"According to yet some more feedback from general@, we need to add NUnit (http://www.nunit.org/) to the NOTICE file.",jghoman,nehanarkhede,Blocker,Resolved,Fixed,29/Nov/11 03:20,30/Nov/11 23:43
Bug,KAFKA-218,12533188,"ZOOKEEPER-961 is nasty, upgrade to zk 3.3.4","3.3.4 is out with ZOOKEEPER-961, which I think is our most reported issue.

http://www.cloudera.com/blog/2011/11/apache-zookeeper-3-3-4-has-been-released/

Should be a one char changes, but the jar hasn't hit the maven repos yet.",pyritschard,cburroughs,Critical,Closed,Fixed,29/Nov/11 20:40,19/Jun/14 05:15
Bug,KAFKA-220,12533392,LogManager test fails on linux,"On Linux, LogManagerTest fails on each and every run
[info] Test Starting: testCleanupExpiredSegments
[error] Test Failed: testCleanupExpiredSegments
junit.framework.AssertionFailedError: Now there should only be only one segment. expected:<1> but was:<12>
        at junit.framework.Assert.fail(Assert.java:47)
        at junit.framework.Assert.failNotEquals(Assert.java:277)
        at junit.framework.Assert.assertEquals(Assert.java:64)
        at junit.framework.Assert.assertEquals(Assert.java:195)
        at kafka.log.LogManagerTest.testCleanupExpiredSegments(LogManagerTest.scala:87)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.internal.runners.TestMethodRunner.executeMethodBody(TestMethodRunner.java:99)
        at org.junit.internal.runners.TestMethodRunner.runUnprotected(TestMethodRunner.java:81)
        at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
        at org.junit.internal.runners.TestMethodRunner.runMethod(TestMethodRunner.java:75)
        at org.junit.internal.runners.TestMethodRunner.run(TestMethodRunner.java:45)
        at org.junit.internal.runners.TestClassMethodsRunner.invokeTestMethod(TestClassMethodsRunner.java:71)
        at org.junit.internal.runners.TestClassMethodsRunner.run(TestClassMethodsRunner.java:35)
        at org.junit.internal.runners.TestClassRunner$1.runUnprotected(TestClassRunner.java:42)
        at org.junit.internal.runners.BeforeAndAfterRunner.runProtected(BeforeAndAfterRunner.java:34)
        at org.junit.internal.runners.TestClassRunner.run(TestClassRunner.java:52)
        at org.junit.internal.runners.CompositeRunner.run(CompositeRunner.java:29)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:121)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:100)
        at org.junit.runner.JUnitCore.run(JUnitCore.java:91)
        at org.scalatest.junit.JUnitSuite$class.run(JUnitSuite.scala:261)
        at kafka.log.LogManagerTest.run(LogManagerTest.scala:28)
        at org.scalatest.tools.ScalaTestFramework$ScalaTestRunner.run(ScalaTestFramework.scala:40)
        at sbt.TestRunner.run(TestFramework.scala:53)
        at sbt.TestRunner.runTest$1(TestFramework.scala:67)
        at sbt.TestRunner.run(TestFramework.scala:76)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11.runTest$2(TestFramework.scala:194)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.TestFramework$$anonfun$10$$anonfun$apply$11$$anonfun$apply$12.apply(TestFramework.scala:205)
        at sbt.NamedTestTask.run(TestFramework.scala:92)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.ScalaProject$$anonfun$sbt$ScalaProject$$toTask$1.apply(ScalaProject.scala:193)
        at sbt.TaskManager$Task.invoke(TaskManager.scala:62)
        at sbt.impl.RunTask.doRun$1(RunTask.scala:77)
        at sbt.impl.RunTask.runTask(RunTask.scala:85)
        at sbt.impl.RunTask.sbt$impl$RunTask$$runIfNotRoot(RunTask.scala:60)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.impl.RunTask$$anonfun$runTasksExceptRoot$2.apply(RunTask.scala:48)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Distributor$Run$Worker$$anonfun$2.apply(ParallelRunner.scala:131)
        at sbt.Control$.trapUnit(Control.scala:19)
        at sbt.Distributor$Run$Worker.run(ParallelRunner.scala:131)
[info] Test Starting: testCleanupSegmentsToMaintainSize

",junrao,nehanarkhede,Critical,Closed,Fixed,01/Dec/11 01:45,19/Jun/14 05:15
Bug,KAFKA-221,12533519,LICENSE and NOTICE problems in Kafka 0.7,"The source LICENSE file for Kafka is incomplete. The LICENSE file needs to accurately reflect the Kafka source and included artifacts.

Similarly, the NOTICE file is likely to be missing information. I'll attach a file with some information that I created. It's incomplete and will need additional work...",jghoman,kevan,Major,Resolved,Fixed,01/Dec/11 21:54,15/Dec/11 18:35
Bug,KAFKA-225,12534563,Bug in mirroring code causes mirroring to halt,"The mirroring code has an API that restarts the consumer connector when a new topic watcher fires. This triggers a rebalancing operation in the consumer connector. But if this rebalancing operation fails, the mirroring code simply throws an exception and never recovers. Ideally, if the rebalancing operation fails due to n retries, we should shut down the mirror",junrao,nehanarkhede,Major,Resolved,Fixed,10/Dec/11 00:09,14/Dec/11 00:09
Bug,KAFKA-226,12535401,SyncProducer connect may return failed connection on reconnect,,junrao,junrao,Major,Resolved,Fixed,16/Dec/11 02:07,16/Dec/11 02:21
Bug,KAFKA-229,12536586,SimpleConsumer is not logging exceptions correctly so detailed stack trace is not coming in the logs,,jkreps,charmalloc,Major,Resolved,Fixed,29/Dec/11 15:57,02/Jan/12 20:55
Bug,HIVE-1874,12494465,fix HBase filter pushdown broken by HIVE-1638,"See comments at end of HIVE-1660 for what happened.
",jvs,jvs,Major,Closed,Fixed,03/Jan/11 04:31,16/Dec/11 23:59
Bug,HIVE-1878,12494560,Set the version of Hive trunk to '0.7.0-SNAPSHOT' to avoid confusing it with a release,"The build.properties file currently sets version=0.7.0, which results in artifacts named
hive-xxx-0.7.0.jar and a tarball with the name hive-0.7.0.tar.gz. Only the actual 0.7.0 release
should generate JARs and tarballs with this name.


",cwsteinbach,cwsteinbach,Major,Closed,Fixed,04/Jan/11 05:17,16/Dec/11 23:59
Bug,HIVE-1879,12494643,Remove hive.metastore.metadb.dir property from hive-default.xml and HiveConf,"The file-based MetaStore implementation was removed in HIVE-143. We also need to
remove the hive.metastore.metadb.dir property from hive-default.xml and HiveConf, as well
as the references to this property that currently appear in HiveMetaStoreClient.",larsfrancke,cwsteinbach,Major,Closed,Fixed,04/Jan/11 21:32,13/Nov/14 19:41
Bug,HIVE-1884,12494701,Potential risk of resource leaks in Hive,"h3.There are couple of resource leaks.
h4.For example,

In CliDriver.java, Method :- processReader() the buffered reader is not closed.

h3.Also there are risk(s) of  resource(s) getting leaked , in such cases we need to re factor the code to move closing of resources in finally block.

h4. For Example :- 

In Throttle.java   Method:- checkJobTracker() , the following code snippet might cause resource leak.

{code}
InputStream in = url.openStream();
in.read(buffer);
in.close();
{code}


Ideally and as per the best coding practices it should be like below

{code}

InputStream in=null;
try   {
        in = url.openStream();
        int numRead = in.read(buffer);
}
finally {
       IOUtils.closeStream(in);
}

{code}

Similar cases, were found in ExplainTask.java, DDLTask.java etc.Need to re factor all such occurrences.


",chinnalalam,mosikri,Major,Closed,Fixed,05/Jan/11 09:52,16/Dec/11 23:56
Bug,HIVE-1892,12494799,show functions also returns internal operators,"show functions: returns bigint etc. in its outputs, which are not valid external functions",priyadarshini,namit,Major,Closed,Fixed,06/Jan/11 02:25,30/Apr/12 21:12
Bug,HIVE-1896,12494825,HBase and Contrib JAR names are missing version numbers,"Also, does anyone know why the hbase and contrib JARs use underscores
instead of dashes in their names? Can I change this or will it break something?

{code}
./build/dist/lib/hive-anttasks-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-cli-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-common-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-exec-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-hwi-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-jdbc-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-metastore-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-serde-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-service-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive-shims-0.7.0-SNAPSHOT.jar
./build/dist/lib/hive_contrib.jar                                   <------
./build/dist/lib/hive_hbase-handler.jar                     <------
{code}

",cwsteinbach,cwsteinbach,Blocker,Closed,Fixed,06/Jan/11 10:21,16/Dec/11 23:59
Bug,HIVE-1897,12494846,"Alter command execution ""when HDFS is down"" results in holding stale data in MetaStore ","Lets  consider, the  *""DFS""* is down , 

And on executing an alter query say  *""alter table firsttable rename to secondtable""*.  
the query execution fails with the following exception:
{color:red} 
InvalidOperationException(message:Unable to access old location hdfs://localhost:9000/user/hive/warehouse/firsttable for table default.firsttable)
{color}

Now after starting the *DFS* and then executing the same query , the client gets the following exception:
{color:red}
NoSuchObjectException(message:default.firsttable table not found)
{color}

h4.Root Cause
In Alter Query execution flow, first *""MetaStore""* operation is executed successfully and then *""DFS""* operation is started. In this scenario, *""DFS""* is down. As a result, execution of the query failed and partial information of the operation is saved.",chinnalalam,chinnalalam,Major,Closed,Fixed,06/Jan/11 14:25,16/Dec/11 23:59
Bug,HIVE-1902,12494992,create script for the metastore upgrade due to HIVE-78,,aprabhakar,namit,Blocker,Closed,Fixed,07/Jan/11 21:37,02/May/13 02:29
Bug,HIVE-1903,12495000,Can't join HBase tables if one's name is the beginning of the other,"I tried joining two tables, let's call them ""table"" and ""table_a"", but I'm seeing an array of errors such as this:

{noformat}
java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat.getRecordReader(HiveHBaseTableInputFormat.java:118)
	at org.apache.hadoop.hive.ql.io.HiveInputFormat.getRecordReader(HiveInputFormat.java:231)
{noformat}

The reason is that HiveInputFormat.pushProjectionsAndFilters matches the aliases with startsWith so in my case the mappers for ""table_a"" were getting the columns from ""table"" as well as its own (and since it had less column, it was trying to get one too far in the array).

I don't know if just changing it to ""equals"" fill fix it, my guess is it won't, since it may break RCFiles.",jvs,jdcryans,Major,Closed,Fixed,07/Jan/11 23:50,24/Jun/15 17:42
Bug,HIVE-1908,12495231,FileHandler leak on partial iteration of the resultset. ,"If the ""resultset"" is not iterated completely ,  one filehandler is leaking

Ex: We need only first row. This case one resource is leaking

{code}

ResultSet resultSet = createStatement.executeQuery(""select * from sampletable"");

if (resultSet.next())
{
	System.out.println(resultSet.getString(1)+""   ""+resultSet.getString(2));
} 

{code}


Command used for checking the filehandlers
{code}
lsof -p {hive_process_id} > runjarlsof.txt
{code}

",chinnalalam,chinnalalam,Major,Closed,Fixed,11/Jan/11 10:38,16/Dec/11 23:59
Bug,HIVE-1912,12495527,Double escaping special chars when removing old partitions in rmr,"If a partition column value contains special characters such as ':', it will be escaped to '%3A' in the partition path. However in FsShell.rmr(oldPath.toUri().toString()), toUri() will double escape '%' to '%25'. This will make the removal fail. ",nzhang,nzhang,Major,Closed,Fixed,13/Jan/11 19:11,16/Dec/11 23:59
Bug,HIVE-1913,12495543,use partition level serde properties,"create table src_part_serde (key int, value string) partitioned by (ds string) stored as sequencefile;
insert overwrite table src_part_serde partition (ds='2011') select * from src;
alter table src_part_serde set serde 'org.apache.hadoop.hive.serde2.lazy.LazySimpleSerDe' with SERDEPROPERTIES ('serialization.format'='\t');
select key, value from src_part_serde where ds='2011' order by key, value limit 20;

will get null results.",he yongqiang,he yongqiang,Major,Closed,Fixed,13/Jan/11 21:24,16/Dec/11 23:59
Bug,HIVE-1915,12495637,authorization on database level is broken.,"CREATE DATABASE IF NOT EXISTS test_db COMMENT 'Hive test database';
SHOW DATABASES;

grant `drop` on DATABASE test_db to user hive_test_user;
grant `select` on DATABASE test_db to user hive_test_user;

show grant user hive_test_user on DATABASE test_db;
DROP DATABASE IF EXISTS test_db;


will fail.",he yongqiang,he yongqiang,Major,Closed,Fixed,14/Jan/11 23:20,10/Oct/12 19:39
Bug,HIVE-1917,12495643,CTAS (create-table-as-select) throws exception when showing results,CTAS throws an exception in CliDriver when showing results at the end of a query. CTAS should not show results because it is not a 'select' query or 'desc'/explain etc. It should be the same as create table/view/index and insert overwrite statements. ,nzhang,nzhang,Major,Closed,Fixed,15/Jan/11 01:25,16/Dec/11 23:59
Bug,HIVE-1922,12496116,"semantic analysis error, when using group by and order by together","When I tried queries like, 'select t.c from t  group by t.c sort by t.c;', hive reported error ,'FAILED: Error in semantic analysis: line 1:40 Invalid Table Alias or Column Reference t'.
But 'select t.c from t  group by t.c ' or 'select t.c from t  sort by t.c;' are ok. 

'select t.c from t  group by t.c sort by c;' is ok too.

The hive server gives stack trace like

11/01/20 03:07:34 INFO parse.SemanticAnalyzer: Get metadata for subqueries
11/01/20 03:07:34 INFO parse.SemanticAnalyzer: Get metadata for destination tables
11/01/20 03:07:34 INFO parse.SemanticAnalyzer: Completed getting MetaData in Semantic Analysis
FAILED: Error in semantic analysis: line 1:40 Invalid Table Alias or Column Reference t
11/01/20 03:07:34 ERROR ql.Driver: FAILED: Error in semantic analysis: line 1:40 Invalid Table Alias or Column Reference t
org.apache.hadoop.hive.ql.parse.SemanticException: line 1:40 Invalid Table Alias or Column Reference t
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:6743)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlan(SemanticAnalyzer.java:4288)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:5446)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6007)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:6583)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:343)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:731)
	at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:116)
	at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:699)
	at org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:677)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

",rhbutani,hongwei,Critical,Resolved,Fixed,20/Jan/11 11:10,19/Nov/13 17:27
Bug,HIVE-1927,12496636,Fix TestHadoop20SAuthBridge failure on Hudson,"The patch for HIVE-1696 is the source a test failure on Hudson. The likely cause of
this failure is that the classpath is not being set correctly for the TestHadoop20SAuthBridge
test, which depends on the security enhanced version of Hadoop.

A couple things to note:

* Hive tests are supposed to run against Hadoop 0.20.0 by default. This value is set in the build.properties file in the project top-level. Altering the Hudson job to set hadoop.version to some other value is not a fix for this issue since the Hudson job will then cease to reflect the default behavior of Hive tests.
* HIVE-1696 added new secure compile targets to the shims/build.xml file. These targets explicitly set the classpath to include the secure version of Hadoop. The fix for this issue likely involves overriding the ""test"" target in shims/build.xml and explicitly setting the classpath in this target to also use the secure version of Hadoop. ",ddas,cwsteinbach,Blocker,Closed,Fixed,25/Jan/11 07:06,16/Dec/11 23:59
Bug,HIVE-1928,12496755,"GRANT/REVOKE should handle privileges as tokens, not identifiers","The grammar for the GRANT and REVOKE Privileges statements currently handle the list of privileges as a list of
identifiers. Since most of the privileges are also keywords in the HQL grammar this requires users
to individually quote-escape each of the privileges, e.g:

{code}
grant `Create` on table authorization_part to user hive_test_user;
grant `Update` on table authorization_part to user hive_test_user;
grant `Drop` on table authorization_part to user hive_test_user;
grant `select` on table src to user hive_test_user;
{code}

Both MySQL and the SQL standard treat privileges as tokens. Hive should do the same.",natty,cwsteinbach,Critical,Closed,Fixed,26/Jan/11 01:53,23/May/12 00:26
Bug,HIVE-1934,12496994,alter table rename messes the location,"create table tmptmp(a string) partitioned by (b string);
alter table tmptmp add partition (b=""1:2:3"");
alter table  tmptmp rename to tmptmp_test;



The location for tmptmp_test partition (b=""1:2:3) is unescaped due to rename, and hence it cannot be dropped.",pauly,namit,Blocker,Closed,Fixed,27/Jan/11 22:36,16/Dec/11 23:59
Bug,HIVE-1935,12497111,set hive.security.authorization.createtable.owner.grants to null by default,It seems an empty setting in hive-size.xml does not overwrite hive-default.xml,he yongqiang,he yongqiang,Major,Closed,Fixed,29/Jan/11 00:02,02/Feb/12 22:20
Bug,HIVE-1936,12497112,hive.semantic.analyzer.hook cannot have multiple values,It should support comma separated values like hive pre/post execution hooks,sdong,namit,Major,Closed,Fixed,29/Jan/11 00:23,17/Dec/11 00:01
Bug,HIVE-1937,12497268,DDLSemanticAnalyzer won't take newly set Hive parameters,"Hive DDLSemanticAnalyzer maintains a static reservedPartitionValue set whose values come from several Hive parameters. However even if these parameters are set to new values, the reservedPartitionValue are not changed. ",nzhang,nzhang,Major,Closed,Fixed,01/Feb/11 00:09,16/Dec/11 23:56
Bug,HIVE-1939,12497385,Fix test failure in TestContribCliDriver/url_hook.q,,cwsteinbach,cwsteinbach,Blocker,Closed,Fixed,01/Feb/11 21:00,16/Dec/11 23:59
Bug,HIVE-1943,12497414,Metastore operations (like drop_partition) could be improved in terms of maintaining consistency of metadata and data,"Currently, metastore operations with associated hdfs operations like drop_partition doesn't do a rollback if the hdfs operation fails. The metastore first updates the metadata, and then tries to do hdfs operation. If the hdfs operation fails for any reason, the data on the hdfs will be orphaned. We should improve the situation.",ddas,ddas,Major,Closed,Fixed,02/Feb/11 02:55,16/Dec/11 23:55
Bug,HIVE-1944,12497427,dynamic partition insert creating different directories for the same partition during merge,"If dynamic partition insert generates multiple partitions and some partitions need to be merge (MR task) and some need not (move task), parallel execution could cause the move task finish first and created the partition directory (say ds=1) first. When the MR job finished, it created a different directory (ds=1_1) for the same partition ds=1. ",nzhang,nzhang,Major,Closed,Fixed,02/Feb/11 06:58,17/Dec/11 00:00
Bug,HIVE-1945,12497497,Support for ESCAPE BY not documented.,"From the client positive test query files - create_escape.q and input_lazyserde.q, it is evident that there is support for specifying ""ESCAPED BY"" when using delimited row format. However this is not documented on Hive Wiki.",,aprabhakar,Minor,Resolved,Fixed,02/Feb/11 19:40,29/Aug/15 22:47
Bug,HIVE-1951,12497603,input16_cc.q is failing in testminimrclidriver,,he yongqiang,namit,Major,Closed,Fixed,03/Feb/11 18:41,16/Dec/11 23:59
Bug,HIVE-1952,12497609,fix some outputs and make some tests deterministic,"Some of the tests are un-deterministic, and are causing intermediate diffs",namit,namit,Major,Closed,Fixed,03/Feb/11 19:35,16/Dec/11 23:59
Bug,HIVE-1959,12497681,Potential memory leak when same connection used for long time. TaskInfo and QueryInfo objects are getting accumulated on executing more queries on the same connection.,*org.apache.hadoop.hive.ql.history.HiveHistory$TaskInfo* and *org.apache.hadoop.hive.ql.history.HiveHistory$QueryInfo* these two objects are getting accumulated on executing more number of queries on the same connection. These objects are getting released only when the connection is closed.,chinnalalam,chinnalalam,Major,Closed,Fixed,04/Feb/11 13:58,16/Dec/11 23:55
Bug,HIVE-1963,12497874,Don't set ivy.home in build-common.xml,"We currently set ivy.home to ${user.home}/.ant in build-common.xml. We should
remove this setting and instead let ivy.home default to ${user.home}/.ivy2 unless
the user specifies otherwise.
",cwsteinbach,cwsteinbach,Minor,Closed,Fixed,07/Feb/11 17:35,16/Dec/11 23:56
Bug,HIVE-1964,12497883,add fully deterministic ORDER BY in test union22.q and input40.q,This test is failing for me; the ORDER BY needs to be on the full key.,jvs,jvs,Blocker,Closed,Fixed,07/Feb/11 18:47,16/Dec/11 23:59
Bug,HIVE-1965,12497893,Auto convert mapjoin should not throw exception if the top operator is union operator.,,liyin,he yongqiang,Major,Closed,Fixed,07/Feb/11 19:39,16/Dec/11 23:56
Bug,HIVE-1969,12497937,TestMinimrCliDriver merge_dynamic_partition2 and 3 are failing on trunk,"I haven't looked into it yet but saw this at the end of the .q.out:

+Ended Job = job_201102071402_0020 with errors
+FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
",nzhang,jvs,Blocker,Closed,Fixed,08/Feb/11 00:34,16/Dec/11 23:59
Bug,HIVE-1973,12497993,Getting error when join on tables where name of table has uppercase letters,"When execute a join query on tables containing Uppercase letters in the table names hit an exception

 Ex:
{noformat}
  create table a(b int);
  create table tabForJoin(b int,c int);

  select * from a join tabForJoin on(a.b=tabForJoin.b);

  Got an exception like this
  FAILED: Error in semantic analysis:  Invalid Table Alias tabForJoin
{noformat}

But if i give without capital letters ,It is working",chinnalalam,chinnalalam,Major,Closed,Fixed,08/Feb/11 15:29,16/Dec/11 23:56
Bug,HIVE-1974,12497994,"In error scenario some opened streams may not closed in ScriptOperator.java, Utilities.java ","1)In error scenario StreamProcessor may not be closed in ScriptOperator.java
2)In error scenario XMLEncoder may not be closed in Utilities.java",chinnalalam,chinnalalam,Major,Closed,Fixed,08/Feb/11 15:32,16/Dec/11 23:56
Bug,HIVE-1975,12497995,"""insert overwrite directory"" Not able to insert data with multi level directory path","Below query execution is failed

Ex:
{noformat}
   insert overwrite directory '/HIVEFT25686/chinna/' select * from dept_j;
{noformat}",chinnalalam,chinnalalam,Major,Closed,Fixed,08/Feb/11 15:34,25/Nov/13 20:13
Bug,HIVE-1976,12497997,"Exception should be thrown when invalid jar,file,archive is given to add command","When executed add command with non existing jar it should throw exception through   HiveStatement

Ex:
{noformat}
  add jar /root/invalidpath/testjar.jar
{noformat}

Here testjar.jar is not exist so it should throw exception.",chinnalalam,chinnalalam,Major,Closed,Fixed,08/Feb/11 15:37,16/Dec/11 23:56
Bug,HIVE-1977,12498035,DESCRIBE TABLE syntax doesn't support specifying a database qualified table name,"The syntax for DESCRIBE is broken. It should be:

{code}
DESCRIBE [EXTENDED] [database DOT]table [column]
{code}

but is actually

{code}
DESCRIBE [EXTENDED] table[DOT col_name]
{code}


Ref: http://dev.mysql.com/doc/refman/5.0/en/describe.html",zhenxiao,cwsteinbach,Major,Closed,Fixed,08/Feb/11 22:08,24/Mar/20 15:17
Bug,HIVE-1979,12498067,fix hbase_bulk.m by setting HiveInputFormat,This was broken by the switch to CombineHiveInputFormat in HIVE-1942.,jvs,jvs,Blocker,Closed,Fixed,09/Feb/11 01:20,16/Dec/11 23:59
Bug,HIVE-1980,12498166,Merging using mapreduce rather than map-only job failed in case of dynamic partition inserts,"In dynamic partition insert and if merge is set to true and hive.mergejob.maponly=false, the merge MapReduce job will fail. ",nzhang,nzhang,Major,Closed,Fixed,09/Feb/11 19:32,16/Dec/11 23:56
Bug,HIVE-1981,12498173,TestHadoop20SAuthBridge failed on current trunk,I'm on the latest trunk and ant package test failed on TestHadoop20SAuthBridge.,cwsteinbach,nzhang,Blocker,Closed,Fixed,09/Feb/11 20:35,17/Dec/11 00:01
Bug,HIVE-1987,12498510,HWI admin_list_jobs JSP page throws exception,"It looks like the admin_list_jobs.jsp page is trying to reference ExecDriver.runningJobKillURIs, which is now a private to ExecDriver:

{code}

RequestURI=/hwi/admin_list_jobs.jsp

Caused by:

org.apache.jasper.JasperException: Unable to compile class for JSP

An error occurred at line: 24 in the jsp file: /admin_list_jobs.jsp
Generated servlet error:
The field ExecDriver.runningJobKillURIs is not visible

An error occurred at line: 27 in the jsp file: /admin_list_jobs.jsp
Generated servlet error:
The field ExecDriver.runningJobKillURIs is not visible


	at org.apache.jasper.compiler.DefaultErrorHandler.javacError(DefaultErrorHandler.java:84)
	at org.apache.jasper.compiler.ErrorDispatcher.javacError(ErrorDispatcher.java:328)
	at org.apache.jasper.compiler.JDTCompiler.generateClass(JDTCompiler.java:409)
	at org.apache.jasper.compiler.Compiler.compile(Compiler.java:288)
	at org.apache.jasper.compiler.Compiler.compile(Compiler.java:267)
	at org.apache.jasper.compiler.Compiler.compile(Compiler.java:255)
	at org.apache.jasper.JspCompilationContext.compile(JspCompilationContext.java:563)
	at org.apache.jasper.servlet.JspServletWrapper.service(JspServletWrapper.java:293)
	at org.apache.jasper.servlet.JspServlet.serviceJspFile(JspServlet.java:314)
	at org.apache.jasper.servlet.JspServlet.service(JspServlet.java:264)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:502)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:363)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:181)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:417)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.handler.RequestLogHandler.handle(RequestLogHandler.java:49)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:324)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:534)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:864)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:533)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:207)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:403)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:522)
{code}",appodictic,cwsteinbach,Major,Closed,Fixed,14/Feb/11 09:15,16/Dec/11 23:56
Bug,HIVE-1988,12498517,Make the delegation token issued by the MetaStore owned by the right user,"The 'owner' of any delegation token issued by the MetaStore is set to the requesting user. When a delegation token is asked by the user himself during a job submission, this is fine. However, in the case where the token is requested for by services (e.g., Oozie), on behalf of the user, the token's owner is set to the user the service is running as. Later on, when the token is used by a MapReduce task, the MetaStore treats the incoming request as coming from Oozie and does operations as Oozie. This means any new directory creations (e.g., create_table) on the hdfs by the MetaStore will end up with Oozie as the owner.

Also, the MetaStore doesn't check whether a user asking for a token on behalf of some other user, is actually authorized to act on behalf of that other user. We should start using the ProxyUser authorization in the MetaStore (HADOOP-6510's APIs).",ddas,ddas,Major,Closed,Fixed,14/Feb/11 11:45,16/Dec/11 23:56
Bug,HIVE-1995,12498712,Mismatched open/commit transaction calls when using get_partition(),Nested executeWithRetry() calls caused by using HiveMetaStore.get_partition() can result in mis-matched open/commit calls. Fixes the same issue as described in HIVE-1760.,pauly,pauly,Minor,Closed,Fixed,16/Feb/11 02:23,16/Dec/11 23:59
Bug,HIVE-1998,12498961,Update README.txt and add missing ASF headers,Need to update README.txt for the 0.7.0 release. Also need to add missing ASF file headers.,cwsteinbach,cwsteinbach,Blocker,Closed,Fixed,17/Feb/11 21:38,16/Dec/11 23:59
Bug,HIVE-2001,12499425,Add inputs and outputs to authorization DDL commands,"When permissions are changed for a table/partition, the respective object should be present in the read/write entities for hooks to act on.",he yongqiang,he yongqiang,Major,Closed,Fixed,22/Feb/11 23:52,16/Dec/11 23:56
Bug,HIVE-2003,12499462,LOAD compilation does not set the outputs during semantic analysis resulting in no authorization checks being done for it.,The table/partition being loaded is not being added to outputs in the LoadSemanticAnalyzer.,n_krishna_kumar,n_krishna_kumar,Minor,Closed,Fixed,23/Feb/11 09:31,16/Dec/11 23:56
Bug,HIVE-2007,12499698,Executing queries using Hive Server is not logging to the log file specified in hive-log4j.properties,"Start Hive Server by specifying the log details ( filelocation , appender , loglevel ) in hive-log4j.properties, but logging is not happening as per the details provided in the hive-log4j.properties.",chinnalalam,chinnalalam,Major,Closed,Fixed,25/Feb/11 11:40,17/Dec/11 00:00
Bug,HIVE-2008,12499772,keyword_1.q is failing,"Paul/Yongqiang, can you guys fix this one?

https://hudson.apache.org/hudson/job/Hive-trunk-h0.20/578/
",pauly,jvs,Major,Closed,Fixed,25/Feb/11 22:39,16/Dec/11 23:56
Bug,HIVE-2011,12499956,upgrade-0.6.0.mysql.sql script attempts to increase size of PK COLUMNS.TYPE_NAME to 4000,"{code}
# mysql flumenewresearch < upgrade-0.6.0.mysql.sql 
ERROR 1071 (42000) at line 16: Specified key was too long; max key length is 767 bytes
{code}

Here's the cause of the problem from upgrade-0.6.0.mysql.sql:

{code}
...
ALTER TABLE `COLUMNS` MODIFY `TYPE_NAME` VARCHAR(4000);
...
ALTER TABLE `COLUMNS` DROP PRIMARY KEY;
ALTER TABLE `COLUMNS` ADD PRIMARY KEY (`SD_ID`, `COLUMN_NAME`);
...
{code}

We need to make sure that the PK on COLUMNS.TYPE_NAME is dropped before the size of the column is bumped to 4000.
",cwsteinbach,cwsteinbach,Blocker,Closed,Fixed,28/Feb/11 18:10,16/Dec/11 23:59
Bug,HIVE-2015,12500017,Eliminate bogus Datanucleus.Plugin Bundle ERROR log messages,"Every time I start up the Hive CLI with logging enabled I'm treated to the following ERROR log messages courtesy of DataNucleus:

{code}
DEBUG metastore.ObjectStore: datanucleus.plugin.pluginRegistryBundleCheck = LOG 
ERROR DataNucleus.Plugin: Bundle ""org.eclipse.jdt.core"" requires ""org.eclipse.core.resources"" but it cannot be resolved. 
ERROR DataNucleus.Plugin: Bundle ""org.eclipse.jdt.core"" requires ""org.eclipse.core.runtime"" but it cannot be resolved. 
ERROR DataNucleus.Plugin: Bundle ""org.eclipse.jdt.core"" requires ""org.eclipse.text"" but it cannot be resolved.
{code}

Here's where this comes from:
* The bin/hive scripts cause Hive to inherit Hadoop's classpath.
* Hadoop's classpath includes $HADOOP_HOME/lib/core-3.1.1.jar, an Eclipse library.
* core-3.1.1.jar includes a plugin.xml file defining an OSGI plugin
* At startup, Datanucleus scans the classpath looking for OSGI plugins, and will attempt to initialize any that it finds, including the Eclipse OSGI plugins located in core-3.1.1.jar
* Initialization of the OSGI plugin in core-3.1.1.jar fails because of unresolved dependencies.
* We see an ERROR message telling us that Datanucleus failed to initialize a plugin that we don't care about in the first place.

I can think of two options for solving this problem:
# Rewrite the scripts in $HIVE_HOME/bin so that they don't inherit ALL of Hadoop's CLASSPATH.
# Replace DataNucleus's NOnManagedPluginRegistry with our own implementation that does nothing.

",zhenxiao,cwsteinbach,Major,Closed,Fixed,01/Mar/11 07:59,15/Oct/13 23:30
Bug,HIVE-2022,12500145,Making JDO thread-safe by default,"If there are multiple thread accessing metastore concurrently, there are cases that JDO threw exceptions because of concurrent access of HashMap inside JDO. Setting javax.jdo.option.Multithreaded to true solves this issue. ",nzhang,nzhang,Major,Closed,Fixed,02/Mar/11 04:45,16/Dec/11 23:57
Bug,HIVE-2024,12500249,"In Driver.execute(), mapred.job.tracker is not restored if one of the task fails.","If automatically one job is determined to run in local mode, and the task fails with error code not 0, mapred.job.tracker will remain to be local and might cause further problems.",sdong,sdong,Major,Closed,Fixed,02/Mar/11 23:05,16/Dec/11 23:56
Bug,HIVE-2025,12500270,Fix TestEmbeddedHiveMetaStore and TestRemoteHiveMetaStore broken by HIVE-2022,"The patch for HIVE-2022 broke TestEmbeddedHiveMetaStore and TestRemoteHiveMetaStore

https://hudson.apache.org/hudson/job/Hive-trunk-h0.20/590/

@Paul: Assigning this to you.",nzhang,cwsteinbach,Critical,Closed,Fixed,03/Mar/11 04:22,16/Dec/11 23:55
Bug,HIVE-2031,12500751,Correct the exception message for the better traceability for the scenario load into the partitioned table having 2  partitions by specifying only one partition in the load statement. ," Load into the partitioned table having 2 partitions by specifying only one partition in the load statement is failing and logging the following exception message.

{noformat}
 org.apache.hadoop.hive.ql.parse.SemanticException: line 1:91 Partition not found '21Oct'
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer$tableSpec.<init>(BaseSemanticAnalyzer.java:685)
	at org.apache.hadoop.hive.ql.parse.LoadSemanticAnalyzer.analyzeInternal(LoadSemanticAnalyzer.java:196)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:736)
	at org.apache.hadoop.hive.service.HiveServer$HiveServerHandler.execute(HiveServer.java:151)
	at org.apache.hadoop.hive.service.ThriftHive$Processor$execute.process(ThriftHive.java:764)
	at org.apache.hadoop.hive.service.ThriftHive$Processor.process(ThriftHive.java:742)
	at org.apache.thrift.server.TThreadPoolServer$WorkerProcess.run(TThreadPoolServer.java:253)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
{noformat}

This needs to be corrected in such a way what is the actual root cause for this.",chinnalalam,chinnalalam,Major,Closed,Fixed,08/Mar/11 11:38,16/Dec/11 23:56
Bug,HIVE-2034,12500818,Backport HIVE-1991 after overridden by HIVE-1950,,sdong,sdong,Trivial,Closed,Fixed,08/Mar/11 21:40,16/Dec/11 23:55
Bug,HIVE-2037,12500848,Merge result file size should honor hive.merge.size.per.task,"The merge job set mapred.min.split.size to the value of hive.merge.size.per.task, which roughly equals to the output file size. However the input split size is also determined by mapred.min.split.size.per.node, mapred.min.split.size.per.rack, and mapred.max.split.size. They should be set the same as hive.merge.size.per.task as well.",nzhang,nzhang,Major,Closed,Fixed,09/Mar/11 07:12,16/Dec/11 23:56
Bug,HIVE-2040,12501082,the retry logic in Hive's concurrency  is not working correctly.,,he yongqiang,he yongqiang,Major,Closed,Fixed,10/Mar/11 23:21,16/Dec/11 23:56
Bug,HIVE-2042,12501117,In error scenario some opened streams may not closed,"1) In error scenario PrintStream may not be closed in execute() of  ExplainTask.java
2) In error scenario InputStream may not be closed in checkJobTracker() of Throttle.java ",chinnalalam,chinnalalam,Major,Closed,Fixed,11/Mar/11 08:23,16/Dec/11 23:57
Bug,HIVE-2045,12501120,TCTLSeparatedProtocol.SimpleTransportTokenizer.nextToken() throws Null Pointer Exception in some cases,"1) In TCTLSeparatedProtocol.SimpleTransportTokenizer.nextToken() is doing null check for the tokenizer.
If tokenizer is null, fillTokenizer() method is called to get the tokenizer object. But fillTokenizer() method also can update the tokenizer with NULL , so NULL check should be done before using the tokenizer.

2) Also improved some logging in TCTLSeparatedProtocol.java",chinnalalam,chinnalalam,Major,Closed,Fixed,11/Mar/11 08:39,16/Dec/11 23:57
Bug,HIVE-2054,12501452,"Exception on windows when using the jdbc driver. ""IOException: The system cannot find the path specified""","It seems something recently changed on the jdbc driver which causes this IOException on windows.

java.lang.RuntimeException: java.io.IOException: The system cannot find the path specified
	at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:237)
	at org.apache.hadoop.hive.jdbc.HiveConnection.<init>(HiveConnection.java:73)
	at org.apache.hadoop.hive.jdbc.HiveDriver.connect(HiveDriver.java:110)
",bennies,bennies,Minor,Closed,Fixed,15/Mar/11 12:47,27/Jul/11 04:20
Bug,HIVE-2055,12501467,Hive should add HBase classpath dependencies when available,"Created an external table in hive , which points to the HBase table. When tried to query a column using the column name in select clause got the following exception : ( java.lang.ClassNotFoundException: org.apache.hadoop.hive.hbase.HiveHBaseTableInputFormat), errorCode:12, SQLState:42000)",ndimiduk,sajithv,Major,Resolved,Fixed,15/Mar/11 14:32,19/Nov/13 20:44
Bug,HIVE-2059,12501611,Add datanucleus.identifierFactory property to HiveConf to avoid unintentional MetaStore Schema corruption,"Hive 0.6.0 we upgraded the version of DataNucleus from 1.0 to 2.0, which changed some of the defaults for how field names get mapped to datastore identifiers. This was problem was resolved in HIVE-1435 by setting datanucleus.identifierFactory=datanucleus in hive-default.xml

However, this property definition was not added to HiveConf. This can result in schema corruption if the user upgrades from Hive 0.5.0 to 0.6.0 or 0.7.0 and retains the Hive 0.5.0 version hive-default.xml on their classpath.",cwsteinbach,cwsteinbach,Blocker,Closed,Fixed,16/Mar/11 18:56,17/Dec/11 00:01
Bug,HIVE-2060,12501716,CLI local mode hit NPE when exiting by ^D,CLI gets an NPE when running in local mode and hit an ^D to exit it. ,nzhang,nzhang,Minor,Closed,Fixed,17/Mar/11 18:28,16/Dec/11 23:56
Bug,HIVE-2061,12501761,Create a hive_contrib.jar symlink to hive-contrib-{version}.jar for backward compatibility,"We have seen a use case where in the user's script, it run 'add jar hive_contrib.jar'. Since Hive has moved the jar file to be hive-contrib-{version}.jar, it introduced backward incompatibility. If we as the user to change the script and when Hive upgrade version again, the user need to change the script again. Creating a symlink seems to be the best solution. ",nzhang,nzhang,Minor,Closed,Fixed,18/Mar/11 05:26,16/Dec/11 23:55
Bug,HIVE-2062,12501785,HivePreparedStatement.executeImmediate always throw exception,"executeImmediate:

try {
  clearWarnings();
  resultSet = null;
  client.execute(sql);
}

but:
  public void clearWarnings() throws SQLException {
    // TODO Auto-generated method stub
    throw new SQLException(""Method not supported"");
  }

in result all calls executeQuery() for prepared statement return exception",humanoid,humanoid,Critical,Closed,Fixed,18/Mar/11 13:09,16/Dec/11 23:57
Bug,HIVE-2064,12501807,Make call to SecurityUtil.getServerPrincipal unambiguous,"HadoopThriftAuthBridge20S calls SecurityUtil.getServerPrincipal and passes null for the 2nd arg. When building against the hadoop security branch this is a compilation error as it matches the signatures of both getServerPrincipal methods (one takes a String for the 2nd arg, one an InetAddress). This call needs to be made unambiguous eg by passing ""0.0.0.0"" instead of null, which per the getServerPrincipal javadoc is equivalent:

{quote}
It replaces hostname pattern with hostname, which should be
   * fully-qualified domain name. If hostname is null or ""0.0.0.0"", it uses
   * dynamically looked-up fqdn of the current host instead.
{quote}
",eli,eli,Blocker,Closed,Fixed,18/Mar/11 17:14,16/Dec/11 23:59
Bug,HIVE-2069,12502100,NullPointerException on getSchemas,Calling getSchemas will cause a nullpointerexception,bennies,bennies,Major,Closed,Fixed,22/Mar/11 20:32,16/Sep/14 08:52
Bug,HIVE-2080,12502662,Few code improvements in the ql and serde packages.,"Few code improvements in the ql and serde packages.
1) Little performance Improvements 
2) Null checks to avoid NPEs
3) Effective varaible management.",chinnalalam,chinnalalam,Major,Closed,Fixed,29/Mar/11 06:42,16/Dec/11 23:56
Bug,HIVE-2083,12502782,Bug: RowContainer was set to 1 in JoinUtils.,This cause the skew join super slow because the row container dump every record to disk before using them.,he yongqiang,he yongqiang,Major,Closed,Fixed,30/Mar/11 00:53,16/Dec/11 23:56
Bug,HIVE-2086,12503051,Add test coverage for external table data loss issue,"Data loss when using ""create external table like"" statement. 

1) Set up an external table S, point to location L. Populate data in S.
2) Create another external table T, using statement like this:
    create external table T like S location L
   Make sure table T point to the same location as the original table S.
3) Query table T, see the same set of data in S.
4) drop table T.
5) Query table S will return nothing, and location L is deleted. ",natty,qlong,Major,Closed,Fixed,31/Mar/11 18:05,16/Dec/11 23:57
Bug,HIVE-2095,12503460,auto convert map join bug,"1) 
when considering to choose one table as the big table candidate for a map join, if at compile time, hive can find out that the total known size of all other tables excluding the big table in consideration is bigger than a configured value, this big table candidate is a bad one, and should not put into plan. Otherwise, at runtime to filter this out may cause more time.

2)
added a null check for back up tasks. Otherwise will see NullPointerException

3)
CommonJoinResolver needs to know a full mapping of pathToAliases. Otherwise it will make wrong decision.

4)
changes made to the ConditionalResolverCommonJoin: added pathToAliases, aliasToSize (alias's input size that is known at compile time, by inputSummary), and intermediate dir path.
So the logic is, go over all the pathToAliases, and for each path, if it is from intermediate dir path, add this path's size to all aliases. And finally based on the size information and others like aliasToTask to choose the big table. 

5)
Conditional task's children contains wrong options, which may cause join fail or incorrect results. Basically when getting all possible children for the conditional task, should use a whitelist of big tables. Only tables in this while list can be considered as a big table.
Here is the logic:

* Get a list of big table candidates. Only the tables in the returned set can be used as big table in the join operation.
* The logic here is to scan the join condition array from left to right. 
** If see a inner join and the bigTableCandidates is empty, add both side of this inner join to big table candidates. 
** If see a left outer join, and the bigTableCandidates is empty, add the left side to it, and 
** if the bigTableCandidates is not empty, do nothing (which means the bigTableCandidates is from left side). 
** If see a right outer join, clear the bigTableCandidates, and add right side to the bigTableCandidates, it means the right side of a right outer join always win. 
** If see a full outer join, return null immediately (no one can be the big table, can not do a mapjoin).
",he yongqiang,he yongqiang,Major,Closed,Fixed,05/Apr/11 21:42,12/Nov/12 21:01
Bug,HIVE-2096,12503478,throw a error if the input is larger than a threshold for index input format,This can hang for ever.,boteku,namit,Major,Closed,Fixed,06/Apr/11 00:07,16/Dec/11 23:56
Bug,HIVE-2098,12503643,Make couple of convenience methods in EximUtil public,readMetaData() and createExportDump() to be public,n_krishna_kumar,n_krishna_kumar,Minor,Closed,Fixed,07/Apr/11 15:06,16/Dec/11 23:56
Bug,HIVE-2100,12503761,virtual column references inside subqueries cause execution exceptions,"example:
create table jssarma_nilzma_bad as select a.fname, a.offset, a.val from (select hash(eventid,userid,eventtime,browsercookie,userstate,useragent,userip,serverip,clienttime,geoid,countrycode\
,actionid,lastimpressionid,lastnavimpressionid,impressiontype,fullurl,fullreferrer,pagesection,modulesection,adsection) as val, INPUT__FILE__NAME as fname, BLOCK__OFFSET__INSIDE__FILE as offset from nectar_impression_lzma_unverified where ds='2010-07-28') a join jssarma_hc_diff b on (a.val=b.val);""

causes

Caused by: java.lang.RuntimeException: Map operator initialization failed
	at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:121)
	... 18 more
Caused by: java.lang.RuntimeException: cannot find field input__file__name from [org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@664310d0, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@3d04fc23, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@12457d21, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@101a0ae6, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@1dc18a4c, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@d5e92d7, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@3bfa681c, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@34c92507, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@19e09a4, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@2e8aeed0, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@2344b18f, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@72e5355f, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@26132ae7, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@3465b738, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@1dfd868, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@ef894ce, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@61f1680f, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@2fe6e305, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@5f4275d4, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@445e228, org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector$MyField@802b249]
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:321)
	at org.apache.hadoop.hive.serde2.objectinspector.UnionStructObjectInspector.getStructFieldRef(UnionStructObjectInspector.java:96)
	at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:57)
	at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:878)
	at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:904)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:60)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:433)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:389)
	at org.apache.hadoop.hive.ql.exec.FilterOperator.initializeOp(FilterOperator.java:73)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:433)
	at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:389)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:133)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
	at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:444)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
	at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:98)
	... 18 more


running the subquery separately fixes the issue.
",salbiz,jsensarma,Major,Closed,Fixed,08/Apr/11 17:47,16/Dec/11 23:56
Bug,HIVE-2101,12503769,mapjoin sometimes gives wrong results if there is a filter in the on condition,"""SELECT / * + mapjoin(src1, src2) * / * FROM src src1 RIGHT OUTER JOIN src src2 ON (src1.key = src2.key AND src1.key < 10 AND src2.key > 10) JOIN src src3 ON (src2.key = src3.key AND src3.key < 10) SORT BY src1.key, src1.value, src2.key, src2.value, src3.key, src3.value;"" will give wrong results in today's hive",navis,he yongqiang,Major,Closed,Fixed,08/Apr/11 19:11,10/Jan/13 19:54
Bug,HIVE-2107,12503988,Log4J initialization info should not be printed out if -S is specified,"SessionState.initHiveLog4j() is called before hive command argument -S is parsed, so initHiveLog4j will printed out the init info even though -S is specified.",nzhang,nzhang,Minor,Closed,Fixed,11/Apr/11 22:31,16/Dec/11 23:56
Bug,HIVE-2113,12504227,"In shell mode, local mode continues if a local-mode task throws exception in pre-hooks","In Driver.execute(), if exception is thrown in prehooks, the execution jumps to the exception handling logic and skips the step of ctx.restoreOriginalTracker(). As a result, the next query will be executed as local mode no matter input size since the job tracker setting is not reverted.",sdong,sdong,Major,Closed,Fixed,14/Apr/11 04:54,16/Dec/11 23:56
Bug,HIVE-2117,12504577,insert overwrite ignoring partition location,"The following code works differently in 0.5.0 vs 0.7.0.

In 0.5.0 the partition location is respected. 

However in 0.7.0 while the initial partition is create with the specified location ""<path>/parta"", the ""insert overwrite ..."" results in the partition written to ""<path>/dt=a"" (note that <path> is the same in both cases).

{code}
create table foo_stg (bar INT, car INT); 
load data local inpath 'data.txt' into table foo_stg;
 
create table foo4 (bar INT, car INT) partitioned by (dt STRING) LOCATION '/user/hive/warehouse/foo4'; 
alter table foo4 add partition (dt='a') location '/user/hive/warehouse/foo4/parta';
 
from foo_stg fs insert overwrite table foo4 partition (dt='a') select *;
{code}

From what I can tell HIVE-1707 introduced this via a change to
org.apache.hadoop.hive.ql.metadata.Hive.loadPartition(Path, String, Map<String, String>, boolean, boolean)
specifically:

{code}
+      Path partPath = new Path(tbl.getDataLocation().getPath(),
+          Warehouse.makePartPath(partSpec));
+
+      Path newPartPath = new Path(loadPath.toUri().getScheme(), loadPath
+          .toUri().getAuthority(), partPath.toUri().getPath());
{code}

Reading the description on HIVE-1707 it seems that this may have been done purposefully, however given the partition location is explicitly specified for the partition in question it seems like that should be honored (esp give the table location has not changed).

This difference in behavior is causing a regression in existing production Hive based code. I'd like to take a stab at addressing this, any suggestions?

",phunt,phunt,Blocker,Closed,Fixed,18/Apr/11 18:26,27/Jul/11 04:20
Bug,HIVE-2120,12504634,auto convert map join may miss good candidates,"In case in a join, there is a subquery which does a simple select, the auto convert map join may miss a good candidate at run time. The plan generated is correct, but the selection at runtime has a bug.

For example:
set hive.smalltable.filesize=1000;
create table src_one as select * from src where key=100;
select count(1)
from 
(
select * from src
) subq 
join 
src_small on src.key = subq.key;

The table src_small can be a small table. This is in the plan, but at runtime it gets filtered out.
",he yongqiang,he yongqiang,Major,Closed,Fixed,19/Apr/11 08:30,16/Dec/11 23:56
Bug,HIVE-2122,12504877,Remove usage of deprecated methods from org.apache.hadoop.io package,Serde code uses some deprecated methods from org.apache.hadoop.io. package. We should remove them.,amareshwari,amareshwari,Minor,Closed,Fixed,21/Apr/11 10:36,16/Dec/11 23:56
Bug,HIVE-2125,12504992,alter table concatenate fails and deletes data,"the number of reducers is not set by this command (unlike other hive queries). since mapred.reduce.tasks=-1 (to let hive infer this automatically) - jobtracker fails the job (number of reducers cannot be negative)

hive> alter table ad_imps_2 partition(ds='2009-06-16') concatenate;
alter table ad_imps_2 partition(ds='2009-06-16') concatenate;
Starting Job = job_201103101203_453180, Tracking URL = http://curium.data.facebook.com:50030/jobdetails.jsp?jobid=job_201103101203_453180
Kill Command = /mnt/vol/hive/sites/curium/hadoop/bin/../bin/hadoop job  -Dmapred.job.tracker=curium.data.facebook.com:50029 -kill job_201103101203_453180
Hadoop job information for null: number of mappers: 0; number of reducers: 0
2011-04-22 10:21:24,046 null map = 100%,  reduce = 100%
Ended Job = job_201103101203_453180 with errors
Moved to trash: /user/facebook/warehouse/ad_imps_2/_backup.ds=2009-06-16
after the job fails - the partition is deleted

thankfully it's still in trash",he yongqiang,jsensarma,Critical,Closed,Fixed,22/Apr/11 17:36,16/Dec/11 23:56
Bug,HIVE-2131,12505102,Bitmap Operation UDF doesn't clear return list,"The AbstractGenericUDFEWAHBitmapBop.java does not clear the return list when evaluate() is called, causing each subsequent call to a bitmap operation to return the wrong values.",mwang5,mwang5,Major,Closed,Fixed,25/Apr/11 10:01,16/Dec/11 23:56
Bug,HIVE-2138,12505858,Exception when no splits returned from index,"Running a query that uses indexing but doesn't return any results give an exception.

{code} java.lang.IllegalArgumentException: Can not create a Path from an empty string
at org.apache.hadoop.fs.Path.checkPathArg(Path.java:82)
at org.apache.hadoop.fs.Path.<init>(Path.java:90)
at org.apache.hadoop.util.StringUtils.stringToPath(StringUtils.java:224)
at org.apache.hadoop.mapred.FileInputFormat.setInputPaths(FileInputFormat.java:282)
at org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat.getSplits(HiveIndexedInputFormat.java:123) {code}

This could potentially be fixed by creating a new empty file to use for the splits.

Once this is fixed, the index_auto_test_if_used.q can be used.",salbiz,rmelick,Major,Closed,Fixed,01/May/11 00:19,16/Dec/11 23:57
Bug,HIVE-2142,12505984,Jobs do not get killed even when they created too many files.,,he yongqiang,he yongqiang,Major,Closed,Fixed,02/May/11 21:56,16/Dec/11 23:56
Bug,HIVE-2145,12506000,NPE during parsing order-by expression,"The following query throws NPE, where it should have throw parsing exception. 

hive> select key, count(1) cnt from src group by key order by count(1) limit 10;
select key, count(1) cnt from src group by key order by count(1) limit 10;
FAILED: Hive Internal Error: java.lang.NullPointerException(null)
java.lang.NullPointerException
	at org.apache.hadoop.hive.ql.plan.ExprNodeGenericFuncDesc.newInstance(ExprNodeGenericFuncDesc.java:153)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.getXpathOrFuncExprNodeDesc(TypeCheckProcFactory.java:640)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory$DefaultExprProcessor.process(TypeCheckProcFactory.java:761)
	at org.apache.hadoop.hive.ql.lib.DefaultRuleDispatcher.dispatch(DefaultRuleDispatcher.java:89)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.dispatch(DefaultGraphWalker.java:88)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.walk(DefaultGraphWalker.java:125)
	at org.apache.hadoop.hive.ql.lib.DefaultGraphWalker.startWalking(DefaultGraphWalker.java:102)
	at org.apache.hadoop.hive.ql.parse.TypeCheckProcFactory.genExprNode(TypeCheckProcFactory.java:156)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:6830)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genExprNodeDesc(SemanticAnalyzer.java:6788)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genReduceSinkPlan(SemanticAnalyzer.java:4303)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genBodyPlan(SemanticAnalyzer.java:5461)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.genPlan(SemanticAnalyzer.java:6022)
	at org.apache.hadoop.hive.ql.parse.SemanticAnalyzer.analyzeInternal(SemanticAnalyzer.java:6607)
	at org.apache.hadoop.hive.ql.parse.BaseSemanticAnalyzer.analyze(BaseSemanticAnalyzer.java:238)
	at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:340)
	at org.apache.hadoop.hive.ql.Driver.run(Driver.java:790)
	at org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:209)
	at org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:286)
	at org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:514)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.util.RunJar.main(RunJar.java:156)


We should still allow functions in the order by clause (as in MySQL) but fix the NPE.",chinnalalam,nzhang,Major,Closed,Fixed,03/May/11 00:52,16/Dec/11 23:56
Bug,HIVE-2146,12506004,Block Sampling should adjust number of reducers accordingly to make it useful,"Now number of reducers of block sampling is not modified, so that queries like:
select c from tab tablesample(1 percent) group by c;
can generate huge number of reducers although the input is sampled to be small.
We need to shrink number of reducers to make block sampling more useful.
Since now number of reducers are determined before get splits, the way to do it probably is not clean enough, but we can do a good guess.",sdong,sdong,Major,Closed,Fixed,03/May/11 01:26,16/Dec/11 23:56
Bug,HIVE-2151,12506358,Too many open files in running negative cli tests,,he yongqiang,he yongqiang,Major,Closed,Fixed,06/May/11 01:57,16/Dec/11 23:55
Bug,HIVE-2152,12506647,Hive throws an NPE if hive-default.xml.,"If you don't have hive-default.xml in your classpath, you get the following error when you try to show tables in the the hive shell:

hive> show tables; 
FAILED: Error in metadata: java.lang.NullPointerException 
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask

It would be a lot more useful to print message indicating that hive-default.xml. This problem will become mute if HIVE-1530 gets accepted.",,fwiffo,Major,Resolved,Fixed,09/May/11 18:41,27/Feb/13 08:18
Bug,HIVE-2153,12506654,Stats JDBC LIKE queries should escape '_' and '%',"DELETE /* Hive stats aggregation: org.apache.hadoop.hive.ql.stats.jdbc.JDBCStatsAggregator */ FROM PARTITION_STAT_TBL WHERE ID LIKE 'hdfs://dfsnode:9000/tmp/hive-root/hive_2011-05-09_04-30-28_586_4184342157898880918/-ext-10000/ds=2011-05-08/table_name=dim_page_to_user_suggest_assoc/%'

It is a prefix query but the '_' in the ID column should be escaped. The same applies to '%' if they appear in ID. ",nzhang,nzhang,Major,Closed,Fixed,09/May/11 20:23,16/Dec/11 23:56
Bug,HIVE-2157,12506762,NPE in MapJoinObjectKey,,he yongqiang,he yongqiang,Major,Closed,Fixed,10/May/11 20:05,16/Dec/11 23:56
Bug,HIVE-2159,12506899,"TableSample(percent ) uses one intermediate size to be int, which overflows for large sampled size, making the sampling never triggered.",,sdong,sdong,Major,Closed,Fixed,11/May/11 19:48,16/Dec/11 23:56
Bug,HIVE-2160,12506970,"Few code improvements in the metastore,hwi and ql packages.","Few code improvements in the metastore,hwi and ql packages.
1) Little performance Improvements 
2) Effective varaible management.",chinnalalam,chinnalalam,Minor,Closed,Fixed,12/May/11 11:31,16/Dec/11 23:55
Bug,HIVE-2176,12507944,Schema creation scripts are incomplete since they leave out tables that are specific to DataNucleus,"When using the DDL SQL scripts to create the Metastore, tables like SEQUENCE_TABLE are missing and force the user to change the configuration to use Datanucleus to do all the provisioning of the Metastore tables. Adding the missing table definitions to the DDL scripts will allow to have a functional Hive Metastore without enabling additional privileges to the Metastore user and/or enabling datanucleus.autoCreateSchema property in hive-site.xml


[After running the hive-schema-0.7.0.mysql.sql and revoking ALTER and CREATE privileges to the 'metastoreuser']

hive> show tables; 
FAILED: Error in metadata: javax.jdo.JDOException: Exception thrown calling table.exists() for `SEQUENCE_TABLE` 
NestedThrowables: 
com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: CREATE command denied to user 'metastoreuser'@'localhost' for table 'SEQUENCE_TABLE' 
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask

",esteban,esteban,Major,Closed,Fixed,21/May/11 00:28,27/Jul/11 04:20
Bug,HIVE-2178,12508139,Log related Check style Comments fixes,Fix Log related Check style Comments,chinnalalam,chinnalalam,Major,Closed,Fixed,24/May/11 06:35,16/Dec/11 23:56
Bug,HIVE-2181,12508159, Clean up the scratch.dir (tmp/hive-root) while restarting Hive server. ,"Now queries leaves the map outputs under scratch.dir after execution. If the hive server is stopped we need not keep the stopped server's map oputputs. So whle starting the server we can clear the scratch.dir. This can help in improved disk usage.
",chinnalalam,sanoj,Minor,Closed,Fixed,24/May/11 09:47,11/Apr/16 23:13
Bug,HIVE-2182,12508273,Avoid null pointer exception when executing UDF,"For using UDF's executed following steps

{noformat}
add jar /home/udf/udf.jar;
create temporary function grade as 'udf.Grade';
select m.userid,m.name,grade(m.maths,m.physics,m.chemistry) from marks m;
{noformat}

But from the above steps if we miss the first step (add jar) and execute remaining steps

{noformat}
create temporary function grade as 'udf.Grade';
select m.userid,m.name,grade(m.maths,m.physics,m.chemistry) from marks m;
{noformat}

In tasktracker it is throwing this exception
{noformat}
Caused by: java.lang.RuntimeException: Map operator initialization failed
		 at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:121)
		 ... 18 more
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
		 at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:115)
		 at org.apache.hadoop.hive.ql.udf.generic.GenericUDFBridge.initialize(GenericUDFBridge.java:126)
		 at org.apache.hadoop.hive.ql.exec.ExprNodeGenericFuncEvaluator.initialize(ExprNodeGenericFuncEvaluator.java:133)
		 at org.apache.hadoop.hive.ql.exec.Operator.initEvaluators(Operator.java:878)
		 at org.apache.hadoop.hive.ql.exec.Operator.initEvaluatorsAndReturnStruct(Operator.java:904)
		 at org.apache.hadoop.hive.ql.exec.SelectOperator.initializeOp(SelectOperator.java:60)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:433)
		 at org.apache.hadoop.hive.ql.exec.Operator.initializeChildren(Operator.java:389)
		 at org.apache.hadoop.hive.ql.exec.TableScanOperator.initializeOp(TableScanOperator.java:133)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
		 at org.apache.hadoop.hive.ql.exec.MapOperator.initializeOp(MapOperator.java:444)
		 at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
		 at org.apache.hadoop.hive.ql.exec.ExecMapper.configure(ExecMapper.java:98)
		 ... 18 more
Caused by: java.lang.NullPointerException
		 at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:768)
		 at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:107)
		 ... 31 more
{noformat}
Instead of null pointer exception it should throw meaning full exception",chinnalalam,chinnalalam,Major,Closed,Fixed,25/May/11 06:17,16/Dec/11 23:56
Bug,HIVE-2183,12508275,In Task class and its subclasses logger is initialized in constructor,"In Task class and its subclasses logger is initialized in constructor. Log object no need to initialize every time in the constructor, Log object can make it as static object.

{noformat}
Ex:
  public ExecDriver() {
    super();
    LOG = LogFactory.getLog(this.getClass().getName());
    console = new LogHelper(LOG);
    this.jobExecHelper = new HadoopJobExecHelper(job, console, this, this);
  }
{noformat}

Need to change like this

{noformat}
private static final Log LOG = LogFactory.getLog(ExecDriver.class);
{noformat}

",chinnalalam,chinnalalam,Minor,Closed,Fixed,25/May/11 06:28,16/Dec/11 23:56
Bug,HIVE-2184,12508280,Few improvements in org.apache.hadoop.hive.ql.metadata.Hive.close(),"1)Hive.close() will call HiveMetaStoreClient.close() in this method the variable ""standAloneClient"" is never become true then client.shutdown() never call.


2)Hive.close() After calling metaStoreClient.close() need to make metaStoreClient=null",chinnalalam,chinnalalam,Major,Closed,Fixed,25/May/11 06:54,30/May/12 22:15
Bug,HIVE-2186,12508478,Dynamic Partitioning Failing because of characters not supported globStatus,Some dynamic queries failed on the stage of loading partitions if dynamic partition columns contain special characters. We need to escape all of them.,franklinhu,sdong,Major,Closed,Fixed,26/May/11 21:22,09/Mar/12 01:08
Bug,HIVE-2192,12509209,Stats table schema incompatible after HIVE-2185,"HIVE-2185 introduced a new column in the intermediate stats table. This introduces incompatibility between old and new branches (multiple branches could be deployed in production): the old branch will not work with the new schema, and the new branch will not work with the old schema. A solution would be to rename the stats table name (requires code change) or use a different database name (requires hive-default.xml conf change).",tnykiel,nzhang,Major,Closed,Fixed,03/Jun/11 17:40,16/Dec/11 23:57
Bug,HIVE-2196,12509259,Ensure HiveConf includes all properties defined in hive-default.xml,There are a bunch of properties that are defined in hive-default.xml but not in HiveConf.,chinnalalam,cwsteinbach,Major,Closed,Fixed,04/Jun/11 07:33,26/Jun/14 19:50
Bug,HIVE-2197,12509304,SessionState used before ThreadLocal set,"while invoke the method like :
SessionState.start(new HiveConf(HiveConf.class))

it comes out following exception stack:
at org.apache.hadoop.hive.ql.session.CreateTableAutomaticGrant.getGrantorInfoList(CreateTableAutomaticGrant.java:101)
        at org.apache.hadoop.hive.ql.session.CreateTableAutomaticGrant.getGrantMap(CreateTableAutomaticGrant.java:79)
        at org.apache.hadoop.hive.ql.session.CreateTableAutomaticGrant.create(CreateTableAutomaticGrant.java:41)
        at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:194)

because the CreateTableAutomaticGrant.getGrantorInfoList will try to get Authenticator via SessionState.get().getAuthenticator() while the 'tts'(thread local) is still null(it is being set at line 195 but the invoke happended at line 194 in SessionState.start(conf) ). ",,zizon,Minor,Closed,Fixed,05/Jun/11 11:44,16/Dec/11 23:56
Bug,HIVE-2198,12509351,"While using Hive in server mode, HiveConnection.close() is not cleaning up server side resources","org.apache.hadoop.hive.service.ThriftHive.Client.clean() method is called for every session end in CLI mode for the cleanup but in HiveServer mode this is not called.
So this can be integrate with the HiveConnection.close()",chinnalalam,chinnalalam,Major,Closed,Fixed,06/Jun/11 13:08,16/Dec/11 23:57
Bug,HIVE-2199,12509399,incorrect success flag passed to jobClose,"For block level merging of RCFiles, jobClose is passed the incorrect variable as the success flag",franklinhu,franklinhu,Minor,Closed,Fixed,06/Jun/11 22:30,16/Dec/11 23:56
Bug,HIVE-2204,12509457,unable to get column names for a specific table that has '_' as part of its table name,"I have a table age_group and I am trying to get list of columns for this table name. As underscore and '%' have special meaning in table search pattern according to JDBC searchPattern string specification, I escape the '_' in my table name when I call getColumns for this single table. But HIVE does not return any columns. My call to getColumns is as follows
catalog	<null>
schemaPattern	""%""
tableNamePattern  ""age\_group""
columnNamePattern  ""%""

If I don't escape the '_' in my tableNamePattern, I am able to get the list of columns.",phunt,mgk.424@gmail.com,Major,Closed,Fixed,07/Jun/11 13:21,16/Dec/11 23:56
Bug,HIVE-2211,12509765,Fix a bug caused by HIVE-243,"Quick fix a bug caused by HIVE-243

HIVE-234 removed the codes to wait for the threads to finish and use ThreadPoolExector.shutdown() to wait for the results. The usage of ThreadPoolExecutor.shutdown(), however, is wrong. The codes assume that the function blocks until all threads finish running but it actually only marks status and won't block. It caused wrong result of Utilities.getInputSummary() and caused many jobs are executed as local mode while they have huge data.

Revert those changes quickly. We can have a follow-up to see how to deal with this more efficiently if you want.",sdong,sdong,Major,Closed,Fixed,09/Jun/11 21:26,16/Dec/11 23:56
Bug,HIVE-2214,12509875,CommandNeedRetryException.java is missing ASF header,Please add one.,ashutoshc,jvs,Major,Closed,Fixed,10/Jun/11 21:00,16/Dec/11 23:56
Bug,HIVE-2222,12510385,runnable queue in Driver and DriverContext is not thread safe,,namit,he yongqiang,Major,Closed,Fixed,15/Jun/11 01:43,16/Dec/11 23:56
Bug,HIVE-2237,12511400,hive fails to build in eclipse due to syntax error in BitmapIndexHandler.java,"I see the following error in helios eclipse with the latest trunk (although build on the command line is fine):

Syntax error on token "";"", delete this token

seems to have been introduced by this change in HIVE-2036

+import org.apache.hadoop.hive.ql.index.HiveIndexedInputFormat;;


I have a patch forthcoming.",phunt,phunt,Major,Closed,Fixed,23/Jun/11 20:05,16/Dec/11 23:56
Bug,HIVE-2243,12512028,Can't publish maven release artifacts to apache repository,"So far I haven't been able to push the maven artifacts to the Apache release repository. Here's the error I get:

{noformat}
% ant maven-publish -Dmvn.publish.repo=releases
...
maven-publish-artifact:
[artifact:install-provider] Installing provider: org.apache.maven.wagon:wagon-http:jar:1.0-beta-2:runtime
[artifact:deploy] Deploying to https://repository.apache.org/content/repositories/releases
[artifact:deploy] Uploading: org/apache/hive/hive-anttasks/0.7.1/hive-anttasks-0.7.1.jar to repository apache.releases.https at https://repository.apache.org/content/repositories/releases
[artifact:deploy] Transferring 9K from apache.releases.https
[artifact:deploy] An error has occurred while processing the Maven artifact tasks.
[artifact:deploy]  Diagnosis:
[artifact:deploy] 
[artifact:deploy] Error deploying artifact 'org.apache.hive:hive-anttasks:jar': Error deploying artifact: Authorization failed: Access denied to: https://repository.apache.org/content/repositories/releases/org/apache/hive/hive-anttasks/0.7.1/hive-anttasks-0.7.1.jar
{noformat}

I get the same error when I try to publish to the staging repository.

I took another look at the Apache ""Publishing Maven Artifacts"" guide (http://www.apache.org/dev/publishing-maven-artifacts.html) and think that we're probably failing to include a couple fields that are required in the pom files. It also looks like we should be pushing this to the staging repository as opposed to the releases repository.

",cwsteinbach,cwsteinbach,Major,Closed,Fixed,28/Jun/11 18:13,16/Dec/11 23:56
Bug,HIVE-2248,12512393,Comparison Operators convert number types to common type instead of double if possible,"Now if the two sides of comparison is of different type, we always convert both to double and compare. It was a slight regression from the change in https://issues.apache.org/jira/browse/HIVE-1638. The old UDFOP<Comparison>, using GenericUDFBridge, always tried to find common type first.

The worse case is this: If you did ""WHERE <BIGINT_COLUMN> = 0 "", we always convert the column and 0 to double and compare, which is wasteful, though it is usually a minor costs in the system. But it is easy to fix.",sdong,sdong,Major,Closed,Fixed,30/Jun/11 23:04,16/Dec/11 23:56
Bug,HIVE-2253,12512698,Merge failing of join tree in exceptional case,"In some very exceptional cases, SemanticAnayzer fails to merge join tree. Example is below.

create table a (val1 int, val2 int)
create table b (val1 int, val2 int)
create table c (val1 int, val2 int)
create table d (val1 int, val2 int)
create table e (val1 int, val2 int)

1. all same(single) join key --> one MR, good
select * from a join b on a.val1=b.val1 join c on a.val1=c.val1 join d on a.val1=d.val1 join e on a.val1=e.val1

2. two join keys --> expected to have two MR, but resulted to three MR
select * from a join b on a.val1=b.val1 join c on a.val1=c.val1 join d on a.val1=d.val1 join e on a.val2=e.val2

3. by changing the join order, we could attain two MR as first-expectation.
select * from a join e on a.val2=e.val2 join c on a.val1=c.val1 join d on a.val1=d.val1 join b on a.val1=b.val1
",navis,navis,Minor,Closed,Fixed,04/Jul/11 07:45,16/Dec/11 23:56
Bug,HIVE-2257,12512890,Enable TestHadoop20SAuthBridge,Looks like this test was accidentally disabled in HIVE-818.,cwsteinbach,cwsteinbach,Major,Closed,Fixed,05/Jul/11 20:45,16/Dec/11 23:56
Bug,HIVE-2259,12512896,Skip comments in hive script,"If you specify something like:

-- This is a comment
add jar jar_path;
select * from my_table;

This fails.

I have created a fix to skip the commented lines.",vaggarw,vaggarw,Major,Closed,Fixed,05/Jul/11 21:21,16/Dec/11 23:55
Bug,HIVE-2260,12512906,ExecDriver::addInputPaths should pass the table properties to the record writer,"Currently when ExecDriver encounters a non-existent partition, it creates an empty file so that the query will be valid (and return 0 results).  However, when it does this and calls {{getHiveRecordWriter()}}, it creates a new instance of Properties, rather than providing the Properties associated with the table.

This causes RecordWriters that pull information from the table through the Properties to fail (such as [Haivvreo|http://bit.ly/iwEQzJ]).  The RecordWriter should be provided the table's Properties, as it is in all other cases where it's called.",jghoman,jghoman,Major,Closed,Fixed,05/Jul/11 22:42,16/Dec/11 23:56
Bug,HIVE-2264,12512993,Hive server is SHUTTING DOWN when invalid queries beeing executed.,"When invalid query is beeing executed, Hive server is shutting down.

{noformat}
""CREATE TABLE SAMPLETABLE(IP STRING , showtime BIGINT ) partitioned by (ds string,ipz int) ROW FORMAT DELIMITED FIELDS TERMINATED BY '\040'""

""ALTER TABLE SAMPLETABLE add Partition(ds='sf') location '/user/hive/warehouse' Partition(ipz=100) location '/user/hive/warehouse'""
{noformat}",navis,rohithsharma,Blocker,Closed,Fixed,06/Jul/11 14:05,16/May/13 21:10
Bug,HIVE-2269,12513071,Hive --auxpath option can't handle multiple colon separated values,,cwsteinbach,cwsteinbach,Major,Resolved,Fixed,07/Jul/11 03:02,21/Mar/14 18:54
Bug,HIVE-2275,12513307,Revert HIVE-2219 and apply correct patch to improve the efficiency of dropping multiple partitions,HIVE-2219 applied an incorrect patch that fails unit tests.  This patch reverts those changes and adds the intended changes to improve the efficiency of dropping multiple partitions.,sohanjain,sohanjain,Major,Closed,Fixed,08/Jul/11 20:20,16/Dec/11 23:56
Bug,HIVE-2276,12513313,Fix Inconsistency between RB and JIRA patches for HIVE-2194,The RB and JIRA patches for HIVE-2194 were out of sync.  An outdated patch for HIVE-2194 was committed.  This patch updates that patch to include the changes from RB.,sohanjain,sohanjain,Major,Closed,Fixed,08/Jul/11 23:06,02/May/13 02:29
Bug,HIVE-2281,12513840,Regression introduced from HIVE-2155,"EXPLAIN SELECT key, count(1) FROM src; throws a null pointer exception due to the operator stack not being checked prior to use for constructing the error message, due to the change introduced in HIVE-2155 to improve error message context tokens.",salbiz,salbiz,Major,Closed,Fixed,12/Jul/11 19:35,16/Dec/11 23:56
Bug,HIVE-2286,12514291,ClassCastException when building index with security.authorization turned on,"When trying to build an index with authorization checks turned on, hive issues the following ClassCastException:
org.apache.hadoop.hive.ql.parse.DDLSemanticAnalyzer cannot be cast to
org.apache.hadoop.hive.ql.parse.SemanticAnalyzer
         at
org.apache.hadoop.hive.ql.Driver.doAuthorization(Driver.java:540)
         at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:431)
         at org.apache.hadoop.hive.ql.Driver.compile(Driver.java:335)
         at org.apache.hadoop.hive.ql.Driver.run(Driver.java:848)
         at
org.apache.hadoop.hive.cli.CliDriver.processCmd(CliDriver.java:224)
         at
org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:358)
         at
org.apache.hadoop.hive.cli.CliDriver.processLine(CliDriver.java:293)
         at
org.apache.hadoop.hive.cli.CliDriver.processReader(CliDriver.java:385)
         at
org.apache.hadoop.hive.cli.CliDriver.processFile(CliDriver.java:392)
         at
org.apache.hadoop.hive.cli.CliDriver.main(CliDriver.java:567)
         at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
         at
sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.jav
a:39)
         at
sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessor
Impl.java:25)
         at java.lang.reflect.Method.invoke(Method.java:597)
         at org.apache.hadoop.util.RunJar.main(RunJar.java:156)",salbiz,salbiz,Major,Closed,Fixed,15/Jul/11 22:30,16/Dec/11 23:55
Bug,HIVE-2287,12514305,Error during UNARCHIVE of a partition,"When running UNARCHIVE on a partition which has many files in its directory we get:
cp: When copying multiple files, destination should be a directory.
FAILED: Error in metadata: org.apache.hadoop.hive.ql.metadata.HiveException: Error while copying files from archive
FAILED: Execution Error, return code 1 from org.apache.hadoop.hive.ql.exec.DDLTask
",marcink,marcink,Major,Closed,Fixed,16/Jul/11 01:14,16/Dec/11 23:56
Bug,HIVE-2292,12514719,Comment clause should immediately follow identifier field in CREATE DATABASE statement,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,19/Jul/11 22:28,16/Dec/11 23:56
Bug,HIVE-2296,12514846,bad compressed file names from insert into,"When INSERT INTO is run on a table with compressed output (hive.exec.compress.output=true) and existing files in the table, it may copy the new files in bad file names:

Before INSERT INTO:
000000_0.gz

After INSERT INTO:
000000_0.gz
000000_0.gz_copy_1

This causes corrupted output when doing a SELECT * on the table.
Correct behavior should be to pick a valid filename such as:
000000_0_copy_1.gz",franklinhu,franklinhu,Major,Closed,Fixed,20/Jul/11 23:13,16/Dec/11 23:56
Bug,HIVE-2298,12514955,Fix UDAFPercentile to tolerate null percentiles,"UDAFPercentile when passed null percentile list will throw a null pointer exception.
Submitting a small fix for that.",vaggarw,vaggarw,Major,Closed,Fixed,21/Jul/11 19:03,16/Dec/11 23:56
Bug,HIVE-2303,12515255,"files with control-A,B are not delimited correctly.","The following is from one of our users:
 
create external table impressions (imp string, msg string)
  row format delimited
    fields terminated by '\t'
    lines terminated by '\n'
  stored as textfile                 
  location '/xxx';
 
Some strings in my data contains Control-A, Control-B etc as internal delimiters.  If I do a
 
Select * from impressions limit 10;
 
All fields were able to print correctly.  However if I do a
 
Select * from impressions where msg regexp '.*' limit 10;
 
The fields were broken by the control characters.  The difference between the 2 commands is that the latter requires a map-reduce job.  
 
",amareshwari,amareshwari,Major,Closed,Fixed,25/Jul/11 13:04,16/Dec/11 23:56
Bug,HIVE-2307,12515357,Schema creation scripts for PostgreSQL use bit(1) instead of boolean,"The specified type for DEFERRED_REBUILD (IDXS) and IS_COMPRESSED (SDS) columns in the metastore is defined as bit(1) type which is not supported by PostgreSQL JDBC.

hive> create table test (id int); 
FAILED: Error in metadata: javax.jdo.JDODataStoreException: Insert of object ""org.apache.hadoop.hive.metastore.model.MStorageDescriptor@4f1adeb7"" using statement ""INSERT INTO ""SDS"" (""SD_ID"",""INPUT_FORMAT"",""OUTPUT_FORMAT"",""LOCATION"",""SERDE_ID"",""NUM_BUCKETS"",""IS_COMPRESSED"") VALUES (?,?,?,?,?,?,?)"" failed : ERROR: column ""IS_COMPRESSED"" is of type bit but expression is of type boolean 
",esteban,esteban,Major,Closed,Fixed,26/Jul/11 05:38,16/Dec/11 23:56
Bug,HIVE-2309,12515462,Incorrect regular expression for extracting task id from filename,"For producing the correct filenames for bucketed tables, there is a method in Utilities.java that extracts out the task id from the filename and replaces it with the bucket number. There is a bug in the regex that is used to extract this value for attempt numbers >= 10:

{code}
>>> re.match(""^.*?([0-9]+)(_[0​-9])?(\\..*)?$"", 'attempt_201107090429_6496​5_m_001210_10').group(1)
'10'
>>> re.match(""^.*?([0-9]+)(_[0​-9])?(\\..*)?$"", 'attempt_201107090429_6496​5_m_001210_9').group(1)
'001210'
{code}",pauly,pauly,Minor,Closed,Fixed,26/Jul/11 22:11,26/Jan/14 17:22
Bug,HIVE-2315,12515548,DatabaseMetadata.getColumns() does not return partition column names for a table,"getColumns() method of DatabaseMetadata for HIVE JDBC Driver does not return the partition column names. Where as from HIVE CLI, if you do a 'describe tablename' you get all columns including the partition columns. It would be nice if getColumns() method returns all columns.",phunt,mgk.424@gmail.com,Critical,Closed,Fixed,27/Jul/11 16:46,16/Dec/11 23:56
Bug,HIVE-2319,12515608,Calling alter_table after changing partition comment throws an exception,"Altering a table's partition key comments raises an InvalidOperationException.  The partition key name and type should not be mutable, but the comment should be able to get changed.",sohanjain,sohanjain,Major,Closed,Fixed,28/Jul/11 06:23,16/Dec/11 23:57
Bug,HIVE-2322,12515705,Add ColumnarSerDe to the list of native SerDes,"We store metadata about ColumnarSerDes in the metastore, so it should be considered a native SerDe.  Then, column information can be retrieved from the metastore instead of from deserialization.

Currently, for non-native SerDes, column comments are only shown as ""from deserializer"".  Adding ColumnarSerDe to the list of native SerDes will persist column comments.  See HIVE-2171 for persisting the column comments of custom SerDes.",sohanjain,sohanjain,Major,Closed,Fixed,28/Jul/11 21:01,16/Dec/11 23:56
Bug,HIVE-2326,12515857,Turn off bitmap indexing when map-side aggregation is turned off,"Simply adding the CLUSTER BY clause on the ROW_OFFSET does not work with a GROUP BY clause, causing a SemanticException when trying to compile the the index builder task. Based on conversation with John Sichi, for now we will just turn off this feature.",salbiz,salbiz,Major,Closed,Fixed,31/Jul/11 00:20,16/Dec/11 23:56
Bug,HIVE-2328,12515887,hive.zookeeper.session.timeout is set to null in hive-default.xml,,cwsteinbach,cwsteinbach,Minor,Closed,Fixed,01/Aug/11 05:17,16/Dec/11 23:56
Bug,HIVE-2329,12515895,"Not using map aggregation, fails to execute group-by after cluster-by with same key","hive.map.aggr=false
select Q1.key_int1, sum(Q1.key_int1), sum(distinct Q1.key_int1) from (select * from t1 cluster by key_int1) Q1 group by Q1.key_int1

resulted..

FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask

from hadoop logs..

Caused by: java.lang.RuntimeException: cannot find field key from []
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorUtils.getStandardStructFieldRef(ObjectInspectorUtils.java:321)
	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.getStructFieldRef(StandardStructObjectInspector.java:119)
	at org.apache.hadoop.hive.ql.exec.ExprNodeColumnEvaluator.initialize(ExprNodeColumnEvaluator.java:82)
	at org.apache.hadoop.hive.ql.exec.GroupByOperator.initializeOp(GroupByOperator.java:198)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:357)
	at org.apache.hadoop.hive.ql.exec.Operator.initialize(Operator.java:433)
........

I think the problem is caused by ReduceSinkDeDuplication, removing RS which was providing rs.key for GBY operation. If child of child RS is a GBY, we should bypass the optimization.",navis,navis,Minor,Closed,Fixed,01/Aug/11 08:48,30/Apr/12 21:11
Bug,HIVE-2331,12516853,Turn off compression when generating index intermediate results,"HiveIndexResult is not compression-aware, so for any index to work (regardless of compact/bitmap) we need to not compress the index intermediate file when we generate it.",salbiz,salbiz,Major,Closed,Fixed,02/Aug/11 00:39,16/Dec/11 23:56
Bug,HIVE-2332,12516864,"If all of the parameters of distinct functions are exists in group by columns, query fails in runtime","select sum(key_int1), sum(distinct key_int1) from t1 group by key_int1;

fails with message..
{code}
FAILED: Execution Error, return code 2 from org.apache.hadoop.hive.ql.exec.MapRedTask
{code}

hadoop says..
{code}
Caused by: java.lang.IndexOutOfBoundsException: Index: 1, Size: 1
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.get(ArrayList.java:322)
	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.init(StandardStructObjectInspector.java:95)
	at org.apache.hadoop.hive.serde2.objectinspector.StandardStructObjectInspector.(StandardStructObjectInspector.java:86)
	at org.apache.hadoop.hive.serde2.objectinspector.ObjectInspectorFactory.getStandardStructObjectInspector(ObjectInspectorFactory.java:252)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.initEvaluatorsAndReturnStruct(ReduceSinkOperator.java:188)
	at org.apache.hadoop.hive.ql.exec.ReduceSinkOperator.processOp(ReduceSinkOperator.java:197)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)
	at org.apache.hadoop.hive.ql.exec.TableScanOperator.processOp(TableScanOperator.java:85)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)
	at org.apache.hadoop.hive.ql.exec.MapOperator.process(MapOperator.java:532)
{code}

I think the deficient number of key expression, compared to number of key column, is the problem, which should be equal or more. 
Would it be solved if add some key expression? I'll try.",navis,navis,Blocker,Closed,Fixed,02/Aug/11 05:58,16/May/13 21:10
Bug,HIVE-2334,12517708,DESCRIBE TABLE causes NPE when hive.cli.print.header=true,,jghoman,cwsteinbach,Major,Closed,Fixed,02/Aug/11 21:57,16/Dec/11 23:57
Bug,HIVE-2335,12517718,Indexes are still automatically queried when out of sync with their source tables,"The automatic index usage does not check whether or not the indexes are still up-to-date when generating the index queries. This can be addressed in two stages, the first is to add a check before generating the index query to ensure that the index is still valid. The next stage may be to add some sort of mode where indexes are automatically updated on table writes.",salbiz,salbiz,Major,Closed,Fixed,02/Aug/11 23:13,16/Dec/11 23:56
Bug,HIVE-2337,12517731,Predicate pushdown erroneously conservative with outer joins,"The predicate pushdown filter is not applying left associativity of joins correctly in determining possible aliases for pushing predicates.

In hive.ql.ppd.OpProcFactory.JoinPPD.getQualifiedAliases, the criteria for pushing aliases is specified as:
{noformat}
    /**
     * Figures out the aliases for whom it is safe to push predicates based on
     * ANSI SQL semantics For inner join, all predicates for all aliases can be
     * pushed For full outer join, none of the predicates can be pushed as that
     * would limit the number of rows for join For left outer join, all the
     * predicates on the left side aliases can be pushed up For right outer
     * join, all the predicates on the right side aliases can be pushed up Joins
     * chain containing both left and right outer joins are treated as full
     * outer join. [...]
     *
     * @param op
     *          Join Operator
     * @param rr
     *          Row resolver
     * @return set of qualified aliases
     */
{noformat}

Since hive joins are left associative, something like ""a RIGHT OUTER JOIN b LEFT OUTER JOIN c INNER JOIN d"" should be interpreted as ""((a RIGHT OUTER JOIN b) LEFT OUTER JOIN c) INNER JOIN d"", so there would be cases where joins with both left and right outer joins can have aliases that can be pushed.  Here, aliases b and d are eligible to be pushed up while the current criteria provide that none are eligible.

Using:
{noformat}
create table t1 (id int, key string, value string);
create table t2 (id int, key string, value string);
create table t3 (id int, key string, value string);
create table t4 (id int, key string, value string);
{noformat}

For example, the query
{noformat}
explain select * from t1 full outer join t2 on t1.id=t2.id join t3 on t2.id=t3.id where t3.id=20; 
{noformat}
currently gives
{noformat}
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        t1 
          TableScan
            alias: t1
            Reduce Output Operator
              key expressions:
                    expr: id
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: id
                    type: int
              tag: 0
              value expressions:
                    expr: id
                    type: int
                    expr: key
                    type: string
                    expr: value
                    type: string
        t2 
          TableScan
            alias: t2
            Reduce Output Operator
              key expressions:
                    expr: id
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: id
                    type: int
              tag: 1
              value expressions:
                    expr: id
                    type: int
                    expr: key
                    type: string
                    expr: value
                    type: string
        t3 
          TableScan
            alias: t3
            Reduce Output Operator
              key expressions:
                    expr: id
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: id
                    type: int
              tag: 2
              value expressions:
                    expr: id
                    type: int
                    expr: key
                    type: string
                    expr: value
                    type: string
      Reduce Operator Tree:
        Join Operator
          condition map:
               Outer Join 0 to 1
               Inner Join 1 to 2
          condition expressions:
            0 {VALUE._col0} {VALUE._col1} {VALUE._col2}
            1 {VALUE._col0} {VALUE._col1} {VALUE._col2}
            2 {VALUE._col0} {VALUE._col1} {VALUE._col2}
          handleSkewJoin: false
          outputColumnNames: _col0, _col1, _col2, _col5, _col6, _col7, _col10, _col11, _col12
          Filter Operator
            predicate:
                expr: (_col10 = 20)
                type: boolean
            Select Operator
              expressions:
                    expr: _col0
                    type: int
                    expr: _col1
                    type: string
                    expr: _col2
                    type: string
                    expr: _col5
                    type: int
                    expr: _col6
                    type: string
                    expr: _col7
                    type: string
                    expr: _col10
                    type: int
                    expr: _col11
                    type: string
                    expr: _col12
                    type: string
              outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
              File Output Operator
                compressed: false
                GlobalTableId: 0
                table:
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1
{noformat}
while the correct behavior is to push the filter ""t3.id=20"" down:
{noformat}
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        t1 
          TableScan
            alias: t1
            Reduce Output Operator
              key expressions:
                    expr: id
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: id
                    type: int
              tag: 0
              value expressions:
                    expr: id
                    type: int
                    expr: key
                    type: string
                    expr: value
                    type: string
        t2 
          TableScan
            alias: t2
            Reduce Output Operator
              key expressions:
                    expr: id
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: id
                    type: int
              tag: 1
              value expressions:
                    expr: id
                    type: int
                    expr: key
                    type: string
                    expr: value
                    type: string
        t3 
          TableScan
            alias: t3
            Filter Operator
              predicate:
                  expr: (id = 20)
                  type: boolean
              Reduce Output Operator
                key expressions:
                      expr: id
                      type: int
                sort order: +
                Map-reduce partition columns:
                      expr: id
                      type: int
                tag: 2
                value expressions:
                      expr: id
                      type: int
                      expr: key
                      type: string
                      expr: value
                      type: string
      Reduce Operator Tree:
        Join Operator
          condition map:
               Outer Join 0 to 1
               Inner Join 1 to 2
          condition expressions:
            0 {VALUE._col0} {VALUE._col1} {VALUE._col2}
            1 {VALUE._col0} {VALUE._col1} {VALUE._col2}
            2 {VALUE._col0} {VALUE._col1} {VALUE._col2}
          handleSkewJoin: false
          outputColumnNames: _col0, _col1, _col2, _col5, _col6, _col7, _col10, _col11, _col12
          Select Operator
            expressions:
                  expr: _col0
                  type: int
                  expr: _col1
                  type: string
                  expr: _col2
                  type: string
                  expr: _col5
                  type: int
                  expr: _col6
                  type: string
                  expr: _col7
                  type: string
                  expr: _col10
                  type: int
                  expr: _col11
                  type: string
                  expr: _col12
                  type: string
            outputColumnNames: _col0, _col1, _col2, _col3, _col4, _col5, _col6, _col7, _col8
            File Output Operator
              compressed: false
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1
{noformat}

The current behavior is actually stranger than this: for a left outer join (similarly for a right outer join), hive finds the leftmost alias referred to in the *predicates* of left outer joins and rejects any alias to the right of it for pushdown.  So in this query the filter ""t2.id=20"" pushed down:
{noformat}
explain select * from t1 join t2 on (t1.id=t2.id) left outer join t3 on (t2.id=t3.id) where t2.id=20;
{noformat}
while it isn't here:
{noformat}
explain select * from t1 join t2 on (t1.id=t2.id) left outer join t3 on (t1.id=t3.id) where t2.id=20;
{noformat}",ccy,ccy,Major,Closed,Fixed,03/Aug/11 01:16,16/Dec/13 11:38
Bug,HIVE-2338,12517744,Alter table always throws an unhelpful error on failure,"Every failure in an alter table function always return a MetaException. When altering tables and catching exceptions, we throw a MetaException in the ""finally"" part of a try-catch-finally block, which overrides any other exceptions thrown.",sohanjain,sohanjain,Minor,Closed,Fixed,03/Aug/11 03:41,16/Dec/11 23:56
Bug,HIVE-2342,12517820,mirror.facebook.net is 404ing,"http://mirror.facebook.net/ and everything under it is 404ing, which is blocking any attempt to build Hive from working.",cwsteinbach,abayer,Major,Closed,Fixed,03/Aug/11 18:10,05/Apr/22 10:36
Bug,HIVE-2343,12517826,"stats not updated for non ""load table desc"" operations","Bug introduced in HIVE-306 so that stats are updated only for LoadTableDesc operations. For other operations (analyze table), null ptr is thrown and stats are not updated.",franklinhu,franklinhu,Major,Closed,Fixed,03/Aug/11 18:48,16/Dec/11 23:56
Bug,HIVE-2344,12517852,filter is removed due to regression of HIVE-1538," select * from 
 (
 select type_bucket,randum123
 from (SELECT *, cast(rand() as double) AS randum123 FROM tbl where ds = ...) a
 where randum123 <=0.1)s where s.randum123>0.1 limit 20;

This is returning results...

and 

 explain
 select type_bucket,randum123
 from (SELECT *, cast(rand() as double) AS randum123 FROM tbl where ds = ...) a
 where randum123 <=0.1

shows that there is no filter.",amareshwari,he yongqiang,Major,Closed,Fixed,03/Aug/11 21:44,09/Jan/13 10:23
Bug,HIVE-2356,12518228,Fix udtf_explode.q and udf_explode.q test failures,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,08/Aug/11 18:07,16/Dec/11 23:56
Bug,HIVE-2358,12518249,JDBC DatabaseMetaData and ResultSetMetaData need to match for particular types,"My patch for HIVE-1631 did not ensure the following (from comment on 1631):
-------------
Mythili Gopalakrishnan added a comment - 08/Aug/11 08:42

Just tested this fix and does NOT work correctly. Here are my findings on a FLOAT column

Without Patch on a FLOAT Column
--------------------------------
DatabaseMetaData.getColumns () COLUMN_SIZE returns 12
DatabaseMetaData.getColumns () DECIMAL_DIGITS - returns 0

ResultSetMetaData.getPrecision() returns 0
ResultSetMetaData.getScale() returns 0

With Patch on a FLOAT Column
----------------------------
DatabaseMetaData.getColumns () COLUMN_SIZE returns 24
DatabaseMetaData.getColumns () DECIMAL_DIGITS - returns 0

ResultSetMetaData.getPrecision() returns 7
ResultSetMetaData.getScale() returns 7

Also both DatabaseMetadata and ResultSetMetaData must return the same information for Precision and Scale for FLOAT,DOUBLE types.
",phunt,phunt,Major,Closed,Fixed,08/Aug/11 21:43,16/Dec/11 23:56
Bug,HIVE-2360,12518267,create dynamic partition if and only if intermediate source has files,There are some conditions under which a partition description is created due to insert overwriting a table using dynamic partitioning for partitions that that are empty (have no files).,franklinhu,franklinhu,Minor,Resolved,Fixed,09/Aug/11 01:00,17/Aug/15 18:38
Bug,HIVE-2362,12518279,HiveConf properties not appearing in the output of 'set' or 'set -v',,cwsteinbach,cwsteinbach,Blocker,Closed,Fixed,09/Aug/11 03:30,16/Dec/11 23:57
Bug,HIVE-2366,12518517,Metastore upgrade scripts for HIVE-2246 do not migrate indexes nor rename the old COLUMNS table,The upgrade scripts for the hive metastore in HIVE-2246 do not upgrade the indexes.  They also need to rename the old COLUMNS table after migration so that old clients will not accidentally access the COLUMNS table.,sohanjain,sohanjain,Major,Closed,Fixed,10/Aug/11 21:42,16/Dec/11 23:57
Bug,HIVE-2368,12518535,Slow dropping of partitions caused by full listing of storage descriptors,"To determine if a column descriptor is unused, we call listStorageDescriptorsWithCD(), which may return a big list of SDs.  This can severely slow down dropping partitions.

We can add a maximum number of SDs to return, and just ask for 1 SD, since we are just doing an existential check.",sohanjain,sohanjain,Major,Closed,Fixed,10/Aug/11 23:28,16/Dec/11 23:57
Bug,HIVE-2369,12518613,Minor typo in error message in HiveConnection.java (JDBC),"There is a minor typo issue in HiveConnection.java (jdbc) :

{code}throw new SQLException(""Could not establish connecton to ""
            + uri + "": "" + e.getMessage(), ""08S01"");{code}

It seems like there's a ""i"" missing.

I know it's a very minor typo but I report it anyway. I won't attach a patch because it would be too long for me to SVN checkout just for 1 letter.",cnotin,cnotin,Trivial,Closed,Fixed,11/Aug/11 17:42,23/May/17 11:01
Bug,HIVE-2372,12518679,"java.io.IOException: error=7, Argument list too long","I execute a huge query on a table with a lot of 2-level partitions. There is a perl reducer in my query. Maps worked ok, but every reducer fails with the following exception:

2011-08-11 04:58:29,865 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: Executing [/usr/bin/perl, <reducer.pl>, <my_argument>]
2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: tablename=null
2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: partname=null
2011-08-11 04:58:29,866 INFO org.apache.hadoop.hive.ql.exec.ScriptOperator: alias=null
2011-08-11 04:58:29,935 FATAL ExecReducer: org.apache.hadoop.hive.ql.metadata.HiveException: Hive Runtime Error while processing row (tag=0) {""key"":{""reducesinkkey0"":129390185139228,""reducesinkkey1"":""00008AF10000000063CA6F""},""value"":{""_col0"":""00008AF10000000063CA6F"",""_col1"":""2011-07-27 22:48:52"",""_col2"":129390185139228,""_col3"":2006,""_col4"":4100,""_col5"":""10017388=6"",""_col6"":1063,""_col7"":""NULL"",""_col8"":""address.com"",""_col9"":""NULL"",""_col10"":""NULL""},""alias"":0}
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:256)
	at org.apache.hadoop.mapred.ReduceTask.runOldReducer(ReduceTask.java:468)
	at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:416)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:268)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1115)
	at org.apache.hadoop.mapred.Child.main(Child.java:262)
Caused by: org.apache.hadoop.hive.ql.metadata.HiveException: Cannot initialize ScriptOperator
	at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:320)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)
	at org.apache.hadoop.hive.ql.exec.SelectOperator.processOp(SelectOperator.java:84)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.Operator.forward(Operator.java:744)
	at org.apache.hadoop.hive.ql.exec.ExtractOperator.processOp(ExtractOperator.java:45)
	at org.apache.hadoop.hive.ql.exec.Operator.process(Operator.java:471)
	at org.apache.hadoop.hive.ql.exec.ExecReducer.reduce(ExecReducer.java:247)
	... 7 more
Caused by: java.io.IOException: Cannot run program ""/usr/bin/perl"": java.io.IOException: error=7, Argument list too long
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:460)
	at org.apache.hadoop.hive.ql.exec.ScriptOperator.processOp(ScriptOperator.java:279)
	... 15 more
Caused by: java.io.IOException: java.io.IOException: error=7, Argument list too long
	at java.lang.UNIXProcess.<init>(UNIXProcess.java:148)
	at java.lang.ProcessImpl.start(ProcessImpl.java:65)
	at java.lang.ProcessBuilder.start(ProcessBuilder.java:453)
	... 16 more

It seems to me, I found the cause. ScriptOperator.java puts a lot of configs as environment variables to the child reduce process. One of variables is mapred.input.dir, which in my case more than 150KB. There are a huge amount of input directories in this variable. In short, the problem is that Linux (up to 2.6.23 kernel version) limits summary size of environment variables for child processes to 132KB. This problem could be solved by upgrading the kernel. But strings limitations still be 132KB per string in environment variable. So such huge variable doesn't work even on my home computer (2.6.32). You can read more information on (http://www.kernel.org/doc/man-pages/online/pages/man2/execve.2.html).

For now all our work has been stopped because of this problem and I can't find the solution. The only solution, which seems to me more reasonable is to get rid of this variable in reducers.


",,sergeant,Critical,Closed,Fixed,12/Aug/11 09:07,11/Jun/14 19:53
Bug,HIVE-2379,12518939,Hive/HBase integration could be improved,"  For now any Hive/HBase queries would require the following jars to be explicitly added via hive's add jar command:

add jar /usr/lib/hive/lib/hbase-0.90.1-cdh3u0.jar;
add jar /usr/lib/hive/lib/hive-hbase-handler-0.7.0-cdh3u0.jar;
add jar /usr/lib/hive/lib/zookeeper-3.3.1.jar;
add jar /usr/lib/hive/lib/guava-r06.jar;

the longer term solution, perhaps, should be to have the code at submit time call hbase's 
TableMapREduceUtil.addDependencyJar(job, HBaseStorageHandler.class) to ship it in distributedcache.",navis,rvs,Critical,Closed,Fixed,15/Aug/11 23:45,26/Jun/14 06:16
Bug,HIVE-2382,12519059,Invalid predicate pushdown from incorrect column expression map for select operator generated by GROUP BY operation,"When a GROUP BY is specified, a select operator is added before the GROUP BY in SemanticAnalyzer.insertSelectAllPlanForGroupBy.  Currently, the column expression map for this is set to the column expression map for the parent operator.  This behavior is incorrect as, for example, the parent operator could rearrange the order of the columns (_col0 => _col0, _col1 => _col2, _col2 => _col1) and the new operator should not repeat this.

The predicate pushdown optimization uses the column expression map to track which columns a filter expression refers to at different operators.  This results in a filter on incorrect columns.

Here is a simple case of this going wrong: Using
{noformat}
create table invites (id int, foo int, bar int);
{noformat}
executing the query
{noformat}
explain select * from (select foo, bar from (select bar, foo from invites c union all select bar, foo from invites d) b) a group by bar, foo having bar=1;
{noformat}
results in
{noformat}
STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        a-subquery1:b-subquery1:c 
          TableScan
            alias: c
            Filter Operator
              predicate:
                  expr: (foo = 1)
                  type: boolean
              Select Operator
                expressions:
                      expr: bar
                      type: int
                      expr: foo
                      type: int
                outputColumnNames: _col0, _col1
                Union
                  Select Operator
                    expressions:
                          expr: _col1
                          type: int
                          expr: _col0
                          type: int
                    outputColumnNames: _col0, _col1
                    Select Operator
                      expressions:
                            expr: _col0
                            type: int
                            expr: _col1
                            type: int
                      outputColumnNames: _col0, _col1
                      Group By Operator
                        bucketGroup: false
                        keys:
                              expr: _col1
                              type: int
                              expr: _col0
                              type: int
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Reduce Output Operator
                          key expressions:
                                expr: _col0
                                type: int
                                expr: _col1
                                type: int
                          sort order: ++
                          Map-reduce partition columns:
                                expr: _col0
                                type: int
                                expr: _col1
                                type: int
                          tag: -1
        a-subquery2:b-subquery2:d 
          TableScan
            alias: d
            Filter Operator
              predicate:
                  expr: (foo = 1)
                  type: boolean
              Select Operator
                expressions:
                      expr: bar
                      type: int
                      expr: foo
                      type: int
                outputColumnNames: _col0, _col1
                Union
                  Select Operator
                    expressions:
                          expr: _col1
                          type: int
                          expr: _col0
                          type: int
                    outputColumnNames: _col0, _col1
                    Select Operator
                      expressions:
                            expr: _col0
                            type: int
                            expr: _col1
                            type: int
                      outputColumnNames: _col0, _col1
                      Group By Operator
                        bucketGroup: false
                        keys:
                              expr: _col1
                              type: int
                              expr: _col0
                              type: int
                        mode: hash
                        outputColumnNames: _col0, _col1
                        Reduce Output Operator
                          key expressions:
                                expr: _col0
                                type: int
                                expr: _col1
                                type: int
                          sort order: ++
                          Map-reduce partition columns:
                                expr: _col0
                                type: int
                                expr: _col1
                                type: int
                          tag: -1
      Reduce Operator Tree:
        Group By Operator
          bucketGroup: false
          keys:
                expr: KEY._col0
                type: int
                expr: KEY._col1
                type: int
          mode: mergepartial
          outputColumnNames: _col0, _col1
          Select Operator
            expressions:
                  expr: _col0
                  type: int
                  expr: _col1
                  type: int
            outputColumnNames: _col0, _col1
            File Output Operator
              compressed: false
              GlobalTableId: 0
              table:
                  input format: org.apache.hadoop.mapred.TextInputFormat
                  output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1
{noformat}
Note that the filter is now ""foo = 1"", while the correct behavior is to have ""bar = 1"".  If we remove the group by, the behavior is correct.",ccy,ccy,Critical,Closed,Fixed,17/Aug/11 00:55,16/Dec/11 23:55
Bug,HIVE-2383,12519060,Incorrect alias filtering for predicate pushdown,"The predicate pushdown optimizer starts at the topmost operators traverses the operator tree, at each stage collecting predicates to be pushed down.  At each operator, ive.ql.ppd.OpProcFactory.DefaultPPD.mergeWithChildrenPred is called, which merges the predicates of the children nodes into the current node.  The predicates are stored in hive.ql.ppd.ExprWalkerInfo.pushdownPreds as a map from the alias a predicate refers to (a predicate may only refer to one alias at a time as only such predicates can be pushed) to a list of such predicates.  Since at each stage the alias the predicate refers to may change (subqueries may change aliases), this is updated for each operator (hive.ql.ppd.ExprWalkerProcFactory.extractPushdownPreds is called which walks the ExprNodeDesc for each predicate). When a JoinOperator is encountered, mergeWithChildrenPred is passed an optional parameter ""aliases"" which contains a set of aliases that can be pushed per ansi semantics (see hive.ql.ppd.OpProcFactory.JoinPPD.getQualifiedAliases).  The part that is incorrect is that aliases are filtered in mergeWithChildrenPred before extractPushdownPreds is called, which associates the predicates with the correct alias in the current operator's context while the filtering should happen after.

In test case Q2 below, when the predicate ""a.bar=3"" comes into the JoinOperator, the alias is ""a"" coming in so it is accepted for pushdown.  When brought into the JoinOperator's context, however, since the predicate refers to b.foo in the inner scope, we should not actually accept this for pushdown.

With the test cases
{noformat}
-- Q1: predicate should not be pushed on the right side of a left outer join (this is correct in trunk)
explain
SELECT a.foo as foo1, b.foo as foo2, b.bar
FROM pokes a LEFT OUTER JOIN pokes2 b
ON a.foo=b.foo
WHERE b.bar=3;

-- Q2: predicate should not be pushed on the right side of a left outer join (this is broken in trunk)
explain
SELECT * FROM
    (SELECT a.foo as foo1, b.foo as foo2, b.bar
    FROM pokes a LEFT OUTER JOIN pokes2 b
    ON a.foo=b.foo) a
WHERE a.bar=3;

-- Q3: predicate should be pushed (this is correct in trunk)
explain
SELECT * FROM
    (SELECT a.foo as foo1, b.foo as foo2, a.bar
    FROM pokes a JOIN pokes2 b
    ON a.foo=b.foo) a
WHERE a.bar=3;
{noformat}
The current output is
{noformat}
hive> 
    > -- Q1: predicate should not be pushed on the right side of a left outer join
    > explain
    > SELECT a.foo as foo1, b.foo as foo2, b.bar
    > FROM pokes a LEFT OUTER JOIN pokes2 b
    > ON a.foo=b.foo
    > WHERE b.bar=3;
OK
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_LEFTOUTERJOIN (TOK_TABREF (TOK_TABNAME pokes) a) (TOK_TABREF (TOK_TABNAME pokes2) b) (= (. (TOK_TABLE_OR_COL a) foo) (. (TOK_TABLE_OR_COL b) foo)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) foo) foo1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) foo) foo2) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) bar))) (TOK_WHERE (= (. (TOK_TABLE_OR_COL b) bar) 3))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        a 
          TableScan
            alias: a
            Reduce Output Operator
              key expressions:
                    expr: foo
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: foo
                    type: int
              tag: 0
              value expressions:
                    expr: foo
                    type: int
        b 
          TableScan
            alias: b
            Reduce Output Operator
              key expressions:
                    expr: foo
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: foo
                    type: int
              tag: 1
              value expressions:
                    expr: foo
                    type: int
                    expr: bar
                    type: int
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join0 to 1
          condition expressions:
            0 {VALUE._col0}
            1 {VALUE._col0} {VALUE._col1}
          handleSkewJoin: false
          outputColumnNames: _col0, _col4, _col5
          Filter Operator
            predicate:
                expr: (_col5 = 3)
                type: boolean
            Select Operator
              expressions:
                    expr: _col0
                    type: int
                    expr: _col4
                    type: int
                    expr: _col5
                    type: int
              outputColumnNames: _col0, _col1, _col2
              File Output Operator
                compressed: false
                GlobalTableId: 0
                table:
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1


Time taken: 0.113 seconds
hive> 
    > -- Q2: predicate should not be pushed on the right side of a left outer join
    > explain
    > SELECT * FROM
    >     (SELECT a.foo as foo1, b.foo as foo2, b.bar
    >     FROM pokes a LEFT OUTER JOIN pokes2 b
    >     ON a.foo=b.foo) a
    > WHERE a.bar=3;
OK
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_LEFTOUTERJOIN (TOK_TABREF (TOK_TABNAME pokes) a) (TOK_TABREF (TOK_TABNAME pokes2) b) (= (. (TOK_TABLE_OR_COL a) foo) (. (TOK_TABLE_OR_COL b) foo)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) foo) foo1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) foo) foo2) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) bar))))) a)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (. (TOK_TABLE_OR_COL a) bar) 3))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        a:a 
          TableScan
            alias: a
            Reduce Output Operator
              key expressions:
                    expr: foo
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: foo
                    type: int
              tag: 0
              value expressions:
                    expr: foo
                    type: int
        a:b 
          TableScan
            alias: b
            Filter Operator
              predicate:
                  expr: (bar = 3)
                  type: boolean
              Reduce Output Operator
                key expressions:
                      expr: foo
                      type: int
                sort order: +
                Map-reduce partition columns:
                      expr: foo
                      type: int
                tag: 1
                value expressions:
                      expr: foo
                      type: int
                      expr: bar
                      type: int
      Reduce Operator Tree:
        Join Operator
          condition map:
               Left Outer Join0 to 1
          condition expressions:
            0 {VALUE._col0}
            1 {VALUE._col0} {VALUE._col1}
          handleSkewJoin: false
          outputColumnNames: _col0, _col4, _col5
          Select Operator
            expressions:
                  expr: _col0
                  type: int
                  expr: _col4
                  type: int
                  expr: _col5
                  type: int
            outputColumnNames: _col0, _col1, _col2
            Select Operator
              expressions:
                    expr: _col0
                    type: int
                    expr: _col1
                    type: int
                    expr: _col2
                    type: int
              outputColumnNames: _col0, _col1, _col2
              File Output Operator
                compressed: false
                GlobalTableId: 0
                table:
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1


Time taken: 0.101 seconds
hive> 
    > -- Q3: predicate should be pushed
    > explain
    > SELECT * FROM
    >     (SELECT a.foo as foo1, b.foo as foo2, a.bar
    >     FROM pokes a JOIN pokes2 b
    >     ON a.foo=b.foo) a
    > WHERE a.bar=3;
OK
ABSTRACT SYNTAX TREE:
  (TOK_QUERY (TOK_FROM (TOK_SUBQUERY (TOK_QUERY (TOK_FROM (TOK_JOIN (TOK_TABREF (TOK_TABNAME pokes) a) (TOK_TABREF (TOK_TABNAME pokes2) b) (= (. (TOK_TABLE_OR_COL a) foo) (. (TOK_TABLE_OR_COL b) foo)))) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) foo) foo1) (TOK_SELEXPR (. (TOK_TABLE_OR_COL b) foo) foo2) (TOK_SELEXPR (. (TOK_TABLE_OR_COL a) bar))))) a)) (TOK_INSERT (TOK_DESTINATION (TOK_DIR TOK_TMP_FILE)) (TOK_SELECT (TOK_SELEXPR TOK_ALLCOLREF)) (TOK_WHERE (= (. (TOK_TABLE_OR_COL a) bar) 3))))

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        a:a 
          TableScan
            alias: a
            Filter Operator
              predicate:
                  expr: (bar = 3)
                  type: boolean
              Reduce Output Operator
                key expressions:
                      expr: foo
                      type: int
                sort order: +
                Map-reduce partition columns:
                      expr: foo
                      type: int
                tag: 0
                value expressions:
                      expr: foo
                      type: int
                      expr: bar
                      type: int
        a:b 
          TableScan
            alias: b
            Reduce Output Operator
              key expressions:
                    expr: foo
                    type: int
              sort order: +
              Map-reduce partition columns:
                    expr: foo
                    type: int
              tag: 1
              value expressions:
                    expr: foo
                    type: int
      Reduce Operator Tree:
        Join Operator
          condition map:
               Inner Join 0 to 1
          condition expressions:
            0 {VALUE._col0} {VALUE._col1}
            1 {VALUE._col0}
          handleSkewJoin: false
          outputColumnNames: _col0, _col1, _col4
          Select Operator
            expressions:
                  expr: _col0
                  type: int
                  expr: _col4
                  type: int
                  expr: _col1
                  type: int
            outputColumnNames: _col0, _col1, _col2
            Select Operator
              expressions:
                    expr: _col0
                    type: int
                    expr: _col1
                    type: int
                    expr: _col2
                    type: int
              outputColumnNames: _col0, _col1, _col2
              File Output Operator
                compressed: false
                GlobalTableId: 0
                table:
                    input format: org.apache.hadoop.mapred.TextInputFormat
                    output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat

  Stage: Stage-0
    Fetch Operator
      limit: -1
{noformat}
Note that Q2 is incorrect because the predicate ""bar = 3"" is incorrectly pushed to the right side of the left outer join (Q1 and Q3 are correct).",ccy,ccy,Critical,Closed,Fixed,17/Aug/11 00:55,16/Dec/11 23:56
Bug,HIVE-2384,12519090,import of multiple partitions from a partitioned table with external location overwrites files,"when we import multiple partitions from an exported partitioned table, if we import it with a specified external location, then the partitions end up overwriting.",n_krishna_kumar,n_krishna_kumar,Major,Closed,Fixed,17/Aug/11 08:46,16/Dec/11 23:56
Bug,HIVE-2386,12519185,Add Mockito to LICENSE file,"Mockito was added in HIVE-2171, but not added to the license file.  ",jghoman,jghoman,Major,Closed,Fixed,17/Aug/11 21:13,16/Dec/11 23:56
Bug,HIVE-2390,12519221,Add UNIONTYPE serialization support to LazyBinarySerDe,"When the union type was introduced, full support for it wasn't provided.  For instance, when working with a union that gets passed to LazyBinarySerde: 
{noformat}Caused by: java.lang.RuntimeException: Unrecognized type: UNION
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.serialize(LazyBinarySerDe.java:468)
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.serializeStruct(LazyBinarySerDe.java:230)
	at org.apache.hadoop.hive.serde2.lazybinary.LazyBinarySerDe.serialize(LazyBinarySerDe.java:184)
{noformat}",suma.shivaprasad,jghoman,Major,Closed,Fixed,18/Aug/11 00:50,13/Nov/14 19:39
Bug,HIVE-2391,12519222,published POMs in Maven repo are incorrect,"The Hive artifacts published in Apache Maven SNAPSHOTS repo are incorrect. 

Dependencies are not complete.

Even after adding as dependencies ALL the Hive artifacts it is not possible to compile a project using Hive JARs (I'm trying to integrate Oozie Hive Action using Apache Hive).

As a reference the Hive CDH POMs dependencies could be used (Using those artifacts I'm able to compile/test/run Hive from within Oozie).

",cwsteinbach,tucu00,Critical,Closed,Fixed,18/Aug/11 01:08,02/May/13 02:29
Bug,HIVE-2393,12519344,Fix whitespace test diff accidentally introduced in HIVE-1360,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,18/Aug/11 21:27,16/Dec/11 23:56
Bug,HIVE-2398,12519477,Hive server doesn't return schema for 'set' command,"The Hive server does process the CLI commands like 'set', 'set -v' sent by ODBC or JDBC clients. But currently only the data is returned to client but not schema for that resultset. This makes it unusable for a ODBC or JDBC client to use this option.
",prasadm,prasadm,Major,Closed,Fixed,19/Aug/11 22:18,16/Dec/11 23:55
Bug,HIVE-2402,12519966,Function like with empty string is throwing null pointer exception,"select emp.ename from emp where ename like ''
This query is throwing null pointer exception",chinnalalam,chinnalalam,Major,Closed,Fixed,24/Aug/11 13:55,16/Dec/11 23:56
Bug,HIVE-2405,12520011,get_privilege does not get user level privilege,"hive> set hive.security.authorization.enabled=true;
hive>  grant all to user heyongqiang;              
hive> show grant user heyongqiang;                 
principalName	heyongqiang	
principalType	USER	
privilege	All	
grantTime	Wed Aug 24 11:51:54 PDT 2011	
grantor	heyongqiang	
Time taken: 0.032 seconds
hive>  CREATE TABLE src (foo INT, bar STRING);     
Authorization failed:No privilege 'Create' found for outputs { database:default}. Use show grant to get more details.
",he yongqiang,he yongqiang,Major,Closed,Fixed,24/Aug/11 19:36,11/Apr/12 21:48
Bug,HIVE-2407,12520113,File extensions not preserved in Hive.checkPaths when renaming new destination file,"In the checkPaths method of Hive.java, a new destination filename will be chosen if the source filename already exists in the destination directory.  This new filename follows the simple schema of adding _copy_N to the source filename (with increasing N until a non-existing filename is found).

If the file has been LZO compressed it is imperative that the extension remain "".lzo"" so that the LZO indexer can find it and create a corresponding index file.  It would be much better to use a prefix of ""copy_N_"" or insert ""_copy_N"" somewhere else in the filename.

Without this, Hive and LZO compression will not work when identical source filenames are inserted into Hive.",,brian.muller,Major,Closed,Fixed,25/Aug/11 13:45,16/Dec/11 23:56
Bug,HIVE-2411,12520189,Metastore server tries to connect to NN without authenticating itself,When metastore server is launching it first calls new HMSHandler() even before it has done a login. That results in createDefaultDB() gets called which then results in getFileSystem() calls which tries to create a connection to NN and then it fails since NN cannot authenticate metastore.,ashutoshc,ashutoshc,Major,Closed,Fixed,26/Aug/11 00:23,16/Dec/11 23:55
Bug,HIVE-2412,12520191,Update Eclipse configuration to include Mockito dependency,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,26/Aug/11 00:55,16/Dec/11 23:56
Bug,HIVE-2413,12520212,BlockMergeTask ignores client-specified jars,User-specified jars are not added to the hadoop tasks while executing a BlockMergeTask resulting in a ClassNotFoundException.,n_krishna_kumar,n_krishna_kumar,Minor,Closed,Fixed,26/Aug/11 05:33,16/Dec/11 23:55
Bug,HIVE-2417,12520545,Merging of compressed rcfiles fails to write the valuebuffer part correctly,The blockmerge task does not create proper rc files when merging compressed rc files as the valuebuffer writing is incorrect.,n_krishna_kumar,n_krishna_kumar,Major,Closed,Fixed,29/Aug/11 10:21,16/Dec/11 23:55
Bug,HIVE-2429,12521626,skip corruption bug that cause data not decompressed,This is a regression of https://issues.apache.org/jira/browse/HIVE-2404,rvadali,he yongqiang,Major,Closed,Fixed,07/Sep/11 17:19,16/Dec/11 23:57
Bug,HIVE-2431,12521835,upgrading thrift version didn't upgrade libthrift.jar symlink correctly,"libthrift.jar and libfb303.jar are symlinks to the current thrift version. With the upgrade to 0.7, there's a bug in the symlink creation. ",nzhang,nzhang,Major,Closed,Fixed,08/Sep/11 00:21,16/Dec/11 23:55
Bug,HIVE-2451,12523273,TABLESAMBLE(BUCKET xxx) sometimes doesn't trigger input pruning as regression of HIVE-1538,"Example:

select count(1) from <bucket_table> TABLESAMPLE(BUCKET xxx out of yyy) where <partition_column> = 'xxx'

will not trigger input pruning.

The reason is that we assume sample filtering operator only happens as the second filter after table scan, which is broken by HIVE-1538, even if the feature doesn't turn on.",sdong,sdong,Major,Closed,Fixed,15/Sep/11 23:49,16/Dec/11 23:56
Bug,HIVE-2455,12523474,Pass correct remoteAddress in proxy user authentication,,ashutoshc,ashutoshc,Major,Closed,Fixed,18/Sep/11 07:49,16/Dec/11 23:56
Bug,HIVE-2459,12523819,remove all @author tags from source,"$  grep --exclude-dir=build --exclude-dir=.svn -r ""@author "" .
./ql/src/java/org/apache/hadoop/hive/ql/parse/ASTNode.java: * @author athusoo
./ql/src/java/org/apache/hadoop/hive/ql/index/IndexSearchCondition.java: * @author John Sichi
",ashutoshc,ashutoshc,Major,Closed,Fixed,21/Sep/11 06:32,16/Dec/11 23:55
Bug,HIVE-2463,12524358,fix Eclipse for javaewah upgrade,I always forget this.,jvs,jvs,Major,Closed,Fixed,23/Sep/11 00:47,16/Dec/11 23:56
Bug,HIVE-2465,12524387,Primitive Data Types returning null if the data is out of range of the data type.,Primitive Data Types returning null if the input data is out of range of the data type. In this case it is better to log the message with the proper message and actual data then user get to know some data is missing.,chinnalalam,chinnalalam,Major,Closed,Fixed,23/Sep/11 08:28,16/Dec/11 23:56
Bug,HIVE-2466,12524434,mapjoin_subquery  dump small table (mapjoin table) to the same file,"in mapjoin_subquery.q  there is a query：
SELECT /*+ MAPJOIN(z) */ subq.key1, z.value
FROM
(SELECT /*+ MAPJOIN(x) */ x.key as key1, x.value as value1, y.key as key2, y.value as value2 
 FROM src1 x JOIN src y ON (x.key = y.key)) subq
 JOIN srcpart z ON (subq.key1 = z.key and z.ds='2008-04-08' and z.hr=11);
when dump x and z to a local file,there all dump to the same file, so we lost the data of x
",binlijin,binlijin,Critical,Closed,Fixed,23/Sep/11 16:39,16/Dec/11 23:56
Bug,HIVE-2472,12525237,Metastore statistics are not being updated for CTAS queries.,We need to add a Statistics task at the end of a CTAS query in order to update the metastore statistics for the table being created.,rsurowka,kevinwilfong,Major,Closed,Fixed,29/Sep/11 20:24,02/Feb/12 22:09
Bug,HIVE-2473,12525383,Hive throws an NPE when $HADOOP_HOME points to a tarball install directory that contains a build/ subdirectory.,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,01/Oct/11 00:20,15/Oct/13 23:29
Bug,HIVE-2474,12525469,Hive PDK needs an Ivy configuration file,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,02/Oct/11 07:17,16/Dec/11 23:55
Bug,HIVE-2481,12525713,HadoopJobExecHelper does not handle null counters well,Sometimes hadoop can return null counters for a mapreduce job if the job has been retired. The HadoopJobExecHelper needs to handle it.,rvadali,rvadali,Major,Closed,Fixed,04/Oct/11 18:32,16/Dec/11 23:56
Bug,HIVE-2486,12525968,Phabricator for code review,Replacing Review Board with Phabricator and Differential.,mareksapota_fb,mareksapota_fb,Major,Closed,Fixed,05/Oct/11 20:54,23/Dec/12 20:52
Bug,HIVE-2487,12525977,"Bug from HIVE-2446, the code that calls client stats publishers run() methods is in wrong place, should be in the same method but inside of while (!rj.isComplete()) {} loop",,rsurowka,rsurowka,Trivial,Closed,Fixed,05/Oct/11 21:40,21/Mar/14 06:04
Bug,HIVE-2488,12525991,PDK tests failing on Hudson because HADOOP_HOME is not defined,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,06/Oct/11 00:07,16/Dec/11 23:56
Bug,HIVE-2492,12526267,PDK PluginTest failing on Hudson,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,07/Oct/11 20:15,16/Dec/11 23:55
Bug,HIVE-2497,12526789,partition pruning  prune some right partition under specific conditions,"create table src3(key string, value string) partitioned by (pt string)
row format delimited fields terminated by ',';

ALTER TABLE src3 ADD IF NOT EXISTS PARTITION (pt='20110911000000') ;
ALTER TABLE src3 ADD IF NOT EXISTS PARTITION (pt='20110912000000') ;
ALTER TABLE src3 ADD IF NOT EXISTS PARTITION (pt='20110913000000') ;



explain extended
select user_id 
from
 (
   select 
    cast(key as int) as user_id
    ,case when (value like 'aaa%' or value like 'vvv%')
            then 1
            else 0  end as tag_student
   from src3
 ) sub
where sub.tag_student > 0;


STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        sub:src3 
          TableScan
            alias: src3
            Filter Operator
              isSamplingPred: false
              predicate:
                  expr: (CASE WHEN (((value like 'aaa%') or (value like 'vvv%'))) THEN (1) ELSE (0) END > 0)
                  type: boolean
              Select Operator
                expressions:
                      expr: UDFToInteger(key)
                      type: int
                      expr: CASE WHEN (((value like 'aaa%') or (value like 'vvv%'))) THEN (1) ELSE (0) END
                      type: int
                outputColumnNames: _col0, _col1
                Filter Operator
                  isSamplingPred: false
                  predicate:
                      expr: (_col1 > 0)
                      type: boolean
                  Select Operator
                    expressions:
                          expr: _col0
                          type: int
                    outputColumnNames: _col0
                    File Output Operator
                      compressed: false
                      GlobalTableId: 0
                      directory: hdfs://localhost:54310/tmp/hive-tianzhao/hive_2011-10-11_19-26-12_894_9085644225727185586/-ext-10001
                      NumFilesPerFileSink: 1
                      table:
                          input format: org.apache.hadoop.mapred.TextInputFormat
                          output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                          properties:
                            columns _col0
                            columns.types int
                            serialization.format 1
                      TotalFiles: 1
                      MultiFileSpray: false
      Needs Tagging: false

  Stage: Stage-0
    Fetch Operator
      limit: -1


if we set hive.optimize.ppd=false;

STAGE DEPENDENCIES:
  Stage-1 is a root stage
  Stage-0 is a root stage

STAGE PLANS:
  Stage: Stage-1
    Map Reduce
      Alias -> Map Operator Tree:
        sub:src3 
          TableScan
            alias: src3
            Select Operator
              expressions:
                    expr: UDFToInteger(key)
                    type: int
                    expr: CASE WHEN (((value like 'aaa%') or (value like 'vvv%'))) THEN (1) ELSE (0) END
                    type: int
              outputColumnNames: _col0, _col1
              Filter Operator
                isSamplingPred: false
                predicate:
                    expr: (_col1 > 0)
                    type: boolean
                Select Operator
                  expressions:
                        expr: _col0
                        type: int
                  outputColumnNames: _col0
                  File Output Operator
                    compressed: false
                    GlobalTableId: 0
                    directory: hdfs://localhost:54310/tmp/hive-tianzhao/hive_2011-10-11_19-27-22_527_1729287213481398480/-ext-10001
                    NumFilesPerFileSink: 1
                    table:
                        input format: org.apache.hadoop.mapred.TextInputFormat
                        output format: org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat
                        properties:
                          columns _col0
                          columns.types int
                          serialization.format 1
                    TotalFiles: 1
                    MultiFileSpray: false
      Needs Tagging: false
      Path -> Alias:
        hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110911000000 [sub:src3]
        hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110912000000 [sub:src3]
        hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110913000000 [sub:src3]
      Path -> Partition:
        hdfs://localhost:54310/user/hive/warehouse/src3/pt=20110911000000 
          Partition
            base file name: pt=20110911000000




",binlijin,binlijin,Major,Closed,Fixed,12/Oct/11 02:27,16/Dec/11 23:55
Bug,HIVE-2498,12526796,Group by operator does not estimate size of Timestamp & Binary data correctly,"It currently defaults to default case and returns constant value, whereas we can do better by getting actual size at runtime.",ashutoshc,ashutoshc,Major,Closed,Fixed,12/Oct/11 05:20,10/Jan/13 19:53
Bug,HIVE-2499,12526810,small table filesize for automapjoin is not consistent in HiveConf.java and hive-default.xml,,binlijin,binlijin,Minor,Closed,Fixed,12/Oct/11 08:24,16/Dec/11 23:56
Bug,HIVE-2501,12526885,"When new instance of Hive (class) is created, the current database is reset to default (current database shouldn't be changed).","This bug manifested to me, when first thing I did after starting Hive, was to call use <db_name>; but then calling show tables; was still showing tables from the default database (and I had to call ""use"" again to actually change database). This bug might have manifested only due to a specific Hive deployment I am using (I didn't investigate this issue that deeply). ",rsurowka,rsurowka,Minor,Closed,Fixed,12/Oct/11 18:01,16/Dec/11 23:56
Bug,HIVE-2503,12526959,HiveServer should provide per session configuration,"Currently ThriftHiveProcessorFactory returns same HiveConf instance to HiveServerHandler, making impossible to use per sesssion configuration. Just wrapping 'conf' -> 'new HiveConf(conf)' seemed to solve this problem.",navis,navis,Major,Closed,Fixed,13/Oct/11 02:01,09/Jan/13 10:24
Bug,HIVE-2504,12527138,Warehouse table subdirectories should inherit the group permissions of the warehouse parent directory,"When the Hive Metastore creates a subdirectory in the Hive warehouse for
a new table it does so with the default HDFS permissions. Since the default
dfs.umask value is 022, this means that the new subdirectory will not inherit the
group write permissions of the hive warehouse directory.

We should make the umask used by Warehouse.mkdirs() configurable, and set
it to use a default value of 002.
",chinnalalam,cwsteinbach,Major,Closed,Fixed,14/Oct/11 01:58,09/Jan/13 10:24
Bug,HIVE-2510,12527638,Hive throws Null Pointer Exception upon CREATE TABLE <db_name>.<table_name> ....     if the given <db_name> doesn't exist,,rsurowka,rsurowka,Trivial,Closed,Fixed,18/Oct/11 18:46,16/Dec/11 23:56
Bug,HIVE-2516,12527849,cleaunup QTestUtil: use test.data.files as current directory if one not specified,This will help debugging via eclipse.,namit,namit,Major,Closed,Fixed,19/Oct/11 23:45,16/Dec/11 23:56
Bug,HIVE-2519,12528027,Dynamic partition insert should enforce the order of the partition spec is the same as the one in schema,"Suppose the table schema is (a string, b string) partitioned by (p1 string, p2 string), a dynamic partition insert is allowed to:

insert overwrite ... partition (p2=""..."", p1);

which will create the wrong HDFS directory structure such as /.../p2=.../p1=.... This is contradictory to the metastore's assumption of the HDFS directory structure. ",nzhang,nzhang,Major,Closed,Fixed,20/Oct/11 21:17,16/Dec/11 23:56
Bug,HIVE-2520,12528136,left semi join will duplicate data,"CREATE TABLE sales (name STRING, id INT)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

CREATE TABLE things (id INT, name STRING)
ROW FORMAT DELIMITED FIELDS TERMINATED BY '\t';

The 'sales' table has data in a file: sales.txt, and the data is：
Joe 2
Hank 2

The 'things' table has data int two files: things.txt and things2.txt：
The content of things.txt is :
2 Tie
The content of things2.txt is :
2 Tie

SELECT * FROM sales LEFT SEMI JOIN things ON (sales.id = things.id);
will output：
Joe 2
Joe 2
Hank 2
Hank 2
so the result is wrong.

In CommonJoinOperator left semi join should use "" genObject(null, 0, new IntermediateObject(new ArrayList[numAliases], 0), true); "" to generate data.
but now it uses "" genUniqueJoinObject(0, 0); "" to generate data.
This patch will solve this problem.",binlijin,binlijin,Critical,Closed,Fixed,21/Oct/11 03:55,30/Apr/12 21:11
Bug,HIVE-2522,12528553,"HIVE-2446 bug (next one) - If constructor of ClientStatsPublisher throws runtime exception it will be propagated to HadoopJobExecHelper's progress method and beyond, whereas it shouldn't",,rsurowka,rsurowka,Major,Closed,Fixed,24/Oct/11 20:42,21/Mar/14 06:04
Bug,HIVE-2531,12529193,Allow people to use only issue numbers without 'HIVE-' prefix with `arc diff --jira`.,Allow people to use only issue numbers without 'HIVE-' prefix with `arc diff --jira`.,mareksapota_fb,mareksapota_fb,Major,Closed,Fixed,28/Oct/11 04:34,16/Dec/11 23:55
Bug,HIVE-2532,12529282,Evaluation of non-deterministic/stateful UDFs should not be skipped even if constant oi is returned.,"Even if constant oi is returned, these may have stateful/side-effect behavior and hence need to be called each cycle.",jonchang,jonchang,Major,Closed,Fixed,28/Oct/11 16:46,02/Aug/12 11:43
Bug,HIVE-2534,12529305,HiveIndexResult creation fails due to file system issue,"If the file system for temp files differs from the default file system in the conf, HiveIndexResult's constructor fails.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,28/Oct/11 19:40,16/Dec/11 23:56
Bug,HIVE-2536,12529523,Support scientific notation for Double literals,Of the form 1.0e10.,jonchang,jonchang,Major,Closed,Fixed,31/Oct/11 18:17,16/Dec/11 23:55
Bug,HIVE-2540,12529703,LATERAL VIEW with EXPLODE produces ConcurrentModificationException,"The following produces {{ConcurrentModificationException}} on the {{for}} loop inside EXPLODE:

{code}
create table foo as select array(1, 2) a from src limit 1;
select a, x.b from foo lateral view explode(a) x as b;
{code}
",navis,electrum,Major,Closed,Fixed,01/Nov/11 17:32,25/Mar/15 18:42
Bug,HIVE-2542,12529778,DROP DATABASE CASCADE does not drop non-native tables. ,"The hive meta store client does not delete the non-native tables during the drop database <dbname> cascade operation. As a result even though the database is deleted the tables still exist. This is related to HCATALOG-144. 

A deeper look at the HiveMetaStoreClient's ""dropDatabase"" function, tells us that the function does not utilize the hive meta hooks of the tables in the database for dropping the non-native tables. ",avandana,avandana,Major,Closed,Fixed,01/Nov/11 23:49,10/Jan/13 19:53
Bug,HIVE-2543,12529788,Compact index table's files merged in creation,"When a compact index is built there is the possibility of a merge task at the end of the task tree.  If this happens, the index table's files will no longer be sorted.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,02/Nov/11 01:17,30/Apr/12 21:11
Bug,HIVE-2544,12529893,Nullpointer on registering udfs.,Currently the Function registry can throw NullPointers when multiple threads are trying to register the same function. The normal put() will replace the existing registered function object even if it's exactly the same function.,appodictic,bennies,Blocker,Closed,Fixed,02/Nov/11 16:44,20/Jan/13 07:15
Bug,HIVE-2548,12530020,How to submit documentation fixes,"I am walking through the developer's guide and tutorial and finding issues: e.g. broken links.   Is there a way to try out updates to the docs and submit patches?

Here is the first example on https://cwiki.apache.org/Hive/tutorial.html


The following examples highlight some salient features of the system. A detailed set of ""query test cases"" can be found at Hive Query Test Cases and the corresponding results can be found at ""Query Test Case Results"".

The first link is listed as http://svn.apache.org/viewvc/hadoop/hive/trunk/ql/src/test/queries/clientpositive/

Second link is http://svn.apache.org/viewvc/hadoop/hive/trunk/ql/src/test/results/clientpositive/

Both links are 404's


",javadba,javadba,Minor,Closed,Fixed,03/Nov/11 08:54,16/Dec/11 23:56
Bug,HIVE-2550,12530146,Provide jira_base_url for improved arc commit workflow,,,mareksapota_fb,Major,Closed,Fixed,04/Nov/11 02:29,16/Dec/11 23:55
Bug,HIVE-2556,12530645,upgrade script 008-HIVE-2246.mysql.sql contains syntax errors,source <script_name> gives syntax errors. ,nzhang,nzhang,Major,Closed,Fixed,07/Nov/11 22:56,16/Dec/11 23:56
Bug,HIVE-2558,12530664,Timestamp comparisons don't work,"I may be missing something, but:

After performing:

create table rrt (r timestamp);
insert into table rrt select '1970-01-01 00:00:01' from src limit 1;

Following queries give undesirable results:

select * from rrt where r in ('1970-01-01 00:00:01');
select * from rrt where r in (0); 
select * from rrt where r = 0; 
select * from rrt where r = '1970-01-01 00:00:01';

At least for the first two, the reason may be the lack of timestamp in numericTypes Map from FunctionRegistry.java (591) . Yet whether we really want to have a linear hierarchy of primitive types in the end, is another question.",,rsurowka,Major,Resolved,Fixed,08/Nov/11 00:57,24/Jan/14 19:20
Bug,HIVE-2562,12530800,HIVE-2247 Changed the Thrift API causing compatibility issues.,"HIVE-2247 Added a parameter to alter_partition in the Metastore Thrift API which has been causing compatibility issues with some scripts.  We would like to change this to have two methods, one called alter_partition which takes the old parameters, and one called something else (I'll leave the naming up to you) which has the new parameters.  The implementation of the old method should just call the new method with null for the new parameter.

This will fix the compatibility issues.",weiyan,kevinwilfong,Major,Closed,Fixed,09/Nov/11 01:28,16/Dec/11 23:56
Bug,HIVE-2565,12530826,Add Java linter to Hive,"Add a linter that will be run at `arc diff` and will check for too long lines, trailing whitespace, etc.",mareksapota_fb,mareksapota_fb,Major,Closed,Fixed,09/Nov/11 07:33,16/Dec/11 23:56
Bug,HIVE-2568,12531039,HIVE-2246 upgrade script needs to drop foreign key in COLUMNS_OLD,"One more bug in the MySQL metastore upgrade script: the foreign key in COLUMNS needs to be dropped, otherwise drop_partition will fail because the SDS row cannot be deleted due to the foreign key constraint. ",nzhang,nzhang,Blocker,Closed,Fixed,10/Nov/11 19:47,16/Dec/11 23:56
Bug,HIVE-2571,12531047,eclipse template .classpath is broken,,rsurowka,rsurowka,Minor,Closed,Fixed,10/Nov/11 20:49,16/Dec/11 23:56
Bug,HIVE-2572,12531048,HIVE-2246 upgrade script changed the COLUMNS_V2.COMMENT length,This changes from varchar(4000) to varchar(128) is backward incompatible. ,nzhang,nzhang,Blocker,Closed,Fixed,10/Nov/11 20:51,16/Dec/11 23:56
Bug,HIVE-2574,12531183,ivy offline mode broken by changingPattern and checkmodified attributes,"As described here:

http://www.mail-archive.com/ivy-user@ant.apache.org/msg03534.html

This wasn't the case formerly (maybe the upgrade to ivy 1.2?)
",jvs,jvs,Major,Closed,Fixed,11/Nov/11 21:55,16/Dec/11 23:56
Bug,HIVE-2578,12531362,Debug mode in some situations doesn't work properly when child JVM  is started from MapRedLocalTask,,rsurowka,rsurowka,Minor,Closed,Fixed,14/Nov/11 20:00,16/Dec/11 23:57
Bug,HIVE-2580,12531403,"Hive build fails with error ""java.io.IOException: Not in GZIP format""",,cwsteinbach,cwsteinbach,Blocker,Closed,Fixed,15/Nov/11 01:54,16/Dec/11 23:56
Bug,HIVE-2581,12531519,explain task: getJSONPlan throws a NPE if the ast is null,,namit,namit,Major,Closed,Fixed,15/Nov/11 19:12,16/Dec/11 23:56
Bug,HIVE-2588,12531742,Update arcconfig to include commit listener,Use CommitListener from Arc-JIRA to modify commit message and include `(author via committer)` when running `arc commit`.,mareksapota_fb,mareksapota_fb,Major,Closed,Fixed,17/Nov/11 04:39,16/Dec/11 23:56
Bug,HIVE-2590,12531830,HBase bulk load wiki page improvements,"Some suggestions on the page https://cwiki.apache.org/confluence/display/Hive/HBaseBulkLoad which seems kind of out of date:

1. It seems like it's required that the number of reduce tasks in the ""Sort Data"" phase be one more than the number of keys selected in the ""Range Partitioning"" step, or else you get an error like this:


Caused by: java.lang.IllegalArgumentException: Can't read partitions file
	at org.apache.hadoop.mapred.lib.TotalOrderPartitioner.configure(TotalOrderPartitioner.java:91)
	... 15 more
Caused by: java.io.IOException: Wrong number of partitions in keyset
	at org.apache.hadoop.mapred.lib.TotalOrderPartitioner.configure(TotalOrderPartitioner.java:72)
	... 15 more

If so, it would be helpful if this was explicitly pointed out.

2. It recommends that you should use the ""loadtable"" ruby script to put data into hbase, but if you run this on newer versions of HBase (e.g. 0.90.3) it errors: 

    DISABLED!!!! Use completebulkload instead.  See tail of http://hbase.apache.org/bulk-loads.html

The instructions should probably be changed to use completebulkload instead of this script.

",xodarap,xodarap,Minor,Closed,Fixed,17/Nov/11 18:51,02/Aug/13 11:31
Bug,HIVE-2597,12531992,Repeated key in GROUP BY is erroneously displayed when using DISTINCT,"The following query was simplified for illustration purposes. 

This works correctly:
{code:sql}
select client_tid, """" as myvalue1, """" as myvalue2 from clients cluster by client_tid
{code}

The intent here is to produce two empty columns in between data.

The following query does not work:
{code:sql}
select distinct client_tid, """" as myvalue1, """" as myvalue2 from clients cluster by client_tid
{code}
{noformat}
FAILED: Error in semantic analysis: Line 1:44 Repeated key in GROUP BY """"
{noformat}
The key is not repeated since the aliases were given. Seems like Hive is ignoring the aliases when the ""distinct"" keyword is specified.
",navis,arov,Major,Closed,Fixed,18/Nov/11 20:06,13/Nov/14 19:43
Bug,HIVE-2598,12532004,Update README.txt file to use description from wiki,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,18/Nov/11 22:04,16/Dec/11 23:55
Bug,HIVE-2613,12533072,HiveCli eclipse launch configuration hangs,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,29/Nov/11 06:53,16/Dec/11 23:57
Bug,HIVE-2617,12533321,Insert overwrite table db.tname fails if partition already exists ,"Insert Overwrite table db.tname fails if partition already exists.
For example-
insert overwrite table db.tname PARTITION(part='p') select .. from t2 where part='p'; fails if partition 'p' already exists. Workaround is - use db; and the fire the command.
From the source code-
alterPartition(tbl.getTableName(), new Partition(tbl, tpart)); takes String tablename as argument and loses db information. Table table = newTable(tablename) is called to retrieve table from name. But, it relies on currentDatabase value (hence the workaround).",chinnalalam,aniket486,Major,Closed,Fixed,30/Nov/11 18:03,30/Apr/12 21:11
Bug,HIVE-2618,12533329,Describe partition returns table columns but should return partition columns,"If a partitioned table and some partitions are created, and then the table is altered adding a columns, if describe is called on the partitions created before the columns were added it will show the new columns, even though it should not.  In particular, in the metastore, the partition will not have these columns.",namit,kevinwilfong,Major,Closed,Fixed,30/Nov/11 18:54,28/Sep/12 22:53
Bug,HIVE-2622,12533561,Hive POMs reference the wrong Hadoop artifacts,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,02/Dec/11 02:16,16/Dec/11 23:56
Bug,HIVE-2624,12533930,Fix eclipse classpath template broken in HIVE-2523,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,05/Dec/11 19:58,16/Dec/11 23:57
Bug,HIVE-2625,12533944,Fix maven-build Ant target,This was broken by HIVE-2523.,cwsteinbach,cwsteinbach,Major,Closed,Fixed,05/Dec/11 21:34,16/Dec/11 23:56
Bug,HIVE-2627,12533981,NPE on MAP-JOIN with a UDF in an external JAR,"When a query is converted into a map join, and it depends on some UDF (ADD JAR...; CREATE TEMPORARY FUNCTION...), then an NPE may happen.  Here is an example.

SELECT
    some_udf(dummy1) as dummies
FROM (
    SELECT        
        a.dummy as dummy1,
        b.dummy as dummy2
    FROM        
        test a    
    LEFT OUTER JOIN
        test b
    ON
        a.dummy = b.dummy
) c;

My guess is that the JAR classes are not getting propagated to the hashmapjoin operator.",,jonchang,Major,Resolved,Fixed,06/Dec/11 02:57,08/Jun/14 18:05
Bug,HIVE-2629,12534090,Make a single Hive binary work with both 0.20.x and 0.23.0,,thw,cwsteinbach,Major,Closed,Fixed,06/Dec/11 22:07,02/May/13 02:29
Bug,HIVE-2630,12534091,TestHiveServer doesn't produce a JUnit report file,"Run `ant test --Dtestcase=TestHiveServer && ant testreport`, the test report will be empty because not 'TEST--*.xml' file was produced by JUnit.",mareksapota_fb,mareksapota_fb,Major,Closed,Fixed,06/Dec/11 22:09,16/Dec/11 23:56
Bug,HIVE-2631,12534122,Make Hive work with Hadoop 1.0.0,"With Hadoop 1.0.0 around the corner ( http://mail-archives.apache.org/mod_mbox/hadoop-general/201111.mbox/%3C9D6B6144-F4E0-4A31-883F-2AC504727A1F%40hortonworks.com%3E ), it will be useful to make Hive work with it.",ashutoshc,ashutoshc,Major,Closed,Fixed,07/Dec/11 01:14,30/Apr/12 21:11
Bug,HIVE-2632,12534247,ignore exception for external jars via reflection,,namit,namit,Major,Closed,Fixed,07/Dec/11 21:09,30/Apr/12 21:11
Bug,HIVE-2634,12534284,revert HIVE-2566,"This is leading to some problems.

I will upload the offending testcase in a new jira.",namit,namit,Major,Closed,Fixed,08/Dec/11 01:55,16/Dec/11 23:56
Bug,HIVE-2635,12534311,wrong class loader used for external jars,,namit,namit,Major,Closed,Fixed,08/Dec/11 07:04,30/Apr/12 21:11
Bug,HIVE-2638,12534398,Tests fail when Hive is run against Hadoop 0.23,,,cwsteinbach,Major,Resolved,Fixed,08/Dec/11 20:49,22/Nov/14 23:24
Bug,HIVE-2643,12534587,Recent patch prevents Hadoop confs from loading in 0.20.204,"After the application of HIVE-2362, we're unable to run queries against the Hive cluster (RC2 and 3):
{noformat}java.io.IOException: Failed to set permissions of path: /user/jhoman-667872320/.staging to 0700
        at org.apache.hadoop.fs.FileUtil.checkReturnValue(FileUtil.java:680)
        at org.apache.hadoop.fs.FileUtil.setPermission(FileUtil.java:653)
        at org.apache.hadoop.fs.RawLocalFileSystem.setPermission(RawLocalFileSystem.java:483)
        at org.apache.hadoop.fs.RawLocalFileSystem.mkdirs(RawLocalFileSystem.java:318)
        at org.apache.hadoop.fs.FilterFileSystem.mkdirs(FilterFileSystem.java:183)
        at org.apache.hadoop.mapreduce.JobSubmissionFiles.getStagingDir(JobSubmissionFiles.java:116){noformat}
this is due to all the hadoop confs not being loaded and the LocalJobRunner (which builds staging directories in this manner) being used instead of the regular one. It's also not possible to access the hdfs since no default fs is specified.  

Reverting 2362 fixes this, but I've not yet looked at that patch to see the exact cause.",cwsteinbach,jghoman,Blocker,Closed,Fixed,10/Dec/11 04:56,26/Nov/12 21:32
Bug,HIVE-2646,12534814,"Hive Ivy dependencies on Hadoop should depend on jars directly, not tarballs","The current Hive Ivy dependency logic for its Hadoop dependencies is problematic - depending on the tarball and extracting the jars from there, rather than depending on the jars directly. It'd be great if this was fixed to actually have the jar dependencies defined directly.",abayer,abayer,Critical,Closed,Fixed,12/Dec/11 18:02,12/May/15 02:59
Bug,HIVE-2647,12534828,Force Bash shell on parallel test slave nodes,"Force use of Bash shell, so user default shell choice doesn't impact the test script.",mareksapota_fb,mareksapota_fb,Major,Closed,Fixed,12/Dec/11 20:03,30/Apr/12 21:12
Bug,HIVE-2648,12534830,Parallel tests fail if master directory is not present,Parallel tests should create directories as needed.,mareksapota_fb,mareksapota_fb,Major,Closed,Fixed,12/Dec/11 20:25,30/Apr/12 21:11
Bug,HIVE-2649,12534831,Allow multiple ptest runs by the same person,"Allow running ptest simultaneously, multiple times by one person on the same machines.",mareksapota_fb,mareksapota_fb,Major,Closed,Fixed,12/Dec/11 20:32,30/Apr/12 21:11
Bug,HIVE-2650,12534851,Parallel test commands that include cd fail,Parallel test commands that include cd fail.,mareksapota_fb,mareksapota_fb,Major,Closed,Fixed,12/Dec/11 23:21,30/Apr/12 21:11
Bug,HIVE-2654,12535149,"""hive.querylog.location"" requires parent directory to be exist or else folder creation fails","if value of ""hive.querylog.location"" is '/tmp/root/hive123/test' if the parent directories not exist the creation of the folder is failed.",chinnalalam,chinnalalam,Major,Closed,Fixed,14/Dec/11 13:10,30/Apr/12 21:11
Bug,HIVE-2657,12535342,builtins JAR is not being published to Maven repo & hive-cli POM does not depend on it either,,cwsteinbach,tucu00,Major,Closed,Fixed,15/Dec/11 17:35,30/Apr/12 21:11
Bug,HIVE-2660,12535416,Need better exception handling in RCFile tolerate corruptions mode,"The exception handling in nextKeyValueTolerateCorruptions treats IOException as follows:
 - if EOFException, corrupt, can be tolerated
 - If CheckSumException, corrupt, can be tolerated
 - else not a corruption, re-throw

But the compression code can also throw IOException in case of corruption, which will get re-thrown in this case.

The correct way of handling IOException is:
 - if BlockMissingException, re-throw.
 - if not BlockMissingException -> corruption, can be tolerated",rvadali,rvadali,Minor,Closed,Fixed,16/Dec/11 07:47,30/Apr/12 21:12
Bug,HIVE-2666,12535907,StackOverflowError when using custom UDF in map join,"When a custom UDF is used as part of a join which is converted to a map join, the XMLEncoder enters an infinite loop when serializing the map reduce task for the second time, as part of sending it to be executed.  This results in a stack overflow error.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,20/Dec/11 21:38,10/Apr/13 03:43
Bug,HIVE-2673,12536108,Eclipse launch configurations fail due to unsatisfied builtins JAR dependency,,cwsteinbach,cwsteinbach,Major,Closed,Fixed,22/Dec/11 00:57,09/Jan/13 10:23
Bug,HIVE-2674,12536117,get_partitions_ps throws TApplicationException if table doesn't exist,"If the table passed to get_partition_ps doesn't exist, a NPE is thrown by getPartitionPsQueryResults.  There should be a check here, which throws a NoSuchObjectException if the table doesn't exist.",kevinwilfong,kevinwilfong,Major,Closed,Fixed,22/Dec/11 02:52,09/Jan/13 10:23
Bug,HIVE-2681,12536393,SUCESS is misspelled,C'mon!,jonchang,jonchang,Major,Closed,Fixed,26/Dec/11 18:21,30/Apr/12 21:11
Bug,HIVE-2689,12536715,ObjectInspectorConverters cannot convert Void types to Array/Map/Struct types.,"In a bunch of places we rely on ObjectInspectorConverters to implicitly convert types.  Unfortunately, an exception will needlessly be thrown if one of the types is a complex type and the other is a void type.  For example,

SELECT ARRAY(ARRAY(), NULL) ...

",jonchang,jonchang,Minor,Closed,Fixed,31/Dec/11 01:11,16/May/13 21:10
Bug,HDFS-1572,12494838,Checkpointer should trigger checkpoint with specified period.,"{code:}
  long now = now();
  boolean shouldCheckpoint = false;
  if(now >= lastCheckpointTime + periodMSec) {
    shouldCheckpoint = true;
  } else {
    long size = getJournalSize();
    if(size >= checkpointSize)
      shouldCheckpoint = true;
  }
{code}
{dfs.namenode.checkpoint.period} in configuration determines the period of checkpoint. However, with above code, the Checkpointer triggers a checkpoint every 5 minutes (periodMSec=5*60*1000). According to SecondaryNameNode.java, the first *if*  statement should be:
 {code:}
if(now >= lastCheckpointTime + 1000 * checkpointPeriod) {
 {code}
",jghoman,liangly,Blocker,Closed,Fixed,06/Jan/11 12:55,12/Dec/11 06:18
Bug,HDFS-1575,12495184,viewing block from web UI broken,"DatanodeJspHelper seems to expect the file path to be in the ""path info"" of the HttpRequest, rather than in a parameter. I see the following exception when visiting the URL {{http://localhost.localdomain:50075/browseBlock.jsp?blockId=5006108823351810567&blockSize=20&genstamp=1001&filename=%2Fuser%2Ftodd%2Fissue&datanodePort=50010&namenodeInfoPort=50070}}

java.io.FileNotFoundException: File does not exist: /
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getBlockLocationsInternal(FSNamesystem.java:834)
...
	at org.apache.hadoop.hdfs.server.datanode.DatanodeJspHelper.generateFileDetails(DatanodeJspHelper.java:258)
	at org.apache.hadoop.hdfs.server.datanode.browseBlock_jsp._jspService(browseBlock_jsp.java:79)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:97)
",atm,tlipcon,Blocker,Closed,Fixed,10/Jan/11 22:56,02/May/13 02:29
Bug,HDFS-1584,12495945,Need to check TGT and renew if needed when fetching delegation tokens using HFTP,"Currently, there is no checking on TGT validity when calling getDelegationToken(). The TGT may expire and the call will fail.",benoyantony,kzhang,Major,Resolved,Fixed,19/Jan/11 00:50,27/Jul/12 06:34
Bug,HDFS-1585,12496033,HDFS-1547 broke MR build,Added a parameter to startDatanodes without maintaining old API,tlipcon,tlipcon,Blocker,Resolved,Fixed,19/Jan/11 18:20,20/Apr/11 23:33
Bug,HDFS-1591,12496316,"Fix javac, javadoc, findbugs warnings",Split from HADOOP-6642,pocheung,pocheung,Major,Closed,Fixed,21/Jan/11 18:44,02/May/13 02:28
Bug,HDFS-1592,12496444,Datanode startup doesn't honor volumes.tolerated ,Datanode startup doesn't honor volumes.tolerated for hadoop 20 version.,bharathm,bharathm,Major,Closed,Fixed,23/Jan/11 16:42,26/Nov/12 11:23
Bug,HDFS-1594,12496536,When the disk becomes full Namenode is getting shutdown and not able to recover,"When the disk becomes full name node is shutting down and if we try to start after making the space available It is not starting and throwing the below exception.



{code:xml} 

2011-01-24 23:23:33,727 ERROR org.apache.hadoop.hdfs.server.namenode.FSNamesystem: FSNamesystem initialization failed.
java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at org.apache.hadoop.io.UTF8.readFields(UTF8.java:117)
	at org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.readString(FSImageSerialization.java:201)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:185)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:60)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:1089)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:1041)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:487)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.loadFSImage(FSDirectory.java:149)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initialize(FSNamesystem.java:306)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:284)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:328)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:356)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:577)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:570)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1529)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1538)
2011-01-24 23:23:33,729 ERROR org.apache.hadoop.hdfs.server.namenode.NameNode: java.io.EOFException
	at java.io.DataInputStream.readFully(DataInputStream.java:180)
	at org.apache.hadoop.io.UTF8.readFields(UTF8.java:117)
	at org.apache.hadoop.hdfs.server.namenode.FSImageSerialization.readString(FSImageSerialization.java:201)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadEditRecords(FSEditLogLoader.java:185)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:93)
	at org.apache.hadoop.hdfs.server.namenode.FSEditLogLoader.loadFSEdits(FSEditLogLoader.java:60)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSEdits(FSImage.java:1089)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.loadFSImage(FSImage.java:1041)
	at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:487)
	at org.apache.hadoop.hdfs.server.namenode.FSDirectory.loadFSImage(FSDirectory.java:149)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initialize(FSNamesystem.java:306)
	at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:284)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:328)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:356)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:577)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:570)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1529)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1538)

2011-01-24 23:23:33,730 INFO org.apache.hadoop.hdfs.server.namenode.NameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down NameNode at linux124/10.18.52.124
************************************************************/


{code} 
",atm,devaraj,Major,Resolved,Fixed,24/Jan/11 13:32,28/May/13 22:56
Bug,HDFS-1597,12496744,Batched edit log syncs can reset synctxid throw assertions,"The top of FSEditLog.logSync has the following assertion:
{code}
        assert editStreams.size() > 0 : ""no editlog streams"";
{code}
which should actually come after checking to see if the sync was already batched in by another thread.

This is related to a second bug in which the same case causes synctxid to be reset to 0",tlipcon,tlipcon,Blocker,Closed,Fixed,25/Jan/11 23:16,12/Dec/11 06:19
Bug,HDFS-1598,12496875,ListPathsServlet excludes .*.crc files,The {{.*.crc}} files are excluded by default.,szetszwo,szetszwo,Major,Closed,Fixed,26/Jan/11 22:05,12/Dec/11 06:19
Bug,HDFS-1600,12496887,editsStored.xml cause release audit warning,The file {{src/test/hdfs/org/apache/hadoop/hdfs/tools/offlineEditsViewer/editsStored.xml}} for any new patch.,tlipcon,szetszwo,Major,Resolved,Fixed,27/Jan/11 01:09,20/Apr/11 23:33
Bug,HDFS-1602,12497010,NameNode storage failed replica restoration is broken,"NameNode storage restore functionality doesn't work (as HDFS-903 demonstrated). This needs to be either disabled, or removed, or fixed. This feature also fails HDFS-1496",boryas,cos,Major,Closed,Fixed,28/Jan/11 02:51,12/Dec/11 06:19
Bug,HDFS-1607,12497393,Fix references to misspelled method name getProtocolSigature,,tlipcon,tlipcon,Trivial,Resolved,Fixed,01/Feb/11 22:27,20/Apr/11 23:33
Bug,HDFS-1610,12497604,TestClientProtocolWithDelegationToken failing,Another instance of the same type of failure as MAPREDUCE-2300 (a mock protocol implementation isn't returning a protocol signature),tlipcon,tlipcon,Blocker,Resolved,Fixed,03/Feb/11 18:56,20/Apr/11 23:33
Bug,HDFS-1611,12497850,Some logical issues need to address.,"Title: Some code level logical issues.

Description:

1. DFSClient:  
  Consider the below case, if we enable only info, then below log will never be logged.
 if (ClientDatanodeProtocol.LOG.isDebugEnabled()) {
      ClientDatanodeProtocol.LOG.info(""ClientDatanodeProtocol addr="" + addr);
    }

2.org.apache.hadoop.hdfs.server.namenode.FSNamesystem.registerMBean()
  
  catch (NotCompliantMBeanException e) {
      e.printStackTrace();
    }

  We can avoid using stackTace(). Better to add log message.  

",umamaheswararao,umamaheswararao,Minor,Resolved,Fixed,07/Feb/11 14:26,27/Sep/13 06:18
Bug,HDFS-1612,12497860,HDFS Design Documentation is outdated,"I was trying to discover details about the Secondary NameNode, and came across the description below in the HDFS design doc.

{quote}
The NameNode keeps an image of the entire file system namespace and file Blockmap in memory. This key metadata item is designed to be compact, such that a NameNode with 4 GB of RAM is plenty to support a huge number of files and directories. When the NameNode starts up, it reads the FsImage and EditLog from disk, applies all the transactions from the EditLog to the in-memory representation of the FsImage, and flushes out this new version into a new FsImage on disk. It can then truncate the old EditLog because its transactions have been applied to the persistent FsImage. This process is called a checkpoint. *In the current implementation, a checkpoint only occurs when the NameNode starts up. Work is in progress to support periodic checkpointing in the near future.*
{quote}
(emphasis mine).

Note that this directly conflicts with information in the hdfs user guide, http://hadoop.apache.org/common/docs/r0.20.2/hdfs_user_guide.html#Secondary+NameNode
and http://hadoop.apache.org/hdfs/docs/current/hdfs_user_guide.html#Checkpoint+Node

I haven't done a thorough audit of that doc-- I only noticed the above inaccuracy.
",joecrobak,joecrobak,Minor,Resolved,Fixed,07/Feb/11 15:53,21/Apr/11 23:32
Bug,HDFS-1615,12498070,seek() on closed DFS input stream throws NPE,"After closing an input stream on DFS, seeking slightly ahead of the last read will throw an NPE:

java.lang.NullPointerException
        at org.apache.hadoop.hdfs.DFSInputStream.seek(DFSInputStream.java:749)
        at org.apache.hadoop.fs.FSDataInputStream.seek(FSDataInputStream.java:42)",scott_carey,tlipcon,Major,Closed,Fixed,09/Feb/11 01:31,12/Dec/11 06:19
Bug,HDFS-1621,12498209,Fix references to hadoop-common-${version} in build.xml,"Similar to MAPREDUCE-2315, we should fix any references to the hadoop common jar that use ${version} instead of ${hadoop-common.version}.",jolly,tlipcon,Major,Closed,Fixed,10/Feb/11 08:18,12/Dec/11 06:18
Bug,HDFS-1625,12498474,TestDataNodeMXBean fails if disk space usage changes during test run,"I've seen this on our internal hudson - we get failures like:

null expected:<...:{""freeSpace"":857683[43552],""usedSpace"":28672,""...> but was:<...:{""freeSpace"":857683[59936],""usedSpace"":28672,""...>

because some other build on the same build slave used up some disk space during the middle of the test.",szetszwo,tlipcon,Minor,Resolved,Fixed,12/Feb/11 22:06,21/Apr/11 23:32
Bug,HDFS-1627,12498677,Fix NullPointerException in Secondary NameNode,Secondary NameNode should not reset namespace if no new image is downloaded from the primary NameNode.,hairong,hairong,Major,Resolved,Fixed,15/Feb/11 19:19,21/May/11 12:38
Bug,HDFS-1656,12499767,getDelegationToken in HftpFileSystem should renew TGT if needed.,Fetching of delegation tokens in HftpFileSystem will fail if TGT has expired. The TGT should be renewed first if needed.,jnp,jnp,Major,Resolved,Fixed,25/Feb/11 21:46,16/Jun/11 19:44
Bug,HDFS-1665,12499786,Balancer sleeps inadequately,The value of {{dfs.heartbeat.interval}} is in seconds.  Balancer seems misused it.,szetszwo,szetszwo,Minor,Closed,Fixed,26/Feb/11 03:40,15/Nov/11 00:52
Bug,HDFS-1687,12500120,HDFS Federation: DirectoryScanner changes for federation,"DirectoryScanner scans substantially all of the directory tree of entire volumes.  It needs to be extended to work with Blockpools in Federation.  

Design notes:

1. The subdirectories of active bpid's will be scanned.  Active bpid's are those associated with currently connected Namenodes.  Each Volume knows the set of all active bpid's, via volume.map.keySet().  I'll add a package-private accessor in FSVolume to return the set of active bpid's for use by DirectoryScanner, DataBlockScanner, etc.  DirectoryScanner will ignore inactive bpid's subdirectories; see item below.  

2. There is no need to compare the volume set of active bpid's with the global set, because the way the code works, they really can't be different.  If differences arise, they will be automatically fixed by the next restart of either the Datanode or the Namenode.

3. Inactive bpid's will be ignored.  Until we are connected to the owner Namenode, we cannot know whether a bpid subdirectory is correctly formatted, has snapshot data, etc.  So it doesn't make sense to try to manage the data under an inactive bpid.

4. DirectoryScanner is currently instantiated and periodically triggered by DataBlockScanner.  Other than both being ""scanners"", these two modules have little in common, and the triggering code is confusing.  (DirectoryScanner scans filesystem directory trees every hour, to detect and fix inconsistencies between disk directories and ReplicasMap.  DataBlockScanner runs every 3 weeks, and traverses all block files, actually reading them out and checksumming them to detect block corruption.)

Separating them, and running DirectoryScanner under its own periodic scheduler, is a small change that will make the code much clearer.  It already runs on its own FixedThreadPool Executor, so it is easy to change it to a ScheduledThreadPool, and instantiate it from DataNode.postStartInit() at the same time as initBlockScanner() is called.
",mattf,mattf,Major,Resolved,Fixed,01/Mar/11 23:09,02/Mar/11 21:54
Bug,HDFS-1691,12500155,"double static declaration in Configuration.addDefaultResource(""hdfs-default.xml"");","in /src/java/org/apache/hadoop/hdfs/tools/DFSck.java
double declaration 

static{
    Configuration.addDefaultResource(""hdfs-default.xml"");
    Configuration.addDefaultResource(""hdfs-site.xml"");
}

1. in head class
2. before main
",humanoid,humanoid,Minor,Closed,Fixed,02/Mar/11 08:24,15/Nov/11 00:52
Bug,HDFS-1692,12500157,"In secure mode, Datanode process doesn't exit when disks fail.","In secure mode, when disks fail more than volumes tolerated, datanode process doesn't exit properly and it just hangs even though shutdown method is called. 

",bharathm,bharathm,Major,Closed,Fixed,02/Mar/11 08:45,02/Sep/11 22:16
Bug,HDFS-1699,12500231,HDFS Federation: fix failure of TestBlockReport,"FS is no longer immediately available when a datanode activates; one must wait for it to negotiate with a namenode and
establish a blockpool.  The current code was attempting to fetch FS too fast and getting null.  
",mattf,mattf,Major,Resolved,Fixed,02/Mar/11 21:28,02/Mar/11 21:54
Bug,HDFS-1700,12500233,Federation: fsck needs to work with federation configuration,"Fsck fails trying to find machine ""0.0.0.0"", because fsck not yet adapted to Federation.

Also probably would not correctly use the ""-fs"" generic option.
",mattf,mattf,Major,Resolved,Fixed,02/Mar/11 21:38,07/Mar/11 22:06
Bug,HDFS-1727,12500614,fsck command can display command usage if user passes any illegal argument,"In fsck command if user passes the arguments like
./hadoop fsck -test -files -blocks -racks
In this case it will take / and will display whole DFS information regarding to files,blocks,racks.

But here, we are hiding the user mistake. Instead of this, we can display the command usage if user passes any invalid argument like above.

If user passes illegal optional arguments like
./hadoop fsck /test -listcorruptfileblocks instead of
./hadoop fsck /test -list-corruptfileblocks also we can display the proper command usage",sravankorumilli,umamaheswararao,Minor,Closed,Fixed,07/Mar/11 10:59,15/Nov/11 00:52
Bug,HDFS-1728,12500659,SecondaryNameNode.checkpointSize is in byte but not MB.,"The unit of SecondaryNameNode.checkpointSize is byte but not MB as stated in the following comment.
{code}
//SecondaryNameNode.java
   private long checkpointSize;    // size (in MB) of current Edit Log
{code}
",szetszwo,szetszwo,Minor,Closed,Fixed,07/Mar/11 18:30,29/Apr/13 23:53
Bug,HDFS-1734,12500752,'Chunk size to view' option is not working in Name Node UI.,"  1. Write a file to DFS
  2. Browse the file using Name Node UI.
  3. give the chunk size to view as 100 and click the refresh.

  It will say Invalid input ( getnstamp absent )
",umamaheswararao,umamaheswararao,Major,Closed,Fixed,08/Mar/11 11:50,15/Nov/11 00:53
Bug,HDFS-1748,12501084,Balancer utilization classification is incomplete,"{code}
//Balancer.java
  /* Return true if the given datanode is overUtilized */
  private boolean isOverUtilized(BalancerDatanode datanode) {
    return datanode.utilization > (avgUtilization+threshold);
  }
  
  /* Return true if the given datanode is above average utilized
   * but not overUtilized */
  private boolean isAboveAvgUtilized(BalancerDatanode datanode) {
    return (datanode.utilization <= (avgUtilization+threshold))
        && (datanode.utilization > avgUtilization);
  }
  
  /* Return true if the given datanode is underUtilized */
  private boolean isUnderUtilized(BalancerDatanode datanode) {
    return datanode.utilization < (avgUtilization-threshold);
  }

  /* Return true if the given datanode is below average utilized 
   * but not underUtilized */
  private boolean isBelowAvgUtilized(BalancerDatanode datanode) {
        return (datanode.utilization >= (avgUtilization-threshold))
                 && (datanode.utilization < avgUtilization);
  }
{code}
Where is {{datanode.utilization == avgUtilization}}?",szetszwo,szetszwo,Major,Closed,Fixed,10/Mar/11 23:42,15/Nov/11 00:53
Bug,HDFS-1750,12501224,fs -ls hftp://file not working,"{noformat}
hadoop dfs -touchz /tmp/file1 # create file. OK
hadoop dfs -ls /tmp/file1  # OK
hadoop dfs -ls hftp://namenode:50070/tmp/file1 # FAILED: not seeing the file
{noformat}",szetszwo,szetszwo,Major,Closed,Fixed,12/Mar/11 02:14,02/Sep/11 22:16
Bug,HDFS-1753,12501364,Resource Leak in org.apache.hadoop.hdfs.server.namenode.StreamFile,"In doGet Method, 
final DFSInputStream in = dfs.open(filename);
final long fileLen = in.getFileLength();
OutputStream os = response.getOutputStream(); 
Here this lines are present at out side of the try block.
If response.getOutputStream() throws any exception then DFSInputStream will not be closed.So, better to move response.getOutputStream() into try block.


 

",umamaheswararao,umamaheswararao,Minor,Resolved,Fixed,14/Mar/11 15:29,11/Jul/11 12:57
Bug,HDFS-1758,12501533,Web UI JSP pages thread safety issue,"The set of JSP pages that web UI uses are not thread safe.  We have observed some problems when requesting Live/Dead/Decommissioning pages from the web UI, incorrect page is displayed.  To be more specific, requesting Dead node list page, sometimes, Live node page is returned.  Requesting decommissioning page, sometimes, dead page is returned.

The root cause of this problem is that JSP page is not thread safe by default.  When multiple requests come in,  each request is assigned to a different thread, multiple threads access the same instance of the servlet class resulted from a JSP page.  A class variable is shared by multiple threads.  The JSP code in 20 branche, for example, dfsnodelist.jsp has
{code}
<!%
  int rowNum = 0;
  int colNum = 0;
  String sorterField = null;
  String sorterOrder = null;
  String whatNodes = ""LIVE"";
  ...
%>
{code}

declared as  class variables.  ( These set of variables are declared within <%! code %> directives which made them class members. )  Multiple threads share the same set of class member variables, one request would step on anther's toe. 

However, due to the JSP code refactor, HADOOP-5857, all of these class member variables are moved to become function local variables.  So this bug does not appear in Apache trunk.  Hence, we have proposed to take a simple fix for this bug on 20 branch alone, to be more specific, branch-0.20-security.

The simple fix is to add jsp ThreadSafe=""false"" directive into the related JSP pages, dfshealth.jsp and dfsnodelist.jsp to make them thread safe, i.e. only on request is processed at each time. 

We did evaluate the thread safety issue for other JSP pages on trunk, we noticed a potential problem is that when we retrieving some statistics from namenode, for example, we make the call to 
{code}
NamenodeJspHelper.getInodeLimitText(fsn);
{code}
in dfshealth.jsp, which eventuality is 

{code}
  static String getInodeLimitText(FSNamesystem fsn) {
    long inodes = fsn.dir.totalInodes();
    long blocks = fsn.getBlocksTotal();
    long maxobjects = fsn.getMaxObjects();
    ....
{code}

some of the function calls are already guarded by readwritelock, e.g. dir.totalInodes, but others are not.  As a result of this, the web ui results are not 100% thread safe.  But after evaluating the prons and cons of adding a giant lock into the JSP pages, we decided not to issue FSNamesystem ReadWrite locks into JSPs.

",tanping,tanping,Minor,Closed,Fixed,16/Mar/11 01:14,02/Sep/11 22:16
Bug,HDFS-1760,12501627,problems with getFullPathName,"FSDirectory's getFullPathName method is flawed.  Given a list of inodes, it starts at index 1 instead of 0 (based on the assumption that inode[0] is always the root inode) and then builds the string with ""/""+inode[i].  This means the empty string is returned for the root, or when requesting the full path of the parent dir for top level items.

In addition, it's not guaranteed that the list of inodes starts with the root inode.  The inode lookup routine will only fill the inode array with the last n-many inodes of a path if the array is smaller than the path.  In these cases, getFullPathName will skip the first component of the relative path, and then assume the second component starts at the root.  ex. ""a/b/c"" becomes ""/b/c"".

There are a few places in the code where the issue was hacked around by assuming that a 0-length path meant a hardcoded ""/"" instead of Path.SEPARATOR.",daryn,daryn,Major,Closed,Fixed,16/Mar/11 22:20,15/Nov/11 00:52
Bug,HDFS-1765,12501721,Block Replication should respect under-replication block priority,"Currently under-replicated blocks are assigned different priorities depending on how many replicas a block has. However the replication monitor works on blocks in a round-robin fashion. So the newly added high priority blocks won't get replicated until all low-priority blocks are done. One example is that on decommissioning datanode WebUI we often observe that ""blocks with only decommissioning replicas"" do not get scheduled to replicate before other blocks, so risking data availability if the node is shutdown for repair before decommission completes.",umamaheswararao,hairong,Major,Closed,Fixed,17/Mar/11 19:01,28/Sep/15 20:58
Bug,HDFS-1776,12502130,Bug in Concat code,There is a bug in the concat code. Specifically: in INodeFile.appendBlocks() we need to first reassign the blocks list and then go through it and update the INode pointer. Otherwise we are not updating the inode pointer on all of the new blocks in the file.,bharathm,dms,Major,Closed,Fixed,23/Mar/11 04:48,15/Nov/11 00:52
Bug,HDFS-1779,12502188,"After NameNode restart , Clients can not read partial files even after client invokes Sync.","In Append HDFS-200 issue,
If file has 10 blocks and after writing 5 blocks if client invokes sync method then NN will persist the blocks information in edits. 
After this if we restart the NN, All the DataNodes will reregister with NN. But DataNodes are not sending the blocks being written information to NN. DNs are sending the blocksBeingWritten information in DN startup. So, here NameNode can not find that the 5 persisted blocks belongs to which datanodes. This information can build based on block reports from DN. Otherwise we will loose this 5 blocks information even NN persisted that block information in edits. 
",umamaheswararao,umamaheswararao,Major,Closed,Fixed,23/Mar/11 14:42,19/Oct/11 17:38
Bug,HDFS-1781,12502328,jsvc executable delivered into wrong package...,"The jsvc executable is delivered in the 0.22 hdfs package, but the script that uses it (bin/hdfs) refers to
$HADOOP_HOME/bin/jsvc to find it.",johnvijoe,johnvijoe,Major,Closed,Fixed,24/Mar/11 21:47,15/Nov/11 00:52
Bug,HDFS-1782,12502329,FSNamesystem.startFileInternal(..) throws NullPointerException,"I'm observing when there is one balancer running trying to run another one results in
""Java.lang.NullPointerException"" error. I was hoping to see message ""Another balancer is running. 
Exiting....  Exiting ..."". This is a reproducible issue.

Details
========

1) Cluster ->elrond

[hdfs@]$ hadoop version

2) Run first balancer
[hdfs]$ hdfs balancer
1
through XX.XX.XX.XX:1004 is succeeded.
[hdfs@]$ hdfs balancer
11/03/09 16:34:32 INFO balancer.Balancer: namenodes = 
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1400)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFile(FSNamesystem.java:1284)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.create(NameNode.java:779)
        at sun.reflect.GeneratedMethodAccessor46.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:346)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1399)
        at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1395)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1094)
        at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1393)
.  Exiting ...
Balancing took 1.366 seconds		
",johnvijoe,johnvijoe,Major,Closed,Fixed,24/Mar/11 21:49,15/Nov/11 00:52
Bug,HDFS-1786,12502398,"Some cli test cases expect a ""null"" message","There are a few tests cases specified in {{TestHDFSCLI}} and {{TestDFSShell}} expecting ""null"" messages.
e.g. in {{testHDFSConf.xml}},
{code}
          <expected-output>get: null</expected-output>
{code}",umamaheswararao,szetszwo,Minor,Closed,Fixed,25/Mar/11 17:50,15/Nov/11 00:53
Bug,HDFS-1797,12502785,New findbugs warning introduced by HDFS-1120,"HDFS-1120 introduced a new findbugs warning:

Unread field: org.apache.hadoop.hdfs.server.datanode.FSDataset$FSVolumeSet.curVolume

This JIRA is to fix the simple error.",tlipcon,tlipcon,Major,Closed,Fixed,30/Mar/11 01:06,03/Jan/12 14:03
Bug,HDFS-1806,12503427,TestBlockReport.blockReport_08() and _09() are timing-dependent and likely to fail on fast servers,"Method waitForTempReplica() polls every 100ms during block replication, attempting to ""catch"" a datanode in the state of having a TEMPORARY replica.  But examination of a current Hudson test failure log shows that the replica goes from ""start"" to ""TEMPORARY"" to ""FINALIZED"" in only 50ms, so of course the poll usually misses it.",mattf,mattf,Major,Closed,Fixed,05/Apr/11 17:26,15/Nov/11 00:52
Bug,HDFS-1808,12503459,"TestBalancer waits forever, errs without giving information","In three locations in the code, TestBalancer waits forever on a condition.  Failures result in Hudson/Jenkins ""Timeout occurred"" error message with no information about where or why.  Need to replace with TimeoutExceptions that throw a stack trace and useful info about the failure mode.

In waitForHeartBeat(), it is waiting on an exact match for a value that may be coarsely quantized -- i.e., significant deviation from the exact ""expected"" result may occur.  Replace with an allowed range of result.",mattf,mattf,Major,Closed,Fixed,05/Apr/11 21:40,15/Nov/11 00:53
Bug,HDFS-1812,12503504,Address the cleanup issues in TestHDFSCLI.java,,umamaheswararao,umamaheswararao,Minor,Closed,Fixed,06/Apr/11 11:27,15/Nov/11 00:53
Bug,HDFS-1813,12503574,Hdfs Federation: Authentication using BlockToken in RPC to datanode fails.," Several issues are causing this problem
1. The last LocatedBlock returned by getBlockLocations doesn't have BlockToken.
2. The blockPoolTokenSecretManager is not initialized before created rpc server in datanode.
3. The getReplicaVisibleLength API in datanode expects WRITE permission in the block token, but the block tokens are generated with read permission only in getBlockLocations at namenode.",jnp,jnp,Major,Resolved,Fixed,07/Apr/11 01:28,08/Jul/14 20:26
Bug,HDFS-1818,12503694,TestHDFSCLI is failing on trunk,"The commit of HADOOP-7202 changed the output of a few FsShell commands. Since HDFS tests rely on the precise format of this output, TestHDFSCLI is now failing.",atm,atm,Major,Closed,Fixed,08/Apr/11 03:51,15/Nov/11 00:53
Bug,HDFS-1820,12503725,FTPFileSystem attempts to close the outputstream even when it is not initialised,"FTPFileSystem's create method attempts to close the outputstream even when it is not initialized causing a null pointer exception. In our case the apache commons FTPClient was not able to create the destination file due to permissions issue. The FtpClient promptly reported a 553 : Permissions issue but it was overlooked in FTPFileSystem create method. 

The following code fails

if (!FTPReply.isPositivePreliminary(client.getReplyCode())) {
      // The ftpClient is an inconsistent state. Must close the stream
      // which in turn will logout and disconnect from FTP server
      fos.close();
      throw new IOException(""Unable to create file: "" + file + "", Aborting"");
    }

as 'fos' is null. As a result the proper error message ""Unable to create file XXX"" is not reported but rather a null pointer exception.
",m.pryahin,sudhan65,Major,Resolved,Fixed,08/Apr/11 11:10,11/May/20 13:46
Bug,HDFS-1821,12503749,FileContext.createSymlink with kerberos enabled sets wrong owner,"TEST SETUP
Using attached sample hdfs java program that illustrates the issue.
Using cluster with Kerberos enabled on cluster

# Compile instructions
$ javac Symlink.java -cp `hadoop classpath`
$ jar -cfe Symlink.jar Symlink Symlink.class

# create test file for symlink to use
1. hadoop fs -touchz /user/username/filetest

# Create symlink using file context
2. hadoop jar Symlink.jar ln /user/username/filetest /user/username/testsymlink

# Verify owner of test file
3. hadoop jar Symlink.jar ls /user/username/
-rw-r--r-- username hdfs /user/jeagles/filetest
-rwxrwxrwx username@XX.XXXX.XXXXX.XXX hdfs /user/username/testsymlink

RESULTS
1. Owner shows 'username@XX.XXXX.XXXXX.XXX' for symlink, expecting 'username'.
2. Symlink is corrupted and can't removed, since it was created with an invalid user


------------------------
Sample program to create Symlink

FileContext fc = FileContext.getFileContext(getConf());
fc.createSymlink(target, symlink, false);

---------------------------------------",johnvijoe,johnvijoe,Major,Closed,Fixed,08/Apr/11 16:20,15/Nov/11 00:52
Bug,HDFS-1822,12503773,Editlog opcodes overlap between 20 security and later releases,"Same opcode are used for different operations between 0.20.security, 0.22 and 0.23. This results in failure to load editlogs on later release, especially during upgrades.",sureshms,sureshms,Blocker,Closed,Fixed,08/Apr/11 19:57,25/Aug/11 20:20
Bug,HDFS-1823,12503785,start-dfs.sh script fails if HADOOP_HOME is not set,HDFS portion of HADOOP-6953,tomwhite,tomwhite,Blocker,Closed,Fixed,08/Apr/11 21:24,02/May/13 02:29
Bug,HDFS-1824,12503808,delay instantiation of file system object until it is needed (linked to HADOOP-7207),also re-factor the code little bit to avoid checking for instance of DFS in multiple places. ,boryas,boryas,Major,Resolved,Fixed,09/Apr/11 01:13,02/May/13 02:29
Bug,HDFS-1827,12504010,"TestBlockReplacement waits forever, errs without giving information","In method checkBlocks(), TestBlockReplacement waits forever on a condition. Failures result in Hudson/Jenkins ""Timeout occurred"" error message with no information about where or why. Need to replace with TimeoutException that throws a stack trace and useful info about the failure mode.

Also investigate possible cause of failure.",mattf,mattf,Major,Closed,Fixed,12/Apr/11 03:54,15/Nov/11 00:52
Bug,HDFS-1829,12504013,"TestNodeCount waits forever, errs without giving information","In three locations in the code, TestNodeCount waits forever on a condition. Failures result in Hudson/Jenkins ""Timeout occurred"" error message with no information about where or why. Need to replace with TimeoutExceptions that throw a stack trace and useful info about the failure mode.

Also investigate possible cause of failure.",mattf,mattf,Major,Closed,Fixed,12/Apr/11 04:47,15/Nov/11 00:53
Bug,HDFS-1835,12504226,DataNode.setNewStorageID pulls entropy from /dev/random,"DataNode.setNewStorageID uses SecureRandom.getInstance(""SHA1PRNG"") which always pulls fresh entropy.

It wouldn't be so bad if this were only the 120 bits needed by sha1, but the default impl of SecureRandom actually uses a BufferedInputStream around /dev/random and pulls 1024 bits of entropy for this one call.

If you are on a system without much entropy coming in, this call can block and block others.

Can we just change this to use ""new SecureRandom().nextInt(Integer.MAX_VALUE)"" instead?",johnyoh,johnyoh,Major,Closed,Fixed,14/Apr/11 04:38,15/Nov/11 00:52
Bug,HDFS-1836,12504237,Thousand of CLOSE_WAIT socket ,"$ /usr/sbin/lsof -i TCP:50010 | grep -c CLOSE_WAIT
4471

It is better if everything runs normal. 
However, from time to time there are some ""DataStreamer Exception: java.net.SocketTimeoutException"" and ""DFSClient.processDatanodeError(2507) | Error Recovery for"" can be found from log file and the number of CLOSE_WAIT socket just keep increasing

The CLOSE_WAIT handles may remain for hours and days; then ""Too many open file"" some day.
",bharathm,hkdennis2k,Major,Closed,Fixed,14/Apr/11 07:40,19/Oct/11 00:25
Bug,HDFS-1845,12504580,symlink comes up as directory after namenode restart,"When a symlink is first created, it get added to EditLogs. When namenode is restarted, it reads from this editlog and represents a symlink correctly and saves this information to its image. If the namenode is restarted again, it reads its from this FSImage, but thinks that a symlink is a directory. This is because it uses ""Block[] blocks"" to determine if an INode is a directory, a file, or symlink. Since both a directory and a symlink has blocks as null, it thinks that a symlink is a directory.",johnvijoe,johnvijoe,Major,Closed,Fixed,18/Apr/11 18:40,15/Nov/11 00:52
Bug,HDFS-1850,12504741,DN should transmit absolute failed volume count rather than increments to the NN,"The API added in HDFS-811 for the DN to report volume failures to the NN is ""inc(DN)"". However the given sequence of events will result in the NN forgetting about reported failed volumes:

# DN loses a volume and reports it
# NN restarts
# DN re-registers to the new NN

A more robust interface would be to have the DN report the total number of volume failures to the NN each heart beat (the same way other volume state is transmitted).",eli,eli,Major,Closed,Fixed,20/Apr/11 01:37,12/Dec/11 06:19
Bug,HDFS-1860,12504998,when renewing/canceling DelegationToken over http we need to pass exception information back to the caller.,"Current implementation is not using XML for that, so we will pass it as a part of response message.
",boryas,boryas,Major,Resolved,Fixed,22/Apr/11 18:32,05/Jun/12 10:25
Bug,HDFS-1869,12505820,mkdirs should use the supplied permission for all of the created directories,Mkdirs only uses the supplied FsPermission for the last directory of the path.  Paths 0..N-1 will all inherit the parent dir's permissions -even if- inheritPermission is false.  This is a regression from somewhere around 0.20.9 and does not follow posix semantics.,daryn,daryn,Major,Closed,Fixed,29/Apr/11 20:36,16/Mar/15 18:07
Bug,HDFS-1871,12505824,Tests using MiniDFSCluster fail to compile due to HDFS-1052 changes,MiniDFSCluster public method signature changes from HDFS-1052 breaks build of mapreduce tests.,sureshms,sureshms,Major,Closed,Fixed,29/Apr/11 21:53,15/Nov/11 00:52
Bug,HDFS-1875,12505977,MiniDFSCluster hard-codes dfs.datanode.address to localhost,"When creating RPC addresses that represent the communication sockets for each simulated DataNode, the MiniDFSCluster class hard-codes the address of the dfs.datanode.address port to be ""127.0.0.1:0""

The DataNodeCluster test tool uses the MiniDFSCluster class to create a selected number of simulated datanodes on a single host. In the DataNodeCluster setup, the NameNode is not simulated but is started as a separate daemon.

The problem is that if the write requrests into the simulated datanodes are originated on a host that is not the same host running the simulated datanodes, the connections are refused. This is because the RPC sockets that are started by MiniDFSCluster are for ""localhost"" (127.0.0.1) and are not accessible from outside that same machine.

It is proposed that the MiniDFSCluster.setupDatanodeAddress() method be overloaded in order to accommodate an environment where the NameNode is on one host, the client is on another host, and the simulated DataNodes are on yet another host (or even multiple hosts simulating multiple DataNodes each).

The overloaded API would add a parameter that would be used as the basis for creating the RPS sockets. By default, it would remain 127.0.0.1
",epayne,epayne,Major,Closed,Fixed,02/May/11 21:01,15/Nov/11 00:53
Bug,HDFS-1876,12505993,One MiniDFSCluster ignores numDataNodes parameter,"After the federation merge, one of the MiniDFSCluster constructors ignores its numDataNodes argument, thus causing TestFileInputFormat to fail (MAPREDUCE-2466)",tlipcon,tlipcon,Blocker,Closed,Fixed,03/May/11 00:02,15/Nov/11 00:53
Bug,HDFS-1878,12506012,TestHDFSServerPorts unit test failure - race condition in FSNamesystem.close() causes NullPointerException without serious consequence,"In 20.204, TestHDFSServerPorts was observed to intermittently throw a NullPointerException.  This only happens when FSNamesystem.close() is called, which means system termination for the Namenode, so this is not a serious bug for .204.  TestHDFSServerPorts is more likely than normal execution to stimulate the race, because it runs two Namenodes in the same JVM, causing more interleaving and more potential to see a race condition.

The race is in FSNamesystem.close(), line 566, we have:
      if (replthread != null) replthread.interrupt();
      if (replmon != null) replmon = null;

Since the interrupted replthread is not waited on, there is a potential race condition with replmon being nulled before replthread is dead, but replthread references replmon in computeDatanodeWork() where the NullPointerException occurs.

The solution is either to wait on replthread or just don't null replmon.  The latter is preferred, since none of the sibling Namenode processing threads are waited on in close().

I'll attach a patch for .205.
",mattf,mattf,Minor,Closed,Fixed,03/May/11 02:17,02/Sep/11 22:16
Bug,HDFS-1881,12506104,Federation: after taking snapshot the current directory of datanode is empty,"After taking a snapshot in Federation (by starting up namenode with option -upgrade), it appears that the current directory of data node does not contain the block files.  We have also verified that upgrading from 20 to Federation does not have this problem.",tanping,tanping,Major,Closed,Fixed,03/May/11 21:48,15/Nov/11 00:53
Bug,HDFS-1888,12506194,MiniDFSCluster#corruptBlockOnDatanodes() access must be public for MapReduce contrib raid,HDFS-1052 during code merge the method was made package private. It needs to be public for access in MapReduce contrib raid.,sureshms,sureshms,Major,Closed,Fixed,04/May/11 16:58,15/Nov/11 00:53
Bug,HDFS-1889,12506198,incorrect path in start/stop dfs script,"HADOOP_HOME in start-dfs.sh and stop-dfs.sh should be changed to HADOOP_HDFS_HOME because hdfs script is in the hdfs
directory and not common directory",johnvijoe,johnvijoe,Major,Closed,Fixed,04/May/11 17:22,15/Nov/11 00:53
Bug,HDFS-1891,12506222,TestBackupNode fails intermittently,TestBackupNode fails due to unexpected ipv6 address format.,sureshms,sureshms,Major,Closed,Fixed,04/May/11 21:05,12/Dec/11 06:19
Bug,HDFS-1897,12506349,Documention refers to removed option dfs.network.script,"The HDFS user guide tells users to use dfs.network.script for rack awareness. In fact, this option has been removed and using it will trigger a fatal error on DataNode startup. Documentation should describe the current rack awareness configuration system.",whangsf,asrabkin,Minor,Resolved,Fixed,05/May/11 23:27,21/May/11 12:38
Bug,HDFS-1898,12506352,Tests failing on trunk due to use of NameNode.format,"After federation merge, NameNode.format no longer works on trunk. Unclear why these tests aren't failing on Hudson, but some seem to fail for me in my checkout (including TestEditLogFileOutputStream for example)",tlipcon,tlipcon,Critical,Closed,Fixed,06/May/11 00:25,15/Nov/11 00:53
Bug,HDFS-1905,12506659,Improve the usability of namenode -format ,"While setting up 0.23 version based cluster, i ran into this issue. When i issue a format namenode command, which got changed in 23, it should let the user know to how to use this command in case where complete options were not specified.

./hdfs namenode -format

I get the following error msg, still its not clear what and how user should use this command.

11/05/09 15:36:25 ERROR namenode.NameNode: java.lang.IllegalArgumentException: Format must be provided with clusterid
	at org.apache.hadoop.hdfs.server.namenode.NameNode.format(NameNode.java:1483)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1623)
	at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1689)
 
The usability of this command can be improved.
",bharathm,bharathm,Minor,Closed,Fixed,09/May/11 22:40,15/Nov/11 00:53
Bug,HDFS-1907,12506670,BlockMissingException upon concurrent read and write: reader was doing file position read while writer is doing write without hflush,"BlockMissingException is thrown under this test scenario:
Two different processes doing concurrent file r/w: one read and the other write on the same file
  - writer keep doing file write
  - reader doing position file read from beginning of the file to the visible end of file, repeatedly

The reader is basically doing:
  byteRead = in.read(currentPosition, buffer, 0, byteToReadThisRound);
where CurrentPostion=0, buffer is a byte array buffer, byteToReadThisRound = 1024*10000;

Usually it does not fail right away. I have to read, close file, re-open the same file a few times to create the problem. I'll pose a test program to repro this problem after I've cleaned up a bit my current test program.

",johnvijoe,cwchung,Major,Closed,Fixed,10/May/11 01:34,15/Nov/11 00:52
Bug,HDFS-1908,12506671,DataTransferTestUtil$CountdownDoosAction.run(..) throws NullPointerException,"In [build #426|https://builds.apache.org/hudson/job/PreCommit-HDFS-Build/426//testReport/org.apache.hadoop.hdfs.server.datanode/TestFiDataTransferProtocol2/pipeline_Fi_17/],
{noformat}
2011-04-28 07:20:10,559 ERROR datanode.DataNode (DataXceiver.java:run(133)) - DatanodeRegistration(127.0.0.1:50589,
 storageID=DS-499221794-127.0.1.1-50589-1303975177998, infoPort=58607, ipcPort=52786):DataXceiver
java.lang.NullPointerException
	at org.apache.hadoop.fi.DataTransferTestUtil$CountdownDoosAction.run(DataTransferTestUtil.java:288)
	at org.apache.hadoop.fi.DataTransferTestUtil$DoosAction.run(DataTransferTestUtil.java:1)
...
{noformat}
",szetszwo,szetszwo,Minor,Closed,Fixed,10/May/11 02:18,15/Nov/11 00:52
Bug,HDFS-1909,12506676,TestHDFSCLI fails due to typo in expected output ,"""apended"" is misspelled in testHDFSConf.xml and causes the test to fail on the 0.22 branch.",tomwhite,tomwhite,Major,Closed,Fixed,10/May/11 03:40,12/Dec/11 06:18
Bug,HDFS-1910,12506680,when dfs.name.dir and dfs.name.edits.dir are same fsimage will be saved twice every time,"when image and edits dir are configured same, the fsimage flushing from memory to disk will be done twice whenever saveNamespace is done. this may impact the performance of backupnode/snn where it does a saveNamespace during every checkpointing time.",,slukog,Minor,Closed,Fixed,10/May/11 05:10,17/Oct/12 18:27
Bug,HDFS-1914,12506777,Federation: namenode storage directory must be configurable specific to a namenode,"Federation allows common configuration where namenode specific configuration are in the same configuration suffixed by nameservice ID. When namenodes use an external storage directory (NFS), in order to make namenodes use different directories on the external server, the storage directory configuration must also allow specific configuration, using nameservice ID.",sureshms,sureshms,Major,Closed,Fixed,10/May/11 22:46,15/Nov/11 00:53
Bug,HDFS-1917,12506888,Clean up duplication of dependent jar files,"For trunk, the build and deployment tree look like this:

hadoop-common-0.2x.y
hadoop-hdfs-0.2x.y
hadoop-mapred-0.2x.y

Technically, hdfs's the third party dependent jar files should be fetch from hadoop-common.  However, it is currently fetching from hadoop-hdfs/lib only.  It would be nice to eliminate the need to repeat duplicated jar files at build time.

There are two options to manage this dependency list, continue to enhance ant build structure to fetch and filter jar file dependencies using ivy.  On the other hand, it would be a good opportunity to convert the build structure to maven, and use maven to manage the provided jar files.
",eyang,eyang,Major,Closed,Fixed,11/May/11 18:13,02/May/13 02:29
Bug,HDFS-1920,12506920,libhdfs does not build for ARM processors,"$ ant compile -Dcompile.native=true -Dcompile.c++=1 -Dlibhdfs=1 -Dfusedfs=1
...
create-libhdfs-configure:
...
     [exec] configure: error: Unsupported CPU architecture ""armv7l""

Once the CPU arch check is fixed in src/c++/libhdfs/m4/apsupport.m4, then next issue is -m32:

$ ant compile -Dcompile.native=true -Dcompile.c++=1 -Dlibhdfs=1 -Dfusedfs=1
...

compile-c++-libhdfs:
     [exec] /bin/bash ./libtool --tag=CC   --mode=compile gcc -DPACKAGE_NAME=\""libhdfs\"" -DPACKAGE_TARNAME=\""libhdfs\"" -DPACKAGE_VERSION=\""0.1.0\"" -DPACKAGE_STRING=\""libhdfs\ 0.1.0\"" -DPACKAGE_BUGREPORT=\""omalley@apache.org\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""libhdfs\"" -DVERSION=\""0.1.0\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" -Dsize_t=unsigned\ int -Dconst=/\*\*/ -Dvolatile=/\*\*/ -I. -I/home/trobinson/dev/hadoop-hdfs/src/c++/libhdfs     -g -O2 -DOS_LINUX -DDSO_DLFCN -DCPU=\""arm\"" -m32 -I/usr/lib/jvm/java-6-openjdk/include -I/usr/lib/jvm/java-6-openjdk/include/arm -Wall -Wstrict-prototypes -MT hdfs.lo -MD -MP -MF .deps/hdfs.Tpo -c -o hdfs.lo /home/trobinson/dev/hadoop-hdfs/src/c++/libhdfs/hdfs.c
     [exec] make: Warning: File `.deps/hdfs_write.Po' has modification time 2.1 s in the future
     [exec] libtool: compile:  gcc -DPACKAGE_NAME=\""libhdfs\"" -DPACKAGE_TARNAME=\""libhdfs\"" -DPACKAGE_VERSION=\""0.1.0\"" ""-DPACKAGE_STRING=\""libhdfs 0.1.0\"""" -DPACKAGE_BUGREPORT=\""omalley@apache.org\"" -DPACKAGE_URL=\""\"" -DPACKAGE=\""libhdfs\"" -DVERSION=\""0.1.0\"" -DSTDC_HEADERS=1 -DHAVE_SYS_TYPES_H=1 -DHAVE_SYS_STAT_H=1 -DHAVE_STDLIB_H=1 -DHAVE_STRING_H=1 -DHAVE_MEMORY_H=1 -DHAVE_STRINGS_H=1 -DHAVE_INTTYPES_H=1 -DHAVE_STDINT_H=1 -DHAVE_UNISTD_H=1 -DHAVE_DLFCN_H=1 -DLT_OBJDIR=\"".libs/\"" ""-Dsize_t=unsigned int"" ""-Dconst=/**/"" ""-Dvolatile=/**/"" -I. -I/home/trobinson/dev/hadoop-hdfs/src/c++/libhdfs -g -O2 -DOS_LINUX -DDSO_DLFCN -DCPU=\""arm\"" -m32 -I/usr/lib/jvm/java-6-openjdk/include -I/usr/lib/jvm/java-6-openjdk/include/arm -Wall -Wstrict-prototypes -MT hdfs.lo -MD -MP -MF .deps/hdfs.Tpo -c /home/trobinson/dev/hadoop-hdfs/src/c++/libhdfs/hdfs.c  -fPIC -DPIC -o .libs/hdfs.o
     [exec] cc1: error: unrecognized command line option ""-m32""
     [exec] make: *** [hdfs.lo] Error 1

Here, gcc does not support -m32 for the ARM target, so -m${JVM_ARCH} must be omitted from CFLAGS and LDFLAGS.
",scurrilous,scurrilous,Major,Closed,Fixed,11/May/11 22:28,28/Jun/12 00:33
Bug,HDFS-1921,12506927,Save namespace can cause NN to be unable to come up on restart,"I discovered this in the course of trying to implement a fix for HDFS-1505.

Per the comment for {{FSImage.saveNamespace(...)}}, the algorithm for save namespace proceeds in the following order:

# rename current to lastcheckpoint.tmp for all of them,
# save image and recreate edits for all of them,
# rename lastcheckpoint.tmp to previous.checkpoint.

The problem is that step 3 occurs regardless of whether or not an error occurs for all storage directories in step 2. Upon restart, the NN will see non-existent or corrupt {{current}} directories, and no {{lastcheckpoint.tmp}} directories, and so will conclude that the storage directories are not formatted.

This issue appears to be present on both 0.22 and 0.23. This should arguably be a 0.22/0.23 blocker.",mattf,atm,Blocker,Closed,Fixed,11/May/11 23:18,15/Nov/11 00:52
Bug,HDFS-1925,12506941,SafeModeInfo should use DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_DEFAULT instead of 0.95,"{{SafeMode()}} constructor has 0.95f default threshold hard-coded. This should be replaced by the constant {{DFS_NAMENODE_SAFEMODE_THRESHOLD_PCT_DEFAULT}}, which is correctly set to 0.999f.",fwiffo,shv,Major,Closed,Fixed,12/May/11 06:39,12/Dec/11 06:19
Bug,HDFS-1927,12506994,"audit logs could ignore certain xsactions and also could contain ""ip=null""","Namenode audit logs could be ignoring certain transactions that are successfully completed. This is because it check if the RemoteIP is null to decide if a transaction is remote or not. In certain cases, RemoteIP could return null but the xsaction could still be ""remote"". An example is a case where a client gets killed while in the middle of the transaction. ",johnvijoe,johnvijoe,Major,Closed,Fixed,12/May/11 14:29,15/Nov/11 00:52
Bug,HDFS-1929,12507056,TestEditLogFileOutputStream fails if running on same host as NN,"This test instantiates NameNode directly rather than using MiniDFSCluster, so it tries to claim the default port 50070 rather than using an ephemeral one. This makes the test fail if you are running a NN on the same machine where you're running the test.",atm,tlipcon,Major,Resolved,Fixed,12/May/11 23:27,21/May/11 12:38
Bug,HDFS-1932,12507062,Ensure that HDFS configuration deprecations are set up in every spot that HDFS configurations are loaded.,"Currently we have code blocks like the following in lots of places:
{code}
  static{
    Configuration.addDefaultResource(""hdfs-default.xml"");
    Configuration.addDefaultResource(""hdfs-site.xml"");
  }
{code}
This is dangerous since, if we don't remember to also classload HdfsConfiguration, the config key deprecations won't work. We should add a method like HdfsConfiguration.init() which would load the default resources as well as ensure that deprecation gets initialized properly.",jolly,tlipcon,Critical,Closed,Fixed,13/May/11 00:09,12/Dec/11 06:19
Bug,HDFS-1934,12507073,Fix NullPointerException when File.listFiles() API returns null,"While testing Disk Fail Inplace, We encountered the NPE from this part of the code. 

File[] files = dir.listFiles();
for (File f : files) {
...
}

This is kinda of an API issue. When a disk is bad (or name is not a directory), this API (listFiles, list) return null rather than throwing an exception. This 'for loop' throws a NPE exception. And same applies to dir.list() API.

Fix all the places where null condition was not checked.
 ",bharathm,bharathm,Major,Closed,Fixed,13/May/11 01:19,02/May/13 02:29
Bug,HDFS-1936,12507148,Updating the layout version from HDFS-1822 causes upgrade problems.,"In HDFS-1822 and HDFS-1842, the layout versions for 203, 204, 22 and trunk were changed. Some of the namenode logic that depends on layout version is broken because of this. Read the comment for more description.",sureshms,sureshms,Blocker,Closed,Fixed,13/May/11 16:51,15/Nov/11 00:53
Bug,HDFS-1938,12507193, Reference ivy-hdfs.classpath not found.,"{noformat}
$ant test-system
...
BUILD FAILED
/export/crawlspace/tsz/hdfs/h1/src/test/aop/build/aop.xml:129: The following error occurred while executing this line:
/export/crawlspace/tsz/hdfs/h1/src/test/aop/build/aop.xml:183: The following error occurred while executing this line:
/export/crawlspace/tsz/hdfs/h1/src/test/aop/build/aop.xml:193: The following error occurred while executing this line:
/export/crawlspace/tsz/hdfs/h1/build.xml:449: Reference ivy-hdfs.classpath not found.
{noformat}",eyang,szetszwo,Minor,Closed,Fixed,14/May/11 00:35,15/Nov/11 00:52
Bug,HDFS-1942,12507326,If all Block Pool service threads exit then datanode should exit.,"Currently, if all block pool service threads exit, Datanode continue to run. This should be fixed.",bharathm,bharathm,Major,Resolved,Fixed,16/May/11 06:06,15/Jun/11 23:03
Bug,HDFS-1943,12507331,fail to start datanode while start-dfs.sh is executed by root user,"When start-dfs.sh is run by root user, we got the following error message:
# start-dfs.sh
Starting namenodes on [localhost ]
localhost: namenode running as process 2556. Stop it first.
localhost: starting datanode, logging to /usr/hadoop/hadoop-common-0.23.0-SNAPSHOT/bin/../logs/hadoop-root-datanode-cspf01.out
localhost: Unrecognized option: -jvm
localhost: Could not create the Java virtual machine.

The -jvm options should be passed to jsvc when we starting a secure
datanode, but it still passed to java when start-dfs.sh is run by root
while secure datanode is disabled. This is a bug of bin/hdfs.
",mattf,weiyj,Blocker,Closed,Fixed,16/May/11 06:51,07/Dec/12 01:45
Bug,HDFS-1952,12507518,FSEditLog.open() appears to succeed even if all EDITS directories fail,"FSEditLog.open() appears to ""succeed"" even if all of the individual directories failed to allow creation of an EditLogOutputStream.  The problem and solution are essentially similar to that of HDFS-1505.",azuriel,mattf,Major,Closed,Fixed,17/May/11 18:23,15/Nov/11 00:53
Bug,HDFS-1953,12507540,Change name node mxbean name in cluster web console,name node mxbean name is changed after the new metrics framework is checked.  Need to change this in ClusterJspHelper.java in order for cluster web console to work again.,tanping,tanping,Minor,Closed,Fixed,17/May/11 23:06,15/Nov/11 00:53
Bug,HDFS-1955,12507671,FSImage.doUpgrade() was made too fault-tolerant by HDFS-1826,"Prior to HDFS-1826, doUpgrade() would fail if any of the storage directories failed to successfully write the new fsimage or edits files.
Now it appears to ""succeed"" even if some or all of the individual directories fail.

There is some discussion about whether doUpgrade() should have some fault tolerance, but for now make it fail on any single storage directory failure, as before.",mattf,mattf,Major,Closed,Fixed,18/May/11 21:31,15/Nov/11 00:53
Bug,HDFS-1964,12507816,Incorrect HTML unescaping in DatanodeJspHelper.java,"HDFS-1575 introduced some HTML unescaping of parameters so that viewing a file would work for paths containing HTML-escaped characters, but in two of the places did the unescaping either too early or too late.",atm,atm,Major,Closed,Fixed,19/May/11 21:25,02/May/13 02:29
Bug,HDFS-1965,12507820,IPCs done using block token-based tickets can't reuse connections,"This is the reason that TestFileConcurrentReaders has been failing a lot. Reproducing a comment from HDFS-1057:

The test has a thread which continually re-opens the file which is being written to. Since the file's in the middle of being written, it makes an RPC to the DataNode in order to determine the visible length of the file. This RPC is authenticated using the block token which came back in the LocatedBlocks object as the security ticket.

When this RPC hits the IPC layer, it looks at its existing connections and sees none that can be re-used, since the block token differs between the two requesters. Hence, it reconnects, and we end up with hundreds or thousands of IPC connections to the datanode.
",tlipcon,tlipcon,Critical,Closed,Fixed,19/May/11 22:13,27/Oct/17 23:15
Bug,HDFS-1969,12507843,Running rollback on new-version namenode destroys namespace,"The following sequence leaves the namespace in an inconsistent/broken state:
- format NN using 0.20 (or any prior release, probably)
- run hdfs namenode -upgrade on 0.22. ^C the NN once it comes up.
- run hdfs namenode -rollback on 0.22  (this should fail but doesn't!)

This leaves the name directory in a state such that the version file claims it's an 0.20 namespace, but the fsimage is in 0.22 format. It then crashes when trying to start up.",tlipcon,tlipcon,Blocker,Closed,Fixed,20/May/11 04:33,10/May/12 05:00
Bug,HDFS-1978,12508015,All but first option in LIBHDFS_OPTS is ignored,"In getJNIEnv, we go though LIBHDFS_OPTS with strok and count the number of args. Then create an array of options based on that information. But when we actually setup the options we only the first arg. I believe the fix is pasted inline.

{noformat}
Index: src/c++/libhdfs/hdfsJniHelper.c
===================================================================
--- src/c++/libhdfs/hdfsJniHelper.c	(revision 1124544)
+++ src/c++/libhdfs/hdfsJniHelper.c	(working copy)
@@ -442,6 +442,7 @@
             int argNum = 1;
             for (;argNum < noArgs ; argNum++) {
                 options[argNum].optionString = result; //optHadoopArg;
+                result = strtok( NULL, jvmArgDelims);
             }
         }
{noformat}",eli,brocknoland,Major,Resolved,Fixed,22/May/11 20:47,06/Jun/11 11:22
Bug,HDFS-1981,12508046,When namenode goes down while checkpointing and if is started again subsequent Checkpointing is always failing,"This scenario is applicable in NN and BNN case.

When the namenode goes down after creating the edits.new, on subsequent restart the divertFileStreams will not happen to edits.new as the edits.new file is already present and the size is zero.

so on trying to saveCheckPoint an exception occurs 
2011-05-23 16:38:57,476 WARN org.mortbay.log: /getimage: java.io.IOException: GetImage failed. java.io.IOException: Namenode has an edit log with timestamp of 2011-05-23 16:38:56 but new checkpoint was created using editlog  with timestamp 2011-05-23 16:37:30. Checkpoint Aborted.

This is a bug or is that the behaviour.





",umamaheswararao,ram_krish,Blocker,Closed,Fixed,23/May/11 11:13,12/Dec/11 06:19
Bug,HDFS-1986,12508116,Add an option for user to return http or https ports regardless of security is on/off in DFSUtil.getInfoServer(),"Currently DFSUtil.getInfoServer gets http port with security off and httpS port with security on.  However, we want to return http port regardless of security on/off for Cluster UI to use.  Add in a third Boolean parameter for user to decide whether to check security or not.",tanping,tanping,Minor,Closed,Fixed,23/May/11 22:51,15/Nov/11 00:53
Bug,HDFS-1990,12508178,Resource leaks in HDFS,Possible resource leakage in HDFS.,umamaheswararao,ram_krish,Minor,Closed,Fixed,24/May/11 12:09,15/Nov/11 00:53
Bug,HDFS-1998,12508373,make refresh-namodenodes.sh refreshing all namenodes,"refresh-namenodes.sh is used to refresh name nodes in the cluster to check for updates of include/exclude list.  It is used when decommissioning or adding a data node.  Currently it only refreshes the name node who serves the defaultFs, if there is defaultFs defined.  Fix it by refreshing all the name nodes in the cluster.",tanping,tanping,Minor,Closed,Fixed,25/May/11 23:08,15/Nov/11 00:52
Bug,HDFS-1999,12508374,Tests use deprecated configs,A few of the HDFS tests (not intended to test deprecation) use config keys which are deprecated.,atm,atm,Major,Closed,Fixed,25/May/11 23:25,15/Nov/11 00:53
Bug,HDFS-2000,12508375,Missing deprecation for io.bytes.per.checksum,"Hadoop long ago deprecated the configuration ""io.bytes.per.checksum"" in favor of ""dfs.bytes-per-checksum"", but when the programmatic deprecation support was added, we didn't add an entry for this pair.

This is causing some tests to fail on branch-0.22 since the inclusion of HADOOP-7287, since some tests which were inadvertently using default config values are now having their settings actually picked up.",atm,atm,Major,Resolved,Fixed,25/May/11 23:25,06/Jun/11 11:22
Bug,HDFS-2002,12508378,Incorrect computation of needed blocks in getTurnOffTip(),{{SafeModeInfo.getTurnOffTip()}} under-reports the number of blocks needed to reach the safemode threshold.,zero45,shv,Major,Closed,Fixed,26/May/11 00:12,25/May/16 21:33
Bug,HDFS-2011,12508729,Removal and restoration of storage directories on checkpointing failure doesn't work properly,Removal and restoration of storage directories on checkpointing failure doesn't work properly. Sometimes it throws a NullPointerException and sometimes it doesn't take off a failed storage directory,raviprak,raviprak,Major,Closed,Fixed,30/May/11 16:30,15/Nov/11 00:52
Bug,HDFS-2012,12508730,Recurring failure of TestBalancer due to incorrect treatment of nodes whose utilization equals avgUtilization.,This has been failing on Hudson for the last two builds and fails on my local box as well.,umamaheswararao,atm,Blocker,Closed,Fixed,30/May/11 16:55,16/Mar/15 17:43
Bug,HDFS-2014,12508751,bin/hdfs no longer works from a source checkout,"bin/hdfs now appears to depend on ../libexec, which doesn't exist inside of a source checkout:

todd@todd-w510:~/git/hadoop-hdfs$ ./bin/hdfs namenode
./bin/hdfs: line 22: /home/todd/git/hadoop-hdfs/bin/../libexec/hdfs-config.sh: No such file or directory
./bin/hdfs: line 138: cygpath: command not found
./bin/hdfs: line 161: exec: : not found
",eyang,tlipcon,Critical,Closed,Fixed,30/May/11 20:19,29/Nov/12 09:11
Bug,HDFS-2019,12508862,Fix all the places where Java method File.list is used with FileUtil.list API,This new method FileUtil.list will throw an exception when disk is bad rather than returning null. ,bharathm,bharathm,Minor,Closed,Fixed,31/May/11 19:26,02/May/13 02:29
Bug,HDFS-2020,12508864,TestDFSUpgradeFromImage fails,"Datanode has a singleton datanodeObject. When running MiniDFSCluster with multiple datanodes, the singleton can point to only one of the datanodes. TestDFSUpgradeFromImage fails related to initialization of this singleton.",sureshms,sureshms,Major,Closed,Fixed,31/May/11 20:07,15/Nov/11 00:53
Bug,HDFS-2021,12508872,TestWriteRead failed with inconsistent visible length of a file ,"The junit test failed when iterates a number of times with larger chunk size on Linux. Once a while, the visible number of bytes seen by a reader is slightly less than what was supposed to be. 

When run with the following parameter, it failed more often on Linux ( as reported by John George) than my Mac:
  private static final int WR_NTIMES = 300;
  private static final int WR_CHUNK_SIZE = 10000;

Adding more debugging output to the source, this is a sample of the output:
Caused by: java.io.IOException: readData mismatch in byte read: expected=2770000 ; got 2765312
        at org.apache.hadoop.hdfs.TestWriteRead.readData(TestWriteRead.java:141)
",johnvijoe,cwchung,Major,Closed,Fixed,31/May/11 21:16,15/Nov/11 00:52
Bug,HDFS-2022,12509007,ant binary should build libhdfs,"Post HDFS-1963 ant binary fails w/ the following. The bin-package is trying to copy from the c++ lib dir which doesn't exist yet. The binary target should check for the existence of this dir or would also be reasonable to depend on the compile-c++-libhdfs (since this is the binary target).

{noformat}
/home/eli/src/hdfs4/build.xml:1115: /home/eli/src/hdfs4/build/c++/Linux-amd64-64/lib not found.
{noformat}
",eyang,eli,Major,Closed,Fixed,01/Jun/11 17:55,15/Nov/11 00:53
Bug,HDFS-2023,12509031,Backport of NPE for File.list and File.listFiles,"Since we have multiple Jira's in trunk for common and hdfs, I am creating another jira for this issue. 

This patch addresses the following:

1. Provides FileUtil API for list and listFiles which throws IOException for null cases. 
2. Replaces most of the code where JDK file API with FileUtil API. ",bharathm,bharathm,Major,Closed,Fixed,01/Jun/11 23:08,02/May/13 02:29
Bug,HDFS-2025,12509075,Go Back to File View link is not working in tail.jsp,"While browsing the file system.
Click on any file link to go to the page where the file contents are displayed, then when we click on '*Tail this file*' link.
The control will go to the tail.jsp here when we
Click on '*Go Back to File View*' option.
HTTP Error page not found will come.

This is because the referrer URL is encoded and the encoded URL is itself being used in the '*Go Back to File View*' hyperlink which will be treated as a relative URL and thus the HTTP request will fail.",ashish singhi,sravankorumilli,Minor,Closed,Fixed,02/Jun/11 10:53,21/Apr/15 16:46
Bug,HDFS-2030,12509221,Fix the usability of namenode upgrade command,"Fixing the Namenode upgrade option along the same line as Namenode format option. 

If clusterid is not given then clusterid will be automatically generated for the upgrade but if clusterid is given then it will be honored.

 ",bharathm,bharathm,Minor,Closed,Fixed,03/Jun/11 18:35,15/Nov/11 00:52
Bug,HDFS-2034,12509257,length in getBlockRange becomes -ve when reading only from currently being written blk,"This came up during HDFS-1907. Posting an example that Todd posted in HDFS-1907 that brought out this issue.
{quote}
Here's an example sequence to describe what I mean:
1. open file, write one and a half blocks
2. call hflush
3. another reader asks for the first byte of the second block
{quote}

In this case since offset is greater than the completed block length, the math in getBlockRange() of DFSInputStreamer.java will set ""length"" to negative.",johnvijoe,johnvijoe,Minor,Closed,Fixed,04/Jun/11 04:44,15/Nov/11 00:52
Bug,HDFS-2041,12509405,Some mtimes and atimes are lost when edit logs are replayed,"The refactoring in HDFS-2003 allowed findbugs to expose two potential bugs:
- the atime field logged with OP_MKDIR is unused
- the timestamp field logged with OP_CONCAT_DELETE is unused

The concat issue is definitely real. The atime for MKDIR might always be identical to mtime in that case, in which case it could be ignored.",tlipcon,tlipcon,Major,Closed,Fixed,06/Jun/11 23:16,15/Nov/11 00:53
Bug,HDFS-2043,12509491,TestHFlush failing intermittently,"I can't reproduce this failure reliably, but it seems like TestHFlush has been failing intermittently, with the frequency increasing of late.

Note the following two pre-commit test runs from different JIRAs where TestHFlush seems to have failed spuriously:

https://builds.apache.org/job/PreCommit-HDFS-Build/734//testReport/
https://builds.apache.org/job/PreCommit-HDFS-Build/680//testReport/",linyiqun,atm,Major,Closed,Fixed,07/Jun/11 18:00,30/Aug/16 01:43
Bug,HDFS-2051,12509614,TestBlockRecovery.testErrorReplicas is timing out,"The Hudson job was green until June 6th when TestBlockRecovery.testErrorReplicas started failing due to timeout, and has consistently failed since then. Recent change or perhaps Hudson is too loaded?

https://builds.apache.org/job/Hadoop-Hdfs-trunk-Commit
",,eli,Major,Resolved,Fixed,08/Jun/11 17:04,18/Sep/12 17:04
Bug,HDFS-2053,12509701,Bug in INodeDirectory#computeContentSummary warning,"*How to reproduce*

{code}
# create test directories
$ hadoop fs -mkdir /hdfs-1377/A
$ hadoop fs -mkdir /hdfs-1377/B
$ hadoop fs -mkdir /hdfs-1377/C

# ...add some test data (few kB or MB) to all three dirs...

# set space quota for subdir C only
$ hadoop dfsadmin -setSpaceQuota 1g /hdfs-1377/C

# the following two commands _on the parent dir_ trigger the warning
$ hadoop fs -dus /hdfs-1377
$ hadoop fs -count -q /hdfs-1377
{code}

Warning message in the namenode logs:

{code}
2011-06-09 09:42:39,817 WARN org.apache.hadoop.hdfs.server.namenode.NameNode: Inconsistent diskspace for directory C. Cached: 433872320 Computed: 438465355
{code}

Note that the commands are run on the _parent directory_ but the warning is shown for the _subdirectory_ with space quota.

*Background*
The bug was introduced by the HDFS-1377 patch, which is currently committed to at least branch-0.20, branch-0.20-security, branch-0.20-security-204, branch-0.20-security-205 and release-0.20.3-rc2.  In the patch, {{src/hdfs/org/apache/hadoop/hdfs/server/namenode/INodeDirectory.java}} was updated to trigger the warning above if the cached and computed diskspace values are not the same for a directory with quota.

The warning is written by {{computecontentSummary(long[] summary)}} in {{INodeDirectory}}. In the method an inode's children are recursively walked through while the {{summary}} parameter is passed and updated along the way.

{code}
  /** {@inheritDoc} */
  long[] computeContentSummary(long[] summary) {
    if (children != null) {
      for (INode child : children) {
        child.computeContentSummary(summary);
      }
    }
{code}

The condition that triggers the warning message compares the current node's cached diskspace (via {{node.diskspaceConsumed()}}) with the corresponding field in {{summary}}.

{code}
      if (-1 != node.getDsQuota() && space != summary[3]) {
        NameNode.LOG.warn(""Inconsistent diskspace for directory ""
          +getLocalName()+"". Cached: ""+space+"" Computed: ""+summary[3]);
{code}

However {{summary}} may already include diskspace information from other inodes at this point (i.e. from different subtrees than the subtree of the node for which the warning message is shown; in our example for the tree at {{/hdfs-1377}}, {{summary}} can already contain information from {{/hdfs-1377/A}} and {{/hdfs-1377/B}} when it is passed to inode {{/hdfs-1377/C}}).  Hence the cached value for {{C}} can incorrectly be different from the computed value.

*How to fix*

The supplied patch creates a fresh summary array for the subtree of the current node.  The walk through the children passes and updates this {{subtreeSummary}} array, and the condition is checked against {{subtreeSummary}} instead of the original {{summary}}.  The original {{summary}} is updated with the values of {{subtreeSummary}} before it returns.

*Unit Tests*

I have run ""ant test"" on my patched build without any errors*.  However the existing unit tests did not catch this issue for the original HDFS-1377 patch, so this might not mean anything. ;-)

That said I am unsure what the most appropriate way to unit test this issue would be.  A straight-forward approach would be to automate the steps in the _How to reproduce section_ above and check whether the NN logs an incorrect warning message.  But I'm not sure how this check could be implemented.  Feel free to provide some pointers if you have some ideas.

*Note about Fix Version/s*

The patch _should_ apply to all branches where the HDFS-1377 patch has committed to.  In my environment, the build was Hadoop 0.20.203.0 release with a (trivial) backport of HDFS-1377 (0.20.203.0 release does not ship with the HDFS-1377 fix).  I could apply the patch successfully to {{branch-0.20-security}}, {{branch-0.20-security-204}} and {{release-0.20.3-rc2}}, for instance.  Since I'm a bit confused regarding the upcoming 0.20.x release versions (0.20.x vs. 0.20.20x.y) I have been so bold and added 0.20.203.0 to the list of affected versions even though it is actually only affected when HDFS-1377 is applied to it...

Best,
Michael


*Well, I get one error for {{TestRumenJobTraces}} but first this seems to be completely unrelated and second I get the same test error when running the tests on the stock 0.20.203.0 release build.",miguno,miguno,Minor,Closed,Fixed,09/Jun/11 11:44,08/Mar/12 02:10
Bug,HDFS-2057,12509781,Wait time to terminate the threads causing unit tests to take longer time,"As a part of datanode process hang, this part of code was introduced in 0.20.204 to clean up all the waiting threads.

-      try {
-          readPool.awaitTermination(10, TimeUnit.SECONDS);
-      } catch (InterruptedException e) {
-       LOG.info(""Exception occured in doStop:"" + e.getMessage());
-      }
-      readPool.shutdownNow();

This was clearly meant for production, but all the unit tests uses minidfscluster and minimrcluster for shutdown which waits on this part of the code. Due to this, we saw increase in unit test run times. So removing this code. 
",bharathm,bharathm,Major,Closed,Fixed,10/Jun/11 00:09,02/Sep/11 22:16
Bug,HDFS-2061,12509852,two minor bugs in BlockManager block report processing,"In a recent review of HDFS-1295 patches (speedup for block report processing), found two very minor bugs in BlockManager, as documented in following comments.",mattf,mattf,Minor,Closed,Fixed,10/Jun/11 17:43,15/Nov/11 00:52
Bug,HDFS-2063,12509859,libhdfs test is broken,"Looks like the recent bin/script shuffling in HDFS-1963 broke the libhdfs test. This works on 22.

{noformat}
$ ant -Dlibhdfs=true compile test
...
     [exec] Hadoop common not found.
     [exec] /home/eli/src/hdfs3/src/c++/libhdfs/tests/test-libhdfs.sh: line 181: /home/eli/src/hdfs3/bin/hadoop-daemon.sh: No such file or directory
     [exec] /home/eli/src/hdfs3/src/c++/libhdfs/tests/test-libhdfs.sh: line 182: /home/eli/src/hdfs3/bin/hadoop-daemon.sh: No such file or directory
     [exec] Wait 30s for the datanode to start up...
{noformat}",eyang,eli,Major,Resolved,Fixed,10/Jun/11 18:20,17/Jun/11 12:53
Bug,HDFS-2065,12509887,Fix NPE in DFSClient.getFileChecksum,"The following code can throw NPE if callGetBlockLocations returns null.

If server returns null 

{code}
    List<LocatedBlock> locatedblocks
        = callGetBlockLocations(namenode, src, 0, Long.MAX_VALUE).getLocatedBlocks();
{code}

The right fix for this is server should throw right exception.

",umamaheswararao,bharathm,Major,Closed,Fixed,11/Jun/11 00:16,16/Mar/15 18:03
Bug,HDFS-2067,12509896,Bump DATA_TRANSFER_VERSION in trunk for protobufs,Forgot to bump DATA_TRANSFER_VERSION in HDFS-2058. We need to do this since the protobufs are incompatible with the old writables.,szetszwo,tlipcon,Major,Closed,Fixed,11/Jun/11 04:10,15/Nov/11 00:52
Bug,HDFS-2071,12510099,Use of isConnected() in DataXceiver is invalid,"The use of Socket.isConnected() in DataXceiver.run() is not valid. It returns false until the connection is made and then always returns true after that. It will never return false after the initial connection is successfully made. Socket.isClosed() or SocketChannel.isOpen() should be used instead, assuming someone is handling SocketException and does Socket.close() or SocketChannel.close(). It seems the op handlers in DataXceiver are diligently using IOUtils.closeStream(), which will invoke SocketChannel.close().

{code}
- } while (s.isConnected() && socketKeepaliveTimeout > 0);
+ } while (!s.isClosed() && socketKeepaliveTimeout > 0);
{code}

The effect of this bug is very minor, as the socket is read again right after. If the connection was closed, the readOp() will throw an EOFException, which is caught and dealt with properly.  The system still functions normally with probably only few microseconds of extra overhead in the premature connection closure cases.",kihwal,kihwal,Minor,Resolved,Fixed,13/Jun/11 14:36,08/Jul/11 23:38
Bug,HDFS-2082,12510736,SecondaryNameNode web interface doesn't show the right info,HADOOP-3741 introduced some useful info to the 2NN web UI. This broke when security was added.,atm,atm,Major,Closed,Fixed,17/Jun/11 22:57,15/Nov/11 00:52
Bug,HDFS-2086,12511006,"If the include hosts list contains host name, after restarting namenode, datanodes registrant is denied ","As the title describes the problem:  if the include host list contains host name, after restarting namenodes, the datanodes registrant is denied by namenodes.  This is because after namenode is restarted, the still alive data node will try to register itself with the namenode and it identifies itself with its *IP address*.  However, namenode only allows all the hosts in its hosts list to registrant and all of them are hostnames. So namenode would deny the datanode registration.
",tanping,tanping,Major,Closed,Fixed,20/Jun/11 21:53,15/Nov/11 00:53
Bug,HDFS-2092,12511044,Create a light inner conf class in DFSClient,"At present, DFSClient stores reference to configuration object. Since, these configuration objects are pretty big at times can blot the processes which has multiple DFSClient objects like in TaskTracker. This is an attempt to remove the reference of conf object in DFSClient. 

This patch creates a light inner conf class and copies the required keys from the Configuration object.",bharathm,bharathm,Major,Closed,Fixed,21/Jun/11 04:59,15/Nov/11 00:52
Bug,HDFS-2109,12511590,Store uMask as member variable to DFSClient.Conf,"As a part of removing reference to conf in DFSClient, I am proposing replacing FsPermission.getUMask(conf) everywhere in DFSClient class with
dfsClientConf.uMask by storing uMask as a member variable to DFSClient.Conf. ",bharathm,bharathm,Major,Closed,Fixed,27/Jun/11 02:01,15/Nov/11 00:52
Bug,HDFS-2114,12512233,re-commission of a decommissioned node does not delete excess replica,"If a decommissioned node is removed from the decommissioned list, namenode does not delete the excess replicas it created while the node was decommissioned.",johnvijoe,johnvijoe,Major,Closed,Fixed,29/Jun/11 16:06,15/Nov/11 00:53
Bug,HDFS-2117,12512293,DiskChecker#mkdirsWithExistsAndPermissionCheck may return true even when the dir is not created,"In branch-0.20-security as part of HADOOP-6566, DiskChecker#mkdirsWithExistsAndPermissionCheck will return true even if it wasn't able to create the directory, which means instead of throwing a DiskErrorException the code will proceed to getFileStatus and throw a FNF exception. Post HADOOP-7040, which modified makeInstance to catch not just DiskErrorExceptions but IOExceptions as well, this is not an issue since now the exception is caught either way. But for future modifications we should still modify this method to return false if it was unable to create the directory. This code is totally different in trunk, so not an issue there.",eli,eli,Minor,Closed,Fixed,30/Jun/11 08:46,19/Oct/11 00:26
Bug,HDFS-2120,12512384,"on reconnect, DN can connect to NN even with different source versions",DN or NN does not check for source versions in cases when NN goes away or has lost connection. The only check that is done by NN is for LAYOUT_VERSION and DN does not check for any version mismatch. ,johnvijoe,johnvijoe,Major,Resolved,Fixed,30/Jun/11 21:41,14/Jul/11 12:41
Bug,HDFS-2132,12513020,Potential resource leak in EditLogFileOutputStream.close,"{{EditLogFileOutputStream.close(...)}} sequentially closes a series of underlying resources. If any of the calls to {{close()}} throw before the last one, the later resources will never be closed.",atm,atm,Major,Closed,Fixed,06/Jul/11 18:15,15/Nov/11 00:53
Bug,HDFS-2152,12514113,TestWriteConfigurationToDFS causing the random failures,,umamaheswararao,umamaheswararao,Major,Resolved,Fixed,14/Jul/11 19:25,18/Jul/11 12:49
Bug,HDFS-2153,12514174,DFSClientAdapter should be put under test,{{DFSClientAdapter}} is a test utility but it is put in src/java.,szetszwo,szetszwo,Minor,Closed,Fixed,15/Jul/11 08:01,15/Nov/11 00:53
Bug,HDFS-2156,12514253,rpm should only require the same major version as common,The rpm for hdfs should only require the same major version (eg. 0.23) of common.,eyang,omalley,Major,Closed,Fixed,15/Jul/11 17:48,15/Nov/11 00:52
Bug,HDFS-2186,12515001,DN volume failures on startup are not counted,"Volume failures detected on startup are not currently counted/reported as such. Eg if you have configured 4 volumes, 2 tolerated failures, and you start a DN with two failed volumes it will come up and report (to the NN) no failed volumes. The DN will still be able to tolerate 2 additional volume failures (ie it's OK with no valid volumes remaining). The intent of the volume failure toleration config value is that if more than this # of volumes of the total set of configured volumes have failed the DN should shutdown, therefore volume failures detected on startup should count against this quota. ",eli,eli,Major,Closed,Fixed,22/Jul/11 07:24,15/Nov/11 00:52
Bug,HDFS-2189,12515136,"guava-r09 dependency missing from ""ivy/hadoop-hdfs-template.xml"" in HDFS.",Corrected version of: https://issues.apache.org/jira/browse/MAPREDUCE-2627,jrottinghuis,zero45,Blocker,Closed,Fixed,22/Jul/11 23:03,12/Dec/11 06:20
Bug,HDFS-2190,12515143,NN fails to start if it encounters an empty or malformed fstime file,"On startup, the NN reads the fstime file of all the configured dfs.name.dirs to determine which one to load. However, if any of the searched directories contain an empty or malformed fstime file, the NN will fail to start. The NN should be able to just proceed with starting and ignore the directory containing the bad fstime file.",atm,atm,Major,Closed,Fixed,23/Jul/11 00:11,19/Oct/11 00:26
Bug,HDFS-2211,12515574,Build does not pass along properties to contrib builds,"Subant call to compile contribs do not pass along parameters from parent build.
Properties such as hadoop-common.version, asfrepo, offline, etc. are not passed along.
Result is that build not connected to Internet fails, hdfs proxy refuses to build against own recently built common but rather downloads 0.22-SNAPSHOT from apache again.
",jrottinghuis,jrottinghuis,Blocker,Closed,Fixed,27/Jul/11 20:50,12/Dec/11 06:19
Bug,HDFS-2213,12515592,DataNode gets stuck while shutting down minicluster,I've seen a couple times where a unit test has timed out. jstacking shows the cluster is stuck trying to shut down one of the DataNode HTTP servers. The DataNodeBlockScanner thread also seems to be in a tight loop in its main loop.,,tlipcon,Major,Resolved,Fixed,28/Jul/11 00:15,10/Mar/15 01:51
Bug,HDFS-2214,12515717,Generated POMs hardcode dependency on hadoop-common version 0.22.0-SNAPSHOT,"The generated poms inject the version of hdfs itsel, but hardcode the version of hadoop-common they depend on.
When trying to build downstream projects for example mapreduce, then they will require hadoop-common-0.22.0-SNAPSHOT.jar.

When trying to do an offline build this will fail to resolve as another hadoop-common has been installed in the local maven repo.
Even during online build, it should compile against the hadoop-common that hdfs compiled against.

When versions mismatch one cannot do a coherent build. That is particularly problematic when making simultaneous change in hadoop-common and hadoop-hdfs.",jrottinghuis,jrottinghuis,Major,Resolved,Fixed,28/Jul/11 23:00,22/Aug/11 22:31
Bug,HDFS-2224,12517865,The libhdfs test is broken,"A somewhat recent change broke the libhdfs test (ant -Dcompile.c++=true -Dlibhdfs=true compile).

{noformat}
    [exec] tar (child): bin.tgz: Cannot open: No such file or directory
    [exec] tar (child): Error is not recoverable: exiting now
    [exec] tar: Child returned status 2
    [exec] tar: Error is not recoverable: exiting now
{noformat}",,eli,Major,Resolved,Fixed,03/Aug/11 22:54,09/Mar/15 22:11
Bug,HDFS-2229,12518021,Deadlock in NameNode,"Either I am doing something incredibly stupid, or something about my environment is completely weird, or may be it really is a valid bug. I am running a NameNode deadlock consistently with 0.23 HDFS. I could never start NN successfully.",szetszwo,vinodkv,Blocker,Closed,Fixed,05/Aug/11 11:18,15/Nov/11 00:53
Bug,HDFS-2232,12518096,TestHDFSCLI fails on 0.22 branch,"Several HDFS CLI tests fail on 0.22 branch. I can see 2 reasons:
# Not generic enough regular expression for host names and paths. Similar to MAPREDUCE-2304.
# Some command outputs have new-line in the end.
# And some seem to produce [much] more output than expected.",zero45,shv,Blocker,Closed,Fixed,06/Aug/11 01:11,16/Mar/15 17:44
Bug,HDFS-2235,12518170,Encode servlet paths,Hftp does not support paths which contain semicolons. The commented out test in HDFS-2234 illustrates this.,eli,eli,Major,Closed,Fixed,08/Aug/11 04:14,02/May/13 02:29
Bug,HDFS-2240,12518254,Possible deadlock between LeaseRenewer and its factory,Lock cycle detected by jcarder,szetszwo,tlipcon,Critical,Closed,Fixed,08/Aug/11 22:33,15/Nov/11 00:53
Bug,HDFS-2242,12518494,HDFS tests are failing due to storage directory issues,This test fails on my host on trunk.,tlipcon,eli,Major,Resolved,Fixed,10/Aug/11 18:25,24/Aug/11 00:47
Bug,HDFS-2245,12518527,BlockManager.chooseTarget(..) throws NPE,"{noformat}
2011-08-10 20:20:51,350 INFO org.apache.hadoop.ipc.Server: IPC Server handler 1 on 8020, call: addBlock(/user/had
oopqa/passwd.1108102020.<NN hostname>.txt, DFSClient_NONMAPREDUCE_1875954430_1, null, null), rpc
 version=1, client version=68, methodsFingerPrint=-1239577025 from <gateway>:38874, error:
java.io.IOException: java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.blockmanagement.BlockManager.chooseTarget(BlockManager.java:1225)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalBlock(FSNamesystem.java:1455)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.addBlock(NameNode.java:786)
        ...
{noformat}
",szetszwo,szetszwo,Major,Closed,Fixed,10/Aug/11 22:55,15/Nov/11 00:52
Bug,HDFS-2258,12518749,TestLeaseRecovery2 fails as lease hard limit is not reset to default,"TestLeaseRecovery2.testSoftLeaseRecovery() fails as lease hard limit remains set to 1 sec from the previous test case. If initial file creation in testSoftLeaseRecovery() takes longer than 1 sec, NN correctly reassigns the lease to itself and starts recovery. The test fails as the client cannot hflush() and close the file.",shv,shv,Major,Closed,Fixed,12/Aug/11 22:04,15/Nov/11 00:52
Bug,HDFS-2259,12518792,DN web-UI doesn't work with paths that contain html ,The 20-based DN web UI doesn't work with paths that contain html. The paths need to be unescaped when used to access the file and escaped when printed for navigation.,eli,eli,Minor,Closed,Fixed,13/Aug/11 20:21,19/Oct/11 00:26
Bug,HDFS-2261,12518932,AOP unit tests are not getting compiled or run ,The tests in src/test/aop are not getting compiled or run.,wheat9,gkesavan,Minor,Resolved,Fixed,15/Aug/11 22:26,21/Mar/19 17:21
Bug,HDFS-2264,12519062,NamenodeProtocol has the wrong value for clientPrincipal in KerberosInfo annotation,"The {{@KerberosInfo}} annotation specifies the expected server and client principals for a given protocol in order to look up the correct principal name from the config. The {{NamenodeProtocol}} has the wrong value for the client config key. This wasn't noticed because most setups actually use the same *value* for for both the NN and 2NN principals ({{hdfs/_HOST@REALM}}), in which the {{_HOST}} part gets replaced at run-time. This bug therefore only manifests itself on secure setups which explicitly specify the NN and 2NN principals.",atm,atm,Major,Closed,Fixed,17/Aug/11 02:18,22/Jan/14 21:10
Bug,HDFS-2267,12519152,DataXceiver thread name incorrect while waiting on op during keepalive,"Since HDFS-941, the DataXceiver can spend time waiting for a second op to come from the client. Currently, its thread name indicates whatever the previous operation was, rather than something like ""Waiting in keepalive for a new request"" or something.",tlipcon,tlipcon,Trivial,Closed,Fixed,17/Aug/11 17:39,15/Nov/11 00:53
Bug,HDFS-2271,12519300,startJournalSpool should invoke ProcessIOError with failed storage directories if createEditLogFile throws any exception.  ,"Even If createEditsLogFile failes in startJournalSpool of BackUpStorage, BackUPNode will proceed with exceptions. 
",umamaheswararao,umamaheswararao,Major,Resolved,Fixed,18/Aug/11 14:30,16/Sep/11 06:26
Bug,HDFS-2276,12519480,src/test/unit tests not being run in mavenized HDFS,"There are about 5 tests in src/test/unit that are no longer being run.

",tlipcon,tlipcon,Critical,Resolved,Fixed,19/Aug/11 23:03,17/Mar/16 16:45
Bug,HDFS-2280,12519867,BackupNode fails with MD5 checksum Exception during checkpoint if BN's image is outdated.,"If BN starts after NN made changes to namespace it fails with MD5 checksum Exception during checkpoint when it reads new image upload from NN. This is happening because {{imageDigest}} is not reset to null, but keeps the value of the originally loaded BN image.",shv,shv,Major,Resolved,Fixed,23/Aug/11 19:15,07/Sep/11 10:36
Bug,HDFS-2281,12519899,NPE in checkpoint during processIOError(),"At the end of checkpoint BackupNode tries to convergeJournalSpool() and calls revertFileStreams(). The latter closes each file stream, and tries to rename the corresponding file to its permanent location current/edits. If for any reason the rename fails processIOError() is called for failed streams. processIOError() will try to close the stream again and will get NPE in EditLogFileOutputStream.close() because bufCurrent was set to null by the previous close.",umamaheswararao,shv,Major,Resolved,Fixed,24/Aug/11 01:19,07/Sep/11 10:36
Bug,HDFS-2285,12520034,BackupNode should reject requests trying to modify namespace,"I am trying to remove file from BackupNode using
{code}hadoop fs -fs hdfs://backup.node.com:50100 -rm /README.txt{code}
which is supposed to fail. But it seems to be hanging forever.
Needs some investigation. It used to throw SafeModeException if I remember correctly.",shv,shv,Major,Closed,Fixed,25/Aug/11 00:20,10/Mar/15 04:36
Bug,HDFS-2287,12520037,TestParallelRead has a small off-by-one bug,Noticed this bug when I was running TestParallelRead - a simple off-by-one error in some internal bounds checking.,tlipcon,tlipcon,Trivial,Resolved,Fixed,25/Aug/11 00:53,28/Mar/12 09:25
Bug,HDFS-2289,12520185,jsvc isn't part of the artifact,"Apparently we had something like this in build.xml:

<property name=""jsvc.location"" value=""http://archive.apache.org/dist/commons/daemon/binaries/1.0.2/linux/commons-daemon-1.0.2-bin-linux-i386.tar.gz"" />

Also, when I manually add in jsvc binary I get this error:
{noformat}
25/08/2011 23:47:18 29805 jsvc.exec error: Cannot find daemon loader org/apache/commons/daemon/support/DaemonLoader
25/08/2011 23:47:18 29778 jsvc.exec error: Service exit with a return value of 1
{noformat}
",tucu00,acmurthy,Blocker,Closed,Fixed,25/Aug/11 23:55,15/Nov/11 00:52
Bug,HDFS-2290,12520221,Block with corrupt replica is not getting replicated,"A block has one replica marked as corrupt and two good ones. countNodes() correctly detects that there are only 2 live replicas, and fsck reports the block as under-replicated. But ReplicationMonitor never schedules replication of good replicas.",benoyantony,shv,Major,Closed,Fixed,26/Aug/11 07:27,16/Mar/15 17:44
Bug,HDFS-2297,12520606,FindBugs OutOfMemoryError,"When running the findbugs target from Jenkins, I get an OutOfMemory error.
The ""effort"" in FindBugs is set to Max which ends up using a lot of memory to go through all the classes. The jvmargs passed to FindBugs is hardcoded to 512 MB max.

We can leave the default to 512M, as long as we pass this as an ant parameter which can be overwritten in individual cases through -D, or in the build.properties file (either basedir, or user's home directory).
",jrottinghuis,jrottinghuis,Blocker,Closed,Fixed,29/Aug/11 17:45,12/Dec/11 06:20
Bug,HDFS-2299,12520617,TestOfflineEditsViewer is failing on trunk,"The relevant bit of the error:

{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer
-------------------------------------------------------------------------------
Tests run: 2, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 5.652 sec <<< FAILURE!
testStored(org.apache.hadoop.hdfs.tools.offlineEditsViewer.TestOfflineEditsViewer)  Time elapsed: 0.038 sec  <<< FAILURE!
java.lang.AssertionError: Reference XML edits and parsed to XML should be same
{noformat}",umamaheswararao,atm,Major,Resolved,Fixed,29/Aug/11 18:44,12/May/16 18:15
Bug,HDFS-2300,12520633,TestFileAppend4 and TestMultiThreadedSync fail on 20.append and 20-security.,"TestFileAppend4 and TestMultiThreadedSync fail on the 20.append and 20-security branch.
",jnp,jnp,Major,Resolved,Fixed,29/Aug/11 20:27,10/Mar/15 03:03
Bug,HDFS-2305,12520977,Running multiple 2NNs can result in corrupt file system,"Here's the scenario:

* You run the NN and 2NN (2NN A) on the same machine.
* You don't have the address of the 2NN configured, so it's defaulting to 127.0.0.1.
* There's another 2NN (2NN B) running on a second machine.
* When a 2NN is done checkpointing, it says ""hey NN, I have an updated fsimage for you. You can download it from this URL, which includes my IP address, which is x""

And here's the steps that occur to cause this issue:

# Some edits happen.
# 2NN A (on the NN machine) does a checkpoint. All is dandy.
# Some more edits happen.
# 2NN B (on a different machine) does a checkpoint. It tells the NN ""grab the newly-merged fsimage file from 127.0.0.1""
# NN happily grabs the fsimage from 2NN A (the 2NN on the NN machine), which is stale.
# NN renames edits.new file to edits. At this point the in-memory FS state is fine, but the on-disk state is missing edits.
# The next time a 2NN (any 2NN) tries to do a checkpoint, it gets an up-to-date edits file, with an outdated fsimage, and tries to apply those edits to that fsimage.
# Kaboom.",atm,atm,Major,Closed,Fixed,31/Aug/11 21:05,17/Oct/12 18:27
Bug,HDFS-2309,12521331,TestRenameWhileOpen fails in branch-0.20-security,TestRenameWhileOpen is failing in branch-0.20-security.,jnp,jnp,Major,Closed,Fixed,05/Sep/11 06:08,19/Oct/11 00:26
Bug,HDFS-2310,12521364,TestBackupNode fails since HADOOP-7524 went in.,"Logs give the following error. This happens because the JournalProtocol is never registered with the server.

2011-09-05 10:44:36,811 INFO  ipc.Server (Server.java:run(1497)) - IPC Server handler 0 on 60758, call: startLogSegment(NamenodeRegistration(localhost:60754, role=NameNode), 3), rpc version=2, client version=1, methodsFingerPrint=-852377201 from 127.0.0.1:60760, error: 
java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol
	at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)
2011-09-05 10:44:36,813 ERROR namenode.FSEditLog (FSEditLog.java:mapJournalsAndReportErrors(1048)) - Error starting log segment 3 (journal JournalAndStream(mgr=org.apache.hadoop.hdfs.server.namenode.BackupJournalManager@2876b359, stream=null))
java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.server.protocol.JournalProtocol
	at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:622)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1489)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1485)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)
	at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1483)
",ikelly,ikelly,Major,Resolved,Fixed,05/Sep/11 15:31,13/May/16 05:13
Bug,HDFS-2312,12521388,FSNamesystem header comment says it's for the DN,"One of the less accurate class comments I've ever seen:

{noformat}
 * FSNamesystem does the actual bookkeeping work for the
 * DataNode.
{noformat}",qwertymaniac,atm,Trivial,Resolved,Fixed,05/Sep/11 21:12,12/May/16 18:18
Bug,HDFS-2313,12521400,Rat excludes has a typo for excluding editsStored files,"In {{hadoop-hdfs-project/hadoop-hdfs/pom.xml}}, this:

{noformat}
<exclude>src/test/resources/editStored*</exclude>
{noformat}

Should be:

{noformat}
<exclude>src/test/resources/editsStored*</exclude>
{noformat}",atm,atm,Major,Resolved,Fixed,06/Sep/11 00:23,12/May/16 18:18
Bug,HDFS-2314,12521427,MRV1 test compilation broken after HDFS-2197,"Runing the following:
  At the trunk level: {{mvn clean install package -Dtar -Pdist -Dmaven.test.skip.exec=true}}
  In hadoop-mapreduce-project: {{ant jar-test -Dresolvers=internal}}

yields the errors:
{code}
    [javac] /home/vinodkv/Workspace/eclipse-workspace/apache-git/hadoop-common/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/security/authorize/TestServiceLevelAuthorization.java:62: cannot find symbol
    [javac] symbol  : method getRpcServer(org.apache.hadoop.hdfs.server.namenode.NameNode)
    [javac] location: class org.apache.hadoop.hdfs.server.namenode.NameNodeAdapter
    [javac]       Set<Class<?>> protocolsWithAcls = NameNodeAdapter.getRpcServer(dfs.getNameNode())
    [javac]                                                        ^
    [javac] /home/vinodkv/Workspace/eclipse-workspace/apache-git/hadoop-common/hadoop-mapreduce-project/src/test/mapred/org/apache/hadoop/security/authorize/TestServiceLevelAuthorization.java:79: cannot find symbol
    [javac] symbol  : method getRpcServer(org.apache.hadoop.hdfs.server.namenode.NameNode)
    [javac] location: class org.apache.hadoop.hdfs.server.namenode.NameNodeAdapter
    [javac]       protocolsWithAcls = NameNodeAdapter.getRpcServer(dfs.getNameNode())
    [javac] 2 errors
{code}",tlipcon,vinodkv,Major,Closed,Fixed,06/Sep/11 08:47,16/Mar/15 18:07
Bug,HDFS-2315,12521511,Build fails with ant 1.7.0 but works with 1.8.0,"Build failure:
https://builds.apache.org/view/G-L/view/Hadoop/job/Hadoop-Hdfs-22-branch/80
build.xml calls build.xml in contrib, which calls fuse build, which in turn uses build-contrib.
The inheritAll=true overrides the basedir in ant 1.7.0 but not in 1.8.0.",jrottinghuis,jrottinghuis,Blocker,Closed,Fixed,06/Sep/11 22:46,12/Dec/11 06:18
Bug,HDFS-2320,12521616,Make merged protocol changes from 0.20-append to 0.20-security compatible with previous releases.,"0.20-append changes have been merged to 0.20-security. The merge has changes to version numbers in several protocols. This jira makes the protocol changes compatible with older release, allowing clients running older version to talk to server running 205 version and clients running 205 version talk to older servers running 203, 204.",sureshms,sureshms,Major,Closed,Fixed,07/Sep/11 16:23,19/Oct/11 00:26
Bug,HDFS-2322,12522303,the build fails in Windows because commons-daemon TAR cannot be fetched,"For windows there is no commons-daemon TAR but a ZIP, plus the name follows a different convention. 

",tucu00,tucu00,Major,Closed,Fixed,08/Sep/11 18:53,16/Mar/15 18:07
Bug,HDFS-2323,12522345,start-dfs.sh script fails for tarball install,"I build Common and HDFS tarballs from trunk then tried to start a cluster with start-dfs.sh, but I got the following error:

{noformat}
Starting namenodes on [localhost ]
sbin/start-dfs.sh: line 55: /Users/tom/tmp/hadoop/libexec/../bin/hadoop-daemons.sh: No such file or directory
sbin/start-dfs.sh: line 68: /Users/tom/tmp/hadoop/libexec/../bin/hadoop-daemons.sh: No such file or directory
Starting secondary namenodes [0.0.0.0 ]
sbin/start-dfs.sh: line 88: /Users/tom/tmp/hadoop/libexec/../bin/hadoop-daemons.sh: No such file or directory
{noformat}",tomwhite,tomwhite,Major,Closed,Fixed,08/Sep/11 23:09,15/Nov/11 00:53
Bug,HDFS-2325,12522645,Fuse-DFS fails to build on Hadoop 20.203.0,"In building fuse-dfs, the compile fails due to an argument mismatch between call to hdfsConnectAsUser on line 40 of src/contrib/fuse-dfs/src/fuse_connect.c and an earlier definition of hdfsConnectAsUser given in src/c++/libhdfs/hdfs.h.
I suggest changing hdfs.h. I made the following change in hdfs.h in my local copy:

106c106,107
<      hdfsFS hdfsConnectAsUser(const char* host, tPort port, const char *user);
---
>   //     hdfsFS hdfsConnectAsUser(const char* host, tPort port, const char *user);
>   hdfsFS hdfsConnectAsUser(const char* host, tPort port, const char *user, const char** groups, int numgroups);

This new version successfully compiles.",kihwal,charlescearl,Blocker,Closed,Fixed,10/Sep/11 13:23,19/Oct/11 00:26
Bug,HDFS-2328,12522907,hftp throws NPE if security is not enabled on remote cluster,"If hftp cannot locate either a hdfs or hftp token in the ugi, it will call {{getDelegationToken}} to acquire one from the remote nn.  This method may return a null {{Token}} if security is disabled(*)  on the remote nn.  Hftp will internally call its {{setDelegationToken}} which will throw a NPE when the token is {{null}}.

(*) Actually, if any problem happens while acquiring the token it assumes security is disabled!  However, it's a pre-existing issue beyond the scope of the token renewal changes.",omalley,daryn,Critical,Closed,Fixed,13/Sep/11 15:19,19/Oct/11 00:25
Bug,HDFS-2331,12522991,Hdfs compilation fails,"I am trying to perform complete build from trunk folder but the compilation fails.

*Commandline:*
mvn clean install  

*Error Message:*

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.
3.2:compile (default-compile) on project hadoop-hdfs: Compilation failure
[ERROR] \Hadoop\SVN\trunk\hadoop-hdfs-project\hadoop-hdfs\src\main\java\org
\apache\hadoop\hdfs\web\WebHdfsFileSystem.java:[209,21] type parameters of <T>T
cannot be determined; no unique maximal instance exists for type variable T with
 upper bounds T,java.lang.Object
[ERROR] -> [Help 1]
[ERROR]
[ERROR] To see the full stack trace of the errors, re-run Maven with the -e swit
ch.
[ERROR] Re-run Maven using the -X switch to enable full debug logging.
[ERROR]
[ERROR] For more information about the errors and possible solutions, please rea
d the following articles:
[ERROR] [Help 1] http://cwiki.apache.org/confluence/display/MAVEN/MojoFailureExc
eption
[ERROR]
[ERROR] After correcting the problems, you can resume the build with the command


This is because of known reason
[http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6302954]



{code:title=WebHdfsFileSystem.java|borderStyle=solid}

Following is the code snippet with issue.

  private <T> T run(final HttpOpParam.Op op, final Path fspath,
      final Param<?,?>... parameters) throws IOException {
    final HttpURLConnection conn = httpConnect(op, fspath, parameters);
    validateResponse(op, conn);
    try {
      return jsonParse(conn.getInputStream());
    } finally {
      conn.disconnect();
    }
  }

{code} 


Fix to the issue


{code:title=WebHdfsFileSystem.java|borderStyle=solid}

Following is the fix to the issue.

  private <T> T run(final HttpOpParam.Op op, final Path fspath,
      final Param<?,?>... parameters) throws IOException {
    final HttpURLConnection conn = httpConnect(op, fspath, parameters);
    validateResponse(op, conn);
    try {
      return WebHdfsFileSystem.<T>jsonParse(conn.getInputStream());
    } finally {
      conn.disconnect();
    }
  }

{code} 

",abhijit.shingate,abhijit.shingate,Major,Closed,Fixed,14/Sep/11 03:24,16/Mar/15 18:07
Bug,HDFS-2333,12523020,HDFS-2284 introduced 2 findbugs warnings on trunk,When HDFS-2284 was submitted it made DFSOutputStream public which triggered two SC_START_IN_CTOR findbug warnings.,szetszwo,ikelly,Major,Closed,Fixed,14/Sep/11 09:37,16/Mar/15 18:02
Bug,HDFS-2341,12523368,Contribs not building,"Contribs are not getting built.
Snippet from Jenkins:

compile:
   [subant] No sub-builds to iterate on",jrottinghuis,jrottinghuis,Blocker,Closed,Fixed,16/Sep/11 17:52,12/Dec/11 06:19
Bug,HDFS-2342,12523393,TestSleepJob and TestHdfsProxy broken after HDFS-2284,"After HDFS-2284, TestSleepJob and TestHdfsProxy are failing.
The both work in rev 1167444 and fail in rev 1167663.
It will be great if they can be fixed for 205.",szetszwo,kihwal,Blocker,Closed,Fixed,16/Sep/11 21:40,19/Oct/11 00:26
Bug,HDFS-2343,12523396,Make hdfs use same version of avro as HBase,"HBase depends on avro 1.5.3 whereas hadoop-common depends on 1.3.2.
When building HBase on top of hadoop, this should be consistent.
Moreover, this should be consistent between common, hdfs, and mapreduce.",jrottinghuis,jrottinghuis,Blocker,Closed,Fixed,16/Sep/11 21:55,12/Dec/11 06:19
Bug,HDFS-2344,12523437,Fix the TestOfflineEditsViewer test failure in 0.23 branch,TestOfflineEditsViewer test fails in 0.23 branch,umamaheswararao,umamaheswararao,Major,Closed,Fixed,17/Sep/11 17:41,15/Nov/11 00:53
Bug,HDFS-2345,12523445,TestLeaseRecovery2 fails on 0.23 branch,"Regression

org.apache.hadoop.hdfs.TestLeaseRecovery2.testHardLeaseRecoveryAfterNameNodeRestart
Failing for the past 1 build (Since Unstable#11 )
Took 8.2 sec.
Error Message

Lease mismatch on /hardLeaseRecovery owned by HDFS_NameNode but is accessed by DFSClient_NONMAPREDUCE_100687476_1  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1598)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.checkLease(FSNamesystem.java:1573)  at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.getAdditionalDatanode(FSNamesystem.java:1511)  at org.apache.hadoop.hdfs.server.namenode.NameNodeRpcServer.getAdditionalDatanode(NameNodeRpcServer.java:419)  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)  at java.lang.reflect.Method.invoke(Method.java:597)  at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:365)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1496)  at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1492)  at java.security.AccessController.doPrivileged(Native Method)  at javax.security.auth.Subject.doAs(Subject.java:396)  at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1135)  at org.apache.hadoop.ipc.Server$Handler.run(Server.java:1490) 

https://builds.apache.org/job/Hadoop-Hdfs-0.23-Build/11/testReport/

failure looks to be random.",umamaheswararao,umamaheswararao,Major,Resolved,Fixed,17/Sep/11 18:14,29/Sep/11 13:16
Bug,HDFS-2346,12523451,TestHost2NodesMap & TestReplicasMap will fail depending upon execution order of test methods,,lakshman,umamaheswararao,Blocker,Closed,Fixed,17/Sep/11 20:48,09/Sep/14 20:58
Bug,HDFS-2347,12523472,checkpointTxnCount's comment still saying about editlog size,"As per the latest changes checkpoint will trigger based on transaction counts instead of editlog size.But checkpointTxnCount comment is still saying about editlog size.

{code}
private long checkpointTxnCount;    // size (in MB) of current Edit Log
{code}",umamaheswararao,umamaheswararao,Trivial,Closed,Fixed,18/Sep/11 06:18,15/Nov/11 00:53
Bug,HDFS-2350,12523632,Secure DN doesn't print output to console when started interactively,"If one starts a secure DN (using jsvc) interactively, the output is not printed to the console, but instead ends up in {{$HADOOP_LOG_DIR/jsvc.err}} and {{$HADOOP_LOG_DIR/jsvc.out}}.",cnauroth,atm,Major,Resolved,Fixed,19/Sep/11 22:49,16/Mar/15 19:49
Bug,HDFS-2358,12524331,NPE when the default filesystem's uri has no authority,"

in core-site.xml if default filesystem's uri is misconfigured , NN does not come up and in NN shows NPE

java.lang.NullPointerException
        at org.apache.hadoop.net.NetUtils.createSocketAddr(NetUtils.java:142)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:195)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.getAddress(NameNode.java:225)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:259)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:458)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:1226)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.main(NameNode.java:1235)


It is better to give meaningful error messege than NPE",daryn,rajsaha,Major,Closed,Fixed,22/Sep/11 20:17,19/Oct/11 00:26
Bug,HDFS-2359,12524356,NPE found in Datanode log while Disk failed during different HDFS operation,"Scenario:
I have a cluster of 4 DN ,each of them have 12disks.

In hdfs-site.xml I have ""dfs.datanode.failed.volumes.tolerated=3"" 

During the execution of distcp (hdfs->hdfs), I am failing 3 disks in one Datanode, by making Data Directory permission 000, The distcp job is successful but , I am getting some NullPointerException in Datanode log

In one thread
$hadoop distcp  /user/$HADOOPQA_USER/data1 /user/$HADOOPQA_USER/data3

In another thread in a datanode
$ chmod 000 /xyz/{0,1,2}/hadoop/var/hdfs/data

where [ dfs.data.dir is set as /xyz/{0..11}/hadoop/var/hdfs/data ]

Log Snippet from the Datanode
=============

2011-09-19 12:43:40,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block
blk_7065198814142552283_62557. BlockInfo not found in volumeMap.
2011-09-19 12:43:40,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block
blk_7066946313092770579_39189. BlockInfo not found in volumeMap.
2011-09-19 12:43:40,314 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block
blk_7070305189404753930_49359. BlockInfo not found in volumeMap.
2011-09-19 12:43:40,327 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Error processing datanode Command
java.io.IOException: Error in deleting blocks.
        at org.apache.hadoop.hdfs.server.datanode.FSDataset.invalidate(FSDataset.java:1820)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1074)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.processCommand(DataNode.java:1036)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:891)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1419)
        at java.lang.Thread.run(Thread.java:619)
2011-09-19 12:43:41,304 ERROR org.apache.hadoop.hdfs.server.datanode.DataNode:
DatanodeRegistration(xx.xxx.xxx.xxx:xxxx, storageID=xx-xxxxxxxxxxxx-xx.xxx.xxx.xxx-xxxx-xxxxxxxxxxx, infoPort=1006,
ipcPort=8020):DataXceiver
java.lang.NullPointerException
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner$LogFileHandler.appendLine(DataBlockScanner.java:788)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.updateScanStatusInternal(DataBlockScanner.java:365)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.verifiedByClient(DataBlockScanner.java:308)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.readBlock(DataXceiver.java:205)
        at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:99)
        at java.lang.Thread.run(Thread.java:619)
2011-09-19 12:43:43,313 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block
blk_7071818644980664768_40827. BlockInfo not found in volumeMap.
2011-09-19 12:43:43,313 WARN org.apache.hadoop.hdfs.server.datanode.DataNode: Unexpected error trying to delete block
blk_7073840977856837621_62108. BlockInfo not found in volumeMap.",jeagles,rajsaha,Major,Closed,Fixed,23/Sep/11 00:20,19/Oct/11 00:25
Bug,HDFS-2361,12524479,hftp is broken,"Distcp with hftp is failing.

{noformat}
$hadoop   distcp hftp://<NNhostname>:50070/user/hadoopqa/1316814737/newtemp 1316814737/as
11/09/23 21:52:33 INFO tools.DistCp: srcPaths=[hftp://<NNhostname>:50070/user/hadoopqa/1316814737/newtemp]
11/09/23 21:52:33 INFO tools.DistCp: destPath=1316814737/as
Retrieving token from: https://<NN IP>:50470/getDelegationToken
Retrieving token from: https://<NN IP>:50470/getDelegationToken?renewer=mapred
11/09/23 21:52:34 INFO security.TokenCache: Got dt for hftp://<NNhostname>:50070/user/hadoopqa/1316814737/newtemp;uri=<NN IP>:50470;t.service=<NN IP>:50470
org.apache.hadoop.ipc.RemoteException: name=hadoopqa != expected=null
	at org.apache.hadoop.ipc.RemoteException.valueOf(RemoteException.java:102)
	at org.apache.hadoop.hdfs.HftpFileSystem$LsParser.startElement(HftpFileSystem.java:385)
	...
{noformat}",jnp,rajsaha,Critical,Closed,Fixed,23/Sep/11 21:57,16/Mar/15 18:01
Bug,HDFS-2368,12524764,"defaults created for web keytab and principal, these properties should not have defaults","the following defaults are set in hdfs-defaults.xml

<property>
  <name>dfs.web.authentication.kerberos.principal</name>
  <value>HTTP/${dfs.web.hostname}@${kerberos.realm}</value>
  <description>
    The HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint.

    The HTTP Kerberos principal MUST start with 'HTTP/' per Kerberos
    HTTP SPENGO specification.
  </description>
</property>

<property>
  <name>dfs.web.authentication.kerberos.keytab</name>
  <value>${user.home}/dfs.web.keytab</value>
  <description>
    The Kerberos keytab file with the credentials for the
    HTTP Kerberos principal used by Hadoop-Auth in the HTTP endpoint.
  </description>
</property>


These properties should not have defaults",szetszwo,arpitgupta,Major,Closed,Fixed,26/Sep/11 23:53,16/Mar/15 18:01
Bug,HDFS-2369,12524768,Fsck Fails,"$hadoop fsck /
Exception in thread ""main"" java.io.IOException: Server returned HTTP response code: 500 for URL: https://<NN hostname>:50470/fsck?ugi=hdfs&path=%2F
	at sun.net.www.protocol.http.HttpURLConnection.getInputStream(HttpURLConnection.java:1313)
	at sun.net.www.protocol.https.HttpsURLConnectionImpl.getInputStream(HttpsURLConnectionImpl.java:234)
	at org.apache.hadoop.hdfs.tools.DFSck$1.run(DFSck.java:141)
	at org.apache.hadoop.hdfs.tools.DFSck$1.run(DFSck.java:110)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.hdfs.tools.DFSck.run(DFSck.java:110)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
	at org.apache.hadoop.hdfs.tools.DFSck.main(DFSck.java:182)
",,rajsaha,Critical,Resolved,Fixed,27/Sep/11 00:09,02/May/13 02:29
Bug,HDFS-2373,12524789,Commands using webhdfs and hftp print unnecessary debug information on the console with security enabled,"run an hdfs command using either hftp or webhdfs and it prints the following line to the console (system out)

Retrieving token from: https://NN_HOST:50470/getDelegationToken


Probably in the code where we get the delegation token. This should be removed as people using the dfs commands to get a handle to the content such as dfs -cat will now get an extra line that is not part of the actual content. This should either be only in the log or not logged at all.",arpitgupta,arpitgupta,Major,Closed,Fixed,27/Sep/11 04:11,12/May/16 18:15
Bug,HDFS-2375,12524805,TestFileAppend4 fails in 0.20.205 branch,"TestFileAppend4 fails due to change from HDFS-2333. The test uses reflection to get to the method DFSOutputStream#getNumCurrentReplicas(). Since HDFS-2333 patch change this method from public to private, reflection get the method fails resulting in test failures.",sureshms,sureshms,Blocker,Closed,Fixed,27/Sep/11 06:55,19/Oct/11 00:26
Bug,HDFS-2379,12525031,0.20: Allow block reports to proceed without holding FSDataset lock,"As disks are getting larger and more plentiful, we're seeing DNs with multiple millions of blocks on a single machine. When page cache space is tight, block reports can take multiple minutes to generate. Currently, during the scanning of the data directories to generate a report, the FSVolumeSet lock is held. This causes writes and reads to block, timeout, etc, causing big problems especially for clients like HBase.

This JIRA is to explore some of the ideas originally discussed in HADOOP-4584 for the 0.20.20x series.",tlipcon,tlipcon,Critical,Closed,Fixed,28/Sep/11 09:03,06/Jul/12 17:43
Bug,HDFS-2382,12525088,TestDfsOverAvroRpc fails,"See https://builds.apache.org/job/PreCommit-HDFS-Build/1309//testReport/org.apache.hadoop.hdfs/TestDfsOverAvroRpc/testWorkingDirectory/
Looks like this is related to two protocol method named the same ""delete"" that is not expected by Avro. I propose turning this test off, it there are no volunteers to fix it.",,sureshms,Major,Resolved,Fixed,28/Sep/11 18:02,09/Sep/14 20:58
Bug,HDFS-2383,12525103,TestDfsOverAvroRpc is failing on 0.22,{{TestDfsOverAvroRpc.testWorkingDirectory()}} is failing. Possible the result of Avro upgrade.,shv,shv,Blocker,Closed,Fixed,28/Sep/11 20:51,12/Dec/11 06:20
Bug,HDFS-2388,12525218,Remove dependency on different version of slf4j in avro,This is the HDFS part of HADOOP-7697.,shv,shv,Major,Closed,Fixed,29/Sep/11 17:16,12/Dec/11 06:19
Bug,HDFS-2392,12525361,Dist with hftp is failing again,"$ hadoop distcp hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000 /user/hadoopqa/out3
11/09/30 18:57:59 INFO tools.DistCp: srcPaths=[hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000]
11/09/30 18:57:59 INFO tools.DistCp: destPath=/user/hadoopqa/out3
11/09/30 18:58:00 INFO security.TokenCache: Got dt for
hftp://<NN Hostname>:50070/user/hadoopqa/input1/part-00000;uri=<NN IP>:50470;t.service=<NN IP>:50470
11/09/30 18:58:00 INFO hdfs.DFSClient: Created HDFS_DELEGATION_TOKEN token 24 for hadoopqa on <NN IP>:8020
11/09/30 18:58:00 INFO security.TokenCache: Got dt for
/user/hadoopqa/out3;uri=<NN IP>:8020;t.service=<NN IP>:8020
11/09/30 18:58:00 INFO tools.DistCp: /user/hadoopqa/out3 does not exist.
11/09/30 18:58:00 INFO tools.DistCp: sourcePathsCount=1
11/09/30 18:58:00 INFO tools.DistCp: filesToCopyCount=1
11/09/30 18:58:00 INFO tools.DistCp: bytesToCopyCount=1.0g
11/09/30 18:58:01 INFO mapred.JobClient: Running job: job_201109300819_0007
11/09/30 18:58:02 INFO mapred.JobClient:  map 0% reduce 0%
11/09/30 18:58:25 INFO mapred.JobClient: Task Id : attempt_201109300819_0007_m_000000_0, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
        at org.apache.hadoop.mapred.Child.main(Child.java:249)

11/09/30 18:58:41 INFO mapred.JobClient: Task Id : attempt_201109300819_0007_m_000000_1, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
        at org.apache.hadoop.mapred.Child.main(Child.java:249)

11/09/30 18:58:56 INFO mapred.JobClient: Task Id : attempt_201109300819_0007_m_000000_2, Status : FAILED
java.io.IOException: Copied: 0 Skipped: 0 Failed: 1
        at org.apache.hadoop.tools.DistCp$CopyFilesMapper.close(DistCp.java:582)
        at org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:57)
        at org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:436)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:372)
        at org.apache.hadoop.mapred.Child$4.run(Child.java:255)
        at java.security.AccessController.doPrivileged(Native Method)
        at javax.security.auth.Subject.doAs(Subject.java:396)
        at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
        at org.apache.hadoop.mapred.Child.main(Child.java:249)

11/09/30 18:59:14 INFO mapred.JobClient: Job complete: job_201109300819_0007
11/09/30 18:59:14 INFO mapred.JobClient: Counters: 6
11/09/30 18:59:14 INFO mapred.JobClient:   Job Counters 
11/09/30 18:59:14 INFO mapred.JobClient:     SLOTS_MILLIS_MAPS=62380
11/09/30 18:59:14 INFO mapred.JobClient:     Total time spent by all reduces waiting after reserving slots (ms)=0
11/09/30 18:59:14 INFO mapred.JobClient:     Total time spent by all maps waiting after reserving slots (ms)=0
11/09/30 18:59:14 INFO mapred.JobClient:     Launched map tasks=4
11/09/30 18:59:14 INFO mapred.JobClient:     SLOTS_MILLIS_REDUCES=0
11/09/30 18:59:14 INFO mapred.JobClient:     Failed map tasks=1
11/09/30 18:59:14 INFO mapred.JobClient: Job Failed: # of failed Map Tasks exceeded allowed limit. FailedCount: 1.
LastFailedTask: task_201109300819_0007_m_000000
With failures, global counters are inaccurate; consider running with -i
Copy failed: java.io.IOException: Job failed!
        at org.apache.hadoop.mapred.JobClient.runJob(JobClient.java:1257)
        at org.apache.hadoop.tools.DistCp.copy(DistCp.java:667)
        at org.apache.hadoop.tools.DistCp.run(DistCp.java:881)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:79)
        at org.apache.hadoop.tools.DistCp.main(DistCp.java:908)





",daryn,rajsaha,Critical,Closed,Fixed,30/Sep/11 20:29,08/Nov/11 23:18
Bug,HDFS-2405,12525960,hadoop dfs command with webhdfs fails on secure hadoop,,jnp,arpitgupta,Critical,Closed,Fixed,05/Oct/11 20:25,10/Mar/15 04:35
Bug,HDFS-2408,12526006,DFSClient#getNumCurrentReplicas is package private in 205 but public in branch-0.20-append,"The below commit broke hdfs-826 for hbase in 205 rc1.  It changes the accessiblity from public to package private on getNumCurrentReplicas and now current shipping hbase's at least cannot get at this method.

{code}
Revision 1174483 - (view) (download) (annotate) - [select for diffs] 
Modified Fri Sep 23 01:30:18 2011 UTC (13 days, 4 hours ago) by szetszwo 
File length: 136876 byte(s) 
Diff to previous 1174479 (colored)
svn merge -c 1171137 from branch-0.20-security for HDFS-2333.
{code}

Here is diff between above change and one just previous:

http://svn.apache.org/viewvc/hadoop/common/branches/branch-0.20-security-205/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java?view=diff&r1=1174479&r2=1174483&diff_format=u

This is a critical facility for us.

It seems like making this one method public again is all thats needed.  I can make a patch like the below:

diff --git a/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java b/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java
index b9cb053..39955c9 100644
--- a/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java
+++ b/src/hdfs/org/apache/hadoop/hdfs/DFSClient.java
@@ -3569,7 +3569,7 @@ public class DFSClient implements FSConstants, java.io.Closeable {
      * block is not yet allocated, then this API will return 0 because there are
      * no replicas in the pipeline.
      */
-    int getNumCurrentReplicas() throws IOException {
+    public int getNumCurrentReplicas() throws IOException {
       synchronized(dataQueue) {
         if (nodes == null) {
           return blockReplication;

Can we get this into RC2?

Thanks,
St.Ack",stack,stack,Blocker,Closed,Fixed,06/Oct/11 06:04,04/Feb/12 14:12
Bug,HDFS-2409,12526087,_HOST in dfs.web.authentication.kerberos.principal.,This is HDFS part of HADOOP-7721. ,jnp,jnp,Major,Closed,Fixed,06/Oct/11 18:01,16/Mar/15 18:07
Bug,HDFS-2411,12526136,with webhdfs enabled in secure mode the auth to local mappings are not being respected.,,jnp,arpitgupta,Major,Closed,Fixed,07/Oct/11 00:54,16/Mar/15 18:01
Bug,HDFS-2412,12526150,Add backwards-compatibility layer for FSConstants,HDFS-1620 renamed FSConstants which we believed to be a private class. But currently the public APIs for safe-mode and datanode reports depend on constants in FSConstants. This is breaking HBase builds against 0.23. This JIRA is to provide a backward-compatibility route.,tlipcon,tlipcon,Blocker,Closed,Fixed,07/Oct/11 04:13,15/Nov/11 00:53
Bug,HDFS-2414,12526222,TestDFSRollback fails intermittently,"When running TestDFSRollback repeatedly in a loop I observed a failure rate of about 3%.  Two separate stack traces are in the output and it appears to have something to do with not writing out a complete snapshot of the data for rollback.

{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdfs.TestDFSRollback
-------------------------------------------------------------------------------
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 8.514 sec <<< FAILURE!
testRollback(org.apache.hadoop.hdfs.TestDFSRollback)  Time elapsed: 8.34 sec  <<< FAILURE!
java.lang.AssertionError: File contents differed:
  /home/evans/src/hadoop-git/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data2/current/VERSION=5b19197114fad0a254e3f318b7f14aec
  /home/evans/src/hadoop-git/hadoop-hdfs-project/hadoop-hdfs/target/test/data/dfs/data1/current/VERSION=ea7b000a6a1711169fc7a836b240a991
        at org.junit.Assert.fail(Assert.java:91)
        at org.apache.hadoop.hdfs.server.namenode.FSImageTestUtil.assertFileContentsSame(FSImageTestUtil.java:250)
        at org.apache.hadoop.hdfs.server.namenode.FSImageTestUtil.assertParallelFilesAreIdentical(FSImageTestUtil.java:236)
        at org.apache.hadoop.hdfs.TestDFSRollback.checkResult(TestDFSRollback.java:86)
        at org.apache.hadoop.hdfs.TestDFSRollback.testRollback(TestDFSRollback.java:171)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at junit.framework.TestCase.runTest(TestCase.java:168)
        at junit.framework.TestCase.runBare(TestCase.java:134)
        at junit.framework.TestResult$1.protect(TestResult.java:110)
        at junit.framework.TestResult.runProtected(TestResult.java:128)
        at junit.framework.TestResult.run(TestResult.java:113)
        at junit.framework.TestCase.run(TestCase.java:124)
        at junit.framework.TestSuite.runTest(TestSuite.java:232)
        at junit.framework.TestSuite.run(TestSuite.java:227)
        at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
        at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
        at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:120)
        at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:145)
        at org.apache.maven.surefire.Surefire.run(Surefire.java:104)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:290)
        at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1017)
{noformat}

is the more common one, but I also saw

{noformat}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hdfs.TestDFSRollback
-------------------------------------------------------------------------------
Tests run: 1, Failures: 1, Errors: 0, Skipped: 0, Time elapsed: 7.471 sec <<< FAILURE!
testRollback(org.apache.hadoop.hdfs.TestDFSRollback)  Time elapsed: 7.304 sec  <<< FAILURE!
junit.framework.AssertionFailedError: Expected substring 'file VERSION has layoutVersion missing' in exception but got: java.lang.IllegalArgumentException: Malformed \uxxxx encoding.
        at java.util.Properties.loadConvert(Properties.java:552)
        at java.util.Properties.load0(Properties.java:374)
        at java.util.Properties.load(Properties.java:325)
        at org.apache.hadoop.hdfs.server.common.Storage.readPropertiesFile(Storage.java:837)
        at org.apache.hadoop.hdfs.server.common.Storage.readPreviousVersionProperties(Storage.java:789)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.doRollback(FSImage.java:439)
        at org.apache.hadoop.hdfs.server.namenode.FSImage.recoverTransitionRead(FSImage.java:270)
        at org.apache.hadoop.hdfs.server.namenode.FSDirectory.loadFSImage(FSDirectory.java:174)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.initialize(FSNamesystem.java:294)
        at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.<init>(FSNamesystem.java:266)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.loadNamesystem(NameNode.java:292)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.initialize(NameNode.java:326)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:452)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.<init>(NameNode.java:444)
        at org.apache.hadoop.hdfs.server.namenode.NameNode.createNameNode(NameNode.java:742)
        at org.apache.hadoop.hdfs.MiniDFSCluster.createNameNode(MiniDFSCluster.java:637)
        at org.apache.hadoop.hdfs.MiniDFSCluster.initMiniDFSCluster(MiniDFSCluster.java:541)
        at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:257)
        at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:85)
        at org.apache.hadoop.hdfs.MiniDFSCluster$Builder.build(MiniDFSCluster.java:243)
        at org.apache.hadoop.hdfs.TestDFSRollback.startNameNodeShouldFail(TestDFSRollback.java:100)
        at org.apache.hadoop.hdfs.TestDFSRollback.testRollback(TestDFSRollback.java:268)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at junit.framework.TestCase.runTest(TestCase.java:168)
        at junit.framework.TestCase.runBare(TestCase.java:134)
        at junit.framework.TestResult$1.protect(TestResult.java:110)
        at junit.framework.TestResult.runProtected(TestResult.java:128)
        at junit.framework.TestResult.run(TestResult.java:113)
        at junit.framework.TestCase.run(TestCase.java:124)
        at junit.framework.TestSuite.runTest(TestSuite.java:232)
        at junit.framework.TestSuite.run(TestSuite.java:227)
        at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
        at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
        at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:120)
        at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:145)
        at org.apache.maven.surefire.Surefire.run(Surefire.java:104)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:290)
        at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1017)

        at junit.framework.Assert.fail(Assert.java:47)
        at org.apache.hadoop.hdfs.TestDFSRollback.startNameNodeShouldFail(TestDFSRollback.java:109)
        at org.apache.hadoop.hdfs.TestDFSRollback.testRollback(TestDFSRollback.java:268)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at junit.framework.TestCase.runTest(TestCase.java:168)
        at junit.framework.TestCase.runBare(TestCase.java:134)
        at junit.framework.TestResult$1.protect(TestResult.java:110)
        at junit.framework.TestResult.runProtected(TestResult.java:128)
        at junit.framework.TestResult.run(TestResult.java:113)
        at junit.framework.TestCase.run(TestCase.java:124)
        at junit.framework.TestSuite.runTest(TestSuite.java:232)
        at junit.framework.TestSuite.run(TestSuite.java:227)
        at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
        at org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:59)
        at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.executeTestSet(AbstractDirectoryTestSuite.java:120)
        at org.apache.maven.surefire.suite.AbstractDirectoryTestSuite.execute(AbstractDirectoryTestSuite.java:145)
        at org.apache.maven.surefire.Surefire.run(Surefire.java:104)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.maven.surefire.booter.SurefireBooter.runSuitesInProcess(SurefireBooter.java:290)
        at org.apache.maven.surefire.booter.SurefireBooter.main(SurefireBooter.java:1017)

{noformat}",tlipcon,revans2,Critical,Closed,Fixed,07/Oct/11 15:08,15/Nov/11 00:53
Bug,HDFS-2422,12526347,The NN should tolerate the same number of low-resource volumes as failed volumes,"We encountered a situation where the namenode dropped into safe mode after a temporary outage of an NFS mount.

At 12:10 the NFS server goes offline

Oct  8 12:10:05 <namenode> kernel: nfs: server <nfs host> not responding, timed out

This caused the namenode to conclude resource issues:

2011-10-08 12:10:34,848 WARN org.apache.hadoop.hdfs.server.namenode.NameNodeResourceChecker: Space available on volume '<nfs host>' is 0, which is below the configured reserved amount 104857600

Temporary loss of NFS mount shouldn't cause safemode.",atm,jwfbean,Major,Closed,Fixed,08/Oct/11 22:29,15/Nov/11 00:53
Bug,HDFS-2434,12526670,TestNameNodeMetrics.testCorruptBlock fails intermittently,"java.lang.AssertionError: Bad value for metric CorruptBlocks expected:<1> but was:<0>
	at org.junit.Assert.fail(Assert.java:91)
	at org.junit.Assert.failNotEquals(Assert.java:645)
	at org.junit.Assert.assertEquals(Assert.java:126)
	at org.junit.Assert.assertEquals(Assert.java:470)
	at org.apache.hadoop.test.MetricsAsserts.assertGauge(MetricsAsserts.java:185)
	at org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.__CLR3_0_2t8sh531i1k(TestNameNodeMetrics.java:175)
	at org.apache.hadoop.hdfs.server.namenode.metrics.TestNameNodeMetrics.testCorruptBlock(TestNameNodeMetrics.java:164)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at junit.framework.TestCase.runTest(TestCase.java:168)
	at junit.framework.TestCase.runBare(TestCase.java:134)
",jingzhao,umamaheswararao,Major,Resolved,Fixed,11/Oct/11 14:33,14/Jun/18 17:11
Bug,HDFS-2436,12526681,FSNamesystem.setTimes(..) expects the path is a file.,"FSNamesystem.setTimes(..) does not work if the path is a directory.

Arpit found this bug when testing webhdfs:
{quote}
settimes api is working when called on a file, but when called on a dir it returns a 404. I should be able to set time on both a file and a directory.
{quote}",umamaheswararao,arpitgupta,Major,Closed,Fixed,11/Oct/11 15:57,16/Mar/15 18:07
Bug,HDFS-2445,12527092,Incorrect exit code for hadoop-hdfs-test tests when exception thrown,Please see MAPREDUCE-3179 for a full description.,jeagles,jeagles,Major,Closed,Fixed,13/Oct/11 20:20,10/Mar/15 04:35
Bug,HDFS-2450,12527263,Only complete hostname is supported to access data via hdfs://,"If my complete hostname is  host1.abc.xyz.com, only complete hostname must be used to access data via hdfs://

I am running following in .20.205 Client to get data from .20.205 NN (host1)
$hadoop dfs -copyFromLocal /etc/passwd  hdfs://host1/tmp
copyFromLocal: Wrong FS: hdfs://host1/tmp, expected: hdfs://host1.abc.xyz.com
Usage: java FsShell [-copyFromLocal <localsrc> ... <dst>]

$hadoop dfs -copyFromLocal /etc/passwd  hdfs://host1.abc/tmp/
copyFromLocal: Wrong FS: hdfs://host1.blue/tmp/1, expected: hdfs://host1.abc.xyz.com
Usage: java FsShell [-copyFromLocal <localsrc> ... <dst>]


$hadoop dfs -copyFromLocal /etc/passwd  hftp://host1.abc.xyz/tmp/
copyFromLocal: Wrong FS: hdfs://host1.blue/tmp/1, expected: hdfs://host1.abc.xyz.com
Usage: java FsShell [-copyFromLocal <localsrc> ... <dst>]


Only following is supported 
$hadoop dfs -copyFromLocal /etc/passwd  hdfs://host1.abc.xyz.com/tmp/
",daryn,rajsaha,Major,Closed,Fixed,14/Oct/11 21:29,11/May/12 16:09
Bug,HDFS-2451,12527314,TestNodeCount.testNodeCount failes with NPE,"in the [commit build #97|http://is.gd/UPuXg2] TestNodeCount.testNodeCount failed with NPE
{noformat}
java.lang.NullPointerException
	at org.apache.hadoop.hdfs.server.namenode.BlockManager.countNodes(BlockManager.java:1436)
	at org.apache.hadoop.hdfs.server.namenode.TestNodeCount.__CLR2_4_39bdgm6whp(TestNodeCount.java:119)
	at org.apache.hadoop.hdfs.server.namenode.TestNodeCount.testNodeCount(TestNodeCount.java:40)

{noformat}",cos,cos,Blocker,Resolved,Fixed,15/Oct/11 20:31,20/Oct/11 12:49
Bug,HDFS-2452,12527380,OutOfMemoryError in DataXceiverServer takes down the DataNode,"OutOfMemoryError brings down DataNode, when DataXceiverServer tries to spawn a new data transfer thread.",umamaheswararao,shv,Major,Closed,Fixed,17/Oct/11 01:30,26/Jan/16 15:32
Bug,HDFS-2467,12527604,HftpFileSystem uses incorrect compare for finding delegation tokens,"When looking for hdfs delegation tokens, Hftp converts the service to a string and compares it to a text.",omalley,omalley,Major,Closed,Fixed,18/Oct/11 15:10,15/Nov/11 00:52
Bug,HDFS-2470,12527689,NN should automatically set permissions on dfs.namenode.*.dir,"Much as the DN currently sets the correct permissions for the dfs.datanode.data.dir, the NN should do the same for the dfs.namenode.(name|edit).dir.",swagle,atm,Major,Resolved,Fixed,19/Oct/11 01:43,04/Oct/19 19:16
Bug,HDFS-2481,12528016,Unknown protocol: org.apache.hadoop.hdfs.protocol.ClientProtocol,"A few unit tests are failing, e.g.
{noformat}
Test set: org.apache.hadoop.hdfs.TestDistributedFileSystem
-------------------------------------------------------------------------------
Tests run: 7, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 16.813 sec <<< FAILURE!
testAllWithDualPort(org.apache.hadoop.hdfs.TestDistributedFileSystem)  Time elapsed: 0.254 sec  <<< ERROR!
java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.hdfs.protocol.ClientProtocol
	at org.apache.hadoop.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:615)
	at org.apache.hadoop.ipc.Server$Handler$1.run(Server.java:1517)
	...
{noformat}
",sanjay.radia,szetszwo,Major,Closed,Fixed,20/Oct/11 19:58,28/Sep/15 20:58
Bug,HDFS-2484,12528231,checkLease should throw FileNotFoundException when file does not exist,"When file is deleted during its creation {{FSNamesystem.checkLease(String src, String holder)}} throws {{LeaseExpiredException}}. It would be more informative if it thrown {{FileNotFoundException}}.",rakeshr,shv,Major,Resolved,Fixed,21/Oct/11 07:54,30/Aug/16 01:42
Bug,HDFS-2491,12528355,TestBalancer can fail when datanode utilization and avgUtilization is exactly same.,"Stack Trace:
junit.framework.AssertionFailedError: 127.0.0.1:60986is not an underUtilized node: utilization=22.0 avgUtilization=22.0 threshold=10.0
at org.apache.hadoop.hdfs.server.balancer.Balancer.initNodes(Balancer.java:1014)
at org.apache.hadoop.hdfs.server.balancer.Balancer.initNodes(Balancer.java:953)
at org.apache.hadoop.hdfs.server.balancer.Balancer.run(Balancer.java:1502)
at org.apache.hadoop.hdfs.server.balancer.TestBalancer.runBalancer(TestBalancer.java:247)
at org.apache.hadoop.hdfs.server.balancer.TestBalancer.test(TestBalancer.java:234)
at org.apache.hadoop.hdfs.server.balancer.TestBalancer.twoNodeTest(TestBalancer.java:312)
at org.apache.hadoop.hdfs.server.balancer.TestBalancer.__CLR2_4_39j3j5b10ou(TestBalancer.java:328)
at org.apache.hadoop.hdfs.server.balancer.TestBalancer.testBalancer0(TestBalancer.java:324)
",umamaheswararao,umamaheswararao,Major,Closed,Fixed,22/Oct/11 05:11,16/Mar/15 17:49
Bug,HDFS-2497,12528539,Fix TestBackupNode failure,TestBackupNode fails due to NullPointerException,sureshms,sureshms,Major,Closed,Fixed,24/Oct/11 18:32,28/Sep/15 20:58
Bug,HDFS-2514,12529390,Link resolution bug for intermediate symlinks with relative targets,There's a bug in the way the Namenode resolves intermediate symlinks (ie the symlink is not the final path component) in paths when the symlink's target is a relative path. Will post the full description in the first comment.,eli,eli,Major,Closed,Fixed,30/Oct/11 01:49,12/Dec/11 06:18
Bug,HDFS-2525,12529620,Race between BlockPoolSliceScanner and append,"I wrote a test which runs append() in a loop on a single file with a single replica, appending 0~100 bytes each time. If this races with the BlockPoolSliceScanner, I observe the BlockPoolSliceScanner getting FNFE, then reporting the block as bad to the NN. This causes the writer thread to loop forever on completeFile() since it doesn't see a valid replica.",brandonli,tlipcon,Critical,Resolved,Fixed,01/Nov/11 04:47,16/Mar/15 18:14
Bug,HDFS-2526,12529624,(Client)NamenodeProtocolTranslatorR23 do not need to keep a reference to rpcProxyWithoutRetry,"HADOOP-7607 made it so that when wrapping an RPC proxy with another proxy, one need not hold on to the underlying proxy object to release resources on close. Both {{ClientNamenodeProtocolTranslatorR23}} and {{NamenodeProtocolTranslatorR23}} do not take advantage of this fact. They both also unnecessarily declare {{getProxyWithoutRetry}} methods which are unused.",atm,atm,Major,Closed,Fixed,01/Nov/11 06:48,28/Sep/15 20:58
Bug,HDFS-2532,12529929,TestDfsOverAvroRpc timing out in trunk,"""java.io.IOException: java.io.IOException: Unknown protocol: org.apache.hadoop.ipc.AvroRpcEngine$TunnelProtocol"" occurs while starting up the DN, and then it hangs waiting for the MiniCluster to start.",umamaheswararao,tlipcon,Critical,Closed,Fixed,02/Nov/11 19:11,28/Sep/15 20:58
Bug,HDFS-2541,12530413,"For a sufficiently large value of blocks, the DN Scanner may request a random number with a negative seed value.","Running off 0.20-security, I noticed that one could get the following exception when scanners are used:

{code}
DataXceiver 
java.lang.IllegalArgumentException: n must be positive 
at java.util.Random.nextInt(Random.java:250) 
at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.getNewBlockScanTime(DataBlockScanner.java:251) 
at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.addBlock(DataBlockScanner.java:268) 
at org.apache.hadoop.hdfs.server.datanode.DataXceiver.writeBlock(DataXceiver.java:432) 
at org.apache.hadoop.hdfs.server.datanode.DataXceiver.run(DataXceiver.java:122)
{code}

This is cause the period, determined in the DataBlockScanner (0.20+) or BlockPoolSliceScanner (0.23+), is cast to an integer before its sent to a Random.nextInt(...) call. For sufficiently large values of the long 'period', the casted integer may be negative. This is not accounted for. I'll attach a sample test that shows this possibility with the numbers.

We should ensure we do a Math.abs(...) before we send it to the Random.nextInt(...) call to avoid this.

With this bug, the maximum # of blocks a scanner may hold in its blocksMap without opening up the chance for beginning this exception (intermittent, as blocks continue to grow) would be 3582718.",qwertymaniac,qwertymaniac,Major,Closed,Fixed,05/Nov/11 08:38,19/Mar/13 17:59
Bug,HDFS-2543,12530640,HADOOP_PREFIX cannot be overriden,"hadoop-config.sh forces HADOOP_prefix to a specific value:
export HADOOP_PREFIX=`dirname ""$this""`/..

It would be nice to make this overridable.
",bmahe,bmahe,Major,Closed,Fixed,07/Nov/11 22:25,10/Mar/15 02:12
Bug,HDFS-2544,12530652,"Hadoop scripts unconditionally source ""$bin""/../libexec/hadoop-config.sh.",It would be nice to be able to specify some other location for hadoop-config.sh,bmahe,bmahe,Major,Closed,Fixed,08/Nov/11 00:02,05/Mar/12 02:49
Bug,HDFS-2545,12530675,Webhdfs: Support multiple namenodes in federation,DatanodeWebHdfsMethods only talks to the default namenode.  It won't work if there are multiple namenodes in federation.,szetszwo,szetszwo,Major,Closed,Fixed,08/Nov/11 03:01,10/Mar/15 02:12
Bug,HDFS-2547,12530958,ReplicationTargetChooser has incorrect block placement comments,"{code}
/** The class is responsible for choosing the desired number of targets
 * for placing block replicas.
 * The replica placement strategy is that if the writer is on a datanode,
 * the 1st replica is placed on the local machine, 
 * otherwise a random datanode. The 2nd replica is placed on a datanode
 * that is on a different rack. The 3rd replica is placed on a datanode
 * which is on the same rack as the **first replca**.
 */
{code}

That should read ""second replica"". The test cases confirm that this is the behavior, as well as the docs.",qwertymaniac,qwertymaniac,Trivial,Closed,Fixed,10/Nov/11 02:36,17/Oct/12 18:27
Bug,HDFS-2553,12531378,BlockPoolSliceScanner spinning in loop,"Playing with trunk, I managed to get a DataNode in a situation where the BlockPoolSliceScanner is spinning in the following loop, using 100% CPU:
        at org.apache.hadoop.hdfs.server.datanode.DataNode$BPOfferService.isAlive(DataNode.java:820)
        at org.apache.hadoop.hdfs.server.datanode.DataNode.isBPServiceAlive(DataNode.java:2962)
        at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner.scan(BlockPoolSliceScanner.java:625)
        at org.apache.hadoop.hdfs.server.datanode.BlockPoolSliceScanner.scanBlockPoolSlice(BlockPoolSliceScanner.java:614)
        at org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:95)
",umamaheswararao,tlipcon,Critical,Closed,Fixed,14/Nov/11 23:12,10/Mar/15 04:36
Bug,HDFS-2567,12532049,"When 0 DNs are available, show a proper error when trying to browse DFS via web UI","Trace:

{code}
HTTP ERROR 500

Problem accessing /nn_browsedfscontent.jsp. Reason:

    n must be positive
Caused by:

java.lang.IllegalArgumentException: n must be positive
	at java.util.Random.nextInt(Random.java:250)
	at org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:556)
	at org.apache.hadoop.net.NetworkTopology.chooseRandom(NetworkTopology.java:524)
	at org.apache.hadoop.hdfs.server.namenode.NamenodeJspHelper.getRandomDatanode(NamenodeJspHelper.java:372)
	at org.apache.hadoop.hdfs.server.namenode.NamenodeJspHelper.redirectToRandomDataNode(NamenodeJspHelper.java:383)
	at org.apache.hadoop.hdfs.server.namenode.nn_005fbrowsedfscontent_jsp._jspService(nn_005fbrowsedfscontent_jsp.java:70)
	at org.apache.jasper.runtime.HttpJspBase.service(HttpJspBase.java:98)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1221)
	at org.apache.hadoop.http.lib.StaticUserWebFilter$StaticUserFilter.doFilter(StaticUserWebFilter.java:109)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.apache.hadoop.http.HttpServer$QuotingInputFilter.doFilter(HttpServer.java:940)
	at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:410)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
{code}

Steps I did to run into this:

1. Start a new NN, freshly formatted.
2. No DNs yet.
3. Visit the DFS browser link {{http://localhost:50070/nn_browsedfscontent.jsp}}
4. Above error shows itself
5. {{hdfs dfs -touchz afile}}
6. Re-visit, still shows the same issue.

Perhaps its cause of no added DN so far.",qwertymaniac,qwertymaniac,Major,Closed,Fixed,19/Nov/11 14:26,05/Mar/12 02:49
Bug,HDFS-2573,12532095,"TestFiDataXceiverServer is failing, not testing OOME","TestFiDataXceiverServer is failing occasionally. It turns out also that the test is not testing the desired condition, because OOME is not caught in DataXceiverServer.run()",cos,shv,Major,Closed,Fixed,20/Nov/11 20:00,10/Mar/15 04:36
Bug,HDFS-2575,12532120,DFSTestUtil may create empty files,"DFSTestUtil creates files with random sizes, but there is no minimum size. So, sometimes, it can make a file with length 0. This will cause tests that use this functionality to fail - for example, TestListCorruptFileBlocks assumes that each of the created files has at least one block. We should add a minSize parameter to prevent this.",tlipcon,tlipcon,Minor,Closed,Fixed,21/Nov/11 07:40,10/Mar/15 02:12
Bug,HDFS-2588,12532505,hdfs jsp pages missing DOCTYPE [post-split branches],"Some jsp pages in the UI are missing a DOCTYPE declaration. This causes the pages to render incorrectly on some browsers, such as IE9.  Please see parent bug HADOOP-7827 for details and patch.",davevr,davevr,Trivial,Closed,Fixed,23/Nov/11 18:00,05/Mar/12 02:49
Bug,HDFS-2589,12532521,unnecessary hftp token fetch and renewal thread,"Instantiation of the hftp filesystem is causing a token to be implicitly created and added to a custom token renewal thread.  With the new token renewal feature in the JT, this causes the mapreduce {{obtainTokensForNamenodes}} to fetch two tokens (an implicit and uncancelled token, and an explicit token) and leave a spurious renewal thread running.  This thread should not be running in the JT.

After speaking with Owen, the quick solution is to lazy fetch the token, and to lazy start the renewer thread.",daryn,daryn,Major,Closed,Fixed,23/Nov/11 19:24,28/Dec/11 10:03
Bug,HDFS-2590,12532565,Some links in WebHDFS forrest doc do not work,Some links are pointing to DistributedFileSystem javadoc but the javadoc of DistributedFileSystem is not generated by default.,szetszwo,szetszwo,Major,Closed,Fixed,24/Nov/11 02:14,10/Mar/15 02:14
Bug,HDFS-2594,12532743,webhdfs HTTP API should implement getDelegationTokens() instead getDelegationToken(),"The current API returns a single delegation token, that method from the FileSystem API is deprecated in favor of the one that returns a list of tokens. The HTTP API should implement the new/undeprecated signature getDelegationTokens().",szetszwo,tucu00,Critical,Closed,Fixed,25/Nov/11 18:07,10/Mar/15 04:36
Bug,HDFS-2596,12532866,TestDirectoryScanner doesn't test parallel scans,"The code from HDFS-854 below doesn't run the test with parallel scanning. They probably intended ""parallelism < 3"".

{code}
+  public void testDirectoryScanner() throws Exception {
+    // Run the test with and without parallel scanning
+    for (int parallelism = 1; parallelism < 2; parallelism++) {
+      runTest(parallelism);
+    }
+  }
{code}",eli,eli,Major,Closed,Fixed,28/Nov/11 06:33,05/Mar/12 02:48
Bug,HDFS-2605,12533206,"CHANGES.txt has two ""Release 0.21.1"" sections","CHANGES.txt in hdfs-project has 2 sections titled ""Release 0.21.1 - Unreleased"". They are not identical, should be merged.",aw,shv,Major,Closed,Fixed,29/Nov/11 22:20,12/May/16 18:16
Bug,HDFS-2606,12533225,webhdfs client filesystem impl must set the content-type header for create/append,"Currently the content-type header is not being set and for some reason for append it is being set to the form encoded content type making jersey parameter parsing fail.

For this and to avoid any kind of proxy transcoding the content-type should be set to binary.

{code}
  conn.setRequestProperty(""Content-Type"", ""application/octet-stream"");
{code}
",tucu00,tucu00,Critical,Closed,Fixed,30/Nov/11 00:15,10/Mar/15 04:36
Bug,HDFS-2614,12529783,hadoop dist tarball is missing hdfs headers,It would be nice to provide hdfs header so one could easily write programs to be linked against that library and access HDFS,tucu00,bmahe,Major,Closed,Fixed,02/Nov/11 00:47,05/Mar/12 02:49
Bug,HDFS-2619,12533467,Remove my personal email address from the libhdfs build file.,"When I first wrote the libhdfs autoconf/automake stuff, I incorrectly put my email address in the AC_INIT line, which means if something goes wrong, you get:

{quote}
> configure: WARNING:     ## Report this to <my-email> ##
{quote}",omalley,omalley,Major,Closed,Fixed,01/Dec/11 16:04,11/Oct/12 17:46
Bug,HDFS-2637,12534106,The rpc timeout for block recovery is too low ,The RPC timeout for block recovery does not take into account that it issues multiple RPCs itself. This can cause recovery to fail if the network is congested or DNs are busy.,eli,eli,Major,Closed,Fixed,06/Dec/11 23:13,17/Oct/12 18:27
Bug,HDFS-2640,12534139,Javadoc generation hangs,Typing 'mvn javadoc:javadoc' causes the build to hang.,tomwhite,tomwhite,Major,Closed,Fixed,07/Dec/11 05:49,10/Mar/15 02:12
Bug,HDFS-2646,12534427,Hadoop HttpFS introduced 4 findbug warnings.,https://builds.apache.org/job/PreCommit-HDFS-Build/1665//artifact/trunk/hadoop-hdfs-project/patchprocess/newPatchFindbugsWarningshadoop-hdfs-httpfs.html,tucu00,umamaheswararao,Major,Closed,Fixed,09/Dec/11 04:14,10/Mar/15 04:36
Bug,HDFS-2649,12534500,eclipse:eclipse build fails for hadoop-hdfs-httpfs,"Building the eclipse:eclipse target fails in the hadoop-hdfs-httpfs project with this error:

[ERROR] Failed to execute goal org.apache.maven.plugins:maven-eclipse-plugin:2.8:eclipse (default-cli) on project hadoop-hdfs-httpfs: Request to merge when 'filtering' is not identical. Original=resource src/main/resources: output=target/classes, include=[httpfs.properties], exclude=[**/*.java], test=false, filtering=true, merging with=resource src/main/resources: output=target/classes, include=[], exclude=[httpfs.properties|**/*.java], test=false, filtering=false -> [Help 1]

This appears to be the same type of issue as reported in HADOOP-7567.",jlowe,jlowe,Major,Closed,Fixed,09/Dec/11 17:05,10/Mar/15 04:36
Bug,HDFS-2657,12534625,TestHttpFSServer and TestServerWebApp are failing on trunk,">>> org.apache.hadoop.fs.http.server.TestHttpFSServer.instrumentation
>>> org.apache.hadoop.lib.servlet.TestServerWebApp.lifecycle",tucu00,eli,Major,Closed,Fixed,10/Dec/11 22:01,19/Apr/22 05:34
Bug,HDFS-2658,12534626,HttpFS introduced 70 javadoc warnings,"{noformat}
hadoop1 (trunk)$ grep warning javadoc.txt |grep -c httpfs
70
{noformat}",tucu00,eli,Major,Closed,Fixed,10/Dec/11 22:08,10/Mar/15 04:35
Bug,HDFS-2673,12534993,"While Namenode processing the blocksBeingWrittenReport, it will log incorrect number blocks count","In NameNode#blocksBeingWrittenReport
 we have the following stateChangeLog
{code}
stateChangeLog.info(""*BLOCK* NameNode.blocksBeingWrittenReport: ""
           +""from ""+nodeReg.getName()+"" ""+blocks.length +"" blocks"");
{code}

here blocks is long array. Every consecutive 3 elements represents a block ( length, blockid, genstamp).

So, here in log message, blocks.length should be blocks.length/3.

 ",umamaheswararao,umamaheswararao,Trivial,Closed,Fixed,13/Dec/11 18:12,28/Dec/11 10:03
Bug,HDFS-2676,12535067,Remove Avro RPC,"Please see the discussion in HDFS-2660 for more details. I have created a branch HADOOP-6659 to save the Avro work, if in the future some one wants to use the work that existed to add support for Avro RPC.",sureshms,sureshms,Major,Closed,Fixed,14/Dec/11 00:48,28/Sep/15 20:58
Bug,HDFS-2690,12535379,trunk doesn't build on clean of .m2 - /bkjournal/pom.xml error,"I cleaned my .m2 and did fresh checkout of trunk and I get the following error:

[tgraves@pvvirt10-pv1 trunk]$ mvn clean install -Pdist -Dtar  
/home/y/libexec/maven/bin/mvn clean install -Pdist -Dtar
[INFO] Scanning for projects...
[ERROR] The build could not read 1 project -> [Help 1]
[ERROR]   
[ERROR]   The project org.apache.hadoop.contrib:hadoop-hdfs-bkjournal:0.24.0-SNAPSHOT (/home/tgraves/testpatch/trunk/hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/pom.xml) has 1 error
[ERROR]     Non-resolvable parent POM: Could not find artifact org.apache.hadoop:hadoop-project-dist:pom:0.24.0-SNAPSHOT and 'parent.relativePath' points at wrong local POM @ line 17, column 11 -> [Help 2]
[ERROR] 

It seems the bkournal/pom.xml has the wrong path to the parent pom.xml, adding another ../ to the relativePath in the pom seems to fix the issue.

--- hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/pom.xml       (revision 1214962)
+++ hadoop-hdfs-project/hadoop-hdfs/src/contrib/bkjournal/pom.xml       (working copy)
@@ -18,7 +18,7 @@
     <groupId>org.apache.hadoop</groupId>
     <artifactId>hadoop-project-dist</artifactId>
     <version>0.24.0-SNAPSHOT</version>
-    <relativePath>../../../../hadoop-project-dist</relativePath>
+    <relativePath>../../../../../hadoop-project-dist</relativePath>
   </parent>
 
   <groupId>org.apache.hadoop.contrib</groupId>
",tgraves,tgraves,Blocker,Resolved,Fixed,15/Dec/11 22:15,10/Mar/15 04:36
Bug,HDFS-2694,12535429,Removal of Avro broke non-PB NN services,"RpcEngine implementations have to register themselves associated with an RpcKind. Both WritableRpcEngine and ProtobufRpcEngine do this registration in static initialization blocks. It used to be that the static initializer block for WritableRpcEngine was triggered when AvroRpcEngine was initialized, since this instantiated a WritableRpcEngine object. With AvroRpcEngine gone, there's nothing in the NN to trigger the WritableRpcEngine static initialization block. Therefore, those RPC services which still use Writable and not PB no longer work.

For example, if I try to run `hdfs groups' on trunk, which uses the GetUserMappingsProtocol, this error gets spit out:

{noformat}
$ hdfs groups
log4j:ERROR Could not find value for key log4j.appender.NullAppender
log4j:ERROR Could not instantiate appender named ""NullAppender"".
Exception in thread ""main"" java.io.IOException: Unknown rpc kind RPC_WRITABLE
	at org.apache.hadoop.ipc.Client.call(Client.java:1136)
	at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:205)
	at $Proxy6.getGroupsForUser(Unknown Source)
	at org.apache.hadoop.tools.GetGroupsBase.run(GetGroupsBase.java:71)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:69)
	at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:83)
	at org.apache.hadoop.hdfs.tools.GetGroups.main(GetGroups.java:56)
{noformat}",atm,atm,Major,Closed,Fixed,16/Dec/11 09:08,28/Sep/15 20:58
Bug,HDFS-2696,12535513,Fix the fuse-fds build,"fuse-dfs has some compilation problems on the 0.23 and 0.24(head) branches. Some details on how to compile fuse-dfs on 0.23 is here : http://mail-archives.apache.org/mod_mbox/hadoop-hdfs-user/201112.mbox/%3C4EE23195.7030002@gmail.com%3E and a wiki page follows. 

The attached patch removes some problems (an inexistnant ivy/library.properties, detecting jni.h location).",bmahe,rigacrypto,Major,Closed,Fixed,16/Dec/11 21:34,28/Sep/15 20:58
Bug,HDFS-2698,12535548,BackupNode is downloading image from NameNode for every checkpoint,"BackupNode can make periodic checkpoints without downloading image and edits files from the NameNode, but with just saving the namespace to local disks. This is not happening because NN renews checkpoint time after every checkpoint, thus making its image ahead of the BN's even though they are in sync.",shv,shv,Major,Resolved,Fixed,17/Dec/11 01:06,29/Dec/11 11:58
Bug,HDFS-2700,12535579,TestDataNodeMultipleRegistrations is failing in trunk,"TestDataNodeMultipleRegistrations  is failing from last couple of builds
https://builds.apache.org/job/PreCommit-HDFS-Build/lastCompletedBuild/testReport/
",umamaheswararao,umamaheswararao,Major,Closed,Fixed,17/Dec/11 16:23,28/Sep/15 20:58
Bug,HDFS-2702,12535592,A single failed name dir can cause the NN to exit ,"There's a bug in FSEditLog#rollEditLog which results in the NN process exiting if a single name dir has failed. Here's the relevant code:

{code}
close()  // So editStreams.size() is 0 
foreach edits dir {
  ..
  eStream = new ...  // Might get an IOE here
  editStreams.add(eStream);
} catch (IOException ioe) {
  removeEditsForStorageDir(sd);  // exits if editStreams.size() <= 1  
}
{code}

If we get an IOException before we've added two edits streams to the list we'll exit, eg if there's an error processing the 1st name dir we'll exit even if there are 4 valid name dirs. The fix is to move the checking out of removeEditsForStorageDir (nee processIOError) or modify it so it can be disabled in some cases, eg here where we don't yet know how many streams are valid.",eli,eli,Critical,Closed,Fixed,17/Dec/11 20:00,21/Apr/12 01:51
Bug,HDFS-2703,12535593,removedStorageDirs is not updated everywhere we remove a storage dir,"There are a number of places (FSEditLog#open, purgeEditLog, and rollEditLog) where we remove a storage directory but don't add it to the removedStorageDirs list. This means a storage dir may have been removed but we don't see it in the log or Web UI. This doesn't affect trunk/23 since the code there is totally different.",eli,eli,Major,Closed,Fixed,17/Dec/11 20:03,05/Apr/12 18:42
Bug,HDFS-2705,12535741,HttpFS server should check that upload requests have correct content-type,"The append/create requests should require 'application/octet-stream' as content-type when uploading data. This is to prevent the default content-type form-encoded (used as default by some HTTP libraries) to be used or text based content-types to be used.

If the form-encoded content type is used, then Jersey tries to process the upload stream as parameters
If a test base content-type is used, HTTP proxies/gateways could do attempt some transcoding on the stream thus corrupting the data.",tucu00,tucu00,Major,Closed,Fixed,19/Dec/11 18:57,08/Dec/16 02:42
Bug,HDFS-2706,12535774,Use configuration for blockInvalidateLimit if it is set,"HDFS-2191 accidentally removed the following code.
{code}
- this.blockInvalidateLimit = conf.getInt(
-        DFSConfigKeys.DFS_BLOCK_INVALIDATE_LIMIT_KEY, this.blockInvalidateLimit);
{code}",szetszwo,szetszwo,Major,Closed,Fixed,20/Dec/11 01:56,10/Mar/15 02:12
Bug,HDFS-2707,12535896,HttpFS should read the hadoop-auth secret from a file instead inline from the configuration,"Similar to HADOOP-7621, the secret should be in a file other than the configuration file.",tucu00,tucu00,Major,Closed,Fixed,20/Dec/11 20:21,10/Mar/15 04:36
Bug,HDFS-2710,12535941,"HDFS part of MAPREDUCE-3529, HADOOP-7933","viewfs implementation of getDelegationTokens(String, Credentials)",,sseth,Critical,Closed,Fixed,21/Dec/11 00:42,10/Mar/15 02:12
Bug,HDFS-2718,12536197,Optimize OP_ADD in edits loading,"During loading the edits journal FSEditLog.loadEditRecords() processes OP_ADD inefficiently. It first removes the existing INodeFile from the directory tree, then adds it back as a regular INodeFile, and then replaces it with INodeFileUnderConstruction if files is not closed. This slows down edits loading. OP_ADD should be done in one shot and retain previously existing data.",shv,shv,Major,Resolved,Fixed,22/Dec/11 19:35,10/Mar/15 04:35
Bug,HDFS-2722,12536335,HttpFs shouldn't be using an int for block size,"{{./hadoop-hdfs-project/hadoop-hdfs-httpfs/src/main/java/org/apache/hadoop/fs/http/server/FSOperations.java: blockSize = fs.getConf().getInt(""dfs.block.size"", 67108864);}}

Should instead be using dfs.blocksize and should instead be long.

I'll post a patch for this after HDFS-1314 is resolved -- which changes the internal behavior a bit (should be getLongBytes, and not just getLong, to gain formatting advantages).",qwertymaniac,qwertymaniac,Major,Closed,Fixed,24/Dec/11 05:56,05/Mar/12 02:49
Bug,HDFS-2725,12536540,"hdfs script usage information is missing the information about ""dfs"" command","hdfs script does not print the command ""dfs"" in the usage.",,prashant,Major,Resolved,Fixed,28/Dec/11 23:38,10/Mar/15 04:36
Bug,HDFS-2728,12536572,Remove dfsadmin -printTopology from branch-1 docs since it does not exist,"It is documented we have -printTopology but we do not really have it in this branch. Possible docs mixup from somewhere in security branch pre-merge?

{code}
➜  branch-1  grep printTopology -R .
./src/docs/src/documentation/content/xdocs/.svn/text-base/hdfs_user_guide.xml.svn-base:      <code>-printTopology</code>
./src/docs/src/documentation/content/xdocs/hdfs_user_guide.xml:      <code>-printTopology</code>
{code}

Lets remove the reference.",qwertymaniac,qwertymaniac,Minor,Closed,Fixed,29/Dec/11 11:48,17/Oct/12 18:27
Bug,HDFS-2739,12536719,SecondaryNameNode doesn't start up,"Built a 0.24-SNAPSHOT tar from today, used a general config, started NN/DN, but SNN won't come up with following error:

{code}
11/12/31 12:13:14 ERROR namenode.SecondaryNameNode: Throwable Exception in doCheckpoint
java.lang.RuntimeException: java.lang.NoSuchFieldException: versionID
	at org.apache.hadoop.ipc.RPC.getProtocolVersion(RPC.java:154)
	at org.apache.hadoop.ipc.WritableRpcEngine$Invocation.<init>(WritableRpcEngine.java:112)
	at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:226)
	at $Proxy9.getTransationId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:185)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:625)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:633)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:386)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:356)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.NoSuchFieldException: versionID
	at java.lang.Class.getField(Class.java:1520)
	at org.apache.hadoop.ipc.RPC.getProtocolVersion(RPC.java:150)
	... 9 more
java.lang.RuntimeException: java.lang.NoSuchFieldException: versionID
	at org.apache.hadoop.ipc.RPC.getProtocolVersion(RPC.java:154)
	at org.apache.hadoop.ipc.WritableRpcEngine$Invocation.<init>(WritableRpcEngine.java:112)
	at org.apache.hadoop.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:226)
	at $Proxy9.getTransationId(Unknown Source)
	at org.apache.hadoop.hdfs.protocolPB.NamenodeProtocolTranslatorPB.getTransactionID(NamenodeProtocolTranslatorPB.java:185)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.countUncheckpointedTxns(SecondaryNameNode.java:625)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.shouldCheckpointBasedOnCount(SecondaryNameNode.java:633)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.doWork(SecondaryNameNode.java:386)
	at org.apache.hadoop.hdfs.server.namenode.SecondaryNameNode.run(SecondaryNameNode.java:356)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.NoSuchFieldException: versionID
	at java.lang.Class.getField(Class.java:1520)
	at org.apache.hadoop.ipc.RPC.getProtocolVersion(RPC.java:150)
	... 9 more
11/12/31 12:13:14 INFO namenode.SecondaryNameNode: SHUTDOWN_MSG: 
/************************************************************
SHUTDOWN_MSG: Shutting down SecondaryNameNode at sho-mba.local/192.168.11.2
************************************************************/
{code}

full error log: http://pastebin.com/mSaVbS34


",jnp,sho.shimauchi,Critical,Closed,Fixed,31/Dec/11 03:45,28/Sep/15 20:58
Bug,HBASE-3408,12494561,AssignmentManager NullPointerException,"If AssignmentManager tries to move a region to an invalid destination server, rather than choosing a random server as intended, it throws an NPE.

Line 1009 should check if existingPlan.getDestination()!=null:

 if (existingPlan == null || forceNewPlan ||
          (existingPlan.getDestination() != null && existingPlan.getDestination().equals(serverToExclude))) {

I triggered it by trying to manually move regions around, probably to an invalid destination server.  I'm not currently able to build the project to test if that's the extent of the problem, so here's a little more info...  

It leaves a stranded region-in-transition until the master and/or regionserver are restarted and causes problems like the following.  ""hbck -fix"" was unable to repair it.

2011-01-04 00:14:10,948 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Scanned 4287 catalog row(s) and gc'd 0 unreferenced parent region(s)
2011-01-04 00:14:18,574 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because 1 region(s) in transition: {23ebce9a5d174f87bfb96ed1da387fdc=RandomValue,,1291219068335.23ebce9a5d174f87bfb96ed1da387fdc. state=OFFLINE, ts=1294118046139}
2011-01-04 00:14:36,142 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  RandomValue,,1291219068335.23ebce9a5d174f87bfb96ed1da387fdc. state=OFFLINE, ts=1294118046139
2011-01-04 00:14:36,142 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been OFFLINE for too long, reassigning RandomValue,,1291219068335.23ebce9a5d174f87bfb96ed1da387fdc. to a random server
2011-01-04 00:14:36,142 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=RandomValue,,1291219068335.23ebce9a5d174f87bfb96ed1da387fdc. state=OFFLINE, ts=1294118046139
2011-01-04 00:14:36,142 ERROR org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor: Caught exception
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.AssignmentManager.getRegionPlan(AssignmentManager.java:934) (i think this is .90.0RC1, so same bug on a different line number)
        at org.apache.hadoop.hbase.master.AssignmentManager.getRegionPlan(AssignmentManager.java:909)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:822)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:663)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:643)
        at org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor.chore(AssignmentManager.java:1481)
        at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
",,mcorgan,Major,Closed,Fixed,04/Jan/11 05:22,20/Nov/15 12:41
Bug,HBASE-3409,12494574,Failed server shutdown processing when retrying hlog split,"2011-01-04 01:14:17,353 WARN org.apache.hadoop.hbase.master.MasterFileSystem: Retrying splitting because of:
org.apache.hadoop.hbase.regionserver.wal.OrphanHLogAfterSplitException: Discovered orphan hlog after split. Maybe the HRegionServer was not dead when we started
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:286)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:187)
        at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:196)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:96)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
2011-01-04 01:14:17,353 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_META_SERVER_SHUTDOWN
java.lang.IllegalStateException: An HLogSplitter instance may only be used once
        at com.google.common.base.Preconditions.checkState(Preconditions.java:145)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:170)
        at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:199)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:96)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
",stack,tlipcon,Blocker,Closed,Fixed,04/Jan/11 09:23,20/Nov/15 12:41
Bug,HBASE-3410,12494593,Unable to set/modify TTL on a column family using the shell.,"When attempting to set the TTL parameter on a column family using the HBase shell, the following error is reported and the parameter is not modified:


hbase(main):042:0> create 't1', {NAME => 'f1', VERSIONS => 1, TTL => 2592000, BLOCKCACHE => true}

ERROR: uninitialized constant Hbase::Admin::HColumnDescriptor
",stack,herberts,Major,Closed,Fixed,04/Jan/11 13:49,20/Nov/15 12:42
Bug,HBASE-3412,12494640,HLogSplitter should handle missing HLogs,"In build #48 (https://hudson.apache.org/hudson/job/hbase-0.90/48/), TestReplication failed because of missing rows on the slave cluster. The reason is that a region server that was killed was able to archive a log at the same time the master was trying to recover it:

{noformat}
[MASTER_META_SERVER_OPERATIONS-vesta.apache.org:47907-0] util.FSUtils(625):
 Recovering file hdfs://localhost:50121/user/hudson/.logs/vesta.apache.org,58598,1294117333857/vesta.apache.org%3A58598.1294117406909
...
[RegionServer:0;vesta.apache.org,58598,1294117333857.logRoller] wal.HLog(740):
 moving old hlog file /user/hudson/.logs/vesta.apache.org,58598,1294117333857/vesta.apache.org%3A58598.1294117406909
 whose highest sequenceid is 422 to /user/hudson/.oldlogs/vesta.apache.org%3A58598.1294117406909
...
[MASTER_META_SERVER_OPERATIONS-vesta.apache.org:47907-0] master.MasterFileSystem(204):
 Failed splitting hdfs://localhost:50121/user/hudson/.logs/vesta.apache.org,58598,1294117333857
 java.io.IOException: Failed to open hdfs://localhost:50121/user/hudson/.logs/vesta.apache.org,58598,1294117333857/vesta.apache.org%3A58598.1294117406909 for append
Caused by: org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException:
 org.apache.hadoop.hdfs.server.namenode.LeaseExpiredException:
 No lease on /user/hudson/.logs/vesta.apache.org,58598,1294117333857/vesta.apache.org%3A58598.1294117406909
 File does not exist. [Lease.  Holder: DFSClient_-986975908, pendingcreates: 1]
{noformat}

We should probably just handle the fact that a file could have been archived (maybe even check in .oldlogs to be sure) and move on to the next log.",jdcryans,jdcryans,Critical,Closed,Fixed,04/Jan/11 20:11,20/Nov/15 12:42
Bug,HBASE-3416,12494729,"For intra-row scanning, the update readers notification resets the query matcher and can lead to incorrect behavior","In {{StoreScanner.resetScannerStack()}}, which is called on the first {{next()}} call after readers have been updated, we do a query matcher reset.  Normally this is not an issue because the query matcher does not need to maintain state between rows.  However, if doing intra-row scanning w/ the specified limit, we could have the query matcher reset in the middle of reading a row.  This could lead to incorrect behavior (too many versions coming back, etc).",apurtell,streamy,Major,Closed,Fixed,05/Jan/11 15:48,20/Nov/15 12:41
Bug,HBASE-3417,12494730,"CacheOnWrite is using the temporary output path for block names, need to use a more consistent block naming scheme","Currently the block names used in the block cache are built using the filesystem path.  However, for cache on write, the path is a temporary output file.

The original COW patch actually made some modifications to block naming stuff to make it more consistent but did not do enough.  Should add a separate method somewhere for generating block names using some more easily mocked scheme (rather than just raw path as we generate a random unique file name twice, once for tmp and then again when moved into place).",streamy,streamy,Critical,Closed,Fixed,05/Jan/11 16:02,20/Nov/15 12:40
Bug,HBASE-3418,12494739,Increment operations can break when qualifiers are split between memstore/snapshot and storefiles,"Doing investigation around some observed resetting counter behavior.

An optimization was added to check memstore/snapshots first and then check storefiles if not all counters were found.  However it looks like this introduced a bug when columns for a given row/family in a single increment operation are spread across memstores and storefiles.

The results from get operations on both memstores and storefiles are appended together but when processed are expected to be fully sorted.  This can lead to invalid results.

Need to sort the combined result of memstores + storefiles.",streamy,streamy,Critical,Closed,Fixed,05/Jan/11 17:10,20/Nov/15 12:40
Bug,HBASE-3419,12494745,"If re-transition to OPENING during log replay fails, server aborts.  Instead, should just cancel region open.","The {{Progressable}} used on region open to tickle the ZK OPENING node to prevent the master from timing out a region open operation will currently abort the RegionServer if this fails for some reason.  However it could be ""normal"" for an RS to have a region open operation aborted by the master, so should just handle as it does other places by reverting the open.

We had a cluster trip over some other issue (for some reason, the tickle was not happening in < 30 seconds, so master was timing out every time).  Because of the abort on BadVersion, this eventually led to every single RS aborting itself eventually taking down the cluster.",streamy,streamy,Critical,Closed,Fixed,05/Jan/11 17:36,20/Nov/15 12:41
Bug,HBASE-3420,12494748,"Handling a big rebalance, we can queue multiple instances of a Close event; messes up state","This is pretty ugly.  In short, on a heavily loaded cluster, we are queuing multiple instances of region close.  They all try to run confusing state.

Long version:

I have a messy cluster.  Its 16k regions on 8 servers.  One node has 5k or so regions on it.  Heaps are 1G all around.  My master had OOME'd.  Not sure why but not too worried about it for now.  So, new master comes up and is trying to rebalance the cluster:

{code}
2011-01-05 00:48:07,385 INFO org.apache.hadoop.hbase.master.LoadBalancer: Calculated a load balance in 14ms. Moving 3666 regions off of 6 overloaded servers onto 3 less loaded servers
{code}

The balancer ends up sending many closes to a single overloaded server are taking so long, the close times out in RIT.  We then do this:

{code}
              case CLOSED:
                LOG.info(""Region has been CLOSED for too long, "" +
                    ""retriggering ClosedRegionHandler"");
                AssignmentManager.this.executorService.submit(
                    new ClosedRegionHandler(master, AssignmentManager.this,
                        regionState.getRegion()));
                break;
{code}

We queue a new close (Should we?).

We time out a few more times (9 times) and each time we queue a new close.

Eventually the close succeeds, the region gets assigned a new location.

Then the next close pops off the eventhandler queue.

Here is the telltale signature of stuff gone amiss:

{code}
2011-01-05 00:52:19,379 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=TestTable,0487405776,1294125523541.b1fa38bb610943e9eadc604babe4d041. state=OPEN, ts=1294188709030
{code}

Notice how state is OPEN when we are forcing offline (It was actually just successfully opened).  We end up assigning same server because plan was still around:

{code}
2011-01-05 00:52:20,705 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Attempted open of TestTable,0487405776,1294125523541.b1fa38bb610943e9eadc604babe4d041. but already online on this server
{code}

But later when plan is cleared, we assign new server and we have dbl-assignment.



",stack,stack,Major,Closed,Fixed,05/Jan/11 17:51,20/Nov/15 12:40
Bug,HBASE-3421,12494769,Very wide rows -- 30M plus -- cause us OOME,"From the list, see 'jvm oom' in http://mail-archives.apache.org/mod_mbox/hbase-user/201101.mbox/browser, it looks like wide rows -- 30M or so -- causes OOME during compaction.  We should check it out. Can the scanner used during compactions use the 'limit' when nexting?  If so, this should save our OOME'ing (or, we need to add to the next a max size rather than count of KVs).",nputnam,stack,Major,Closed,Fixed,05/Jan/11 21:33,20/Nov/15 12:41
Bug,HBASE-3423,12494790,hbase-env.sh over-rides HBASE_OPTS incorrectly.,"conf/hbase-env.sh has the following line:

   export HBASE_OPTS=""-ea -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode""

This should be

   export HBASE_OPTS=""$HBASE_OPTS -ea -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode""

",,tdunning,Major,Closed,Fixed,06/Jan/11 01:08,20/Nov/15 12:43
Bug,HBASE-3430,12494977,hbase-daemon.sh should clean up PID files on process stop,"I just did a simple watchdog script for internal use to enable HBase process restarts on failure.  While most of it was easy, I found no way to distinguish between a process failure and intentional stop, since the HBase scripts leave PID files sitting around after shutdown.

I think it would be generally useful for hbase-daemon.sh to cleanup PID files on successful process stop.  This is really just a matter of adding ""rm $pid"" after stop completes.  Any concerns/objections?",ghelmling,ghelmling,Trivial,Closed,Fixed,07/Jan/11 19:06,20/Nov/15 12:43
Bug,HBASE-3431,12495008,Regionserver is not using the name given it by the master; double entry in master listing of servers,"Our man Ted Dunning found the following where RS checks in with one name, the master tells it use another name but we seem to go ahead and continue with our original name.

In RS logs I see:

{code}
2011-01-07 15:45:50,757 INFO  org.apache.hadoop.hbase.regionserver.HRegionServer [regionserver60020]: Master passed us address to use. Was=perfnode11:60020, Now=10.10.30.11:60020
{code}


On master I see

{code}
2011-01-07 15:45:38,613 INFO  org.apache.hadoop.hbase.master.ServerManager [IPC Server handler 0 on 60000]: Registering server=10.10.30.11,60020,1294443935414, regionCount=0, userLoad=false
{code}

....

then later

{code}
2011-01-07 15:45:44,247 INFO  org.apache.hadoop.hbase.master.ServerManager [IPC Server handler 2 on 60000]: Registering server=perfnode11,60020,1294443935414, regionCount=0, userLoad=true
{code}

This might be since we started letting servers register in other than with the reportStartup.",stack,stack,Blocker,Closed,Fixed,08/Jan/11 01:39,20/Nov/15 12:44
Bug,HBASE-3440,12495460,Clean out load_table.rb and make sure all roads lead to completebulkload tool,Up on list Vidhya tried using load_table.rb with 0.90 and new master and it don't work any more now we assign differently.  Clean out this script.  Make sure all doc points at completebulkload tool instead.,vidhyash,stack,Major,Closed,Fixed,13/Jan/11 06:03,20/Nov/15 12:41
Bug,HBASE-3443,12495555,ICV optimization to look in memstore first and then store files (HBASE-3082) does not work when deletes are in the mix,"For incrementColumnValue() HBASE-3082 adds an optimization to check memstores first, and only if not present in the memstore then check the store files. In the presence of deletes, the above optimization is not reliable.

If the column is marked as deleted in the memstore, one should not look further into the store files. But currently, the code does so.

Sample test code outline:

{code}
admin.createTable(desc)

table = HTable.new(conf, tableName)

table.incrementColumnValue(Bytes.toBytes(""row""), cf1name, Bytes.toBytes(""column""), 5);

admin.flush(tableName)
sleep(2)

del = Delete.new(Bytes.toBytes(""row""))
table.delete(del)

table.incrementColumnValue(Bytes.toBytes(""row""), cf1name, Bytes.toBytes(""column""), 5);

get = Get.new(Bytes.toBytes(""row""))
keyValues = table.get(get).raw()
keyValues.each do |keyValue|
  puts ""Expect 5; Got Value=#{Bytes.toLong(keyValue.getValue())}"";
end
{code}

The above prints:
{code}
Expect 5; Got Value=10
{code}
",larsh,kannanm,Critical,Closed,Fixed,13/Jan/11 23:18,18/Sep/13 22:13
Bug,HBASE-3444,12495604,Test to prove Bytes.toBytesBinary and Bytes.toStringBinary()  is reversible,"Bytes.toStringBinary() doesn't escape \.

Otherwise the transformation isn't reversible

byte[] a = {'\', 'x' , '0', '0'}

Bytes.toBytesBinary(Bytes.toStringBinary(a)) won't be equal to a
",dallmkp,khemani,Minor,Closed,Fixed,14/Jan/11 17:14,23/Sep/13 18:31
Bug,HBASE-3445,12495617,Master crashes on data that was moved from different host,"While testing an upgrade to 0.90.0 RC3 I noticed that if I seeded our test data on one machine and transferred to another machine the HMaster on the new machine dies on startup.

Based on the following stack trace it looks as though it is attempting to find the .meta region with the ip address of the original machine.  Instead of waiting around for RegionServer's to register with new location data, HMaster throws it's hands up with a FATAL exception.

Note that deleting the zookeeper dir makes no difference.

Also note that so far I have only reproduced this in my own environment using the hbase-trx extension of HBase and an ApplicationStarter that starts the Master and RegionServer together in the same JVM.  While the issue seems likely isolated from those factors it is far from a vanilla HBase environment.

I will spend some time trying to reproduce the issue in a proper hbase test.  But perhaps someone can beat me to it?  How do I simulate the IP switch? May require a data.tar upload. 

[14/01/11 10:45:20] 6396   [     Thread-298] ERROR server.quorum.QuorumPeerConfig  - Invalid configuration, only one server specified (ignoring)
[14/01/11 10:45:21] 7178   [           main] INFO  ion.service.HBaseRegionService  - troove> region port:       60010
[14/01/11 10:45:21] 7180   [           main] INFO  ion.service.HBaseRegionService  - troove> region interface:  org.apache.hadoop.hbase.ipc.IndexedRegionInterface
[14/01/11 10:45:21] 7180   [           main] INFO  ion.service.HBaseRegionService  - troove> root dir: hdfs://localhost:8701/hbase
[14/01/11 10:45:21] 7180   [           main] INFO  ion.service.HBaseRegionService  - troove> Initializing region server.
[14/01/11 10:45:21] 7631   [           main] INFO  ion.service.HBaseRegionService  - troove> Starting region server thread.
[14/01/11 10:46:54] 100764 [        HMaster] FATAL he.hadoop.hbase.master.HMaster  - Unhandled exception. Starting shutdown.
java.net.SocketTimeoutException: 20000 millis timeout while waiting for channel to be ready for connect. ch : java.nio.channels.SocketChannel[connection-pending remote=192.168.1.102/192.168.1.102:60020]
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:213)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:404)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:311)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:865)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:732)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:258)
	at $Proxy14.getProtocolVersion(Unknown Source)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:419)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:393)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:444)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:349)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:954)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:384)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:283)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyMetaRegionLocation(CatalogTracker.java:478)
	at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:435)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:382)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:277)
",stack,jk-public@troove.net,Critical,Closed,Fixed,14/Jan/11 19:14,20/Nov/15 12:43
Bug,HBASE-3446,12495731,"ProcessServerShutdown fails if META moves, orphaning lots of regions","I ran a rolling restart on a 5 node cluster with lots of regions, and afterwards had LOTS of regions left orphaned. The issue appears to be that ProcessServerShutdown failed because the server hosting META was restarted around the same time as another server was being processed",stack,tlipcon,Blocker,Closed,Fixed,17/Jan/11 02:27,20/Nov/15 12:41
Bug,HBASE-3447,12495739,Split parents ending up deployed along with daughters,"Testing rc3 got several regions in this state as reported by hbck:
ERROR: Region UNKNOWN_REGION on haus02.sf.cloudera.com:57020, key=9f2822a04028c86813fe71264da5c167, not on HDFS or in META but deployed on haus02.sf.cloudera.com:57020
(this without any injected failures or anything)",stack,tlipcon,Blocker,Closed,Fixed,17/Jan/11 06:06,20/Nov/15 12:42
Bug,HBASE-3449,12495828,Server shutdown handlers deadlocked waiting for META,"I have a situation where both of my MASTER_META_SERVER_OPERATIONS handlers are handling server shutdowns, and both of them are waiting on ROOT, which isn't coming up. Unclear exactly how this happened, but I triggered it by doing a rolling restart.",stack,tlipcon,Blocker,Closed,Fixed,17/Jan/11 21:35,20/Nov/15 12:41
Bug,HBASE-3456,12496192,Fix hardcoding of 20 second socket timeout down in HBaseClient,"There is this code in HBaseClient:

{code}
            NetUtils.connect(this.socket, remoteId.getAddress(), 20000);
{code}

We need to be able to set this if only for testing.",apurtell,stack,Major,Closed,Fixed,20/Jan/11 22:57,20/Nov/15 12:43
Bug,HBASE-3465,12496401,Hbase should use a HADOOP_HOME environment variable if available.,"I have been burned a few times lately while developing code by having the make sure that the hadoop jar in hbase/lib is exactly correct.  In my own deployment, there are actually 3 jars and a native library to keep in sync that hbase shouldn't have to know about explicitly.  A similar problem arises when using stock hbase with CDH3 because of the security patches changing the wire protocol.

All of these problems could be avoided by not assuming that the hadoop library is in the local directory.  Moreover, I think it might be possible to assemble the distribution such that the compile time hadoop dependency is in a cognate directory to lib and is referenced using a default value for HADOOP_HOME.

Does anybody have any violent antipathies to such a change?",tucu00,tdunning,Major,Closed,Fixed,23/Jan/11 00:30,20/Nov/15 12:43
Bug,HBASE-3476,12496639,HFile -m option need not scan key values,bin/hbase org.apache.hadoop.io.hfile.HFile -m -f <filename> doesn't have to scan the KVs in the file,khemani,khemani,Minor,Closed,Fixed,25/Jan/11 07:39,20/Nov/15 12:42
Bug,HBASE-3478,12496750,HBase fails to recover from failed DNS resolution of stale meta connection info,"This looks like a variant of HBASE-3445:

One of our developers ran a seed program with configuration A to generate some test data on his local machine. He then moved that data into a development environment on the same machine with a different hbase configuration B.

On startup the HMaster waits for new regionserver to register itself:

[25/01/11 15:37:25] 162161 [  HRegionServer] INFO  ase.regionserver.HRegionServer  - Telling master at 10.0.1.4:7801 that we are up
[25/01/11 15:37:25] 162165 [ice-EventThread] DEBUG .hadoop.hbase.zookeeper.ZKUtil  - master:7801-0x12dbf879abe0000 Retrieved 13 byte(s) of data from znode /hbase/rs/10.0.1.4,7802,1295998613814 and set watcher; 10.0.1.4:7802

Then ROOT region comes online at the right place: 10.0.1.4,7802

[25/01/11 15:37:31] 168369 [yTasks:70236052] INFO  ase.catalog.RootLocationEditor  - Setting ROOT region location in ZooKeeper as 10.0.1.4:7802
3:57 [25/01/11 15:37:31] 168408 [10.0.1.4:7801-0] DEBUG er.handler.OpenedRegionHandler  - Opened region -ROOT-,,0.70236052 on 10.0.1.4,7802,1295998613814

But then HMaster chokes on the stale META region location.

[25/01/11 15:37:31] 168448 [        HMaster] ERROR he.hadoop.hbase.HServerAddress  - Could not resolve the DNS name of warren:60020
[25/01/11 15:37:31] 168448 [        HMaster] FATAL he.hadoop.hbase.master.HMaster  - Unhandled exception. Starting shutdown.
java.lang.IllegalArgumentException: Could not resolve the DNS name of warren:60020
   at org.apache.hadoop.hbase.HServerAddress.checkBindAddressCanBeResolved(HServerAddress.java:105)
   at org.apache.hadoop.hbase.HServerAddress.<init>(HServerAddress.java:66)
   at org.apache.hadoop.hbase.catalog.MetaReader.readLocation(MetaReader.java:344)
   at org.apache.hadoop.hbase.catalog.MetaReader.readMetaLocation(MetaReader.java:281)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:280)
   at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyMetaRegionLocation(CatalogTracker.java:482)
   at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:435)
   at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:382)
   at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:277)
   at java.lang.Thread.run(Thread.java:680)

First of all, we do not yet understand why in configuration A the RegionInfo resolved to ""warren:60020"" whereas in configuration B we get ""10.0.1.4:7802"".  The port numbers make sense but not the ""warren"" hostname. It's probably specific to Warren's mac environment somehow because no other developer gets this problem when doing the same thing.  ""warren"" isn't in his hosts file so that remains a mystery.

But irrespective of that, since the ports differ we expect the stale meta connection data to cause connection failure anyway.  Perhaps in the form of a SocketTimeoutException as in hbase-3445.

But shouldn't the HMaster handle that by catching the exception and letting verifyMetaRegionLocation() fail so that meta regions get reassigned to the new region server?

Probably the safeguards in CatalogTracker.getCachedConnection() should move up to getMetaServerConnection() so as to encompass MetaReader.readMetaLocation() also. Essentially if getMetaServerConnection() encounters ANY exception connection to meta RegionServer it should probably just return null to force meta region reassignment.


",,jk-public@troove.net,Major,Closed,Fixed,26/Jan/11 00:34,12/Jun/22 00:52
Bug,HBASE-3481,12496775,max seq id in flushed file can be larger than its correct value causing data loss during recovery,"[While doing some cluster kill tests, I noticed some missing data after log recovery. Upon investigating further, and pretty printing contents of HFiles and recovered logs, this is my analysis of the situation/bug. Please confirm the theory and pitch in with suggestions.]

When memstores are flushed, the max sequence id recorded in  the HFile should be the max sequence id of all KVs in the memstore. However, we seem to simply obtain the current sequence id from the HRegion, and stamp the HFile's MAX_SEQ_ID with it.

From HRegion.java:
{code}
    sequenceId = (wal == null)? myseqid: wal.startCacheFlush();
{code}

where, startCacheFlush() is:

{code}
public long startCacheFlush() {
    this.cacheFlushLock.lock();
    return obtainSeqNum();
 }
{code}

where, obtainSeqNum() is simply: 

{code}
private long obtainSeqNum() {
    return this.logSeqNum.incrementAndGet();
  }
{code}

So let's say a memstore contains edits with sequence number 1..10.

Meanwhile, say more Puts come along, and are going through this flow (in pseudo-code)

{code}
1. HLog.append();
       1.1  obtainSeqNum()
       1.2 writeToWAL()

2 updateMemStore()
{code}

So it is possible that the sequence number has already been incremented to say 15 if there are 5 more outstanding puts. Say the writeToWAL() is still in progress for these puts. In this case, none of these edits (11..15) would have been written to memstore yet.

At this point if a cache flush of the memstore happens, then we'll record its MAX_SEQ_ID as 16 in the store file instead of 10 (because that's what obtainSeqNum() would return as the next sequence number to use, right?).

Assume that the edits 11..15 eventually complete. And so HLogs do contain the data for edits 11..15.

Now, at this point if the region server were to crash, and we run log recovery, the splits all go through correctly, and a correct recovered.edits file is generated with the edits 11..15. 

Next, when the region is opened, the HRegion notes that one of the store file says MAX_SEQ_ID is 16. So, when it replays the recovered.edits file, it  skips replaying edits 11..15. Or in other words, data loss.

----



",ryanobjc,kannanm,Blocker,Closed,Fixed,26/Jan/11 08:46,20/Nov/15 12:40
Bug,HBASE-3483,12496883,No soft flush trigger on global memstore limit,"I think this is the reason people see long blocking periods under write load.

Currently when we hit the global memstore limit, we call reclaimMemStoreMemory() which is synchronized - thus everyone has to wait until the memory has flushed down to the low water mark. This causes every writer to block for 10-15 seconds on a large heap.

Instead we should start triggering flushes (in another thread) whenever we're above the low water mark. Then only block writers when we're above the high water mark.",tlipcon,tlipcon,Critical,Closed,Fixed,26/Jan/11 23:42,20/Nov/15 12:41
Bug,HBASE-3488,12497114,Add CellCounter to count multiple versions of rows,"Currently RowCounter only retrieves latest version for each row.
Some applications would store multiple versions for the same row.

RowCounter should accept a new parameter for the number of versions to return.
Scan object would be configured with version parameter (for scan.maxVersions).
Then the following API should be called:
{code}
  public KeyValue[] raw() {
{code}
",iamknome,yuzhihong@gmail.com,Major,Closed,Fixed,29/Jan/11 00:40,20/Nov/15 12:40
Bug,HBASE-3489,12497144,.oldlogs not being cleaned out,"The .oldlogs folder is never being cleaned up. The hbase.master.logcleaner.ttl has been set to clean up the old logs but the clean up is never kicking in. The limit of 10 files is not the problem. After running for 5 days not a single log file has ever been deleted and the logcleaner is set to 2 days (from the default of 7 days). It is assumed that the replication changes that want to be sure to keep these logs around if needed have caused the cleanup to be blocked. There is no replication defined (knowingly).
 ",,wav100,Major,Closed,Fixed,30/Jan/11 02:32,12/Jun/22 00:55
Bug,HBASE-3492,12497184,NPE while splitting table with empty column family store,"I did a simple test on trunk where I create a table (after wiping the local /tmp/hbase-<username>):

{code}
hbase(main):001:0> create 'testtable', 'cf1', 'cf2'                                                                                                  
{code}

Then I inserted 17k rows:

{code}
hbase(main):002:0> for i in 'a'..'z' do for j in 'a'..'z' do for k in 'a'..'z' do put 'testtable', ""row-#{i}#{j}#{k}"", ""cf1:#{k}"", ""#{k}"" end end end
{code}

and called 

{code}
hbase(main):003:0> split 'testttable'
{code}

and the logs gave this NPE and _no_ split was performed:

{code}
2011-01-31 10:06:38,534 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c., current region memstore size 3.5m
2011-01-31 10:06:38,575 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2011-01-31 10:06:38,856 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at file:/tmp/hbase-larsgeorge/hbase/testtable/2528e23534565a83e2c8590d33f3a47c/.tmp/5265602271926296451 to file:/tmp/hbase-larsgeorge/hbase/testtable/2528e23534565a83e2c8590d33f3a47c/cf1/5349044325262044918
2011-01-31 10:06:38,861 INFO org.apache.hadoop.hbase.regionserver.Store: Added file:/tmp/hbase-larsgeorge/hbase/testtable/2528e23534565a83e2c8590d33f3a47c/cf1/5349044325262044918, entries=17576, sequenceid=17588, memsize=3.5m, filesize=549.9k
2011-01-31 10:06:38,863 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~3.5m for region testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c. in 328ms, sequenceid=17588, compaction requested=false
2011-01-31 10:06:38,869 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c.
2011-01-31 10:06:38,870 INFO org.apache.hadoop.hbase.regionserver.HRegion: aborted compaction on region testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c. after 0sec
2011-01-31 10:06:38,872 ERROR org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction failed for region testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c.
java.lang.NullPointerException
        at org.apache.hadoop.hbase.regionserver.Store.checkSplit(Store.java:1367)
        at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:633)
        at org.apache.hadoop.hbase.regionserver.HRegion.compactStores(HRegion.java:793)
        at org.apache.hadoop.hbase.regionserver.CompactSplitThread.run(CompactSplitThread.java:81)
2011-01-31 10:06:38,873 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for testtable,,1296461957099.2528e23534565a83e2c8590d33f3a47c. because User-triggered split; priority=1, compaction queue size=0
{code}

I added a row with data in cf2:

{code}
hbase(main):005:0> put 'testtable', 'row1', 'cf2', 'test1'                                                                                           
{code}

and the tried to split the table again like above and now it worked.",larsgeorge,larsgeorge,Minor,Closed,Fixed,31/Jan/11 09:15,20/Nov/15 12:42
Bug,HBASE-3493,12497244,HMaster sometimes hangs during initialization due to missing notify call,"During HMaster.finishInitialization(), assignRootAndMeta() is called, which at some point does:

{code}
this.assignmentManager.waitForAssignment(HRegionInfo.FIRST_META_REGIONINFO);
{code}

And waitForAssignment does this:

{code}
    synchronized(regions) {
      while(!regions.containsKey(regionInfo)) {
        regions.wait();
      }
    }
{code}

However, I could not find any call to regions.notify(), so this could wait forever.

I have not noticed this problem on a real cluster, only when using HBaseTestingUtility on a slow and low-memory Hudson server (I can reproduce it on my local machine by creating some background load). Adding a notify() call when regions.put() is called seems to fix the problem. Will attach patch.

For reference, this was based on seeing the following from jstack:

{noformat}
""Master:0;lat:44776"" prio=10 tid=0x08d5b400 nid=0x381a in Object.wait() [0x9c76d000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        - waiting on <0xa5196fb8> (a java.util.TreeMap)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.hadoop.hbase.master.AssignmentManager.waitForAssignment(AssignmentManager.java:1152)
        - locked <0xa5196fb8> (a java.util.TreeMap)
        at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:440)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:382)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:277)
        at java.lang.Thread.run(Thread.java:619)
{noformat}",bruno,bruno,Major,Closed,Fixed,31/Jan/11 19:27,20/Nov/15 12:43
Bug,HBASE-3494,12497257,checkAndPut implementation doesnt verify row param and writable row are the same,"the API checkAndPut, and on the server side checkAndMutate doesn't enforce that the row in the API call and the row in the passed writable that should be executed if the check passes, are the same row!  Looking at the code, if someone were to 'fool' us, we'd probably end up with rows in the wrong region in the worst case.  Or we'd end up with non-locked puts/deletes to different rows since the checkAndMutate grabs the row lock and calls put/delete methods that do not grab row locks.",ryanobjc,ryanobjc,Major,Closed,Fixed,31/Jan/11 21:40,20/Nov/15 12:43
Bug,HBASE-3495,12497291,Shell is failing on subsequent split calls,"While working on HBASE-3492 I came across another oddity with manual splits:

{code}
hbase(main):003:0> split 'testtable'                                                                                                                 
0 row(s) in 3.0590 seconds

hbase(main):004:0> scan '.META.', { COLUMNS => ['info:regioninfo'] }                                                                                 
ROW                                       COLUMN+CELL                                                                                                            
 testtable,,1296545855212.5e4ef9631cacb6b column=info:regioninfo, timestamp=1296545855770, value=REGION => {NAME => 'testtable,,1296545855212.5e4ef9631cacb6b2c6c
 2c6c338140c53cad4.                       338140c53cad4.', STARTKEY => '', ENDKEY => 'row-mdc', ENCODED => 5e4ef9631cacb6b2c6c338140c53cad4, TABLE => {{NAME => '
                                          testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION 
                                          => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'cf2', BLOO
                                          MFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => 
                                          '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                                
 testtable,row-mdc,1296545855212.46e57f0c column=info:regioninfo, timestamp=1296545855774, value=REGION => {NAME => 'testtable,row-mdc,1296545855212.46e57f0ca4eb
 a4eba8d3e5bef6365159a660.                a8d3e5bef6365159a660.', STARTKEY => 'row-mdc', ENDKEY => '', ENCODED => 46e57f0ca4eba8d3e5bef6365159a660, TABLE => {{NA
                                          ME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPR
                                          ESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'cf2
                                          ', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKS
                                          IZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                         
2 row(s) in 0.6690 seconds

hbase(main):005:0> split 'testtable'                                
0 row(s) in 0.4030 seconds

hbase(main):006:0> split 'testtable'

ERROR: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4.
        at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2376)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.splitRegion(HRegionServer.java:2196)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:309)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1057)

Here is some help for this command:
Split entire table or pass a region to split individual region.  With the 
second parameter, you can specify an explicit split key for the region.  
Examples:
    split 'tableName'
    split 'regionName' # format: 'tableName,startKey,id'
    split 'tableName', 'splitKey'
    split 'regionName', 'splitKey'

{code}

It takes minutes for this to clear out eventually. Why is this not retried or flushed out right away?

A few minutes (!) later I see this in the logs:

{code}
2011-02-01 08:42:42,062 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference testtable,,1296545879295.dfcc24e02e27e60160612dd5398cbd1e., qualifier=splitA, from parent testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4.
2011-02-01 08:42:42,064 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: getRegionInfo 1
2011-02-01 08:42:42,064 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: delete 0
2011-02-01 08:42:42,064 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference testtable,row-dau,1296545879295.4073eb6c82755aab57778af2dba39e22., qualifier=splitB, from parent testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4.
2011-02-01 08:42:42,064 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Deleting region testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4. because daughter splits no longer hold references
2011-02-01 08:42:42,065 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: DELETING region file:/tmp/hbase-larsgeorge/hbase/testtable/5e4ef9631cacb6b2c6c338140c53cad4
2011-02-01 08:42:42,067 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: getRegionInfo 1
2011-02-01 08:42:42,067 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: delete 0
2011-02-01 08:42:42,067 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted region testtable,,1296545855212.5e4ef9631cacb6b2c6c338140c53cad4. from META
2011-02-01 08:42:42,069 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: getRegionInfo 0
2011-02-01 08:42:42,070 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: delete 1
2011-02-01 08:42:42,071 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference testtable,row-mdc,1296545879558.94cb351e5dd36c269247dd8a1a79373c., qualifier=splitA, from parent testtable,row-mdc,1296545855212.46e57f0ca4eba8d3e5bef6365159a660.
2011-02-01 08:42:42,073 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: getRegionInfo 1
2011-02-01 08:42:42,074 DEBUG org.apache.hadoop.hbase.ipc.HBaseRPC: Call: delete 1
2011-02-01 08:42:42,074 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference testtable,row-seq,1296545879558.43c5ffe1ca7dd6d1374b7b7430a7d261., qualifier=splitB, from parent testtable,row-mdc,1296545855212.46e57f0ca4eba8d3e5bef6365159a660.
2011-02-01 08:42:42,074 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Deleting region testtable,row-mdc,1296545855212.46e57f0ca4eba8d3e5bef6365159a660. because daughter splits no longer hold references
2011-02-01 08:42:42,074 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: DELETING region file:/tmp/hbase-larsgeorge/hbase/testtable/46e57f0ca4eba8d3e5bef6365159a660
{code}

The the next split call works while the subsequent ones fail again. In other words the split is dropped somewhere and picked up by the catalog classes later while the shell does not see the new daughter regions?

Even .META. is off

{code}

hbase(main):011:0> scan '.META.', { COLUMNS => ['info:regioninfo'] }
ROW                                       COLUMN+CELL                                                                                                            
 testtable,,1296545879295.dfcc24e02e27e60 column=info:regioninfo, timestamp=1296546225693, value=REGION => {NAME => 'testtable,,1296545879295.dfcc24e02e27e601606
 160612dd5398cbd1e.                       12dd5398cbd1e.', STARTKEY => '', ENDKEY => 'row-dau', ENCODED => dfcc24e02e27e60160612dd5398cbd1e, OFFLINE => true, SPL
                                          IT => true, TABLE => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0
                                          ', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE 
                                          => 'true'}, {NAME => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TT
                                          L => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                
 testtable,,1296546225506.f3a53bfa1bfd5ae column=info:regioninfo, timestamp=1296546225763, value=REGION => {NAME => 'testtable,,1296546225506.f3a53bfa1bfd5ae6cbb
 6cbb0641d43f8a242.                       0641d43f8a242.', STARTKEY => '', ENDKEY => 'row-aaa', ENCODED => f3a53bfa1bfd5ae6cbb0641d43f8a242, TABLE => {{NAME => '
                                          testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION 
                                          => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'cf2', BLOO
                                          MFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => 
                                          '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                                
 testtable,row-aaa,1296546225506.4253ecd9 column=info:regioninfo, timestamp=1296546225761, value=REGION => {NAME => 'testtable,row-aaa,1296546225506.4253ecd9c94c
 c94c38b66bdf8cd17b07efcb.                38b66bdf8cd17b07efcb.', STARTKEY => 'row-aaa', ENDKEY => 'row-dau', ENCODED => 4253ecd9c94c38b66bdf8cd17b07efcb, TABLE 
                                          => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3'
                                          , COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME 
                                          => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647',
                                           BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                  
 testtable,row-dau,1296545879295.4073eb6c column=info:regioninfo, timestamp=1296546225913, value=REGION => {NAME => 'testtable,row-dau,1296545879295.4073eb6c8275
 82755aab57778af2dba39e22.                5aab57778af2dba39e22.', STARTKEY => 'row-dau', ENDKEY => 'row-mdc', ENCODED => 4073eb6c82755aab57778af2dba39e22, OFFLIN
                                          E => true, SPLIT => true, TABLE => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATI
                                          ON_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false
                                          ', BLOCKCACHE => 'true'}, {NAME => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION
                                           => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                  
 testtable,row-dau,1296546225769.529fdb6b column=info:regioninfo, timestamp=1296546225971, value=REGION => {NAME => 'testtable,row-dau,1296546225769.529fdb6bcca8
 cca8459349c81b518a24436b.                459349c81b518a24436b.', STARTKEY => 'row-dau', ENDKEY => 'row-gbo', ENCODED => 529fdb6bcca8459349c81b518a24436b, TABLE 
                                          => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3'
                                          , COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME 
                                          => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647',
                                           BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                  
 testtable,row-gbo,1296546225769.374d4364 column=info:regioninfo, timestamp=1296546225968, value=REGION => {NAME => 'testtable,row-gbo,1296546225769.374d4364574a
 574ad1c5f522aa55b3d81586.                d1c5f522aa55b3d81586.', STARTKEY => 'row-gbo', ENDKEY => 'row-mdc', ENCODED => 374d4364574ad1c5f522aa55b3d81586, TABLE 
                                          => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3'
                                          , COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME 
                                          => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647',
                                           BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                  
 testtable,row-mdc,1296545879558.94cb351e column=info:regioninfo, timestamp=1296545879815, value=REGION => {NAME => 'testtable,row-mdc,1296545879558.94cb351e5dd3
 5dd36c269247dd8a1a79373c.                6c269247dd8a1a79373c.', STARTKEY => 'row-mdc', ENDKEY => 'row-seq', ENCODED => 94cb351e5dd36c269247dd8a1a79373c, TABLE 
                                          => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3'
                                          , COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME 
                                          => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647',
                                           BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                  
 testtable,row-seq,1296545879558.43c5ffe1 column=info:regioninfo, timestamp=1296546226107, value=REGION => {NAME => 'testtable,row-seq,1296545879558.43c5ffe1ca7d
 ca7dd6d1374b7b7430a7d261.                d6d1374b7b7430a7d261.', STARTKEY => 'row-seq', ENDKEY => '', ENCODED => 43c5ffe1ca7dd6d1374b7b7430a7d261, OFFLINE => tr
                                          ue, SPLIT => true, TABLE => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOP
                                          E => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOC
                                          KCACHE => 'true'}, {NAME => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NO
                                          NE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                         
 testtable,row-seq,1296546225975.c9188f86 column=info:regioninfo, timestamp=1296546226161, value=REGION => {NAME => 'testtable,row-seq,1296546225975.c9188f869822
 9822da3ff21215a98a99ff5a.                da3ff21215a98a99ff5a.', STARTKEY => 'row-seq', ENDKEY => 'row-vfk', ENCODED => c9188f869822da3ff21215a98a99ff5a, TABLE 
                                          => {{NAME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3'
                                          , COMPRESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME 
                                          => 'cf2', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647',
                                           BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                  
 testtable,row-vfk,1296546225975.682a4dbf column=info:regioninfo, timestamp=1296546226156, value=REGION => {NAME => 'testtable,row-vfk,1296546225975.682a4dbf9800
 980035dc379c6ccd7418cb08.                35dc379c6ccd7418cb08.', STARTKEY => 'row-vfk', ENDKEY => '', ENCODED => 682a4dbf980035dc379c6ccd7418cb08, TABLE => {{NA
                                          ME => 'testtable', FAMILIES => [{NAME => 'cf1', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPR
                                          ESSION => 'NONE', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}, {NAME => 'cf2
                                          ', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'NONE', TTL => '2147483647', BLOCKS
                                          IZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}}                                                         
10 row(s) in 0.2610 seconds
{code}

Look at the ENKDEYs.",larsgeorge,larsgeorge,Major,Closed,Fixed,01/Feb/11 07:46,20/Nov/15 12:41
Bug,HBASE-3497,12497395,TableMapReduceUtil.initTableReducerJob broken due to setConf method in TableOutputFormat,"setConf() method in TableOutputFormat gets called and it replaces the hbase.zookeeper.quorum address in the job conf xml when you run a CopyTable job from one cluster to another. The conf gets set to the peer.addr that is specified, which makes the job read and write from/to the peer cluster instead of reading from the original cluster and writing to the peer.

Possibly caused due to the change in https://issues.apache.org/jira/browse/HBASE-3111",jdcryans,amansk,Major,Closed,Fixed,01/Feb/11 22:32,20/Nov/15 12:41
Bug,HBASE-3499,12497496,Users upgrading to 0.90.0 need to have their .META. table updated with the right MEMSTORE_SIZE,"With Jack Levin, we were able to figure that users that are upgrading from a 0.20.x era cluster have their .META. schema set with a 16KB MEMSTORE_SIZE. This was done in order to minimize lost meta rows when append wasn't available but even if we changed it in HTD, we also have to make sure all users upgrading to 0.90 have it changed too.

In Jack's case, he ended up with 2143 storefiles in .META. during a cold start, slowing everything down. He reported a few times in the past that his .META. was always extremely busy.

We should be able to do it as a one-off thing in HMaster when opening .META. (an update in place).",,jdcryans,Blocker,Closed,Fixed,02/Feb/11 19:40,20/Nov/15 12:42
Bug,HBASE-3502,12497529,Can't open region because can't open .regioninfo because AlreadyBeingCreatedException,"Testing killing .META. I tripped over this one.  Last thing seen on regionserver killed was:

{code}
2011-02-02 21:44:48,379 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Instantiated TestTable,0591556500,1296683085472.76c9a32c5f068d16240e42a15fed8417.
{code}

... which means we could have been inside checkRegioninfoOnFilesystem when we were killed.

This tries to create the .regioninfo file.  Seems like that was started over at the NN but then the RS died shortly afterward.  Its stopping the Region opening.  I suppose I could try and open it for append to shut it then reopen?",stack,stack,Critical,Closed,Fixed,02/Feb/11 23:48,20/Nov/15 12:43
Bug,HBASE-3513,12497941,upgrade thrift to 0.5.0 and use mvn version,"We should upgrade our thrift to 0.5.0, it is the latest and greatest and is in apache maven repo.

Doing some testing with a thrift 0.5.0 server, and an older pre-release php client shows the two are on-wire compatible.

Given that the upgrade is entirely on the server side, and has no wire-impact this should be a relatively low-impact change.",ryanobjc,ryanobjc,Major,Closed,Fixed,08/Feb/11 01:38,20/Nov/15 12:42
Bug,HBASE-3515,12498015,[replication] ReplicationSource can miss a log after RS comes out of GC,"This is from Hudson build 1738, if a log is about to be rolled and the ZK connection is already closed then the replication code will fail at adding the new log in ZK but the log will still be rolled and it's possible that some edits will make it in.

From the log:

{quote}
2011-02-08 10:21:20,618 FATAL [RegionServer:0;vesta.apache.org,46117,1297160399378.logRoller] regionserver.HRegionServer(1383):
 ABORTING region server serverName=vesta.apache.org,46117,1297160399378, load=(requests=1525, regions=12,
 usedHeap=273, maxHeap=1244): Failed add log to list
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for 
 /1/replication/rs/vesta.apache.org,46117,1297160399378/2/vesta.apache.org%3A46117.1297160480509
...

2011-02-08 10:21:22,444 DEBUG [MASTER_META_SERVER_OPERATIONS-vesta.apache.org:56008-0] wal.HLogSplitter(258):
 Splitting hlog 8 of 8: hdfs://localhost:55474/user/hudson/.logs/vesta.apache.org,46117,1297160399378/vesta.apache.org%3A46117.1297160480509, length=0

2011-02-08 10:21:22,862 DEBUG [MASTER_META_SERVER_OPERATIONS-vesta.apache.org:56008-0] wal.HLogSplitter(436):
 Pushed=31 entries from hdfs://localhost:55474/user/hudson/.logs/vesta.apache.org,46117,1297160399378/vesta.apache.org%3A46117.1297160480509
{quote}

The easiest thing to do would be let the exception out and cancel the log roll.",jdcryans,jdcryans,Critical,Closed,Fixed,08/Feb/11 18:46,20/Nov/15 12:42
Bug,HBASE-3517,12498049,Store build version in hbase-default and verify at runtime,"This is a second attempt at HBASE-3470 (now reverted) which didn't work properly at MR job submission time since MR unjars the job jar into /tmp when submitting.

This new JIRA will store the hbase version inside hbase-default, and at runtime verify that the hbase-default matches the version of the code.",tlipcon,tlipcon,Blocker,Closed,Fixed,09/Feb/11 00:03,20/Nov/15 12:42
Bug,HBASE-3524,12498311,NPE from CompactionChecker,"I recently updated production data to use HBase 0.90.0.
Now I'm periodically seeing:

[10/02/11 17:23:27] 30076066 [mpactionChecker] ERROR nServer$MajorCompactionChecker  - Caught exception
java.lang.NullPointerException
	at org.apache.hadoop.hbase.regionserver.Store.isMajorCompaction(Store.java:832)
	at org.apache.hadoop.hbase.regionserver.Store.isMajorCompaction(Store.java:810)
	at org.apache.hadoop.hbase.regionserver.HRegion.isMajorCompaction(HRegion.java:2800)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$MajorCompactionChecker.chore(HRegionServer.java:1047)
	at org.apache.hadoop.hbase.Chore.run(Chore.java:66)

The only negative effect is that this is interrupting compactions from happening. But that is pretty serious and this might be a sign of data corruption?

Maybe it's just my data, but this task should at least involve improving the handling to catch the NPE and still iterate through the other onlineRegions that might compact without error.  The MajorCompactionChecker.chore() method only catches IOExceptions and so this NPE breaks out of that loop. 
",ryanobjc,jk-public@troove.net,Blocker,Closed,Fixed,11/Feb/11 01:56,20/Nov/15 12:43
Bug,HBASE-3525,12498386,mvn assembly is over-filling the hbase lib dir,"Here is what our lib dir looks this in 0.90.1:

{code}
-rwxr-xr-x  1 Stack  staff    62983 Mar 16  2009 activation-1.1.jar
-rwxr-xr-x  1 Stack  staff  1034049 May 21  2009 ant-1.6.5.jar
-rwxr-xr-x  1 Stack  staff  1323005 Jul 20  2009 ant-1.7.1.jar
-rwxr-xr-x  1 Stack  staff    12143 Jul 20  2009 ant-launcher-1.7.1.jar
-rwxr-xr-x  1 Stack  staff    43033 May  5  2009 asm-3.1.jar
-rwxr-xr-x  1 Stack  staff   339831 Oct 18 10:05 avro-1.3.3.jar
-rwxr-xr-x  1 Stack  staff    41123 Dec  8  2009 commons-cli-1.2.jar
-rwxr-xr-x  1 Stack  staff    58160 Oct 18 10:05 commons-codec-1.4.jar
-rwxr-xr-x  1 Stack  staff   112341 Mar 16  2009 commons-el-1.0.jar
-rwxr-xr-x  1 Stack  staff   305001 Mar 16  2009 commons-httpclient-3.1.jar
-rwxr-xr-x  1 Stack  staff   279193 May 17  2010 commons-lang-2.5.jar
-rwxr-xr-x  1 Stack  staff    60686 Mar 13  2009 commons-logging-1.1.1.jar
-rwxr-xr-x  1 Stack  staff   180792 Mar  4  2010 commons-net-1.4.1.jar
-rwxr-xr-x  1 Stack  staff  3566844 Jun  5  2009 core-3.1.1.jar
-rwxr-xr-x  1 Stack  staff   936397 Oct 18 10:05 guava-r06.jar
-rwxr-xr-x  1 Stack  staff  2707856 Jan 11 13:26 hadoop-core-0.20-append-r1056497.jar
-rwxr-xr-x  1 Stack  staff  2241521 Feb  9 15:57 hbase-0.90.1.jar
-rwxr-xr-x  1 Stack  staff   706710 Mar  4  2010 hsqldb-1.8.0.10.jar
-rwxr-xr-x  1 Stack  staff   171958 Oct 18 10:05 jackson-core-asl-1.5.5.jar
-rwxr-xr-x  1 Stack  staff    17065 Oct 18 10:05 jackson-jaxrs-1.5.5.jar
-rwxr-xr-x  1 Stack  staff   386509 Oct 18 10:05 jackson-mapper-asl-1.4.2.jar
-rwxr-xr-x  1 Stack  staff    24745 Oct 18 10:05 jackson-xc-1.5.5.jar
-rwxr-xr-x  1 Stack  staff   408133 May 21  2010 jasper-compiler-5.5.23.jar
-rwxr-xr-x  1 Stack  staff    76844 May 17  2010 jasper-runtime-5.5.23.jar
-rwxr-xr-x  1 Stack  staff   103515 May  6  2009 jaxb-api-2.1.jar
-rwxr-xr-x  1 Stack  staff   867801 Mar  4  2010 jaxb-impl-2.1.12.jar
-rwxr-xr-x  1 Stack  staff   455517 Oct 18 10:05 jersey-core-1.4.jar
-rwxr-xr-x  1 Stack  staff   142827 Oct 18 10:05 jersey-json-1.4.jar
-rwxr-xr-x  1 Stack  staff   677600 Oct 18 10:05 jersey-server-1.4.jar
-rwxr-xr-x  1 Stack  staff   377780 Mar  4  2010 jets3t-0.7.1.jar
-rwxr-xr-x  1 Stack  staff    67758 May  6  2009 jettison-1.1.jar
-rwxr-xr-x  1 Stack  staff   539912 Jan  3 16:51 jetty-6.1.26.jar
-rwxr-xr-x  1 Stack  staff   177131 Jan  3 16:51 jetty-util-6.1.26.jar
-rwxr-xr-x  1 Stack  staff    87325 Jul 20  2009 jline-0.9.94.jar
-rwxr-xr-x  1 Stack  staff  4477138 Jan  3 16:51 jruby-complete-1.0.3.jar
-rwxr-xr-x  1 Stack  staff  1024680 May 17  2010 jsp-2.1-6.1.14.jar
-rwxr-xr-x  1 Stack  staff   134910 May 17  2010 jsp-api-2.1-6.1.14.jar
-rwxr-xr-x  1 Stack  staff    46367 Mar  4  2010 jsr311-api-1.1.1.jar
-rwxr-xr-x  1 Stack  staff   121070 Mar 13  2009 junit-3.8.1.jar
-rwxr-xr-x  1 Stack  staff    11981 Mar  4  2010 kfs-0.3.jar
-rwxr-xr-x  1 Stack  staff   481535 Oct 18 10:05 log4j-1.2.16.jar
-rwxr-xr-x  1 Stack  staff    65261 Apr 14  2009 oro-2.0.8.jar
-rwxr-xr-x  1 Stack  staff    29392 Jun 14  2010 paranamer-2.2.jar
-rwxr-xr-x  1 Stack  staff     5420 Jun 14  2010 paranamer-ant-2.2.jar
-rwxr-xr-x  1 Stack  staff     6931 Jun 14  2010 paranamer-generator-2.2.jar
-rwxr-xr-x  1 Stack  staff   328635 Mar  4  2010 protobuf-java-2.3.0.jar
-rwxr-xr-x  1 Stack  staff   173236 Jun 14  2010 qdox-1.10.1.jar
drwxr-xr-x  7 Stack  staff      238 Feb  8 16:23 ruby
-rwxr-xr-x  1 Stack  staff   132368 May 17  2010 servlet-api-2.5-6.1.14.jar
-rwxr-xr-x  1 Stack  staff    23445 Mar  4  2010 slf4j-api-1.5.8.jar
-rwxr-xr-x  1 Stack  staff     9679 Mar  4  2010 slf4j-log4j12-1.5.8.jar
-rwxr-xr-x  1 Stack  staff    26514 May  6  2009 stax-api-1.0.1.jar
-rwxr-xr-x  1 Stack  staff   187530 Mar  4  2010 thrift-0.2.0.jar
-rwxr-xr-x  1 Stack  staff    15010 Mar  4  2010 xmlenc-0.52.jar
-rwxr-xr-x  1 Stack  staff   598364 Dec 10 15:13 zookeeper-3.3.2.jar
{code}

We are picking up bunch of hadoop dependencies.  I'd think it harmless other than the bulk.",stack,stack,Major,Closed,Fixed,11/Feb/11 15:39,20/Nov/15 12:42
Bug,HBASE-3528,12498445,Maven should restrict more exported deps,"Our maven build exports a lot of deps, and they flow to clients who depend on us.

for example we shouldnt export thrift,protobuf,jetty,jruby,tomcat,jersey,guava, etc.  

Clients should be able to depend on any version of the above libraries or NOT depend on them.

",,ryanobjc,Major,Closed,Fixed,12/Feb/11 06:44,12/Jun/22 17:29
Bug,HBASE-3531,12498573,"When under global memstore pressure, may try to flush unflushable regions in a tight loop","Ted ran into this in cluster testing. If the largest region is unflushable (eg it's in the midst of closing during a split, and hence doing its own flush), the global memstore pressure code doesn't notice this. So, it keeps trying to flush it, and ignores the false return code from flushRegion.

Instead, we should iterate down the list of regions and keep trying to flush them until we find one that works.",tlipcon,tlipcon,Blocker,Closed,Fixed,14/Feb/11 21:25,20/Nov/15 12:42
Bug,HBASE-3532,12498574,HRegion#equals is broken,"It's currently implemented by comparing the hashcode, which is not unique! Currently it only appears to be depended upon by HRegionServer#getRegionsToCheck, which is as best I can tell unused.",yuzhihong@gmail.com,tlipcon,Major,Closed,Fixed,14/Feb/11 21:28,20/Nov/15 12:40
Bug,HBASE-3534,12498625,Action should not store or serialize regionName,"Action stores the regionName, BUT an action comes from a MultiAction, which contains:

  public Map<byte[], List<Action<R>>> actions 

Which means we are storing the regionName multiple times. In fact, no one even calls the accessor getRegionName!

It changes the serialization of Action and MultiAction, but reduces the byte overhead.",yuzhihong@gmail.com,ryanobjc,Major,Closed,Fixed,15/Feb/11 09:24,20/Nov/15 12:43
Bug,HBASE-3535,12498648,[site] Fix home page so it shows the search-hadoop.com box,"Search is present on most other pages, not the home page.",stack,stack,Major,Closed,Fixed,15/Feb/11 15:23,12/Jun/22 17:29
Bug,HBASE-3538,12498682,Column families allow to have slashes in name,"The check in HColumnDescriptor.isLegalFamilyName() does not check for slashes and may cause issue.

{code}
create 'test2', 'cf/am/2'

$ bin/hadoop dfs -lsr /hbase/test2/d1da5042d2b233f056f7604398f29537
drwxr-xr-x   - larsgeorge supergroup          0 2011-02-15 21:12 /hbase/test2/d1da5042d2b233f056f7604398f29537/.oldlogs
-rw-r--r--   3 larsgeorge supergroup        124 2011-02-15 21:12 /hbase/test2/d1da5042d2b233f056f7604398f29537/.oldlogs/hlog.1297800748239
-rw-r--r--   3 larsgeorge supergroup        714 2011-02-15 21:12 /hbase/test2/d1da5042d2b233f056f7604398f29537/.regioninfo
drwxr-xr-x   - larsgeorge supergroup          0 2011-02-15 21:12 /hbase/test2/d1da5042d2b233f056f7604398f29537/cf
drwxr-xr-x   - larsgeorge supergroup          0 2011-02-15 21:12 /hbase/test2/d1da5042d2b233f056f7604398f29537/cf/am
drwxr-xr-x   - larsgeorge supergroup          0 2011-02-15 21:12 /hbase/test2/d1da5042d2b233f056f7604398f29537/cf/am/2
{code}",iamknome,larsgeorge,Major,Closed,Fixed,15/Feb/11 20:19,20/Nov/15 12:42
Bug,HBASE-3539,12498768,Improve shell help to reflect all possible options,"The shell is not consistent in its help texts. For example:

{code}
Scan a table; pass table name and optionally a dictionary of scanner
specifications.  Scanner specifications may include one or more of
the following: LIMIT, STARTROW, STOPROW, TIMESTAMP, or COLUMNS.  If
no columns are specified, all columns will be scanned.
{code}

but in the code you have

{code}
 filter = args[""FILTER""]
 startrow = args[""STARTROW""] || ''
 stoprow = args[""STOPROW""]
 timestamp = args[""TIMESTAMP""]
 columns = args[""COLUMNS""] || args[""COLUMN""] || get_all_columns
 cache = args[""CACHE_BLOCKS""] || true
 versions = args[""VERSIONS""] || 1
{code}

VERSIONS is missing from the help. 

Check all commands and make sure all options are stated and examples given.",qwertymaniac,larsgeorge,Trivial,Closed,Fixed,16/Feb/11 14:40,20/Nov/15 12:40
Bug,HBASE-3545,12498926,Possible liveness issue with MasterServerAddress in HRegionServer getMaster ,"As part of our evaluation of HBase we have been testing failure scenarios to see how HBase fails in certain situations.

One of these is the outright failure of a HBase master.

What presently happens, if a HBase master is shutdown, is that the standby master becomes the active master in the Zookeeper. At the same time the region servers fail to connect to the dead master and typically fail their own heartbeats as part of the reportForDuty() method.

Following this the region server attempts to get a connection to a working HBase master, inside the getMaster() method the first action is to get the address of a potentially working master server from zookeeper. Following this the code is put into a tight loop whereupon it keeps attempting to connect to the address of the master found in Zookeeper.

Unfortunately it appears that during master fail-over, it becomes possible to get the address of the old, broken master, this address is then put into the connection attempt loop, whereupon the region server attempts to infinitely connect to the failed, none existent master. At this point nothing is able to break the loop in getMaster so the RS is unable to contact the master.

At the same time the new master is waiting patiently for the existing region servers as reported in Zookeeper to re-establish contact with it.

Attached is a patch that rectifies this issue in our test cluster for both the 0.90.0 tag and trunk versions (as of git SHA b72a24f71b67598e4077a9d1452f903082b0a9b7) of HBase.

This patch is also available in a forked repository here https://github.com/GregBowyer/hbase/commit/543f5903731ef6bbfd58c990e04a2c635e5c94b4",,gbowyer@fastmail.co.uk,Major,Closed,Fixed,17/Feb/11 17:32,20/Nov/15 12:42
Bug,HBASE-3548,12499026,Fix type in documentation of pseudo distributed mode,"See http://hbase.apache.org/pseudo-distributed.html, it has ""psuedo"" as opposed to ""pseudo"" - Trivial fix.",stack,larsgeorge,Trivial,Closed,Fixed,18/Feb/11 11:48,20/Nov/15 12:42
Bug,HBASE-3550,12499113,FilterList reports false positives,"When FilterList is set to Operator.MUST_PASS_ALL, if a child Filter returns ReturnCode.SEEK_NEXT_USING_HINT, that return code gets swallowed and ReturnCode.INCLUDE gets returned instead. This causes false positives with ColumnPrefixFilter.",billgraham,billgraham,Major,Closed,Fixed,19/Feb/11 06:11,20/Nov/15 12:42
Bug,HBASE-3552,12499274,Coprocessors Are Unable To Load If RegionServer is Launched Using a Different Classloader than System Default,"If a region server is launched in a context such that its classloader is different from the system class loader, then the Class object used to represent the Coprocessor interface of the coprocessor will be different than the Coprocessor Class object that is used by RegionCoprocessorHost.loadSystemCoprocessors() .

There's a few options that come to mind to fix this problem:

1. Remove the logic where loadSystemCoprocessors changes the context's ClassLoader back to the system default classloader.

2. Remove the cast to Coprocessor in CoprocessorHost.load() and invoke methods via reflection.

3. Set the class loader back to the system default before launching any daemon threads.",apurtell,ekohlwey,Major,Closed,Fixed,21/Feb/11 20:22,12/Jun/22 00:51
Bug,HBASE-3560,12499541,"the hbase-default entry of ""hbase.defaults.for.version"" causes tests not to run via not-maven","using the default setup of a maven project for intellij, tests fail because the hbase-default.xml contains a token @@@VERSION@@@.  The maven build creates a substitute file, but this file isn't by default on the classpath.

Is there a different place to put the hbase-default.xml file than target/classes?  That is excluded from the classpath because IJ has it's own compiler. Including that path in the project classpath seems dangerous.

Can we get the output to go to just 'target' or 'target/generated-sources/org' perhaps?
",,ryanobjc,Major,Closed,Fixed,23/Feb/11 22:36,20/Nov/15 12:43
Bug,HBASE-3561,12499598,OPTS arguments are duplicated,"Some of the command line arguments constructed in the bash scripts are getting duplicated

Here is what my HMaster process looks like with a ps aux | grep java.

{code}
/Library/Java/Home/bin/java 
-Xmx1000m 
-ea -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode 
-ea -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode 
-ea -XX:+HeapDumpOnOutOfMemoryError -XX:+UseConcMarkSweepGC -XX:+CMSIncrementalMode 
-Dcom.sun.management.jmxremote.ssl=false 
-Dcom.sun.management.jmxremote.authenticate=false 
-Dcom.sun.management.jmxremote.port=10101 
-Dcom.sun.management.jmxremote.ssl=false 
-Dcom.sun.management.jmxremote.authenticate=false 
-Dcom.sun.management.jmxremote.port=10101 
-Dcom.sun.management.jmxremote.ssl=false 
-Dcom.sun.management.jmxremote.authenticate=false 
-Dcom.sun.management.jmxremote.port=10101 
-Dhbase.log.dir=/Users/tims/workspace/hbase-trunk/bin/../logs 
-Dhbase.log.file=hbase-tims-master-grassmann.local.log 
-Dhbase.home.dir=/Users/tims/workspace/hbase-trunk/bin/.. 
-Dhbase.id.str=tims -Dhbase.root.logger=INFO,DRFA 
-classpath <blablablablabla>
org.apache.hadoop.hbase.master.HMaster start
{code}

This wouldn't really be a problem except if you try to add a java agent in the hbase-env.sh like so:
{code}
export HBASE_MASTER_OPTS=""$HBASE_MASTER_OPTS $HBASE_JMX_BASE -Dcom.sun.management.jmxremote.port=10101 -javaagent:lib/HelloWorldAgent.jar""
{code}
It adds the option 3 times and it starts three java agents. My example agent print hello world once per second, but I end up with three hello world lines per second.

I attached my HelloWorldAgent.jar to demonstrate.


",stack,tim_s,Major,Closed,Fixed,24/Feb/11 14:46,20/Nov/15 12:41
Bug,HBASE-3566,12499663, writeToWAL is not serialized for increment operation,"Class org.apache.hadoop.hbase.client.Increment has a member
boolean writeToWAL;
that is not serialized/deserialized in write/readFields functions. As a result an operation to increment several columns within a single row always writes to WAL, even if a client calls
increment.setWriteToWAL(false);",apurtell,dmitrip,Major,Closed,Fixed,25/Feb/11 02:31,20/Nov/15 12:44
Bug,HBASE-3572,12499790,memstore lab can leave half inited data structs (bad!),"in Chunk.init() if new byte[] fails it leaves the Chunk in its uninitialized state, other threads will assume someone else will init it and get stuck in an infinite loop.",ryanobjc,ryanobjc,Major,Closed,Fixed,26/Feb/11 07:08,20/Nov/15 12:44
Bug,HBASE-3575,12499827,Update rename table script,Update rename_table.rb script.  Also fix bug where it does not update .regioninfo after rename.,stack,stack,Major,Closed,Fixed,26/Feb/11 19:16,20/Nov/15 12:43
Bug,HBASE-3578,12499910,TableInputFormat does not setup the configuration for HBase mapreduce jobs correctly,"In 0.20.x and earlier TableMapReduceUtil (and other Input/OutputFormat classes) used to setup the HTable with a HBaseConfiguration object, now that has been deprecated in #HBASE-2036 they are constructed with Hadoop configuration objects which do not contain the configuration xml file resources required to setup HBase. I think it is currently expected this is done when constructing the job but as this needs to be done for every HBase mapreduce job it would be cleaner if the TableMapReduceUtil class did this whilst setting up the TableInput/OutputFormat classes. ",danharvey,danharvey,Major,Closed,Fixed,28/Feb/11 11:51,23/Sep/13 19:08
Bug,HBASE-3582,12499983,HMaster and HRegionServer should be able to login from Kerberos keytab when running on security-enabled Hadoop,"Currently HBase can run on top of Hadoop 0.20 with security APIs, but only using the ""simple"" authentication method.  There is currently no configuration or hooks for the HBase process to obtain Kerberos credentials so it can authenticate against secure Hadoop.

We should extend the current {{org.apache.hadoop.hbase.security.User}} hooks to allow obtaining credentials from a keytab file when security is enabled.",ghelmling,ghelmling,Major,Closed,Fixed,28/Feb/11 21:53,20/Nov/15 12:43
Bug,HBASE-3583,12499994,Coprocessors: RegionObserver: ScannerNext and ScannerClose hooks are called when get() is invoked,"RegionObserver upcalls are expected to be triggered by corresponding client calls. 

I found that if a HTable.get() is issued, ScannerNext, and ScannerClose hooks are also invoked. 

Here is the reason: HRegion.get() is implemented with an internal scanner:

{code}
    InternalScanner scanner = null;
    try {
      scanner = getScanner(scan);
      scanner.next(results);
    } finally {
      if (scanner != null)
        scanner.close();
    }
{code}

where scanner.next, and scanner.close() are implemented with RegionObserver hooks. 

",mingjielai,mingjielai,Major,Closed,Fixed,28/Feb/11 23:25,20/Nov/15 12:42
Bug,HBASE-3585,12500084,isLegalFamilyName() can throw ArrayOutOfBoundException,org.apache.hadoop.hbase.HColumnDescriptor.isLegalFamilyName(byte[]) accesses byte[0] w/o first checking the array length.,umamaheswararao,khemani,Minor,Closed,Fixed,01/Mar/11 18:20,11/Aug/14 18:24
Bug,HBASE-3589,12500213,test jar should not include mapred-queues.xml and log4j.properties,"Right now we include these files in the test jar, which might cause problems when this jar ends up on the MR classpath. Similar to HBASE-3143",tlipcon,tlipcon,Major,Closed,Fixed,02/Mar/11 18:56,20/Nov/15 12:41
Bug,HBASE-3591,12500280,completebulkload doesn't honor generic -D options,"When running the completebulkload tool, the -D arguments are not being honored. For example, when setting -Dhbase.zookeeper.property.clientPort to something other than the 2181, it still tried to connect to ZK on 2181.",,michaelk,Minor,Closed,Fixed,03/Mar/11 07:01,12/Jun/22 18:12
Bug,HBASE-3593,12500324,DemoClient.cpp is outdated.,"The Thrift example of DemoClient.cpp uses old APIs in 0.90.
So, I update and clean up DemoClient.cpp.",,repeatedly,Minor,Closed,Fixed,03/Mar/11 17:29,12/Jun/22 18:13
Bug,HBASE-3594,12500339,Rest server fails because of missing asm jar,"HBASE-3525 turned off the inclusion of transitive dependencies in the hbase/lib/ dir. This means that we no longer get the asm library, which is needed by jersey.",tlipcon,tlipcon,Blocker,Closed,Fixed,03/Mar/11 18:57,20/Nov/15 12:41
Bug,HBASE-3595,12500344,get_counter broken in shell,"hbase(main):010:0> incr 't', 'r1', 'f1:c1'
COUNTER VALUE = 2

hbase(main):011:0> get_counter 't', 'r1', 'f1:c1'

ERROR: undefined method `first' for #<#<Class:01x79f7abae>:0x73286b10>
",tlipcon,tlipcon,Critical,Closed,Fixed,03/Mar/11 19:36,20/Nov/15 12:41
Bug,HBASE-3597,12500372,ageOfLastAppliedOp should update after cluster replication failures,"The value of ageOfLastAppliedOp in JMX doesn't update after replication starts failing, and it should. See: http://search-hadoop.com/m/jFPgF1HfnLc",jdcryans,otis,Major,Closed,Fixed,03/Mar/11 22:03,20/Nov/15 12:43
Bug,HBASE-3598,12500378,Broken formatting in LRU stats output,"I've seen this a few times - the output for LRU stats ends up with some invalid characters in it:

11/03/03 14:36:06 DEBUG hfile.LruBlockCache: LRU Stats: total=1.49 MB, free=180.56 MB, max=182.05 MB, blocks=0, accesses=0, hits=0, hitRatio=ï¿½%, cachingAccesses=0, cachingHits=0, cachingHitsRatio=ï¿½%, evictions=0, evicted=0, evictedPerRun=NaN

Note the messed up ""hitRatio"" and ""cachingHitsRatio"". I can't figure out how to reproduce, though.",eonnen,tlipcon,Major,Closed,Fixed,03/Mar/11 23:08,20/Nov/15 12:41
Bug,HBASE-3601,12500384,TestMasterFailover broken in TRUNK,"After HBASE-3573, went in, TestMasterFailover broke.  The change in shutdown technique revealed an issue with our in-memory accounting when a master joins an already cluster; we don't add .META. and -ROOT- to our set of online regions in the new master so could make for some interesting issues as the new master progressed (Previous shutdown did a count of remaining servers, new shutdown process looks at in-memory state to see if only catalog carrying regionservers online... this is what was going out of whack in new master).",stack,stack,Major,Closed,Fixed,04/Mar/11 00:28,20/Nov/15 12:41
Bug,HBASE-3605,12500454,Fix balancer log message,"From Gaojinchao up on user list:

In balanceCluster function , It should be ""leastloaded="" + serversByLoad.firstKey ().getLoad().getNumberOfRegions())""

{code}
if(serversByLoad.lastKey().getLoad().getNumberOfRegions() <= max &&
      serversByLoad.firstKey().getLoad().getNumberOfRegions() >= min) {
     // Skipped because no server outside (min,max) range
     LOG.info(""Skipping load balancing.  servers="" + numServers + "" "" +
         ""regions="" + numRegions + "" average="" + average + "" "" +
         ""mostloaded="" + serversByLoad.lastKey().getLoad().getNumberOfRegions() +
         "" leastloaded="" + serversByLoad.lastKey().getLoad().getNumberOfRegions());
     return null;
   }
{code}",,stack,Major,Closed,Fixed,04/Mar/11 18:07,20/Nov/15 12:41
Bug,HBASE-3608,12500695,MemstoreFlusher error message doesnt include exception!,"The log message in MemstoreFlusher doesn't contain the excception which caused it (ARGH).

Patch is like:
@@ -239,7 +239,7 @@ class MemStoreFlusher extends Thread implements FlushRequester {
       } catch (ConcurrentModificationException ex) {
         continue;
       } catch (Exception ex) {
-        LOG.error(""Cache flusher failed for entry "" + fqe);
+        LOG.error(""Cache flusher failed for entry "" + fqe, ex);
         if (!server.checkFileSystem()) {
           break;
         }
",ryanobjc,ryanobjc,Major,Closed,Fixed,07/Mar/11 22:32,20/Nov/15 12:41
Bug,HBASE-3612,12500804,HBaseAdmin::isTableAvailable( name ) returns true when the table does not exist,"HBaseAdmin::isTableAvailable( name )  returns true for a table in which HBaseAdmin::tableExists( name ) returns false.

It appears from the code that the default return value from isTableAvailable() is true and false is only returned in the case where the table is found and not all the region servers are online.",apurtell,jjh,Major,Closed,Fixed,08/Mar/11 19:19,20/Nov/15 12:43
Bug,HBASE-3613,12500850,NPE in MemStoreFlusher,"every now and again in a 0.90.1-2 load run I get a NPE on this line:


      if (bestAnyRegion.memstoreSize.get() > 2 * bestFlushableRegion.memstoreSize.get()) {
",ryanobjc,ryanobjc,Major,Closed,Fixed,09/Mar/11 07:33,20/Nov/15 12:40
Bug,HBASE-3617,12501056,NoRouteToHostException during balancing will cause Master abort,"Via Tatsuya up on the list:

{code}
2011-03-10 07:48:39,192 FATAL org.apache.hadoop.hbase.master.HMaster:
Remote unexpected exception
java.net.NoRouteToHostException: No route to host
      at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
      at
sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
      at
org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:
206)
      at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:408)
      at org.apache.hadoop.hbase.ipc.HBaseClient
$Connection.setupIOstreams(HBaseClient.java:328)
      at
org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:
883)
      at
org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:750)
      at org.apache.hadoop.hbase.ipc.HBaseRPC
$Invoker.invoke(HBaseRPC.java:257)
      at $Proxy6.closeRegion(Unknown Source)
      at
org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:
589)
      at
org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:
1093)
      at
org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:
1040)
      at
org.apache.hadoop.hbase.master.AssignmentManager.balance(AssignmentManager.java:
1831)
      at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:
692)
      at org.apache.hadoop.hbase.master.HMaster$1.chore(HMaster.java:
583)
      at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
2011-03-10 07:48:39,192 INFO org.apache.hadoop.hbase.master.HMaster:
Aborting
2011-03-10 07:48:39,192 INFO org.apache.hadoop.hbase.master.HMaster:
balance hri=SpecialObject_Speed_Test,,
1299710751983.f0e5544339870a510c338b3029979d3e.,
src=ap13.secur2,60020,1299710609447,
dest=ap12.secur2,60020,1299710609148
2011-03-10 07:48:39,192 DEBUG
org.apache.hadoop.hbase.master.AssignmentManager: Starting
unassignment of region SpecialObject_Speed_Test,,
1299710751983.f0e5544339870a510c338b3029979d3e. (offlining)
2011-03-10 07:48:39,852 DEBUG org.apache.hadoop.hbase.master.HMaster:
Stopping service threads
2011-03-10 07:48:39,852 INFO org.apache.hadoop.ipc.HBaseServer:
Stopping server on 60000
2011-03-10 07:48:39,852 FATAL org.apache.hadoop.hbase.master.HMaster:
Remote unexpected exception
java.io.InterruptedIOException: Interruped while waiting for IO on
channel java.nio.channels.SocketChannel[connection-pending remote=/
10.X.X.18:60020]. 19340 millis timeout left.
      at org.apache.hadoop.net.SocketIOWithTimeout
$SelectorPool.select(SocketIOWithTimeout.java:349)
      at
org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:
203)
      at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:408)
      at org.apache.hadoop.hbase.ipc.HBaseClient
$Connection.setupIOstreams(HBaseClient.java:328)
      at
org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:
883)
      at
org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:750)
      at org.apache.hadoop.hbase.ipc.HBaseRPC
$Invoker.invoke(HBaseRPC.java:257)
      at $Proxy6.closeRegion(Unknown Source)
      at
org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.java:
589)
      at
org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:
1093)
      at
org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:
1040)
      at
org.apache.hadoop.hbase.master.AssignmentManager.balance(AssignmentManager.java:
1831)
      at org.apache.hadoop.hbase.master.HMaster.balance(HMaster.java:
692)
      at org.apache.hadoop.hbase.master.HMaster$1.chore(HMaster.java:
583)
      at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
2011-03-10 07:48:39,852 INFO org.apache.hadoop.hbase.master.HMaster:
Aborting
{code}",yuzhihong@gmail.com,stack,Critical,Closed,Fixed,10/Mar/11 19:28,20/Nov/15 12:43
Bug,HBASE-3621,12501089,The timeout handler in AssignmentManager does an RPC while holding lock on RIT; a big no-no,J-D found this debugging a failure on Dmitriy's cluster; we're RPC'ing under a synchronized(regionsInTransition).  Fix.,yuzhihong@gmail.com,stack,Major,Closed,Fixed,11/Mar/11 00:34,20/Nov/15 12:43
Bug,HBASE-3624,12501106,Only one coprocessor of each priority type can be loaded for a table,"Coprocessors are added to HBase using a TreeSet that is initialized with an EnvironmentPriorityComparator. The net effect is that only one coprocessor of a given priority can be loaded at a time for a given table. This appears to be due to how the TreeSet uses the EnvironmentPriorityComparator to determine whether there are duplicate entries - if the coprocessors have the same priority (e.g., User), they are considered the same and won't be added to the Set.",apurtell,jessedaniels,Major,Closed,Fixed,11/Mar/11 04:23,20/Nov/15 12:41
Bug,HBASE-3627,12501217,NPE in EventHandler when region already reassigned,"When a region takes too long to open, it will try to update the unassigned znode and will fail on an ugly NPE like this:

{quote}
DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x22dc571dde04ca7 Attempting to transition node 0519dc3b62a569347526875048c37faa from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: regionserver:60020-0x22dc571dde04ca7 Unable to get data of znode /hbase/unassigned/0519dc3b62a569347526875048c37faa because node does not exist (not necessarily an error)
ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_RS_OPEN_REGION
java.lang.NullPointerException
	at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:75)
	at org.apache.hadoop.hbase.executor.RegionTransitionData.fromBytes(RegionTransitionData.java:198)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.transitionNode(ZKAssign.java:672)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.retransitionNodeOpening(ZKAssign.java:585)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.tickleOpening(OpenRegionHandler.java:322)
	at org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler.process(OpenRegionHandler.java:97)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
{quote}

I think the region server in this case should be closing the region ASAP.",stack,jdcryans,Critical,Closed,Fixed,12/Mar/11 00:13,20/Nov/15 12:43
Bug,HBASE-3630,12501292,DemoClient.Java is outdated.,"Similar to HBASE-3593. This patch upgrades DemoClient.Java to Thrift 0.5/0.6, fixes the check for allowing non-utf8 in row names, and eliminates the hard-coded host/port.",moazreyad,moazreyad,Minor,Closed,Fixed,13/Mar/11 09:06,20/Nov/15 12:41
Bug,HBASE-3631,12501319,CLONE - HBase 2984 breaks ability to specify BLOOMFILTER & COMPRESSION via shell,"HBase 2984 breaks ability to specify BLOOMFILTER & COMPRESSION via shell

0.90 was fixed but in trunk there is still bug",kannanm,anih,Minor,Closed,Fixed,14/Mar/11 07:43,20/Nov/15 12:42
Bug,HBASE-3633,12501324,ZKUtil::createSetData should only create a node when it nonexists,"This is a typo.  The predicate tests for the wrong answer.  Please see the patch, which is clear.",,xgp,Major,Closed,Fixed,14/Mar/11 09:29,20/Nov/15 12:42
Bug,HBASE-3636,12501389,a bug about deciding whether this key is a new key for the ROWCOL bloomfilter,"When ROWCOL bloomfilter needs to decide whether this key is a new key or not,
it will call the matchingRowColumn function, which will compare the timestamp offset between this kv and last kv.
But when checking the timestamp offset, it didn't deduct the original offset of the keyvalue itself.

For example, when 2 keyvalue objects have the same row key and col key, but from different storefiles. It is highly likely that these 2 keyvalue objects have different offset value. So the timestamp offset of these 2 objects are totally different. They will be regard as new keys to add into bloomfilters.
So after compaction, the key count of bloomfilter will increase immediately, which is almost equal to the number of entries.

The solution is straightforward. Just compare the relevant timestamp offset, which is the timestamp offset - key_value offset.

This also may explain this jira: https://issues.apache.org/jira/browse/HBASE-3007",,liyin,Major,Closed,Fixed,14/Mar/11 18:31,17/Mar/11 19:34
Bug,HBASE-3639,12501413,FSUtils.getRootDir should qualify path,Currently you can run into a StackOverflowError if the hbase root dir is on the non-default filesystem. Making FSUtils.getRootDir qualify its path solves this issue.,tlipcon,tlipcon,Critical,Closed,Fixed,14/Mar/11 23:05,20/Nov/15 12:41
Bug,HBASE-3641,12501415,LruBlockCache.CacheStats.getHitCount() is not using the correct variable,"{code}
    public long getHitCount() {
      return hitCachingCount.get();
    }
{code}

This should be {{hitCount.get()}}",streamy,streamy,Major,Closed,Fixed,14/Mar/11 23:22,20/Nov/15 12:42
Bug,HBASE-3648,12501494,[replication] failover is sloppy with znodes,"ReplicationZookeeper is a bit sloppy in how it handles the znodes during failover:

- when creating the lock, it doesn't cleanly handle the situation where the parent znode might already be deleted.
- when deleting the znodes after a successful move, it doesn't make sure to delete the lock znode last.
- after deleting the lock, there's a window where another region server could have already created another lock and deleted the znodes which would abort the first region server (saw it on one cluster).",jdcryans,jdcryans,Critical,Closed,Fixed,15/Mar/11 18:34,20/Nov/15 12:42
Bug,HBASE-3650,12501512,HBA.delete can return too fast,"One of our engineers got a weird TableExistsException in his code and I see that the client-side logging says the table was deleted in less than a second while it took the master 5 seconds to do it. Doing code inspection, the .META. scanner in HBA.delete can set _found_  to _true_ and then set it back to _false_ in the case where the deleted table isn't the last one. We should just do a scan that would get the rows specific to the table instead of scanning *all* the rows.",jdcryans,jdcryans,Blocker,Closed,Fixed,15/Mar/11 21:05,20/Nov/15 12:42
Bug,HBASE-3654,12501544,Weird blocking between getOnlineRegion and createRegionLoad,"Saw this when debugging something else:
{code}

""regionserver60020"" prio=10 tid=0x00007f538c1c0000 nid=0x4c7 runnable [0x00007f53931da000]
   java.lang.Thread.State: RUNNABLE
	at org.apache.hadoop.hbase.regionserver.Store.getStorefilesIndexSize(Store.java:1380)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.createRegionLoad(HRegionServer.java:916)
	- locked <0x0000000672aa0a00> (a java.util.concurrent.ConcurrentSkipListMap)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.buildServerLoad(HRegionServer.java:767)
	- locked <0x0000000656f62710> (a java.util.HashMap)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryRegionServerReport(HRegionServer.java:722)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:591)
	at java.lang.Thread.run(Thread.java:662)

""IPC Reader 9 on port 60020"" prio=10 tid=0x00007f538c1be000 nid=0x4c6 waiting for monitor entry [0x00007f53932db000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getFromOnlineRegions(HRegionServer.java:2295)
	- waiting to lock <0x0000000656f62710> (a java.util.HashMap)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getOnlineRegion(HRegionServer.java:2307)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2333)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$QosFunction.isMetaRegion(HRegionServer.java:379)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$QosFunction.apply(HRegionServer.java:422)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$QosFunction.apply(HRegionServer.java:361)
	at org.apache.hadoop.hbase.ipc.HBaseServer.getQosLevel(HBaseServer.java:1126)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:982)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:946)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:522)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:316)
	- locked <0x0000000656e60068> (a org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
...

""IPC Reader 0 on port 60020"" prio=10 tid=0x00007f538c08b000 nid=0x4bd waiting for monitor entry [0x00007f5393be4000]
   java.lang.Thread.State: BLOCKED (on object monitor)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getFromOnlineRegions(HRegionServer.java:2295)
	- waiting to lock <0x0000000656f62710> (a java.util.HashMap)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getOnlineRegion(HRegionServer.java:2307)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.getRegion(HRegionServer.java:2333)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$QosFunction.isMetaRegion(HRegionServer.java:379)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$QosFunction.apply(HRegionServer.java:422)
	at org.apache.hadoop.hbase.regionserver.HRegionServer$QosFunction.apply(HRegionServer.java:361)
	at org.apache.hadoop.hbase.ipc.HBaseServer.getQosLevel(HBaseServer.java:1126)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:982)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:946)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:522)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:316)
	- locked <0x0000000656e635c8> (a org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}

All the readers are blocked! I have the feeling something much better could be done.",iamknome,jdcryans,Blocker,Closed,Fixed,16/Mar/11 04:09,20/Nov/15 12:43
Bug,HBASE-3659,12501640,TestHLog fails on newer Hadoop,"There are some subtle changes in IPC in the newer Hadoop trees that cause TestHLog to fail. The issue is that when it restarts the minicluster, the new DNs pick up a cached IPC connection to the old NN, which has closed its end of the IPC pipe. This causes the new DNs to fail to start and the test fails. Adding a strategic sleep fixes it.",tlipcon,tlipcon,Minor,Closed,Fixed,17/Mar/11 01:18,20/Nov/15 12:42
Bug,HBASE-3660,12501669,HMaster will exit when starting with stale data in cached locations such as -ROOT- or .META.,"later edit: I've mixed up two issues here. The main problem is that a client (that could be HMaster) will read stale data from -ROOT- or .META. and not deal correctly with the raised exceptions. 

I've noticed this when the IP on my machine changed (it's even easier to detect when LZO doesn't work)

Master loads .META. successfully and then starts assigning regions.
However LZO doesn't work so HRegionServer can't open the regions. 
A client attempts to get data from a table so it reads the location from .META. but goes to a totally different server (the old value in .META.)

This could happen without the LZO story too.",stack,clehene,Critical,Closed,Fixed,17/Mar/11 11:09,20/Nov/15 12:43
Bug,HBASE-3662,12501706,REST server does not respect client supplied max versions when creating scanner,"In {{org.apache.hadoop.hbase.rest.ScannerResource.update()}}, where a new scanner is created on the server, we don't seem to respect the max versions value sent by the client:
{noformat}

    RowSpec spec = new RowSpec(model.getStartRow(), endRow,
      model.getColumns(), model.getStartTime(), model.getEndTime(), 1);
{noformat}

Looks to me like the last argument of ""1"" should instead be {{model.getMaxVersions()}}.",apurtell,ghelmling,Major,Closed,Fixed,17/Mar/11 17:25,20/Nov/15 12:42
Bug,HBASE-3664,12501717,[replication] Adding a slave when there's none may kill the cluster,"RSM logs all the new hlogs in memory but if there's no slave then it won't be added in zookeeper. This is a problem when a slave is finally added because the first time it logs the progress in ZK it will try to clean the old logs and will try to delete a bunch of znodes that don't exist. This throws a NoNodeException which is handled by aborting the region server. It looks like this:

{quote}
2011-03-17 17:38:29,832 INFO org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager: Removing 42 logs in the list: [sv2borg172%3A60020.1300351782310, sv2borg172%3A60020.1300352001486, sv2borg172%3A60020.1300352121367, sv2borg172%3A60020.1300352772392, sv2borg172%3A60020.1300354158262, sv2borg172%3A60020.1300355578566, sv2borg172%3A60020.1300356840451, sv2borg172%3A60020.1300358106115, sv2borg172%3A60020.1300359494020, sv2borg172%3A60020.1300360803514, sv2borg172%3A60020.1300362078570, sv2borg172%3A60020.1300363300908, sv2borg172%3A60020.1300364449495, sv2borg172%3A60020.1300365539396, sv2borg172%3A60020.1300366546548, sv2borg172%3A60020.1300367485952, sv2borg172%3A60020.1300368371234, sv2borg172%3A60020.1300369227069, sv2borg172%3A60020.1300370079940, sv2borg172%3A60020.1300370899710, sv2borg172%3A60020.1300371697355, sv2borg172%3A60020.1300372472873, sv2borg172%3A60020.1300373238890, sv2borg172%3A60020.1300374001201, sv2borg172%3A60020.1300374738469, sv2borg172%3A60020.1300375453876, sv2borg172%3A60020.1300376155468, sv2borg172%3A60020.1300376860049, sv2borg172%3A60020.1300377555922, sv2borg172%3A60020.1300378246690, sv2borg172%3A60020.1300378917995, sv2borg172%3A60020.1300379573664, sv2borg172%3A60020.1300380218543, sv2borg172%3A60020.1300380861201, sv2borg172%3A60020.1300381485824, sv2borg172%3A60020.1300381550415, sv2borg172%3A60020.1300381596287, sv2borg172%3A60020.1300381655746, sv2borg172%3A60020.1300381720905, sv2borg172%3A60020.1300382140669, sv2borg172%3A60020.1300382598288, sv2borg172%3A60020.1300383006771]
2011-03-17 17:38:29,868 FATAL org.apache.hadoop.hbase.regionserver.HRegionServer: ABORTING region server serverName=sv2borg172,60020,1300351781748, load=(requests=3869, regions=526, usedHeap=4475, maxHeap=7973): Failed remove from list
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase/replication/rs/sv2borg172,60020,1300351781748/2/sv2borg172%3A60020.1300351782310
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:728)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:959)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:948)
        at org.apache.hadoop.hbase.replication.ReplicationZookeeper.removeLogFromList(ReplicationZookeeper.java:413)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSourceManager.logPositionAndCleanOldLogs(ReplicationSourceManager.java:141)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:332)

{quote}

If there's no slave, only the latest hlog should be kept in memory.",jdcryans,jdcryans,Major,Closed,Fixed,17/Mar/11 18:30,20/Nov/15 12:41
Bug,HBASE-3666,12501727,TestScannerTimeout fails occasionally,"LeaseExceptionIf I loop TestScannerTimeout, it eventually fails with:

org.apache.hadoop.hbase.regionserver.LeaseException: org.apache.hadoop.hbase.regionserver.LeaseException: lease '-4526340287831625207' does not exist
        at org.apache.hadoop.hbase.regionserver.Leases.cancelLease(Leases.java:209)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:1816)
...
        at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:83)
        at org.apache.hadoop.hbase.client.ScannerCallable.call(ScannerCallable.java:38)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.java:1003)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.next(HTable.java:1103)
        at org.apache.hadoop.hbase.client.HTable$ClientScanner.next(HTable.java:1175)
        at org.apache.hadoop.hbase.client.TestScannerTimeout.test2772(TestScannerTimeout.java:133)

I think the issue is a race where at the top of the function, the scanner does exist, but by the time it gets to cancelLease, it has timed out.",tlipcon,tlipcon,Major,Closed,Fixed,17/Mar/11 20:18,20/Nov/15 12:41
Bug,HBASE-3668,12501739,CatalogTracker.waitForMeta can wait forever and totally stall a RS,"We got into a weird situation after hitting HBASE-3664 leaving almost half the region servers unable to open regions (we didn't kill the master but all RS were restarted).

Here's the relevant jstack from the region servers:

{code}

""regionserver60020-EventThread"" daemon prio=10 tid=0x0000000041297800 nid=0x5c5 in Object.wait() [0x00007fbd2b7f6000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
	at java.lang.Object.wait(Native Method)
	- waiting on <0x00007fbd4e1ac6d0> (a java.util.concurrent.atomic.AtomicBoolean)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:328)
	- locked <0x00007fbd4e1ac6d0> (a java.util.concurrent.atomic.AtomicBoolean)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:363)
	at org.apache.hadoop.hbase.zookeeper.MetaNodeTracker.nodeDeleted(MetaNodeTracker.java:64)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.nodeCreated(ZooKeeperNodeTracker.java:154)
	- locked <0x00007fbd4d7ab060> (a org.apache.hadoop.hbase.zookeeper.MetaNodeTracker)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.nodeDataChanged(ZooKeeperNodeTracker.java:179)
	- locked <0x00007fbd4d7ab060> (a org.apache.hadoop.hbase.zookeeper.MetaNodeTracker)
	at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:268)
	at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:530)
	at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:506)
{code}

Every server in that state cannot receive any ZK event. See how we first do a nodeDataChanged, then nodeCreated, then nodeDeleted. This is correlated in the logs:

{code}
2011-03-17 17:57:03,636 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: regionserver:60020-0x12d627b723e081f Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/unassigned/1028785192
2011-03-17 17:57:03,636 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: regionserver:60020-0x12d627b723e081f Unable to get data of znode /hbase/unassigned/1028785192 because node does not exist (not an error)
2011-03-17 17:57:03,637 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: regionserver:60020-0x12d627b723e081f Set watcher on existing znode /hbase/unassigned/1028785192
2011-03-17 17:57:03,637 INFO org.apache.hadoop.hbase.zookeeper.MetaNodeTracker: Detected completed assignment of META, notifying catalog tracker
2011-03-17 17:57:06,988 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: Lookedup root region location, connection=org.
{code}

Node is updated, then is missing, than came back! The reason we're stuck is that CT.waitForMeta will wait forever:
{code}
     if (getMetaServerConnection(true) != null) {
        return metaLocation;
      }
      while(!stopped && !metaAvailable.get() &&
          (timeout == 0 || System.currentTimeMillis() < stop)) {
        metaAvailable.wait(timeout);
      }
{code}

So when it tried getMetaServerConnection the first time it didn't work, and then since the timeout is 0 then it waits. Even if it was looping, metaAvailable will never be true since we're already inside the event thread thus blocking any other event!

This is what happens when the RS then tries to update .META. after opening a region:

{code}
2011-03-17 17:59:06,557 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Interrupting thread Thread[PostOpenDeployTasks:f06b630822a161d0a8fe37481d851d05,5,main]
2011-03-17 17:59:06,557 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Exception running postOpenDeployTasks; region=f06b630822a161d0a8fe37481d851d05
org.apache.hadoop.hbase.NotAllMetaRegionsOnlineException: Interrupted
        at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:365)
        at org.apache.hadoop.hbase.catalog.MetaEditor.updateRegionLocation(MetaEditor.java:142)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.postOpenDeployTasks(HRegionServer.java:1359)

{code}

The master eventually tries to assign the region somewhere else where it may work, but then the balancer will screw everything up again.",jdcryans,jdcryans,Blocker,Closed,Fixed,17/Mar/11 21:57,20/Nov/15 12:44
Bug,HBASE-3670,12501772,Fix error handling in get(List<Get> gets),"See HBASE-3634 for details. The get(List<Get> gets) call needs to catch (or rather use a try/finally) the exception thrown by batch() and copy the Result instances over and return it. If that is not intended then we need to fix the JavaDoc in HTableInterface to reflect the new behavior. 

In general it seems to make sense to check the various methods (list based put, get, delete compared to batch) and agree on the correct behavior.",qwertymaniac,larsgeorge,Major,Closed,Fixed,18/Mar/11 09:57,20/Nov/15 12:42
Bug,HBASE-3671,12501811,Split report before we finish parent region open; workaround till 0.92; Race between split and OPENED processing,"This issue is about adding a workaround to 0.90 until we get proper fix in 0.92 (HBASE-3559).

Here is the sequence of events:

1. We start to process OPENED region event.
2. We receive a SPLIT of this region report.
3. SPLIT processing offline the region and onlines daughters.
4. Metascanner runs and clears out the region from .META. deleting it
5. The OPENED handler runs.  Marks the region online in Master memory.
6. Balancer runs.  Trys to balance a region that has been deleted.

Loops for ever.

Here is excerpt from logs.  It happened during startup, lots going on.  Could happen on regionserver crash I suppose, maybe, but we're susceptible during cluster start:

{code}
# We assign the region
2011-03-16 15:18:29,053 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x22e286f0b9c98f1 Async create of unassigned node for 3516b74d0c9d4458c2f2f715249e3f78 with OFFLINE state
...
2011-03-16 15:18:32,298 DEBUG org.apache.hadoop.hbase.master.AssignmentManager$CreateUnassignedAsyncCallback: rs=tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. state=OFFLINE, ts=1300313909053, server=sv4borg39,60020,1300313564807
...
2011-03-16 15:18:32,732 DEBUG org.apache.hadoop.hbase.master.AssignmentManager$ExistsUnassignedAsyncCallback: rs=tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. state=OFFLINE, ts=1300313909053
...
2011-03-16 15:23:02,114 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: master:60000-0x22e286f0b9c98f1 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/prodjobs/unassigned/3516b74d0c9d4458c2f2f715249e3f78
...
2011-03-16 15:23:02,183 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x22e286f0b9c98f1 Retrieved 127 byte(s) of data from znode /prodjobs/unassigned/3516b74d0c9d4458c2f2f715249e3f78 and set watcher; region=tsdb,^@^D2McZ@^@^@^A^@^@G^@^@^L^@^@f^@^@^U^@^@�^@^@(^@^C^G,1299401073466.3516b74d0c9d4458c2f2f715249e3f78., server=sv4borg39,60020,1300313564807, state=RS_ZK_REGION_OPENED
2011-03-16 15:23:02,183 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=sv4borg39,60020,1300313564807, region=3516b74d0c9d4458c2f2f715249e3f78

# At this point we've queued an Excecutor to run to process the OPENED event.  Now in comes the SPLIT.
2011-03-16 15:23:18,199 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78.: Daughters; tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1300314189812.74c51400bb8dfa127fadfd11a04d72f2., tsdb,\x00\x042MmD\x88\x00\x00\x01\x00\x00S\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x029\x00\x00(\x00\x03\x03,1300314189812.87b061739a11d0f9d02acfb92ef961a2. from sv4borg39,60020,1300313564807
2011-03-16 15:23:18,870 WARN org.apache.hadoop.hbase.master.AssignmentManager: Split report has RIT node (shouldnt have one): REGION => {NAME => 'tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78.', STARTKEY => '\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07', ENDKEY => '\x00\x043L\xE7\xF50\x00\x00\x01\x00\x00I\x00\x00\x0C\x00\x00f\x00\x00\x0E\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x02u', ENCODED => 3516b74d0c9d4458c2f2f715249e3f78, TABLE => {{NAME => 'tsdb', FAMILIES => [{NAME => 't', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', VERSIONS => '3', COMPRESSION => 'LZO', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}} node: region=tsdb,^@^D2McZ@^@^@^A^@^@G^@^@^L^@^@f^@^@^U^@^@�^@^@(^@^C^G,1299401073466.3516b74d0c9d4458c2f2f715249e3f78., server=sv4borg39,60020,1300313564807, state=RS_ZK_REGION_OPENED

# Now metascanner runs and actually removes the parent region, deleting it all

2011-03-16 15:28:34,352 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1300314189812.74c51400bb8dfa127fadfd11a04d72f2., qualifier=splitA, from parent tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78.
2011-03-16 15:28:34,356 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted daughter reference tsdb,\x00\x042MmD\x88\x00\x00\x01\x00\x00S\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x029\x00\x00(\x00\x03\x03,1300314189812.87b061739a11d0f9d02acfb92ef961a2., qualifier=splitB, from parent tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78.
2011-03-16 15:28:34,356 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Deleting region tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. because daughter splits no longer hold references
2011-03-16 15:28:34,356 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Deleting region tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. because daughter splits no longer hold references

2011-03-16 15:28:34,444 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: DELETING region hdfs://sv4borg29:9000/hbase/tsdb/3516b74d0c9d4458c2f2f715249e3f78

2011-03-16 15:28:34,542 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Deleted region tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. from META

# Now the OPENED executor runs, a good while after the above
#

2011-03-16 15:30:26,679 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for 3516b74d0c9d4458c2f2f715249e3f78; deleting unassigned node
2011-03-16 15:30:26,679 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x22e286f0b9c98f1 Deleting existing unassigned node for 3516b74d0c9d4458c2f2f715249e3f78 that is in expected state RS_ZK_REGION_OPENED


2011-03-16 15:30:26,725 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x22e286f0b9c98f1 Retrieved 127 byte(s) of data from znode /prodjobs/unassigned/3516b74d0c9d4458c2f2f715249e3f78; data=region=tsdb,^@^D2McZ@^@^@^A^@^@G^@^@^L^@^@f^@^@^U^@^@�^@^@(^@^C^G,1299401073466.3516b74d0c9d4458c2f2f715249e3f78., server=sv4borg39,60020,1300313564807, state=RS_ZK_REGION_OPENED


2011-03-16 15:30:26,875 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x22e286f0b9c98f1 Successfully deleted unassigned node for region 3516b74d0c9d4458c2f2f715249e3f78 in expected state RS_ZK_REGION_OPENED


2011-03-16 15:30:27,051 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: master:60000-0x22e286f0b9c98f1 Received ZooKeeper Event, type=NodeDeleted, state=SyncConnected, path=/prodjobs/unassigned/3516b74d0c9d4458c2f2f715249e3f78

2011-03-16 15:30:27,051 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. on sv4borg39,60020,1300313564807

# Now we have a region online in Master's memory but its not out in .META. nor in the FS.
# The balancer runs

2011-03-16 23:18:41,716 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78., src=sv4borg39,60020,1300313564807, dest=sv4borg33,60020,1300342574666

2011-03-16 23:18:41,716 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. (offlining)

2011-03-16 23:18:41,718 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Server serverName=sv4borg39,60020,1300313564807, load=(requests=2, regions=504, usedHeap=929, maxHeap=6973) returned org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Received close for tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. but we are not serving it for 3516b74d0c9d4458c2f2f715249e3f78

2011-03-16 23:20:34,436 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. state=PENDING_CLOSE, ts=1300342802734

2011-03-16 23:20:34,437 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78.

2011-03-16 23:20:34,437 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x22e286f0b9c98f1 Set watcher on existing znode /prodjobs/unassigned/3516b74d0c9d4458c2f2f715249e3f78

2011-03-16 23:20:34,437 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. (offlining)
2011-03-16 23:20:34,438 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Attempting to unassign region tsdb,\x00\x042McZ@\x00\x00\x01\x00\x00G\x00\x00\x0C\x00\x00f\x00\x00\x15\x00\x00\xA9\x00\x00(\x00\x03\x07,1299401073466.3516b74d0c9d4458c2f2f715249e3f78. which is already pending close but forcing an additional close

{code}

Ad infinitum",stack,stack,Major,Closed,Fixed,18/Mar/11 17:42,12/Jun/22 00:54
Bug,HBASE-3674,12501846,Treat ChecksumException as we would a ParseException splitting logs; else we replay split on every restart,"In short, a ChecksumException will fail log processing for a server so we skip out w/o archiving logs.  On restart, we'll then reprocess the logs -- hit the checksumexception anew, usually -- and so on.

Here is the splitLog method (edited):

{code}
  private List<Path> splitLog(final FileStatus[] logfiles) throws IOException {
    ....
    outputSink.startWriterThreads(entryBuffers);
    
    try {
      int i = 0;
      for (FileStatus log : logfiles) {
       Path logPath = log.getPath();
        long logLength = log.getLen();
        splitSize += logLength;
        LOG.debug(""Splitting hlog "" + (i++ + 1) + "" of "" + logfiles.length
            + "": "" + logPath + "", length="" + logLength);
        try {
          recoverFileLease(fs, logPath, conf);
          parseHLog(log, entryBuffers, fs, conf);
          processedLogs.add(logPath);
        } catch (EOFException eof) {
          // truncated files are expected if a RS crashes (see HBASE-2643)
          LOG.info(""EOF from hlog "" + logPath + "". Continuing"");
          processedLogs.add(logPath);
        } catch (FileNotFoundException fnfe) {
          // A file may be missing if the region server was able to archive it
          // before shutting down. This means the edits were persisted already
          LOG.info(""A log was missing "" + logPath +
              "", probably because it was moved by the"" +
              "" now dead region server. Continuing"");
          processedLogs.add(logPath);
        } catch (IOException e) {
          // If the IOE resulted from bad file format,
          // then this problem is idempotent and retrying won't help
          if (e.getCause() instanceof ParseException ||
              e.getCause() instanceof ChecksumException) {
            LOG.warn(""ParseException from hlog "" + logPath + "".  continuing"");
            processedLogs.add(logPath);
          } else {
            if (skipErrors) {
              LOG.info(""Got while parsing hlog "" + logPath +
                "". Marking as corrupted"", e);
              corruptedLogs.add(logPath);
            } else {
              throw e;
            }
          }
        }
      }
      if (fs.listStatus(srcDir).length > processedLogs.size()
          + corruptedLogs.size()) {
        throw new OrphanHLogAfterSplitException(
            ""Discovered orphan hlog after split. Maybe the ""
            + ""HRegionServer was not dead when we started"");
      }
      archiveLogs(srcDir, corruptedLogs, processedLogs, oldLogDir, fs, conf);      
    } finally {
      splits = outputSink.finishWritingAndClose();
    }
    return splits;
  }
{code}

Notice how we'll only archive logs only if we successfully split all logs.  We won't archive 31 of 35 files if we happen to get a checksum exception on file 32.

I think we should treat a ChecksumException the same as a ParseException; a retry will not fix it if HDFS could not get around the ChecksumException (seems like in our case all replicas were corrupt).

Here is a play-by-play from the logs:

{code}
813572 2011-03-18 20:31:44,687 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Splitting hlog 34 of 35: hdfs://sv2borg170:9000/hbase/.logs/sv2borg182,60020,1300384550664/sv2borg182%3A60020.1300461329481, length=150       65662813573 2011-03-18 20:31:44,687 INFO org.apache.hadoop.hbase.util.FSUtils: Recovering file hdfs://sv2borg170:9000/hbase/.logs/sv2borg182,60020,1300384550664/sv2borg182%3A60020.1300461329481
....
813617 2011-03-18 20:31:46,238 INFO org.apache.hadoop.fs.FSInputChecker: Found checksum error: b[0, 512]=000000cd000000502037383661376439656265643938636463343433386132343631323633303239371d6170695f6163636573735f746f6b656e5f7374       6174735f6275636b65740000000d9fa4d5dc0000012ec9c7cbaf00ffffffff000000010000006d0000005d00000008002337626262663764626431616561366234616130656334383436653732333132643a32390764656661756c746170695f616e64726f69645f6c6f67676564       696e5f73686172655f70656e64696e675f696e69740000012ec956b02804000000000000000100000000ffffffff4e128eca0eb078d0652b0abac467fd09000000cd000000502034663166613763666165333930666332653138346233393931303132623366331d6170695f6163       636573735f746f6b656e5f73746174735f6275636b65740000000d9fa4d5dd0000012ec9c7cbaf00ffffffff000000010000006d0000005d00000008002366303734323966643036323862636530336238333938356239316237386633353a32390764656661756c746170695f61       6e64726f69645f6c6f67676564696e5f73686172655f70656e64696e675f696e69740000012ec9569f1804000000000000000100000000000000d30000004e2066663763393964303633343339666531666461633761616632613964643631331b6170695f6163636573735f746f       6b656e5f73746174735f68
813618 org.apache.hadoop.fs.ChecksumException: Checksum error: /blk_7781725413191608261:of:/hbase/.logs/sv2borg182,60020,1300384550664/sv2borg182%3A60020.1300461329481 at 15064576
813619         at org.apache.hadoop.fs.FSInputChecker.verifySum(FSInputChecker.java:277)
813620         at org.apache.hadoop.fs.FSInputChecker.readChecksumChunk(FSInputChecker.java:241)
813621         at org.apache.hadoop.fs.FSInputChecker.fill(FSInputChecker.java:176)
813622         at org.apache.hadoop.fs.FSInputChecker.read1(FSInputChecker.java:193)
813623         at org.apache.hadoop.fs.FSInputChecker.read(FSInputChecker.java:158)
813624         at org.apache.hadoop.hdfs.DFSClient$BlockReader.read(DFSClient.java:1175)
813625         at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.readBuffer(DFSClient.java:1807)
813626         at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:1859)
813627         at java.io.DataInputStream.read(DataInputStream.java:132)
813628         at java.io.DataInputStream.readFully(DataInputStream.java:178)
813629         at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:63)
813630         at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:101)
813631         at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1937)
813632         at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1837)
813633         at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1883)
813634         at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:198)
813635         at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:172)
813636         at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.parseHLog(HLogSplitter.java:429)
813637         at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:262)
813638         at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:188)
813639         at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:197)
813640         at org.apache.hadoop.hbase.master.MasterFileSystem.splitLogAfterStartup(MasterFileSystem.java:181)
813641         at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:384)
813642         at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:283)
813643 2011-03-18 20:31:46,239 WARN org.apache.hadoop.hdfs.DFSClient: Found Checksum error for blk_7781725413191608261_14589573 from 10.20.20.182:50010 at 15064576
813644 2011-03-18 20:31:46,240 INFO org.apache.hadoop.hdfs.DFSClient: Could not obtain block blk_7781725413191608261_14589573 from any node: java.io.IOException: No live nodes contain current block. Will get new block locations        from namenode and retry...
813645 2011-03-18 20:31:49,243 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Pushed=80624 entries from hdfs://sv2borg170:9000/hbase/.logs/sv2borg182,60020,1300384550664/sv2borg182%3A60020.1300461329481
....
{code}

See code above.  On exception we'll dump edits read so far from this block, close out all writers tying off recovered.edits so far written.  We'll skip archiving these files because we only archive if all files are processed; we won't archive files 30 of 35 if we failed splitting on file 31.

I think checksumexception should be treated same as a ParseException

  

",stack,stack,Critical,Closed,Fixed,18/Mar/11 23:49,20/Nov/15 12:42
Bug,HBASE-3685,12502072,"when multiple columns are combined with TimestampFilter, only one column is returned","As reported by an Hbase user: 

""I have a ThreadMetadata column family, and there are two columns in it: v12:th: and v12:me. The following code only returns v12:me

get.addColumn(Bytes.toBytes(""ThreadMetadata""), Bytes.toBytes(""v12:th:"");
get.addColumn(Bytes.toBytes(""ThreadMetadata""), Bytes.toBytes(""v12:me:"");
List<Long> threadIds = new ArrayList<Long>();
threadIds.add(10709L);
TimestampFilter filter = new TimestampFilter(threadIds);
get.setFilter(filter);
get.setMaxVersions();
Result result = table.get(get);

I checked hbase for the key/value, they are present. Also other combinations like no timestampfilter, it returns both.""

Kannan was able to do a small repro of the issue and commented that if we drop the get.setMaxVersions(), then the problem goes away. ",,gqchen,Minor,Closed,Fixed,22/Mar/11 18:17,12/Jun/22 00:56
Bug,HBASE-3686,12502081,ClientScanner skips too many rows on recovery if using scanner caching,"This can cause rows to be lost from a scan.

See this thread where the issue was brought up: http://search-hadoop.com/m/xITBQ136xGJ1

If hbase.regionserver.lease.period is higher on the client than the server we can get this series of events: 

1. Client is scanning along happily, and does something slow.
2. Scanner times out on region server
3. Client calls HTable.ClientScanner.next()
4. The region server throws an UnknownScannerException
5. Client catches exception and sees that it's not longer then it's hbase.regionserver.lease.period config, so it doesn't throw a ScannerTimeoutException. Instead, it treats it like a NSRE.

Right now the workaround is to make sure the configs are consistent. 

A possible fix would be to use whatever the region server's scanner timeout is, rather than the local one.",ssechrist,ssechrist,Minor,Closed,Fixed,22/Mar/11 18:58,12/Jun/22 00:56
Bug,HBASE-3687,12502089,Bulk assign on startup should handle a ServerNotRunningException,"On startup, we do bulk assign.  At the moment, if any problem during bulk assign, we consider startup failed and expectation is that you need to retry (We need to make this better but that is not what this issue is about).  One exception that we should handle is the case where a RS is slow coming up and its rpc is not yet up listening.  In this case it will throw: ServerNotRunningException.  We should retry at least this one exception during bulk assign.

We had this happen to us starting up a prod cluster.",stack,stack,Major,Closed,Fixed,22/Mar/11 19:18,20/Nov/15 12:42
Bug,HBASE-3688,12502107,Setters of class HTableDescriptor do not work properly,"Setters ""setName()"" and ""setDeferredLogFlush()"" do not work properly because for example after calling setName() the internal property nameAsString is not modified and then if you call getNameAsString() you get the previous value and not the new one. Something similar happens to the setter ""setDeferredLogFlush()""",,adrianromero,Major,Closed,Fixed,22/Mar/11 21:29,20/Nov/15 12:43
Bug,HBASE-3690,12502123,Option to Exclude Bulk Import Files from Minor Compaction,We ran an incremental scrape with HFileOutputFormat and encountered major compaction storms.  This is caused by the bug in HBASE-3404.  The permanent fix is a little tricky without HBASE-2856.  We realized that a quicker solution for avoiding these compaction storms is to simply exclude bulk import files from minor compactions and let them only be handled by time-based major compactions.  Add with functionality along with a config option to enable it.,nspiegelberg,nspiegelberg,Minor,Closed,Fixed,23/Mar/11 01:17,12/Oct/12 05:35
Bug,HBASE-3697,12502246,Admin actions that use MetaReader to iterate regions need to skip offline ones,If a table has offline regions in META then clients using HBaseAdmin actions like #flush or #compact will get a NSRE and abort. For example the shell. This can happen normally after a split before the old parent is collected. ,apurtell,apurtell,Major,Closed,Fixed,24/Mar/11 01:29,20/Nov/15 12:41
Bug,HBASE-3702,12502432,Exec throws a npe while writing a method that has a null value argument,"Exec write method invokes getClass() on its arguments list for finding the argument's class, which gives a npe in case the argument is null. There is already an parameterClasses array in Invoker (its super class), which is populated with correct values (by method.getParameterTypes()). One can use this array.",ghelmling,v.himanshu,Major,Closed,Fixed,25/Mar/11 23:49,20/Nov/15 12:42
Bug,HBASE-3703,12502477,hbase-config.sh needs to be updated so it can auto-detects the sun jdk provided by RHEL6,"RHEL6 will install its sun jdk in /usr/lib/jvm/java-1.6.0-sun-1.6.0.<update_version>.<arch>.
So this ticket is about adding this path to the jdk autodetection mechanism used in hbase-config.sh",bmahe,bmahe,Major,Closed,Fixed,26/Mar/11 20:26,20/Nov/15 12:40
Bug,HBASE-3708,12502637,createAndFailSilent is not so silent; leaves lots of logging in ensemble logs,"Clients on startup create a ZKWatcher instance.  Part of construction is check that hbase dirs are all up in zk.  Its done by making the following call: http://hbase.apache.org/xref/org/apache/hadoop/hbase/zookeeper/ZKUtil.html#898

A user complains that its making for lots of logging every second over on the zk ensemble:

14:59 ....seeing lots of these in the ZK log though, dozens per second of ""Got user-level KeeperException when processing sessionid:0x42daa1daab0ecbe type:create cxid:0x1 zxid:0xfffffffffffffffe txntype:unknown reqpath:n/a Error Path:/hbase Error:KeeperErrorCode = NodeExists for /hbase""",dvryaboy,stack,Major,Closed,Fixed,28/Mar/11 23:20,20/Nov/15 12:42
Bug,HBASE-3709,12502641,HFile compression not sharing configuration,"In o.a.h.h.io.hfile.Compression, we defeat codec pooling. We also cause the XML resources of the configuration to be read and parsed upon every reinit().",apurtell,apurtell,Major,Closed,Fixed,29/Mar/11 00:09,20/Nov/15 12:41
Bug,HBASE-3712,12502767,HTable.close() doesn't shutdown thread pool,HTable.close() doesn't shutdown thread pool,yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Closed,Fixed,29/Mar/11 22:59,20/Nov/15 12:42
Bug,HBASE-3714,12502882,completebulkload does not use HBase configuration,"The completebulkupload tool should be using the HBaseConfiguration.create() method to get the HBase configuration in 0.90.*. In it's present state, you receive a connection error when running this tool.",kntreadway,kntreadway,Minor,Closed,Fixed,30/Mar/11 14:48,12/Jun/22 18:29
Bug,HBASE-3716,12502909,Intermittent TestRegionRebalancing failure,"See HBase-TRUNK build #1820

This could be due to HBASE-3681
In trunk, default value of ""hbase.regions.slop"" is 20%. It is possible for load balancer to see region distribution which falls within 20% of optimal distribution.
However, assertRegionsAreBalanced() uses 10% slop.

One solution is to align the slop in assertRegionsAreBalanced() with ""hbase.regions.slop"" value.",yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Closed,Fixed,30/Mar/11 18:09,20/Nov/15 12:42
Bug,HBASE-3722,12503083, A lot of data is lost when name node crashed,"I'm not sure exactly what arose it. there is some split failed logs .
the master should shutdown itself when the HDFS is crashed.

 The logs is :
 2011-03-22 13:21:55,056 WARN 
 org.apache.hadoop.hbase.master.LogCleaner: Error while cleaning the 
 logs
 java.net.ConnectException: Call to C4C1/157.5.100.1:9000 failed on connection exception: java.net.ConnectException: Connection refused
         at org.apache.hadoop.ipc.Client.wrapException(Client.java:844)
         at org.apache.hadoop.ipc.Client.call(Client.java:820)
         at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:221)
         at $Proxy5.getListing(Unknown Source)
         at sun.reflect.GeneratedMethodAccessor9.invoke(Unknown Source)
         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
         at java.lang.reflect.Method.invoke(Method.java:597)
         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
         at $Proxy5.getListing(Unknown Source)
         at org.apache.hadoop.hdfs.DFSClient.listPaths(DFSClient.java:614)
         at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:252)
         at org.apache.hadoop.hbase.master.LogCleaner.chore(LogCleaner.java:121)
         at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
         at 
 org.apache.hadoop.hbase.master.LogCleaner.run(LogCleaner.java:154)
 Caused by: java.net.ConnectException: Connection refused
         at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
         at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
         at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
         at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:408)
         at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:332)
         at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:202)
         at org.apache.hadoop.ipc.Client.getConnection(Client.java:943)
         at org.apache.hadoop.ipc.Client.call(Client.java:788)
         ... 13 more
 2011-03-22 13:21:56,056 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 0 time(s).
 2011-03-22 13:21:57,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 1 time(s).
 2011-03-22 13:21:58,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 2 time(s).
 2011-03-22 13:21:59,057 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 3 time(s).
 2011-03-22 13:22:00,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 4 time(s).
 2011-03-22 13:22:01,058 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 5 time(s).
 2011-03-22 13:22:02,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 6 time(s).
 2011-03-22 13:22:03,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 7 time(s).
 2011-03-22 13:22:04,059 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 8 time(s).
 2011-03-22 13:22:05,060 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 9 time(s).
 2011-03-22 13:22:05,060 ERROR 
 org.apache.hadoop.hbase.master.MasterFileSystem: Failed splitting 
 hdfs://C4C1:9000/hbase/.logs/C4C9.site,60020,1300767633398
 java.net.ConnectException: Call to C4C1/157.5.100.1:9000 failed on connection exception: java.net.ConnectException: Connection refused
         at org.apache.hadoop.ipc.Client.wrapException(Client.java:844)
         at org.apache.hadoop.ipc.Client.call(Client.java:820)
         at org.apache.hadoop.ipc.RPC$Invoker.invoke(RPC.java:221)
         at $Proxy5.getFileInfo(Unknown Source)
         at sun.reflect.GeneratedMethodAccessor4.invoke(Unknown Source)
         at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
         at java.lang.reflect.Method.invoke(Method.java:597)
         at org.apache.hadoop.io.retry.RetryInvocationHandler.invokeMethod(RetryInvocationHandler.java:82)
         at org.apache.hadoop.io.retry.RetryInvocationHandler.invoke(RetryInvocationHandler.java:59)
         at $Proxy5.getFileInfo(Unknown Source)
         at org.apache.hadoop.hdfs.DFSClient.getFileInfo(DFSClient.java:623)
         at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:461)
         at org.apache.hadoop.fs.FileSystem.exists(FileSystem.java:690)
         at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:177)
         at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:196)
         at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:95)
         at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:151)
         at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
         at java.lang.Thread.run(Thread.java:662)
 Caused by: java.net.ConnectException: Connection refused
         at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
         at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
         at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
         at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:408)
         at org.apache.hadoop.ipc.Client$Connection.setupIOstreams(Client.java:332)
         at org.apache.hadoop.ipc.Client$Connection.access$2000(Client.java:202)
         at org.apache.hadoop.ipc.Client.getConnection(Client.java:943)
         at org.apache.hadoop.ipc.Client.call(Client.java:788)
         ... 18 more
 2011-03-22 13:22:45,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 0 time(s).
 2011-03-22 13:22:46,600 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 1 time(s).
 2011-03-22 13:22:47,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 2 time(s).
 2011-03-22 13:22:48,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 3 time(s).
 2011-03-22 13:22:49,601 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 4 time(s).
 2011-03-22 13:22:50,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 5 time(s).
 2011-03-22 13:22:51,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 6 time(s).
 2011-03-22 13:22:52,602 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 7 time(s).
 2011-03-22 13:22:53,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 8 time(s).
 2011-03-22 13:22:54,603 INFO org.apache.hadoop.ipc.Client: Retrying connect to server: C4C1/157.5.100.1:9000. Already tried 9 time(s).
 2011-03-22 13:22:54,603 WARN 
 org.apache.hadoop.hbase.master.LogCleaner: Error while cleaning the 
 logs
 java.net.ConnectException: Call to C4C1/157.5.100.1:9000 failed on connection exception: java.net.ConnectException: Connection refused
         at org.apache.hadoop.ipc.Client.wrapException(Client.java:844)
         at org.apache.hadoop.ipc.Client.call(Client.java:820)
         at org.apache.hadoop.ipc.RPC$Invok

",sunnygao,sunnygao,Major,Closed,Fixed,01/Apr/11 01:07,20/Nov/15 12:40
Bug,HBASE-3723,12503098,Major compact should be done when there is only one storefile and some keyvalue is outdated.,"In the function store.isMajorCompaction:
      if (filesToCompact.size() == 1) {
        // Single file
        StoreFile sf = filesToCompact.get(0);
        long oldest =
            (sf.getReader().timeRangeTracker == null) ?
                Long.MIN_VALUE :
                now - sf.getReader().timeRangeTracker.minimumTimestamp;
        if (sf.isMajorCompaction() &&
            (this.ttl == HConstants.FOREVER || oldest < this.ttl)) {
          if (LOG.isDebugEnabled()) {
            LOG.debug(""Skipping major compaction of "" + this.storeNameStr +
                "" because one (major) compacted file only and oldestTime "" +
                oldest + ""ms is < ttl="" + this.ttl);
          }
        }
      } else {
When there is only one storefile in the store, and some keyvalues' TTL are overtime, the majorcompactchecker should send this region to the compactquene and run a majorcompact to clean these outdated data. But according to the code in 0.90.1, it will do nothing. ",zhoushuaifeng,zhoushuaifeng,Major,Closed,Fixed,01/Apr/11 08:03,20/Nov/15 12:40
Bug,HBASE-3728,12503257,NPE in HTablePool.closeTablePool,When I use HTablePool and try to close it on application shutdown I've got NPE calling closeTablePool method because I didn't borrow any tables with the given name. Could you please add a null check for queue in closeTablePool or add ability to get all table names used in a pool or just add a destroy method to close all existed table in a pool.,yuzhihong@gmail.com,borislav.andruschuk,Minor,Closed,Fixed,04/Apr/11 06:40,20/Nov/15 12:42
Bug,HBASE-3733,12503364,MemStoreFlusher.flushOneForGlobalPressure() shouldn't be using TreeSet for HRegion,"v-himanshu found that since HRegion doesn't implement Comparable, it cannot be placed in TreeSet.",yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Closed,Fixed,05/Apr/11 04:42,20/Nov/15 12:43
Bug,HBASE-3734,12503365,HBaseAdmin creates new configurations in getCatalogTracker,"HBaseAdmin.getCatalogTracker creates new Configuration every time it's called, instead HBA should reuse the same one and do the copy inside the constructor.",jdcryans,jdcryans,Major,Closed,Fixed,05/Apr/11 05:01,20/Nov/15 12:41
Bug,HBASE-3739,12503471,HMaster.getProtocolVersion() ignores HMasterRegionInterface.VERSION,"The recent split of RPC protocol version numbers doesn't work quite as expected with HMasterRegionInterface.  Since HMaster implements both HMasterInterface and HMasterRegionInterface, its getProtocolVersion() implementation needs to check the requested protocol name and return either the HMasterInterface.VERSION value or HMasterRegionInterface.VERSION value as appropriate.",ghelmling,ghelmling,Minor,Closed,Fixed,05/Apr/11 22:50,20/Nov/15 12:41
Bug,HBASE-3740,12503473,hbck doesn't reset the number of errors when retrying,"Using hbck to fix a problem, I see that when it retries it doesn't reset the number of inconsistencies so the number doubles.",jdcryans,jdcryans,Major,Closed,Fixed,05/Apr/11 23:35,20/Nov/15 12:40
Bug,HBASE-3741,12503476,Make HRegionServer aware of the regions it's opening/closing,"This is a serious issue about a race between regions being opened and closed in region servers. We had this situation where the master tried to unassign a region for balancing, failed, force unassigned it, force assigned it somewhere else, failed to open it on another region server (took too long), and then reassigned it back to the original region server. A few seconds later, the region server processed the first closed and the region was left unassigned.

This is from the master log:

{quote}
11-04-05 15:11:17,758 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to serverName=sv4borg42,60020,1300920459477, load=(requests=187, regions=574, usedHeap=3918, maxHeap=6973) for region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:12:10,021 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 state=PENDING_CLOSE, ts=1302041477758
2011-04-05 15:12:10,021 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_CLOSE for too long, running forced unassign again on region=stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
...
2011-04-05 15:14:45,783 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 state=CLOSED, ts=1302041685733
2011-04-05 15:14:45,783 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x42ec2cece810b68 Creating (or updating) unassigned node for 1470298961 with OFFLINE state
...
2011-04-05 15:14:45,885 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961; plan=hri=stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961, src=sv4borg42,60020,1300920459477, dest=sv4borg40,60020,1302041218196
2011-04-05 15:14:45,885 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 to sv4borg40,60020,1302041218196
2011-04-05 15:15:39,410 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 state=PENDING_OPEN, ts=1302041700944
2011-04-05 15:15:39,410 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been PENDING_OPEN for too long, reassigning region=stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:39,410 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 state=PENDING_OPEN, ts=1302041700944
...
2011-04-05 15:15:39,410 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 so generated a random one; hri=stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961, src=, dest=sv4borg42,60020,1300920459477; 19 (online=19, exclude=null) available servers
2011-04-05 15:15:39,410 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 to sv4borg42,60020,1300920459477
2011-04-05 15:15:40,951 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: master:60000-0x42ec2cece810b68 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/prodjobs/unassigned/1470298961
2011-04-05 15:15:40,952 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x42ec2cece810b68 Retrieved 93 byte(s) of data from znode /prodjobs/unassigned/1470298961 and set watcher; region=stumbles_by_userid2,'����穗���6,1266566087256, server=sv4borg42,60020,1300920459477, state=RS_ZK_REGION_OPENED
2011-04-05 15:15:40,952 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=sv4borg42,60020,1300920459477, region=1470298961
2011-04-05 15:15:42,222 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for 1470298961; deleting unassigned node
...
2011-04-05 15:15:55,812 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x42ec2cece810b68 Retrieved 93 byte(s) of data from znode /prodjobs/unassigned/1470298961 and set watcher; region=stumbles_by_userid2,'����穗���6,1266566087256, server=sv4borg42,60020,1300920459477, state=RS_ZK_REGION_CLOSING
2011-04-05 15:15:55,812 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling new unassigned node: /prodjobs/unassigned/1470298961 (region=stumbles_by_userid2,'����穗���6,1266566087256, server=sv4borg42,60020,1300920459477, state=RS_ZK_REGION_CLOSING)
2011-04-05 15:15:55,812 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_CLOSING, server=sv4borg42,60020,1300920459477, region=1470298961
2011-04-05 15:15:55,812 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSING for region 1470298961 from server sv4borg42,60020,1300920459477 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
{quote}

And from sv4borg42:

{quote}
2011-04-05 15:09:58,755 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:11:17,757 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:12:10,021 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:14:45,675 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Processing close of stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:14:45,700 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961: disabling compactions & flushes
2011-04-05 15:14:45,701 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:14:45,701 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:14:45,758 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Closed region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:39,410 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received request to open region: stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:39,410 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Processing open of stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:39,486 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Opening region: REGION => {NAME => 'stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256', STARTKEY => '\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6', ENDKEY => '\x00'\x9AU\x7F\xFF\xFE\xEBQ\xB0\xC3\xEF\x00Jr\xF2', ENCODED => 1470298961, TABLE => ...
2011-04-05 15:15:39,487 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Instantiated stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:40,399 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961; next sequenceid=37627407247
2011-04-05 15:15:40,488 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Updated row stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 in region .META.,,1 with server=sv4borg42:60020, startcode=1300920459477
2011-04-05 15:15:40,582 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Opened stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:55,776 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Processing close of stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:55,809 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961: disabling compactions & flushes
2011-04-05 15:15:55,809 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Updates disabled for region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:55,809 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:55,842 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Closed region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:55,943 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Processing close of stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961
2011-04-05 15:15:55,943 WARN org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Received CLOSE for region stumbles_by_userid2,\x00'\x8E\xE8\x7F\xFF\xFE\xE7\xA9\x97\xFC\xDF\x01\x10\xCC6,1266566087256.1470298961 but currently not serving

{quote}",jdcryans,jdcryans,Blocker,Closed,Fixed,05/Apr/11 23:47,20/Nov/15 12:41
Bug,HBASE-3744,12503553,createTable blocks until all regions are out of transition,"In HBASE-3305, the behavior of createTable was changed and introduced this bug: createTable now blocks until all regions have been assigned, since it uses BulkStartupAssigner. BulkStartupAssigner.waitUntilDone calls assignmentManager.waitUntilNoRegionsInTransition, which waits across all regions, not just the regions of the table that has just been created.

We saw an issue where one table had a region which was unable to be opened, so it was stuck in RegionsInTransition permanently (every open was failing). Since this was the case, waitUntilDone would always block indefinitely even though the newly created table had been assigned.",yuzhihong@gmail.com,tlipcon,Critical,Closed,Fixed,06/Apr/11 20:45,20/Nov/15 12:43
Bug,HBASE-3749,12503579,Master can't exit when open port failed,"When Hmaster crashed  and restart , The Hmaster is hung up.

    // start up all service threads.
    startServiceThreads();                                  ----this open port failed!

    // Wait for region servers to report in.  Returns count of regions.
    int regionCount = this.serverManager.waitForRegionServers();

    // TODO: Should do this in background rather than block master startup
    this.fileSystemManager.
      splitLogAfterStartup(this.serverManager.getOnlineServers());

    // Make sure root and meta assigned before proceeding.
assignRootAndMeta();                                       --- hung up this function, because of root can't be assigned.

  if (!catalogTracker.verifyRootRegionLocation(timeout)) {
      this.assignmentManager.assignRoot();
      this.catalogTracker.waitForRoot();                   --- This statement code is hung up. 
      assigned++;
}

Log is as：

2011-04-07 16:38:22,850 INFO org.mortbay.log: Logging to org.slf4j.impl.Log4jLoggerAdapter(org.mortbay.log) via org.mortbay.log.Slf4jLog
2011-04-07 16:38:22,908 INFO org.apache.hadoop.http.HttpServer: Port returned by webServer.getConnectors()[0].getLocalPort() before open() is -1. Opening the listener on 60010
2011-04-07 16:38:22,909 FATAL org.apache.hadoop.hbase.master.HMaster: Failed startup
java.net.BindException: Address already in use
         at sun.nio.ch.Net.bind(Native Method)
         at sun.nio.ch.ServerSocketChannelImpl.bind(ServerSocketChannelImpl.java:119)
         at sun.nio.ch.ServerSocketAdaptor.bind(ServerSocketAdaptor.java:59)
         at org.mortbay.jetty.nio.SelectChannelConnector.open(SelectChannelConnector.java:216)
         at org.apache.hadoop.http.HttpServer.start(HttpServer.java:445)
         at org.apache.hadoop.hbase.master.HMaster.startServiceThreads(HMaster.java:542)
         at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:373)
         at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:278)
2011-04-07 16:38:22,910 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2011-04-07 16:38:22,911 INFO org.apache.hadoop.hbase.master.ServerManager: Exiting wait on regionserver(s) to checkin; count=0, stopped=true, count of regions out on cluster=0
2011-04-07 16:38:22,914 DEBUG org.apache.hadoop.hbase.master.MasterFileSystem: No log files to split, proceeding...
2011-04-07 16:38:22,930 INFO org.apache.hadoop.ipc.HbaseRPC: Server at 167-6-1-12/167.6.1.12:60020 could not be reached after 1 tries, giving up.
2011-04-07 16:38:22,930 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Unsetting ROOT region location in ZooKeeper
2011-04-07 16:38:22,941 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x22f2c49d2590021 Creating (or updating) unassigned node for 70236052 with OFFLINE state
2011-04-07 16:38:22,956 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Server stopped; skipping assign of -ROOT-,,0.70236052 state=OFFLINE, ts=1302165502941
2011-04-07 16:38:32,746 INFO org.apache.hadoop.hbase.master.AssignmentManager$TimeoutMonitor: 167-6-1-11:60000.timeoutMonitor exiting
2011-04-07 16:39:22,770 INFO org.apache.hadoop.hbase.master.LogCleaner: master-167-6-1-11:60000.oldLogCleaner exiting                  
",sunnygao,sunnygao,Major,Closed,Fixed,07/Apr/11 03:17,20/Nov/15 12:43
Bug,HBASE-3750,12503581,HTablePool.putTable() should call tableFactory.releaseHTableInterface() for discarded table,"Currently HTablePool.putTable() doesn't call table.flushCommits()

When HTable instance is discarded in putTable(), we should call tableFactory.releaseHTableInterface().",yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Closed,Fixed,07/Apr/11 03:29,20/Nov/15 12:41
Bug,HBASE-3755,12503668,Catch zk's ConnectionLossException and augment error message with more help,"0.90 has a different behavior regarding ZK connections, it tends to create too many of them and it's not obvious to users what they should do to fix. I think I've helped at least 5 different users this week with this error.

By catching ConnectionLossException and augmenting its message, we could say something like ""it's possible that the ZooKeeper server has too many connections from this IP, see doc at blah"" since the ZK server isn't nice enough to let us know what's going on.",jdcryans,jdcryans,Major,Closed,Fixed,07/Apr/11 19:23,20/Nov/15 12:40
Bug,HBASE-3756,12503686,Can't move META or ROOT from shell,"Fails with unknownregionexception:

{code}
ERROR: java.lang.reflect.UndeclaredThrowableException: org.apache.hadoop.hbase.UnknownRegionException: -ROOT-,,0,70236052
        at org.apache.hadoop.hbase.master.HMaster.move(HMaster.java:729)
        at sun.reflect.GeneratedMethodAccessor5.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)
{code}",stack,stack,Critical,Closed,Fixed,07/Apr/11 22:58,20/Nov/15 12:43
Bug,HBASE-3758,12503768,HTable.delete triggers pre/postScannerOpen upcalls of RegionObserver,"Currently RegionObserver pre/postScannerOpen() upcalls are injected at HRegion.instantiateInternalScanner(). If someone uses scanner methods at HRegion, ScannerOpen upcalls can be triggered unexpectedly. HBase-3583 pulled scannerNext and scannerClose from HRegion to HRegionServer. We also need to pull scannerOpen to HRegionObserver to prevent unexpected triggers. ",mingjielai,mingjielai,Major,Closed,Fixed,08/Apr/11 18:51,20/Nov/15 12:42
Bug,HBASE-3762,12503859,HTableFactory.releaseHTableInterface() wraps IOException in RuntimeException,"Currently HTableFactory.releaseHTableInterface() wraps IOException in RuntimeException.
We should let HTableInterfaceFactory.releaseHTableInterface() throw IOException explicitly.",yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Closed,Fixed,10/Apr/11 10:45,20/Nov/15 12:40
Bug,HBASE-3771,12504088,All jsp pages don't clean their HBA,"Noticed by Dave Latham, refreshing the zk web page will eventually make that machine run out of connections with ZK. It's because we don't close the connection created inside HBA.",jdcryans,jdcryans,Blocker,Closed,Fixed,12/Apr/11 18:52,20/Nov/15 12:42
Bug,HBASE-3781,12504267,"hbase shell cannot start ""NoMethodError: undefined method `close' for nil:NilClass""","After following the instruction for installing hbase on windows with cygwin, and trying to run the hbase shell i got the following error
NoMethodError: undefined method `close' for nil:NilClass
  initialize at C:/_servers/db/hbase/V090~1.2//bin/../lib/ruby/irb/hirb.rb:39
       start at C:\_servers\db\hbase\V090~1.2\/bin/hirb.rb:171
      (root) at C:\_servers\db\hbase\V090~1.2\/bin/hirb.rb:183

After a while of investigation it seems that the error comes from the fact that ""/dev/null"" is not functioning as expected. When changing to ""NUL"" (in lib/ruby/irb/hirb.rb line 34), which is window equivalent, the shell worked ok.
The fix 
change:  f = File.open(""/dev/null"", ""w"") to f = File.open(""NUL"", ""w"")
",,mikaels,Major,Closed,Fixed,14/Apr/11 13:38,20/Nov/15 12:42
Bug,HBASE-3783,12504279,hbase-0.90.2.jar exists in hbase root and in 'lib/',The official HBase 0.90.2 release contains a hbase-0.90.2.jar in'<hbase root>' and in '<hbase-root>/lib/'.,stack,mr_luk,Blocker,Closed,Fixed,14/Apr/11 15:39,20/Nov/15 12:43
Bug,HBASE-3787,12504342,Increment is non-idempotent but client retries RPC,"The HTable.increment() operation is non-idempotent. The client retries the increment RPC a few times (as specified by configuration) before throwing an error to the application. This makes it possible that the same increment call be applied twice at the server.

For increment operations, is it better to use HConnectionManager.getRegionServerWithoutRetries()? Another  option would be to enhance the IPC module to make the RPC server correctly identify if the RPC is a retry attempt and handle accordingly.",sershe,dhruba,Blocker,Closed,Fixed,15/Apr/11 07:55,21/Feb/15 23:33
Bug,HBASE-3788,12504410,Two error handlings in AssignmentManager.setOfflineInZooKeeper(),"Lars George discovered this:
{code}
 boolean setOfflineInZooKeeper(final RegionState state) {
   if (!state.isClosed() && !state.isOffline()) {
       new RuntimeException(""Unexpected state trying to OFFLINE; "" + state);
     this.master.abort(""Unexpected state trying to OFFLINE; "" + state,
       new IllegalStateException());
     return false;
   }
{code}
Bernd Fondermann commented on the proper fix.",yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Closed,Fixed,15/Apr/11 21:07,20/Nov/15 12:42
Bug,HBASE-3790,12504429,ExecResult.write() throws NPE if result value is null,"Similar to HBASE-3702 on the response side.  If the wrapped value is null, ExecResult.write() will throw an NPE.",ghelmling,ghelmling,Major,Closed,Fixed,16/Apr/11 00:14,20/Nov/15 12:41
Bug,HBASE-3793,12504455,HBASE-3468 Broke checkAndPut with null value,"The previous code called Bytes.equal() which does a check for ""null"" on the left or right argument. Now the comparator calls Bytes.compareTo() - which has no check for null. But this is a valid input and checks for existence. I actually noticed this running 

https://github.com/larsgeorge/hbase-book/blob/master/ch04/src/main/java/client/CheckAndPutExample.java

This used to work, now it throws an NPE

{noformat}
Caused by: java.lang.NullPointerException
	at org.apache.hadoop.hbase.util.Bytes.compareTo(Bytes.java:854)
	at org.apache.hadoop.hbase.filter.WritableByteArrayComparable.compareTo(WritableByteArrayComparable.java:63)
	at org.apache.hadoop.hbase.regionserver.HRegion.checkAndMutate(HRegion.java:1681)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.checkAndMutate(HRegionServer.java:1693)
	... 6 more


	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.java:1026)
	at org.apache.hadoop.hbase.client.HTable.checkAndPut(HTable.java:750)
	at client.CheckAndPutExample.main(CheckAndPutExample.java:33)
{noformat}

Easy fixable, just needs to handle the null value before even calling comparator.compareTo().",mingma,larsgeorge,Blocker,Closed,Fixed,16/Apr/11 17:42,20/Nov/15 12:41
Bug,HBASE-3794,12504578,TestRpcMetrics fails on machine where region server is running,"Since whole test suite takes over an hour to run, I ran them on Linux where region server is running.

Here is the consistent TestRpcMetrics failure I saw: 
{code}
Tests run: 1, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 0.196 sec <<< FAILURE!
testCustomMetrics(org.apache.hadoop.hbase.regionserver.TestRpcMetrics)  Time elapsed: 0.079 sec  <<< ERROR!
java.net.BindException: Problem binding to /10.202.50.107:60020 : Address already in use
        at org.apache.hadoop.hbase.ipc.HBaseServer.bind(HBaseServer.java:216)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.<init>(HBaseServer.java:283)
        at org.apache.hadoop.hbase.ipc.HBaseServer.<init>(HBaseServer.java:1189)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.<init>(WritableRpcEngine.java:266)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getServer(WritableRpcEngine.java:233)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getServer(WritableRpcEngine.java:46)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getServer(HBaseRPC.java:379)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getServer(HBaseRPC.java:368)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.<init>(HRegionServer.java:336)
        at org.apache.hadoop.hbase.regionserver.TestRpcMetrics$TestRegionServer.<init>(TestRpcMetrics.java:58)
        at org.apache.hadoop.hbase.regionserver.TestRpcMetrics.testCustomMetrics(TestRpcMetrics.java:119)
{code}",posix4e,yuzhihong@gmail.com,Major,Closed,Fixed,18/Apr/11 18:26,20/Nov/15 12:41
Bug,HBASE-3796,12504599,Per-Store Entries in Compaction Queue,"Although compaction is decided on a per-store basis, right now the CompactSplitThread only deals at the Region level for queueing.  Store-level compaction queue entries will give us more visibility into compaction workload + allow us to stop summarizing priorities.",nspiegelberg,nspiegelberg,Minor,Closed,Fixed,18/Apr/11 20:52,20/Nov/15 12:40
Bug,HBASE-3800,12504740,If HMaster is started after NN without starting DN in Hbase 090.2 then HMaster is not able to start due to AlreadyCreatedException for /hbase/hbase.version,"It reproduces when HMaster is started for the first time and NN is started without starting DN

Hmaster logs:
2011-04-19 16:49:09,208 DEBUG org.apache.hadoop.hbase.master.ActiveMasterManager: A master is now available
2011-04-19 16:49:09,400 WARN org.apache.hadoop.hbase.util.FSUtils: Version file was empty, odd, will try to set it.
2011-04-19 16:51:09,674 WARN org.apache.hadoop.hdfs.DFSClient: DataStreamer Exception: org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /hbase/hbase.version could only be replicated to 0 nodes, instead of 1
...........

2011-04-19 16:51:09,674 WARN org.apache.hadoop.hdfs.DFSClient: Error Recovery for block null bad datanode[0] nodes == null
2011-04-19 16:51:09,674 WARN org.apache.hadoop.hdfs.DFSClient: Could not get block locations. Source file ""/hbase/hbase.version"" - Aborting...
2011-04-19 16:51:09,674 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at hdfs://C4C1:9000/hbase, retrying: org.apache.hadoop.ipc.RemoteException: java.io.IOException: File /hbase/hbase.version could only be replicated to 0 nodes, instead of 1
...........

2011-04-19 16:56:19,695 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at hdfs://C4C1:9000/hbase, retrying: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /hbase/hbase.version for DFSClient_hb_m_C4C1.site:60000_1303202948768 on client 157.5.100.1 because current leaseholder is trying to recreate file.
org.apache.hadoop.hdfs.protocol.AlreadyBeingCreatedException: failed to create file /hbase/hbase.version for DFSClient_hb_m_C4C1.site:60000_1303202948768 on client 157.5.100.1 because current leaseholder is trying to recreate file.
       at org.apache.hadoop.hdfs.server.namenode.FSNamesystem.startFileInternal(FSNamesystem.java:1068)
....
",apurtell,sunnygao,Major,Closed,Fixed,20/Apr/11 01:08,20/Nov/15 12:44
Bug,HBASE-3802,12504803,Redundant list creation in HRegion,"{code}
Index: src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java
===================================================================
--- src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java     (revision 1095238)
+++ src/main/java/org/apache/hadoop/hbase/regionserver/HRegion.java     (working copy)
@@ -3092,8 +3092,7 @@
         get.addFamily(family);
       }
     }
-    List<KeyValue> results =  new ArrayList<KeyValue>();
-    results = get(get, true);
+    List<KeyValue> results = get(get, true);
     return new Result(results);
   }
{code}
Reported by Lars George up on dev list.",stack,stack,Major,Closed,Fixed,20/Apr/11 16:32,20/Nov/15 12:43
Bug,HBASE-3806,12504830,distributed log splitting double escapes task names,During startup master double-escapes the (log split) task names when submitting them ... I had missed this in my testing because I was using task names like foo and bar instead of those that need escaping - like hdfs://... Also at startup even though the master fails to acquire the orphan tasks ... the tasks are acquired anyways when master sees the logs that need splitting.,khemani,khemani,Major,Closed,Fixed,20/Apr/11 23:01,20/Nov/15 12:41
Bug,HBASE-3807,12504849,Fix units in RS UI metrics,Currently the metrics are a mix of MB and bytes.  Its confusing.,subramanian,stack,Major,Closed,Fixed,21/Apr/11 06:28,20/Nov/15 12:43
Bug,HBASE-3808,12504850,Implement Executor.toString for master handlers at least,"On shutdown, if still outstanding Executors queued then when ExecutorService lists what is outstanding, the list will be other than a list of default toString implementations of ServerShutdownHandler objects.",brocknoland,stack,Minor,Closed,Fixed,21/Apr/11 06:30,20/Nov/15 12:43
Bug,HBASE-3813,12504988,"Change RPC callQueue size from ""handlerCount * MAX_QUEUE_SIZE_PER_HANDLER;""","Yesterday debugging w/ Jack we noticed that with few handlers on a big box, he was seeing stats like this:

{code}
2011-04-21 11:54:49,451 DEBUG org.apache.hadoop.ipc.HBaseServer: Server connection from X.X.X.X:60931; # active connections: 11; # queued calls: 2500
{code}

We had 2500 items in the rpc queue waiting to be processed.

Turns out he had too few handlers for number of clients (but also, it seems like he figured hw issues in that his RAM bus was running at 1/4 the rate that it should have been running at).

Chatting w/ J-D this morning, he asked if the queues hold 'data'.  The queues hold 'Calls'.  Calls are the client request.  They contain data.

Jack had 2500 items queued.  If each item to insert was 1MB, thats 2.5k * 1MB of memory that is outside of our generally accounting.

Currently the queue size is handlers * MAX_QUEUE_SIZE_PER_HANDLER where MAX_QUEUE_SIZE_PER_HANDLER is hardcoded to be 100.

If the queue is full we block (LinkedBlockingQueue).

Going to change the queue size from 100 to 10 by default -- but also will make it configurable and will doc. this as possible cause of OOME.  Will try it on production here before committing patch.

",stack,stack,Critical,Closed,Fixed,22/Apr/11 16:59,16/Feb/16 16:54
Bug,HBASE-3817,12505059,HBase Shell has an issue accepting FILTER for the 'scan' command.,"Stack had encountered+fixed an issue nearly a couple of years ago related to FILTER not being accepted by the 'scan' command in the shell. This, however, didn't make it to the trunk I believe. I hit it today while revamping some docs for HBASE-3539

The thread where Stack had posted a patch: http://mail-archives.apache.org/mod_mbox/hbase-user/200912.mbox/%3C7c962aed0912181049l2f9110c3q43b1e3d897a2768e@mail.gmail.com%3E",qwertymaniac,qwertymaniac,Trivial,Closed,Fixed,23/Apr/11 18:07,20/Nov/15 12:40
Bug,HBASE-3819,12505144,TestSplitLogWorker has too many SLWs running -- makes for contention and occasional failures,I noticed that TSPLW has a background SLW running.  Sometimes it wins the race for tasks messing up tests.,stack,stack,Major,Closed,Fixed,25/Apr/11 23:00,20/Nov/15 12:41
Bug,HBASE-3820,12505149,Splitlog() executed while the namenode was in safemode may cause data-loss,"I found this problem while the namenode went into safemode due to some unclear reasons. 
There's one patch about this problem:

   try {
      HLogSplitter splitter = HLogSplitter.createLogSplitter(
        conf, rootdir, logDir, oldLogDir, this.fs);
      try {
        splitter.splitLog();
      } catch (OrphanHLogAfterSplitException e) {
        LOG.warn(""Retrying splitting because of:"", e);
        // An HLogSplitter instance can only be used once.  Get new instance.
        splitter = HLogSplitter.createLogSplitter(conf, rootdir, logDir,
          oldLogDir, this.fs);
        splitter.splitLog();
      }
      splitTime = splitter.getTime();
      splitLogSize = splitter.getSize();
    } catch (IOException e) {
      checkFileSystem();
      LOG.error(""Failed splitting "" + logDir.toString(), e);
      master.abort(""Shutting down HBase cluster: Failed splitting hlog files..."", e);
    } finally {
      this.splitLogLock.unlock();
    }

And it was really give some useful help to some extent, while the namenode process exited or been killed, but not considered the Namenode safemode exception.
   I think the root reason is the method of checkFileSystem().
   It gives out an method to check whether the HDFS works normally(Read and write could be success), and that maybe the original propose of this method. This's how this method implements:

    DistributedFileSystem dfs = (DistributedFileSystem) fs;
    try {
      if (dfs.exists(new Path(""/""))) {  
        return;
      }
    } catch (IOException e) {
      exception = RemoteExceptionHandler.checkIOException(e);
    }
   
   I have check the hdfs code, and learned that while the namenode was in safemode ,the dfs.exists(new Path(""/"")) returned true. Because the file system could provide read-only service. So this method just checks the dfs whether could be read. I think it's not reasonable.
    
   ",,jeason,Major,Closed,Fixed,26/Apr/11 01:15,20/Nov/15 12:42
Bug,HBASE-3821,12505159," ""NOT flushing memstore for region"" keep on printing for half an hour","""NOT flushing memstore for region"" keep on printing for half an hour in the regionserver. Then I restart hbase. I think there may be deadlock or cycling.
I know that when splitting region, it will doclose of region, and set writestate.writesEnabled = false  and may run close preflush. This will make flush fail and print ""NOT flushing memstore for region"". But It should be finished after a while.

logs:
2011-04-18 16:28:27,960 DEBUG org.apache.hadoop.hbase.regionserver.CompactSplitThread: Compaction requested for ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610. because regionserver60020.cacheFlusher; priority=-1, compaction queue size=1
2011-04-18 16:28:30,171 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.
2011-04-18 16:28:30,171 WARN org.apache.hadoop.hbase.regionserver.MemStoreFlusher: Region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610. has too many store files; delaying flush up to 90000ms
2011-04-18 16:28:32,119 INFO org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Using syncFs -- HDFS-200
2011-04-18 16:28:32,285 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /hbase/.logs/linux253,60020,1303123943360/linux253%3A60020.1303124206693, entries=5226, filesize=255913736. New hlog /hbase/.logs/linux253,60020,1303123943360/linux253%3A60020.1303124311822
2011-04-18 16:28:32,287 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: Found 1 hlogs to remove out of total 2; oldest outstanding sequenceid is 11037 from region 031f37c9c23fcab17797b06b90205610
2011-04-18 16:28:32,288 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: moving old hlog file /hbase/.logs/linux253,60020,1303123943360/linux253%3A60020.1303123945481 whose highest sequenceid is 6052 to /hbase/.oldlogs/linux253%3A60020.1303123945481
2011-04-18 16:28:42,701 INFO org.apache.hadoop.hbase.regionserver.Store: Completed major compaction of 4 file(s), new file=hdfs://10.18.52.108:9000/hbase/ufdr/031f37c9c23fcab17797b06b90205610/value/4398465741579485290, size=281.4m; total size for store is 468.8m
2011-04-18 16:28:42,712 INFO org.apache.hadoop.hbase.regionserver.HRegion: completed compaction on region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610. after 1mins, 40sec
2011-04-18 16:28:42,741 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Starting split of region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.
2011-04-18 16:28:42,770 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.: disabling compactions & flushes
2011-04-18 16:28:42,770 INFO org.apache.hadoop.hbase.regionserver.HRegion: Running close preflush of ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.
2011-04-18 16:28:42,771 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610., current region memstore size 105.6m
2011-04-18 16:28:42,818 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Finished snapshotting, commencing flushing stores
2011-04-18 16:28:42,846 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610., flushing=false, writesEnabled=false
2011-04-18 16:28:42,849 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.
2011-04-18 16:28:42,849 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610., flushing=false, writesEnabled=false ......
2011-04-18 17:04:08,803 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610., flushing=false, writesEnabled=false
2011-04-18 17:04:08,803 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on ufdr,,1303124043153.031f37c9c23fcab17797b06b90205610.
Mon Apr 18 17:04:24 IST 2011 Starting regionserver on linux253 ulimit -n 1024
",zhoushuaifeng,zhoushuaifeng,Major,Closed,Fixed,26/Apr/11 07:10,20/Nov/15 12:41
Bug,HBASE-3827,12505716,"hbase-1502, removing heartbeats, broke master joining a running cluster and was returning master hostname for rs to use",A couple of silly issues in hbase-1502 turned up by cluster testing TRUNK.,stack,stack,Major,Closed,Fixed,28/Apr/11 19:49,20/Nov/15 12:41
Bug,HBASE-3829,12505739,TestMasterFailover failures in jenkins,"We'll fail the TestMasterFailover tests on occasion up on jenkins.  One reason for the 180000 timeouts it that test completed but a regionserver won't go down because its stuck over in getMaster.  Looking into it, we have all these loops in the regionserver; we have the main run loop but then there are loops trying to send regionserver reportForDuty and then over in the regionserver report method.  In a recent fail up on jenkins we were stuck in one of these outer loops trying to get master.

This patch removes a bunch of the outer loops instead having the outer loops run around the HRegionServer#run loop.",stack,stack,Major,Closed,Fixed,29/Apr/11 00:08,20/Nov/15 12:41
Bug,HBASE-3832,12505828,Failing TestMasterFailover.testMasterFailoverWithMockedRITOnDeadRS up on jenkins,"Root region is stuck in RIT.

Seems to be because of this:

3316 2011-04-29 05:53:11,941 WARN  [Thread-642-EventThread] master.AssignmentManager(518): Received OPENED for region 70236052/-ROOT- from server vesta.apache.org,57336,1304056370834 but region was in the state null and not in expected PENDING_OPEN or OPENING states

Later I see this:

3334 2011-04-29 05:53:12,014 DEBUG [Master:0;vesta.apache.org,36450,1304056384388] master.AssignmentManager(260): Found REGION => {NAME => '-ROOT-,,0', STARTKEY => '', ENDKEY => '', ENCODED => 70236052,       TABLE => {{NAME => '-ROOT-', IS_ROOT => 'true', IS_META => 'true', FAMILIES => [{NAME => 'info', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', COMPRESSION => 'NONE', VERSIONS => '10', TTL => '2147483647', BLOCKSIZE => '8192', IN_MEMORY => 'true', BLOCKCACHE => 'true'}]}}=vesta.apache.org,57336,1304056370834 in RITs

The former makes it so we don't clear a successfully opened -ROOT- from RIT so we get the second line and then the test fails with:

5192 2011-04-29 05:55:42,181 DEBUG [Thread-642] zookeeper.ZKAssign(815): ZK RIT -> 70236052

printed over and over again.

I don't get why the data is null in the zk when RS has updated it a couple of times.  I see we do a double regionOnline in master code.  This clears in-memory master state.  I don't think this it but will commit this and more logging to help w/ the debug.",,stack,Major,Closed,Fixed,29/Apr/11 23:22,12/Jun/22 18:19
Bug,HBASE-3834,12505837,Store ignores checksum errors when opening files,"If you corrupt one of the storefiles in a region (eg using vim to muck up some bytes), the region will still open, but that storefile will just be ignored with a log message. We should probably not do this in general - better to keep that region unassigned and force an admin to make a decision to remove the bad storefile.",xieliang007,tlipcon,Critical,Closed,Fixed,30/Apr/11 06:56,12/Jun/22 18:18
Bug,HBASE-3838,12505860,RegionCoprocesorHost.preWALRestore throws npe in case there is no RegionObserver registered.,"It seems the check to bypass the Observers chain is at wrong place in case of pre/post WALRestore. It should be inside the ""if statement"" that checks whether the CP is instance of RegionObserver or not.
",v.himanshu,v.himanshu,Minor,Closed,Fixed,01/May/11 02:02,20/Nov/15 12:42
Bug,HBASE-3843,12505895,splitLogWorker starts too early,splitlogworker should be started in startServiceThreads() instead of in initializeZookeeper(). This will ensure that the region server accepts a split-logging tasks only after it has successfully done reportForDuty() to the master.,khemani,khemani,Major,Closed,Fixed,02/May/11 02:12,20/Nov/15 12:42
Bug,HBASE-3845,12505973,data loss because lastSeqWritten can miss memstore edits,"(I don't have a test case to prove this yet but I have run it by Dhruba and Kannan internally and wanted to put this up for some feedback.)

In this discussion let us assume that the region has only one column family. That way I can use region/memstore interchangeably.

After a memstore flush it is possible for lastSeqWritten to have a log-sequence-id for a region that is not the earliest log-sequence-id for that region's memstore.

HLog.append() does a putIfAbsent into lastSequenceWritten. This is to ensure that we only keep track  of the earliest log-sequence-number that is present in the memstore.

Every time the memstore is flushed we remove the region's entry in lastSequenceWritten and wait for the next append to populate this entry again. This is where the problem happens.

step 1:
flusher.prepare() snapshots the memstore under HRegion.updatesLock.writeLock().

step 2 :
as soon as the updatesLock.writeLock() is released new entries will be added into the memstore.

step 3 :
wal.completeCacheFlush() is called. This method removes the region's entry from lastSeqWritten.

step 4:
the next append will create a new entry for the region in lastSeqWritten(). But this will be the log seq id of the current append. All the edits that were added in step 2 are missing.

==

as a temporary measure, instead of removing the region's entry in step 3 I will replace it with the log-seq-id of the region-flush-event.

",ram_krish,khemani,Critical,Closed,Fixed,02/May/11 19:59,20/Nov/15 12:43
Bug,HBASE-3847,12505990,Turn off DEBUG logging of RPCs in WriteableRPCEngine on TRUNK,,stack,stack,Major,Closed,Fixed,02/May/11 23:35,20/Nov/15 12:42
Bug,HBASE-3848,12506001,request is always zero in WebUI for region server,"request is always zero in WebUI for region server
>
> Metrics request=0.0, regions=36, stores=36, storefiles=148, 
> storefileIndexSize=29, memstoreSize=253, compactionQueueSize=24, 
> flushQueueSize=0, usedHeap=655, maxHeap=8175, blockCacheSize=14230920, 
> blockCacheFree=1700269560, blockCacheCount=21, 
> blockCacheHitCount=2887, blockCacheMissCount=204829, 
> blockCacheEvictedCount=0, blockCacheHitRatio=1, 
> blockCacheHitCachingRatio=99
>
> requests is not zero in WebUI for Hmaster requests=15000, regions=35, 
> usedHeap=513, maxHeap=8175
>
> Is there any different for these metrics?
> How do I use it?
> Thanks.
>
>
",sunnygao,sunnygao,Minor,Closed,Fixed,03/May/11 00:57,20/Nov/15 12:42
Bug,HBASE-3849,12506025,Fix master ui; hbase-1502 broke requests/second,,stack,stack,Major,Closed,Fixed,03/May/11 05:45,20/Nov/15 12:43
Bug,HBASE-3853,12506140,TestInfoServers failing after HBASE-3835,"Forgot to update this test - the test was checking the redirections from index.html to the JSP pages, but since the JSP pages no longer exist, the test case needs to be fixed.",tlipcon,tlipcon,Major,Closed,Fixed,04/May/11 07:01,20/Nov/15 12:41
Bug,HBASE-3862,12506350,Race conditions in aggregate calculation,"The AggregationClient requests aggregations from multiple region servers in parallel. The calculations in the reducer callbacks of the AggregationClient are not thread safe, and therefore could return an incorrect result due to simultaneous/interleaved execution.",,heitmann,Major,Closed,Fixed,06/May/11 00:07,20/Nov/15 12:43
Bug,HBASE-3863,12506369,Fix failing TestHBaseFsck.testHBaseFsckMeta unit test,"Failing with:

{code}
Error Message

org.apache.hadoop.hbase.HServerAddress cannot be cast to org.apache.hadoop.hbase.ServerName
Stacktrace

java.lang.ClassCastException: org.apache.hadoop.hbase.HServerAddress cannot be cast to org.apache.hadoop.hbase.ServerName
	at org.apache.hadoop.hbase.util.HBaseFsckRepair.fixDupeAssignment(HBaseFsckRepair.java:59)
	at org.apache.hadoop.hbase.util.TestHBaseFsck.testHBaseFsckMeta(TestHBaseFsck.java:163)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
....
{code}",stack,stack,Major,Closed,Fixed,06/May/11 06:24,20/Nov/15 12:42
Bug,HBASE-3864,12506437,Rename of hfile.min.blocksize.size in HBASE-2899 reverted in HBASE-1861,"HBASE-2899 renamed {{hfile.min.blocksize.size}} to {{hbase.mapreduce.hfileoutputformat.blocksize}}. However, HBASE-1861 (committed after HBASE-2899) reverted this change.",atm,atm,Major,Closed,Fixed,06/May/11 17:52,20/Nov/15 12:42
Bug,HBASE-3865,12506461,Failing TestWALReplay,Failed last few times up on jenkins.  The change to the signature of the internalFlushCache to add passing of a MonitorVisitor meant the override here was no longer called.,stack,stack,Major,Closed,Fixed,06/May/11 21:01,20/Nov/15 12:40
Bug,HBASE-3867,12506555,"when cluster is stopped and server which hosted meta region is removed from cluster, master breaks down after restarting cluster.","When cluster stopped and romove server from cluster which contains meta region, then restart cluster,
From the following code throws ""NoRouteToHostException""

package org.apache.hadoop.hbase.catalog;
public class CatalogTracker 

 private HRegionInterface getMetaServerConnection(boolean refresh)
  throws IOException, InterruptedException {
    synchronized (metaAvailable) {
      if (metaAvailable.get()) {
        HRegionInterface current = getCachedConnection(metaLocation);
        if (!refresh) {
          return current;
        }
        if (verifyRegionLocation(current, this.metaLocation, META_REGION)) {
          return current;
        }
        resetMetaLocation();
      }
      HRegionInterface rootConnection = getRootServerConnection();
      if (rootConnection == null) {
        return null;
      }
      HServerAddress newLocation = MetaReader.readMetaLocation(rootConnection);
      if (newLocation == null) {
        return null;
      }
      ////////the following line throws the exception
HRegionInterface newConnection = getCachedConnection(newLocation);
      if (verifyRegionLocation(newConnection, this.metaLocation, META_REGION)) {
        setMetaLocation(newLocation);
        return newConnection;
      }
      return null;
    }
  }

/////////////the following method don't handle the exception.
public class CatalogTracker 
  public boolean verifyMetaRegionLocation(final long timeout)
  throws InterruptedException, IOException {
    return getMetaServerConnection(true) != null;
  }


//////////////////master call the CatalogTracker's method and don't handle the problem too.
package org.apache.hadoop.hbase.master;
public class HMaster
int assignRootAndMeta()
  throws InterruptedException, IOException, KeeperException {
    int assigned = 0;
    long timeout = this.conf.getLong(""hbase.catalog.verification.timeout"", 1000);

    // Work on ROOT region.  Is it in zk in transition?
    boolean rit = this.assignmentManager.
      processRegionInTransitionAndBlockUntilAssigned(HRegionInfo.ROOT_REGIONINFO);
    if (!catalogTracker.verifyRootRegionLocation(timeout)) {
      this.assignmentManager.assignRoot();
      this.catalogTracker.waitForRoot();
      assigned++;
    }
    LOG.info(""-ROOT- assigned="" + assigned + "", rit="" + rit +
      "", location="" + catalogTracker.getRootLocation());

    // Work on meta region
    rit = this.assignmentManager.
      processRegionInTransitionAndBlockUntilAssigned(HRegionInfo.FIRST_META_REGIONINFO);
///////////////////////////////
when restart cluster master break down here.
////////////////////////////////
    if (!this.catalogTracker.verifyMetaRegionLocation(timeout)) {
      this.assignmentManager.assignMeta();
      this.catalogTracker.waitForMeta();
      // Above check waits for general meta availability but this does not
      // guarantee that the transition has completed
      this.assignmentManager.waitForAssignment(HRegionInfo.FIRST_META_REGIONINFO);
      assigned++;
    }
    LOG.info("".META. assigned="" + assigned + "", rit="" + rit +
      "", location="" + catalogTracker.getMetaLocation());
    return assigned;
  }

Thanks to JunQiang Yuan in www.alipay.com  for providing information about this bug. ",,liujia_ict,Critical,Closed,Fixed,09/May/11 02:24,20/Nov/15 12:43
Bug,HBASE-3870,12506652,Purge copyrights from src headers,"Henry  Yandell wrote me:

{quote}
A quick note that I noticed that there were copyright statements in the HBase code.

For example:

http://svn.apache.org/repos/asf/hbase/trunk/src/main/java/org/apache/hadoop/hbase/ClusterStatus.java

My understanding from http://www.apache.org/legal/src-headers.html is
that they shouldn't be there.
{quote}",stack,stack,Blocker,Closed,Fixed,09/May/11 19:49,23/Sep/13 18:30
Bug,HBASE-3872,12506681,Hole in split transaction rollback; edits to .META. need to be rolled back even if it seems like they didn't make it,"Saw this interesting one on a cluster of ours.  The cluster was configured with too few handlers so lots of the phenomeneon where actions were queued but then by the time they got into the server and tried respond to the client, the client had disconnected because of the timeout of 60 seconds.  Well, the meta edits for a split were queued at the regionserver carrying .META. and by the time it went to write back, the client had gone (the first insert of parent offline with daughter regions added as info:splitA and info:splitB).  The client presumed the edits failed and 'successfully' rolled back the transaction (failing to undo .META. edits thinking they didn't go through).

A few minutes later the .META. scanner on master runs.  It sees 'no references' in daughters -- the daughters had been cleaned up as part of the split transaction rollback -- so it thinks its safe to delete the parent.

Two things:

+ Tighten up check in master... need to check daughter region at least exists and possibly the daughter region has an entry in .META.
+ Dependent on the edit that fails, schedule rollback edits though it will seem like they didn't go through.

This is pretty critical one.",stack,stack,Blocker,Closed,Fixed,10/May/11 05:12,20/Nov/15 12:44
Bug,HBASE-3874,12506787,ServerShutdownHandler fails on NPE if a plan has a random region assignment,"By chance, we were able to revert the ulimit on one of our clusters to 1024 and it started dying non-stop on ""Too many open files"". Now the bad thing is that some region servers weren't completely ServerShutdownHandler'd because they failed on:

{quote}

2011-05-07 00:04:46,203 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.AssignmentManager.processServerShutdown(AssignmentManager.java:1804)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:101)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{quote}

Reading the code, it seems the NPE is in the if statement:

{code}
Map.Entry<String, RegionPlan> e = i.next();
if (e.getValue().getDestination().equals(hsi)) {
  // Use iterator's remove else we'll get CME
  i.remove();
}
{code}

Which means that the destination (HSI) is null. Looking through the code, it seems we instantiate a RegionPlan with a null HSI when it's a random assignment. 

It means that if there's a random assignment going on while a node dies then this issue might happen.

Initially I thought that this could mean data loss, but the logs are already split so it's just the reassignment that doesn't happen (still bad).

Also it left the master with dead server being processed, so for two days the balancer didn't run failing on:

bq. org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): []

And the reason why the array is empty is because we are running 0.90.3 which removes the RS from the dead list if it comes back.",jdcryans,jdcryans,Blocker,Closed,Fixed,11/May/11 00:12,20/Nov/15 12:43
Bug,HBASE-3876,12506900,TestCoprocessorInterface.testCoprocessorInterface broke on jenkins and local,Recent commit seems to have broken this.,stack,stack,Blocker,Closed,Fixed,11/May/11 19:53,20/Nov/15 12:42
Bug,HBASE-3878,12506966,Hbase client throws NoSuchElementException,"Soft reference objects, which are cleared at the discretion of the 
garbage collector in response to memory demand. 

I used ycsb to put data and threw exception.

>>>> 
>>>>  Hbase Code:
>>>>     // Cut the cache so that we only get the part that could contain
>>>>     // regions that match our key
>>>>     SoftValueSortedMap<byte[], HRegionLocation> matchingRegions =
>>>>       tableLocations.headMap(row);
>>>> 
>>>>     // if that portion of the map is empty, then we're done. otherwise,
>>>>     // we need to examine the cached location to verify that it is
>>>>     // a match by end key as well.
>>>>     if (!matchingRegions.isEmpty()) {
>>>>       HRegionLocation possibleRegion =
>>>>         matchingRegions.get(matchingRegions.lastKey());
>>>> 
>>>>   ycsb client log:
>>>> 
>>>>   [java] begin StatusThread run
>>>>    [java] java.util.NoSuchElementException
>>>>    [java]     at java.util.TreeMap.key(TreeMap.java:1206)
>>>>    [java]     at
>> java.util.TreeMap$NavigableSubMap.lastKey(TreeMap.java:1435)
>>>>    [java]     at
>> org.apache.hadoop.hbase.util.SoftValueSortedMap.lastKey(SoftValueSort
>> edMap.java:131)
>>>>    [java]     at
>> org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplemen
>> tation.getCachedLocation(HConnectionManager.java:841)
>>>>    [java]     at
>> org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplemen
>> tation.locateRegionInMeta(HConnectionManager.java:664)
>>>>    [java]     at
>> org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplemen
>> tation.locateRegion(HConnectionManager.java:590)
>>>>    [java]     at
>> org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplemen
>> tation.processBatch(HConnectionManager.java:1114)
>>>>    [java]     at
>> org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplemen
>> tation.processBatchOfPuts(HConnectionManager.java:1234)
>>>>    [java]     at
>> org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:819)
>>>>    [java]     at
>> org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:675)
>>>>    [java]     at
>> org.apache.hadoop.hbase.client.HTable.put(HTable.java:665)
>>>>    [java]     at com.yahoo.ycsb.db.HBaseClient.update(Unknown Source)
>>>>    [java]     at com.yahoo.ycsb.db.HBaseClient.insert(Unknown Source)
>>>>    [java]     at com.yahoo.ycsb.DBWrapper.insert(Unknown Source)
>>>>    [java]     at com.yahoo.ycsb.workloads.MyWorkload.doInsert(Unknown
>> Source)
>>>>    [java]     at com.yahoo.ycsb.ClientThread.run(Unknown Source)
",yuzhihong@gmail.com,sunnygao,Major,Closed,Fixed,12/May/11 11:09,20/Nov/15 12:42
Bug,HBASE-3881,12507162,Add disable balancer in graceful_stop.sh script,"If balancer is on when graceful_stop.sh runs, it can get messy.  Add disable of balancer to the script.",stack,stack,Major,Closed,Fixed,13/May/11 18:37,20/Nov/15 12:40
Bug,HBASE-3882,12507194,hbase-config.sh needs to be updated so it can auto-detects the sun jre provided by RHEL6,"RHEL6 will install its sun jdk in /usr/lib/jvm/java-1.6.0-sun-1.6.0.<update_version>.<arch>.
So this ticket is about adding this path to the jdk autodetection mechanism used in hbase-config.sh",rvs,rvs,Major,Closed,Fixed,14/May/11 01:31,20/Nov/15 12:41
Bug,HBASE-3889,12507352,NPE in Distributed Log Splitting,"There is an issue with the log splitting under the specific condition of edits belonging to a non existing region (which went away after a split for example). The HLogSplitter fails to check the condition, which is handled on a lower level, logging manifests it as 

{noformat}
2011-05-16 13:56:10,300 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: This region's directory doesn't exist: hdfs://localhost:8020/hbase/usertable/30c4d0a47703214845d0676d0c7b36f0. It is very likely that it was already split so it's safe to discard those edits.
{noformat}

The code returns a null reference which is not check in HLogSplitter.splitLogFileToTemp():

{code}
...
        WriterAndPath wap = (WriterAndPath)o;
        if (wap == null) {
          wap = createWAP(region, entry, rootDir, tmpname, fs, conf);
          if (wap == null) {
            logWriters.put(region, BAD_WRITER);
          } else {
            logWriters.put(region, wap);
          }
        }
        wap.w.append(entry);
...
{code}

The createWAP does return ""null"" when the above message is logged based on the obsolete region reference in the edit.

What made this difficult to detect is that the error (and others) are silently ignored in SplitLogWorker.grabTask(). I added a catch and error logging to see the NPE that was caused by the above.

{code}
...
          break;
      }
    } catch (Exception e) {
      LOG.error(""An error occurred."", e);
    } finally {
      if (t > 0) {
...
{code}

As a side note, there are other errors/asserts triggered that this try/finally not handles. For example

{noformat}
2011-05-16 13:58:30,647 WARN org.apache.hadoop.hbase.regionserver.SplitLogWorker: BADVERSION failed to assert ownership for /hbase/splitlog/hdfs%3A%2F%2Flocalhost%2Fhbase%2F.logs%2F10.0.0.65%2C60020%2C1305406356765%2F10.0.0.65%252C60020%252C1305406356765.1305409968389
org.apache.zookeeper.KeeperException$BadVersionException: KeeperErrorCode = BadVersion for /hbase/splitlog/hdfs%3A%2F%2Flocalhost%2Fhbase%2F.logs%2F10.0.0.65%2C60020%2C1305406356765%2F10.0.0.65%252C60020%252C1305406356765.1305409968389
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:106)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1038)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.ownTask(SplitLogWorker.java:329)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.access$100(SplitLogWorker.java:68)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker$2.progress(SplitLogWorker.java:265)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFileToTemp(HLogSplitter.java:432)
        at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLogFileToTemp(HLogSplitter.java:354)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker$1.exec(SplitLogWorker.java:113)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:260)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:191)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:164)
        at java.lang.Thread.run(Thread.java:680)
{noformat}

This should probably be handled - or at least documented - in another issue?

The NPE made the log split end and the SplitLogManager add an endless amount of RESCAN entries as this never came to an end.",anirudhtodi,larsgeorge,Blocker,Closed,Fixed,16/May/11 12:13,20/Nov/15 12:43
Bug,HBASE-3890,12507361,Scheduled tasks in distributed log splitting not in sync with ZK,"This is in continuation to HBASE-3889:

Note that there must be more slightly off here. Although the splitlogs znode is now empty the master is still stuck here:

{noformat}
Doing distributed log split in hdfs://localhost:8020/hbase/.logs/10.0.0.65,60020,1305406356765	
- Waiting for distributed tasks to finish. scheduled=2 done=1 error=0   4380s

Master startup	
- Splitting logs after master startup   4388s
{noformat}

There seems to be an issue with what is in ZK and what the TaskBatch holds. In my case it could be related to the fact that the task was already in ZK after many faulty restarts because of the NPE. Maybe it was added once (since that is keyed by path, and that is unique on my machine), but the reference count upped twice? Now that the real one is done, the done counter has been increased, but will never match the scheduled.

The code could also check if ZK is actually depleted, and therefore treat the scheduled task as bogus? This of course only treats the symptom, not the root cause of this condition. ",jeffreyz,larsgeorge,Critical,Closed,Fixed,16/May/11 13:17,25/Sep/13 16:08
Bug,HBASE-3892,12507436,Table can't disable,"In TimeoutMonitor : 
if node exists and node state is RS_ZK_REGION_CLOSED
We should send a zk message again when close region is timeout.
in this case, It may be loss some message.


I See. It seems like a bug. This is my analysis.

// disable table and master sent Close message to region server, Region state was set PENDING_CLOSE

2011-05-08 17:44:25,745 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to serverName=C4C4.site,60020,1304820199467, load=(requests=0, regions=123, usedHeap=4097, maxHeap=8175) for region ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66.
2011-05-08 17:44:45,530 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:45:45,542 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467

// received splitting message and cleared Region state (PENDING_CLOSE)

2011-05-08 17:46:45,303 WARN org.apache.hadoop.hbase.master.AssignmentManager: Overwriting 4418fb197685a21f77e151e401cf8b66 on serverName=C4C4.site,60020,1304820199467, load=(requests=0, regions=123, usedHeap=4097, maxHeap=8175)
2011-05-08 17:46:45,538 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:47:45,548 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:48:45,545 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:49:46,108 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:50:46,105 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:51:46,117 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:52:46,112 INFO org.apache.hadoop.hbase.master.ServerManager: Received REGION_SPLIT: ufdr,2011050812#8613817306227#0516,1304845660567.8e9a3b05abe1c3a692999cf5e8dfd9dd.: Daughters; ufdr,2011050812#8613817306227#0516,1304847764729.5e4bca85c33fa6605ffc9a5c2eb94e62., ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. from C4C4.site,60020,1304820199467
2011-05-08 17:52:47,309 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:60000-0x22fcd582836003d Retrieved 125 byte(s) of data from znode /hbase/unassigned/4418fb197685a21f77e151e401cf8b66 and set watcher; region=ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66., server=C4C4.site,60020,1304820199467, state=RS_ZK_REGION_CLOSED
2011-05-08 17:52:47,388 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling new unassigned node: /hbase/unassigned/4418fb197685a21f77e151e401cf8b66 (region=ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66., server=C4C4.site,60020,1304820199467, state=RS_ZK_REGION_CLOSED)
2011-05-08 17:52:47,388 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_CLOSED, server=C4C4.site,60020,1304820199467, region=4418fb197685a21f77e151e401cf8b66

// region server had closed region, but the region state had cleared. So it printed warning log.

2011-05-08 17:52:47,388 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received CLOSED for region 4418fb197685a21f77e151e401cf8b66 from server C4C4.site,60020,1304820199467 but region was in  the state null and not in expected PENDING_CLOSE or CLOSING states
2011-05-08 17:52:47,397 WARN org.apache.hadoop.hbase.master.AssignmentManager: Overwriting 4418fb197685a21f77e151e401cf8b66 on serverName=C4C4.site,60020,1304820199467, load=(requests=0, regions=123, usedHeap=4097, maxHeap=8175)

// The region state was set PENDING_CLOSE again.  the table couldn't disable and enable.
2011-05-08 17:52:47,398 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region ufdr,2011050812#8613817398167#4032,1304847764729.4418fb197685a21f77e151e401cf8b66. (offlining)


",,sunnygao,Major,Closed,Fixed,17/May/11 01:27,20/Nov/15 12:41
Bug,HBASE-3894,12507520,Thread contention over row locks set monitor,"HRegion maintains a set of row locks.  Whenever any thread attempts to lock or release a row it needs to acquire the monitor on that set.  We've been encountering cases with 30 handler threads all contending for that monitor, blocked progress on the region server.  Clients timeout, and retry making it worse, and the region server stops responding to new clients almost entirely.",davelatham,davelatham,Blocker,Closed,Fixed,17/May/11 18:55,20/Nov/15 12:42
Bug,HBASE-3895,12507567,Fix order of parameters after HBASE-1511,"The order of the parameters are not proper in hbase-daemon.sh. Here is a simple change that fixes it:

{noformat}
--- bin/hbase-daemon.sh (revision 1101351)
+++ bin/hbase-daemon.sh (working copy)
@@ -143,7 +143,7 @@
    echo ""`ulimit -a`"" >> $loglog 2>&1
    nohup nice -n $HBASE_NICENESS ""$HBASE_HOME""/bin/hbase \
        --config ""${HBASE_CONF_DIR}"" \
-        $command $startStop ""$@"" > ""$logout"" 2>&1 < /dev/null &
+        $command ""$@"" $startStop > ""$logout"" 2>&1 < /dev/null &
    echo $! > $pid
    sleep 1; head ""$logout""
    ;;
{noformat}

Also see https://issues.apache.org/jira/browse/HBASE-3833?page=com.atlassian.jira.plugin.system.issuetabpanels:comment-tabpanel&focusedCommentId=13028332#comment-13028332

I think this is an oversight and should have been part of the patch. Currently the local-regionservers.sh simply prints out the usage and exits, because the -D are not before the command as expected. ",larsgeorge,larsgeorge,Trivial,Closed,Fixed,18/May/11 07:11,20/Nov/15 12:44
Bug,HBASE-3897,12507691,Docs (notsoquick guide) suggest invalid XML,"If you follow http://hbase.apache.org/notsoquick.html, you'll put the following in your hbase-site.xml:
{noformat}
<configuration>
  ...
  <property>
    <name>hbase.rootdir</name>
    <value>hdfs://localhost:9000/hbase</value>
    <description>The directory shared by region servers.
    </description>
  </property>
  <property>
    <name>dfs.replication</name>
    <value>1</value>
    <description>The replication count for HLog & HFile storage. Should not be greater than HDFS datanode count.
    </description>
  </property>
  ...
</configuration>
{noformat}

Except, oops, that's invalid XML:

{noformat}
[Fatal Error] hbase-site.xml:34:50: The entity name must immediately follow the '&' in the entity reference.
Exception in thread ""main"" java.lang.RuntimeException: org.xml.sax.SAXParseException: The entity name must immediately follow the '&' in the entity reference.
	at org.apache.hadoop.conf.Configuration.loadResource(Configuration.java:1393)
	at org.apache.hadoop.conf.Configuration.loadResources(Configuration.java:1261)
	at org.apache.hadoop.conf.Configuration.getProps(Configuration.java:1192)
	at org.apache.hadoop.conf.Configuration.get(Configuration.java:415)
	at org.apache.hadoop.hbase.HBaseConfiguration.checkDefaultsVersion(HBaseConfiguration.java:63)
	at org.apache.hadoop.hbase.HBaseConfiguration.addHbaseResources(HBaseConfiguration.java:76)
	at org.apache.hadoop.hbase.HBaseConfiguration.create(HBaseConfiguration.java:86)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.main(HRegionServer.java:2737)

{noformat}

Trivial patch to follow.",,philip,Minor,Closed,Fixed,19/May/11 00:19,20/Nov/15 12:43
Bug,HBASE-3903,12507824,A successful write to client write-buffer may be lost or not visible,"A client can do a write to a client side 'write buffer' if enabled via hTable.setAutoFlush(false). Now, assume a client puts value v under key k. Two wrongs things can happen, violating the ACID semantics  of Hbase given at: http://hbase.apache.org/acid-semantics.html

1) Say the client fails immediately after the put succeeds. In this case, the put will be lost, violating the durability property:

<quote> Any operation that returns a ""success"" code (eg does not throw an exception) will be made durable. </quote>
 
2) Say the client issues a read for k immediately after writing k. The put will be stored in the client side write buffer, while the read will go to the region server, returning an older value, instead of v, violating the visibility property:

<quote>
When a client receives a ""success"" response for any mutation, that mutation
is immediately visible to both that client and any client with whom it later
communicates through side channels.
</quote>

Thanks,
Tallat

",dmeil,tallat,Minor,Closed,Fixed,19/May/11 22:43,20/Nov/15 12:42
Bug,HBASE-3904,12507826,HConnection.isTableAvailable returns true even with not all regions available.,"This function as per the java doc is supposed to return true iff ""all the regions in the table are available"". But if the table is still being created this function may return inconsistent results (For example, when a table with a large number of split keys is created). ",yuzhihong@gmail.com,vidhyash,Minor,Closed,Fixed,19/May/11 22:49,12/Jun/22 18:50
Bug,HBASE-3905,12507835,HBaseAdmin.createTableAsync() should check for invalid split keys.,,yuzhihong@gmail.com,vidhyash,Minor,Closed,Fixed,20/May/11 00:17,20/Nov/15 12:43
Bug,HBASE-3908,12507882,"TableSplit not implementing ""hashCode"" problem","
reported by Lucian Iordache on hbase-user mail list. will attach the patch asap
-------------------------------------------

Hi guys,

I've just found a problem with the class TableSplit. It implements ""equals"",
but it does not implement hashCode also, as it should have.
I've discovered it by trying to use a HashSet of TableSplit's, and I've
noticed that some duplicate splits are added to the set.

The only option I have for now is to extend TableSplit and to use the
subclass.
I use cloudera hbase cdh3u0 version.

Do you know about this problem? Should I open a Jira issue for that, or it
already exists?

Thanks,
Lucian
",,diancu,Major,Closed,Fixed,20/May/11 15:51,20/Nov/15 12:42
Bug,HBASE-3912,12508055,[Stargate] Columns not handle by Scan,"There is an issue with ScannerModel only adding the column families to the scan model, not actual columns. Easy fix.",larsgeorge,larsgeorge,Minor,Closed,Fixed,23/May/11 13:43,20/Nov/15 12:41
Bug,HBASE-3914,12508125,ROOT region appeared in two regionserver's onlineRegions at the same time,"This could be happen under the following steps with little probability:
(I suppose the cluster nodes names are RS1/RS2/HM, and there's more than 10,000 regions in the cluster)

1.Root region was opened in RS1.
2.Due to some reason(Maybe the hdfs process was got abnormal),RS1 aborted.
3.ServerShutdownHandler process start.
4.HMaster was restarted, during the finishInitialization's handling, ROOT region was unsetted, and assigned to RS2. 
5.Root region was opened successfully in RS2.
6.But after while, ROOT region was unsetted again by RS1's ServerShutdownHandler. Then it was reassigned. Before that, the RS1 was restarted. So there's two possibilities:
 Case a:
   ROOT region was assigned to RS1. 
   It seemed nothing would be affected. But the root region was still online in RS2.  
   
 Case b:
   ROOT region was assigned to RS2.    
   The ROOT Region couldn't be opened until it would be reassigned to other regionserver, because it was showed online in this regionserver.

This could be proved from the logs:

1. ROOT region was opened with two times:
2011-05-17 10:32:59,188 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region -ROOT-,,0.70236052 on 162-2-77-0,20020,1305598359031
2011-05-17 10:33:01,536 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region -ROOT-,,0.70236052 on 162-2-16-6,20020,1305597548212

2.Regionserver 162-2-16-6 was aborted, so it was reassigned to 162-2-77-0, but already online on this server:
10:49:30,920 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received request to open region: -ROOT-,,0.70236052 10:49:30,920 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Processing open of -ROOT-,,0.70236052 10:49:30,920 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Attempted open of -ROOT-,,0.70236052 but already online on this server

This could be cause a long break of ROOT region offline, though it happened under a special scenario. And I have checked the code, it seems a tiny bug here.

There's 2 references about assignRoot():

1.
HMaster# assignRootAndMeta:

    if (!catalogTracker.verifyRootRegionLocation(timeout)) {
      this.assignmentManager.assignRoot();
      this.catalogTracker.waitForRoot();
      assigned++;
    }

2.
ServerShutdownHandler# process: 
    
      if (isCarryingRoot()) { // -ROOT-      
        try {        
           this.services.getAssignmentManager().assignRoot();
        } catch (KeeperException e) {
           this.server.abort(""In server shutdown processing, assigning root"", e);
           throw new IOException(""Aborting"", e);
        }
      }    

I think each time call the method of assignRoot(), we should verify Root Region's Location first. Because before the assigning, the ROOT region could have been assigned by another place.



",jeason,jeason,Major,Closed,Fixed,24/May/11 01:51,20/Nov/15 12:43
Bug,HBASE-3915,12508131,Binary row keys in hbck and other miscellaneous binary key display issues,This is a patch of miscellany that addresses print out of binary keys in zk and in hbck.  Fixes small issue too in hbck where it says all tables are inconsistent when later in its display it says they are not....(and they are not).,stack,stack,Major,Closed,Fixed,24/May/11 03:56,20/Nov/15 12:43
Bug,HBASE-3916,12508171,Fix problem with default bind address of ThriftServer,"The command line help states that when no -b bind address is given it uses 0.0.0.0. That is not the case though:

{code}
     InetAddress listenAddress = null;
     if (cmd.hasOption(""bind"")) {
       try {
         listenAddress = InetAddress.getByName(cmd.getOptionValue(""bind""));
       } catch (UnknownHostException e) {
         LOG.error(""Could not bind to provided ip address"", e);
         printUsageAndExit(options, -1);
       }
     } else {
       listenAddress = InetAddress.getLocalHost();
     }
{code}

The latter is not 0.0.0.0 but the current IP:

beanshell% InetAddress.getLocalHost()
de1-app-mbp-2/10.0.0.65

So we either need to change the command line help or set the address to 0.0.0.0 instead.
",li,larsgeorge,Minor,Closed,Fixed,24/May/11 11:37,20/Nov/15 12:41
Bug,HBASE-3919,12508247,More places output binary data to text,I noticed some more binary output appearing in the regionserver log.,davelatham,davelatham,Trivial,Closed,Fixed,24/May/11 21:39,20/Nov/15 12:41
Bug,HBASE-3920,12508250,HLog hbase.regionserver.flushlogentries no longer supported,"While searching for config options on syncing the HLog, I was a bit confused by hbase.regionserver.flushlogentries which is still in the code and in hbase-default.xml but isn't actually used.",davelatham,davelatham,Minor,Closed,Fixed,24/May/11 21:52,20/Nov/15 12:40
Bug,HBASE-3923,12508411,HBASE-1502 Broke Shell's status 'simple' and 'detailed',This is due to the JRuby code using the now removed HServerInfo. Also getServers() is now getServersSize() etc.,larsgeorge,larsgeorge,Major,Closed,Fixed,26/May/11 08:54,20/Nov/15 12:42
Bug,HBASE-3934,12508712,MemStoreFlusher.getMemStoreLimit() doesn't honor defaultLimit,"From Lars George:
See below from MemStoreFlusher.java (trunk):
{code}
 static long getMemStoreLimit(final long max, final float limit,
     final float defaultLimit) {
   if (limit >= 0.9f || limit < 0.1f) {
     LOG.warn(""Setting global memstore limit to default of "" + defaultLimit +
       "" because supplied value outside allowed range of 0.1 -> 0.9"");
   }
   return (long)(max * limit);
 }
{code}
The log message says it is using defaultLimit, but the code is not.
Should we fix either?",yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Closed,Fixed,30/May/11 14:06,20/Nov/15 12:41
Bug,HBASE-3939,12508900,Some crossports of Hadoop IPC fixes,"A few fixes from Hadoop IPC that we should probably cross-port into our copy:
- HADOOP-7227: remove the protocol version check at call time
- HADOOP-7146: fix a socket leak in server
- HADOOP-7121: fix behavior when response serialization throws an exception
- HADOOP-7346: send back nicer error response when client is using an out of date IPC version",,tlipcon,Critical,Closed,Fixed,01/Jun/11 01:53,20/Nov/15 12:41
Bug,HBASE-3946,12509045,The splitted region can be online again while the standby hmaster becomes the active one,"(The cluster has two HMatser, one active and one standby)

1.While the active HMaster shutdown, the standby one would become the active one, and went into the processFailover() method:
    if (regionCount == 0) {
      LOG.info(""Master startup proceeding: cluster startup"");
      this.assignmentManager.cleanoutUnassigned();
      this.assignmentManager.assignAllUserRegions();
    } else {
      
      LOG.info(""Master startup proceeding: master failover"");
      this.assignmentManager.processFailover();
    }
2.After that, the user regions would be rebuild.
  Map<HServerInfo,List<Pair<HRegionInfo,Result>>> deadServers = rebuildUserRegions(); 

3.Here's how the rebuildUserRegions worked. All the regions(contain the splitted regions) would be added to the offlineRegions of offlineServers.

   for (Result result : results) {
      Pair<HRegionInfo,HServerInfo> region =
        MetaReader.metaRowToRegionPairWithInfo(result);
      if (region == null) continue;
      HServerInfo regionLocation = region.getSecond();
      HRegionInfo regionInfo = region.getFirst();
      if (regionLocation == null) {
        // Region not being served, add to region map with no assignment
        // If this needs to be assigned out, it will also be in ZK as RIT
        this.regions.put(regionInfo, null);
      } else if (!serverManager.isServerOnline(
          regionLocation.getServerName())) {
        // Region is located on a server that isn't online
        List<Pair<HRegionInfo,Result>> offlineRegions =
          offlineServers.get(regionLocation);
        if (offlineRegions == null) {
          offlineRegions = new ArrayList<Pair<HRegionInfo,Result>>(1);
          offlineServers.put(regionLocation, offlineRegions);
        }
        offlineRegions.add(new Pair<HRegionInfo,Result>(regionInfo, result));
      } else {
        // Region is being served and on an active server
        regions.put(regionInfo, regionLocation);
        addToServers(regionLocation, regionInfo);
      }
    }

4.It seems that all the offline regions will be added to RIT and online again:
ZKAssign will creat node for each offline never consider the splitted ones. 

AssignmentManager# processDeadServers
  private void processDeadServers(
      Map<HServerInfo, List<Pair<HRegionInfo, Result>>> deadServers)
  throws IOException, KeeperException {
    for (Map.Entry<HServerInfo, List<Pair<HRegionInfo,Result>>> deadServer :
      deadServers.entrySet()) {
      List<Pair<HRegionInfo,Result>> regions = deadServer.getValue();
      for (Pair<HRegionInfo,Result> region : regions) {
        HRegionInfo regionInfo = region.getFirst();
        Result result = region.getSecond();
        // If region was in transition (was in zk) force it offline for reassign
        try {
          ZKAssign.createOrForceNodeOffline(watcher, regionInfo,
              master.getServerName());
        } catch (KeeperException.NoNodeException nne) {
          // This is fine
        }
        // Process with existing RS shutdown code
        ServerShutdownHandler.processDeadRegion(regionInfo, result, this,
            this.catalogTracker);
      }
    }
  }

AssignmentManager# processFailover
    // Process list of dead servers
    processDeadServers(deadServers);
    // Check existing regions in transition
    List<String> nodes = ZKUtil.listChildrenAndWatchForNewChildren(watcher,
        watcher.assignmentZNode);
    if (nodes.isEmpty()) {
      LOG.info(""No regions in transition in ZK to process on failover"");
      return;
    }
    LOG.info(""Failed-over master needs to process "" + nodes.size() +
        "" regions in transition"");
    for (String encodedRegionName: nodes) {
      processRegionInTransition(encodedRegionName, null);
    }

So I think before add the region into RIT, check it at first.
",jeason,jeason,Major,Closed,Fixed,02/Jun/11 00:55,20/Nov/15 12:42
Bug,HBASE-3950,12509140,IndexOutOfBoundsException reading results,"I discovered this while testing out HBASE-3789, I can recreate this bug without my patch.

When running TestFromClient, I get failures in testListTables and testJiraTest867. The assertion error is on a number mismatch, but when you look at the log you see:

{quote}
2011-06-02 16:51:24,602 WARN  [IPC Client (47) connection to hbasedev/10.10.1.177:56606 from an unknown user] ipc.HBaseClient$Connection(489): Unexpected exception receiving call responses
java.lang.IndexOutOfBoundsException
        at java.io.BufferedInputStream.read(BufferedInputStream.java:310)
        at java.io.DataInputStream.readFully(DataInputStream.java:178)
        at org.apache.hadoop.hbase.client.Result.readArray(Result.java:652)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:540)
        at org.apache.hadoop.hbase.io.HbaseObjectWritable.readFields(HbaseObjectWritable.java:288)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:563)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:486)
2011-06-02 16:51:24,603 WARN  [IPC Reader 2 on port 56606] ipc.HBaseServer$Listener(600): IPC Server listener on 56606: readAndProcess threw exception java.io.IOException: Connection reset by peer. Count of bytes read: 0
java.io.IOException: Connection reset by peer
        at sun.nio.ch.FileDispatcher.read0(Native Method)
        at sun.nio.ch.SocketDispatcher.read(SocketDispatcher.java:21)
        at sun.nio.ch.IOUtil.readIntoNativeBuffer(IOUtil.java:233)
        at sun.nio.ch.IOUtil.read(IOUtil.java:206)
        at sun.nio.ch.SocketChannelImpl.read(SocketChannelImpl.java:236)
        at org.apache.hadoop.hbase.ipc.HBaseServer.channelRead(HBaseServer.java:1518)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1001)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:596)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:390)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

{quote}

It's not clear to me how I can debug this, but adding some debug inside Result.readArray shows me that the last ints being read are out of whack:

{quote}
numKeys 3
0 keyLen 687
0 offset 55551
1 keyLen 127
1 offset 56242
2 keyLen 130
2 offset 56373
numKeys 3
0 keyLen 666
0 offset 56511
1 keyLen 120
1 offset 57181
2 keyLen 123
2 offset 57305
numKeys 1768842863
0 keyLen 1919248233
0 offset 57436
{quote}

Here I'm printing the tail of the reading of an array of Results where each has 3 KVs. As you can see, the last one has a pretty big number of keys and then the keyLen is also completely off. Looking at the server side when writing, I see that the real number of that last keyLen should be 448.",,jdcryans,Critical,Closed,Fixed,03/Jun/11 00:03,12/Jun/22 18:52
Bug,HBASE-3955,12509382,"Hadoop requirements section needs to be recast; mention of 0.20.clusterbomb release, and cleanup of confusion around where to get apache prebuilt append version","Make a matrix of hadoop versions and of what and what does not work with what version of hbase.

Also, address the misunderstanding our doc. is causing here: http://search-hadoop.com/m/e80zCzRpl51/Hadoop+not+working+after+replacing+hadoop-core.jar+with+hadoop-core-append.jar&subj=Re+Hadoop+not+working+after+replacing+hadoop+core+jar+with+hadoop+core+append+jar

In particular see Joe Pallas paraphrasing of what our manual says.  Address.",stack,stack,Blocker,Closed,Fixed,06/Jun/11 19:50,12/Jun/22 18:53
Bug,HBASE-3958,12509444,use Scan with setCaching() and PageFilter have a problem,"I have a table with 3 ranges,then I scan the table cross all 3 ranges.

Scan scan = new Scan();
scan.setCaching(10);
scan.setFilter(new PageFilter(21));
[result rows count = 63]
the Result has 63 rows, each range has scaned,and locally limit to page_szie.That is expect result.

Then if the page_size = N * caching_size, then result has only page_size rows,only the first range has scanned.
If page_size is Multiple of caching_size,one range rsult just align fill the caching,then client NOT trrige next range scan.
Example:
Scan scan = new Scan();
scan.setCaching(10);
scan.setFilter(new PageFilter(20));
[result rows count = 20]

",,duhb,Minor,Closed,Fixed,07/Jun/11 11:17,12/Jun/22 18:59
Bug,HBASE-3959,12509505,hadoop-snappy version in the pom.xml is incorrect,it should be 0.0.1-SNAPSHOT,tucu00,tucu00,Major,Closed,Fixed,07/Jun/11 20:16,20/Nov/15 11:55
Bug,HBASE-3962,12509593,HConnectionManager.getConnection(HBaseConfiguration) returns new connection in default HTable constructor,"The HBase instance are currently indexed by Configuration, which since HBASE-1976 does not have any other equivalence that the object equivalence.
So, everytime a new configuration is passed to the method a new connection is created.
If we create many HTable connections with the same configuration, there is no problem:

HBaseConfiguration config = HBaseConfiguration.create();
HTable table 1 = new HTable(config, ""table1""); // init connection
HTable table 2 = new HTable(config, ""table2""); // re-use connection
HTable table 3 = new HTable(config, ""table3""); // re-use connection


However, if we call the default constructor, or re-call HBaseConfiguration.create();, we will pass a new instance of the configuration to the constructor. This will cause many connections to be created:
HTable table 1 = new HTable(""table1""); // init connection
HTable table 2 = new HTable(""table2""); // init new connection
HTable table 3 = new HTable(""table3""); // init new connection

I know connection should be pooled, but sometimes we have to create a new connection, and without having access to a previously instanced configuration object.
Since zookeeper has a max client connection (default was 30, now is 10), after creating 30 instances of HTable, we can no longer access to the database.

In addition to this, the HBASE_INSTANCES map does not close the connection when removing the eldest entry. So if we have a larger maxConnection value than the hard-coded MAX_CACHED_HBASE_INSTANCES variable, connections will remain but won't be closed. MAX_CACHED_HBASE_INSTANCES should actually be set from the hbase.zookeeper.property.maxClientCnxns parameter (value + 1).

",,guinotphil,Major,Closed,Fixed,08/Jun/11 14:44,12/Jun/22 18:36
Bug,HBASE-3970,12509741,Address HMaster crash/failure half way through meta migration,"When HMaster tries to migrate (after HBASE-451 goes live) the old HRI (with HTD) to new HRI (with out HTD) and if the Master or the migration process crashes/fails midway, it will leave the .META. in a corrupt state and may not allow successful cluster startup. 

",iamknome,iamknome,Blocker,Closed,Fixed,09/Jun/11 17:20,20/Nov/15 11:52
Bug,HBASE-3971,12509761,Compression.java uses ClassLoader.getSystemClassLoader() to load codec,It should use either the context classloader or the same classloader as the Hbase classes.,tucu00,tucu00,Major,Closed,Fixed,09/Jun/11 21:03,20/Nov/15 11:54
Bug,HBASE-3972,12509763,HBaseObjectWritable should support arbitrary arrays as long as their contents are supported by HBaseObjectWritable,,,ekohlwey,Major,Closed,Fixed,09/Jun/11 21:09,12/Jun/22 18:34
Bug,HBASE-3974,12509774,Client: Ability to Discard Bad HTable Puts,"While debugging an application consistency issue, we noticed that a single, synchronous Put request threw a NoServerForRegionException but eventually succeeded 90 seconds later.  The problem is that failed put requests are not actually removed from the HTable's writeBuffer.  This makes sense for asynchronous puts using setAutoFlush(false) but don't make sense for the default case where we expect synchronous operation.  We should discard all failed puts for the synchronous case and provide an API so asynchronous requests can have their failed puts cleared.",nspiegelberg,nspiegelberg,Critical,Closed,Fixed,09/Jun/11 22:49,12/Oct/12 06:19
Bug,HBASE-3975,12509775,NoServerForRegionException stalls write pipeline,"When we process a batch of puts, the current algorithm basically goes like this:

1. Find all servers for the Put requests
2. Partition Puts by servers
3. Make requests
4. Collect success/error results

If we throw an IOE in step 1 or 2, we will abort the whole batch operation.  In our case, this was an NoServerForRegionException due to region rebalancing.  However, the asynchronous put case normally has requests going to a wide variety of servers.  We should fail all the put requests that throw an IOE in Step 1 but continue to try all the put requests that succeed at this stage.",nspiegelberg,nspiegelberg,Major,Closed,Fixed,09/Jun/11 22:59,12/Jun/22 18:34
Bug,HBASE-3978,12509878,rowlock lease renew doesn't work when custom coprocessor/RegionObserver indicates to bypass default action,"Region server will keep extending the lease on the rowlock as long as there is some action on the row. This is done inside HRegionServer.getLockFromId, where it renew the lease on the rowlock. However, when coprocessor ""pre-action"" observer is used, getLockFromId is skipped if default action is bypassed. Take HRegionServer.exists as example, RegionCoprocessorHost.preExists could return Boolean object if one of the regionobservers indicates default action should be bypassed; thus getLockFromId didn't called and the lease on the lock doesn't get renewed.

  public boolean exists(byte[] regionName, Get get) throws IOException {
    checkOpen();
    requestCount.incrementAndGet();
    try {
      HRegion region = getRegion(regionName);
      if (region.getCoprocessorHost() != null) {
        Boolean result = region.getCoprocessorHost().preExists(get);
        if (result != null) {
          return result.booleanValue();
        }
      }
      Result r = region.get(get, getLockFromId(get.getLockId()));
      boolean result = r != null && !r.isEmpty();
      if (region.getCoprocessorHost() != null) {
        result = region.getCoprocessorHost().postExists(get, result);
      }
      return result;
    } catch (Throwable t) {
      throw convertThrowableToIOE(cleanup(t));
    }
  }


The application scenario is:
a) client application passes in a rowlock object in an action.
b) there is custom coprocessor/observer used.
c) For a given action, the custom coprocessor/observer might tell coprocessor framework to bypass the default action.

From client application point of view, the behavior is sometimes the rowlock will timeout even though client is accessing the row all the time, depending on whether coprocessor/observer wants to bypass the default action.

This applies to several other actions as well, increment, checkAndPut, checkandDelete.",mingma,mingma,Major,Closed,Fixed,10/Jun/11 21:25,20/Nov/15 11:52
Bug,HBASE-3979,12509882,"Trivial fixes in code, document","src/main/java/org/apache/hadoop/hbase/client/HConnectionManager.java, line 980, 981 aren't needed.

value = regionInfoRow.getValue(HConstants.CATALOG_FAMILY,
HConstants.SERVER_QUALIFIER);

src/main/java/org/apache/hadoop/hbase/master/SplitLogManager.java, document on function splitLogDistributed ""param serverName"" doesn't match the actual parameter.

src/main/java/org/apache/hadoop/hbase/regionserver/RegionCoprocessorHost.java, document on function postIncrement""param result the result returned by incrementColumnValue"" is incorrect.",mingma,mingma,Trivial,Closed,Fixed,10/Jun/11 21:51,20/Nov/15 11:53
Bug,HBASE-3983,12510075,list command in shell seems broken,"hbase(main):007:0> list

ERROR: wrong number of arguments (1 for 2)
",stack,tlipcon,Blocker,Closed,Fixed,13/Jun/11 04:06,20/Nov/15 11:54
Bug,HBASE-3984,12510126,"CT.verifyRegionLocation isn't doing a very good check, can delay cluster recovery","After some extensive debugging in the thread [A sudden msg of ""java.io.IOException: Server not running, aborting""|http://search-hadoop.com/m/Qb0BMnrTPZ1], we figured that the region servers weren't able to talk to the new .META. location because the old one was still alive but on it's way down after a OOME.

It translates into exceptions like ""Server not running"" coming from trying to edit .META. and digging in the code I see that CT.waitForMetaServerConnectionDefault -> waitForMeta -> getMetaServerConnection(true) calls verifyRegionLocation since we force the refresh. In this method we check if the RS is good by calling getRegionInfo which *does not* check if the region server is trying to close.

What this means is that a cluster can't recover a .META.-serving RS failure until it has fully shutdown since every time a RS tries to open a region (like right after the log splitting) or split it fails editing .META.",jdcryans,jdcryans,Blocker,Closed,Fixed,13/Jun/11 18:36,20/Nov/15 11:53
Bug,HBASE-3985,12510250,Same Region could be picked out twice in LoadBalancer,"From the HMaster logs, I found something weird:

2011-05-24 11:12:11,152 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=hello,122130,1305944329350.7d6c96428e2563c3d8676474d0a9f814., src=158-1-101-202,20020,1306205409671, dest=158-1-101-222,20020,1306205940117
2011-05-24 11:12:31,536 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=hello,122130,1305944329350.7d6c96428e2563c3d8676474d0a9f814., src=158-1-101-202,20020,1306205409671, dest=158-1-101-222,20020,1306205940117

We can see that, the same region was balanced twice.

To describe the problem, I give out one simple example:

1. Suppose regions count is 10 in RegionServer A.
   Max: 5  Min:4
2. So the regions count need to move is: 5.
3. Before the movement of calculate, the list was shuffled.
4. The 5 moving region was picked out from the back.
5. The nextRegionForUnload value is 5.
6. So if the neededRegions is not zero. Maybe there's still one region should be picked out from RegionServer A.
   This time , the picked Index is 5 which has been picked once!!!!! 
                                  
                          |<-----5-------|                               
------------*--*--*--*--*--*--*--*--*--*----
                           |
                   getNextRegionForUnload                             

Here's the analysis from code:           

1. Walk down most loaded, pruning each to the max. Picked region from back of the list(by reverse order)   
Map<HServerInfo,BalanceInfo> serverBalanceInfo =
      new TreeMap<HServerInfo,BalanceInfo>();
    for(Map.Entry<HServerInfo, List<HRegionInfo>> server :
      serversByLoad.descendingMap().entrySet()) {
      HServerInfo serverInfo = server.getKey();
      int regionCount = serverInfo.getLoad().getNumberOfRegions();
      if(regionCount <= max) {
        serverBalanceInfo.put(serverInfo, new BalanceInfo(0, 0));
        break;
      }
      serversOverloaded++;
      List<HRegionInfo> regions = randomize(server.getValue());
      int numToOffload = Math.min(regionCount - max, regions.size());
      int numTaken = 0;
      for (int i = regions.size() - 1; i >= 0; i--) {
        HRegionInfo hri = regions.get(i);
        // Don't rebalance meta regions.
        if (hri.isMetaRegion()) continue;
        regionsToMove.add(new RegionPlan(hri, serverInfo, null));
        numTaken++; 
        if (numTaken >= numToOffload) break;
      }
      /**********************************************************/
      /***set the nextRegionForUnload  value is numToOffload ****/
      /**********************************************************/
      serverBalanceInfo.put(serverInfo,
          new BalanceInfo(numToOffload, (-1)*numTaken));
    }
2. The second pass of picked one region from the Max regionserver by order.
    if (neededRegions != 0) {
      // Walk down most loaded, grabbing one from each until we get enough
      for(Map.Entry<HServerInfo, List<HRegionInfo>> server :
        serversByLoad.descendingMap().entrySet()) {
        BalanceInfo balanceInfo = serverBalanceInfo.get(server.getKey());
        int idx =
          balanceInfo == null ? 0 : balanceInfo.getNextRegionForUnload();
        if (idx >= server.getValue().size()) break;
        HRegionInfo region = server.getValue().get(idx);
        if (region.isMetaRegion()) continue; // Don't move meta regions.
        regionsToMove.add(new RegionPlan(region, server.getKey(), null));
        if(--neededRegions == 0) {
          // No more regions needed, done shedding
          break;
        }
      }
    }
",jeason,jeason,Major,Closed,Fixed,14/Jun/11 00:50,20/Nov/15 11:52
Bug,HBASE-3987,12510373,Fix a NullPointerException on a failure to load Bloom filter data,"This is a fix for an NullPointerException that happens in passesBloomFilter. The meta block fails to load, and the IOException catch block sets the Bloom filter to null. Then all other threads waiting on the Bloom filter to load get a chance to try to load the meta block, and one of them eventually succeeds and goes on to query the Bloom filter in StoreFile.passesBloomFilter, but bloomFilter has been already set to null. The fix is to cache the bloomFilter variable in a local variable in passesBloomFilter so that it cannot be made null while the thread is waiting for another thread to load Bloom filter bits.",mikhail,mikhail,Major,Closed,Fixed,14/Jun/11 22:28,12/Oct/12 05:35
Bug,HBASE-3988,12510377,Infinite loop for secondary master,"There seems be a bug that the secondary master didn't come out when the primary master dead. 
Because the secondary master will be in a loop forever to watch a local variable before setting a zk watcher.
However this local variable is changed by the zk call back function.
So the secondary master will be in the infinite loop forever.",liyin,liyin,Major,Closed,Fixed,14/Jun/11 22:52,20/Nov/15 11:55
Bug,HBASE-3989,12510386,"Error occured while RegionServer report to Master ""we are up"" should get master address again","I happened to fall across a problem, after some further analysis, I found the problem(The logs was attached at the end of the email)

Consider the following scenario which is similar with my problem :

1. Due to some unclear reason,  the report to master got error. And retrying several times, but also failed.
2. During this time , the standby master becomes the active one. So the endless loop is still running, and it won't success, for the master address has updated, but it didn't know. And won't know again.

   while (!stopped && (masterAddress = getMaster()) == null) {
      sleeper.sleep();
      LOG.warn(""Unable to get master for initialization"");
    }

    MapWritable result = null;
    long lastMsg = 0;
    while (!stopped) {
      try {
        this.requestCount.set(0);
        lastMsg = System.currentTimeMillis();
        ZKUtil.setAddressAndWatch(zooKeeper,
          ZKUtil.joinZNode(zooKeeper.rsZNode, ZKUtil.getNodeName(serverInfo)),
          this.serverInfo.getServerAddress());
        this.serverInfo.setLoad(buildServerLoad());
        LOG.info(""Telling master at "" + masterAddress + "" that we are up"");
        result = this.hbaseMaster.regionServerStartup(this.serverInfo,
            EnvironmentEdgeManager.currentTimeMillis());
        break;
      } catch (RemoteException e) {
        IOException ioe = e.unwrapRemoteException();
        if (ioe instanceof ClockOutOfSyncException) {
          LOG.fatal(""Master rejected startup because clock is out of sync"",
              ioe);
          // Re-throw IOE will cause RS to abort
          throw ioe;
        } else {
          LOG.warn(""remote error telling master we are up"", e);
        }
      } catch (IOException e) {
        LOG.warn(""error telling master we are up"", e);
      } catch (KeeperException e) {
        LOG.warn(""error putting up ephemeral node in zookeeper"", e);
      }
      sleeper.sleep(lastMsg);
    }


Here's the logs:

2011-06-13 11:25:12,236 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: error telling master we are up
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:207)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:419)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:328)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:883)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:750)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
	at $Proxy5.regionServerStartup(Unknown Source)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.reportForDuty(HRegionServer.java:1511)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryReportForDuty(HRegionServer.java:1479)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:571)
	at java.lang.Thread.run(Thread.java:662)
2011-06-13 11:25:15,231 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Telling master at 157-5-111-22:20000 that we are up
2011-06-13 11:25:15,232 WARN org.apache.hadoop.hbase.regionserver.HRegionServer: error telling master we are up
java.net.ConnectException: Connection refused
	at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
	at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:574)
	at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:207)
	at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:419)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:328)
	at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:883)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:750)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
	at $Proxy5.regionServerStartup(Unknown Source)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.reportForDuty(HRegionServer.java:1511)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.tryReportForDuty(HRegionServer.java:1479)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.run(HRegionServer.java:571)
	at java.lang.Thread.run(Thread.java:662)
2011-06-13 11:25:18,225 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Telling master at 157-5-111-22:20000 that we are up

So I think, while the error orrured, we should re-get the master address. This problem could be solved.
",jeason,jeason,Major,Closed,Fixed,15/Jun/11 01:59,20/Nov/15 11:52
Bug,HBASE-3995,12510546,HBASE-3946 broke TestMasterFailover,"TestMasterFailover is all about a new master coming up on an existing cluster.  Previous to HBASE-3946, the new master joining a cluster processing any dead servers would assign all regions found on the dead server even if they were split parents.  We don't want that.

But TestMasterFailover mocks up some pretty interesting conditions.  The one we were failing on was that while the master was offine, we'd manually add a region to zk that was in CLOSING state.  We'd then go and disable the table up in zk (while master was offline).  Finally, we'd' kill the server that was supposed to be hosting the region from the disabled table in CLOSING state. Then we'd have the master join the cluster.  It had to figure it out.

Before HBASE-3946, we'd just force offline every region that had been on the dead server.  This would call all to be assigned only on assign, regions from disabled tables are skipped, so it all ""worked"" (except would online parent of a split should there be one).

",stack,stack,Blocker,Closed,Fixed,16/Jun/11 09:15,20/Nov/15 11:53
Bug,HBASE-4003,12510741,Cleanup Calls Conservatively On Timeout,"In the event of a socket timeout, the {{HBaseClient}} iterates over the outstanding calls (on that socket), and notifies them that a {{SocketTimeoutException}} has occurred. Ideally, we should be cleanup up just those calls that have been outstanding for longer than the specified socket timeout.",karthick,karthick,Major,Closed,Fixed,18/Jun/11 01:45,20/Nov/15 11:55
Bug,HBASE-4005,12510777,close_region bugs,Lars found and fixed some issues with close_region.  See http://search-hadoop.com/m/L8dD1V5yts1/Issues+with+close_region&subj=Issues+with+close_region,stack,stack,Major,Closed,Fixed,18/Jun/11 22:19,20/Nov/15 11:56
Bug,HBASE-4007,12510797,distributed log splitting can get indefinitely stuck,"After the configured number of retries SplitLogManager is not going to resubmit log-split tasks. In this situation even if the splitLogWorker that owns the task dies the task will not get resubmitted.

When a regionserver goes away then all the split-log tasks that it owned should be resubmitted by the SplitLogMaster.",khemani,khemani,Critical,Closed,Fixed,19/Jun/11 02:35,20/Nov/15 11:54
Bug,HBASE-4008,12510937,Problem while stopping HBase,"stop-hbase.sh stops the server successfully if and only if the server is instantiated properly. 

When u Run 

start-hbase.sh; sleep 10; stop-hbase.sh; ( This works totally fine and has no issues )

Whereas when u run 

start-hbase.sh; stop-hbase.sh; ( This never stops the server and neither the server gets initialized and starts properly )",thehellmaker,thehellmaker,Major,Closed,Fixed,20/Jun/11 10:26,20/Nov/15 11:55
Bug,HBASE-4016,12511184,HRegion.incrementColumnValue() doesn't have a consistent behavior when the field that we are incrementing is less than 8 bytes long,"We wanted to use an int (32-bit) atomic counter and we initialize it with a certain value when the row is created. Later, we increment the counter using HTable.incrementColumnValue(). This call results in one of two outcomes. 

1. The call succeeds, but the column value now is a long (64-bit) and is corrupt (by additional data that was in the buffer read).
2. Throws IOException/IllegalArgumentException.
Java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: offset (65547) + length (8) exceed the capacity of the array: 65551
        at org.apache.hadoop.hbase.util.Bytes.explainWrongLengthOrOffset(Bytes.java:502)
        at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:480)
        at org.apache.hadoop.hbase.regionserver.HRegion.incrementColumnValue(HRegion.java:3139)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.incrementColumnValue(HRegionServer.java:2468)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)

Based on our incorrect usage of counters (initializing it with a 32 bit value and later using it as a counter), I would expect that we fail consistently with mode 2 rather than silently corrupting data with mode 1. However, the exception is thrown only rarely and I am not sure what determines the case to be executed. I am wondering if this has something to do with flush.

Here is a HRegion unit test that can reproduce this problem. http://paste.lisp.org/display/122822

We modified our code to initialize the counter with a 64 bit value. But, I was also wondering if something has to change in HRegion.incrementColumnValue() to handle inconsistent counter sizes gracefully without corrupting existing data.

Please let me know if you need additional information.
",li,kprav33n,Major,Closed,Fixed,21/Jun/11 23:07,20/Nov/15 11:53
Bug,HBASE-4020,12511391,"""testWritesWhileGetting"" unit test needs to be fixed. ","The unit test ""testWritesWhileGetting"" in the org.apache.hadoop.hbase.regionserver.TestHRegion test needs to be corrected. It is current using the table name and method name for initializing a HRegion as ""testWritesWhileScanning"". It should be ""testWritesWhileGetting"". 

Due to this, the test fails as the ""initHRegion"" method fails in creating a new HRegion for the test. ",avandana,avandana,Major,Closed,Fixed,23/Jun/11 18:34,20/Nov/15 11:52
Bug,HBASE-4022,12511405,Shell reports error on space prefixed colfam name,"See this test:

{noformat}
hbase(main):002:0> create 'test', ' cf'

hbase(main):003:0> scan 'test'
ROW                                           COLUMN+CELL                                                                                                                         
0 row(s) in 0.1050 seconds

hbase(main):004:0> describe 'test'
DESCRIPTION                                                                                                        ENABLED                                                        
{NAME => 'test', FAMILIES => [{NAME => ' cf', BLOOMFILTER => 'NONE', REPLICATION_SCOPE => '0', COMPRESSION => 'NO true                                                           
NE', VERSIONS => '3', TTL => '2147483647', BLOCKSIZE => '65536', IN_MEMORY => 'false', BLOCKCACHE => 'true'}]}                                                                   
1 row(s) in 0.0540 seconds

hbase(main):005:0> put 'test', 'r1', ' cf:1', 'v1'
0 row(s) in 0.0580 seconds

hbase(main):006:0> scan 'test', { COLUMNS => [' cf'] }
ROW                                           COLUMN+CELL                                                                                                                         

ERROR: Unknown column family! Valid column names:  cf

Here is some help for this command:
Scan a table; pass table name and optionally a dictionary of scanner
specifications.  Scanner specifications may include one or more of:
TIMERANGE, FILTER, LIMIT, STARTROW, STOPROW, TIMESTAMP, MAXLENGTH,
or COLUMNS. If no columns are specified, all columns will be scanned.
To scan all members of a column family, leave the qualifier empty as in
'col_family:'.

Some examples:

 hbase> scan '.META.'
 hbase> scan '.META.', {COLUMNS => 'info:regioninfo'}
 hbase> scan 't1', {COLUMNS => ['c1', 'c2'], LIMIT => 10, STARTROW => 'xyz'}
 hbase> scan 't1', {FILTER => org.apache.hadoop.hbase.filter.ColumnPaginationFilter.new(1, 0)}
 hbase> scan 't1', {COLUMNS => 'c1', TIMERANGE => [1303668804, 1303668904]}

For experts, there is an additional option -- CACHE_BLOCKS -- which
switches block caching for the scanner on (true) or off (false).  By
default it is enabled.  Examples:

 hbase> scan 't1', {COLUMNS => ['c1', 'c2'], CACHE_BLOCKS => false}
8:57 PM
haha
8:58 PM
hbase(main):008:0> scan 'test'                          
ROW                                           COLUMN+CELL                                                                                                                         
r1                                           column= cf:1, timestamp=1308855379447, value=v1                                                                                     
1 row(s) in 0.0450 seconds
{noformat}

We should handle this better.",,larsgeorge,Minor,Closed,Fixed,23/Jun/11 20:59,12/Jun/22 19:03
Bug,HBASE-4024,12511414,"Major compaction may not be triggered, even though region server log says it is triggered","The trunk version of regionserver/Store.java, method   List<StoreFile> compactSelection(List<StoreFile> candidates) has this code to determine whether major compaction should be done or not: 

    // major compact on user action or age (caveat: we have too many files)
    boolean majorcompaction = (forcemajor || isMajorCompaction(filesToCompact))
      && filesToCompact.size() < this.maxFilesToCompact;


The isMajorCompaction(filesToCompact) method internally determines whether or not major compaction is required (and logs this as ""Major compaction triggered ... "" log message. However, after the call, the compactSelection method subsequently applies the filesToCompact.size() < this.maxFilesToCompact check which can turn off major compaction. 

This would result in a ""Major compaction triggered"" log message without actually triggering a major compaction.

The filesToCompact.size() check should probably be moved inside the isMajorCompaction(filesToCompact) method.",yuzhihong@gmail.com,svarma,Trivial,Closed,Fixed,23/Jun/11 22:50,20/Nov/15 11:55
Bug,HBASE-4025,12511423,"Server startup fails during startup due to failure in loading all table descriptors. We should ignore .logs,.oldlogs,.corrupt,.META.,-ROOT- folders while reading descriptors ","2011-06-23 21:39:52,524 WARN org.apache.hadoop.hbase.monitoring.TaskMonitor: Status org.apache.hadoop.hbase.monitoring.MonitoredTaskImpl@2f56f920 appears to have been leaked
2011-06-23 21:40:06,465 WARN org.apache.hadoop.hbase.master.HMaster: Failed getting all descriptors
java.io.FileNotFoundException: No status for hdfs://ciq.com:9000/hbase/.corrupt
	at org.apache.hadoop.hbase.util.FSUtils.getTableInfoModtime(FSUtils.java:888)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:122)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.getAll(FSTableDescriptors.java:149)
	at org.apache.hadoop.hbase.master.HMaster.getHTableDescriptors(HMaster.java:1442)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:340)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1138)
2011-06-23 21:40:26,790 WARN org.apache.hadoop.hbase.master.HMaster: Failed getting all descriptors
java.io.FileNotFoundException: No status for hdfs://ciq.com:9000/hbase/.corrupt
	at org.apache.hadoop.hbase.util.FSUtils.getTableInfoModtime(FSUtils.java:888)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:122)
	at org.apache.hadoop.hbase.util.FSTableDescriptors.getAll(FSTableDescriptors.java:149)
	at org.apache.hadoop.hbase.master.HMaster.getHTableDescriptors(HMaster.java:1442)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:340)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1138)",iamknome,iamknome,Major,Closed,Fixed,23/Jun/11 23:54,12/Jun/22 18:47
Bug,HBASE-4028,12511441,Hmaster crashes caused by splitting log.,"In my performance cluster(0.90.3), The Hmaster memory from 100 M up to 4G when one region server crashed.
I added some print in function doneWriting and found the values of totalBuffered is negative.

10:29:52,119 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: gjc:release Used -565832
hbase-root-master-157-5-111-21.log:2011-06-24 10:29:52,119 WARN org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: gjc:release Used -565832release size25168

void doneWriting(RegionEntryBuffer buffer) {
      synchronized (this) {
    	LOG.warn(""gjc1: relase currentlyWriting +biggestBufferKey "" + buffer.encodedRegionName );
        boolean removed = currentlyWriting.remove(buffer.encodedRegionName);
        assert removed;
      }
      long size = buffer.heapSize();

      synchronized (dataAvailable) {
        totalBuffered -= size;
        LOG.warn(""gjc:release Used "" + totalBuffered );
        // We may unblock writers
        dataAvailable.notifyAll();
      }
      LOG.warn(""gjc:release Used "" + totalBuffered + ""release size""+ size);
    }",sunnygao,sunnygao,Major,Closed,Fixed,24/Jun/11 06:14,20/Nov/15 11:55
Bug,HBASE-4029,12511488,Inappropriate checking of Logging Mode in HRegionServer,"There is a condition check for Debug mode logging in HRegionServer.java . Because of this the region server never closes the META region while stopping hbase and thus never stops, if DEBUG mode is not enable in logging. ",thehellmaker,thehellmaker,Major,Closed,Fixed,24/Jun/11 16:12,20/Nov/15 11:52
Bug,HBASE-4032,12511517,HBASE-451 improperly breaks public API HRegionInfo#getTableDesc,"After HBASE-451, HRegionInfo#getTableDesc has been modified to always return {{null}}. 

One immediate effect is broken unit tests.

That aside, it is not in the spirit of deprecation to actually break the method until after the deprecation cycle, it's a bug.",stack,apurtell,Blocker,Closed,Fixed,25/Jun/11 01:34,20/Nov/15 11:52
Bug,HBASE-4033,12511518,The shutdown RegionServer could be added to AssignmentManager.servers again,"The folling steps can easily recreate the problem:
1. There's thousands of regions in the cluster.
2. Stop the cluster.
3. Start the cluster. Killing one regionserver while the regions were opening. Restarted it after 10 seconds.

The shutted regionserver will appear in the AssignmentManager.servers list again.

For example:

Issue 1:

2011-06-23 14:14:30,775 DEBUG org.apache.hadoop.hbase.master.LoadBalancer: Server information: 167-6-1-12,20020,1308803390123=2220, 167-6-1-13,20020,1308803391742=2374, 167-6-1-11,20020,1308803386333=2205, 167-6-1-13,20020,1308803514394=2183

Two regionservers(One of it had aborted) had the same hostname but different startcode:
167-6-1-13,20020,1308803391742=2374
167-6-1-13,20020,1308803514394=2183

Issue 2:

(1).The Rs 167-6-1-11,20020,1308105402003 finished shutdown at ""10:46:37,774"":
10:46:37,774 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Finished processing of shutdown of 167-6-1-11,20020,1308105402003

(2).Overwriting happened, it seemed the RS was still exist in the set of AssignmentManager#regions:
10:45:55,081 WARN org.apache.hadoop.hbase.master.AssignmentManager: Overwriting 612342de1fe4733f72299d70addb6d11 on serverName=167-6-1-11,20020,1308105402003, load=(requests=0, regions=0, usedHeap=0, maxHeap=0)

(3).Region was assigned to this dead RS again at ""10:50:20,671"":
10:50:20,671 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region Jeason10,08058613800000030,1308032774777.612342de1fe4733f72299d70addb6d11. to 167-6-1-11,20020,1308105402003",jeason,jeason,Major,Closed,Fixed,25/Jun/11 02:11,20/Nov/15 11:54
Bug,HBASE-4034,12511525,HRegionServer should be stopped even if no META regions are hosted by the HRegionServer,"HRegionServer always makes sure one META region is hosted for it to stop. This should be changed so that even if no META regions are hosted, the HRegionServer should be stopped once all user regions are closed.",thehellmaker,thehellmaker,Major,Closed,Fixed,25/Jun/11 06:07,20/Nov/15 11:55
Bug,HBASE-4035,12511526,Fix local-master-backup.sh - parameter order wrong,"I think this is a regression from HBASE-3895 (if so, my bad!). The local-master-backup.sh needs the parameters adjusted to work.

{noformat}
$ bin/local-master-backup.sh start 1          
starting master, logging to /var/lib/hbase/logs/hbase-larsgeorge-1-master-de1-app-mbp-2.out
Usage: Master [opts] start|stop
 start  Start Master. If local mode, start Master and RegionServer in same JVM
 stop   Start cluster shutdown; Master signals RegionServer shutdown
 where [opts] are:
   --minServers=<servers>    Minimum RegionServers needed to host user tables.
   --backup                  Master should start in backup mode
{noformat}

The log has

{noformat}
2011-06-25 10:26:34,864 ERROR org.apache.hadoop.hbase.master.HMasterCommandLine: Could not parse: 
org.apache.commons.cli.UnrecognizedOptionException: Unrecognized option: -D
        at org.apache.commons.cli.Parser.processOption(Parser.java:363)
        at org.apache.commons.cli.Parser.parse(Parser.java:199)
        at org.apache.commons.cli.Parser.parse(Parser.java:85)
        at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:73)
        at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
        at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
        at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1346)
{noformat}

Fix:

{noformat}
Index: bin/local-master-backup.sh
===================================================================
--- bin/local-master-backup.sh  (revision 1139502)
+++ bin/local-master-backup.sh  (working copy)
@@ -21,9 +21,9 @@
   DN=$2
   export HBASE_IDENT_STRING=""$USER-$DN""
   HBASE_MASTER_ARGS=""\
-    --backup \
     -D hbase.master.port=`expr 60000 + $DN` \
-    -D hbase.master.info.port=`expr 60010 + $DN`""
+    -D hbase.master.info.port=`expr 60010 + $DN` \
+    --backup""
   ""$bin""/hbase-daemon.sh $1 master $HBASE_MASTER_ARGS
 }
{noformat}
",larsgeorge,larsgeorge,Minor,Closed,Fixed,25/Jun/11 09:24,20/Nov/15 11:52
Bug,HBASE-4041,12512155,createTable(splitKeys) should be synchronous,"Currently, there is a bug where createTable will return once the first region in the table has gone online, but this is supposed to be a synchronized operation.  ",yuzhihong@gmail.com,itapai,Minor,Closed,Fixed,29/Jun/11 01:48,12/Jun/22 18:47
Bug,HBASE-4043,12512167,0.90 jenkins build failing because of 'too many open files',There was a recent restart of jenkins and we seem to have lost ulimit > 1024.  I tried to up it but I can't as user jenkins.  Apache infrastructure recognize it as an issue: https://issues.apache.org/jira/browse/INFRA-3734  Hopefully a fix soon.,,stack,Blocker,Closed,Fixed,29/Jun/11 04:43,20/Nov/15 11:55
Bug,HBASE-4045,12512261,[replication] NPE in ReplicationSource when ZK is gone,"We got this in production, it killed the replication thread but the server itself was fine and the master kept the logs:

{quote}
2011-06-26 16:02:56,092 INFO org.apache.zookeeper.ClientCnxn: Client session timed out, have not heard from server in 26667ms for sessionid 0x22f9dcb30ab01b8, closing socket connection and attempting reconnect
2011-06-26 16:02:56,213 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: connection to cluster: 5-0x22f9dcb30ab01b8-0x22f9dcb30ab01b8 Received ZooKeeper Event, type=None, state=Disconnected, path=null
2011-06-26 16:02:56,213 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: connection to cluster: 5-0x22f9dcb30ab01b8-0x22f9dcb30ab01b8 Received Disconnected from ZooKeeper, ignoring
2011-06-26 16:02:56,213 WARN org.apache.hadoop.hbase.replication.ReplicationZookeeper: Cannot get peer's region server addresses
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/rs
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.getChildren(ZooKeeper.java:1243)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.listChildrenNoWatch(ZKUtil.java:389)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.listChildrenAndGetAsAddresses(ZKUtil.java:355)
        at org.apache.hadoop.hbase.replication.ReplicationZookeeper.fetchSlavesAddresses(ReplicationZookeeper.java:228)
        at org.apache.hadoop.hbase.replication.ReplicationZookeeper.getSlavesAddresses(ReplicationZookeeper.java:216)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.chooseSinks(ReplicationSource.java:205)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:588)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:341)2011-06-26 16:02:56,222 ERROR org.apache.hadoop.hbase.replication.regionserver.ReplicationSource: Closing source 5 because an error occurred: Uncaught exception during runtime
java.lang.Exception: java.lang.NullPointerException
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource$1.uncaughtException(ReplicationSource.java:628)
        at java.lang.Thread.dispatchUncaughtException(Thread.java:1874)Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.chooseSinks(ReplicationSource.java:208)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.shipEdits(ReplicationSource.java:588)
        at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.run(ReplicationSource.java:341)

{quote}",jdcryans,jdcryans,Minor,Closed,Fixed,29/Jun/11 22:53,20/Nov/15 11:53
Bug,HBASE-4051,12512392,[Coprocessors] Table coprocessor loaded twice when region is initialized,"I'm debugging a prePut hook which I've implemented as part of the coprocessor work being developed. This hook is loaded via a table COPROCESSOR attribute and I've noticed that the prePut method is being called twice for a single Put. After setting up the region server to run in a debugger, I'm noticing the call to loadTableCoprocessors() being invoked twice during region initialization, specifically: 

1.       HRegion.init => RegionCoprocessorHost.init => RegionCoprocessorHost.loadTableCoprocessors 

2.       ... => RegionCoprocessorHost.preOpen => RegionCoprocessorHost.loadTableCoprocessors 

This results in two RegionEnvironment instances, each containing a new instance of my coprocessor, being added to the RegionCoprocessorHost. When I issue a put, the list of RegionEnvironments is iterated over and each calls the prePut method in my coprocessor. Reason why this is posing a problem for me is that I modify the family map passed in to my prePut method. Since this family map is the same instance used in both prePut calls, the second prePut call operates on the modified family map, which leads to an unexpected result. 

Is the double loading of the same coprocessor class intentional, is this a bug?",apurtell,terry.siu,Major,Closed,Fixed,30/Jun/11 23:03,20/Nov/15 11:52
Bug,HBASE-4052,12512427,"Enabling a table after master switch does not allow table scan, throwing NotServingRegionException","Following is the scenario:

Start RS and Active and standby masters
Create table and insert data.
Disable the table.
Stop the active master and switch to the standby master.
Now enable the table.
Do a scan on the enabled table.
NotServingRegionException is Thrown.

But the same works well when we dont switch the master.
",ram_krish,ram_krish,Major,Closed,Fixed,01/Jul/11 04:47,20/Nov/15 11:52
Bug,HBASE-4053,12512448,Most of the regions were added into AssignmentManager#servers twice,"Here's the scenario of how did the problem happened:

1. When HMaster start, all regionservers checkin ok, and count of regions out on cluster is 10083, which is the actual region number count.
2. Then OpenedRegionHandler#process received zookeeper's events, and added 9923 regions to the hris list.
   but the 9923 regions already exists, force added.
3. The LoadBalancer get the wrong Region numbers of 20006 (10083 + 9923).

AssignmentManager#addToServers method:
private void addToServers(final HServerInfo hsi, final HRegionInfo hri) {
  List<HRegionInfo> hris = servers.get(hsi);
  if (hris == null) {
    hris = new ArrayList<HRegionInfo>();
    servers.put(hsi, hris);
  }
  hris.add(hri); // Same region was double added here
}

logs:
2011-06-27 16:13:06,845 INFO org.apache.hadoop.hbase.master.ServerManager: Exiting wait on regionserver(s) to checkin; count=3, stopped=false, count of regions out on cluster=10083
2011-06-27 16:13:17,334 INFO org.apache.hadoop.hbase.master.AssignmentManager: Failed-over master needs to process 9923 regions in transition
2011-06-27 16:21:45,135 DEBUG org.apache.hadoop.hbase.master.LoadBalancer: Balance parameter: numRegions=20006, numServers=3, max=6669, min=6668",yuzhihong@gmail.com,jeason,Major,Closed,Fixed,01/Jul/11 09:02,20/Nov/15 11:55
Bug,HBASE-4059,12512747,"If a region is split during RS shutdown process, the daughter regions are NOT made online by master","When a region is splitted during the RS shutdown process, RS just written the daughter region infos to META, but not make them online. Then, for master, in its ServerShutdownHandler, the function isDaughterMissing() uses FindDaughterVisitor to check whether daughter region is OK. However, this visitor doesn't check whether the value for HConstants.SERVER_QUALIFIER carries non-null value.

Therefore for the scenario, isDaughterMissing() returns false, skipping the following line:
     assignmentManager.assign(daughter, true);",yuzhihong@gmail.com,whjiang,Major,Closed,Fixed,05/Jul/11 03:24,12/Jun/22 19:03
Bug,HBASE-4061,12512832,getTableDirs is missing directories to skip,"The getTableDirs() is missing extra checks:

{code}
  public static List<Path> getTableDirs(final FileSystem fs, final Path rootdir)
  throws IOException {
    // presumes any directory under hbase.rootdir is a table
    FileStatus [] dirs = fs.listStatus(rootdir, new DirFilter(fs));
    List<Path> tabledirs = new ArrayList<Path>(dirs.length);
    for (FileStatus dir: dirs) {
      Path p = dir.getPath();
      String tableName = p.getName();
      if (tableName.equals(HConstants.HREGION_LOGDIR_NAME) ||
          tableName.equals(Bytes.toString(HConstants.ROOT_TABLE_NAME)) ||
          tableName.equals(Bytes.toString(HConstants.META_TABLE_NAME)) ||
          tableName.equals(HConstants.HREGION_OLDLOGDIR_NAME) ) {
        continue;
      }
      tabledirs.add(p);
    }
    return tabledirs;
  }
{code}

It needs to also skip 
* .tmp
* .corrupt
* splitlog

A broader check should be performed to make sure it is all covered.

The missing .corrupt check causes for example:

{noformat}
2011-07-05 11:34:33,364 WARN org.apache.hadoop.hbase.master.HMaster: Failed getting all descriptors
java.io.FileNotFoundException: No status for hdfs://localhost:8020/hbase/.corrupt
        at org.apache.hadoop.hbase.util.FSUtils.getTableInfoModtime(FSUtils.java:888)
        at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:122)
        at org.apache.hadoop.hbase.util.FSTableDescriptors.getAll(FSTableDescriptors.java:149)
        at org.apache.hadoop.hbase.master.HMaster.getHTableDescriptors(HMaster.java:1429)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:312)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1065)
{noformat}

Not sure yet why others do not have this issue, could be me being on trunk and fiddling?",yuzhihong@gmail.com,larsgeorge,Major,Closed,Fixed,05/Jul/11 09:43,20/Nov/15 11:54
Bug,HBASE-4065,12512929,TableOutputFormat ignores failure to create table instance,"If TableOutputFormat in the new API fails to create a table, it simply logs this at ERROR level and then continues on its way. Then, the first write() to the table will throw a NPE since table hasn't been set.

Instead, it should probably rethrow the exception as a RuntimeException in setConf, or do what the old-API TOF does and not create the HTable instance until getRecordWriter, where it can throw an IOE.",brocknoland,tlipcon,Major,Closed,Fixed,06/Jul/11 04:56,12/Oct/12 05:35
Bug,HBASE-4072,12513077,Deprecate/disable and remove support for reading ZooKeeper zoo.cfg files from the classpath,"This issue was found by Lars: http://search-hadoop.com/m/n04sthNcji2/zoo.cfg+vs+hbase-site.xml&subj=Re+zoo+cfg+vs+hbase+site+xml

Lets fix the inconsistency found and fix the places where we use non-zk attribute name for a zk attribute in hbase (There's only a few places that I remember -- maximum client connections is one IIRC)",qwertymaniac,stack,Major,Closed,Fixed,07/Jul/11 04:03,06/Apr/18 04:34
Bug,HBASE-4074,12513093,"When a RS has hostname with uppercase letter, there are two RS entries in master","When a RS has uppercase letter in its hostname, e.g. Harpertown08-15.sh.intel.com. Then, there will be two RS entries in master report, they are
 harpertown08-15.sh.intel.com
 Harpertown08-15.sh.intel.com

This leads to wrong region allocation.

This problem is caused by the implementation of java.net.InetSocketAddress.

The logic is:
1. RS Harpertown08-15.sh.intel.com sends its DNS resolved hostname harpertown08-15.sh.intel.com to master for registration in HRegionServer.reportForDuty().
2. Master handles it and returns harpertown08-15.sh.intel.com via HServerAddress object to RS to notify it this new name (actually the same as the one reported by RS).
3. HServerAddress deserialize this object by first read out hostname as string (""harpertown08-15.sh.intel.com"") and construct a InetSocketAddress object.
4. RS get the new name by call this InetSocketAddress.getHostName() method which returns Harpertown08-15.sh.intel.com instead.
5. In latter hearbeat communication (HRegionServer.tryRegionServerReport()), RS uses this new name (Harpertown08-15.sh.intel.com) to report to master and master regard it as a new RS. Thus, two RS entries exist. ",whjiang,whjiang,Major,Closed,Fixed,07/Jul/11 07:46,20/Nov/15 11:55
Bug,HBASE-4075,12513114,A bug in TestZKBasedOpenCloseRegion,"The following two test cases was executed orderly:

1.TestZKBasedOpenCloseRegion#testCloseRegion()
2.TestZKBasedOpenCloseRegion#testRSAlreadyProcessingRegion()

I found the problem while the two test cases used the same region with little possibility. The following describtion called the region as RegionA

(1). Region-A was closed in the test of ""testCloseRegion"". And it was trying to be opening at the end of ""testCloseRegion"".
{noformat}
   2011-06-20 08:14:24,967 DEBUG [main-EventThread] master.AssignmentManager(374): Handling transition=RS_ZK_REGION_OPENING, server=linux1.site,41784,1308528851644, region=5251635727486eb97dfbe6f953c587c3
   2011-06-20 08:14:24,967 DEBUG [RS_OPEN_REGION-linux1.site,41784,1308528851644-2] regionserver.HRegion(311): Instantiated TestZKBasedOpenCloseRegion,ccc,1308528860373.5251635727486eb97dfbe6f953c587c3.
   2011-06-20 08:14:24,986 INFO  [Thread-397] master.TestZKBasedOpenCloseRegion(213): Done with testCloseRegion
{noformat}   
(2). The region next test case used was just the same region which was opening:
{noformat}
   2011-06-20 08:14:25,012 INFO  [main] master.TestZKBasedOpenCloseRegion(139): .META.,,1.1028785192
   2011-06-20 08:14:25,013 INFO  [main] master.TestZKBasedOpenCloseRegion(139): TestZKBasedOpenCloseRegion,ccc,1308528860373.5251635727486eb97dfbe6f953c587c3.
{noformat}
(3) In test case 2, the code of ""while (!reopenEventProcessed.get())"" got an un-expect OPENED event from the prev opening.
{noformat}
    EventHandlerListener openListener =
    new ReopenEventListener(hri.getRegionNameAsString(),
          reopenEventProcessed, EventType.RS_ZK_REGION_OPENED);
    cluster.getMaster().executorService.
      registerListener(EventType.RS_ZK_REGION_OPENED, openListener);

    // now ask the master to move the region to hr1, will fail
    TEST_UTIL.getHBaseAdmin().move(hri.getEncodedNameAsBytes(),
        Bytes.toBytes(hr1.getServerName()));
    
    while (!reopenEventProcessed.get()) {
      Threads.sleep(100);
    }
{noformat}	
{noformat}
   2011-06-20 08:14:25,032 DEBUG [MASTER_OPEN_REGION-linux1.site:34061-4] handler.OpenedRegionHandler(108): Opened region TestZKBasedOpenCloseRegion,ccc,1308528860373.5251635727486eb97dfbe6f953c587c3. on linux1.site,41784,1308528851644
   2011-06-20 08:14:25,033 INFO  [MASTER_OPEN_REGION-linux1.site:34061-4] master.TestZKBasedOpenCloseRegion$ReopenEventListener(170): afterProcess(org.apache.hadoop.hbase.master.handler.OpenedRegionHandler@711185e7)
   2011-06-20 08:14:25,033 INFO  [MASTER_OPEN_REGION-linux1.site:34061-4] master.TestZKBasedOpenCloseRegion$ReopenEventListener(172): Finished processing RS_ZK_REGION_OPENED
{noformat}
   So it think the region was opened, but really not.
  
(4) So the test failed.
   {noformat}
   java.lang.reflect.UndeclaredThrowableException
	   at $Proxy24.move(Unknown Source)
	   at org.apache.hadoop.hbase.client.HBaseAdmin.move(HBaseAdmin.java:978)
	   at org.apache.hadoop.hbase.master.TestZKBasedOpenCloseRegion.testRSAlreadyProcessingRegion(TestZKBasedOpenCloseRegion.java:291)
	   
	Caused by: org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.UnknownRegionException: 5251635727486eb97dfbe6f953c587c3
	   at org.apache.hadoop.hbase.master.HMaster.move(HMaster.java:725)
	{noformat}
Exchange the position of the two test cases will solve the problem.	",jeason,jeason,Minor,Closed,Fixed,07/Jul/11 09:44,20/Nov/15 11:55
Bug,HBASE-4077,12513170,Deadlock if WrongRegionException is thrown from getLock in HRegion.delete,"In the HRegion.delete function, If getLock throws a WrongRegionException, no lock id is ever returned, yet in the finally block, it tries to release the row lock using that lock id (which is null). This causes an NPE in the finally clause, and the closeRegionOperation() to never execute, keeping a read lock open forever.

ERROR org.apache.hadoop.hbase.regionserver.HRegionServer: 
java.lang.NullPointerException 
at org.apache.hadoop.hbase.util.Bytes.compareTo(Bytes.java:840) 
at org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator.compare(Bytes.java:108) 
at org.apache.hadoop.hbase.util.Bytes$ByteArrayComparator.compare(Bytes.java:100) 
at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:351) 
at java.util.TreeMap.getEntry(TreeMap.java:322) 
at java.util.TreeMap.remove(TreeMap.java:580) 
at java.util.TreeSet.remove(TreeSet.java:259) 
at org.apache.hadoop.hbase.regionserver.HRegion.releaseRowLock(HRegion.java:2145) 
at org.apache.hadoop.hbase.regionserver.HRegion.delete(HRegion.java:1174) 
at org.apache.hadoop.hbase.regionserver.HRegionServer.delete(HRegionServer.java:1914) 
at sun.reflect.GeneratedMethodAccessor22.invoke(Unknown Source) 
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
at java.lang.reflect.Method.invoke(Method.java:597) 
at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570) 
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)

When the region later attempts to close, the write lock can never be acquired, and the region remains in transition forever.",awarring,awarring,Critical,Closed,Fixed,07/Jul/11 18:23,20/Nov/15 11:55
Bug,HBASE-4078,12513182,Silent Data Offlining During HDFS Flakiness,"See HBASE-1436 .  The bug fix for this JIRA is a temporary workaround for improperly moving partially-written files from TMP into the region directory when a FS error occurs.  Unfortunately, the fix is to ignore all IO exceptions, which masks off-lining due to FS flakiness.  We need to permanently fix the problem that created HBASE-1436 & then at least have the option to not open a region during times of flakey FS.",blackpearl,nspiegelberg,Blocker,Closed,Fixed,07/Jul/11 19:50,12/Oct/12 05:34
Bug,HBASE-4081,12513315,Issues with HRegion.compactStores methods,"HRegion.java,

  byte [] compactStores(final boolean majorCompaction)
  throws IOException {
    if (majorCompaction) {
      this.triggerMajorCompaction();
    }
    return compactStores();
  }

  /**
   * Compact all the stores and return the split key of the first store that needs
   * to be split.
   */
  public byte[] compactStores() throws IOException {
    for(Store s : getStores().values()) {
      CompactionRequest cr = s.requestCompaction();
      if(cr != null) {
        try {
          compact(cr);
        } finally {
          s.finishRequest(cr);
        }
      }
      byte[] splitRow = s.checkSplit();
      if (splitRow != null) {
        return splitRow;
      }
    }
    return null;
  }

1. It seems the second method's intention is to compact all the stores. However, if a store requires split, the process will stop.
2. Only MetaUtils, HRegion.merge, HRegion.processTable use these two methods. No caller uses the return value.
3. HRegion.merge expects major compaction for each store after the call and has code like below to check error condition.

      // Because we compacted the source regions we should have no more than two
      // HStoreFiles per family and there will be no reference store
      if (srcFiles.size() == 2)


So it seems like the fixes are: a) take out s.CheckSplit() call inside compactStores. b) make the return type ""void"" for these two compactStores functions.",mingma,mingma,Major,Closed,Fixed,08/Jul/11 23:35,20/Nov/15 11:55
Bug,HBASE-4083,12513673,"If Enable table is not completed and is partial, then scanning of the table is not working ","Consider the following scenario
Start the Master, Backup master and RegionServer.
Create a table which in turn creates a region.
Disable the table.
Enable the table again. 
Kill the Active master exactly at the point before the actual region assignment is started.
Restart or switch master.
Scan the table.
NotServingRegionExcepiton is thrown.
",ram_krish,ram_krish,Major,Closed,Fixed,11/Jul/11 13:25,12/Oct/12 05:35
Bug,HBASE-4084,12513708,Auto-Split runs only if there are many store files per region,"Currently, MemStoreFlusher.flushRegion() is the driver of auto-splitting. It only decides to auto-split a region if there are too many store files per region. Since it's not guaranteed that the number of store files per region always grows above the ""too many"" count before compaction reduces the count, there is no guarantee that auto-split will ever happen. In my test setup, compaction seems to always win the race and I haven't noticed auto-splitting happen once.

It appears that the intention is to have split be mutually exclusive with compaction, and to have flushing be mutually exclusive with regions badly in need of compaction, but that resulted in auto-splitting being nested in a too-restrictive spot.

I'm not sure what the right fix is. Having one method that is essentially requestSplitOrCompact would probably help readability, and could be the ultimate solution if it replaces other calls of requestCompaction().",,heitmann,Major,Closed,Fixed,11/Jul/11 19:04,12/Jun/22 18:51
Bug,HBASE-4086,12513778,documentation/javadoc error in SingleColumnValueFilter constructor,"The behaviour when the column is not found is documented differently in the constructor and in the setter. The constructor is actually wrong: by default, when the column is not found, the row is emitted (may be the opposite would be better, but it's another question)

SingleColumnValueFilter
public SingleColumnValueFilter(byte[] family,
                               byte[] qualifier,
                               CompareFilter.CompareOp compareOp,
                               byte[] value)

Constructor for binary compare of the value of a single column. If the column is found and the condition passes, all columns of the row will be emitted. If the column is not found or the condition fails, the row will not be emitted.


setFilterIfMissing
public void setFilterIfMissing(boolean filterIfMissing)
Set whether entire row should be filtered if column is not found.
If true, the entire row will be skipped if the column is not found.
If false, the row will pass if the column is not found. This is default. 



Possible correction for the constructor documentation:
If the column is found and the condition passes, all columns of the row will be emitted. If the condition fails, the row will not be emitted. The behavior when the column is not found is defined by setFilterIfMissing.
",dmeil,nkeywal,Trivial,Closed,Fixed,12/Jul/11 11:11,12/Jun/22 18:51
Bug,HBASE-4087,12513814,HBaseAdmin should perform validation of connection it holds,"Through HBASE-3777, HConnectionManager reuses the connection to HBase servers.
One challenge, discovered in troubleshooting HBASE-4052, is how we invalidate connection(s) to server which gets restarted.
There're at least two ways.
1. HConnectionManager utilizes background thread(s) to periodically perform validation of connections in HBASE_INSTANCES and remove stale connection(s).
2. Allow HBaseClient (including HBaseAdmin) to provide feedback to HConnectionManager.

The solution can be a combination of both of the above.",yuzhihong@gmail.com,yuzhihong@gmail.com,Critical,Closed,Fixed,12/Jul/11 16:10,20/Nov/15 11:54
Bug,HBASE-4088,12513825,npes in server shutdown,"2011-07-11 10:26:01,268 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:145)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
2011-07-11 10:26:01,268 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Splitting logs for sv2borg164,60020,1309996983328
2011-07-11 10:26:01,269 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN
java.lang.NullPointerException
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:145)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)",stack,stack,Critical,Closed,Fixed,12/Jul/11 17:41,20/Nov/15 11:53
Bug,HBASE-4093,12514007,"When verifyAndAssignRoot throws exception, the deadServers state cannot be changed","When verifyAndAssignRoot throw exception, The deadServers state can not be changed.
The Hmaster log has a lot of 'Not running balancer because processing dead regionserver(s): []' information.


HMaster log:
2011-07-09 01:38:31,820 INFO org.apache.hadoop.hbase.regionserver.wal.HLogSplitter: Closed path hdfs://162.2.16.6:9000/hbase/Htable_UFDR_035/fe7e51c0a74fac096cea8cdb3c9497a6/recovered.edits/0000000000204525422 (wrote 8 edits in 61583ms)
2011-07-09 01:38:31,836 ERROR org.apache.hadoop.hbase.master.MasterFileSystem: Failed splitting hdfs://162.2.16.6:9000/hbase/.logs/162-2-6-187,20020,1310107719056
java.io.IOException: hdfs://162.2.16.6:9000/hbase/.logs/162-2-6-187,20020,1310107719056/162-2-6-187%3A20020.1310143885352, entryStart=1878997244, pos=1879048192, end=2003890606, edit=80274
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.addFileInfoToException(SequenceFileLogReader.java:244)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:200)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:172)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.parseHLog(HLogSplitter.java:429)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:262)
	at org.apache.hadoop.hbase.regionserver.wal.HLogSplitter.splitLog(HLogSplitter.java:188)
	at org.apache.hadoop.hbase.master.MasterFileSystem.splitLog(MasterFileSystem.java:201)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:114)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Could not obtain block: blk_1310107715558_225636 file=/hbase/.logs/162-2-6-187,20020,1310107719056/162-2-6-187%3A20020.1310143885352
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.chooseDataNode(DFSClient.java:2491)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.blockSeekTo(DFSClient.java:2256)
	at org.apache.hadoop.hdfs.DFSClient$DFSInputStream.read(DFSClient.java:2441)
	at java.io.DataInputStream.read(DataInputStream.java:132)
	at java.io.DataInputStream.readFully(DataInputStream.java:178)
	at org.apache.hadoop.io.DataOutputBuffer$Buffer.write(DataOutputBuffer.java:63)
	at org.apache.hadoop.io.DataOutputBuffer.write(DataOutputBuffer.java:101)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1984)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1884)
	at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:1930)
	at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.next(SequenceFileLogReader.java:198)
	... 10 more
2011-07-09 01:38:33,052 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [162-2-6-187,20020,1310107719056]
2011-07-09 01:39:29,946 WARN org.apache.hadoop.hbase.master.CatalogJanitor: Failed scan of catalog table
java.net.SocketTimeoutException: Call to /162.2.6.187:20020 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/162.2.6.187:38721 remote=/162.2.6.187:20020]
	at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:802)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:775)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
	at $Proxy6.getRegionInfo(Unknown Source)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRegionLocation(CatalogTracker.java:424)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:272)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:331)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:364)
	at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:255)
	at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:237)
	at org.apache.hadoop.hbase.master.CatalogJanitor.scan(CatalogJanitor.java:116)
	at org.apache.hadoop.hbase.master.CatalogJanitor.chore(CatalogJanitor.java:85)
	at org.apache.hadoop.hbase.Chore.run(Chore.java:66)
Caused by: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/162.2.6.187:38721 remote=/162.2.6.187:20020]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:165)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.FilterInputStream.read(FilterInputStream.java:116)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection$PingInputStream.read(HBaseClient.java:299)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
	at java.io.DataInputStream.readInt(DataInputStream.java:370)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:539)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:477)
2011-07-09 01:39:29,946 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_META_SERVER_SHUTDOWN
java.net.SocketTimeoutException: Call to /162.2.6.187:20020 failed on socket timeout exception: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/162.2.6.187:38721 remote=/162.2.6.187:20020]
	at org.apache.hadoop.hbase.ipc.HBaseClient.wrapException(HBaseClient.java:802)
	at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:775)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
	at $Proxy6.getRegionInfo(Unknown Source)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRegionLocation(CatalogTracker.java:424)
	at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyRootRegionLocation(CatalogTracker.java:471)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.verifyAndAssignRoot(ServerShutdownHandler.java:90)
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:126)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.net.SocketTimeoutException: 60000 millis timeout while waiting for channel to be ready for read. ch : java.nio.channels.SocketChannel[connected local=/162.2.6.187:38721 remote=/162.2.6.187:20020]
	at org.apache.hadoop.net.SocketIOWithTimeout.doIO(SocketIOWithTimeout.java:165)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:155)
	at org.apache.hadoop.net.SocketInputStream.read(SocketInputStream.java:128)
	at java.io.FilterInputStream.read(FilterInputStream.java:116)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection$PingInputStream.read(HBaseClient.java:299)
	at java.io.BufferedInputStream.fill(BufferedInputStream.java:218)
	at java.io.BufferedInputStream.read(BufferedInputStream.java:237)
	at java.io.DataInputStream.readInt(DataInputStream.java:370)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:539)
	at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:477)
2011-07-09 01:40:26,474 DEBUG org.apache.hadoop.hbase.master.ServerManager: Server 162-2-6-187,20020,1310146825674 came back up, removed it from the dead servers list
2011-07-09 01:40:26,515 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=162-2-6-187,20020,1310146825674, regionCount=0, userLoad=false
2011-07-09 01:40:28,410 INFO org.apache.hadoop.hbase.catalog.CatalogTracker: Failed verification of .META.,,1 at address=162-2-6-187:20020; org.apache.hadoop.hbase.NotServingRegionException: org.apache.hadoop.hbase.NotServingRegionException: Region is not online: .META.,,1
...
2011-07-09 01:53:33,052 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): []
2011-07-09 01:58:33,060 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): []
2011-07-09 02:03:33,061 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): []
2011-07-09 02:08:33,061 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): []",fulin,fulin,Major,Closed,Fixed,14/Jul/11 03:22,12/Jun/22 18:51
Bug,HBASE-4095,12514051,Hlog may not be rolled in a long time if checkLowReplication's request of LogRoll is blocked,"Some large Hlog files(Larger than 10G) appeared in our environment, and I got the reason why they got so huge:

1. The replicas is less than the expect number. So the method of checkLowReplication will be called each sync.

2. The method checkLowReplication request log-roll first, and set logRollRequested as true: 

{noformat}
private void checkLowReplication() {
// if the number of replicas in HDFS has fallen below the initial
// value, then roll logs.
try {
  int numCurrentReplicas = getLogReplication();
  if (numCurrentReplicas != 0 &&
	  numCurrentReplicas < this.initialReplication) {
	LOG.warn(""HDFS pipeline error detected. "" +
		""Found "" + numCurrentReplicas + "" replicas but expecting "" +
		this.initialReplication + "" replicas. "" +
		"" Requesting close of hlog."");
	requestLogRoll();
	logRollRequested = true;
  }
} catch (Exception e) {
  LOG.warn(""Unable to invoke DFSOutputStream.getNumCurrentReplicas"" + e +
	  "" still proceeding ahead..."");
}
}
{noformat}
3.requestLogRoll() just commit the roll request. It may not execute in time, for it must got the un-fair lock of cacheFlushLock.
But the lock may be carried by the cacheflush threads.

4.logRollRequested was true until the log-roll executed. So during the time, each request of log-roll in sync() was skipped.

Here's the logs while the problem happened(Please notice the file size of hlog ""193-195-5-111%3A20020.1309937386639"" in the last row):

2011-07-06 15:28:59,284 WARN org.apache.hadoop.hbase.regionserver.wal.HLog: HDFS pipeline error detected. Found 2 replicas but expecting 3 replicas.  Requesting close of hlog.
2011-07-06 15:29:46,714 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309937339119, entries=32434, filesize=239589754. New hlog /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309937386639
2011-07-06 15:29:56,929 WARN org.apache.hadoop.hbase.regionserver.wal.HLog: HDFS pipeline error detected. Found 2 replicas but expecting 3 replicas.  Requesting close of hlog.
2011-07-06 15:29:56,933 INFO org.apache.hadoop.hbase.regionserver.Store: Renaming flushed file at hdfs://193.195.5.112:9000/hbase/Htable_UFDR_034/a3780cf0c909d8cf8f8ed618b290cc95/.tmp/4656903854447026847 to hdfs://193.195.5.112:9000/hbase/Htable_UFDR_034/a3780cf0c909d8cf8f8ed618b290cc95/value/8603005630220380983
2011-07-06 15:29:57,391 INFO org.apache.hadoop.hbase.regionserver.Store: Added hdfs://193.195.5.112:9000/hbase/Htable_UFDR_034/a3780cf0c909d8cf8f8ed618b290cc95/value/8603005630220380983, entries=445880, sequenceid=248900, memsize=207.5m, filesize=130.1m
2011-07-06 15:29:57,478 INFO org.apache.hadoop.hbase.regionserver.HRegion: Finished memstore flush of ~207.5m for region Htable_UFDR_034,07664,1309936974158.a3780cf0c909d8cf8f8ed618b290cc95. in 10839ms, sequenceid=248900, compaction requested=false
2011-07-06 15:28:59,236 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309926531955, entries=216459, filesize=2370387468. New hlog /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309937339119
2011-07-06 15:29:46,714 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309937339119, entries=32434, filesize=239589754. New hlog /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309937386639
2011-07-06 16:29:58,775 DEBUG org.apache.hadoop.hbase.regionserver.LogRoller: Hlog roll period 3600000ms elapsed
2011-07-06 16:29:58,775 DEBUG org.apache.hadoop.hbase.regionserver.LogRoller: Hlog roll period 3600000ms elapsed
2011-07-06 16:30:01,978 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Roll /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309937386639, entries=1135576, filesize=19220372830. New hlog /hbase/.logs/193-195-5-111,20020,1309922880081/193-195-5-111%3A20020.1309940998890

",jeason,jeason,Major,Closed,Fixed,14/Jul/11 11:55,20/Nov/15 11:55
Bug,HBASE-4101,12514128,Regionserver Deadlock,"We periodically see a situation where the regionserver process exists in the process list, zookeeper thread sends the keepalive so the master won't remove it from the active list, yet the regionserver will not serve data.

Hadoop(cdh3u0), HBase 0.90.3 (Apache version), under load from an internal testing tool.


Attached is the full JStack",ram_krish,mattdavies,Blocker,Closed,Fixed,14/Jul/11 21:53,20/Nov/15 11:55
Bug,HBASE-4105,12514163,Stargate does not support Content-Type: application/json and Content-Encoding: gzip in parallel,"When:
curl -H ""Accept: application/json"" http://localhost:3000/version -v

Response is:

About to connect() to localhost port 3000 (#0)
Trying 127.0.0.1... connected
Connected to localhost (127.0.0.1) port 3000 (#0)
> GET /version HTTP/1.1
> User-Agent: curl/7.19.7 (universal-apple-darwin10.0) libcurl/7.19.7 OpenSSL/0.9.8r zlib/1.2.3
> Host: localhost:3000
> Accept: application/json
> 
< HTTP/1.1 200 OK
< Cache-Control: no-cache
< Content-Type: application/json
< Transfer-Encoding: chunked
<
Connection #0 to host localhost left intact
Closing connection #0 {""Server"":""jetty/6.1.26"",""REST"":""0.0.2"",""OS"":""Linux 2.6.32-bpo.5-amd64 amd64"",""Jersey"":""1.4"",""JVM"":""Sun Microsystems Inc. 1.6.0_22-17.1-b03""}

but with compression:
curl -H ""Accept: application/json"" http://localhost:3000/version -v --compressed

Reponse is:

About to connect() to localhost port 3000 (#0)
Trying 127.0.0.1 ... connected
Connected to localhost (127.0.0.1) port 3000 (#0)
> GET /version HTTP/1.1
> User-Agent: curl/7.19.7 (universal-apple-darwin10.0) libcurl/7.19.7 OpenSSL/0.9.8r zlib/1.2.3
> Host: localhost:3000
> Accept-Encoding: deflate, gzip
> Accept: application/json
> 
< HTTP/1.1 200 OK
< Cache-Control: no-cache
< Content-Type: application/json
< Content-Encoding: gzip
< Transfer-Encoding: chunked
<
Connection #0 to host localhost left intact
Closing connection #0

and the stargate server throws the following exception:

11/07/14 11:21:44 ERROR mortbay.log: /version
java.lang.ClassCastException: org.mortbay.jetty.HttpConnection$Output cannot be cast to org.apache.hadoop.hbase.rest.filter.GZIPResponseStream
at org.apache.hadoop.hbase.rest.filter.GzipFilter.doFilter(GzipFilter.java:54)
at org.mortbay.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1212)
at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:399)
at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
at org.mortbay.jetty.Server.handle(Server.java:326)
at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)

This is not reproduceable with content type text/plain and gzip.

This is somehow related to https://issues.apache.org/jira/browse/HBASE-3275",apurtell,jpkoenig,Major,Closed,Fixed,15/Jul/11 06:01,12/Oct/12 05:35
Bug,HBASE-4109,12514285,"Hostname returned via reverse dns lookup contains trailing period if configured interface is not ""default""","If you are using an interface anything other than 'default' (literally that keyword) DNS.java 's getDefaultHost will return a string which will 
have a trailing period at the end. It seems javadoc of reverseDns in DNS.java (see below) is conflicting with what that function is actually doing. 
It is returning a PTR record while claims it returns a hostname. The PTR record always has period at the end , RFC:  http://irbs.net/bog-4.9.5/bog47.html 

We make call to DNS.getDefaultHost at more than one places and treat that as actual hostname.

Quoting HRegionServer for example
{code}
String machineName = DNS.getDefaultHost(conf.get(
        ""hbase.regionserver.dns.interface"", ""default""), conf.get(
        ""hbase.regionserver.dns.nameserver"", ""default""));
{code}

This causes inconsistencies. An example of such inconsistency was observed while debugging the issue ""Regions not getting reassigned if RS is brought down"". More here 
http://search-hadoop.com/m/CANUA1qRCkQ1 

We may want to sanitize the string returned from DNS class. Or better we can take a path of overhauling the way we do DNS name matching all over.
",shrijeet,shrijeet,Major,Closed,Fixed,15/Jul/11 21:38,20/Nov/15 11:55
Bug,HBASE-4112,12514393,Creating table may throw NullPointerException," It happened in latest branch 0.90. but I can't reproduce it.
>
> It seems using api getHRegionInfoOrNull is better or check the input parameter before call getHRegionInfo.
>
> Code:
>  public static Writable getWritable(final byte [] bytes, final 
> Writable w)
>  throws IOException {
>    return getWritable(bytes, 0, bytes.length, w);
>  }
> return getWritable(bytes, 0, bytes.length, w);  // It seems input 
> parameter bytes is null
>
> logs:
> 11/07/15 10:15:42 INFO zookeeper.ClientCnxn: Socket connection 
> established to C4C3.site/157.5.100.3:2181, initiating session
> 11/07/15 10:15:42 INFO zookeeper.ClientCnxn: Session establishment 
> complete on server C4C3.site/157.5.100.3:2181, sessionid = 0x2312b8e3f700002, negotiated timeout = 180000 [INFO] Create : ufdr111 222!
> [INFO] Create : ufdr111 start!
> java.lang.NullPointerException
>        at 
> org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:75)
>        at 
> org.apache.hadoop.hbase.util.Writables.getHRegionInfo(Writables.java:1
> 19)
>        at 
> org.apache.hadoop.hbase.client.HBaseAdmin$1.processRow(HBaseAdmin.java
> :306)
>        at 
> org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:1
> 90)
>        at 
> org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:9
> 5)
>        at 
> org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:7
> 3)
>        at 
> org.apache.hadoop.hbase.client.HBaseAdmin.createTable(HBaseAdmin.java:
> 325)
>        at createTable.main(createTable.java:96)
>
",sunnygao,sunnygao,Major,Closed,Fixed,18/Jul/11 02:51,20/Nov/15 11:55
Bug,HBASE-4115,12514502,HBase shell assign and unassign unusable if region name includes binary-encoded data,"When using the hbase shell assign and unassign commands, we should be able to copy region names from the hbck utility or the web page hosted by the HMaster process.  But if these names have encoded binary data, they region name won't match and the command will fail.

This is easily fixed by using Bytes.toBytesBinary on the region name in these commands rather than the raw Bytes.ToBytes.",,rbrush,Minor,Closed,Fixed,18/Jul/11 20:42,20/Nov/15 11:53
Bug,HBASE-4116,12514520,[stargate] StringIndexOutOfBoundsException in row spec parse,"From user@hbase, Allan Yan writes:

There might be a bug for REST web service to get rows with given startRow and endRow.

For example, to get a list of rows with startRow=testrow1, endRow=testrow2, I send GET request:

curl http://localhost:8123/TestRowResource/testrow1,testrow2/a:1

And got StringIndexOutOfBoundsException.

This was because in the RowSpec.java, parseRowKeys method, startRow value was changed:

{code}

       startRow = sb.toString();
       int idx = startRow.indexOf(',');
       if (idx != -1) {
         startRow = URLDecoder.decode(startRow.substring(0, idx),
           HConstants.UTF8_ENCODING);
         endRow = URLDecoder.decode(startRow.substring(idx + 1),
           HConstants.UTF8_ENCODING);
       } else {
         startRow = URLDecoder.decode(startRow, HConstants.UTF8_ENCODING);
       }
{code}

 After change to this, it works:

{code}
       String row = sb.toString();
       int idx = row.indexOf(',');
       if (idx != -1) {
         startRow = URLDecoder.decode(row.substring(0, idx),
           HConstants.UTF8_ENCODING);
         endRow = URLDecoder.decode(row.substring(idx + 1),
           HConstants.UTF8_ENCODING);
       } else {
         startRow = URLDecoder.decode(row, HConstants.UTF8_ENCODING);
       }
{code}

I've also created a unit test method in TestRowResource.java,

{code}

   @Test
   public void testStartEndRowGetPutXML() throws IOException, JAXBException {
     String[] rows = {ROW_1,ROW_2,ROW_3};
     String[] values = {VALUE_1,VALUE_2,VALUE_3}; 
     Response response = null;
     for(int i=0; i<rows.length; i++){
         response = putValueXML(TABLE, rows[i], COLUMN_1, values[i]);
         assertEquals(200, response.getCode());
         checkValueXML(TABLE, rows[i], COLUMN_1, values[i]);
     }

     response = getValueXML(TABLE, rows[0], rows[2], COLUMN_1);
     assertEquals(200, response.getCode());
     CellSetModel cellSet = (CellSetModel)
       unmarshaller.unmarshal(new ByteArrayInputStream(response.getBody()));
     assertEquals(2, cellSet.getRows().size());
     for(int i=0; i<cellSet.getRows().size()-1; i++){
         RowModel rowModel = cellSet.getRows().get(i);
         for(CellModel cell : rowModel.getCells()){
             assertEquals(COLUMN_1, Bytes.toString(cell.getColumn()));
             assertEquals(values[i], Bytes.toString(cell.getValue()));
         }   
     }
    
     for(String row : rows){
         response = deleteRow(TABLE, row);
         assertEquals(200, response.getCode());
     }
   }

   private static Response getValueXML(String table, String startRow, String
 endRow, String column)
           throws IOException {
         StringBuilder path = new StringBuilder();
         path.append('/');
         path.append(table);
         path.append('/');
         path.append(startRow);
         path.append("","");
         path.append(endRow);
         path.append('/');
         path.append(column);
         return getValueXML(path.toString());
   }
{code}
",,apurtell,Major,Closed,Fixed,18/Jul/11 23:31,12/Oct/12 05:35
Bug,HBASE-4118,12514797,method regionserver.MemStore#updateColumnValue: the check for qualifier and family is missing,"{code}
           [...]
            while (it.hasNext()) {
                KeyValue kv = it.next();

                // if this isnt the row we are interested in, then bail:
                if (!firstKv.matchingColumn(family, qualifier) || !firstKv.matchingRow(kv)) {
                    break; // rows dont match, bail.
                }

                [...]
            }
{code}
should be replaced by:
{code}
                // if this isnt the row we are interested in, then bail:
                if (!kv.matchingColumn(family, qualifier) || !firstKv.matchingRow(kv)) {
                    break; // rows dont match, bail.
                }
{code}",nkeywal,nkeywal,Minor,Closed,Fixed,20/Jul/11 15:17,20/Nov/15 11:54
Bug,HBASE-4124,12515012,"ZK restarted while a region is being assigned, new active HM re-assigns it but the RS warns 'already online on this server'.","ZK restarted while assigning a region, new active HM re-assign it but the RS warned 'already online on this server'.

Issue:
The RS failed besause of 'already online on this server' and return; The HM can not receive the message and report 'Regions in transition timed out'.
",sunnygao,fulin,Major,Closed,Fixed,22/Jul/11 10:07,20/Nov/15 11:52
Bug,HBASE-4127,12515065,HBaseAdmin : Don't modify table's name away,"One of the developers was using the default constructor for HTableDescriptor, which is sadly a bad constructor that should never be used. It made the tablename empty in META & caused an ERROR cycle as region onlining kept failing. We should have never let this happen. Don't do table modifications if the HTableDescriptor name doesn't match the table name passed in.",nspiegelberg,nspiegelberg,Blocker,Closed,Fixed,22/Jul/11 20:47,20/Nov/15 11:54
Bug,HBASE-4129,12515151,hbase-3872 added a warn message 'CatalogJanitor: Daughter regiondir does not exist' that is triggered though its often legit that daughter is not present,"If a daughter region is split before the catalog janitor runs, we'll see:

{code}
2011-07-22 16:10:26,398 WARN org.apache.hadoop.hbase.master.CatalogJanitor: Daughter regiondir does not exist: hdfs://sv4borg227:10000/hbase/TestTable/a1023b2b00fe44c86bd8ae3633f531fa
{code}

Its legit that the daughter region does not exist in this case (it was just cleaned up by the catalogjanitor).",stack,stack,Major,Closed,Fixed,23/Jul/11 04:53,20/Nov/15 11:55
Bug,HBASE-4138,12515226,If zookeeper.znode.parent is not specifed explicitly in Client code then HTable object loops continuously waiting for the root region by using /hbase as the base node.,"Change the zookeeper.znode.parent property (default is /hbase).
Now do not specify this change in the client code.

Use the HTable Object.
The HTable is not able to find the root region and keeps continuously looping.

Find the stack trace:
====================
Object.wait(long) line: not available [native method]		 
RootRegionTracker(ZooKeeperNodeTracker).blockUntilAvailable(long) line: 122

RootRegionTracker.waitRootRegionLocation(long) line: 73		 
HConnectionManager$HConnectionImplementation.locateRegion(byte[],
byte[], boolean) line: 578
HConnectionManager$HConnectionImplementation.locateRegion(byte[],
byte[]) line: 558
HConnectionManager$HConnectionImplementation.locateRegionInMeta(byte[],
byte[], byte[], boolean, Object) line: 687
HConnectionManager$HConnectionImplementation.locateRegion(byte[],
byte[], boolean) line: 589
HConnectionManager$HConnectionImplementation.locateRegion(byte[],
byte[]) line: 558
HConnectionManager$HConnectionImplementation.locateRegionInMeta(byte[],
byte[], byte[], boolean, Object) line: 687
HConnectionManager$HConnectionImplementation.locateRegion(byte[],
byte[], boolean) line: 593
HConnectionManager$HConnectionImplementation.locateRegion(byte[],
byte[]) line: 558
HTable.<init>(Configuration, byte[]) line: 171		 
HTable.<init>(Configuration, String) line: 145		 
HBaseTest.test() line: 45",ram_krish,ram_krish,Major,Closed,Fixed,25/Jul/11 09:02,20/Nov/15 11:56
Bug,HBASE-4139,12515286,[stargate] Update ScannerModel with support for filter package additions,Filters have been added to the o.a.h.h.filters package without updating o.a.h.h.rest.model.ScannerModel. Bring ScannerModel up to date.,apurtell,apurtell,Major,Closed,Fixed,25/Jul/11 16:58,20/Nov/15 11:52
Bug,HBASE-4144,12515540, RS does not abort if the initialization of RS fails," If any exception occurs while initialization of RS the RS doesnot get
 aborted whereas only the RPC server gets stopped.
  private void preRegistrationInitialization()
  throws IOException, InterruptedException {
    try {
      initializeZooKeeper();
      initializeThreads();
      int nbBlocks = conf.getInt(""hbase.regionserver.nbreservationblocks"",4);
      for (int i = 0; i < nbBlocks; i++) {
        reservedSpace.add(new
 byte[HConstants.DEFAULT_SIZE_RESERVATION_BLOCK]);
      }
    } catch (Throwable t) {
      // Call stop if error or process will stick around for ever since
 server
      // puts up non-daemon threads.
      LOG.error(""Stopping HRS because failed initialize"", t);
      this.rpcServer.stop();
    }
  }

 So if any exception occurs while initilization the RPC server gets stopped
 but RS process is still running. But the log says stopping HRegionServer.

 So in the below code the catch() block will be executed when the RPCServer
 stop fails?

 In all other cases it doesnt handle any initialization failure.

    try {
      // Do pre-registration initializations; zookeeper, lease threads,tc.

      preRegistrationInitialization();
    } catch (Exception e) {
      abort(""Fatal exception during initialization"", e);
    }",ram_krish,ram_krish,Minor,Closed,Fixed,27/Jul/11 15:02,20/Nov/15 11:55
Bug,HBASE-4148,12515804,HFileOutputFormat doesn't fill in TIMERANGE_KEY metadata,"When HFiles are flushed through the normal path, they include an attribute TIMERANGE_KEY which can be used to cull HFiles when performing a time-restricted scan. Files produced by HFileOutputFormat are currently missing this metadata.",jmhsieh,tlipcon,Major,Closed,Fixed,29/Jul/11 19:59,20/Nov/15 11:55
Bug,HBASE-4149,12515824,Javadoc for Result.getRow is confusing to new users.,org.apache.hadoop.hbase.client.Result getRow is confusing to new users.  The documentation could be read to mean the raw data of the row.  In addition it is written with improper grammar.,dmeil,eclark,Trivial,Closed,Fixed,30/Jul/11 03:07,12/Jun/22 19:04
Bug,HBASE-4150,12515879,Potentially too many connections may be opened if ThreadLocalPool or RoundRobinPool is used,"See 'Problem with hbase.client.ipc.pool.type=threadlocal in trunk' discussion started by Lars George.

From Lars Hofhansl:
Looking at HBaseClient.getConnection(...) I see this:
{code}
     synchronized (connections) {
       connection = connections.get(remoteId);
       if (connection == null) {
         connection = new Connection(remoteId);
         connections.put(remoteId, connection);
       }
     }
{code}

At the same time PoolMap.ThreadLocalPool.put is defined like this:
{code}
   public R put(R resource) {
     R previousResource = get();
     if (previousResource == null) {
...
       if (poolSize.intValue() >= maxSize) {
         return null;
       }
...
   }
{code}
So... If the ThreadLocalPool reaches its capacity it always returns null and hence all new threads will create a
new connection every time getConnection is called!

I have also verified with a test program that works fine as long as the number of client threads (which include
the threads in HTable's threadpool of course) is < poolsize. Once that is no longer the case the number of
connections ""explodes"" and the program dies with OOMEs (mostly because each Connection is associated with
yet another thread).

It's not clear what should happen, though. Maybe (1) the ThreadLocalPool should not have a limit, or maybe
(2) allocations past the pool size should throw an exception (i.e. there's a hard limit), or maybe (3) in that case
a single connection is returned for all threads while the pool it over its limit or (4) we start round robin with the other
connection in the other thread locals.

For #1 means that the number of client threads needs to be more carefully managed by the client app.
In this case it would also be somewhat pointless that Connection have their own threads, we just pass stuff
between threads.
#2 would work, but puts more logic in the client.
#3 would lead to hard to debug performance issues.
And #4 is messy :)

From Ted Yu:
For HBaseClient, at least the javadoc doesn't match:
{code}
   * @param config configuration
   * @return either a {@link PoolType#Reusable} or {@link PoolType#ThreadLocal}
   */
  private static PoolType getPoolType(Configuration config) {
    return PoolType.valueOf(config.get(HConstants.HBASE_CLIENT_IPC_POOL_TYPE),
        PoolType.RoundRobin, PoolType.ThreadLocal);
{code}
I think for RoundRobinPool, we shouldn't allow maxSize to be Integer#MAX_VALUE. Otherwise connection explosion described by Lars may incur.
",karthick,yuzhihong@gmail.com,Major,Closed,Fixed,31/Jul/11 21:35,20/Nov/15 11:52
Bug,HBASE-4153,12516031,Handle RegionAlreadyInTransitionException in AssignmentManager,"Comment from Stack over in HBASE-3741:

{quote}
Question: Looking at this patch again, if we throw a RegionAlreadyInTransitionException, won't we just assign the region elsewhere though RegionAlreadyInTransitionException in at least one case here is saying that the region is already open on this regionserver?
{quote}

Indeed looking at the code it's going to be handled the same way other exceptions are. Need to add special cases for assign and unassign.",ram_krish,jdcryans,Major,Closed,Fixed,01/Aug/11 21:03,20/Nov/15 11:52
Bug,HBASE-4156,12517689,ZKConfig defaults clientPort improperly,"ZKConfig#makeZKProps() should use ""clientPort"" as the client port key in its output when defaulting instead of ""hbase.zookeeper.property.clientPort"".  This method strips the ""hbase.zookeeper.property."" prefix from all of the properties it returns, so the client port key should not have it.  The result is that the default is not properly picked up.",,cim_michajlomatijkiw,Trivial,Closed,Fixed,02/Aug/11 19:33,20/Nov/15 11:54
Bug,HBASE-4160,12517851,HBase shell move and online may be unusable if region name or server includes binary-encoded data,"Similar to HBASE-4115, this entails a conversion of org.apache.hadoop.hbase.utils.Bytes.toBytes to a to_java_bytes call in the 'move' and 'online' call.",jmhsieh,jmhsieh,Minor,Closed,Fixed,03/Aug/11 21:41,20/Nov/15 11:55
Bug,HBASE-4161,12517860,Incorrect use of listStatus() in HBase region initialization.,"While opening a region, the HBase regionserver tries to list all the children in a ""recovered.edits"" directory. This directory may not exist and depending on the version of HDFS listStatus() might return null or an exception. If it does throw an exception the entire process of opening the region is aborted, just because the recovered.edits directory is not present.",,blackpearl,Major,Closed,Fixed,03/Aug/11 22:21,20/Nov/15 11:54
Bug,HBASE-4167,12517977,Potential leak of HTable instances when using HTablePool with PoolType.ThreadLocal,"(Initially discussed in HBASE-4150)

In HTablePool, when obtaining a table:
{code}
private HTableInterface findOrCreateTable(String tableName) {
    HTableInterface table = tables.get(tableName);
    if (table == null) {
      table = createHTable(tableName);
    }
    return table;
  }
{code}

In the case of {{ThreadLocalPool}}, it seems like there's an exposure here between when the table is created initially and when {{ThreadLocalPool.put()}} is called to set the thread local variable (on {{PooledHTable.close()}}).


Potential solution described by Karthick Sankarachary:

For one thing, we might want to clear the tables variable when the {{HTablePool}} is closed (as shown below). For another, we should override ThreadLocalPool#get method so that it removes the resource, otherwise it might end up referencing a HTableInterface that's has been released.
{code}
1 diff --git a/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java b/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
      2 index 952a3aa..c198f15 100755
      3 --- a/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
      4 +++ b/src/main/java/org/apache/hadoop/hbase/client/HTablePool.java
     13 @@ -309,6 +310,7 @@ public class HTablePool implements Closeable {
     14      for (String tableName : tables.keySet()) {
     15        closeTablePool(tableName);
     16      }
     17 +    this.tables.clear();
     18    }
{code}
",karthick,ghelmling,Major,Closed,Fixed,04/Aug/11 23:04,20/Nov/15 11:54
Bug,HBASE-4168,12518049,A client continues to try and connect to a powered down regionserver,"Experiment-1

Started a dev cluster - META is on the same regionserver as my key-value. I kill the regionserver process but donot power down the machine.
The META is able to migrate to a new regionserver and the regions are also able to reopen elsewhere.
The client is able to talk to the META and find the new kv location and get it.

Experiment-2

Started a dev cluster - META is on a different regionserver as my key-value. I kill the regionserver process but donot power down the machine.
The META remains where it is and the regions are also able to reopen elsewhere.
The client is able to talk to the META and find the new kv location and get it.

Experiment-3

Started a dev cluster - META is on a different regionserver as my key-value. I power down the machine hosting this regionserver.
The META remains where it is and the regions are also able to reopen elsewhere.
The client is able to talk to the META and find the new kv location and get it.

Experiment-4 (This is the problematic one)

Started a dev cluster - META is on the same regionserver as my key-value. I power down the machine hosting this regionserver.
The META is able to migrate to a new regionserver - however - it takes a really long time (~30 minutes)
The regions on that regionserver DONOT reopen (I waited for 1 hour)
The client is able to find the new location of the META, however, the META keeps redirecting the client to powered down
regionserver as the location of the key-value it is trying to get. Thus the client's get is unsuccessful.",anirudhtodi,anirudhtodi,Critical,Closed,Fixed,05/Aug/11 17:10,20/Nov/15 11:55
Bug,HBASE-4169,12518061,FSUtils LeaseRecovery for non HDFS FileSystems.,"FSUtils.recoverFileLease uses HDFS's recoverLease method to get lease before splitting hlog file.
This might not work for other filesystem implementations. ",lohit,lohit,Major,Closed,Fixed,05/Aug/11 18:10,20/Nov/15 11:55
Bug,HBASE-4171,12518099,HBase shell broken in trunk,"The shell displays for any command entered:

ERROR: undefined method `getZooKeeper' for #<Java::OrgApacheHadoopHbaseZookeeper::ZooKeeperWatcher:0x1c904f75>
",larsh,larsh,Major,Closed,Fixed,06/Aug/11 02:10,20/Nov/15 11:53
Bug,HBASE-4175,12518135,Fix FSUtils.createTableDescriptor(),"Currently createTableDescriptor() doesn't return anything.
The caller wouldn't know whether the descriptor is created or not. See exception handling:
{code}
   } catch(IOException ioe) {
     LOG.info(""IOException while trying to create tableInfo in HDFS"", ioe);
   }
{code}
We should return a boolean.

If the table descriptor exists already, maybe we should deserialize from hdfs and compare with htableDescriptor argument. If they differ, I am not sure what the proper action would be.

Maybe we can add a boolean argument, force, to createTableDescriptor(). When force is true, existing table descriptor would be overwritten.",ram_krish,yuzhihong@gmail.com,Major,Closed,Fixed,07/Aug/11 14:04,12/Jun/22 19:01
Bug,HBASE-4177,12518226,"Handling read failures during recovery‏ - when HMaster calls Namenode recovery, recovery may be a failure leading to read failure while splitting logs","As per the mailing thread with the heading
'Handling read failures during recovery‏' we found this problem.
As part of split Logs the HMaster calls Namenode recovery.  The recovery is an asynchronous process. 
In HDFS
=======
Even though client is getting the updated block info from Namenode on first
read failure, client is discarding the new info and using the old info only
to retrieve the data from datanode. So, all the read
retries are failing. [Method parameter reassignment - Not reflected in
caller]. 
In HBASE
=======
In HMaster code we tend to wait for  1sec.  But if the recovery had some failure then split log may not happen and may lead to dataloss.
So may be we need to decide upon the actual delay that needs to be introduced once Hmaster calls NN recovery.

",ram_krish,ram_krish,Critical,Closed,Fixed,08/Aug/11 17:49,23/Sep/13 19:08
Bug,HBASE-4179,12518272,Failed to run RowCounter on top of Hadoop branch-0.22,":~/hadoop$ HADOOP_CLASSPATH=`~/hbase/bin/hbase classpath` bin/hadoop jar ~/hbase/hbase-0.91.0-SNAPSHOT.jar rowcounter usertable 
Exception in thread ""main"" java.lang.NoSuchMethodError: org.apache.hadoop.util.ProgramDriver.driver([Ljava/lang/String;)V 
        at org.apache.hadoop.hbase.mapreduce.Driver.main(Driver.java:51) 
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39) 
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25) 
        at java.lang.reflect.Method.invoke(Method.java:597) 
        at org.apache.hadoop.util.RunJar.main(RunJar.java:192) 
",michaelweng,michaelweng,Major,Closed,Fixed,09/Aug/11 01:50,20/Nov/15 11:52
Bug,HBASE-4180,12518274,HBase should check the isSecurityEnabled flag,"Hadoop 0.21.0's UserGroupInfomation support the security check flag and always returns false.
HBase should check both the method existence and the return value.
",ghelmling,sinofool,Major,Closed,Fixed,09/Aug/11 02:12,20/Nov/15 11:54
Bug,HBASE-4181,12518276,HConnectionManager can't find cached HRegionInterface which makes client very slow,"HRegionInterface getHRegionConnection(final String hostname,
        final int port, final InetSocketAddress isa, final boolean master)
        throws IOException 


/////////////////////////
	String rsName = isa != null ? isa.toString() : Addressing
          .createHostAndPortStr(hostname, port); 

                
////here,if isa is null, the Addressing created a address like ""node41:60010""
                                                                 ////should use ""isa.toString():new InetSocketAddress(hostname, port).toString();"" 
                                                                 ////instead of ""Addressing.createHostAndPortStr(hostname, port);""


 	server = this.servers.get(rsName);                                      
      if (server == null) {
        // create a unique lock for this RS (if necessary)
        this.connectionLock.putIfAbsent(rsName, rsName);
        // get the RS lock
        synchronized (this.connectionLock.get(rsName)) {
          // do one more lookup in case we were stalled above
          server = this.servers.get(rsName);
          if (server == null) {
            try {
              if (clusterId.hasId()) {
                conf.set(HConstants.CLUSTER_ID, clusterId.getId());
              }
              // Only create isa when we need to.
              InetSocketAddress address = isa != null ? isa
                  : new InetSocketAddress(hostname, port);
              // definitely a cache miss. establish an RPC for this RS
              server = (HRegionInterface) HBaseRPC.waitForProxy(
                  serverInterfaceClass, HRegionInterface.VERSION, address,
                  this.conf, this.maxRPCAttempts, this.rpcTimeout,
                  this.rpcTimeout);
              this.servers.put(address.toString(), server);    

          
////but here address.toString() send an address like ""node41/10.61.2l.171:60010
////so this method can never get cached address and make client request very slow due to it's synchronized.
	

                  
            } catch (RemoteException e) {
              LOG.warn(""RemoteException connecting to RS"", e);
              // Throw what the RemoteException was carrying.
              throw RemoteExceptionHandler.decodeRemoteException(e);
            }
          }
        }
///////////////////////",liujia_ict,liujia_ict,Critical,Closed,Fixed,09/Aug/11 02:51,20/Nov/15 11:54
Bug,HBASE-4184,12518394,"CatalogJanitor doesn't work properly when ""fs.default.name"" isn't set in config file.","In our system, hbase.rootdir is set to a hdfs path and hbase can figure out the FileSystem and set ""fs.default.name"" accordingly on the Configuration object and pass around including to RS. That is handled in HMaster.java and MasterFileSystem.java.

CatalogJanitor uses deprecated HRegionInfo.getTableDesc. The method creates a default configuration and get FileSystem from there. That will be RawLocalFileSystem. It returns the following exception.


java.lang.IllegalArgumentException: Wrong FS: hdfs://sea-esxi-0:54310/tmp/hbase/
testtb/.tableinfo, expected: file:///
        at org.apache.hadoop.fs.FileSystem.checkPath(FileSystem.java:454)
        at org.apache.hadoop.fs.RawLocalFileSystem.pathToFile(RawLocalFileSystem.java:67)
        at org.apache.hadoop.fs.RawLocalFileSystem.listStatus(RawLocalFileSystem.java:307)
        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1085)
        at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1110)
        at org.apache.hadoop.fs.ChecksumFileSystem.listStatus(ChecksumFileSystem.java:494)
        at org.apache.hadoop.hbase.util.FSUtils.getTableInfoModtime(FSUtils.java:833)
        at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:127)
        at org.apache.hadoop.hbase.util.FSTableDescriptors.get(FSTableDescriptors.java:99)
        at org.apache.hadoop.hbase.HRegionInfo.getTableDesc(HRegionInfo.java:560)
        at org.apache.hadoop.hbase.master.CatalogJanitor$1.compare(CatalogJanitor.java:118)
        at org.apache.hadoop.hbase.master.CatalogJanitor$1.compare(CatalogJanitor.java:110)
        at java.util.TreeMap.put(TreeMap.java:530)        at org.apache.hadoop.hbase.master.CatalogJanitor$2.visit(CatalogJanitor.java:138)",mingma,mingma,Major,Closed,Fixed,10/Aug/11 01:45,20/Nov/15 11:53
Bug,HBASE-4186,12518474,No region is added to regionsInTransitionInRS,"We have a skip list set called regionsInTransitionInRS (introduced in HBASE-3741) where we try to maintain a list to know the currently processing regions for closing and opening.
In open region handler we are trying to throw an error if the regions are in transition on that RS when we get an open call for the same region.
But we are not adding the region into the set anywhere.",yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Closed,Fixed,10/Aug/11 16:27,20/Nov/15 11:55
Bug,HBASE-4195,12518654,"Possible inconsistency in a memstore read after a reseek, possible performance improvement","This follows the dicussion around HBASE-3855, and the random errors (20% failure on trunk) on the unit test org.apache.hadoop.hbase.regionserver.TestHRegion.testWritesWhileGetting

I saw some points related to numIterReseek, used in the MemStoreScanner#getNext (line 690):

{noformat}679	    protected KeyValue getNext(Iterator it) {
680	      KeyValue ret = null;
681	      long readPoint = ReadWriteConsistencyControl.getThreadReadPoint();
682	      //DebugPrint.println( "" MS@"" + hashCode() + "": threadpoint = "" + readPoint);
683	 
684	      while (ret == null && it.hasNext()) {
685	        KeyValue v = it.next();
686	        if (v.getMemstoreTS() <= readPoint) {
687	          // keep it.
688	          ret = v;
689	        }
690	        numIterReseek--;
691	        if (numIterReseek == 0) {
692	          break;
693	         }
694	      }
695	      return ret;
696	    }{noformat}

This function is called by seek, reseek, and next. The numIterReseek is only usefull for reseek.

There are some issues, I am not totally sure it's the root cause of the test case error, but it could explain partly the randomness of the error, and one point is for sure a bug.

1) In getNext, numIterReseek is decreased, then compared to zero. The seek function sets numIterReseek to zero before calling getNext. It means that the value will be actually negative, hence the test will always fail, and the loop will continue. It is the expected behaviour, but it's quite smart.

2) In ""reseek"", numIterReseek is not set between the loops on the two iterators. If the numIterReseek is equals to zero after the loop on the first one, the loop on the second one will never call seek, as numIterReseek will be negative.

3) Still in ""reseek"", the test to call ""seek"" is (kvsetNextRow == null && numIterReseek == 0). In other words, if kvsetNextRow is not null when numIterReseek equals zero, numIterReseek will start to be negative at the next iteration and seek will never be called.

4) You can have side effects if reseek ends with a numIterReseek > 0: the following calls to the ""next"" function will decrease numIterReseek to zero, and getNext will break instead of continuing the loop. As a result, later calls to next() may return null or not depending on how is configured the default value for numIterReseek.

To check if the issue comes from point 4, you can set the numIterReseek to zero before returning in reseek:

{noformat}      numIterReseek = 0;
      return (kvsetNextRow != null || snapshotNextRow != null);
    }{noformat}

On my env, on trunk, it seems to work, but as it's random I am not really sure. I also had to modify the test (I added a loop) to make it fails more often, the original test was working quite well here.

It has to be confirmed that this totally fix (it could be partial or unrelated) org.apache.hadoop.hbase.regionserver.TestHRegion.testWritesWhileGetting before implementing a complete solution.
",nkeywal,nkeywal,Critical,Closed,Fixed,11/Aug/11 22:17,20/Nov/15 11:53
Bug,HBASE-4196,12518684,TableRecordReader may skip first row of region,"After the following scenario, the first record of region is skipped, without being sent to Mapper:
 - the reader is initialized with TableRecordReader.init()
 - then nextKeyValue is called, causing call to scanner.next() - here ScannerTimeoutException occurs
 - the scanner is restarted by call to restart() and then *two* calls to scanner.next() occur, causing we have lost the first row
",mingma,je.ik,Major,Closed,Fixed,12/Aug/11 09:54,20/Nov/15 11:52
Bug,HBASE-4197,12518721,RegionServer expects all scanner to be subclasses of HRegion.RegionScanner,"Returning just an InternalScanner from RegionObsever.{pre|post}OpenScanner leads to the following exception when using the scanner.

java.io.IOException: InternalScanner implementation is expected to be HRegion.RegionScanner.
        at org.apache.hadoop.hbase.regionserver.HRegionServer.next(HRegionServer.java:2023)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:314)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1225)

The problem is in HRegionServer.next(...):
{code} 
    InternalScanner s = this.scanners.get(scannerName);
...
      // Call coprocessor. Get region info from scanner.
      HRegion region = null;
      if (s instanceof HRegion.RegionScanner) {
        HRegion.RegionScanner rs = (HRegion.RegionScanner) s;
        region = getRegion(rs.getRegionName().getRegionName());
      } else {
        throw new IOException(""InternalScanner implementation is expected "" +
            ""to be HRegion.RegionScanner."");
      }
{code} ",larsh,larsh,Major,Closed,Fixed,12/Aug/11 17:26,20/Nov/15 11:54
Bug,HBASE-4203,12518969,While master restarts and if the META region's state is OPENING then master cannot assign META until timeout monitor deducts,"1. Start Master and 2 RS.
2. If any exception happens while opening the META region the state in znode will be OPENING.
3. If at this point the master restarts then the master will start processing the regions in RIT.
4. If the znode is found to be in OPENING then master waits for timeout monitor to deduct and then call opening.
5. If default timeout monitor is configured(1800000 sec/30 min) then it will take 30 mins to open the META region itself.
Soln:
====
Better not to wait for the Timeout monitor period to open catalog tables on Master restart",ram_krish,ram_krish,Minor,Closed,Fixed,16/Aug/11 09:21,20/Nov/15 11:52
Bug,HBASE-4204,12518985,book.xml - typo in rolling restart,"Found by Shrijeet Paliwal....


Following line under 'A.5.1. Rolling Restart' should be :

$ echo ""balance_switch false"" | ./bin/hbase

$ echo ""balance_switch false"" | ./bin/hbase shell

-Shrijeet

",dmeil,dmeil,Minor,Closed,Fixed,16/Aug/11 12:10,12/Jun/22 19:15
Bug,HBASE-4209,12519048,The HBase hbase-daemon.sh SIGKILLs master when stopping it,There's a bit of code in hbase-daemon.sh that makes HBase master being SIGKILLed when stopping it rather than trying SIGTERM (like it does for other daemons). When HBase is executed in a standalone mode (and the only daemon you need to run is master) that causes newly created tables to go missing as unflushed data is thrown out. If there was not a good reason to kill master with SIGKILL perhaps we can take that special case out and rely on SIGTERM.,rvs,rvs,Major,Closed,Fixed,16/Aug/11 21:41,20/Nov/15 11:53
Bug,HBASE-4211,12519074,Do init-sizing of the StringBuilder making a ServerName.,"Simple patch from Benoît.

---
 .../java/org/apache/hadoop/hbase/ServerName.java   |    3 ++-
 1 files changed, 2 insertions(+), 1 deletions(-)

diff --git a/src/main/java/org/apache/hadoop/hbase/ServerName.java b/src/main/java/org/apache/hadoop/hbase/ServerName.java
index 6b03832..4ddb5b7 100644
--- a/src/main/java/org/apache/hadoop/hbase/ServerName.java
+++ b/src/main/java/org/apache/hadoop/hbase/ServerName.java
@@ -128,7 +128,8 @@ public class ServerName implements Comparable<ServerName> {
   * startcode formatted as <code>&lt;hostname> ',' &lt;port> ',' &lt;startcode></code>
   */
  public static String getServerName(String hostName, int port, long startcode) {
-    StringBuilder name = new StringBuilder(hostName);
+    final StringBuilder name = new StringBuilder(hostName.length() + 1 + 5 + 1 + 13);
+    name.append(hostName);
    name.append(SERVERNAME_SEPARATOR);
    name.append(port);
    name.append(SERVERNAME_SEPARATOR);
--
1.7.6.434.g1d2b3",tsuna,stack,Minor,Closed,Fixed,17/Aug/11 04:58,20/Nov/15 11:55
Bug,HBASE-4212,12519088,TestMasterFailover fails occasionally,"It seems a bug. The root in RIT can't be moved..
In the failover process, it enforces root on-line. But not clean zk node. 
test will wait forever.

  void processFailover() throws KeeperException, IOException, InterruptedException {
     
    // we enforce on-line root.
    HServerInfo hsi =
      this.serverManager.getHServerInfo(this.catalogTracker.getMetaLocation());
    regionOnline(HRegionInfo.FIRST_META_REGIONINFO, hsi);
    hsi = this.serverManager.getHServerInfo(this.catalogTracker.getRootLocation());
    regionOnline(HRegionInfo.ROOT_REGIONINFO, hsi);

It seems that we should wait finished as meta region 
  int assignRootAndMeta()
  throws InterruptedException, IOException, KeeperException {
    int assigned = 0;
    long timeout = this.conf.getLong(""hbase.catalog.verification.timeout"", 1000);

    // Work on ROOT region.  Is it in zk in transition?
    boolean rit = this.assignmentManager.
      processRegionInTransitionAndBlockUntilAssigned(HRegionInfo.ROOT_REGIONINFO);
    if (!catalogTracker.verifyRootRegionLocation(timeout)) {
      this.assignmentManager.assignRoot();
      this.catalogTracker.waitForRoot();

      //we need add this code and guarantee that the transition has completed
      this.assignmentManager.waitForAssignment(HRegionInfo.ROOT_REGIONINFO);
      assigned++;
    }

logs:
2011-08-16 07:45:40,715 DEBUG [RegionServer:0;C4S2.site,47710,1313495126115-EventThread] zookeeper.ZooKeeperWatcher(252): regionserver:47710-0x131d2690f780004 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/unassigned/70236052
2011-08-16 07:45:40,715 DEBUG [RS_OPEN_ROOT-C4S2.site,47710,1313495126115-0] zookeeper.ZKAssign(712): regionserver:47710-0x131d2690f780004 Successfully transitioned node 70236052 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING
2011-08-16 07:45:40,715 DEBUG [Thread-760-EventThread] zookeeper.ZooKeeperWatcher(252): master:60701-0x131d2690f780009 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/unassigned/70236052
2011-08-16 07:45:40,716 INFO  [PostOpenDeployTasks:70236052] catalog.RootLocationEditor(62): Setting ROOT region location in ZooKeeper as C4S2.site:47710
2011-08-16 07:45:40,716 DEBUG [Thread-760-EventThread] zookeeper.ZKUtil(1109): master:60701-0x131d2690f780009 Retrieved 52 byte(s) of data from znode /hbase/unassigned/70236052 and set watcher; region=-ROOT-,,0, server=C4S2.site,47710,1313495126115, state=RS_ZK_REGION_OPENING
2011-08-16 07:45:40,717 DEBUG [Thread-760-EventThread] master.AssignmentManager(477): Handling transition=RS_ZK_REGION_OPENING, server=C4S2.site,47710,1313495126115, region=70236052/-ROOT-
2011-08-16 07:45:40,725 DEBUG [RS_OPEN_ROOT-C4S2.site,47710,1313495126115-0] zookeeper.ZKAssign(661): regionserver:47710-0x131d2690f780004 Attempting to transition node 70236052/-ROOT- from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2011-08-16 07:45:40,727 DEBUG [RS_OPEN_ROOT-C4S2.site,47710,1313495126115-0] zookeeper.ZKUtil(1109): regionserver:47710-0x131d2690f780004 Retrieved 52 byte(s) of data from znode /hbase/unassigned/70236052; data=region=-ROOT-,,0, server=C4S2.site,47710,1313495126115, state=RS_ZK_REGION_OPENING
2011-08-16 07:45:40,740 DEBUG [RegionServer:0;C4S2.site,47710,1313495126115-EventThread] zookeeper.ZooKeeperWatcher(252): regionserver:47710-0x131d2690f780004 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/unassigned/70236052
2011-08-16 07:45:40,740 DEBUG [Thread-760-EventThread] zookeeper.ZooKeeperWatcher(252): master:60701-0x131d2690f780009 Received ZooKeeper Event, type=NodeDataChanged, state=SyncConnected, path=/hbase/unassigned/70236052
2011-08-16 07:45:40,740 DEBUG [RS_OPEN_ROOT-C4S2.site,47710,1313495126115-0] zookeeper.ZKAssign(712): regionserver:47710-0x131d2690f780004 Successfully transitioned node 70236052 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2011-08-16 07:45:40,741 DEBUG [RS_OPEN_ROOT-C4S2.site,47710,1313495126115-0] handler.OpenRegionHandler(121): Opened -ROOT-,,0.70236052
2011-08-16 07:45:40,741 DEBUG [Thread-760-EventThread] zookeeper.ZKUtil(1109): master:60701-0x131d2690f780009 Retrieved 52 byte(s) of data from znode /hbase/unassigned/70236052 and set watcher; region=-ROOT-,,0, server=C4S2.site,47710,1313495126115, state=RS_ZK_REGION_OPENED
2011-08-16 07:45:40,741 DEBUG [Thread-760-EventThread] master.AssignmentManager(477): Handling transition=RS_ZK_REGION_OPENED, server=C4S2.site,47710,1313495126115, region=70236052/-ROOT-

//.............................................It said that zk node can't be cleaned because of we have enforced on-line the root.......................................
// The test will wait forever.

2011-08-16 07:45:40,741 WARN  [Thread-760-EventThread] master.AssignmentManager(540): Received OPENED for region 70236052/-ROOT- from server C4S2.site,47710,1313495126115 but region was in  the state null and not in expected PENDING_OPEN or OPENING states

2011-08-16 07:45:41,018 DEBUG [Master:0;C4S2.site:60701] zookeeper.ZKUtil(1109): master:60701-0x131d2690f780009 Retrieved 52 byte(s) of data from znode /hbase/unassigned/70236052 and set watcher; region=-ROOT-,,0, server=C4S2.site,47710,1313495126115, state=RS_ZK_REGION_OPENED
2011-08-16 07:45:41,233 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:41,337 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:41,439 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:41,543 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:41,645 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:41,748 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:41,900 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:42,002 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:42,105 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:42,206 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:42,308 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
2011-08-16 07:45:42,410 DEBUG [Thread-760] zookeeper.ZKAssign(807): ZK RIT -> 70236052
",sunnygao,sunnygao,Major,Closed,Fixed,17/Aug/11 08:26,20/Nov/15 11:54
Bug,HBASE-4215,12519151,RS requestsPerSecond counter seems to be off,"In testing trunk, I had YCSB reporting some 40,000 requests/second, but the summary info on the master webpage was consistently indicating somewhere around 3x that. I'm guessing that we may have a bug where we forgot to divide by time.",subramanian,tlipcon,Critical,Closed,Fixed,17/Aug/11 17:18,20/Nov/15 11:53
Bug,HBASE-4220,12519197,Lots of DNS queries from client,"In running a YCSB workload, I managed to DDOS a DNS server since it seems to be flooding lots of DNS requests. Installing nscd on the client machines improved throughput by a factor of 6 and stopped killing the server. These are long-running clients, so it's not clear why we do so many lookups.",stack,tlipcon,Critical,Closed,Fixed,17/Aug/11 22:20,20/Nov/15 11:53
Bug,HBASE-4230,12519391,Compaction threads need names,"The CompactSplitThread creates executors for doing compaction work, but threads end up named things like ""pool-2-thread-1"" which isn't very useful.",apurtell,tlipcon,Major,Closed,Fixed,19/Aug/11 09:12,20/Nov/15 11:54
Bug,HBASE-4234,12519490,Let regionserver abort if rollback fail after point-of-no-return on branch,"It has been changed on trunk. But not on branch.
If rollback fail after poing-of-no-return and do nothing, the parent has been closed, and doesn't report to HMaster. This cause the inconsistance region state between HMaster and HRegionServer.",jeason,jeason,Major,Closed,Fixed,20/Aug/11 03:10,20/Nov/15 11:54
Bug,HBASE-4238,12519744,CatalogJanitor can clear a daughter that split before processing its parent,"I didn't dig a lot into this issue, but by splitting a table twice in a row I was able to trigger a situation where a daughter of the first split was deleted by the CatalogJanitor before it processed its parent. Will post log in a comment.",stack,jdcryans,Critical,Closed,Fixed,22/Aug/11 22:33,20/Nov/15 11:55
Bug,HBASE-4252,12520040,TestLogRolling's low-probability failure ,"Before I explain why it could happen, I describe how does this test(testLogRollOnDatanodeDeath) works:
1. There's two datanodes A & B in env. So the log has two replications first which is the expect and default value.
2. Add a new datanode C and wait it active.
3. Kill A who is in the pipelines. 
4. Write data. So trigger a new rollWriter while the next sync. And it only happens once. For the new log has two replications.
5. Kill another datanode B.
6. Write batch data to trigger consecutive rollWriter. So LowReplication-Roller will be disabled.
7. Add a new datanode D and wait it active.
8. Send a rollWriter request. So expect the new log will has the default replications.
9. Write batch data. Assert the LogReplication-Roller will be enabled.

Maybe the rollWriter request in step 8 can't be executed affected by the previous roll requests from step 6. So the current log replication is not the expect value.
{noformat}
  public byte [][] rollWriter() throws FailedLogCloseException, IOException {
    // Return if nothing to flush.
    if (this.writer != null && this.numEntries.get() <= 0) {
      return null;
    }
{noformat}

So the following assertion must be safeguarded.
{noformat}
  log.rollWriter();
  batchWriteAndWait(table, 14, true, 10000);
  assertTrue(""LowReplication Roller should've been enabled"",
      log.isLowReplicationRollEnabled());
{noformat}",jeason,jeason,Major,Closed,Fixed,25/Aug/11 01:50,20/Nov/15 11:52
Bug,HBASE-4253,12520084,Intermittent test failure because of missing config parameter in new HTable(tablename),"As per the description in HBASE-4138 this issue is raised to fix the random testcase failure.
Consider the log in the failed build #2132 for the testcase TestScannerTimeOut

2011-08-23 04:30:11,195 INFO  [main] zookeeper.MiniZooKeeperCluster(141): Failed binding ZK Server to client port: 21818
2011-08-23 04:30:11,226 INFO  [main] zookeeper.MiniZooKeeperCluster(164): Started MiniZK Cluster and connect 1 ZK server on client port: 21819

By default we try connecting to 21818 but as it was not bindable we connect to 21819. (may be the port was busy).

After starting the miniZkCluster

this.conf.set(""hbase.zookeeper.property.clientPort"",
      Integer.toString(clientPort));

we set this port in the config object.
So for RS and Master the zookeeper client port will be 21819.
Now when the testcase starts running there is no testcase till TestScannerTimeout#test3686a where we need a new client connection.
Now as part of test3686a we create new HTable() which calls
{code}
this(HBaseConfiguration.create(), tableName);
{code}
Here we create a new configuration object. Hence the zookeeper client port is taken to be 21818.
Ideally due to improper shutdown of some prev zk cluster that was running in 21818 the test case was able to connect to this but the port being different it could not find the /hbase node.
Hence the failure has happened.
The remaining two testcases in TestHTablePool that failed also has the similar problem. Even the failure in build #2119 is exactly the same.
There should be a mechanism from the test for the client code to know to which zk he should connect to. 
Another intersting thing
All testcases are using new HTable(conf, tablename).
Only these 3 test cases are using it like new HTable(tablename). Hence the problem.",ram_krish,ram_krish,Major,Closed,Fixed,25/Aug/11 09:21,20/Nov/15 11:55
Bug,HBASE-4265,12520490,zookeeper.KeeperException$NodeExistsException if HMaster restarts while table is being disabled,"There seems to be more than just one issue regarding the following scenario. I would provide a fix later just for this exception.

1. A table is being disabled.
2. HMaster restarted.
3. At HMaster startup, it tries to transition from disabling to disabled state. It got the following exception.


org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /hbase/unassigned/419b902243c836c285108ba555b712fa
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:110)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
	at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:637)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.createNonSequential(RecoverableZooKeeper.java:475)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.create(RecoverableZooKeeper.java:457)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndWatch(ZKUtil.java:742)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.createNodeClosing(ZKAssign.java:461)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1440)
	at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1406)
	at org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.run(DisableTableHandler.java:141)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)



This issue is this specific region is in a special state before HMaster restarts; it has been closed by RS properly thus the zk state is RS_ZK_REGION_CLOSED. However, HMaster hasn't got a chance to process ClosedRegionHandler yet and thus the node remains at zk. After RS restarts, this node is added to online region list first in AssignmentManager.rebuildUserRegions and tries to unassign it later.
",mingma,mingma,Major,Closed,Fixed,28/Aug/11 17:53,20/Nov/15 11:53
Bug,HBASE-4270,12520515,IOE ignored during flush-on-close causes dataloss,"If the RS experiences an exception during the flush of a region while closing it, it currently catches the exception, logs a warning, and keeps going. If the exception was a DroppedSnapshotException, this means that it will silently drop any data that was in memstore when the region was closed.

Instead, the RS should do a hard abort so that its logs will be replayed.",tlipcon,tlipcon,Blocker,Closed,Fixed,29/Aug/11 04:09,20/Nov/15 11:55
Bug,HBASE-4271,12520516,Clean up coprocessor's handlings of table operations,"Couple fixes we can do w.r.t coprocessor's handlings of table operations.

1. Honor MasterObserver's requests to bypass default action.
2. Fix up the function signatures for preCreateTable to use HRegionInfo as parameter instead.
3. Invoke postEnableTable, etc. methods after the operations are done.",mingma,mingma,Major,Closed,Fixed,29/Aug/11 05:02,20/Nov/15 11:53
Bug,HBASE-4273,12520518,java.lang.NullPointerException when a table is being disabled and HMaster restarts,"This bug occurs in following scenario. 

1. For some reason, the regionLocation isn't set in .META. table for some regions. Perhaps createTable didn't complete successfully.
1. The table of those regions is being disabled.
2. HMaster restarted.
3. At HMaster startup, it tries to transition from disabling to disabled state. It got the following exception.

java.lang.NullPointerException: Passed server is null
        at
org.apache.hadoop.hbase.master.ServerManager.sendRegionClose(ServerManager.
java:581)
        at
org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager
.java:1093)
        at
org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager
.java:1040)
        at
org.apache.hadoop.hbase.master.handler.DisableTableHandler$BulkDisabler$1.r
un(DisableTableHandler.java:132)
        at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.j
ava:886)


In AssignmentManager.rebuildUserRegions, it added such regions to its regions list,

      if (regionLocation == null) {
        // Region not being served, add to region map with no assignment
        // If this needs to be assigned out, it will also be in ZK as RIT
        // add if the table is not in disabled and enabling state
        if (false == checkIfRegionBelongsToDisabled(regionInfo)
            && false == checkIfRegionsBelongsToEnabling(regionInfo)) {
          regions.put(regionInfo, regionLocation);
        }

Perhaps, it should be

      if (regionLocation == null) {
        // Region not being served, add to region map with no assignment
        // If this needs to be assigned out, it will also be in ZK as RIT
        // add if the table is not in disabled and enabling state
        if (true == checkIfRegionBelongsToEnabled(regionInfo) {
          regions.put(regionInfo, regionLocation);
        }


",mingma,mingma,Major,Closed,Fixed,29/Aug/11 05:18,20/Nov/15 11:55
Bug,HBASE-4277,12520535,HRS.closeRegion should be able to close regions with only the encoded name,"As suggested by Stack in HBASE-4217 creating a new issue to provide a patch for 0.90.x version.


We had some sort of an outage this morning due to a few racks losing power, and some regions were left in the following state:

ERROR: Region UNKNOWN_REGION on sv4r17s9:60020, key=e32bbe1f48c9b3633c557dc0291b90a3, not on HDFS or in META but deployed on sv4r17s9:60020

That region was deleted by the master but the region server never got the memo. Right now there's no way to force close it because HRS.closeRegion requires an HRI and the only way to create one is to get it from .META. which in our case doesn't contain a row for that region. Basically we have to wait until that server is dead to get rid of the region and make hbck happy.

The required change is to have closeRegion accept an encoded name in both HBA (when the RS address is provided) and HRS since it's able to find it anyways from it's list of live regions.
bq.If a 0.90 version, we maybe should do that in another issue.",ram_krish,ram_krish,Critical,Closed,Fixed,29/Aug/11 08:18,20/Nov/15 11:55
Bug,HBASE-4282,12520638,RegionServer should abort when WAL close encounters an error with unflushed edits,"The ability to ride over WAL close errors on log rolling added in HBASE-4222 could lead to missing HLog entries if:
* A table has DEFERRED_LOG_FLUSH=true
* There are unflushed WALEdit entries for that table in the current SequenceFile writer buffer

Since the writes were already acknowledged to the client, just ignoring the close error to allow for another log roll doesn't seem like the right thing to do here.

We could easily flag this state and only ride over the close error if there aren't unflushed entries.  This would bring the above condition back to the previous behavior of aborting the region server.  However, aborting the region server in this state is still guaranteeing data loss.  Is there anything we can do better in this case?  ",ghelmling,ghelmling,Blocker,Closed,Fixed,29/Aug/11 21:12,20/Nov/15 11:52
Bug,HBASE-4283,12520646,HBaseAdmin never recovers from restarted cluster,"While testing common scenarios that we might encounter I found that HBaseAdmin does not recover from a restarted cluster.

It turns out HBaseClient.Connection.stop() is send into an endless loop here:
{code}
    // wait until all connections are closed
    while (!connections.isEmpty()) {
      try {
        Thread.sleep(100);
      } catch (InterruptedException ignored) {
      }
    }
{code}
The reason is that PoolMap.remove(k,v) does not remove empty pools, and hence connections.isEmpty() is never true if there ever was any connection in there.
My fix is to remove the pool from the poolMap when it is empty. (Alternatively one could change PoolMap.isEmpty() to also look inside of all pools and see if their size is 0).


When I fixed that I noticed that if the master wasn't running when HBaseAdmin is created it also will not recover from that.
Even creating a new HBaseAdmin from the same Configuration will still use the old stale HConnection.

In that case a MasterNotRunningException is thrown, which is not handled in HBaseAdmin's constructor.

The HConnection handling in HConnectionManager is funky. There should never be a closed connection in the HBASE_INSTANCES.
I might look at that as well but in a separate issue.",larsh,larsh,Minor,Closed,Fixed,29/Aug/11 22:29,20/Nov/15 11:52
Bug,HBASE-4286,12520652,NPE when master shuts down during processing of server shutdowns,"If the master shuts down while trying to process a server crash, it will escape out of the loop trying to read META, and then the {{hris}} variable is null. This results in the following NPE:

2011-08-29 16:05:52,436 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Reassigning 0 region(s) that c0317.hal.cloudera.com,60020,1314658642935 was carrying (skipping 0 regions(s) that are already in transition)
2011-08-29 16:05:52,436 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_META_SERVER_SHUTDOWN
java.lang.NullPointerException
	at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:207)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
",,tlipcon,Major,Closed,Fixed,29/Aug/11 23:13,12/Jun/22 19:19
Bug,HBASE-4288,12520658,"""Server not running"" exception during meta verification causes RS abort","The master tried to verify the META location just as that server was shutting down due to an abort. This caused the ""Server not running"" exception to get thrown, which wasn't handled properly in the master, causing it to abort.",,tlipcon,Critical,Closed,Fixed,29/Aug/11 23:40,12/Jun/22 19:20
Bug,HBASE-4290,12520668,HLogSplitter doesn't mark its MonitoredTask as complete in non-distributed case,"This can leave entries left in the ""monitored tasks"" display, which might confuse users.",tlipcon,tlipcon,Trivial,Closed,Fixed,30/Aug/11 01:19,20/Nov/15 11:55
Bug,HBASE-4294,12520673,HLogSplitter sleeps with 1-second granularity,"The HLogSplitter writer threads wait in 1-second intervals for data. When data arrives, notify() is called and they wake up. But when the main thread needs to notify the writers to shut down, it doesn't notify. Hence, the writers always wait up to a full extra second for each log split.",tlipcon,tlipcon,Trivial,Closed,Fixed,30/Aug/11 01:27,20/Nov/15 11:52
Bug,HBASE-4295,12520684,rowcounter does not return the correct number of rows in certain circumstances,"When you run

{noformat}
hadoop jar hbase.jar rowcounter <table>
{noformat}

the org.apache.hadoop.hbase.mapreduce.RowCounter class is run.
The RowCounterMapper class in the RowCounter mapreduce job contains the following:

{noformat}
    @Override
    public void map(ImmutableBytesWritable row, Result values,
      Context context)
    throws IOException {
      for (KeyValue value: values.list()) {
        if (value.getValue().length > 0) {
          context.getCounter(Counters.ROWS).increment(1);
          break;
        }
      }
    }
{noformat}

The intention is to go through the column values in the row, and increment the ROWS counter if some value is non-empty. However, values.list() always has size 1. This is because the createSubmittableJob static method uses a Scan as follows:

{noformat}
    Scan scan = new Scan();
    scan.setFilter(new FirstKeyOnlyFilter());
{noformat}

So the input map splits always contain just the first KV. If the column corresponding to that first KV is empty, even though other columns are non-empty, that row is skipped.
This way, rowcounter can return an incorrect result.

One way to reproduce this is to create an hbase table with two columns, say f1:q1 and f2:q2. Create some (say 2) rows with empty f1:q1 but non-empty f2:q2, and some (say 3) rows with empty f2:q2 and non-empty f1:q1.
Then run rowcounter (specifying only the table but not any columns). The count will be either 2 short or 3 short.



",dave_revell,wypoon,Critical,Closed,Fixed,30/Aug/11 05:02,20/Nov/15 11:53
Bug,HBASE-4296,12520686,Deprecate HTable[Interface].getRowOrBefore(...),"HTable's getRowOrBefore(...) internally calls into Store.getRowKeyAtOrBefore. That method was created to allow our scanning of .META. (see HBASE-2600).

Store.getRowKeyAtOrBefore(...) lists a bunch of requirements for this to be performant that a user of HTable will not be aware of.

I propose deprecating this in the public interface in 0.92 and removing it from the public interface in 0.94. If we don't get to HBASE-2600 in 0.94 it will still remain as internal interface for scanning meta.

Comments?",larsh,larsh,Trivial,Closed,Fixed,30/Aug/11 05:19,20/Nov/15 11:52
Bug,HBASE-4297,12520700,TableMapReduceUtil overwrites user supplied options,"Job configuration is overwritten by hbase-default and hbase-site in TableMapReduceUtil.initTable(Mapper|Reducer)Job, causing unexpected behavior in the following code:
{noformat}
Configuration conf = HBaseConfiguration.create();

// change keyvalue size
conf.setInt(""hbase.client.keyvalue.maxsize"", 20971520);

Job job = new Job(conf, ...);

TableMapReduceUtil.initTableMapperJob(...);

// the job doesn't have the option changed, uses it from hbase-site or hbase-default
job.submit();
{noformat}

Although in this case it could be fixed by moving the set() after initTableMapperJob(), in case where user wants to change some option using GenericOptionsParser and -D this is impossible, making this cool feature useless.

In the 0.20.x era this code behaved as expected. The solution of this problem should be that we don't overwrite the options, but just read them if they are missing.",,je.ik,Major,Closed,Fixed,30/Aug/11 07:43,20/Nov/15 11:54
Bug,HBASE-4300,12520796,Start of new-version master fails if old master's znode is hanging around,"I shut down an 0.90 cluster, and had to do so uncleanly. I then started a trunk (0.92) cluster before the old master znode had expired. This cased:

java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.substring(String.java:1937)
        at org.apache.hadoop.hbase.ServerName.parseHostname(ServerName.java:81)
        at org.apache.hadoop.hbase.ServerName.<init>(ServerName.java:63)
        at org.apache.hadoop.hbase.master.ActiveMasterManager.blockUntilBecomingActiveMaster(ActiveMasterManager.java:148)
        at org.apache.hadoop.hbase.master.HMaster.becomeActiveMaster(HMaster.java:342)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:297)
",stack,tlipcon,Critical,Closed,Fixed,30/Aug/11 19:35,20/Nov/15 11:55
Bug,HBASE-4301,12520797,META migration from 0.90 to trunk fails,"I started a trunk cluster as an upgrade from 0.90.4ish, and now I can't scan my .META. table, etc, and other operations fail.",iamknome,tlipcon,Blocker,Closed,Fixed,30/Aug/11 19:49,20/Nov/15 11:52
Bug,HBASE-4303,12520799,HRegionInfo.toString has bad quoting,"Currently it's outputting:
REGION => {NAME => '.META.,,1 TableName => ', STARTKEY => '', ENDKEY => '', ENCODED => 1028785192,}

Notice the missing quotes around tableName, etc.",tlipcon,tlipcon,Trivial,Closed,Fixed,30/Aug/11 19:59,20/Nov/15 11:52
Bug,HBASE-4304,12520831,requestsPerSecond counter stuck at 0,"Running trunk @ r1163343, all of the requestsPerSecond counters are showing 0 both in the master UI and in the RS UI. The writeRequestsCount metric is properly updating in the RS UI.",li,tlipcon,Critical,Closed,Fixed,30/Aug/11 20:45,20/Nov/15 11:54
Bug,HBASE-4308,12520864,Race between RegionOpenedHandler and AssignmentManager,"When the master is processing a ZK event for REGION_OPENED, it calls delete() on the znode before it removes the node from RegionsInTransition. If the notification of that delete comes back into AssignmentManager before the region is removed from RIT, you see an error like:

2011-08-30 17:43:29,537 WARN  [main-EventThread] master.AssignmentManager(861): Node deleted but still in RIT: .META.,,1.1028785192 state=OPEN, ts=1314751409532, server=todd-w510,55655,1314751396840

Not certain if it causes issues, but it's a concerning log message.",ram_krish,tlipcon,Major,Closed,Fixed,31/Aug/11 00:50,20/Nov/15 11:52
Bug,HBASE-4309,12520873,slow query log metrics spewing warnings,"Lots of these in the logs in trunk:
2011-08-30 22:54:36,100 WARN org.apache.hadoop.hbase.ipc.HBaseRpcMetrics: Got inc() request for method that doesnt exist: get.slowResponse.
2011-08-30 22:54:36,100 WARN org.apache.hadoop.hbase.ipc.HBaseRpcMetrics: Got inc() request for method that doesnt exist: get.aboveOneSec.
...",riley,tlipcon,Critical,Closed,Fixed,31/Aug/11 05:55,20/Nov/15 11:54
Bug,HBASE-4315,12520957,RPC logging too verbose in trunk,"IPC logs these at info level in the client, including the shell. Seems more like DEBUG output.
11/08/31 11:10:18 INFO ipc.HBaseRPC: Using org.apache.hadoop.hbase.ipc.WritableRpcEngine for org.apache.hadoop.hbase.ipc.HMasterInterface
11/08/31 11:10:18 INFO ipc.HBaseRPC: Using org.apache.hadoop.hbase.ipc.WritableRpcEngine for org.apache.hadoop.hbase.ipc.HRegionInterface
",tlipcon,tlipcon,Trivial,Closed,Fixed,31/Aug/11 18:13,20/Nov/15 11:54
Bug,HBASE-4325,12521132,Improve error message when using STARTROW for meta scans.,"While in shell, when doing one of these:
{code}
get '.META.','table,d110818d237e63f-b236-4c89-a2c7-5f6342110818'
scan '.META.', {LIMIT => 10 , STARTROW=>'table,d110818d3f72650-4a61-4352-ab49-707b42110818'}
{code}

The call fails with a cryptic message:

{code}
ERROR: org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server hp-node02.phx1.mozilla.com:60020 for region .META.,,1, row 'crash_reports,d110818d237e63f-b236-4c89-a2c7-5f6342110818', but failed after 7 attempts.
Exceptions:
java.io.IOException: java.io.IOException: java.lang.IllegalArgumentException: No 44 in <E9table,d110818d237e63f-b236-4c89-a2c7-5f6342110818��������>, length=43, offset=24
	at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:987)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.convertThrowableToIOE(HRegionServer.java:976)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1632)
	at sun.reflect.GeneratedMethodAccessor30.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:570)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1039)
Caused by: java.lang.IllegalArgumentException: No 44 in <E9table,d110818d237e63f-b236-4c89-a2c7-5f6342110818��������>, length=43, offset=24
	at org.apache.hadoop.hbase.KeyValue.getRequiredDelimiterInReverse(KeyValue.java:1281)
	at org.apache.hadoop.hbase.KeyValue$MetaKeyComparator.compareRows(KeyValue.java:1827)
	at org.apache.hadoop.hbase.KeyValue$KeyComparator.compare(KeyValue.java:1866)
	at org.apache.hadoop.hbase.util.Bytes.binarySearch(Bytes.java:1159)
	at org.apache.hadoop.hbase.io.hfile.HFile$BlockIndex.blockContainingKey(HFile.java:1622)
	at org.apache.hadoop.hbase.io.hfile.HFile$Reader.blockContainingKey(HFile.java:918)
	at org.apache.hadoop.hbase.io.hfile.HFile$Reader$Scanner.seekTo(HFile.java:1300)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seekAtOrAfter(StoreFileScanner.java:136)
	at org.apache.hadoop.hbase.regionserver.StoreFileScanner.seek(StoreFileScanner.java:96)
	at org.apache.hadoop.hbase.regionserver.StoreScanner.<init>(StoreScanner.java:77)
	at org.apache.hadoop.hbase.regionserver.Store.getScanner(Store.java:1345)
	at org.apache.hadoop.hbase.regionserver.HRegion$RegionScanner.<init>(HRegion.java:2276)
	at org.apache.hadoop.hbase.regionserver.HRegion.instantiateInternalScanner(HRegion.java:1131)
	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1123)
	at org.apache.hadoop.hbase.regionserver.HRegion.getScanner(HRegion.java:1107)
	at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2998)
	at org.apache.hadoop.hbase.regionserver.HRegion.get(HRegion.java:2900)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.get(HRegionServer.java:1630)
	... 5 more
{code}

Apparently the solution is to have the proper number of commas in the query.  

{code}
get '.META.','table,d110818d237e63f-b236-4c89-a2c7-5f6342110818,'
scan '.META.', {LIMIT => 10 , STARTROW=>'table,d110818d3f72650-4a61-4352-ab49-707b42110818,'}
{code}

A better actionable error message should be reported.",jmhsieh,jmhsieh,Major,Closed,Fixed,02/Sep/11 01:49,20/Nov/15 11:52
Bug,HBASE-4326,12521136,Tests that use HBaseTestingUtility.startMiniCluster(n) should shutdown with HBaseTestingUtility.shutdownMiniCluster.,"Most tests that use mini clusters use this pattern

{code}
 private final static HBaseTestingUtility UTIL = new HBaseTestingUtility();

  @BeforeClass
  public static void beforeClass() throws Exception {
    UTIL.startMiniCluster(1);
  }

  @AfterClass
  public static void afterClass() throws IOException {
    UTIL.shutdownMiniCluster();
  }
{code}

Some tests (like hbase-4269)
{code}
  @BeforeClass
  public static void beforeClass() throws Exception {
    UTIL.startMiniCluster(1);
  }

  @AfterClass
  public static void afterClass() throws IOException {
    UTIL.getMiniCluster().shutdown();
    // or UTIL.shutdownMiniHBaseCluster();
    // and likely others.
  }
{code}

There is a difference between the two shutdown -- the former deletes files created during the tests while the latter does not.  This funny state persisting (zk or hbase/mr data) may be the cause of strange inter-testcase problems when full suites are run.
",nkeywal,jmhsieh,Major,Closed,Fixed,02/Sep/11 02:19,12/Oct/12 05:34
Bug,HBASE-4327,12521140,Compile HBase against hadoop 0.22,"Pom contains a profile for hadoop-0.20 and one for hadoop-0.23, but not one for hadoop-0.22.

When overriding hadoop.version to 0.22, then the (compile-time) dependency on hadoop-annotations cannot be met.
That exists on 0.23 and 0.24/trunk, but not on 0.22.",jrottinghuis,jrottinghuis,Major,Closed,Fixed,02/Sep/11 02:38,20/Nov/15 11:53
Bug,HBASE-4330,12521254,Fix races in slab cache,A few races are still lingering in the slab cache. Here are some tests and proposed fixes.,li,tlipcon,Major,Closed,Fixed,03/Sep/11 01:02,20/Nov/15 11:55
Bug,HBASE-4331,12521317,Bypassing default actions in prePut fails sometimes with HTable client,"While testing some other scenario I found calling CoprocessorEnvironment.bypass() fails if all trailing puts in a batch are bypassed that way. By extension a single bypassed put will also fail.

The problem is that the puts are removed from the batch in a way that does not align them with the result-status, and in addition the result is never marked as success.

A possible fix is to just mark bypassed puts as SUCCESS and filter them in the following logic.
(I also contemplated a new BYPASSED OperationStatusCode, but that turned out to be not necessary).",larsh,larsh,Major,Closed,Fixed,04/Sep/11 21:18,20/Nov/15 11:55
Bug,HBASE-4333,12521498,Client does not check for holes in .META.,"If there is a temporary hole in .META., the client may get the wrong region from HConnection.locateRegion.  HConnectionManager.HConnectionImplementation.locateRegionInMeta should check the end key of the region found with getClosestRowBefore, just as it checks the offline status, when it looks at the region info.",,jpallas,Major,Closed,Fixed,06/Sep/11 21:47,12/Jun/22 19:16
Bug,HBASE-4334,12521499,HRegion.get never validates row,"If a client gets confused (possibly by a hole in .META., see HBASE-4333), it may send a request to the wrong region.  Paths through put, delete, incrementColumnValue, and checkAndMutate all call checkRow either directly or indirectly (through getLock).  But get apparently does not.  This can result in returning an incorrect empty result instead of a WrongRegionException.",larsh,jpallas,Major,Closed,Fixed,06/Sep/11 21:47,20/Nov/15 11:54
Bug,HBASE-4335,12521500,Splits can create temporary holes in .META. that confuse clients and regionservers,"When a SplitTransaction is performed, three updates are done to .META.:
1. The parent region is marked as splitting (and hence offline)
2. The first daughter region is added (same start key as parent)
3. The second daughter region is added (split key is start key)
(later, the original parent region is deleted, but that's not important to this discussion)

Steps 2 and 3 are actually done concurrently by SplitTransaction.DaughterOpener threads.  While the master is notified when a split is complete, the only visibility that clients have is whether the daughter regions have appeared in .META.

If the second daughter is added to .META. first, then .META. will contain the (offline) parent region followed by the second daughter region.  If the client looks up a key that is greater than (or equal to) the split, the client will find the second daughter region and use it.  If the key is less than the split key, the client will find the parent region and see that it is offline, triggering a retry.

If the first daughter is added to .META. before the second daughter, there is a window during which .META. has a hole: the first daughter effectively hides the parent region (same start key), but there is no entry for the second daughter.  A region lookup will find the first daughter for all keys in the parent's range, but the first daughter does not include keys at or beyond the split key.

See HBASE-4333 and HBASE-4334 for details on how this causes problems and suggestions for mitigating this in the client and regionserver.
",larsh,jpallas,Critical,Closed,Fixed,06/Sep/11 21:47,20/Nov/15 11:53
Bug,HBASE-4338,12521519,Package build for rpm and deb are broken,"Environment variable final.name was removed in HBASE-3629, and this prevents rpm/deb packaging from building.",eyang,eyang,Major,Closed,Fixed,07/Sep/11 01:27,20/Nov/15 11:54
Bug,HBASE-4340,12521529,Hbase can't balance if ServerShutdownHandler encountered exception,"Version: 0.90.4
Cluster : 40 boxes
As I saw below logs. It said that balance couldn't work because of a dead RS.
I dug deeply and found two issues:

1.       shutdownhandler didn't clear numProcessing deal with some exceptions. It seems whatever exceptions we should clear the flag or close master.

2.       ""dead regionserver(s): [158-1-130-12,20020,1314971097929]"" is inaccurate. The dead sever should be "" 158-1-130-10,20020,1315068597979""

//master logs:
2011-09-05 00:28:00,487 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 00:33:00,489 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 00:38:00,493 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 00:43:00,495 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 00:48:00,499 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 00:53:00,501 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 00:58:00,501 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:03:00,502 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:08:00,506 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:13:00,508 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:18:00,512 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:23:00,514 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:28:00,518 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:33:00,520 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:38:00,524 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:43:00,526 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:48:00,530 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:53:00,532 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 01:58:00,536 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 02:03:00,537 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 02:08:00,538 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 02:13:00,539 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]
2011-09-05 02:18:00,543 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because processing dead regionserver(s): [158-1-130-12,20020,1314971097929]

// the exception logs :.
2011-09-03 18:13:27,550 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=158-1-133-11,20020,1315069437236, region=0db4088d75c58dd22f93f389d90ba6cc
2011-09-03 18:13:27,550 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event M_SERVER_SHUTDOWN java.lang.NullPointerException
         at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:480)
         at org.apache.hadoop.hbase.util.Bytes.toLong(Bytes.java:454)
         at org.apache.hadoop.hbase.catalog.MetaReader.metaRowToRegionPairWithInfo(MetaReader.java:400)
         at org.apache.hadoop.hbase.catalog.MetaReader.getServerUserRegions(MetaReader.java:591)
         at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:176)
         at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
         at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
         at java.lang.Thread.run(Thread.java:662)
2011-09-03 18:13:27,550 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Splitting logs for 158-1-134-15,20020,1315065238916
2011-09-03 18:13:27,566 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: No previous transition plan was found (or we are ignoring an existing plan) for ufdr,001146,1314955304624.22f6d43e78c903196f206881fc149488. so generated a random one; hri=ufdr,001146,1314955304624.22f6d43e78c903196f206881fc149488., src=, dest=158-1-132-17,20020,1315069441916; 31 (online=31, exclude=null) available servers
201
",sunnygao,sunnygao,Major,Closed,Fixed,07/Sep/11 02:26,20/Nov/15 11:53
Bug,HBASE-4341,12521531,HRS#closeAllRegions should take care of HRS#onlineRegions's weak consistency,"This's the reason of why did ""https://builds.apache.org/job/hbase-0.90/282"" get failure . In this test, one case was timeout and cause the whole test process got killed.

[logs]
Here's the related logs(From org.apache.hadoop.hbase.mapreduce.TestTableMapReduce-output.txt):
{noformat}
2011-08-31 10:09:01,089 INFO  [RegionServer:0;vesta.apache.org,52257,1314785332968.leaseChecker] regionserver.Leases(124): RegionServer:0;vesta.apache.org,52257,1314785332968.leaseChecker closing leases
2011-08-31 10:09:01,089 INFO  [RegionServer:0;vesta.apache.org,52257,1314785332968.leaseChecker] regionserver.Leases(131): RegionServer:0;vesta.apache.org,52257,1314785332968.leaseChecker closed leases
2011-08-31 10:09:01,403 INFO  [RegionServer:0;vesta.apache.org,52257,1314785332968] regionserver.HRegionServer(709): Waiting on 1 regions to close
2011-08-31 10:09:01,403 DEBUG [RegionServer:0;vesta.apache.org,52257,1314785332968] regionserver.HRegionServer(713): {74a7a8befdf9561dc1d90c4241afeac7=mrtest,uuu,1314785328546.74a7a8befdf9561dc1d90c4241afeac7.}
2011-08-31 10:09:01,697 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
2011-08-31 10:09:02,697 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
2011-08-31 10:09:03,008 INFO  [vesta.apache.org:50036.timeoutMonitor] hbase.Chore(79): vesta.apache.org:50036.timeoutMonitor exiting
2011-08-31 10:09:03,697 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
2011-08-31 10:09:04,697 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
2011-08-31 10:09:05,698 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
2011-08-31 10:09:06,698 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
2011-08-31 10:09:07,698 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
{noformat}
[Analysis]
One region was opened during the RS's stopping. 
This is method of ""HRS#closeAllRegions"":
{noformat}

  protected void closeAllRegions(final boolean abort) {
    closeUserRegions(abort);
    -------------------------
    if (meta != null) closeRegion(meta.getRegionInfo(), abort, false);
    if (root != null) closeRegion(root.getRegionInfo(), abort, false);
  }
{noformat}

HRS#onlineRegions is a ConcurrentHashMap. So walk down this map may not get all the data if some entries are been added during the traverse. Once one region was missed, it can't be closed anymore. And this regionserver will not be stopped normally. Then the following logs occurred:
{noformat}
2011-08-31 10:09:01,403 INFO  [RegionServer:0;vesta.apache.org,52257,1314785332968] regionserver.HRegionServer(709): Waiting on 1 regions to close
2011-08-31 10:09:01,403 DEBUG [RegionServer:0;vesta.apache.org,52257,1314785332968] regionserver.HRegionServer(713): {74a7a8befdf9561dc1d90c4241afeac7=mrtest,uuu,1314785328546.74a7a8befdf9561dc1d90c4241afeac7.}
2011-08-31 10:09:01,697 INFO  [Master:0;vesta.apache.org:50036] master.ServerManager(465): Waiting on regionserver(s) to go down vesta.apache.org,52257,1314785332968
{noformat}",jeason,jeason,Major,Closed,Fixed,07/Sep/11 02:42,20/Nov/15 11:54
Bug,HBASE-4350,12521963,Fix a Bloom filter bug introduced by HFile v2 and TestMultiColumnScanner that caught it,"Nicolas pointed out to me that the new unit test TestMultiColumnScanner that I wrote for the multi-column scanner Bloom filter optimization (which we will soon release) did not pass on the open-source trunk, and it bisected down to the HFile v2 commit. I debugged the unit test and found that there was a serious bug in HFile v2 Bloom filter lookup not caught by any of the existing unit tests: Bloom filters were used for ""non-Get"" Scans, which did not have minimum/maximum row set correctly, and some scan results were not returned.

This diff is the unit test that helped catch the problem and a one-line fix for the bug.
",mikhail,mikhail,Major,Closed,Fixed,08/Sep/11 04:43,20/Nov/15 11:55
Bug,HBASE-4351,12522223,"If from Admin we try to unassign a region forcefully, though a valid region name is given the master is not able to identify the region to unassign.","The following is the problem
Get the exact region name from UI and call
HBaseAdmin.unassign(regionname, true).
Here true is forceful option.
As part of unassign api
{code}
  public void unassign(final byte [] regionName, final boolean force)
  throws IOException {
    Pair<HRegionInfo, HServerAddress> pair =
      MetaReader.getRegion(this.catalogTracker, regionName);
    if (pair == null) throw new UnknownRegionException(Bytes.toStringBinary(regionName));
    HRegionInfo hri = pair.getFirst();
    if (force) this.assignmentManager.clearRegionFromTransition(hri);
    this.assignmentManager.unassign(hri, force);
  }
{code}
As part of clearRegionFromTransition()
{code}
    synchronized (this.regions) {
      this.regions.remove(hri);
      for (Set<HRegionInfo> regions : this.servers.values()) {
        regions.remove(hri);
      }
    }
{code}
the region is also removed.  Hence when the master tries to identify the region
{code}
      if (!regions.containsKey(region)) {
        debugLog(region, ""Attempted to unassign region "" +
          region.getRegionNameAsString() + "" but it is not "" +
          ""currently assigned anywhere"");
        return;
      }
{code}
It is not able to identify the region.  It exists in trunk and 0.90.x also.",ram_krish,ram_krish,Major,Closed,Fixed,08/Sep/11 07:22,20/Nov/15 11:53
Bug,HBASE-4357,12522341,Region stayed in transition - in closing state,"Got the following during testing, 

1. On a given machine, kill ""RS process id"". Then kill ""HMaster process id"".
2. Start RS first via ""bin/hbase-daemon.sh --config ./conf start regionserver."". Start HMaster via ""bin/hbase-daemon.sh --config ./conf start master"".

One region of a table stayed in closing state.

According to zookeeper,
794a6ff17a4de0dd0a19b984ba18eea9 miweng_500region,H\xB49X\x10bM\xB1,1315338786464.794a6ff17a4de0dd0a19b984ba18eea9. state=CLOSING, ts=Wed Sep 07 17:21:44 PDT 2011 (75701s ago), server=sea-esxi-0,60000,1315428682281 

According to .META. table, the region has been assigned to from sea-esxi-0 to sea-esxi-4.

miweng_500region,H\xB49X\x10bM\xB1,1315338786464.794a6ff17a4de0dd0a19b984ba18eea9. sea-esxi-4:60030  H\xB49X\x10bM\xB1 I7K\xC6\xA7\xEF\x9D\x90 0 ",mingma,mingma,Major,Closed,Fixed,08/Sep/11 22:00,20/Nov/15 11:52
Bug,HBASE-4363,12522485,[replication] ReplicationSource won't close if failing to contact the sink,"When trying to close a source, it will hang if it's already in shipEdits() and has issues reaching the sink. The reason is that in that method the while loop only checks if the RS is going down but not if the source was asked to shutdown.",jdcryans,jdcryans,Major,Closed,Fixed,09/Sep/11 22:47,20/Nov/15 11:52
Bug,HBASE-4367,12522638,Deadlock in MemStore flusher due to JDK internally synchronizing on current thread,"We observed a deadlock in production between the following threads:
- IPC handler thread holding the monitor lock on MemStoreFlusher inside reclaimMemStoreMemory, waiting to obtain MemStoreFlusher.lock (the reentrant lock member)
- cacheFlusher thread inside flushRegion holds MemStoreFlusher.lock, and then calls PriorityCompactionQueue.add, which calls PriorityCompactionQueue.addToRegionsInQueue, which calls CompactionRequest.toString(), which calls Date.toString. If this occurs just after a GC under memory pressure, Date.toString needs to reload locale information (stored in a soft reference), so it calls ResourceBundle.loadBundle, which uses Thread.currentThread() as a synchronizer (see sun bug http://bugs.sun.com/view_bug.do?bug_id=6915621). Since the current thread is the MemStoreFlusher itself, we have a lock order inversion and a deadlock.",tlipcon,tlipcon,Critical,Closed,Fixed,10/Sep/11 08:13,28/May/20 19:58
Bug,HBASE-4378,12522836,[hbck] Does not complain about regions with startkey==endkey.,hbck doesn't seem to complain or have an error condition if there is a region where startkey==endkey.,jmhsieh,jmhsieh,Major,Closed,Fixed,13/Sep/11 01:49,20/Nov/15 11:55
Bug,HBASE-4379,12522837,"[hbck] Does not complain about tables with no end region [Z,]",hbck does not detect or have an error condition when the last region of a table is missing (end key != '').,anoopsamjohn,jmhsieh,Major,Closed,Fixed,13/Sep/11 01:51,18/Sep/13 23:18
Bug,HBASE-4383,12522856,SlabCache reports negative heap sizes,"2011-09-13 00:36:17,734 INFO org.apache.hadoop.hbase.io.hfile.slab.SlabCache: Request Stats
2011-09-13 00:36:17,734 INFO org.apache.hadoop.hbase.io.hfile.slab.SingleSizeCache: For Slab of size 72089: 0 occupied, out of a capacity of 226398 blocks. HeapSize is -798.5m bytes., churnTime=0sec
2011-09-13 00:36:17,734 INFO org.apache.hadoop.hbase.io.hfile.slab.SingleSizeCache: For Slab of size 137625: 0 occupied, out of a capacity of 29647 blocks. HeapSize is -202.1m bytes., churnTime=0sec
2011-09-13 00:36:17,735 INFO org.apache.hadoop.hbase.io.hfile.slab.SlabCache: Current heap size is: -1000.7m
2011-09-13 00:36:17,735 INFO org.apache.hadoop.hbase.io.hfile.slab.SlabCache: Successfully Cached Stats
2011-09-13 00:36:17,735 INFO org.apache.hadoop.hbase.io.hfile.slab.SingleSizeCache: For Slab of size 72089: 0 occupied, out of a capacity of 226398 blocks. HeapSize is -798.5m bytes., churnTime=0sec
2011-09-13 00:36:17,735 INFO org.apache.hadoop.hbase.io.hfile.slab.SingleSizeCache: For Slab of size 137625: 0 occupied, out of a capacity of 29647 blocks. HeapSize is -202.1m bytes., churnTime=0sec
2011-09-13 00:36:17,735 INFO org.apache.hadoop.hbase.io.hfile.slab.SlabCache: Current heap size is: -1000.7m
",li,tlipcon,Major,Closed,Fixed,13/Sep/11 07:41,20/Nov/15 11:55
Bug,HBASE-4386,12522913,NPE in TaskMonitor,"Saw the following hitting /rs-status
<pre>    INTERNAL_SERVER_ERROR</pre></p><h3>Caused by:</h3><pre>java.lang.NullPointerException
        at org.apache.hadoop.hbase.monitoring.TaskMonitor.purgeExpiredTasks(TaskMonitor.java:97)
        at org.apache.hadoop.hbase.monitoring.TaskMonitor.getTasks(TaskMonitor.java:127)
        at org.apache.hbase.tmpl.common.TaskMonitorTmplImpl.renderNoFlush(TaskMonitorTmplImpl.java:50)
        at org.apache.hbase.tmpl.common.TaskMonitorTmpl.renderNoFlush(TaskMonitorTmpl.java:170)
        at org.apache.hbase.tmpl.regionserver.RSStatusTmplImpl.renderNoFlush(RSStatusTmplImpl.java:70)
        at org.apache.hbase.tmpl.regionserver.RSStatusTmpl.renderNoFlush(RSStatusTmpl.java:176)
        at org.apache.hbase.tmpl.regionserver.RSStatusTmpl.render(RSStatusTmpl.java:167)
        at org.apache.hadoop.hbase.regionserver.RSStatusServlet.doGet(RSStatusServlet.java:48)
",tlipcon,tlipcon,Critical,Closed,Fixed,13/Sep/11 15:53,20/Nov/15 11:54
Bug,HBASE-4387,12522920,Error while syncing: DFSOutputStream is closed,"In a billion-row load on ~25 servers, I see ""error while syncing"" reasonable often with the error ""DFSOutputStream is closed"" around a roll. We have some race where a roll at the same time as heavy inserts causes a problem.",larsh,tlipcon,Critical,Closed,Fixed,13/Sep/11 16:13,20/Nov/15 11:52
Bug,HBASE-4388,12522935,Second start after migration from 90 to trunk crashes,"I started a trunk cluster to upgrade from 90, inserted a ton of data, then did a clean shutdown. When I started again, I got the following exception:

11/09/13 12:29:09 INFO master.HMaster: Meta has HRI with HTDs. Updating meta now.
11/09/13 12:29:09 FATAL master.HMaster: Unhandled exception. Starting shutdown.
java.lang.NegativeArraySizeException: -102
        at org.apache.hadoop.hbase.util.Bytes.readByteArray(Bytes.java:147)
        at org.apache.hadoop.hbase.HTableDescriptor.readFields(HTableDescriptor.java:606)
        at org.apache.hadoop.hbase.migration.HRegionInfo090x.readFields(HRegionInfo090x.java:641)
        at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:133)
        at org.apache.hadoop.hbase.util.Writables.getWritable(Writables.java:103)
        at org.apache.hadoop.hbase.util.Writables.getHRegionInfoForMigration(Writables.java:228)
        at org.apache.hadoop.hbase.catalog.MetaEditor.getHRegionInfoForMigration(MetaEditor.java:350)
        at org.apache.hadoop.hbase.catalog.MetaEditor$1.visit(MetaEditor.java:273)
        at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:633)
        at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:255)
        at org.apache.hadoop.hbase.catalog.MetaReader.fullScan(MetaReader.java:235)
        at org.apache.hadoop.hbase.catalog.MetaEditor.updateMetaWithNewRegionInfo(MetaEditor.java:284)
        at org.apache.hadoop.hbase.catalog.MetaEditor.migrateRootAndMeta(MetaEditor.java:298)
        at org.apache.hadoop.hbase.master.HMaster.updateMetaWithNewHRI(HMaster.java:529)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:472)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:309)
",stack,tlipcon,Blocker,Closed,Fixed,13/Sep/11 19:30,20/Nov/15 11:55
Bug,HBASE-4390,12522955,[replication] ReplicationSource's UncaughtExceptionHandler shouldn't join,"From Jeff Whiting on the ML:

{quote}
""regionserver60020.replicationSource,dev2"" daemon prio=10 tid=0x00002aaaf0312800 nid=0x69f8 in Object.wait() [0x000000004533e000]
  java.lang.Thread.State: TIMED_WAITING (on object monitor)
   at java.lang.Object.wait(Native Method)
   - waiting on <0x00002aaab12464c0> (a org.apache.hadoop.hbase.replication.regionserver.ReplicationSource)
   at java.lang.Thread.join(Thread.java:1151)
   - locked <0x00002aaab12464c0> (a org.apache.hadoop.hbase.replication.regionserver.ReplicationSource)
   at org.apache.hadoop.hbase.util.Threads.shutdown(Threads.java:91)
   at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource.terminate(ReplicationSource.java:649)
   at org.apache.hadoop.hbase.replication.regionserver.ReplicationSource$1.uncaughtException(ReplicationSource.java:628)
   at java.lang.Thread.dispatchUncaughtException(Thread.java:1831)
{quote}

That's pretty dumb, the thread is trying to join itself. UncaughtExceptionHandler shouldn't try to terminate() but just clear resources and then return.",jdcryans,jdcryans,Major,Closed,Fixed,13/Sep/11 20:58,20/Nov/15 11:54
Bug,HBASE-4395,12522962,EnableTableHandler races with itself,"Very often when we try to enable a big table we get something like:

{quote}
2011-09-02 12:21:56,619 FATAL org.apache.hadoop.hbase.master.HMaster: Unexpected state trying to OFFLINE; huge_ass_region_name state=PENDING_OPEN, ts=1314991316616
java.lang.IllegalStateException
        at org.apache.hadoop.hbase.master.AssignmentManager.setOfflineInZooKeeper(AssignmentManager.java:1074)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1030)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:858)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:838)
        at org.apache.hadoop.hbase.master.handler.EnableTableHandler$BulkEnabler$1.run(EnableTableHandler.java:154)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
2011-09-02 12:21:56,620 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
{quote}

The issue is that EnableTableHandler calls multiple BulkEnabler and it's possible that by the time it calls it a second time, using a stale list of still-not-enabled regions, that it tries to set one region offline in ZK but just after its state changed. Case in point:

{quote}
2011-09-02 12:21:56,616 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region huge_ass_region_name to sv4r23s16,60020,1314880035029
2011-09-02 12:21:56,619 FATAL org.apache.hadoop.hbase.master.HMaster: Unexpected state trying to OFFLINE; huge_ass_region_name state=PENDING_OPEN, ts=1314991316616
{quote}

Here the first line is the first assign done in the first thread, and the second line is the second thread that got to process the same region around the same time. 3ms difference in time. After that, the master dies, and it's pretty sad when it restarts because it failovers an enabling table and it's ungodly slow.

I'm pretty sure there's a window where double assignment are possible.

Talking with Stack, it doesn't really make sense to call multiple enables since the list of regions is static (the table is disabled!). We should just call it and wait. Also there's a lot of cleanup to do in EnableTableHandler since it refers to disabling the table (copy pasta I guess).",jdcryans,jdcryans,Blocker,Closed,Fixed,13/Sep/11 21:47,20/Nov/15 11:56
Bug,HBASE-4397,12522979,"-ROOT-, .META. tables stay offline for too long in recovery phase after all RSs are shutdown at the same time","1. Shutdown all RSs.
2. Bring all RS back online.

The ""-ROOT-"", "".META."" stay in offline state until timeout monitor force assignment 30 minutes later. That is because HMaster can't find a RS to assign the tables to in assign operation.


011-09-13 13:25:52,743 WARN org.apache.hadoop.hbase.master.AssignmentManager: Failed assignment of -ROOT-,,0.70236052 to sea-lab-4,60020,1315870341387, trying to assign elsewhere instead; retry=0
java.net.ConnectException: Connection refused
        at sun.nio.ch.SocketChannelImpl.checkConnect(Native Method)
        at sun.nio.ch.SocketChannelImpl.finishConnect(SocketChannelImpl.java:567)
        at org.apache.hadoop.net.SocketIOWithTimeout.connect(SocketIOWithTimeout.java:206)
        at org.apache.hadoop.net.NetUtils.connect(NetUtils.java:373)
        at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.setupIOstreams(HBaseClient.java:345)
        at org.apache.hadoop.hbase.ipc.HBaseClient.getConnection(HBaseClient.java:1002)
        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:854)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:148)
        at $Proxy9.openRegion(Unknown Source)
        at org.apache.hadoop.hbase.master.ServerManager.sendRegionOpen(ServerManager.java:407)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1408)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1153)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1128)
        at org.apache.hadoop.hbase.master.AssignmentManager.assign(AssignmentManager.java:1123)
        at org.apache.hadoop.hbase.master.AssignmentManager.assignRoot(AssignmentManager.java:1788)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.verifyAndAssignRoot(ServerShutdownHandler.java:100)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.verifyAndAssignRootWithRetries(ServerShutdownHandler.java:118)
        at org.apache.hadoop.hbase.master.handler.ServerShutdownHandler.process(ServerShutdownHandler.java:181)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
2011-09-13 13:25:52,743 WARN org.apache.hadoop.hbase.master.AssignmentManager: Unable to find a viable location to assign region -ROOT-,,0.70236052



Possible fixes:

1. Have serverManager handle ""server online"" event similar to how RegionServerTracker.java calls servermanager.expireServer in the case server goes down.
2. Make timeoutMonitor handle the situation better. This is a special situation in the cluster. 30 minutes timeout can be skipped.",mingma,mingma,Major,Closed,Fixed,14/Sep/11 00:25,12/Oct/12 05:35
Bug,HBASE-4398,12522982,"If HRegionPartitioner is used in MapReduce, client side configurations are overwritten by hbase-site.xml.","If HRegionPartitioner is used in MapReduce, client side configurations are overwritten by hbase-site.xml.

We can reproduce the problem by the following instructions:

{noformat}
- Add HRegionPartitioner.class to the 4th argument of

TableMapReduceUtil#initTableReducerJob()

at line around 133
in src/test/java/org/apache/hadoop/hbase/mapreduce/TestTableMapReduce.java

- Change or remove ""hbase.zookeeper.property.clientPort"" property

in hbase-site.xml ( for example, changed to 12345 ).

- run testMultiRegionTable()
{noformat}

Then I got error messages as following:

{noformat}
2011-09-12 22:28:51,020 DEBUG [Thread-832] zookeeper.ZKUtil(93): hconnection opening connection to ZooKeeper with ensemble (localhost:12345)
2011-09-12 22:28:51,022 INFO  [Thread-832] zookeeper.RecoverableZooKeeper(89): The identifier of this process is 43200@imac.local
2011-09-12 22:28:51,123 WARN  [Thread-832] zookeeper.RecoverableZooKeeper(161): Possibly transient ZooKeeper exception: org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/master
2011-09-12 22:28:51,123 INFO  [Thread-832] zookeeper.RecoverableZooKeeper(173): The 1 times to retry ZooKeeper after sleeping 1000 ms

 =====

2011-09-12 22:29:02,418 ERROR [Thread-832] mapreduce.HRegionPartitioner(125): java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@2e54e48d closed
2011-09-12 22:29:02,422 WARN  [Thread-832] mapred.LocalJobRunner$Job(256): job_local_0001
java.lang.NullPointerException
       at org.apache.hadoop.hbase.mapreduce.HRegionPartitioner.setConf(HRegionPartitioner.java:128)
       at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:62)
       at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:117)
       at org.apache.hadoop.mapred.MapTask$NewOutputCollector.<init>(MapTask.java:527)
       at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:613)
       at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
       at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
{noformat}

I think HTable should connect to ZooKeeper at port 21818 configured at client side instead of 12345 in hbase-site.xml
and It might be caused by ""HBaseConfiguration.addHbaseResources(conf);"" in HRegionPartitioner#setConf(Configuration).

And this might mean that all of client side configurations, also configured in hbase-site.xml, are overwritten caused by this problem.
",ueshin,ueshin,Major,Closed,Fixed,14/Sep/11 01:15,03/Apr/13 05:54
Bug,HBASE-4400,12522999,.META. getting stuck if RS hosting it is dead and znode state is in RS_ZK_REGION_OPENED,"Start 2 RS.
The .META. is being hosted by RS2 but while processing it goes down.

Now restart the master and RS1.  Master gets the RS name from the znode in RS_ZK_REGION_OPENED.  But as RS2 is not online still the master is not able to process the META at all.  Please find the logs
{noformat}
2011-09-14 16:43:51,949 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=linux76,60020,1315998828523, region=70236052/-ROOT-
2011-09-14 16:43:51,968 INFO org.apache.hadoop.hbase.master.HMaster: -ROOT- assigned=1, rit=false, location=linux76:60020
2011-09-14 16:43:51,970 INFO org.apache.hadoop.hbase.master.AssignmentManager: Processing region .META.,,1.1028785192 in state RS_ZK_REGION_OPENED
2011-09-14 16:43:51,970 INFO org.apache.hadoop.hbase.master.AssignmentManager: Failed to find linux146,60020,1315998414623 in list of online servers; skipping registration of open of .META.,,1.1028785192
2011-09-14 16:43:51,971 INFO org.apache.hadoop.hbase.master.AssignmentManager: Waiting on 1028785192/.META.
2011-09-14 16:43:51,983 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENED, server=linux76,60020,1315998828523, region=70236052/-ROOT-
2011-09-14 16:43:51,986 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Handling OPENED event for 70236052; deleting unassigned node
2011-09-14 16:43:51,986 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x13267854032001d Deleting existing unassigned node for 70236052 that is in expected state RS_ZK_REGION_OPENED
2011-09-14 16:43:51,998 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x13267854032001d Successfully deleted unassigned node for region 70236052 in expected state RS_ZK_REGION_OPENED
2011-09-14 16:43:51,999 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region -ROOT-,,0.70236052 on linux76,60020,1315998828523
2011-09-14 16:44:00,945 INFO org.apache.hadoop.hbase.master.ServerManager: Registering server=linux146,60020,1315998839724, regionCount=0, userLoad=false
2011-09-14 16:46:20,003 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  .META.,,1.1028785192 state=OPEN, ts=0
2011-09-14 16:46:20,004 ERROR org.apache.hadoop.hbase.master.AssignmentManager: Region has been OPEN for too long, we don't know where region was opened so can't do anything
{noformat}

{code}
        regionsInTransition.put(encodedRegionName, new RegionState(
            regionInfo, RegionState.State.OPEN, data.getStamp()));
          ................
        } else {
          HServerInfo hsi = this.serverManager.getServerInfo(sn);
          if (hsi == null) {
            LOG.info(""Failed to find "" + sn +
              "" in list of online servers; skipping registration of open of "" +
              regionInfo.getRegionNameAsString());
          } else {
            new OpenedRegionHandler(master, this, regionInfo, hsi).process();
          }
        }
{code}
So timeout monitor is not able to do anything here
{code}
          LOG.error(""Region has been OPEN for too long, "" +
          ""we don't know where region was opened so can't do anything"");
          synchronized(regionState) {
            regionState.update(regionState.getState());
          }
{code}",ram_krish,ram_krish,Major,Closed,Fixed,14/Sep/11 05:35,20/Nov/15 11:53
Bug,HBASE-4402,12523007,Retaining locality after restart broken,"In DefaultLoadBalancer, we implement the ""retain assignment"" function like so:
{code}
      if (sn != null && servers.contains(sn)) {
        assignments.get(sn).add(region.getKey());
{code}

but this will never work since after a cluster restart, all servers have a new ServerName with a new startcode.",tlipcon,tlipcon,Blocker,Closed,Fixed,14/Sep/11 07:19,20/Nov/15 11:52
Bug,HBASE-4406,12523090,TestOpenRegionHandler failing after HBASE-4287,The separate test cases in the file are interfering with each other by using the same region.,tlipcon,tlipcon,Critical,Closed,Fixed,14/Sep/11 19:27,20/Nov/15 11:54
Bug,HBASE-4412,12523118,No need to retry scan operation on the same server in case of RegionServerStoppedException,"ScannerCallable.java doesn't retry on the same server in case of NotServingRegionException.

        if (ioe instanceof NotServingRegionException) {
          // Throw a DNRE so that we break out of cycle of calling NSRE
          // when what we need is to open scanner against new location.
          // Attach NSRE to signal client that it needs to resetup scanner.
          throw new DoNotRetryIOException(""Reset scanner"", ioe);


In the scenario when region server is in shutdown process, RegionServerStoppedException will be thrown. ScannerCallable still retry. It isn't necessary.",mingma,mingma,Major,Closed,Fixed,14/Sep/11 22:24,20/Nov/15 11:54
Bug,HBASE-4414,12523136,Region splits by size not being triggered in at least some cases,"We seem to have lost the triggering of region splits by size somewhere in trunk.

Running a simple test to load data only:

1. create 'usertable', 'f1' in hbase shell
2. run a YCSB load of 10M records

I wind up with a single region containing all records, around 13GB, despite max region size being configured to 640MB.
{noformat}
    ip-10-160-217-155.us-west-1.compute.internal:8120 1316045713501
        requestsPerSecond=0, numberOfOnlineRegions=1, usedHeapMB=1544, maxHeapMB=2962
        usertable,,1316045755455.1e11a9f71072113258942e03dabaa468.
            numberOfStores=1, numberOfStorefiles=16, storefileUncompressedSizeMB=13611, storefileSizeMB=13621, compressionRatio=1.0007, memstoreSizeMB=50, storefileIndexSizeMB=0, readRequestsCount=0, writeRequestsCount=1930, rootIndexSizeKB=108, totalStaticIndexSizeKB=10511, totalStaticBloomSizeKB=0, totalCompactingKVs=3356000, currentCompactedKVs=3356000, compactionProgressPct=1.0
{noformat}

As best I can tell, the changes introduced in HBASE-3797 and HBASE-1476 dropped some cases where we were triggering region splits when we didn't compact.
",ghelmling,ghelmling,Blocker,Closed,Fixed,15/Sep/11 01:00,20/Nov/15 11:54
Bug,HBASE-4417,12523256,HBaseAdmin.checkHBaseAvailable() doesn't close ZooKeeper connections,"HBaseAdmin.checkHBaseAvailable(conf) clones the passed connection, later in HBaseAdmin constructor the connection is cloned again. Thus a new HConnection object with ZooKeeper connections is created.",seelmann,seelmann,Minor,Closed,Fixed,15/Sep/11 21:46,20/Nov/15 11:53
Bug,HBASE-4420,12523378,MasterObserver preMove() and postMove() should throw IOException instead of UnknownRegionException,"We've standardized on IOException as the main way for coprocessors to communicate errors back out of the Observer hooks.  All Observer hooks throw IOE except for MasterObserver.preMove() and MasterObserver.postMove(), which throw UnknownRegionException, since that's what HMasterInterface.move() declares.  In hindsight, making these two MasterObserver methods inconsistent seems like a mistake.

I think we should change MasterObserver.preMove() and MasterObserver.postMove() to throw IOException for consistency with the other methods.  We could deprecate the existing HMasterInterface.move() method to have it switch over to throwing IOException as well, but this would require creating a version with a new name, which seems unnecessarily ugly.  So I'd suggest we just have HMaster.move() handle the IOException and use it to init an UnknownRegionException.  Wonky as that is, it seems the lesser evil.",ghelmling,ghelmling,Major,Closed,Fixed,16/Sep/11 18:39,20/Nov/15 11:53
Bug,HBASE-4423,12523388,HBASE-4238 broke TestCatalogJanitor#testCleanParent test,"This issue was spotted by Ram over in HBASE-4238.  Over in that issue he suggested this fix:

{code}
diff --git a/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java b/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
index 47a1c21..028425a 100644
--- a/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
+++ b/src/test/java/org/apache/hadoop/hbase/master/TestCatalogJanitor.java
@@ -218,14 +218,6 @@ public class TestCatalogJanitor {
     assertFalse(janitor.cleanParent(parent, r));
     // Remove the reference file and try again.
     assertTrue(fs.delete(p, true));
-    // We will fail!!! Because split b is empty, which is right... we should
-    // not remove parent if daughters do not exist in fs.
-    assertFalse(janitor.cleanParent(parent, r));
-    // Put in place daughter dir for b... that should make it so parent gets
-    // cleaned up.
-    storedir = Store.getStoreHomedir(tabledir, splitb.getEncodedName(),
-      htd.getColumnFamilies()[0].getName());
-    assertTrue(fs.mkdirs(storedir));
     assertTrue(janitor.cleanParent(parent, r));
   }
-}

{code}

HBASE-4238 changed our behavior on rollback.. we leave the regiondir in place so the above test was failing.",ram_krish,stack,Major,Closed,Fixed,16/Sep/11 20:38,20/Nov/15 11:54
Bug,HBASE-4428,12523428,Two methods in CacheTestUtils don't call setDaemon() on the threads,"hammerSingleKey() and hammerEviction() don't call t.setDaemon(true) on the threads they create.
The correct pattern is:
{code}
      t.setDaemon(true);
      ctx.addThread(t);
{code}",yuzhihong@gmail.com,yuzhihong@gmail.com,Major,Closed,Fixed,17/Sep/11 13:00,12/Jun/22 19:23
Bug,HBASE-4444,12523626,Miscommit of  HBASE-4195 to 0.90 branch (committed as though it were hbase-4423),"See here: http://svn.apache.org/viewvc?view=revision&revision=1171766 (Or search on the list for message titled 'SVN commit branches/0.90@1171766').

",stack,stack,Major,Closed,Fixed,19/Sep/11 21:35,12/Jun/22 19:32
Bug,HBASE-4445,12523634,Not passing --config when checking if distributed mode or not,"If config other than under hbase and we set distributed mode, we were not passing the config to our little property value setter",stack,stack,Major,Closed,Fixed,19/Sep/11 23:04,20/Nov/15 11:53
Bug,HBASE-4446,12523637,"Rolling restart RSs scenario, regions could stay in OPENING state","Keep Master up all the time, do rolling restart of RSs like this - stop RS1, wait for 2 seconds, stop RS2, start RS1, wait for 2 seconds, stop RS3, start RS2, wait for 2 seconds, etc. Region sometimes can just stay in OPENING state even after timeoutmonitor period.


2011-09-19 08:10:33,131 WARN org.apache.hadoop.hbase.master.AssignmentManager: While timing out a region in state OPENING, found ZK node in unexpected state: RS_ZK_REGION_FAILED_OPEN

The issue - RS was shutdown when a region is being opened, it was transitioned to RS_ZK_REGION_FAILED_OPEN in ZK. In timeoutmonitor, it didn't take care of RS_ZK_REGION_FAILED_OPEN.

processOpeningState
...
   else if (dataInZNode.getEventType() != EventType.RS_ZK_REGION_OPENING &&
        LOG.warn(""While timing out a region in state OPENING, ""
            + ""found ZK node in unexpected state: ""
            + dataInZNode.getEventType());
        return;
      }

",mingma,mingma,Major,Closed,Fixed,19/Sep/11 23:22,20/Nov/15 11:52
Bug,HBASE-4449,12523766,LoadIncrementalHFiles should be able to handle CFs with blooms,"When LoadIncrementalHFiles loads a store file that crosses region boundaries, it will split the file at the boundary to create two store files. If the store file is for a column family that has a bloom filter, then a ""java.lang.ArithmeticException: / by zero"" will be raised because ByteBloomFilter() is called with maxKeys of 0.

The included patch assumes that the number of keys in each split child will be equal to the number of keys in the parent's bloom filter (instead of 0). This is an overestimate, but it's safe and easy.",dave_revell,dave_revell,Major,Closed,Fixed,20/Sep/11 18:38,20/Nov/15 11:53
Bug,HBASE-4452,12523865,Possibility of RS opening a region though tickleOpening fails due to znode version mismatch,"Consider the following code
{code}
    long period = Math.max(1, assignmentTimeout/ 3);
    long lastUpdate = now;
    while (!signaller.get() && t.isAlive() && !this.server.isStopped() &&
        !this.rsServices.isStopping() && (endTime > now)) {
      long elapsed = now - lastUpdate;
      if (elapsed > period) {
        // Only tickle OPENING if postOpenDeployTasks is taking some time.
        lastUpdate = now;
        tickleOpening(""post_open_deploy"");
      }
{code}
Whenever the postopenDeploy tasks takes considerable time we try to tickleOpening so that there is no timeout deducted.  But before it could do this if the TimeoutMonitor tries to assign the node to another RS then the other RS will move the node from OFFLINE to OPENING.  Hence when the first RS tries to do tickleOpening the operation will fail. Now here lies the problem,
{code}
    String encodedName = this.regionInfo.getEncodedName();
    try {
      this.version =
        ZKAssign.retransitionNodeOpening(server.getZooKeeper(),
          this.regionInfo, this.server.getServerName(), this.version);
    } catch (KeeperException e) {
{code}
Now this.version becomes -1 as the operation failed.
Now as in the first code snippet as the return type is not captured after tickleOpening() fails we go on with moving the node to OPENED.  Here again we dont have any check for this condition as already the version has been changed to -1.  Hence the OPENING to OPENED becomes successful. Chances of double assignment.
{noformat}
2011-09-22 00:57:29,930 WARN org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1328ceaa1ff000d Attempt to transition the unassigned node for 69797d064f773d1aa9adba56e7ff90a3 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENING failed, the node existed but was version 5 not the expected version 2
2011-09-22 00:57:33,494 WARN org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Failed refreshing OPENING; region=69797d064f773d1aa9adba56e7ff90a3, context=post_open_deploy
2011-09-22 00:58:02,356 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1328ceaa1ff000d Attempting to transition node 69797d064f773d1aa9adba56e7ff90a3 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2011-09-22 00:58:11,853 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x1328ceaa1ff000d Successfully transitioned node 69797d064f773d1aa9adba56e7ff90a3 from RS_ZK_REGION_OPENING to RS_ZK_REGION_OPENED
2011-09-22 00:58:13,956 DEBUG org.apache.hadoop.hbase.regionserver.handler.OpenRegionHandler: Opened t9,,1316633193606.69797d064f773d1aa9adba56e7ff90a3.

{noformat}
Correct me if this analysis is wrong.",ram_krish,ram_krish,Critical,Closed,Fixed,21/Sep/11 13:30,20/Nov/15 11:54
Bug,HBASE-4453,12523881,TestReplication failing up on builds.a.o because already running zk with new format root servername,"When the test runs, I see this at the start of the output:

{code}
2011-09-21 04:56:35,282 INFO  [main] zookeeper.MiniZooKeeperCluster(122): Failed binding ZK Server to client port: 21818
2011-09-21 04:56:35,344 INFO  [main] zookeeper.MiniZooKeeperCluster(136): Started MiniZK Server on client port: 21819
{code}

So, I think what is going on is that the test is connecting to the wrong zk ensemble because it does this on setup:

{code}
  @BeforeClass
  public static void setUpBeforeClass() throws Exception {
    conf1 = HBaseConfiguration.create();
    conf1.set(HConstants.ZOOKEEPER_ZNODE_PARENT, ""/1"");
    // smaller block size and capacity to trigger more operations
    // and test them
    conf1.setInt(""hbase.regionserver.hlog.blocksize"", 1024*20);
    conf1.setInt(""replication.source.size.capacity"", 1024);
    conf1.setLong(""replication.source.sleepforretries"", 100);
    conf1.setInt(""hbase.regionserver.maxlogs"", 10);
    conf1.setLong(""hbase.master.logcleaner.ttl"", 10);
    conf1.setBoolean(HConstants.REPLICATION_ENABLE_KEY, true);
    conf1.setBoolean(""dfs.support.append"", true);
    conf1.setLong(HConstants.THREAD_WAKE_FREQUENCY, 100);

    utility1 = new HBaseTestingUtility(conf1);
    utility1.startMiniZKCluster();
....
{code}

and we refer to conf1 subsequently rather than to HTU.getConfiguration.",stack,stack,Major,Closed,Fixed,21/Sep/11 16:19,20/Nov/15 11:55
Bug,HBASE-4455,12524222,"Rolling restart RSs scenario, -ROOT-, .META. regions are lost in AssignmentManager","Keep Master up all the time, do rolling restart of RSs like this - stop RS1, wait for 2 seconds, stop RS2, start RS1, wait for 2 seconds, stop RS3, start RS2, wait for 2 seconds, etc. After a while, you will find the -ROOT-, .META. regions aren't in ""regions in transtion"" from AssignmentManager point of view, but they aren't assigned to any regions. Here are the issues.

1. .-ROOT- or .META. location is stale when MetaServerShutdownHandler is invoked to check if it contains -ROOT- region. That is due to long delay from ZK notification and async nature of the system. Here is an example, even though new root region server sea-lab-1,60020,1316380133656 is set at T2, at T3 the shutdown process for sea-lab-1,60020,1316380133656, the root location still points to old server sea-lab-3,60020,1316380037898.




T1: 2011-09-18 14:08:52,470 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:6
0000-0x1327e43175e0000 Retrieved 29 byte(s) of data from znode /hbase/root-regio
n-server and set watcher; sea-lab-3,60020,1316380037898

T2: 2011-09-18 14:08:57,173 INFO org.apache.hadoop.hbase.catalog.RootLocationEditor: Setting ROOT region location in ZooKeeper as sea-lab-1,60020,1316380133656


T3: 2011-09-18 14:10:26,393 DEBUG org.apache.hadoop.hbase.master.ServerManager: Adde
d=sea-lab-1,60020,1316380133656 to dead servers, submitted shutdown handler to be executed, root=false, meta=true, current Root Location: sea-lab-3,60020,1316380037898

T4: 2011-09-18 14:12:37,314 DEBUG org.apache.hadoop.hbase.zookeeper.ZKUtil: master:6
0000-0x1327e43175e0000 Retrieved 29 byte(s) of data from znode /hbase/root-region-server and set watcher; sea-lab-1,60020,1316380133656


2. The MetaServerShutdownHandler worker thread that waits for -ROOT- or .META. availability could be blocked. If meanwhile, the new server that -ROOT- or .META. is being assigned restarted, another instance of MetaServerShutdownHandler is queued. Eventually, all MetaServerShutdownHandler worker threads are filled up. It looks like HBASE-4245.

",mingma,mingma,Major,Closed,Fixed,22/Sep/11 00:20,20/Nov/15 11:52
Bug,HBASE-4459,12524320,"HbaseObjectWritable code is a byte, we will eventually run out of codes","There are about 90 classes/codes in HbaseObjectWritable currently and Byte.MAX_VALUE is 127.  In addition, anyone wanting to add custom classes but not break compatibility might want to leave a gap before using codes and that's difficult in such limited space.

Eventually we should get rid of this pattern that makes compatibility difficult (better client/server protocol handshake) but we should probably at least bump this to a short for 0.94.",ram_krish,streamy,Critical,Closed,Fixed,22/Sep/11 18:46,20/Nov/15 11:55
Bug,HBASE-4468,12524413,Wrong resource name in an error massage: webapps instead of hbase-webapps,org.apache.hadoop.hbase.util.InfoServer loads a resource in 'hbase-webapps' but displays a message about 'webapps' when it does not find it.,nkeywal,nkeywal,Trivial,Closed,Fixed,23/Sep/11 13:00,20/Nov/15 11:54
Bug,HBASE-4470,12524449,ServerNotRunningException coming out of assignRootAndMeta kills the Master,"I'm surprised we still have issues like that and I didn't get a hit while googling so forgive me if there's already a jira about it.

When the master starts it verifies the locations of root and meta before assigning them, if the server is started but not running you'll get this:

{quote}
2011-09-23 04:47:44,859 WARN org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: RemoteException connecting to RS
org.apache.hadoop.ipc.RemoteException: org.apache.hadoop.hbase.ipc.ServerNotRunningException: Server is not running yet
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1038)

        at org.apache.hadoop.hbase.ipc.HBaseClient.call(HBaseClient.java:771)
        at org.apache.hadoop.hbase.ipc.HBaseRPC$Invoker.invoke(HBaseRPC.java:257)
        at $Proxy6.getProtocolVersion(Unknown Source)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:419)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:393)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:444)
        at org.apache.hadoop.hbase.ipc.HBaseRPC.waitForProxy(HBaseRPC.java:349)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getHRegionConnection(HConnectionManager.java:969)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.getCachedConnection(CatalogTracker.java:388)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.getMetaServerConnection(CatalogTracker.java:287)
        at org.apache.hadoop.hbase.catalog.CatalogTracker.verifyMetaRegionLocation(CatalogTracker.java:484)
        at org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:441)
        at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:388)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:282)
{quote}

I hit that 3-4 times this week while debugging something else. The worst is that when you restart the master it sees that as a failover, but none of the regions are assigned so it takes an eternity to get back fully online.",gchanan,jdcryans,Critical,Closed,Fixed,23/Sep/11 17:54,18/Sep/13 20:15
Bug,HBASE-4472,12524465,MiniHBaseCluster.shutdown() doesn't work if no active master,"Running tests over in HBASE-4014 brought up this issue.  If the active master in a MiniHBaseCluster has aborted, then calling MiniHBaseCluster.shutdown() will just hang in JVMClusterUtil.shutdown(), waiting to join each of the region server threads.

Seems like we should explicitly stop each region server instead of just relying on an active master instance deleting the cluster status znode?",ghelmling,ghelmling,Major,Closed,Fixed,23/Sep/11 19:43,20/Nov/15 11:55
Bug,HBASE-4473,12524481,NPE when executors are down but events are still coming in,"Minor annoyance when shutting down a cluster and the master is still receiving events from Zookeeper:
{quote}
2011-09-22 23:53:01,552 DEBUG org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher: master:60000-0x3292d87deb004f Received InterruptedException, doing nothing here
java.lang.InterruptedException
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.zookeeper.ClientCnxn.submitRequest(ClientCnxn.java:1317)
        at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:726)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:938)
        at org.apache.hadoop.hbase.zookeeper.ZKAssign.deleteNode(ZKAssign.java:407)
        at org.apache.hadoop.hbase.zookeeper.ZKAssign.deleteOpenedNode(ZKAssign.java:284)
        at org.apache.hadoop.hbase.master.handler.OpenedRegionHandler.process(OpenedRegionHandler.java:88)
        at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:156)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
...
2011-09-22 23:53:01,558 DEBUG org.apache.hadoop.hbase.executor.ExecutorService: Executor service [MASTER_OPEN_REGION-sv2borg170:60000] not found in {}
2011-09-22 23:53:01,558 ERROR org.apache.zookeeper.ClientCnxn: Error while calling watcher
java.lang.NullPointerException
        at org.apache.hadoop.hbase.executor.ExecutorService.submit(ExecutorService.java:220)
        at org.apache.hadoop.hbase.master.AssignmentManager.handleRegion(AssignmentManager.java:447)
        at org.apache.hadoop.hbase.master.AssignmentManager.nodeDataChanged(AssignmentManager.java:546)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:281)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:530)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:506)
{quote}

It's annoying because it then spams you with a bunch of NPEs that have nothing to do with the reason the Master is shutting down. Googling I saw someone also had that issue in June: http://pastebin.com/5Tqrj0nq",jdcryans,jdcryans,Minor,Closed,Fixed,23/Sep/11 22:09,20/Nov/15 11:53
Bug,HBASE-4476,12524493,Compactions must fail if column tracker gets columns out of order,"We found this in ScanWildcardColumnTracker:

    // new col < oldcol
    // if (cmp < 0) {
    // WARNING: This means that very likely an edit for some other family
    // was incorrectly stored into the store for this one. Continue, but
    // complain.
    LOG.error(""ScanWildcardColumnTracker.checkColumn ran "" +
  		""into a column actually smaller than the previous column: "" +

This went under the radar in our dark launch cluster when a column family name was misspelled first, but then was ""renamed"" by renaming directories in the HBase storage directory tree. We ended up with inconsistent data, but compactions still succeeded most of the time, likely discarding part of input data.
",mikhail,mikhail,Major,Closed,Fixed,24/Sep/11 00:39,12/Oct/12 05:35
Bug,HBASE-4478,12524502,Improve AssignmentManager.handleRegion so that it can process certain ZK state in the case of RS offline,"Currently AssignmentManager.handleRegion skips processing of ZK event change if the RS is offline. It relies on TimeoutMonitor and ServerShutdownHandler to process RIT.

      // Verify this is a known server
      if (!serverManager.isServerOnline(sn) &&
          !this.master.getServerName().equals(sn)) {
        LOG.warn(""Attempted to handle region transition for server but "" +
          ""server is not online: "" + Bytes.toString(data.getRegionName()));
        return;
      }

For certain states like OPENED, OPENING, FAILED_OPEN, CLOSED, it can continue the progressing even if the RS is offline.",ram_krish,mingma,Major,Closed,Fixed,24/Sep/11 04:47,20/Nov/15 11:56
Bug,HBASE-4479,12524531,TestMasterFailover failure in Hbase-0.92#17,"When the master restarted it was not able to get any servers online and the restart was a clean restart.
Hence there were no regions to assign.
Hence the retainAssignment tries to get one of the regions and uses RANDOM.getInt(size).  Here size is 0.

So ideally 0 is not accepted here.  Hence we have got an exception making the master not to come up and the test case timeout.

Though we need to see if really no regions was expected when the master came up, but this JIRA's intent is to deal such scenario where the size can be 0.

",ram_krish,ram_krish,Minor,Closed,Fixed,24/Sep/11 17:49,20/Nov/15 11:55
Bug,HBASE-4494,12524771,AvroServer:: get fails with NPE on a non-existent row,"Try to submit a get request to the avro gateway. 

If the row specified for a given table does not exist, the server request fails with a NPE.

",kaykay.unique,kaykay.unique,Major,Closed,Fixed,27/Sep/11 00:26,20/Nov/15 11:53
Bug,HBASE-4496,12524808,HFile V2 does not honor setCacheBlocks when scanning.,"While testing the LRU cache during the scanning I noticed quite some churn in the cache even when Scan.cacheBlocks is set to false. After debugging this, I found that HFile V2 always caches blocks in the LRU cache regardless of the cacheBlocks setting.

Here's a trace (from Eclipse) showing the problem:

HFileReaderV2.readBlock(long, int, boolean, boolean, boolean) line: 279	
HFileReaderV2.readBlockData(long, long, int, boolean) line: 219	
HFileBlockIndex$BlockIndexReader.seekToDataBlock(byte[], int, int, HFileBlock) line: 191	
HFileReaderV2$ScannerV2.seekTo(byte[], int, int, boolean) line: 502	
HFileReaderV2$ScannerV2.reseekTo(byte[], int, int) line: 539	
StoreFileScanner.reseekAtOrAfter(HFileScanner, KeyValue) line: 151	
StoreFileScanner.reseek(KeyValue) line: 110	
KeyValueHeap.reseek(KeyValue) line: 255	
StoreScanner.reseek(KeyValue) line: 409	
StoreScanner.next(List<KeyValue>, int) line: 304	
KeyValueHeap.next(List<KeyValue>, int) line: 114	
KeyValueHeap.next(List<KeyValue>) line: 143	
HRegion$RegionScannerImpl.nextRow(byte[]) line: 2774	
HRegion$RegionScannerImpl.nextInternal(int) line: 2722	
HRegion$RegionScannerImpl.next(List<KeyValue>, int) line: 2682	
HRegion$RegionScannerImpl.next(List<KeyValue>) line: 2699	
HRegionServer.next(long, int) line: 2092	

Every scanner.next causes a reseek, which eventually causes a call to HFileBlockIndex$BlockIndexReader.seekToDataBlock(...) at which point the cacheBlocks information is lost. HFileReaderV2.readBlockData calls HFileReaderV2.readBlock with cacheBlocks set unconditionally to true.

The fix is not immediately clear, unless we want to pass cacheBlocks to HFileBlockIndex$BlockIndexReader.seekToDataBlock and then on to HFileBlock.BasicReader.readBlockData and all its implementers, which is ugly as readBlockData should not care about caching.

Avoiding caching during scans is somewhat important for us.
",mikhail,larsh,Major,Closed,Fixed,27/Sep/11 07:22,12/Oct/12 05:35
Bug,HBASE-4501,12524980,[replication] Shutting down a stream leaves recovered sources running,"When removing a peer it will call ReplicationSourceManager.removePeer which calls closeRecoveredQueue which does this:

{code}
LOG.info(""Done with the recovered queue "" + src.getPeerClusterZnode());
this.oldsources.remove(src);
this.zkHelper.deleteSource(src.getPeerClusterZnode(), false);
{code}

This works in the case where the recovered source is done and is calling this method, but when removing a peer it never calls terminate on thus it leaving it running.",jdcryans,jdcryans,Major,Closed,Fixed,27/Sep/11 23:13,20/Nov/15 11:54
Bug,HBASE-4505,12525081,SubstringComparator - Javadoc refers to a class that doesn't exist,"For example... http://hbase.apache.org/apidocs/org/apache/hadoop/hbase/filter/SubstringComparator.html

""This comparator is for use with ColumnValueFilter,""

No such class exists.  It should be SingleColumnValueFilter.  The code example is wrong too.",dmeil,dmeil,Trivial,Closed,Fixed,28/Sep/11 17:04,12/Jun/22 19:36
Bug,HBASE-4508,12525128,Backport HBASE-3777 to 0.90 branch,"See discussion here: http://search-hadoop.com/m/MJBId1aazTR1/backporting+HBASE-3777+to+0.90&subj=backporting+HBASE+3777+to+0+90

Rocketfuel has been running 0.90.3 with HBASE-3777 since its resolution.
They have 10 RS nodes , 1 Master and 1 Zookeeper
Live writes and reads but super heavy on reads. Cache hit is pretty high.
The qps on one of their data centers is 50K.",bfulton,yuzhihong@gmail.com,Major,Closed,Fixed,29/Sep/11 02:10,20/Nov/15 11:55
Bug,HBASE-4511,12525152,There is data loss when master failovers,"It goes like this:

Master crashed ,  at the same time RS with meta is crashing, but RS doesn't eixt.
Master startups again and finds all living RS. 
Master verifies the meta failed,  because this RS is crashing.
Master reassigns the meta, but it doesn't split the Hlog. 

So some meta data is loss.

About the logs of a failover test case fail. 

//It said that we want to kill a RS

2011-09-28 19:54:45,694 INFO  [Thread-988] regionserver.HRegionServer(1443): STOPPED: Killing for unit test
2011-09-28 19:54:45,694 INFO  [Thread-988] master.TestMasterFailover(1007): 

RS 192.168.2.102,54385,1317264874629 killed 

//Rs didn't crash. 
2011-09-28 19:54:51,763 INFO  [Master:0;192.168.2.102,54557,1317264885720] master.HMaster(458): Registering server found up in zk: 192.168.2.102,54385,1317264874629
2011-09-28 19:54:51,763 INFO  [Master:0;192.168.2.102,54557,1317264885720] master.ServerManager(232): Registering server=192.168.2.102,54385,1317264874629
2011-09-28 19:54:51,770 DEBUG [Master:0;192.168.2.102,54557,1317264885720] zookeeper.ZKUtil(491): master:54557-0x132b31adbb30005 Unable to get data of znode /hbase/unassigned/1028785192 because node does not exist (not an error)
2011-09-28 19:54:51,771 DEBUG [Master:0;192.168.2.102,54557,1317264885720] zookeeper.ZKUtil(1003): master:54557-0x132b31adbb30005 Retrieved 33 byte(s) of data from znode /hbase/root-region-server and set watcher; 192.168.2.102,54383,131726487...

//Meta verification failed and ressigned the meta. So all the regions in the meta is loss.

2011-09-28 19:54:51,773 INFO  [Master:0;192.168.2.102,54557,1317264885720] catalog.CatalogTracker(476): Failed verification of .META.,,1 at address=192.168.2.102,54385,1317264874629; org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: Server 192.168.2.102,54385,1317264874629 not running, aborting
2011-09-28 19:54:51,773 DEBUG [Master:0;192.168.2.102,54557,1317264885720] catalog.CatalogTracker(316): new .META. server: 192.168.2.102,54385,1317264874629 isn't valid. Cached .META. server: null
2011-09-28 19:54:52,274 DEBUG [Master:0;192.168.2.102,54557,1317264885720] zookeeper.ZKUtil(1003): master:54557-0x132b31adbb30005 Retrieved 33 byte(s) of data from znode /hbase/root-region-server and set watcher; 192.168.2.102,54383,131726487...
2011-09-28 19:54:52,277 INFO  [Master:0;192.168.2.102,54557,1317264885720] catalog.CatalogTracker(476): Failed verification of .META.,,1 at address=192.168.2.102,54385,1317264874629; org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: Server 192.168.2.102,54385,1317264874629 not running, aborting
2011-09-28 19:54:52,277 DEBUG [Master:0;192.168.2.102,54557,1317264885720] catalog.CatalogTracker(316): new .META. server: 192.168.2.102,54385,1317264874629 isn't valid. Cached .META. server: null
2011-09-28 19:54:52,778 DEBUG [Master:0;192.168.2.102,54557,1317264885720] zookeeper.ZKUtil(1003): master:54557-0x132b31adbb30005 Retrieved 33 byte(s) of data from znode /hbase/root-region-server and set watcher; 192.168.2.102,54383,131726487...
2011-09-28 19:54:52,782 INFO  [Master:0;192.168.2.102,54557,1317264885720] catalog.CatalogTracker(476): Failed verification of .META.,,1 at address=192.168.2.102,54385,1317264874629; org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: org.apache.hadoop.hbase.regionserver.RegionServerStoppedException: Server 192.168.2.102,54385,1317264874629 not running, aborting
2011-09-28 19:54:52,782 DEBUG [Master:0;192.168.2.102,54557,1317264885720] catalog.CatalogTracker(316): new .META. server: 192.168.2.102,54385,1317264874629 isn't valid. Cached .META. server: null
2011-09-28 19:54:52,782 DEBUG [Master:0;192.168.2.102,54557,1317264885720] zookeeper.ZKAssign(264): master:54557-0x132b31adbb30005 Creating (or updating) unassigned node for 1028785192 with OFFLINE state
2011-09-28 19:54:52,825 DEBUG [Thread-988-EventThread] zookeeper.ZooKeeperWatcher(233): master:54557-0x132b31adbb30005 Received ZooKeeper Event, type=NodeCreated, state=SyncConnected, path=/hbase/unassigned/1028785192


//It said that Master clean the cluster.
2011-09-28 19:54:52,889 INFO  [Master:0;192.168.2.102,54557,1317264885720] master.AssignmentManager(383): Clean cluster startup. Assigning userregions
2011-09-28 19:54:52,889 DEBUG [Master:0;192.168.2.102,54557,1317264885720] zookeeper.ZKAssign(494): master:54557-0x132b31adbb30005 Deleting any existing unassigned nodes",stack,sunnygao,Minor,Closed,Fixed,29/Sep/11 08:10,20/Nov/15 11:54
Bug,HBASE-4512,12525210,JVMClusterUtil throwing wrong exception when master thread cannot be created,"In JVMClusterUtil.MasterThread createMasterThread
{code}
throw new RuntimeException(""Failed construction of RegionServer: "" +
{code}
It must be failed construction of Master.
Very trivial one.",ram_krish,ram_krish,Trivial,Closed,Fixed,29/Sep/11 16:38,20/Nov/15 11:52
Bug,HBASE-4513,12525220,NOTICES.txt refers to Facebook for Thrift,"The NOTICES.txt file says:

{quote}
In addition, this product includes software developed by:

Facebook, Inc. (http://developers.facebook.com/thrift/ -- Page includes the Thrift Software License)
{quote}

Since Thrift is now an Apache project, this seems to be obsolete.",stack,jpallas,Minor,Closed,Fixed,29/Sep/11 17:51,05/Apr/22 12:14
Bug,HBASE-4515,12525241,User.getCurrent() can fail to initialize the current user,"When testing with miniclusters that shutdown and are restarted, sometimes a call to User.getCurrent().getName() NPEs when attempting to restart hbase.  Oddly this happens consistently on particular branches and not on others. I don't know or understand why this happens but it has something to do with the getCurrentUGI call in  o.a.h.h.security.User.HadoopUser sometimes returning null and sometimes returning data.

{code}
   private HadoopUser() {
      try {
        ugi = (UserGroupInformation) callStatic(""getCurrentUGI"");
        if (ugi == null) {
          LOG.warn(""Although successfully retrieved UserGroupInformation"" 
              + ""  it was null!"");
        }
      } catch (RuntimeException re) {
{code}

This patch essentially is a workaround -- it propagates the null so that clients can check and avoid the NPE.",ghelmling,jmhsieh,Major,Closed,Fixed,29/Sep/11 21:15,20/Nov/15 11:52
Bug,HBASE-4526,12525379,special case for stopping master in hbase-daemon.sh is no longer required,"Now that HBASE-4209 is finally done (many thanks to stack for help and encouragement!) I don't think there's any reason to keep this bit of code in hbase-daemon.sh:

{noformat}
        if [ ""$command"" = ""master"" ]; then
          echo ""`date` Killing $command"" >> $loglog
          kill -9 `cat $pid` > /dev/null 2>&1
        else
          echo ""`date` Killing $command"" >> $loglog
          kill `cat $pid` > /dev/null 2>&1
        fi
{noformat}

I suggest we remove the special case completely, since I don't think it serves any useful purpose (patch attached).

Now, as an additional precautionary measure. We can try to follow up with a SIGKILL once a certain timeout expires.
Let me know if you think it is necessary to do so and I'll update the patch.",rvs,rvs,Minor,Closed,Fixed,30/Sep/11 23:07,20/Nov/15 11:54
Bug,HBASE-4531,12525544,hbase-4454 failsafe broke mvn site; back it out or fix,"mvn site is broke in head of trunk.  If I back out the last pom change it works again:

{code}
------------------------------------------------------------------------
r1177168 | stack | 2011-09-29 05:42:13 +0000 (Thu, 29 Sep 2011) | 1 line

HBASE-4454 Add failsafe plugin to build and rename integration tests
{code}",thehellmaker,stack,Blocker,Closed,Fixed,03/Oct/11 16:33,20/Nov/15 11:54
Bug,HBASE-4537,12525743,Bad imports in TestUser from HBASE-4515 change,"The HBASE-4515 patch mistakenly added unused imports to TestUser:
{code}
import org.apache.hadoop.security.UnixUserGroupInformation;
import org.apache.hadoop.security.UserGroupInformation;
{code}

Both should be removed.  UnixUserGroupInformation is specific to non-secure Hadoop and importing it will break compilation against secure Hadoop variants (0.20.205, 0.22, 0.23).",ghelmling,ghelmling,Major,Closed,Fixed,04/Oct/11 22:52,20/Nov/15 11:55
Bug,HBASE-4539,12525774,OpenedRegionHandler racing with itself in ServerShutDownhandler flow leading to HMaster abort,"Steps to reproduce
==================
-> Region R1 is being opened in RS1.  
->After processing the znode to OPENED RS1 goes down.
->Now before the OpenedRegionHandler executor deletes the znode if ServerShutDownHandler tries to assign the region to RS2, RS2 transits the node to OPENED and this OpenedRegionHandler executor deletes the znode.  
->Now if the first OpenedRegionHandler tries deleting the znode it throws NoNode Exception and causes the HMaster to abort.

",ram_krish,ram_krish,Major,Closed,Fixed,05/Oct/11 09:07,20/Nov/15 11:53
Bug,HBASE-4540,12525778,OpenedRegionHandler is not enforcing atomicity of the operation it is performing,"-> OpenedRegionHandler has not yet deleted the znode of the region R1 opened by RS1.
-> RS1 goes down.
-> Servershutdownhandler assigns the region R1 to RS2.
-> The znode of R1 is moved to OFFLINE state by master or OPENING state by RS2 if RS2 has started opening the region.
-> Now the first OpenedRegionHandler tries to delete the znode thinking its in OPENED state but fails.
-> Though it fails it removes the node from RIT and adds RS1 as the owner of R1 in master's memory.
-> Now when RS2 completes opening the region the master is not able to open the region as already the reigon has been deleted from RIT.
{code}
Master
======
2011-10-05 20:49:45,301 INFO org.apache.hadoop.hbase.master.handler.ServerShutdownHandler: Finished processing of shutdown of linux146,60020,1317827727647
2011-10-05 20:49:54,177 DEBUG org.apache.hadoop.hbase.master.HMaster: Not running balancer because 1 region(s) in transition: {3e69d628a8bd8e9b7c5e7a2a6e03aad9=t1,,1317827883842.3e69d628a8bd8e9b7c5e7a2a6e03aad9. state=PENDING_OPEN, ts=1317827985272, server=linux76,60020,1317827746847}
2011-10-05 20:49:57,720 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE, server=linux76,60000,1317827742012, region=3e69d628a8bd8e9b7c5e7a2a6e03aad9
2011-10-05 20:50:14,501 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x132d3dc13090023 Deleting existing unassigned node for 3e69d628a8bd8e9b7c5e7a2a6e03aad9 that is in expected state RS_ZK_REGION_OPENED
2011-10-05 20:50:14,505 WARN org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x132d3dc13090023 Attempting to delete unassigned node 3e69d628a8bd8e9b7c5e7a2a6e03aad9 in RS_ZK_REGION_OPENED state but node is in RS_ZK_REGION_OPENING state

After the region is opened in RS2
=================================
2011-10-05 20:50:48,066 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=linux76,60020,1317827746847, region=3e69d628a8bd8e9b7c5e7a2a6e03aad9, which is more than 15 seconds late
2011-10-05 20:50:48,290 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region 3e69d628a8bd8e9b7c5e7a2a6e03aad9 from server linux76,60020,1317827746847 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
2011-10-05 20:50:53,743 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_OPENING, server=linux76,60020,1317827746847, region=3e69d628a8bd8e9b7c5e7a2a6e03aad9
2011-10-05 20:50:54,182 DEBUG org.apache.hadoop.hbase.master.CatalogJanitor: Scanned 1 catalog row(s) and gc'd 0 unreferenced parent region(s)
2011-10-05 20:50:54,397 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received OPENING for region 3e69d628a8bd8e9b7c5e7a2a6e03aad9 from server linux76,60020,1317827746847 but region was in  the state null and not in expected PENDING_OPEN or OPENING states
{code}
",ram_krish,ram_krish,Major,Closed,Fixed,05/Oct/11 09:37,20/Nov/15 11:52
Bug,HBASE-4547,12526119,TestAdmin failing in 0.92 because .tableinfo not found,"I've been running tests before commit and found the following happens with some regularity, sporadic of course, but they fail fairly frequently:
{code}
Failed tests:   testOnlineChangeTableSchema(org.apache.hadoop.hbase.client.TestAdmin)
  testForceSplit(org.apache.hadoop.hbase.client.TestAdmin): expected:<2> but was:<1>
  testForceSplitMultiFamily(org.apache.hadoop.hbase.client.TestAdmin): expected:<2> but was:<1>
{code}
Looking, it seems like we fail to find .tableinfo in the tests that modify table schema while table is online.

The update of a table schema just does an overwrite.  In the tests we sometimes fail to find the newly written file or we get EOFE reading it.",stack,stack,Critical,Closed,Fixed,06/Oct/11 21:14,20/Nov/15 11:54
Bug,HBASE-4551,12526152,Small fixes to compile against 0.23-SNAPSHOT,"- fix pom.xml to properly pull the test artifacts
- fix TestHLog to not use the private cluster.getNameNode() API",tlipcon,tlipcon,Major,Closed,Fixed,07/Oct/11 04:45,20/Nov/15 11:52
Bug,HBASE-4552,12526154,multi-CF bulk load is not atomic across column families,"Currently the bulk load API simply imports one HFile at a time. With multi-column-family support, this is inappropriate, since different CFs show up separately. Instead, the IPC endpoint should take a of CF -> HFiles, so we can online them all under a single region-wide lock.",jmhsieh,tlipcon,Major,Closed,Fixed,07/Oct/11 05:21,16/Nov/16 21:02
Bug,HBASE-4556,12526303,Fix all incorrect uses of InternalScanner.next(...),"There are cases all over the code where InternalScanner.next(...) is not used correctly.

I see this a lot:
{code}
while(scanner.next(...)) {
}
{code}

The correct pattern is:
{code}
boolean more = false;
do {
   more = scanner.next(...);
} while (more);
{code}
",larsh,larsh,Major,Closed,Fixed,08/Oct/11 05:31,20/Nov/15 11:54
Bug,HBASE-4562,12526400,"When split doing offlineParentInMeta encounters error, it'll cause data loss","Follow below steps to replay the problem:
1. change the SplitTransaction.java as below,just like mock the timeout error.
   {code:title=SplitTransaction.java|borderStyle=solid}
      if (!testing) {
        MetaEditor.offlineParentInMeta(server.getCatalogTracker(),
           this.parent.getRegionInfo(), a.getRegionInfo(), b.getRegionInfo());
        throw new IOException(""some unexpected error in split"");
      }
   {code} 
2. update the regionserver code,restart;
3. create a table & put some data to the table;
4. split the table;
5. kill the regionserver hosted the table;
6. wait some time after master ServerShutdownHandler.process execute,then scan the table,u'll find the data wrote before lost.

We can fix the bug just use the patch.",bluedavy,bluedavy,Blocker,Closed,Fixed,10/Oct/11 03:04,20/Nov/15 11:53
Bug,HBASE-4563,12526401,"When error occurs in this.parent.close(false) of split, the split region cannot write or read","Follow below steps to replay the problem:
1. change the SplitTransaction.java as below,just like mock the hdfs error.
   {code:title=SplitTransaction.java|borderStyle=solid}
      List<StoreFile> hstoreFilesToSplit = this.parent.close(false);
      throw new IOException(""some unexpected error in close store files"");
   {code} 
2. update the regionserver code,restart;
3. create a table & put some data to the table;
4. split the table;
5. scan the table,then it'll fail.

We can fix the bug just use the patch.",bluedavy,bluedavy,Blocker,Closed,Fixed,10/Oct/11 03:13,20/Nov/15 11:55
Bug,HBASE-4565,12526464,Maven HBase build broken on cygwin with copynativelib.sh call.,"This is broken in both 0.92 as well as trunk pom.xml

Here's a sample maven log snippet from trunk (from Mayuresh on user mailing list)

[INFO] [antrun:run {execution: package}]
[INFO] Executing tasks

main:
   [mkdir] Created dir: D:\workspace\mkshirsa\hbase-trunk\target\hbase-0.93-SNAPSHOT\hbase-0.93-SNAPSHOT\lib\native\${build.platform}
    [exec] ls: cannot access D:workspacemkshirsahbase-trunktarget/nativelib: No such file or directory
    [exec] tar (child): Cannot connect to D: resolve failed
[INFO] ------------------------------------------------------------------------
[ERROR] BUILD ERROR
[INFO] ------------------------------------------------------------------------
[INFO] An Ant BuildException has occured: exec returned: 3328

There are two issues: 
1) The ant run task below doesn't resolve the windows file separator returned by the project.build.directory - this causes the above resolve failed.
<!-- Using Unix cp to preserve symlinks, using script to handle wildcards -->
<echo file=""${project.build.directory}/copynativelibs.sh"">
    if [ `ls ${project.build.directory}/nativelib | wc -l` -ne 0]; then


2) The tar argument value below also has a similar issue in that the path arg doesn't resolve right.
<!-- Using Unix tar to preserve symlinks -->
<exec executable=""tar"" failonerror=""yes"" dir=""${project.build.directory}/${project.artifactId}-${project.version}"">
    <arg value=""czf""/>
    <arg value=""/cygdrive/c/workspaces/hbase-0.92-svn/target/${project.artifactId}-${project.version}.tar.gz""/>
    <arg value="".""/>
</exec>

In both cases, the fix would probably be to use a cross-platform way to handle the directory locations. 
",svarma,svarma,Major,Closed,Fixed,10/Oct/11 16:23,24/Oct/12 05:40
Bug,HBASE-4570,12526531,Scan ACID problem with concurrent puts.,"When scanning a table sometimes rows that have multiple column families get split into two rows if there are concurrent writes.  In this particular case we are overwriting the contents of a Get directly back onto itself as a Put.

For example, this is a two cf row (with ""f1"", ""f2"", .. ""f9"" cfs).  It is actually returned as two rows (#55 and #56). Interestingly if the two were merged we would have a single proper row.

Row row0000024461 had time stamps: [55: keyvalues={row0000024461/f0:data/1318200440867/Put/vlen=1000, row0000024461/f0:qual/1318200440867/Put/vlen=10, row0000024461/f1:data/1318200440867/Put/vlen=1000, row0000024461/f1:qual/1318200440867/Put/vlen=10, row0000024461/f2:data/1318200440867/Put/vlen=1000, row0000024461/f2:qual/1318200440867/Put/vlen=10, row0000024461/f3:data/1318200440867/Put/vlen=1000, row0000024461/f3:qual/1318200440867/Put/vlen=10, row0000024461/f4:data/1318200440867/Put/vlen=1000, row0000024461/f4:qual/1318200440867/Put/vlen=10}, 
56: keyvalues={row0000024461/f5:data/1318200440867/Put/vlen=1000, row0000024461/f5:qual/1318200440867/Put/vlen=10, row0000024461/f6:data/1318200440867/Put/vlen=1000, row0000024461/f6:qual/1318200440867/Put/vlen=10, row0000024461/f7:data/1318200440867/Put/vlen=1000, row0000024461/f7:qual/1318200440867/Put/vlen=10, row0000024461/f8:data/1318200440867/Put/vlen=1000, row0000024461/f8:qual/1318200440867/Put/vlen=10, row0000024461/f9:data/1318200440867/Put/vlen=1000, row0000024461/f9:qual/1318200440867/Put/vlen=10}]

I've only tested this on 0.90.1+patches and 0.90.3+patches, but it is consistent and duplicatable.",jmhsieh,jmhsieh,Major,Closed,Fixed,10/Oct/11 23:05,20/Nov/15 11:55
Bug,HBASE-4577,12526776,Region server reports storefileSizeMB bigger than storefileUncompressedSizeMB,"Minor issue while looking at the RS metrics:

bq. numberOfStorefiles=8, storefileUncompressedSizeMB=2418, storefileSizeMB=2420, compressionRatio=1.0008

I guess there's a truncation somewhere when it's adding the numbers up.

FWIW there's no compression on that table.",sunnygao,jdcryans,Minor,Closed,Fixed,11/Oct/11 23:24,20/Nov/15 11:52
Bug,HBASE-4578,12526778,NPE when altering a table that has moving regions,"I'm still not a 100% sure on the source of this error, but here's what I was able to get twice while altering a table that was doing a bunch of splits:

{quote}

2011-10-11 23:48:59,344 INFO org.apache.hadoop.hbase.master.handler.SplitRegionHandler: Handled SPLIT report); parent=TestTable,0002608338,1318376880454.a75d6815fdfc513fb1c8aabe086c6763. daughter a=TestTable,0002608338,1318376938764.ef170ff6cd8695dc8aec92e542dc9ac1.daughter b=TestTable,0003301408,1318376938764.36eb2530341bd46888ede312c5559b5d.
2011-10-11 23:49:09,579 DEBUG org.apache.hadoop.hbase.master.handler.TableEventHandler: Ignoring table not disabled exception for supporting online schema changes.
2011-10-11 23:49:09,580 INFO org.apache.hadoop.hbase.master.handler.TableEventHandler: Handling table operation C_M_MODIFY_TABLE on table TestTable
2011-10-11 23:49:09,612 INFO org.apache.hadoop.hbase.util.FSUtils: TableInfoPath = hdfs://sv4r11s38:9100/hbase/TestTable/.tableinfo tmpPath = hdfs://sv4r11s38:9100/hbase/TestTable/.tmp/.tableinfo.1318376949612
2011-10-11 23:49:09,692 INFO org.apache.hadoop.hbase.util.FSUtils: TableDescriptor stored. TableInfoPath = hdfs://sv4r11s38:9100/hbase/TestTable/.tableinfo
2011-10-11 23:49:09,693 INFO org.apache.hadoop.hbase.util.FSUtils: Updated tableinfo=hdfs://sv4r11s38:9100/hbase/TestTable/.tableinfo to blah
2011-10-11 23:49:09,695 INFO org.apache.hadoop.hbase.master.handler.TableEventHandler: Bucketing regions by region server...
2011-10-11 23:49:09,695 DEBUG org.apache.hadoop.hbase.client.MetaScanner: Scanning .META. starting at row=TestTable,,00000000000000 for max=2147483647 rows
2011-10-11 23:49:09,709 DEBUG org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation: The connection to hconnection-0x132f043bbde02e9 has been closed.
2011-10-11 23:49:09,709 ERROR org.apache.hadoop.hbase.executor.EventHandler: Caught throwable while processing event C_M_MODIFY_TABLE
java.lang.NullPointerException
	at java.util.TreeMap.getEntry(TreeMap.java:324)
	at java.util.TreeMap.containsKey(TreeMap.java:209)
	at org.apache.hadoop.hbase.master.handler.TableEventHandler.reOpenAllRegions(TableEventHandler.java:114)
	at org.apache.hadoop.hbase.master.handler.TableEventHandler.process(TableEventHandler.java:90)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:168)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

{quote}

The first time the shell reported that all the regions were updated correctly, the second time it got stuck for a while:

{quote}

6/14 regions updated.
0/14 regions updated.
...
0/14 regions updated.
2/16 regions updated.
...
2/16 regions updated.
8/9 regions updated.
...
8/9 regions updated.
{quote}

After which I killed it, redid the alter and it worked.",sunnygao,jdcryans,Blocker,Closed,Fixed,11/Oct/11 23:58,20/Nov/15 11:54
Bug,HBASE-4579,12526781,"CST.requestCompaction semantics changed, logs are now spammed when too many store files","Another bug I'm not so sure what's going on. I see this in my log:

{quote}
2011-10-12 00:23:43,435 DEBUG org.apache.hadoop.hbase.regionserver.Store: info: no store files to compact
2011-10-12 00:23:44,335 DEBUG org.apache.hadoop.hbase.regionserver.Store: info: no store files to compact
2011-10-12 00:23:45,236 DEBUG org.apache.hadoop.hbase.regionserver.Store: info: no store files to compact
2011-10-12 00:23:46,136 DEBUG org.apache.hadoop.hbase.regionserver.Store: info: no store files to compact
2011-10-12 00:23:47,036 DEBUG org.apache.hadoop.hbase.regionserver.Store: info: no store files to compact
2011-10-12 00:23:47,936 DEBUG org.apache.hadoop.hbase.regionserver.Store: info: no store files to compact
{quote}

It spams for a while, and a little later instead I get:

{quote}
2011-10-12 00:26:52,139 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of info.  Only 2 file(s) of size 176.4m have met compaction criteria.
2011-10-12 00:26:53,040 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of info.  Only 2 file(s) of size 176.4m have met compaction criteria.
2011-10-12 00:26:53,940 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of info.  Only 2 file(s) of size 176.4m have met compaction criteria.
2011-10-12 00:26:54,840 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of info.  Only 2 file(s) of size 176.4m have met compaction criteria.
2011-10-12 00:26:55,741 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of info.  Only 2 file(s) of size 176.4m have met compaction criteria.
2011-10-12 00:26:56,641 DEBUG org.apache.hadoop.hbase.regionserver.Store: Skipped compaction of info.  Only 2 file(s) of size 176.4m have met compaction criteria.
{quote}

I believe I also saw something like that for flushes, but the region was closing so at least I know why it was spamming (would be nice if it just unrequested the flush):

{quote}
2011-10-12 00:26:40,693 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,694 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
2011-10-12 00:26:40,733 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,733 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
2011-10-12 00:26:40,873 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,873 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
2011-10-12 00:26:40,873 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,873 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
2011-10-12 00:26:40,921 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,922 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
2011-10-12 00:26:40,923 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,923 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
2011-10-12 00:26:40,923 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Flush requested on TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5.
2011-10-12 00:26:40,923 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: NOT flushing memstore for region TestTable,0038168581,1318378894213.2beb8a1e29382a8d3e90a88b9662e5f5., flushing=false, writesEnabled=false
{quote}",jdcryans,jdcryans,Critical,Closed,Fixed,12/Oct/11 00:36,20/Nov/15 11:54
Bug,HBASE-4580,12526782,Some invalid zk nodes were created when a clean cluster restarts,"The below logs said that we created a invalid zk node when restarted a cluster.
it mistakenly believed that the regions belong to a dead server.

2011-10-11 05:05:29,127 INFO org.apache.hadoop.hbase.master.HMaster: Meta updated status = true
2011-10-11 05:05:29,127 INFO org.apache.hadoop.hbase.master.HMaster: ROOT/Meta already up-to date with new HRI.
2011-10-11 05:05:29,151 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for 771d63e9327383159553619a4f2dc74f with OFFLINE state
2011-10-11 05:05:29,161 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for 3cf860dd323fe6360f571aeafc129f95 with OFFLINE state
2011-10-11 05:05:29,170 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for 4065350214452a9d5c55243c734bef08 with OFFLINE state
2011-10-11 05:05:29,178 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for 4e81613f82a39fc6e5e89f96e7b3ccc4 with OFFLINE state
2011-10-11 05:05:29,187 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for e21b9e1545a28953aba0098fda5c9cd9 with OFFLINE state
2011-10-11 05:05:29,195 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for 5cd9f55eecd43d088bbd505f6795131f with OFFLINE state
2011-10-11 05:05:29,229 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for db5f641452a70b09b85a92970e4198c7 with OFFLINE state
2011-10-11 05:05:29,237 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for a7b20a653919e7f41bfb2ed349af7d21 with OFFLINE state
2011-10-11 05:05:29,253 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Creating (or updating) unassigned node for c9385619425f737eab1a6624d2e097a8 with OFFLINE state

// we cleaned all zk nodes.
2011-10-11 05:05:29,262 INFO org.apache.hadoop.hbase.master.AssignmentManager: Clean cluster startup. Assigning userregions
2011-10-11 05:05:29,262 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Deleting any existing unassigned nodes
2011-10-11 05:05:29,367 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 9 region(s) across 1 server(s), retainAssignment=true
2011-10-11 05:05:29,369 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Timeout-on-RIT=9000
2011-10-11 05:05:29,369 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning 9 region(s) to C3S3,54366,1318323920153
2011-10-11 05:05:29,369 INFO org.apache.hadoop.hbase.master.AssignmentManager: Bulk assigning done
2011-10-11 05:05:29,371 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for 771d63e9327383159553619a4f2dc74f with OFFLINE state
2011-10-11 05:05:29,371 INFO org.apache.hadoop.hbase.master.HMaster: Master has completed initialization
2011-10-11 05:05:29,371 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for 3cf860dd323fe6360f571aeafc129f95 with OFFLINE state
2011-10-11 05:05:29,371 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for 4065350214452a9d5c55243c734bef08 with OFFLINE state
2011-10-11 05:05:29,371 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for 4e81613f82a39fc6e5e89f96e7b3ccc4 with OFFLINE state
2011-10-11 05:05:29,371 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for e21b9e1545a28953aba0098fda5c9cd9 with OFFLINE state
2011-10-11 05:05:29,372 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for 5cd9f55eecd43d088bbd505f6795131f with OFFLINE state
2011-10-11 05:05:29,372 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for db5f641452a70b09b85a92970e4198c7 with OFFLINE state
2011-10-11 05:05:29,372 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for a7b20a653919e7f41bfb2ed349af7d21 with OFFLINE state
2011-10-11 05:05:29,372 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:58198-0x132f23a9a380000 Async create of unassigned node for c9385619425f737eab1a6624d2e097a8 with OFFLINE state
",sunnygao,sunnygao,Major,Closed,Fixed,12/Oct/11 00:52,20/Nov/15 11:52
Bug,HBASE-4582,12526891,Store.java cleanup (failing TestHeapSize and has warnings),"I broke TestHeapSize on 32-bit when I committed HBASE-4422.  Fix it here, remove another boolean variable I forgot to delete, and fix a couple warnings while we are at it.",streamy,streamy,Minor,Closed,Fixed,12/Oct/11 18:28,20/Nov/15 11:54
Bug,HBASE-4583,12526942,Integrate RWCC with Append and Increment operations,"Currently Increment and Append operations do not work with RWCC and hence a client could see the results of multiple such operation mixed in the same Get/Scan.
The semantics might be a bit more interesting here as upsert adds and removes to and from the memstore.
",larsh,larsh,Major,Closed,Fixed,12/Oct/11 22:11,28/Dec/17 05:57
Bug,HBASE-4588,12527123,The floating point arithmetic to validate memory allocation configurations need to be done as integers,"The floating point arithmetic to validate memory allocation configurations need to be done as integers.

On our cluster, we had block cache = 0.6 and memstore = 0.2.  It was saying this was > 0.8 when it is actually equal.

Minor bug but annoying nonetheless.",dhruba,streamy,Minor,Closed,Fixed,13/Oct/11 23:31,20/Nov/15 11:53
Bug,HBASE-4589,12527147,CacheOnWrite broken in some cases because it can conflict with evictOnClose,"Commit of HBASE-4078 added some extra StoreFile verification which just did an open of a StoreFile reader and then closes it, ensuring there's no exception.  If evict-on-close is on, which it is by default, this causes all blocks of a file to be evicted even though it's still open.

We need to add the boolean into the close call in the way we have booleans for cacheBlocks at some point since we need to make localized decisions in some cases.

In lots of places, we can always rely on cacheConf.shouldEvictOnClose() so shouldn't be too burdensome.",streamy,streamy,Critical,Closed,Fixed,14/Oct/11 06:02,20/Nov/15 11:55
Bug,HBASE-4595,12527293,HFilePrettyPrinter Scanned kv count always 0,"The ""count"" variable used to print the ""Scanned kv count"" is never incremented.
A local ""count"" variable in scanKeysValues() method is updated instead.",mbertozzi,mbertozzi,Minor,Closed,Fixed,15/Oct/11 14:10,20/Nov/15 11:53
Bug,HBASE-4599,12527356,[book] performance.xml - nit grammatical error in EC2 section,,dmeil,dmeil,Trivial,Closed,Fixed,16/Oct/11 15:13,12/Jun/22 19:39
Bug,HBASE-4607,12527509,Split log worker should terminate properly when waiting for znode,"This is an attempt to fix the fact that SplitLogWorker threads are not being terminated properly in some unit tests. This probably does not happen in production because the master always creates the log-splitting ZK node, but it does happen in 89-fb. Thanks to Prakash Khemani for help on this.",mikhail,mikhail,Minor,Closed,Fixed,17/Oct/11 22:17,12/Oct/12 05:35
Bug,HBASE-4609,12527520,"ThriftServer.getRegionInfo() is expecting old ServerName format, need to use new Addressing class instead",ThriftServer.getRegionInfo() is expecting the old ServerName that doesn't include start code.  Need to fix.,streamy,streamy,Minor,Closed,Fixed,17/Oct/11 23:43,12/Oct/12 05:34
Bug,HBASE-4610,12527521,"Port HBASE-3380 (Master failover can split logs of live servers) to 92/trunk (definitely bring in config params, decide if we need to do more to fix the bug)","Over in HBASE-3380 we were having some TestMasterFailover flakiness.  We added some more config parameters to better control the master startup loop where it waits for RS to heartbeat in.  We had thought at the time that 92 would have a different solution but it is still relying on heartbeats to learn about RSs.

For now, we should definitely bring these config params into 92/trunk.  Otherwise this is an incompatible regression and adding these will also make things like what was just reported over in HBASE-4603 trivial to fix in an optimal way.",streamy,streamy,Major,Closed,Fixed,17/Oct/11 23:50,12/Oct/12 05:35
Bug,HBASE-4620,12527786,I broke the build when I submitted HBASE-3581 (Send length of the rpc response),"Thanks to Ted, Ram and Gao for figuring my messup.",stack,stack,Blocker,Closed,Fixed,19/Oct/11 17:07,20/Nov/15 11:52
Bug,HBASE-4621,12527816,TestAvroServer fails quite often intermittently,,thehellmaker,thehellmaker,Major,Closed,Fixed,19/Oct/11 20:19,20/Nov/15 11:54
Bug,HBASE-4626,12527833,Filters unnecessarily copy byte arrays...,Just looked at SingleCol and ValueFilter... And on every column compared they create a copy of the column and/or value portion of the KV.,larsh,larsh,Major,Closed,Fixed,19/Oct/11 21:24,12/Oct/12 05:35
Bug,HBASE-4634,12527948,"""test.build.data"" property overused leading to write data at the wrong place","""test.build.data"" is overloaded in HBase.At the beginning, it's the ""Default parent directory for test output."", but then it's rewritten to be the directory itself in functions like HBaseTestingUtility#startMiniDFSCluster

It seems that this value is already used by MiniDFS (i.e. outside of HBase): 
""Name is as it is because mini dfs has hard-codings to put test data here.""

As it is today, there is at least a bug in HBaseTestingUtility:

{noformat}
  public void initTestDir() {
    if (System.getProperty(TEST_DIRECTORY_KEY) == null) {
      clusterTestBuildDir = setupClusterTestBuildDir();
      System.setProperty(TEST_DIRECTORY_KEY, clusterTestBuildDir.getPath());
    }
  }
{noformat}

if you set a value for ""test.build.data"", the test dir will be the parent directory and not a temp subdir, leading to issues as multiple tests will end-ups in the same (bad) directory. This function is barely used today, hence it's not visible, but I would like to use it in some new code.

A possible fix is to remove the check for null and continue with the overloading, but I don't think it would be a big issue to create a new key(like ""test.build.data.rootdirectory"") specific to the root directory and to use ""test.build.data"" only to communicate with MiniDFS. Feedback welcome.",nkeywal,nkeywal,Major,Closed,Fixed,20/Oct/11 15:42,20/Nov/15 11:56
Bug,HBASE-4641,12528040,Block cache can be mistakenly instantiated on Master,"After changes in the block cache instantiation over in HBASE-4422, it looks like the HMaster can now end up with a block cache instantiated.  Not a huge deal but prevents the process from shutting down properly.",streamy,streamy,Critical,Closed,Fixed,20/Oct/11 22:15,20/Nov/15 11:54
Bug,HBASE-4645,12528283,Edits Log recovery losing data across column families,"There is a data loss happening (for some of the column families) when we do the replay logs.

The bug seems to be from the fact that during replay-logs we only choose to replay
the logs from the maximumSequenceID across *ALL* the stores. This is wrong. If a
column family is ahead of others (because the crash happened before all the column
families were flushed), then we lose data for the column families that have not yet
caught up.

The correct logic for replay should begin the replay from the minimum across the
maximum in each store. ",amitanand,amitanand,Major,Closed,Fixed,21/Oct/11 14:45,24/Mar/13 05:18
Bug,HBASE-4647,12528403,RAT finds about 40 files missing licenses,"Giri is trying to hook up the patch build.  We need to pass the RAT tool.  I ran it, http://incubator.apache.org/rat/index.html, and found about 40 files missing licenses (and about 5 purely empty files).",stack,stack,Blocker,Closed,Fixed,23/Oct/11 00:00,20/Nov/15 11:52
Bug,HBASE-4648,12528411,Bytes.toBigDecimal() doesn't use offset,"The Bytes.toBigDecimal(byte[], offset, len) method does not use the offset, thus you will get an incorrect result for the BigDecimal unless the BigDecimal's bytes are at the beginning of the byte array.",,bryanck,Major,Closed,Fixed,23/Oct/11 09:52,12/Oct/12 05:34
Bug,HBASE-4658,12528592,Put attributes are not exposed via the ThriftServer,The Put api also takes in a bunch of arbitrary attributes that an application can use to associate metadata with each put operation. This is not exposed via Thrift.,dhruba,dhruba,Major,Closed,Fixed,25/Oct/11 00:27,12/Jul/13 12:49
Bug,HBASE-4660,12528608,Place to publish RegionServer information such as webuiport and coprocessors loaded,"HBASE-4070 added loaded CoProcessors to HServerLoad which seems like wrong place to carry this info.  We need a locus for static info of this type such as loaded CoProcessors and webuiport as well as stuff like how many cpus, RAM, etc: e.g. in regionserver znode or available on invocation of an HRegionServer method (master can ask HRegionServer when it needs it).",,stack,Major,Closed,Fixed,25/Oct/11 03:45,12/Jun/22 19:44
Bug,HBASE-4670,12528629,Fix javadoc warnings,We have hundreds of javadoc warnings emitted on every build.,stack,stack,Major,Closed,Fixed,25/Oct/11 06:41,20/Nov/15 11:53
Bug,HBASE-4671,12528693,HBaseTestingUtility unable to connect to regionserver because of 127.0.0.1 / 127.0.1.1 discrepancy,"When /etc/hosts contains following lines (and this is not uncommon) it will cause HBaseTestingUtility to malfunction.
127.0.0.1	localhost
127.0.1.1	myMachineName

Symptoms:
2011-10-25 17:38:30,875 WARN  master.AssignmentManager - Failed assignment of -ROOT-,,0.70236052 to serverName=localhost,34462,1319557102914, load=(requests=0, regions=0, usedHeap=46, maxHeap=865), trying to assign elsewhere instead; retry=0
org.apache.hadoop.hbase.client.RetriesExhaustedException: Failed setting up proxy interface org.apache.hadoop.hbase.ipc.HRegionInterface to /127.0.0.1:34462 after attempts=1

because

2011-10-25 17:38:28,371 INFO  regionserver.HRegionServer - Serving as localhost,34462,1319557102914, RPC listening on /127.0.1.1:34462, sessionid=0x1333bbb7a180002

caused by /127.0.0.1:34462 vs /127.0.1.1:34462

Workaround:
Changing 127.0.1.1 to 127.0.0.1 works.

Permanent solution:
Dunno, my understanding of inner workings is not sufficient enough. Although it seems like it has something to do with changing the machine name from myMachineName to localhost during the test:
2011-10-25 17:38:28,056 INFO  regionserver.HRegionServer - Master passed us address to use. Was=myMachineName:34462, Now=localhost:34462",,ferdy.g,Major,Closed,Fixed,25/Oct/11 15:42,12/Jun/22 19:44
Bug,HBASE-4673,12528758,NPE in HFileReaderV2.close during major compaction when hfile.block.cache.size is set to 0 ,"On a test system got this exception when hfile.block.cache.size is set to 0:

java.lang.NullPointerException
at org.apache.hadoop.hbase.io.hfile.HFileReaderV2.close(HFileReaderV2.java:321)
at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.close(StoreFile.java:1065)
at org.apache.hadoop.hbase.regionserver.StoreFile.closeReader(StoreFile.java:539)
at org.apache.hadoop.hbase.regionserver.StoreFile.deleteReader(StoreFile.java:549)
at org.apache.hadoop.hbase.regionserver.Store.completeCompaction(Store.java:1314)
at org.apache.hadoop.hbase.regionserver.Store.compact(Store.java:686)
at org.apache.hadoop.hbase.regionserver.HRegion.compact(HRegion.java:1016)
at org.apache.hadoop.hbase.regionserver.compactions.CompactionRequest.run(CompactionRequest.java:178)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619) 

Minor issue as nobody in their right mind with have hfile.block.cache.size=0

Looks like this is due to HBASE-4422",larsh,larsh,Minor,Closed,Fixed,25/Oct/11 21:06,12/Oct/12 05:35
Bug,HBASE-4679,12528809,Thrift null mutation error,"When using null as a value for a mutation, HBasse thrift client failed and threw an error. We should instad check for a null byte buffer.
",nspiegelberg,nspiegelberg,Major,Closed,Fixed,26/Oct/11 02:09,12/Oct/12 05:34
Bug,HBASE-4681,12528822,StringIndexOutOfBoundsException parsing Hostname,"Starting a 0.92 on 0.90 data with 0.90 zk ensemble I got this:

{code}
2011-10-26 06:13:53,920 ERROR org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Node /hbase/master already exists and this is not a retry
2011-10-26 06:13:53,927 FATAL org.apache.hadoop.hbase.master.HMaster: Unhandled exception. Starting shutdown.
java.lang.StringIndexOutOfBoundsException: String index out of range: -1
        at java.lang.String.substring(String.java:1937)
        at org.apache.hadoop.hbase.ServerName.parseHostname(ServerName.java:81)
        at org.apache.hadoop.hbase.ServerName.<init>(ServerName.java:63)
        at org.apache.hadoop.hbase.master.ActiveMasterManager.blockUntilBecomingActiveMaster(ActiveMasterManager.java:148)
        at org.apache.hadoop.hbase.master.HMaster.becomeActiveMaster(HMaster.java:346)
        at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:301)
        at java.lang.Thread.run(Thread.java:662)
2011-10-26 06:13:53,929 INFO org.apache.hadoop.hbase.master.HMaster: Aborting
2011-10-26 06:13:53,929 DEBUG org.apache.hadoop.hbase.master.HMaster: Stopping service thre
{code}

I thought this had been fixed.  Dig in .",,stack,Major,Closed,Fixed,26/Oct/11 06:15,20/Nov/15 11:54
Bug,HBASE-4684,12528998,REST server is leaking ZK connections in 0.90,"As reported a month ago, http://search-hadoop.com/m/FD6gmKzrxY1, the REST server is leak ZK connections. Upon investigation I see that TableResource.scanTransformAttrs creates a new HBA per minute per table (when the server is getting requests) but never deletes the connection created in there.

There are a bunch of other places where HBAs are created but not cleaned after like SchemaResource, StorageClusterStatusResource, StorageClusterVersionResource, ExistsResource, etc. Those places shouldn't be as leaky under normal circumstances tho.

Thanks to Jack Levin for bringing up this issue again when he tried to upgrade.",apurtell,jdcryans,Critical,Closed,Fixed,27/Oct/11 00:07,20/Nov/15 11:52
Bug,HBASE-4685,12529071,TestDistributedLogSplitting.testOrphanLogCreation failing because of ArithmeticException: / by zero.,,,stack,Major,Closed,Fixed,27/Oct/11 16:08,20/Nov/15 11:53
Bug,HBASE-4686,12529111,[89-fb] Fix per-store metrics aggregation ,"In r1182034 per-Store metrics were broken, because the aggregation of StoreFile metrics over all stores in a region was replaced by overriding them every time. We saw these metrics drop by a factor of numRegions on a production cluster -- thanks to Kannan for noticing this!  We need to fix the metrics and add a unit test to ensure regressions like this don't happen in the future.",mikhail,mikhail,Major,Closed,Fixed,27/Oct/11 18:32,12/Jun/22 19:45
Bug,HBASE-4687,12529123,regionserver may miss zk-heartbeats to master when replaying edits at region open,replayRecoveredEdits() should do another reporter.progress() before returning.,khemani,khemani,Major,Closed,Fixed,27/Oct/11 19:01,20/Nov/15 11:54
Bug,HBASE-4688,12529124,Make it so can run PE w/o having to put hbase jar on CLASSPATH,"I need this:

{code}
diff --git a/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java b/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java
index 3982eff..ef47d0d 100644
--- a/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java
+++ b/src/test/java/org/apache/hadoop/hbase/PerformanceEvaluation.java
@@ -570,6 +570,9 @@ public class PerformanceEvaluation {
     TextOutputFormat.setOutputPath(job, new Path(inputDir,""outputs""));
 
     TableMapReduceUtil.addDependencyJars(job);
+    // Add a Class from the hbase.jar so it gets registered too.
+    TableMapReduceUtil.addDependencyJars(job.getConfiguration(),
+      org.apache.hadoop.hbase.util.Bytes.class);
     job.waitForCompletion(true);
   }
{code}",stack,stack,Major,Closed,Fixed,27/Oct/11 19:10,20/Nov/15 11:55
Bug,HBASE-4691,12529177,Remove more unnecessary byte[] copies from KeyValues,Just looking through the code I found some more spots where we unnecessarily copy byte[] rather than just passing offset and length around.,larsh,larsh,Minor,Closed,Fixed,28/Oct/11 01:35,12/Oct/12 05:35
Bug,HBASE-4692,12529195,HBASE-4300 broke the build,Rebasing my patch I missed encoding of master name up in zk.  Creating a separate issue so can try patch build with my fix rather than run all tests locally.,stack,stack,Blocker,Closed,Fixed,28/Oct/11 04:46,20/Nov/15 11:52
Bug,HBASE-4695,12529312,WAL logs get deleted before region server can fully flush,"To replicate the problem do the following:

1. check /hbase/.logs/XXXX directory to see if you have WAL logs for the region server you are shutting down.
2. executing kill <pid> (where pid is a regionserver pid)
3. Watch the regionserver log to start flushing, you will see how many regions are left to flush:

09:36:54,665 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Waiting on 489 regions to close
09:56:35,779 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Waiting on 116 regions to close

4. Check /hbase/.logs/XXXX -- you will notice that it has dissapeared.
5. Check namenode logs:

09:26:41,607 INFO org.apache.hadoop.hdfs.server.namenode.FSNamesystem.audit: ugi=root ip=/10.101.1.5 cmd=delete src=/hbase/.logs/rdaa5.prod.imageshack.com,60020,1319749

Note that, if you kill -9 the RS now, and it crashes on flush, you won't have any WAL logs to replay.  We need to make sure that logs are deleted or moved out only when RS has fully flushed. Otherwise its possible to lose data.",sunnygao,jacque74,Blocker,Closed,Fixed,28/Oct/11 20:28,20/Nov/15 11:54
Bug,HBASE-4700,12529342,TestSplitTransactionOnCluster fails on occasion when it tries to move a region,"TestSplitTransactionOnCluster tries to have meta on one server and table region on a different server.  If the luck of the draw has the table and meta on same server, it'll move the table region elsewhere.  This has been failing since hbase-4300 because we've been doing HBaseAdmin#move passing versioned ServerName bytes when HBaseAdmin#move expects the raw, unversioned bytes.",stack,stack,Major,Closed,Fixed,29/Oct/11 04:16,20/Nov/15 11:52
Bug,HBASE-4701,12529344,TestMasterObserver fails up on jenkins,"This fails for various reasons from not being able to find the region we're to move in master list of regions to fails because of 'too many open files' (I believe Giri has fixed this on some of the hadoop nodes but looks like not all).

Here is a refactor around the move w/ asserts, more logging, use of updated apis, and with check that what we're moving has a server.",stack,stack,Major,Closed,Fixed,29/Oct/11 04:58,20/Nov/15 11:55
Bug,HBASE-4707,12529488,Avoid unnecessary DNS lookups for locality computation in HMaster,"We recently hit an issue where upon each RS heartbeat we were looking up and resolving DNS name + address of the RS in the master, but needed it only for locality based assignment on startup.

Some flakiness in the DNS subsystem cause one of the threads to get stuck in the lookup and the synchronized call at:

ServerManager.java:528
processMsgs() {
...
synchronized (this.master.getRegionManager()) {
// does dns lookup
}
}

The offending stack trace was:

""IPC Server handler 232 on 60000"" daemon prio=10 tid=0x00007fcb64164000 nid=0x7d16 runnable [0x0000000052e7f000]
java.lang.Thread.State: RUNNABLE
at java.net.Inet4AddressImpl.lookupAllHostAddr(Native Method)
at java.net.InetAddress$1.lookupAllHostAddr(InetAddress.java:849)
at java.net.InetAddress.getAddressFromNameService(InetAddress.java:1200)
at java.net.InetAddress.getAllByName0(InetAddress.java:1153)
at java.net.InetAddress.getAllByName0(InetAddress.java:1128)
at java.net.InetAddress.getHostFromNameService(InetAddress.java:550)
at java.net.InetAddress.getHostName(InetAddress.java:476)
at java.net.InetAddress.getHostName(InetAddress.java:448)
at java.net.InetSocketAddress.getHostName(InetSocketAddress.java:210)
at org.apache.hadoop.hbase.HServerAddress.getHostname(HServerAddress.java:117)
at org.apache.hadoop.hbase.master.RegionManager.regionsAwaitingAssignment(RegionManager.java:469)
at org.apache.hadoop.hbase.master.RegionManager.assignRegions(RegionManager.java:263)
at org.apache.hadoop.hbase.master.ServerManager.processMsgs(ServerManager.java:500)
- locked <0x00007fcb985b2030> (a org.apache.hadoop.hbase.master.RegionManager)
at org.apache.hadoop.hbase.master.ServerManager.processRegionServerAllsWell(ServerManager.java:425)
at org.apache.hadoop.hbase.master.ServerManager.regionServerReport(ServerManager.java:335)
at org.apache.hadoop.hbase.master.HMaster.regionServerReport(HMaster.java:841)
at sun.reflect.GeneratedMethodAccessor15.invoke(Unknown Source)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at org.apache.hadoop.hbase.ipc.HBaseRPC$Server.call(HBaseRPC.java:585)
at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:933)",karthik.ranga,karthik.ranga,Major,Closed,Fixed,31/Oct/11 14:51,12/Jun/22 19:48
Bug,HBASE-4709,12529581,Hadoop metrics2 setup in test MiniDFSClusters spewing JMX errors,"Since switching over HBase to build with Hadoop 0.20.205.0, we've been getting a lot of metrics related errors in the log files for tests:
{noformat}
2011-10-30 22:00:22,858 INFO  [main] log.Slf4jLog(67): jetty-6.1.26
2011-10-30 22:00:22,871 INFO  [main] log.Slf4jLog(67): Extract jar:file:/home/jenkins/.m2/repository/org/apache/hadoop/hadoop-core/0.20.205.0/hadoop-core-0.20.205.0.jar!/webapps/datanode to /tmp/Jetty_localhost_55751_datanode____.kw16hy/webapp
2011-10-30 22:00:23,048 INFO  [main] log.Slf4jLog(67): Started SelectChannelConnector@localhost:55751
Starting DataNode 1 with dfs.data.dir: /home/jenkins/jenkins-slave/workspace/HBase-TRUNK/trunk/target/test-data/7ba65a16-03ad-4624-b769-57405945ef58/dfscluster_3775fc23-1b51-4966-8133-205564bae762/dfs/data/data3,/home/jenkins/jenkins-slave/workspace/HBase-TRUNK/trunk/target/test-data/7ba65a16-03ad-4624-b769-57405945ef58/dfscluster_3775fc23-1b51-4966-8133-205564bae762/dfs/data/data4
2011-10-30 22:00:23,237 WARN  [main] impl.MetricsSystemImpl(137): Metrics system not started: Cannot locate configuration: tried hadoop-metrics2-datanode.properties, hadoop-metrics2.properties
2011-10-30 22:00:23,237 WARN  [main] util.MBeans(59): Hadoop:service=DataNode,name=MetricsSystem,sub=Control
javax.management.InstanceAlreadyExistsException: MXBean already registered with name Hadoop:service=NameNode,name=MetricsSystem,sub=Control
	at com.sun.jmx.mbeanserver.MXBeanLookup.addReference(MXBeanLookup.java:120)
	at com.sun.jmx.mbeanserver.MXBeanSupport.register(MXBeanSupport.java:143)
	at com.sun.jmx.mbeanserver.MBeanSupport.preRegister2(MBeanSupport.java:183)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerDynamicMBean(DefaultMBeanServerInterceptor.java:941)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:917)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:312)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:482)
	at org.apache.hadoop.metrics2.util.MBeans.register(MBeans.java:56)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.initSystemMBean(MetricsSystemImpl.java:500)
	at org.apache.hadoop.metrics2.impl.MetricsSystemImpl.init(MetricsSystemImpl.java:140)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.init(DefaultMetricsSystem.java:40)
	at org.apache.hadoop.metrics2.lib.DefaultMetricsSystem.initialize(DefaultMetricsSystem.java:50)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1483)
	at org.apache.hadoop.hdfs.server.datanode.DataNode.instantiateDataNode(DataNode.java:1459)
	at org.apache.hadoop.hdfs.MiniDFSCluster.startDataNodes(MiniDFSCluster.java:417)
	at org.apache.hadoop.hdfs.MiniDFSCluster.<init>(MiniDFSCluster.java:280)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniDFSCluster(HBaseTestingUtility.java:349)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:518)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:474)
	at org.apache.hadoop.hbase.HBaseTestingUtility.startMiniCluster(HBaseTestingUtility.java:461)
{noformat}

This seems to be due to errors initializing the new hadoop metrics2 code by default, when running in a mini cluster.  The errors themselves seem to be harmless -- they're not breaking any tests -- but we should figure out what configuration we need to eliminate them.",,ghelmling,Minor,Closed,Fixed,31/Oct/11 23:21,23/Sep/13 18:30
Bug,HBASE-4710,12529598,HBaseRPC$UnknownProtocolException should abort any client retries in HConnectionManager,"While {{HBaseRPC$UnknownProtocolException}} currently extends {{DoNotRetryIOException}}, it's still allowing retries of client RPCs when encountered in {{HConnectionManager.getRegionServerWithRetries()}}.  It turns out that {{UnknownProtocolException}} is missing a public constructor taking a single {{String}} argument, which is required when unwrapping an {{IOException}} from a {{RemoteException}} in {{RemoteExceptionHandler.decodeRemoteException()}}.",ghelmling,ghelmling,Major,Closed,Fixed,01/Nov/11 00:47,20/Nov/15 11:54
Bug,HBASE-4716,12529698,Improve locking for single column family bulk load,"HBASE-4552 changed the locking behavior for single column family bulk load, namely we don't need to take write lock.
A read lock would suffice in this scenario.",yuzhihong@gmail.com,yuzhihong@gmail.com,Critical,Closed,Fixed,01/Nov/11 17:10,20/Nov/15 11:52
Bug,HBASE-4718,12529713,Backport HBASE-4552 to 0.90 branch,"In discussion of HBASE-4552 / HBASE-4677 there has been some discussion about whether and how to backport HBASE-4552 to the 0.90 branch.  This is a potentially compatibility breaking so several approaches hav ebeen suggested.

1) provide patch but do not integrate
2) integrate patch that extends and deprecates old api without removing old api.  It has been argued that  clients are supposed to use LoadIncrementalHFiles api and not at the internal HRegionServer RPC api.",jmhsieh,jmhsieh,Major,Closed,Fixed,01/Nov/11 18:33,20/Nov/15 11:52
Bug,HBASE-4719,12529714,HBase script assumes pre-Hadoop 0.21 layout of jar files,"The following in the bin/hbase:

{noformat}
HADOOPCPPATH=$(append_path ""${HADOOPCPPATH}"" `ls ${HADOOP_HOME}/hadoop-core*.jar`)
{noformat}

assumes a pre-21 Hadoop layout. It'll be better to dynamically account for 
either hadoop-core* or hadoop-common*, hadoop-hdfs*, hadoop-mapreduce*
",rvs,rvs,Major,Closed,Fixed,01/Nov/11 18:36,20/Nov/15 11:55
Bug,HBASE-4724,12529815,TestAdmin hangs randomly in trunk,"fom the logs in my env
{noformat}
2011-11-01 15:48:40,744 WARN  [Master:0;localhost,39664,1320187706355] master.AssignmentManager(1471): Failed assignment of -ROOT-,,0.70236052 to localhost,44046,1320187706849, trying to assign elsewhere instead; retry=1
org.apache.hadoop.hbase.ipc.HBaseRPC$VersionMismatch: Protocol org.apache.hadoop.hbase.ipc.HRegionInterface version mismatch. (client = 28, server = 29)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine.getProxy(WritableRpcEngine.java:185)
	at org.apache.hadoop.hbase.ipc.HBaseRPC.getProxy(HBaseRPC.java:300)
{noformat}
Anyway, after this the logs finishes with:
{noformat}
2011-11-01 15:54:35,132 INFO  [Master:0;localhost,39664,1320187706355.oldLogCleaner] hbase.Chore(80): Master:0;localhost,39664,1320187706355.oldLogCleaner exiting
Process Thread Dump: Automatic Stack Trace every 60 seconds waiting on Master:0;localhost,39664,1320187706355
{noformat}
it's in
{noformat}
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:156)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:121)
    org.apache.hadoop.util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:149)
    org.apache.hadoop.hbase.util.Threads.threadDumpingIsAlive(Threads.java:113)
    org.apache.hadoop.hbase.LocalHBaseCluster.join(LocalHBaseCluster.java:405)
    org.apache.hadoop.hbase.MiniHBaseCluster.join(MiniHBaseCluster.java:408)
    org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniHBaseCluster(HBaseTestingUtility.java:616)
    org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:590)
    org.apache.hadoop.hbase.client.TestAdmin.tearDownAfterClass(TestAdmin.java:89)
{noformat}
So that's at least why adding a timeout wont help and may be why it does not end at all. Adding a maximum retry to Threads#threadDumpingIsAlive could help.

I also wonder if the root cause of the non ending is my modif on the wal, with some threads surprised to have updates that were not written in the wal. Here is the full stack dump:
{noformat}
Thread 354 (IPC Client (47) connection to localhost/127.0.0.1:52227 from nkeywal):
  State: TIMED_WAITING
  Blocked count: 360
  Waited count: 359
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.ipc.Client$Connection.waitForWork(Client.java:702)
    org.apache.hadoop.ipc.Client$Connection.run(Client.java:744)
Thread 272 (Master:0;localhost,39664,1320187706355-EventThread):
  State: WAITING
  Blocked count: 0
  Waited count: 4
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@107b954b
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)
Thread 271 (Master:0;localhost,39664,1320187706355-SendThread(localhost:21819)):
  State: RUNNABLE
  Blocked count: 2
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1107)
Thread 152 (Master:0;localhost,39664,1320187706355):
  State: WAITING
  Blocked count: 217
  Waited count: 174
  Waiting on org.apache.hadoop.hbase.zookeeper.RootRegionTracker@6621477c
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:485)
    org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.blockUntilAvailable(ZooKeeperNodeTracker.java:131)
    org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.blockUntilAvailable(ZooKeeperNodeTracker.java:104)
    org.apache.hadoop.hbase.catalog.CatalogTracker.waitForRoot(CatalogTracker.java:277)
    org.apache.hadoop.hbase.master.HMaster.assignRootAndMeta(HMaster.java:523)
    org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:468)
    org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:309)
    java.lang.Thread.run(Thread.java:662)
Thread 165 (LruBlockCache.EvictionThread):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread@3e9d7b56
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:485)
    org.apache.hadoop.hbase.io.hfile.LruBlockCache$EvictionThread.run(LruBlockCache.java:593)
    java.lang.Thread.run(Thread.java:662)
Thread 151 (main-EventThread):
  State: WAITING
  Blocked count: 40
  Waited count: 39
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@22531380
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:502)
Thread 150 (main-SendThread(localhost:21819)):
  State: RUNNABLE
  Blocked count: 55
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.zookeeper.ClientCnxn$SendThread.run(ClientCnxn.java:1107)
Thread 149 (IPC Server handler 4 on 39664):
  State: WAITING
  Blocked count: 3
  Waited count: 1968
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 148 (IPC Server handler 3 on 39664):
  State: WAITING
  Blocked count: 7
  Waited count: 1969
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 147 (IPC Server handler 2 on 39664):
  State: WAITING
  Blocked count: 11
  Waited count: 1968
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 146 (IPC Server handler 1 on 39664):
  State: WAITING
  Blocked count: 6
  Waited count: 1969
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 145 (IPC Server handler 0 on 39664):
  State: WAITING
  Blocked count: 3
  Waited count: 1970
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@73f5173f
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1224)
Thread 133 (IPC Server listener on 39664):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener.run(HBaseServer.java:580)
Thread 144 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.hbase.ipc.HBaseServer$Responder.run(HBaseServer.java:754)
Thread 143 (IPC Reader 9 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 142 (IPC Reader 8 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 141 (IPC Reader 7 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 140 (IPC Reader 6 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 139 (IPC Reader 5 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 138 (IPC Reader 4 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 137 (IPC Reader 3 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 136 (IPC Reader 2 on port 39664):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 135 (IPC Reader 1 on port 39664):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 134 (IPC Reader 0 on port 39664):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:471)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 124 (LeaseChecker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 407
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.DFSClient$LeaseChecker.run(DFSClient.java:1252)
    java.lang.Thread.run(Thread.java:662)
Thread 121 (ProcessThread:-1):
  State: WAITING
  Blocked count: 0
  Waited count: 269
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@7f5d84e0
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.server.PrepRequestProcessor.run(PrepRequestProcessor.java:103)
Thread 120 (SyncThread:0):
  State: WAITING
  Blocked count: 2
  Waited count: 252
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@3b25b27c
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.zookeeper.server.SyncRequestProcessor.run(SyncRequestProcessor.java:94)
Thread 119 (SessionTracker):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 207
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.zookeeper.server.SessionTrackerImpl.run(SessionTrackerImpl.java:142)
Thread 118 (NIOServerCxn.Factory:0.0.0.0/0.0.0.0:21819):
  State: RUNNABLE
  Blocked count: 38
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.zookeeper.server.NIOServerCnxn$Factory.run(NIOServerCnxn.java:232)
Thread 116 (org.apache.hadoop.hdfs.server.datanode.DataBlockScanner@2e6f947b):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 407
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:620)
    java.lang.Thread.run(Thread.java:662)
Thread 113 (IPC Server handler 2 on 39899):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1b4c72c8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 112 (IPC Server handler 1 on 39899):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1b4c72c8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 111 (IPC Server handler 0 on 39899):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@1b4c72c8
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 106 (IPC Server listener on 39899):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:439)
Thread 108 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:605)
Thread 102 (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@45d1c3cd):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
    sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:152)
    sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
    java.lang.Thread.run(Thread.java:662)
Thread 109 (DataNode: [/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data5,/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data6]):
  State: TIMED_WAITING
  Blocked count: 137
  Waited count: 283
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:986)
    org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1421)
    java.lang.Thread.run(Thread.java:662)
Thread 107 (pool-4-thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:333)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 105 (Timer-3):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:509)
    java.util.TimerThread.run(Timer.java:462)
Thread 104 (1353906974@qtp-1197660496-1 - Acceptor0 SelectChannelConnector@localhost:59858):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 103 (1185716172@qtp-1197660496-0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 8
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 101 (refreshUsed-/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data6):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:80)
    java.lang.Thread.run(Thread.java:662)
Thread 98 (refreshUsed-/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data5):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:80)
    java.lang.Thread.run(Thread.java:662)
Thread 90 (org.apache.hadoop.hdfs.server.datanode.DataBlockScanner@605b28c9):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 408
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:620)
    java.lang.Thread.run(Thread.java:662)
Thread 86 (IPC Server handler 2 on 56098):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@78ff22ed
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 85 (IPC Server handler 1 on 56098):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@78ff22ed
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 84 (IPC Server handler 0 on 56098):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@78ff22ed
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 80 (IPC Server listener on 56098):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:439)
Thread 82 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:605)
Thread 76 (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@2b68989e):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
    sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:152)
    sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
    java.lang.Thread.run(Thread.java:662)
Thread 83 (DataNode: [/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data3,/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data4]):
  State: TIMED_WAITING
  Blocked count: 149
  Waited count: 290
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:986)
    org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1421)
    java.lang.Thread.run(Thread.java:662)
Thread 81 (pool-3-thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:333)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 79 (Timer-2):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:509)
    java.util.TimerThread.run(Timer.java:462)
Thread 78 (348878159@qtp-164967086-1 - Acceptor0 SelectChannelConnector@localhost:36427):
  State: RUNNABLE
  Blocked count: 2
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 77 (1905729203@qtp-164967086-0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 8
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 75 (refreshUsed-/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data4):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:80)
    java.lang.Thread.run(Thread.java:662)
Thread 72 (refreshUsed-/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data3):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:80)
    java.lang.Thread.run(Thread.java:662)
Thread 61 (org.apache.hadoop.hdfs.server.datanode.DataBlockScanner@6709da93):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 408
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataBlockScanner.run(DataBlockScanner.java:620)
    java.lang.Thread.run(Thread.java:662)
Thread 60 (IPC Server handler 2 on 40388):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@56afd9e3
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 59 (IPC Server handler 1 on 40388):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@56afd9e3
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 58 (IPC Server handler 0 on 40388):
  State: WAITING
  Blocked count: 0
  Waited count: 1
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@56afd9e3
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 53 (IPC Server listener on 40388):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:439)
Thread 55 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:605)
Thread 49 (org.apache.hadoop.hdfs.server.datanode.DataXceiverServer@48baa31b):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.ServerSocketChannelImpl.accept0(Native Method)
    sun.nio.ch.ServerSocketChannelImpl.accept(ServerSocketChannelImpl.java:152)
    sun.nio.ch.ServerSocketAdaptor.accept(ServerSocketAdaptor.java:84)
    org.apache.hadoop.hdfs.server.datanode.DataXceiverServer.run(DataXceiverServer.java:131)
    java.lang.Thread.run(Thread.java:662)
Thread 57 (DataNode: [/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data1,/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data2]):
  State: TIMED_WAITING
  Blocked count: 148
  Waited count: 283
  Stack:
    java.lang.Object.wait(Native Method)
    org.apache.hadoop.hdfs.server.datanode.DataNode.offerService(DataNode.java:986)
    org.apache.hadoop.hdfs.server.datanode.DataNode.run(DataNode.java:1421)
    java.lang.Thread.run(Thread.java:662)
Thread 54 (pool-2-thread-1):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:333)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 52 (Timer-1):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 15
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:509)
    java.util.TimerThread.run(Timer.java:462)
Thread 51 (1031841975@qtp-1068922666-1 - Acceptor0 SelectChannelConnector@localhost:56535):
  State: RUNNABLE
  Blocked count: 1
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 50 (1191055321@qtp-1068922666-0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 8
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 48 (refreshUsed-/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data2):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:80)
    java.lang.Thread.run(Thread.java:662)
Thread 45 (refreshUsed-/tmp/d846d12d-19f3-439a-8cab-e05db67ae60b/dfscluster_2773516a-bc5a-4029-9ee0-fc499eee6ca8/dfs/data/data1):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 1
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.fs.DU$DURefreshThread.run(DU.java:80)
    java.lang.Thread.run(Thread.java:662)
Thread 31 (IPC Server handler 9 on 52227):
  State: WAITING
  Blocked count: 0
  Waited count: 64
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@64473a14
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 30 (IPC Server handler 8 on 52227):
  State: WAITING
  Blocked count: 3
  Waited count: 65
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@64473a14
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 29 (IPC Server handler 7 on 52227):
  State: WAITING
  Blocked count: 2
  Waited count: 66
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@64473a14
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 28 (IPC Server handler 6 on 52227):
  State: WAITING
  Blocked count: 1
  Waited count: 64
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@64473a14
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 27 (IPC Server handler 5 on 52227):
  State: WAITING
  Blocked count: 4
  Waited count: 68
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@64473a14
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 26 (IPC Server handler 4 on 52227):
  State: WAITING
  Blocked count: 0
  Waited count: 64
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@64473a14
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 25 (IPC Server handler 3 on 52227):
  State: WAITING
  Blocked count: 0
  Waited count: 65
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@64473a14
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 24 (IPC Server handler 2 on 52227):
  State: WAITING
  Blocked count: 1
  Waited count: 66
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@64473a14
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 23 (IPC Server handler 1 on 52227):
  State: WAITING
  Blocked count: 0
  Waited count: 64
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@64473a14
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 22 (IPC Server handler 0 on 52227):
  State: WAITING
  Blocked count: 3
  Waited count: 65
  Waiting on java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject@64473a14
  Stack:
    sun.misc.Unsafe.park(Native Method)
    java.util.concurrent.locks.LockSupport.park(LockSupport.java:158)
    java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
    java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
    org.apache.hadoop.ipc.Server$Handler.run(Server.java:1364)
Thread 15 (IPC Server listener on 52227):
  State: RUNNABLE
  Blocked count: 43
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener.run(Server.java:439)
Thread 17 (IPC Server Responder):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.apache.hadoop.ipc.Server$Responder.run(Server.java:605)
Thread 21 (Timer-0):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 17
  Stack:
    java.lang.Object.wait(Native Method)
    java.util.TimerThread.mainLoop(Timer.java:509)
    java.util.TimerThread.run(Timer.java:462)
Thread 20 (245022965@qtp-589734556-1 - Acceptor0 SelectChannelConnector@localhost:33079):
  State: RUNNABLE
  Blocked count: 2
  Waited count: 1
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    org.mortbay.io.nio.SelectorManager$SelectSet.doSelect(SelectorManager.java:498)
    org.mortbay.io.nio.SelectorManager.doSelect(SelectorManager.java:192)
    org.mortbay.jetty.nio.SelectChannelConnector.accept(SelectChannelConnector.java:124)
    org.mortbay.jetty.AbstractConnector$Acceptor.run(AbstractConnector.java:708)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)
Thread 19 (1821050251@qtp-589734556-0):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 8
  Stack:
    java.lang.Object.wait(Native Method)
    org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:626)
Thread 16 (pool-1-thread-1):
  State: RUNNABLE
  Blocked count: 45
  Waited count: 44
  Stack:
    sun.nio.ch.EPollArrayWrapper.epollWait(Native Method)
    sun.nio.ch.EPollArrayWrapper.poll(EPollArrayWrapper.java:210)
    sun.nio.ch.EPollSelectorImpl.doSelect(EPollSelectorImpl.java:65)
    sun.nio.ch.SelectorImpl.lockAndDoSelect(SelectorImpl.java:69)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:80)
    sun.nio.ch.SelectorImpl.select(SelectorImpl.java:84)
    org.apache.hadoop.ipc.Server$Listener$Reader.run(Server.java:333)
    java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    java.lang.Thread.run(Thread.java:662)
Thread 14 (org.apache.hadoop.hdfs.server.namenode.DecommissionManager$Monitor@620fa83):
  State: TIMED_WAITING
  Blocked count: 1
  Waited count: 147
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.namenode.DecommissionManager$Monitor.run(DecommissionManager.java:65)
    java.lang.Thread.run(Thread.java:662)
Thread 13 (org.apache.hadoop.hdfs.server.namenode.FSNamesystem$ReplicationMonitor@d5d4de6):
  State: TIMED_WAITING
  Blocked count: 2
  Waited count: 147
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.namenode.FSNamesystem$ReplicationMonitor.run(FSNamesystem.java:2689)
    java.lang.Thread.run(Thread.java:662)
Thread 12 (org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor@73d32e45):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 220
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.namenode.LeaseManager$Monitor.run(LeaseManager.java:368)
    java.lang.Thread.run(Thread.java:662)
Thread 11 (org.apache.hadoop.hdfs.server.namenode.FSNamesystem$HeartbeatMonitor@19be4777):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 89
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.namenode.FSNamesystem$HeartbeatMonitor.run(FSNamesystem.java:2666)
    java.lang.Thread.run(Thread.java:662)
Thread 10 (org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks$PendingReplicationMonitor@658f7386):
  State: TIMED_WAITING
  Blocked count: 0
  Waited count: 2
  Stack:
    java.lang.Thread.sleep(Native Method)
    org.apache.hadoop.hdfs.server.namenode.PendingReplicationBlocks$PendingReplicationMonitor.run(PendingReplicationBlocks.java:186)
    java.lang.Thread.run(Thread.java:662)
Thread 4 (Signal Dispatcher):
  State: RUNNABLE
  Blocked count: 0
  Waited count: 0
  Stack:
Thread 3 (Finalizer):
  State: WAITING
  Blocked count: 9
  Waited count: 10
  Waiting on java.lang.ref.ReferenceQueue$Lock@3d1a70a7
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:118)
    java.lang.ref.ReferenceQueue.remove(ReferenceQueue.java:134)
    java.lang.ref.Finalizer$FinalizerThread.run(Finalizer.java:159)
Thread 2 (Reference Handler):
  State: WAITING
  Blocked count: 11
  Waited count: 12
  Waiting on java.lang.ref.Reference$Lock@270d75a3
  Stack:
    java.lang.Object.wait(Native Method)
    java.lang.Object.wait(Object.java:485)
    java.lang.ref.Reference$ReferenceHandler.run(Reference.java:116)
Thread 1 (main):
  State: RUNNABLE
  Blocked count: 104
  Waited count: 97
  Stack:
    sun.management.ThreadImpl.getThreadInfo1(Native Method)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:156)
    sun.management.ThreadImpl.getThreadInfo(ThreadImpl.java:121)
    org.apache.hadoop.util.ReflectionUtils.printThreadInfo(ReflectionUtils.java:149)
    org.apache.hadoop.hbase.util.Threads.threadDumpingIsAlive(Threads.java:113)
    org.apache.hadoop.hbase.LocalHBaseCluster.join(LocalHBaseCluster.java:405)
    org.apache.hadoop.hbase.MiniHBaseCluster.join(MiniHBaseCluster.java:408)
    org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniHBaseCluster(HBaseTestingUtility.java:616)
    org.apache.hadoop.hbase.HBaseTestingUtility.shutdownMiniCluster(HBaseTestingUtility.java:590)
    org.apache.hadoop.hbase.client.TestAdmin.tearDownAfterClass(TestAdmin.java:89)
    sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    java.lang.reflect.Method.invoke(Method.java:597)
    org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
    org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
    org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
    org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:37)
    org.junit.runners.ParentRunner.run(ParentRunner.java:236)
    org.apache.maven.surefire.junit4.JUnit4TestSet.execute(JUnit4TestSet.java:53)
{noformat}",nkeywal,nkeywal,Major,Closed,Fixed,02/Nov/11 06:37,12/Jun/22 19:40
Bug,HBASE-4725,12529888,NPE in AM#updateTimers,,stack,stack,Major,Closed,Fixed,02/Nov/11 16:33,20/Nov/15 11:55
Bug,HBASE-4728,12529905,Clean up noisy HBaseAdmin#close messages,See tail of HBASE-4724,,stack,Major,Closed,Fixed,02/Nov/11 17:25,12/Jun/22 19:40
Bug,HBASE-4729,12529908,Clash between region unassign and splitting kills the master,"I was running an online alter while regions were splitting, and suddenly the master died and left my table half-altered (haven't restarted the master yet).

What killed the master:

{quote}
2011-11-02 17:06:44,428 FATAL org.apache.hadoop.hbase.master.HMaster: Unexpected ZK exception creating node CLOSING
org.apache.zookeeper.KeeperException$NodeExistsException: KeeperErrorCode = NodeExists for /hbase/unassigned/f7e1783e65ea8d621a4bc96ad310f101
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:110)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
        at org.apache.zookeeper.ZooKeeper.create(ZooKeeper.java:637)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.createNonSequential(RecoverableZooKeeper.java:459)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.create(RecoverableZooKeeper.java:441)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndWatch(ZKUtil.java:769)
        at org.apache.hadoop.hbase.zookeeper.ZKAssign.createNodeClosing(ZKAssign.java:568)
        at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1722)
        at org.apache.hadoop.hbase.master.AssignmentManager.unassign(AssignmentManager.java:1661)
        at org.apache.hadoop.hbase.master.BulkReOpen$1.run(BulkReOpen.java:69)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{quote}

A znode was created because the region server was splitting the region 4 seconds before:

{quote}
2011-11-02 17:06:40,704 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Starting split of region TestTable,0012469153,1320253135043.f7e1783e65ea8d621a4bc96ad310f101.
2011-11-02 17:06:40,704 DEBUG org.apache.hadoop.hbase.regionserver.SplitTransaction: regionserver:62023-0x132f043bbde0710 Creating ephemeral node for f7e1783e65ea8d621a4bc96ad310f101 in SPLITTING state
2011-11-02 17:06:40,751 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde0710 Attempting to transition node f7e1783e65ea8d621a4bc96ad310f101 from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLITTING
...
2011-11-02 17:06:44,061 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde0710 Successfully transitioned node f7e1783e65ea8d621a4bc96ad310f101 from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLIT
2011-11-02 17:06:44,061 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Still waiting on the master to process the split for f7e1783e65ea8d621a4bc96ad310f101
{quote}

Now that the master is dead the region server is spewing those last two lines like mad.",stack,jdcryans,Critical,Closed,Fixed,02/Nov/11 17:35,12/Oct/12 05:35
Bug,HBASE-4739,12530088,Master dying while going to close a region can leave it in transition forever,"I saw this in the aftermath of HBASE-4729 on a 0.92 refreshed yesterday, when the master died it had just created the RIT znode for a region but didn't tell the RS to close it yet.

When the master restarted it saw the znode and started printing this:

{quote}
2011-11-03 00:02:49,130 INFO org.apache.hadoop.hbase.master.AssignmentManager: Regions in transition timed out:  TestTable,0007560564,1320253568406.f76899564cabe7e9857c3aeb526ec9dc. state=CLOSING, ts=1320253605285, server=sv4r11s38,62003,1320195046948
2011-11-03 00:02:49,130 INFO org.apache.hadoop.hbase.master.AssignmentManager: Region has been CLOSING for too long, this should eventually complete or the server will expire, doing nothing
{quote}

It's never going to happen, and it's blocking balancing.

I'm marking this as minor since I believe this situation is pretty rare unless you hit other bugs while trying out stuff to root bugs out.",sunnygao,jdcryans,Minor,Closed,Fixed,03/Nov/11 17:41,20/Nov/15 11:52
Bug,HBASE-4740,12530090,[bulk load]  the HBASE-4552 API can't tell if errors on region server are recoverable,"Running TestHFileOutputFormat more frequently seems to show that it has become flaky.   It is difficult to tell if this is because of a unrecoverable failure or a recoverable failure.   To make this visiable from test and for users, we need to make a change to bulkload call's interface on HRegionServer.  The change should make successful rpcs return true, recoverable failures return false, and unrecoverable failure throw an IOException.  This is an RPC change, so it would be really good to get this api right before the final 0.92 goes out.",jmhsieh,jmhsieh,Blocker,Closed,Fixed,03/Nov/11 17:58,20/Nov/15 11:53
Bug,HBASE-4741,12530091,Online schema change doesn't return errors,"Still after the fun I had over in HBASE-4729, I tried to finish altering my table (remove a family) since only half of it was changed so I did this:

{quote}
hbase(main):002:0> alter 'TestTable', NAME => 'allo', METHOD => 'delete' 
Updating all regions with the new schema...
244/244 regions updated.
Done.
0 row(s) in 1.2480 seconds
{quote}

Nice it all looks good, but over in the master log:

{quote}
org.apache.hadoop.hbase.InvalidFamilyOperationException: Family 'allo' does not exist so cannot be deleted
        at org.apache.hadoop.hbase.master.handler.TableDeleteFamilyHandler.handleTableOperation(TableDeleteFamilyHandler.java:56)
        at org.apache.hadoop.hbase.master.handler.TableEventHandler.process(TableEventHandler.java:86)
        at org.apache.hadoop.hbase.master.HMaster.deleteColumn(HMaster.java:1011)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:348)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1242)
{quote}

Maybe we should do checks before launching the async task.

Marking critical as this is a regression.",stack,jdcryans,Critical,Closed,Fixed,03/Nov/11 18:00,20/Nov/15 11:54
Bug,HBASE-4745,12530132,LRU Statistics thread should be daemon,"Here was from 'HBase 0.92/Hadoop 0.22 test results' discussion on dev@hbase
{code}
""LRU Statistics #0"" prio=10 tid=0x00007f4edc7dd800 nid=0x211a waiting
on condition [0x00007f4e631e2000]
  java.lang.Thread.State: TIMED_WAITING (parking)
       at sun.misc.Unsafe.park(Native Method)
       - parking to wait for  <0x00007f4e88acc968> (a
java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
       at java.util.concurrent.locks.LockSupport.parkNanos(LockSupport.java:198)
       at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.awaitNanos(AbstractQueuedSynchronizer.java:2025)
       at java.util.concurrent.DelayQueue.take(DelayQueue.java:164)
       at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:583)
       at java.util.concurrent.ScheduledThreadPoolExecutor$DelayedWorkQueue.take(ScheduledThreadPoolExecutor.java:576)
       at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
       at java.lang.Thread.run(Thread.java:619)
{code}
We should make this thread daemon thread.",apurtell,yuzhihong@gmail.com,Major,Closed,Fixed,03/Nov/11 23:36,12/Oct/12 05:35
Bug,HBASE-4749,12530265,TestMasterFailover#testMasterFailoverWithMockedRITOnDeadRS occasionally fails,"look this logs:
https://builds.apache.org/view/G-L/view/HBase/job/HBase-0.92/105/testReport/org.apache.hadoop.hbase.master/TestMasterFailover/testMasterFailoverWithMockedRITOnDeadRS/

",stack,sunnygao,Critical,Closed,Fixed,04/Nov/11 12:45,20/Nov/15 11:52
Bug,HBASE-4753,12530478,org.apache.hadoop.hbase.regionserver.TestHRegionInfo#testGetSetOfHTD throws NPE on trunk,"testGetSetOfHTD(org.apache.hadoop.hbase.regionserver.TestHRegionInfo)  Time elapsed: 0.011 sec  <<< ERROR!
java.lang.NullPointerException
	at org.apache.hadoop.hbase.util.FSTableDescriptors.deleteTableDescriptorIfExists(FSTableDescriptors.java:433)
	at org.apache.hadoop.hbase.regionserver.TestHRegionInfo.testGetSetOfHTD(TestHRegionInfo.java:72)

Because the 'getTableInfoPath' can return null and it's not tested.
",nkeywal,nkeywal,Major,Closed,Fixed,06/Nov/11 09:10,20/Nov/15 11:54
Bug,HBASE-4754,12530528,FSTableDescriptors.getTableInfoPath() should handle FileNotFoundException,"As reported by Roman in the thread entitled 'HBase 0.92/Hadoop 0.22 test results', table creation would result in the following if hadoop 0.22 is the underlying platform:
{code}
11/11/05 19:08:48 INFO handler.CreateTableHandler: Attemping to create
the table b
11/11/05 19:08:48 ERROR handler.CreateTableHandler: Error trying to
create the table b
java.io.FileNotFoundException: File
hdfs://ip-10-110-254-200.ec2.internal:17020/hbase/b does not exist.
       at org.apache.hadoop.hdfs.DistributedFileSystem.listStatus(DistributedFileSystem.java:387)
       at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1085)
       at org.apache.hadoop.fs.FileSystem.listStatus(FileSystem.java:1110)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:257)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.getTableInfoPath(FSTableDescriptors.java:243)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:566)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:535)
       at org.apache.hadoop.hbase.util.FSTableDescriptors.createTableDescriptor(FSTableDescriptors.java:519)
       at org.apache.hadoop.hbase.master.handler.CreateTableHandler.handleCreateTable(CreateTableHandler.java:140)
       at org.apache.hadoop.hbase.master.handler.CreateTableHandler.process(CreateTableHandler.java:126)
       at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:168)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:619)
{code}
This was due to how DistributedFileSystem.listStatus() in 0.22 handles non-existent directory:
{code}
  @Override
  public FileStatus[] listStatus(Path p) throws IOException {
    String src = getPathName(p);

    // fetch the first batch of entries in the directory
    DirectoryListing thisListing = dfs.listPaths(
        src, HdfsFileStatus.EMPTY_NAME);

    if (thisListing == null) { // the directory does not exist
      throw new FileNotFoundException(""File "" + p + "" does not exist."");
    }
{code}
So in FSTableDescriptors.getTableInfoPath(), we should catch FileNotFoundException and treat it the same way as status being null.",yuzhihong@gmail.com,yuzhihong@gmail.com,Blocker,Closed,Fixed,07/Nov/11 05:07,20/Nov/15 11:53
Bug,HBASE-4757,12530643,[89-fb] Fix TestHQuorumPeer for non-default values of hbase.tmp.dir ,"TestHQuorumPeer currently fails if hbase.tmp.dir is different from /tmp/hbase-<username>. However, for our internal parallel test runner we use a different temporary HBase directory.",mikhail,mikhail,Minor,Closed,Fixed,07/Nov/11 22:50,12/Jun/22 19:47
Bug,HBASE-4758,12530661,[89-fb] Make test methods independent in TestMasterTransitions,"Currently TestMasterTransitions is flaky, and one way to hopefully make it more stable is to create a separate MiniHBaseCluster for every test method, and get rid of BeforeClass/AfterClass. So far I have successfully run TestMasterTransitions a few times with the fix, while it was failing without the fix.

TestMasterTransitions in trunk is a different story (most of the test is commented out in the trunk) and is out of scope of this JIRA.",mikhail,mikhail,Minor,Closed,Fixed,08/Nov/11 00:30,12/Jun/22 19:50
Bug,HBASE-4769,12531063,Abort RegionServer Immediately on OOME,"Currently, when the HRegionServer runs out of the memory, it will call master, which will cause more heap allocations and throw a second exception that it's run out of memory again. The easiest & safest way to avoid this OOME storm is to abort the RegionServer immediately when it hits the memory boundary.  Part of the 89-fb to trunk port.",nspiegelberg,nspiegelberg,Major,Closed,Fixed,10/Nov/11 22:27,20/Feb/13 11:40
Bug,HBASE-4773,12531108,HBaseAdmin may leak ZooKeeper connections,"When master crashs, HBaseAdmin will leaks ZooKeeper connections
I think we should close the zk connetion when throw MasterNotRunningException

 public HBaseAdmin(Configuration c)
  throws MasterNotRunningException, ZooKeeperConnectionException {
    this.conf = HBaseConfiguration.create(c);
    this.connection = HConnectionManager.getConnection(this.conf);
    this.pause = this.conf.getLong(""hbase.client.pause"", 1000);
    this.numRetries = this.conf.getInt(""hbase.client.retries.number"", 10);
    this.retryLongerMultiplier = this.conf.getInt(""hbase.client.retries.longer.multiplier"", 10);

    //we should add this code and close the zk connection
    try{
      this.connection.getMaster();
    }catch(MasterNotRunningException e){
      HConnectionManager.deleteConnection(conf, false);
      throw e;      
    }
  }",xufeng,sunnygao,Critical,Closed,Fixed,11/Nov/11 09:23,20/Nov/15 11:54
Bug,HBASE-4775,12531193,Remove -ea from all but tests; enable it if you need it testing,Follows on from discussion on the tail of HBASE-2753,stack,stack,Major,Closed,Fixed,11/Nov/11 23:07,20/Nov/15 11:54
Bug,HBASE-4776,12531196,HLog.closed should be checked inside of updateLock,Concurrency issue: HLog.closed is set inside the updateLock but not checked inside the lock.,khemani,nspiegelberg,Major,Closed,Fixed,11/Nov/11 23:21,12/Oct/12 05:34
Bug,HBASE-4777,12531203,Write back to client 'incompatible' if we show up with wrong version,"We changed the RPC_VERSION to 4 in hbase-3939.  If a client comes in volunteering RPC_VERSION is 3, currently, we'll log 'wrong version' but we'll close the connection; the client has no chance of knowing why the server went away.

Returning -1 as id up out of here is what causes the connection close:

{code}
    private void setupBadVersionResponse(int clientVersion) throws IOException {
      String errMsg = ""Server IPC version "" + CURRENT_VERSION +
      "" cannot communicate with client version "" + clientVersion;
      ByteArrayOutputStream buffer = new ByteArrayOutputStream();

      if (clientVersion >= 3) {
        Call fakeCall =  new Call(-1, null, this, responder);
        // Versions 3 and greater can interpret this exception
        // response in the same manner
        setupResponse(buffer, fakeCall, Status.FATAL,
            null, VersionMismatch.class.getName(), errMsg);

        responder.doRespond(fakeCall);
      }
    }
{code}

Instead, we need to return an id that does not close the connection so cilent gets chance to read the response.

Suggestion is that we return a 0 for the id.... the connection will stay up.

If an old client and it sends the wrong version, it'll move on to do getProtocolVersion... and will fail there.

Other clients, e.g. asynchbase, if they get a response will have a response to switch what they send to suit the new server.

(There are other issues -- e.g. Invocation is versioned now -- but Benoit needs some means of figuring whats on other side)",stack,stack,Major,Closed,Fixed,12/Nov/11 00:51,20/Nov/15 11:55
Bug,HBASE-4778,12531207,Don't ignore corrupt StoreFiles when opening a region,"We used to ignore StoreFiles that failed to open, which led to a situation when only a subset of regions was opened, and HBase did not return results to clients for the affected set of keys. This change makes sure we propagate IOExceptions coming from an attempt to open a StoreFile all the way up to HRegionServer.openRegion, where it will lead to a failure to open the whole region. This way we can avoid returning corrupt data to the application.",mikhail,nspiegelberg,Major,Closed,Fixed,12/Nov/11 02:57,12/Oct/12 05:34
Bug,HBASE-4784,12531391,Void return types not handled correctly for CoprocessorProtocol methods,"If a CoprocessorProtocol derived interface defines a method with a void return type, the method cannot be called using HTable.coprocessorExec().  Instead ExecResult will throw an IOException on the client trying to do a Class.forName() on ""void"".

Looking at ExecResult, it appears that the valueType field (which causes the error) is no longer even used, so I'd suggest we just get rid of it.",ghelmling,ghelmling,Critical,Closed,Fixed,15/Nov/11 00:42,20/Nov/15 11:54
Bug,HBASE-4789,12531415,"On split, parent region is sticking around in oldest sequenceid to region map though not online; we don't cleanup WALs.","Here is log for a particular region:

{code}
2011-11-15 05:46:31,382 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Still waiting on the master to process the split for 8bbd7388262dc8cb1ce2cf4f04a7281d
2011-11-15 05:46:31,483 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:7003-0x1337b0b92cd000a-0x1337b0b92cd000a Attempting to transition node 8bbd7388262dc8cb1ce2cf4f04a7281d from RS_ZK_REGION_SPLIT to RS_ZK_REG
ION_SPLIT
2011-11-15 05:46:31,484 INFO org.apache.hadoop.hbase.regionserver.SplitRequest: Region split, META updated, and report to master. Parent=TestTable,0862220095,1321335865649.8bbd7388262dc8cb1ce2cf4f04a7281d., new regions: TestTab
le,0862220095,1321335989689.f00c683df3182d8ef33e315f77ca539c., TestTable,0892568091,1321335989689.a56ca1eff5b4401432fcba04b4e851f8.. Split took 1sec
2011-11-15 05:46:37,705 DEBUG org.apache.hadoop.hbase.regionserver.Store: Compacting hdfs://sv4r11s38:7000/hbase/TestTable/a56ca1eff5b4401432fcba04b4e851f8/info/9ce16d8fa94e4938964c04775a6fa1a7.8bbd7388262dc8cb1ce2cf4f04a7281d-
hdfs://sv4r11s38:7000/hbase/TestTable/8bbd7388262dc8cb1ce2cf4f04a7281d/info/9ce16d8fa94e4938964c04775a6fa1a7-top, keycount=717559, bloomtype=NONE, size=711.1m
2011-11-15 05:46:37,705 DEBUG org.apache.hadoop.hbase.regionserver.Store: Compacting hdfs://sv4r11s38:7000/hbase/TestTable/a56ca1eff5b4401432fcba04b4e851f8/info/9213f4d7ee9b4fda857a97603a001f9e.8bbd7388262dc8cb1ce2cf4f04a7281d-
hdfs://sv4r11s38:7000/hbase/TestTable/8bbd7388262dc8cb1ce2cf4f04a7281d/info/9213f4d7ee9b4fda857a97603a001f9e-top, keycount=416691, bloomtype=NONE, size=412.9m
2011-11-15 05:46:53,090 DEBUG org.apache.hadoop.hbase.regionserver.Store: Compacting hdfs://sv4r11s38:7000/hbase/TestTable/f00c683df3182d8ef33e315f77ca539c/info/9ce16d8fa94e4938964c04775a6fa1a7.8bbd7388262dc8cb1ce2cf4f04a7281d-
hdfs://sv4r11s38:7000/hbase/TestTable/8bbd7388262dc8cb1ce2cf4f04a7281d/info/9ce16d8fa94e4938964c04775a6fa1a7-bottom, keycount=717559, bloomtype=NONE, size=711.1m
2011-11-15 05:46:53,090 DEBUG org.apache.hadoop.hbase.regionserver.Store: Compacting hdfs://sv4r11s38:7000/hbase/TestTable/f00c683df3182d8ef33e315f77ca539c/info/9213f4d7ee9b4fda857a97603a001f9e.8bbd7388262dc8cb1ce2cf4f04a7281d-
hdfs://sv4r11s38:7000/hbase/TestTable/8bbd7388262dc8cb1ce2cf4f04a7281d/info/9213f4d7ee9b4fda857a97603a001f9e-bottom, keycount=416691, bloomtype=NONE, size=412.9m
2011-11-15 05:48:00,690 DEBUG org.apache.hadoop.hbase.regionserver.wal.HLog: Found 3 hlogs to remove out of total 12; oldest outstanding sequenceid is 5699 from region 8bbd7388262dc8cb1ce2cf4f04a7281d
2011-11-15 05:57:54,083 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Too many hlogs: logs=33, maxlogs=32; forcing flush of 1 regions(s): 8bbd7388262dc8cb1ce2cf4f04a7281d
2011-11-15 05:57:54,083 WARN org.apache.hadoop.hbase.regionserver.LogRoller: Failed to schedule flush of 8bbd7388262dc8cb1ce2cf4f04a7281dr=null, requester=null
2011-11-15 05:58:01,358 INFO org.apache.hadoop.hbase.regionserver.wal.HLog: Too many hlogs: logs=34, maxlogs=32; forcing flush of 1 regions(s): 8bbd7388262dc8cb1ce2cf4f04a7281d
2011-11-15 05:58:01,359 WARN org.apache.hadoop.hbase.regionserver.LogRoller: Failed to schedule flush of 8bbd7388262dc8cb1ce2cf4f04a7281dr=null, requester=null
{code}",stack,stack,Critical,Closed,Fixed,15/Nov/11 06:31,20/Nov/15 11:55
Bug,HBASE-4790,12531428,Occasional TestDistributedLogSplitting failure,"looks this link:
https://builds.apache.org/job/PreCommit-HBASE-Build/253//testReport/org.apache.hadoop.hbase.master/TestDistributedLogSplitting/testRecoveredEdits/

// it said that regions is 0.
2011-11-15 03:53:11,215 INFO  [Thread-2335] master.TestDistributedLogSplitting(211): #regions = 0
2011-11-15 03:53:11,215 DEBUG [RegionServer:0;asf001.sp2.ygridcore.net,36721,1321329179789.logSyncer] wal.HLog$LogSyncer(1192): RegionServer:0;asf001.sp2.ygridcore.net,36721,1321329179789.logSyncer interrupted while waiting for sync requests
2011-11-15 03:53:11,215 INFO  [RegionServer:0;asf001.sp2.ygridcore.net,36721,1321329179789.logSyncer] wal.HLog$LogSyncer(1194): RegionServer:0;asf001.sp2.ygridcore.net,36721,1321329179789.logSyncer exiting
2011-11-15 03:53:11,215 DEBUG [Thread-2335] wal.HLog(967): closing hlog writer in hdfs://localhost:46229/user/jenkins/.logs/asf001.sp2.ygridcore.net,36721,1321329179789
2011-11-15 03:53:11,637 DEBUG [Thread-2335] master.SplitLogManager(233): Scheduling batch of logs to split

",sunnygao,sunnygao,Minor,Closed,Fixed,15/Nov/11 08:57,12/Oct/12 05:35
Bug,HBASE-4792,12531550,"SplitRegionHandler doesn't care if it deletes the znode or not, leaves the parent region stuck offline","Saw this on a little test cluster, really easy to trigger.

First the master log:

{quote}
2011-11-15 22:28:57,900 DEBUG org.apache.hadoop.hbase.master.handler.SplitRegionHandler: Handling SPLIT event for e5be6551c8584a6a1065466e520faf4e; deleting node
2011-11-15 22:28:57,900 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x132f043bbde08c1 Deleting existing unassigned node for e5be6551c8584a6a1065466e520faf4e that is in expected state RS_ZK_REGION_SPLIT
2011-11-15 22:28:57,975 WARN org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x132f043bbde08c1 Attempting to delete unassigned node in RS_ZK_REGION_SPLIT state but after verifying state, we got a version mismatch
2011-11-15 22:28:57,975 INFO org.apache.hadoop.hbase.master.handler.SplitRegionHandler: Handled SPLIT report); parent=TestTable,0001355346,1321396080924.e5be6551c8584a6a1065466e520faf4e. daughter a=TestTable,0001355346,1321396132414.df9b549eb594a1f8728608a2a431224a.daughter b=TestTable,0001368082,1321396132414.de861596db4337dc341138f26b9c8bc2.
...
2011-11-15 22:28:58,052 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_SPLIT, server=sv4r28s44,62023,1321395865619, region=e5be6551c8584a6a1065466e520faf4e
2011-11-15 22:28:58,052 WARN org.apache.hadoop.hbase.master.AssignmentManager: Region e5be6551c8584a6a1065466e520faf4e not found on server sv4r28s44,62023,1321395865619; failed processing
2011-11-15 22:28:58,052 WARN org.apache.hadoop.hbase.master.AssignmentManager: Received SPLIT for region e5be6551c8584a6a1065466e520faf4e from server sv4r28s44,62023,1321395865619 but it doesn't exist anymore, probably already processed its split
(repeated forever)
{quote}

The master processes the split but when it calls ZKAssign.deleteNode it doesn't check the boolean that's returned. In this case it was false. So for the master the split was completed, but for the region server it's another story:

{quote}
2011-11-15 22:28:57,661 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde08d3 Attempting to transition node e5be6551c8584a6a1065466e520faf4e from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLIT
2011-11-15 22:28:57,775 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde08d3 Successfully transitioned node e5be6551c8584a6a1065466e520faf4e from RS_ZK_REGION_SPLITTING to RS_ZK_REGION_SPLIT
2011-11-15 22:28:57,775 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Still waiting on the master to process the split for e5be6551c8584a6a1065466e520faf4e
2011-11-15 22:28:57,876 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde08d3 Attempting to transition node e5be6551c8584a6a1065466e520faf4e from RS_ZK_REGION_SPLIT to RS_ZK_REGION_SPLIT
2011-11-15 22:28:57,967 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde08d3 Successfully transitioned node e5be6551c8584a6a1065466e520faf4e from RS_ZK_REGION_SPLIT to RS_ZK_REGION_SPLIT
2011-11-15 22:28:58,067 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde08d3 Attempting to transition node e5be6551c8584a6a1065466e520faf4e from RS_ZK_REGION_SPLIT to RS_ZK_REGION_SPLIT
2011-11-15 22:28:58,108 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:62023-0x132f043bbde08d3 Successfully transitioned node e5be6551c8584a6a1065466e520faf4e from RS_ZK_REGION_SPLIT to RS_ZK_REGION_SPLIT
(printed forever)
{quote}

Since the znode isn't really deleted, it thinks the master just haven't got to process its region thus waits which leaves the region *unavailable*.

We need to just retry the delete master-side ASAP since the RS will wait 100ms between retries.

At the same time, it would be nice if ZKAssign.deleteNode always printed out the name of the region in its messages because it took me a while to see that the delete didn't take affect while looking at a grep.",jdcryans,jdcryans,Critical,Closed,Fixed,15/Nov/11 23:26,12/Oct/12 05:34
Bug,HBASE-4793,12531558,HBase shell still using deprecated methods removed in HBASE-4436,"The patch applied in HBASE-4622 (subtask of HBASE-4436) to remove deprecated methods seems to have missed some usage of those methods by the HBase shell.  At least src/main/ruby/hbase/admin.rb is still using some of the removed methods, breaking some shell commands:

{noformat}
hbase(main):007:0> alter 'privatetable', { NAME => 'f1', VERSIONS => 2}

ERROR: wrong number of arguments (3 for 2)
Backtrace: /usr/lib/hbase/bin/../bin/../lib/ruby/hbase/admin.rb:344:in `alter'
           org/jruby/RubyArray.java:1572:in `each'
           /usr/lib/hbase/bin/../bin/../lib/ruby/hbase/admin.rb:317:in `alter'
           /usr/lib/hbase/bin/../bin/../lib/ruby/shell/commands/alter.rb:79:in `command'
           /usr/lib/hbase/bin/../bin/../lib/ruby/shell/commands.rb:68:in `format_simple_command'
           /usr/lib/hbase/bin/../bin/../lib/ruby/shell/commands/alter.rb:78:in `command'
           /usr/lib/hbase/bin/../bin/../lib/ruby/shell/commands.rb:31:in `command_safe'
           /usr/lib/hbase/bin/../bin/../lib/ruby/shell/commands.rb:74:in `translate_hbase_exceptions'
           /usr/lib/hbase/bin/../bin/../lib/ruby/shell/commands.rb:31:in `command_safe'
           /usr/lib/hbase/bin/../bin/../lib/ruby/shell.rb:110:in `command'
           (eval):2:in `alter'
{noformat}

This trace translates to the line:
{code}
  @admin.modifyColumn(table_name, column_name, descriptor)
{code}

which is calling one of the removed methods.
",ghelmling,ghelmling,Critical,Closed,Fixed,16/Nov/11 00:12,20/Nov/15 11:55
Bug,HBASE-4795,12531566,Fix TestHFileBlock when running on a 32-bit JVM,Our Hudson test server seems to run a 32-bit JVM. This patch fixes TestHFileBlock to work correctly for both 64-bit and 32-bit JVM.,mikhail,mikhail,Minor,Closed,Fixed,16/Nov/11 01:39,12/Oct/12 05:35
Bug,HBASE-4796,12531572,Race between SplitRegionHandlers for the same region kills the master,"I just saw that multiple SplitRegionHandlers can be created for the same region because of the RS tickling, but it becomes deadly when more than 1 are trying to delete the znode at the same time:

{quote}
2011-11-16 02:25:28,778 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_SPLIT, server=sv4r7s38,62023,1321410237387, region=f80b6a904048a99ce88d61420b8906d1
2011-11-16 02:25:28,780 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_SPLIT, server=sv4r7s38,62023,1321410237387, region=f80b6a904048a99ce88d61420b8906d1
2011-11-16 02:25:28,796 DEBUG org.apache.hadoop.hbase.master.handler.SplitRegionHandler: Handling SPLIT event for f80b6a904048a99ce88d61420b8906d1; deleting node
2011-11-16 02:25:28,798 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x132f043bbde094b Deleting existing unassigned node for f80b6a904048a99ce88d61420b8906d1 that is in expected state RS_ZK_REGION_SPLIT
2011-11-16 02:25:28,804 DEBUG org.apache.hadoop.hbase.master.handler.SplitRegionHandler: Handling SPLIT event for f80b6a904048a99ce88d61420b8906d1; deleting node
2011-11-16 02:25:28,806 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x132f043bbde094b Deleting existing unassigned node for f80b6a904048a99ce88d61420b8906d1 that is in expected state RS_ZK_REGION_SPLIT
2011-11-16 02:25:28,821 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:62003-0x132f043bbde094b Successfully deleted unassigned node for region f80b6a904048a99ce88d61420b8906d1 in expected state RS_ZK_REGION_SPLIT
2011-11-16 02:25:28,821 INFO org.apache.hadoop.hbase.master.handler.SplitRegionHandler: Handled SPLIT report); parent=TestTable,0000006304,1321409743253.f80b6a904048a99ce88d61420b8906d1. daughter a=TestTable,0000006304,1321410325564.e0f5d201683bcabe14426817224334b8.daughter b=TestTable,0000007054,1321410325564.1b82eeb5d230c47ccc51c08256134839.
2011-11-16 02:25:28,829 WARN org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper: Node /hbase/unassigned/f80b6a904048a99ce88d61420b8906d1 already deleted, and this is not a retry
2011-11-16 02:25:28,830 FATAL org.apache.hadoop.hbase.master.HMaster: Error deleting SPLIT node in ZK for transition ZK node (f80b6a904048a99ce88d61420b8906d1)
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase/unassigned/f80b6a904048a99ce88d61420b8906d1
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:102)
	at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
	at org.apache.zookeeper.ZooKeeper.delete(ZooKeeper.java:728)
	at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.delete(RecoverableZooKeeper.java:107)
	at org.apache.hadoop.hbase.zookeeper.ZKUtil.deleteNode(ZKUtil.java:884)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.deleteNode(ZKAssign.java:506)
	at org.apache.hadoop.hbase.zookeeper.ZKAssign.deleteNode(ZKAssign.java:453)
	at org.apache.hadoop.hbase.master.handler.SplitRegionHandler.process(SplitRegionHandler.java:95)
	at org.apache.hadoop.hbase.executor.EventHandler.run(EventHandler.java:168)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{quote}

Stack and I came up with the solution that we need just manage that exception because handleSplitReport is an in-memory thing.",ram_krish,jdcryans,Major,Closed,Fixed,16/Nov/11 02:46,20/Nov/15 11:53
Bug,HBASE-4797,12531573,[availability] Skip recovered.edits files with edits we know older than what region currently has,"Testing 0.92, I crashed all servers out.  Another bug makes it so WALs are not getting cleaned so I had 7000 regions to replay.  The distributed split code did a nice job and cluster came back but interesting is that some hot regions ended up having loads of recovered.edits files -- tens if not hundreds -- to replay against the region (can we bulk load recovered.edits instead of replaying them?).  Each recovered.edits file is taking about a second to process (though only about 30 odd edits per file it seems).  The region is unavailable during this time.",jxiang,stack,Critical,Closed,Fixed,16/Nov/11 02:47,05/Aug/14 20:11
Bug,HBASE-4799,12531616,Catalog Janitor logic bug causes region leackage,"When region split takes a significant amount of time, CatalogJanitor can cleanup one of SPLIT records, but left another in META. When another split finish, janitor cleans left SPLIT record, but parent regions haven't removed from FS and META not cleared.

The race condition is follows:
1. region split started
2. one of regions splitted, i.e. A (have no reference storefiles) but other (B) doesn't
3. janitor started and in routine checkDaughter removes SPLITA from meta, but see that SPLITB has references and does nothing.
4. region B completes split
5. janitor wakes up, removes SPLITB, but see that there is no records for A and does nothing again.

Result - parent region hangs forever.",shmuma,shmuma,Critical,Closed,Fixed,16/Nov/11 13:36,20/Nov/15 11:52
Bug,HBASE-4800,12531658,Result.compareResults is incorrect,"A coworker of mine (James Taylor) found a bug in Result.compareResults(...).
This condition:
{code}
      if (!ourKVs[i].equals(replicatedKVs[i]) &&
          !Bytes.equals(ourKVs[i].getValue(), replicatedKVs[i].getValue())) {
        throw new Exception(""This result was different: ""
{code}
should be
{code}
      if (!ourKVs[i].equals(replicatedKVs[i]) ||
          !Bytes.equals(ourKVs[i].getValue(), replicatedKVs[i].getValue())) {
        throw new Exception(""This result was different: ""
{code}

Just checked, this is wrong in all branches.",larsh,larsh,Major,Closed,Fixed,16/Nov/11 18:06,20/Nov/15 11:54
Bug,HBASE-4801,12531693,alter_status shell prints sensible message at completion,The alter_status command used to print 0/0 once an alter operation had completed and its progress was no longer available. Now it instad indicates that all regions were updated.,nspiegelberg,nspiegelberg,Trivial,Closed,Fixed,16/Nov/11 21:15,20/Nov/15 11:52
Bug,HBASE-4802,12531695,Disable show table metrics in bulk loader,"During bulk load, the Configuration object may be set to null.  This caused an NPE in per-CF metrics because it consults the Configuration to determine whether to show the Table name.  Need to add simple change to allow the conf to be null & not specify table name in that instance.",liyin,nspiegelberg,Trivial,Closed,Fixed,16/Nov/11 21:27,23/Sep/13 18:45
Bug,HBASE-4804,12531711,Minor Dyslexia in CHANGES.txt,I was going through the 0.92 CHANGES and found are a few entries in CHANGES.txt where jira numbers don't match up descriptions.  ,jmhsieh,jmhsieh,Major,Closed,Fixed,16/Nov/11 23:18,20/Nov/15 11:52
Bug,HBASE-4805,12531712,Allow better control of resource consumption in HTable,"From some internal discussions at Salesforce we concluded that we need better control over the resources (mostly threads) consumed by HTable when used in a AppServer with many client threads.

Since HTable is not thread safe, the only options are cache them (in a custom thread local or using HTablePool) or to create them on-demand.

I propose a simple change: Add a new constructor to HTable that takes an optional ExecutorService and HConnection instance. That would make HTable a pretty lightweight object and we would manage the ES and HC separately.

I'll upload a patch a soon to get some feedback.",larsh,larsh,Major,Closed,Fixed,16/Nov/11 23:22,02/May/13 02:29
Bug,HBASE-4806,12531731,Fix logging message in HbaseObjectWritable,This is a trivial fix to HBASE-3316 to fix an error message.,jmhsieh,jmhsieh,Trivial,Closed,Fixed,17/Nov/11 01:59,20/Nov/15 11:56
Bug,HBASE-4816,12531879,Regionserver wouldn't go down because split happened exactly at same time we issued bulk user region close call on our way out,"A regionserver wouldn't go down because it was waiting on a user region to close only the user-space region had just been opened as part of a split transaction -- it was a new daughter -- just as we'd issued the bulk close to all user regions on receipt of a cluster shutdown call.

We need to add a check for this condition -- user tables that did not get the close.",stack,stack,Major,Closed,Fixed,18/Nov/11 00:54,20/Nov/15 11:53
Bug,HBASE-4819,12531979,TestShell broke in trunk; typo,,stack,stack,Major,Closed,Fixed,18/Nov/11 18:44,12/Oct/12 05:35
Bug,HBASE-4823,12532030,long running scans lose benefit of bloomfilters and timerange hints,"When you have a long running scan due to say an MR job, you can lose the benefit of timerange hints & bloom filters midway if your scanner gets reset. [Note: The scanners can get reset say due to a flush or compaction].

In one of our workloads, we periodically want to do rollups on recent 15 minutes of data in a column family... but the timerange hint benefit is lost midway when this resetScannerStack (shown below) happens. And end result-- we end up reading all the old HFiles rather than just the recent HFiles.

{code}

 private void resetScannerStack(KeyValue lastTopKey) throws IOException {
    if (heap != null) {
      throw new RuntimeException(""StoreScanner.reseek run on an existing heap!"");
    }

    /* When we have the scan object, should we not pass it to getScanners()
     * to get a limited set of scanners? We did so in the constructor and we
     * could have done it now by storing the scan object from the constructor */
    List<KeyValueScanner> scanners = getScanners();
{code}

The comment in the code seems to be aware of this issue and even has the suggested fix!
",amitanand,kannanm,Major,Closed,Fixed,19/Nov/11 02:15,12/Jun/22 19:49
Bug,HBASE-4825,12532053,TestRegionServersMetrics and TestZKLeaderManager are not categorized (small/medium/large),see title,nkeywal,nkeywal,Major,Closed,Fixed,19/Nov/11 18:07,12/Oct/12 05:35
Bug,HBASE-4826,12532056,Modify hbasetests.sh to take into account the new pom.xml with surefire,see title,nkeywal,nkeywal,Major,Closed,Fixed,19/Nov/11 18:26,12/Oct/12 05:34
Bug,HBASE-4828,12532066,"Fix failing TestShell, needs same addendum as HBASE-4815 got","Do this:

{code}
Index: src/test/java/org/apache/hadoop/hbase/client/TestShell.java
===================================================================
--- src/test/java/org/apache/hadoop/hbase/client/TestShell.java (revision 1204082)
+++ src/test/java/org/apache/hadoop/hbase/client/TestShell.java (working copy)
@@ -45,6 +45,7 @@
   @BeforeClass
   public static void setUpBeforeClass() throws Exception {
     // Start mini cluster
+    TEST_UTIL.getConfiguration().setBoolean(""hbase.online.schema.update.enable"", true);
     TEST_UTIL.getConfiguration().setInt(""hbase.regionserver.msginterval"", 100);
     TEST_UTIL.getConfiguration().setInt(""hbase.client.pause"", 250);
     TEST_UTIL.getConfiguration().setInt(""hbase.client.retries.number"", 6);
{code}",,stack,Major,Closed,Fixed,19/Nov/11 21:47,12/Jun/22 19:52
Bug,HBASE-4829,12532067,Fix javadoc warnings in 0.92 branch,,stack,stack,Major,Closed,Fixed,19/Nov/11 22:10,20/Nov/15 11:54
Bug,HBASE-4830,12532069,Regionserver BLOCKED on WAITING DFSClient$DFSOutputStream.waitForAckedSeqno running 0.20.205.0+,"Running 0.20.205.1 (I was not at tip of the branch) I ran into the following hung regionserver:

{code}
""regionserver7003.logRoller"" daemon prio=10 tid=0x00007fd98028f800 nid=0x61af in Object.wait() [0x00007fd987bfa000]
   java.lang.Thread.State: WAITING (on object monitor)
        at java.lang.Object.wait(Native Method)
        at java.lang.Object.wait(Object.java:485)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.waitForAckedSeqno(DFSClient.java:3606)
        - locked <0x00000000f8656788> (a java.util.LinkedList)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.flushInternal(DFSClient.java:3595)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.closeInternal(DFSClient.java:3687)
        - locked <0x00000000f8656458> (a org.apache.hadoop.hdfs.DFSClient$DFSOutputStream)
        at org.apache.hadoop.hdfs.DFSClient$DFSOutputStream.close(DFSClient.java:3626)
        at org.apache.hadoop.fs.FSDataOutputStream$PositionCache.close(FSDataOutputStream.java:61)
        at org.apache.hadoop.fs.FSDataOutputStream.close(FSDataOutputStream.java:86)
        at org.apache.hadoop.io.SequenceFile$Writer.close(SequenceFile.java:966)
        - locked <0x00000000f8655998> (a org.apache.hadoop.io.SequenceFile$Writer)
        at org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter.close(SequenceFileLogWriter.java:214)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.cleanupCurrentWriter(HLog.java:791)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.rollWriter(HLog.java:578)
        - locked <0x00000000c443deb0> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.LogRoller.run(LogRoller.java:94)
        at java.lang.Thread.run(Thread.java:662)
{code}


Other threads are like this (here's a sample):
{code}

""regionserver7003.logSyncer"" daemon prio=10 tid=0x00007fd98025e000 nid=0x61ae waiting for monitor entry [0x00007fd987cfb000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.syncer(HLog.java:1074)
        - waiting to lock <0x00000000c443deb0> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.sync(HLog.java:1195)
        at org.apache.hadoop.hbase.regionserver.wal.HLog$LogSyncer.run(HLog.java:1057)
        at java.lang.Thread.run(Thread.java:662)

....

""IPC Server handler 0 on 7003"" daemon prio=10 tid=0x00007fd98049b800 nid=0x61b8 waiting for monitor entry [0x00007fd9872f1000]
   java.lang.Thread.State: BLOCKED (on object monitor)
        at org.apache.hadoop.hbase.regionserver.wal.HLog.append(HLog.java:1007)
        - waiting to lock <0x00000000c443deb0> (a java.lang.Object)
        at org.apache.hadoop.hbase.regionserver.HRegion.doMiniBatchPut(HRegion.java:1798)
        at org.apache.hadoop.hbase.regionserver.HRegion.put(HRegion.java:1668)
        at org.apache.hadoop.hbase.regionserver.HRegionServer.multi(HRegionServer.java:2980)
        at sun.reflect.GeneratedMethodAccessor636.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
        at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1325)

{code}

Looks like HDFS-1529?  (Todd?)",stack,stack,Major,Closed,Fixed,19/Nov/11 23:09,09/Dec/15 06:11
Bug,HBASE-4832,12532084,TestRegionServerCoprocessorExceptionWithAbort fails if the region server stops too fast,"The current implementation of HRegionServer#stop is

{noformat}
  public void stop(final String msg) {
    this.stopped = true;
    LOG.info(""STOPPED: "" + msg);
    synchronized (this) {
      // Wakes run() if it is sleeping
      notifyAll(); // FindBugs NN_NAKED_NOTIFY
    }
  }
{noformat}

The notification is sent on the wrong object and does nothing. As a consequence, the region server continues to sleep instead of waking up and stopping immediately. A correct implementation is:

{noformat}
  public void stop(final String msg) {
    this.stopped = true;
    LOG.info(""STOPPED: "" + msg);
    // Wakes run() if it is sleeping
    sleeper.skipSleepCycle();
  }
{noformat}

Then the region server stops immediately. This makes the region server stops 0,5s faster on average, which is quite useful for unit tests.

However, with this fix, TestRegionServerCoprocessorExceptionWithAbort does not work.
It likely because the code does no expect the region server to stop that fast.

The exception is:
{noformat}
testExceptionFromCoprocessorDuringPut(org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithAbort)  Time elapsed: 30.06 sec  <<< ERROR!
java.lang.Exception: test timed out after 30000 milliseconds
	at java.lang.Throwable.fillInStackTrace(Native Method)
	at java.lang.Throwable.<init>(Throwable.java:196)
	at java.lang.Exception.<init>(Exception.java:41)
	at java.lang.InterruptedException.<init>(InterruptedException.java:48)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:1019)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:804)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.relocateRegion(HConnectionManager.java:778)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionLocation(HConnectionManager.java:697)
	at org.apache.hadoop.hbase.client.ServerCallable.connect(ServerCallable.java:75)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.java:1280)
	at org.apache.hadoop.hbase.client.HTable.getRowOrBefore(HTable.java:585)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:154)
	at org.apache.hadoop.hbase.client.MetaScanner.access$000(MetaScanner.java:52)
	at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:130)
	at org.apache.hadoop.hbase.client.MetaScanner$1.connect(MetaScanner.java:127)
	at org.apache.hadoop.hbase.client.HConnectionManager.execute(HConnectionManager.java:357)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:127)
	at org.apache.hadoop.hbase.client.MetaScanner.metaScan(MetaScanner.java:103)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.prefetchRegionCache(HConnectionManager.java:866)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegionInMeta(HConnectionManager.java:920)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.locateRegion(HConnectionManager.java:808)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:1469)
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1354)
	at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:892)
	at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:750)
	at org.apache.hadoop.hbase.client.HTable.put(HTable.java:725)
	at org.apache.hadoop.hbase.coprocessor.TestRegionServerCoprocessorExceptionWithAbort.testExceptionFromCoprocessorDuringPut(TestRegionServerCoprocessorExceptionWithAbort.java:84)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.FailOnTimeout$StatementThread.run(FailOnTimeout.java:62)
{noformat}

We have this exception because we entered a loop of retries.


",ekoontz,nkeywal,Minor,Closed,Fixed,20/Nov/11 08:30,02/May/13 02:29
Bug,HBASE-4842,12532230,[hbck] Fix intermittent failures on TestHBaseFsck.testHBaseFsck,"Its seems that on the 0.92 branch in particular, TestHBaseFsck.testHBaseFsck is intermittently failing.

In the test, a region's assignment is purposely changed in META but not in ZK.  After the equivalent of 'hbck -fix', a subsequent check that should be clean comes up with a new ZK assignment but with META still being inconsistent with ZK.  The RS in ZK sometimes this points to the same RS, but sometimes it ""moves"" to another ZK. ",jmhsieh,jmhsieh,Major,Closed,Fixed,21/Nov/11 22:24,20/Nov/15 11:53
Bug,HBASE-4848,12532362,TestScanner failing because hostname can't be null,,stack,stack,Major,Closed,Fixed,22/Nov/11 17:00,20/Nov/15 11:55
Bug,HBASE-4849,12532368,TestCatalogTracker can fail if an existing zookeeper running,This fact sunk my attempt at building an RC.  Fix.,stack,stack,Major,Closed,Fixed,22/Nov/11 17:29,20/Nov/15 11:54
Bug,HBASE-4853,12532412,HBASE-4789 does overzealous pruning of seqids,"Working w/ J-D on failing replication test turned up hole in seqids made by the patch over in hbase-4789.  With this patch in place we see lots of instances of the suspicious: 'Last sequenceid written is empty. Deleting all old hlogs'

At a minimum, these lines need removing:

{code}
diff --git a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
index 623edbe..a0bbe01 100644
--- a/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
+++ b/src/main/java/org/apache/hadoop/hbase/regionserver/wal/HLog.java
@@ -1359,11 +1359,6 @@ public class HLog implements Syncable {
       // Cleaning up of lastSeqWritten is in the finally clause because we
       // don't want to confuse getOldestOutstandingSeqNum()
       this.lastSeqWritten.remove(getSnapshotName(encodedRegionName));
-      Long l = this.lastSeqWritten.remove(encodedRegionName);
-      if (l != null) {
-        LOG.warn(""Why is there a raw encodedRegionName in lastSeqWritten? name="" +
-          Bytes.toString(encodedRegionName) + "", seqid="" + l);
-       }
       this.cacheFlushLock.unlock();
     }
   }
{code}

... but above is no good w/o figuring why WALs are not being rotated off.",stack,stack,Critical,Closed,Fixed,22/Nov/11 23:53,12/Oct/12 05:34
Bug,HBASE-4854,12532414,it seems that CLASSPATH elements coming from Hadoop change HBase behaviour,"It looks like HBASE-3465 introduced a slight change in behavior. The ordering of classpath elements makes Hadoop ones go before the HBase ones, which leads to log4j properties picked up from the wrong place, etc. It seems that the easies way to fix that would be to revert the ordering of classpath.",rvs,rvs,Major,Closed,Fixed,23/Nov/11 00:05,20/Nov/15 11:52
Bug,HBASE-4855,12532480,SplitLogManager hangs on cluster restart due to batch.installed doubly counted,"Start a master and RS
RS goes down (kill -9)
Wait for ServerShutDownHandler to create the splitlog nodes. As no RS is there it cannot be processed.
Restart both master and bring up an RS.
The master hangs in SplitLogManager.waitforTasks().

I feel that batch.done is not getting incremented properly.  Not yet digged in fully.

This may be the reason for occasional failure of TestDistributedLogSplitting.testWorkerAbort(). 
",ram_krish,ram_krish,Major,Closed,Fixed,23/Nov/11 15:30,20/Nov/15 11:55
Bug,HBASE-4857,12532501,Recursive loop on KeeperException in AuthenticationTokenSecretManager/ZKLeaderManager,"Looking through stack traces for {{TestMasterFailover}}, I see a case where the leader {{AuthenticationTokenSecretManager}} can get into a recursive loop when a {{KeeperException}} is encountered:
{noformat}
Thread-1-EventThread"" daemon prio=10 tid=0x00007f9fb47b2800 nid=0x77f6 waiting on condition [0x00007f9fab376000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
        at java.lang.Thread.sleep(Native Method)
        at java.lang.Thread.sleep(Thread.java:302)
        at java.util.concurrent.TimeUnit.sleep(TimeUnit.java:328)
        at org.apache.hadoop.hbase.util.RetryCounter.sleepUntilNextRetry(RetryCounter.java:55)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.exists(RecoverableZooKeeper.java:206)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.createAndFailSilent(ZKUtil.java:891)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.createBaseZNodes(ZooKeeperWatcher.java:161)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.<init>(ZooKeeperWatcher.java:154)
        at org.apache.hadoop.hbase.master.HMaster.tryRecoveringExpiredZKSession(HMaster.java:1397)
        at org.apache.hadoop.hbase.master.HMaster.abortNow(HMaster.java:1435)
        at org.apache.hadoop.hbase.master.HMaster.abort(HMaster.java:1374)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.abort(ZooKeeperWatcher.java:450)
        at org.apache.hadoop.hbase.zookeeper.ZKLeaderManager.stepDownAsLeader(ZKLeaderManager.java:166)
        at org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector.stop(AuthenticationTokenSecretManager.java:293)
        at org.apache.hadoop.hbase.zookeeper.ZKLeaderManager.stepDownAsLeader(ZKLeaderManager.java:167)
        at org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector.stop(AuthenticationTokenSecretManager.java:293)
        at org.apache.hadoop.hbase.zookeeper.ZKLeaderManager.stepDownAsLeader(ZKLeaderManager.java:167)
        at org.apache.hadoop.hbase.security.token.AuthenticationTokenSecretManager$LeaderElector.stop(AuthenticationTokenSecretManager.java:293)
        at org.apache.hadoop.hbase.zookeeper.ZKLeaderManager.handleLeaderChange(ZKLeaderManager.java:96)
        at org.apache.hadoop.hbase.zookeeper.ZKLeaderManager.nodeDeleted(ZKLeaderManager.java:78)
        at org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.process(ZooKeeperWatcher.java:286)
        at org.apache.zookeeper.ClientCnxn$EventThread.processEvent(ClientCnxn.java:521)
        at org.apache.zookeeper.ClientCnxn$EventThread.run(ClientCnxn.java:497)
{noformat}

The {{KeeperException}} causes {{ZKLeaderManager}} to call {{AuthenticationTokenSecretManager$LeaderElector.stop()}}, which calls {{ZKLeaderManager.stepDownAsLeader()}}, which will encounter another {{KeeperException}}, and so on...",ghelmling,ghelmling,Critical,Closed,Fixed,23/Nov/11 17:50,20/Nov/15 11:55
Bug,HBASE-4859,12532547,Correctly PreWarm HBCK ThreadPool,"See description at HBASE-3553.  We had a patch ready for this in HBASE-3620 but never applied it publicly.  Testing showed massive speedup in HBCK, especially when RegionServers were down or had long response times.",nspiegelberg,nspiegelberg,Major,Closed,Fixed,23/Nov/11 23:37,20/Nov/15 11:52
Bug,HBASE-4860,12532559,RegionSplitter Should Allow NSFRE during logical split verification,"When verifying that a split request has been logically completed, the client can throw a NoServerForRegionException if the parent & daughter regions are in flux at this time.  We should catch that error and retry later when the daughter regions should have been brought online.",nspiegelberg,nspiegelberg,Minor,Closed,Fixed,24/Nov/11 01:18,20/Nov/15 11:53
Bug,HBASE-4861,12532579,Fix some misspells and extraneous characters in logs; set some to TRACE,Some small clean up in logs.,stack,stack,Major,Closed,Fixed,24/Nov/11 06:12,20/Nov/15 11:52
Bug,HBASE-4862,12532582,Splitting hlog and opening region concurrently may cause data loss,"Case Description:
1.Split hlog thread creat writer for the file region A/recoverd.edits/123456 and is appending log entry
2.Regionserver is opening region A now, and in the process replayRecoveredEditsIfAny() ,it will delete the file region A/recoverd.edits/123456 
3.Split hlog thread catches the io exception, and stop parse this log file 
and if skipError = true , add it to the corrupt logs....However, data in other regions in this log file will loss 
4.Or if skipError = false, it will check filesystem.Of course, the file system is ok , and it only prints a error log, continue assigning regions. Therefore, data in other log files will also loss!!

The case may happen in the following:
1.Move region from server A to server B
2.kill server A and Server B
3.restart server A and Server B

We could prevent this exception throuth forbiding deleting  recover.edits file 
which is appending by split hlog thread",zjushch,zjushch,Critical,Closed,Fixed,24/Nov/11 06:46,20/Nov/15 11:53
Bug,HBASE-4874,12532775,"Run tests with non-secure random, some tests hang otherwise",TestHCM#testClosing fails on Linux if not enough entropy is available in /dev/random,larsh,yuzhihong@gmail.com,Major,Closed,Fixed,26/Nov/11 05:14,12/Oct/12 05:34
Bug,HBASE-4877,12532805,TestHCM failing sporadically on jenkins and always for me on an ubuntu machine,"TestHCM takes 13 minutes for me on ubuntu and fails in testClosing.  It runs fine on a mac.  The problem test is not testClosing as I thought originally, its the test just previous, testConnectionUniqueness.  testConnectionUniqueness creates the maximum cached HConnections + 10 to verify each is unique if the passed in Configuration has a unique hash.  Problem comes when zk enforces its default max from single host of 30 connections which is < (max cached + 10).  The max does not seem to be enforced on mac for me.  The max connections runs up to max of 31 -- zk max + 1 -- and works fine until we do the +10.  On ubuntu, when we hit the zk max of 30, we'll then go into a fail mode where we cannot set up a zk session... each attempt takes a while.  Test passes, it just takes a while.

Only, the uniqueness test does not clean up after itself and so all sessions to zk are outstanding so then when the subsequent testClosing runs, it can't set up connections successfully so fails.",stack,stack,Major,Closed,Fixed,26/Nov/11 21:02,20/Nov/15 11:53
Bug,HBASE-4878,12532815,Master crash when splitting hlog may cause data loss,"Let's see the code of HlogSplitter#splitLog(final FileStatus[] logfiles)
{code}
private List<Path> splitLog(final FileStatus[] logfiles) throws IOException {
 try {
  for (FileStatus log : logfiles) {
  parseHLog(in, logPath, entryBuffers, fs, conf, skipErrors);
 }
 archiveLogs(srcDir, corruptedLogs, processedLogs, oldLogDir, fs, conf);
 } finally {
      status.setStatus(""Finishing writing output logs and closing down."");
      splits = outputSink.finishWritingAndClose();
    }
}
{code}


If master is killed, after finishing archiveLogs(srcDir, corruptedLogs, processedLogs, oldLogDir, fs, conf), 
but before finishing splits = outputSink.finishWritingAndClose();
Log date would loss!
",zjushch,zjushch,Major,Closed,Fixed,27/Nov/11 06:54,12/Oct/12 05:34
Bug,HBASE-4880,12532862,"Region is on service before openRegionHandler completes, may cause data loss","OpenRegionHandler in regionserver is processed as the following steps:
{code}
1.openregion()(Through it, closed = false, closing = false)
2.addToOnlineRegions(region)
3.update .meta. table 
4.update ZK's node state to RS_ZK_REGION_OPEND
{code}
We can find that region is on service before Step 4.
It means client could put data to this region after step 3.
What will happen if step 4 is failed processing?
It will execute OpenRegionHandler#cleanupFailedOpen which will do closing region, and master assign this region to another regionserver.
If closing region is failed, the data which is put between step 3 and step 4 may loss, because the region has been opend on another regionserver and be put new data. Therefore, it may not be recoverd through replayRecoveredEdit() because the edit's LogSeqId is smaller than current region SeqId.",zjushch,zjushch,Major,Closed,Fixed,28/Nov/11 05:17,20/Nov/15 11:52
Bug,HBASE-4881,12532865,Unhealthy region is on service caused by rollback of region splitting,"If region splitting is failed in the state of JournalEntry.CLOSED_PARENT_REGION
It will be rollback as the following steps:
{code}
1.case CLOSED_PARENT_REGION:
  this.parent.initialize();
        break;
2.case CREATE_SPLIT_DIR:
    	this.parent.writestate.writesEnabled = true;
        cleanupSplitDir(fs, this.splitdir);
        break;
3.case SET_SPLITTING_IN_ZK:
        if (server != null && server.getZooKeeper() != null) {
          cleanZK(server, this.parent.getRegionInfo());
        }
        break;
{code}
If this.parent.initialize() throws IOException in step 1,
If check filesystem is ok. it will do nothing.
However, the parent region is on service now.",zjushch,zjushch,Major,Closed,Fixed,28/Nov/11 06:28,20/Nov/15 11:55
Bug,HBASE-4883,12532951,TestCatalogTracker failing for me on ubuntu,"{code}
-------------------------------------------------------------------------------
Test set: org.apache.hadoop.hbase.catalog.TestCatalogTracker
-------------------------------------------------------------------------------
Tests run: 9, Failures: 0, Errors: 1, Skipped: 0, Time elapsed: 5.276 sec <<< FAILURE!
testNoTimeoutWaitForMeta(org.apache.hadoop.hbase.catalog.TestCatalogTracker)  Time elapsed: 1.051 sec  <<< ERROR!
org.mockito.exceptions.misusing.WrongTypeOfReturnValue:
Result cannot be returned by getConfiguration()
getConfiguration() should return Configuration
        at org.apache.hadoop.hbase.catalog.TestCatalogTracker.testNoTimeoutWaitForMeta(TestCatalogTracker.java:378)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:45)
        at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
        at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:42)
        at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
        at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
        at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:30)
        at org.junit.runners.ParentRunner.runLeaf(ParentRunner.java:263)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:68)
        at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:47)
{code}

The above is strange since it seems to pass on jenkins and on macosx.",,stack,Major,Closed,Fixed,28/Nov/11 19:05,12/Jun/22 19:55
Bug,HBASE-4885,12533003,Building against Hadoop 0.23 uses out-of-date MapReduce artifacts,"The ""hadoop-mapred"" artifacts have been replaced by ""hadoop-mapreduce-*"" artifacts in 0.23 onwards.",tomwhite,tomwhite,Major,Closed,Fixed,28/Nov/11 21:47,23/Sep/13 19:08
Bug,HBASE-4886,12533008,truncate fails in HBase shell,"Seeing this in trunk:

{noformat}
hbase(main):001:0> truncate 'table'
Truncating 'table' table (it may take a while):

ERROR: wrong number of arguments (1 for 3)

Here is some help for this command:
  Disables, drops and recreates the specified table.
{noformat}

... caused by the removal of the HTable(String) constructor.",larsh,larsh,Minor,Closed,Fixed,28/Nov/11 21:59,12/Oct/12 05:34
Bug,HBASE-4889,12533136,HRegionInfo.isMetaTable() should be true for -ROOT- regions,"According to its javadoc, HRegionInfo.isMetaTable() should return true if the region is either a .META. or -ROOT- region, but only does so for .META. regions.

The change in behavior happened in HBASE-451",,dferro,Major,Closed,Fixed,29/Nov/11 16:35,20/Nov/15 11:53
Bug,HBASE-4890,12533145,fix possible NPE in HConnectionManager,"I was running YCSB against a 0.92 branch and encountered this error message:

{code}
11/11/29 08:47:16 WARN client.HConnectionManager$HConnectionImplementation: Failed all from region=usertable,user3917479014967760871,1322555655231.f78d161e5724495a9723bcd972f97f41., hostname=c0316.hal.cloudera.com, port=57020
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.lang.NullPointerException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatchCallback(HConnectionManager.java:1501)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.processBatch(HConnectionManager.java:1353)
        at org.apache.hadoop.hbase.client.HTable.flushCommits(HTable.java:898)
        at org.apache.hadoop.hbase.client.HTable.doPut(HTable.java:775)
        at org.apache.hadoop.hbase.client.HTable.put(HTable.java:750)
        at com.yahoo.ycsb.db.HBaseClient.update(Unknown Source)
        at com.yahoo.ycsb.DBWrapper.update(Unknown Source)
        at com.yahoo.ycsb.workloads.CoreWorkload.doTransactionUpdate(Unknown Source)
        at com.yahoo.ycsb.workloads.CoreWorkload.doTransaction(Unknown Source)
        at com.yahoo.ycsb.ClientThread.run(Unknown Source)
Caused by: java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithoutRetries(HConnectionManager.java:1315)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3.call(HConnectionManager.java:1327)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3.call(HConnectionManager.java:1325)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
        at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:158)
        at $Proxy4.multi(Unknown Source)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1330)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation$3$1.call(HConnectionManager.java:1328)
        at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithoutRetries(HConnectionManager.java:1309)
        ... 7 more
{code}

It looks like the NPE is caused by server being null in the MultiRespone call() method.

{code}
     public MultiResponse call() throws IOException {
         return getRegionServerWithoutRetries(
             new ServerCallable<MultiResponse>(connection, tableName, null) {
               public MultiResponse call() throws IOException {
                 return server.multi(multi);
               }
               @Override
               public void connect(boolean reload) throws IOException {
                 server =
                   connection.getHRegionConnection(loc.getHostname(), loc.getPort());
               }
             }
         );
{code}",stack,jmhsieh,Blocker,Closed,Fixed,29/Nov/11 17:04,12/Oct/12 05:34
Bug,HBASE-4893,12533181,HConnectionImplementation is closed but not deleted,"In abort() of HConnectionManager$HConnectionImplementation, instance of HConnectionImplementation is marked as this.closed=true.

There is no way for client application to check the hbase client connection whether it is still opened/good (this.closed=false) or not. We need a method to validate the state of a connection like isClosed().

{code}
public boolean isClosed(){
   return this.closed;
} 
{code}

Once the connection is closed and it should get deleted. Client application still gets a connection from HConnectionManager.getConnection(Configuration) and tries to make a RPC call to RS, since connection is already closed, HConnectionImplementation.getRegionServerWithRetries throws RetriesExhaustedException with error message

{code}
Caused by: org.apache.hadoop.hbase.client.RetriesExhaustedException: Trying to contact region server null for region , row 'xxxxxxxx-xxxx-xxxx-xxxx-xxxxxxx', but failed after 10 attempts.
Exceptions:
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
java.io.IOException: org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation@7eab48a7 closed
	at org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.getRegionServerWithRetries(HConnectionManager.java:1008)
	at org.apache.hadoop.hbase.client.HTable.get(HTable.java:546)
{code}",mubarakseyed,mubarakseyed,Major,Closed,Fixed,29/Nov/11 19:45,20/Nov/15 11:52
Bug,HBASE-4894,12533193,Remove sbin from tgz made building 0.92.,"When I undo the tgz, I see we have an sbin dir with update-hbase-env.sh in it.  Lets do this messing in 0.94.  Its incomplete story in 0.92.  Removing sbin dir from tgz.",stack,stack,Major,Closed,Fixed,29/Nov/11 21:07,20/Nov/15 11:55
Bug,HBASE-4899,12533245,Region would be assigned twice easily with continually  killing server and moving region in testing environment,"Before assigning region in ServerShutdownHandler#process, it will check whether region is in RIT,
however, this checking doesn't work as the excepted in the following case:
1.move region A from server B to server C
2.kill server B
3.start server B immediately

Let's see what happen in the code for the above case
{code}
for step1:
1.1 server B close the region A,
1.2 master setOffline for region A,(AssignmentManager#setOffline:this.regions.remove(regionInfo))
1.3 server C start to open region A.(Not completed)
for step3:
master ServerShutdownHandler#process() for server B
{
..
splitlog()
...
List<RegionState> regionsInTransition =
        this.services.getAssignmentManager()
        .processServerShutdown(this.serverName);
...
Skip regions that were in transition unless CLOSING or PENDING_CLOSE
...
assign region
}
{code}
In fact, when running ServerShutdownHandler#process()#this.services.getAssignmentManager().processServerShutdown(this.serverName), region A is in RIT (step1.3 not completed), but the return List<RegionState> regionsInTransition doesn't contain it, because region A has removed from AssignmentManager.regions by AssignmentManager#setOffline in step 1.2
Therefore, region A will be assigned twice.

Actually, one server killed and started twice will also easily cause region assigned twice.
Exclude the above reason, another probability : 
when execute ServerShutdownHandler#process()#MetaReader.getServerUserRegions ,region is included which is in RIT now.
But after completing MetaReader.getServerUserRegions, the region has been opened in other server and is not in RIT now.

In our testing environment where balancing,moving and killing are executed periodly, assigning region twice often happens, and it is hateful because it will affect other test cases.",zjushch,zjushch,Critical,Closed,Fixed,30/Nov/11 05:58,20/Nov/15 11:53
Bug,HBASE-4900,12533247,Fix javadoc warnings out on 0.90 branch,,stack,stack,Major,Closed,Fixed,30/Nov/11 06:50,20/Nov/15 11:53
Bug,HBASE-4904,12533311,Fix overcommit to 0.90 (hbase-4352+hbase-4900),Last night I made two commits.  The first was an overcommit (my tree was dirty).  This issue is about fixup.,stack,stack,Major,Closed,Fixed,30/Nov/11 16:26,20/Nov/15 11:54
Bug,HBASE-4918,12533376,HTablePool Constructor may cause unintended behavior,"{code:title=HTablePool.java}
96   public HTablePool(final Configuration config, final int maxSize,
97       final HTableInterfaceFactory tableFactory) {
98     this(config, maxSize, null, PoolType.Reusable);
99   }
{code} 
I think that 3rd argument in line 98 should be ""tableFactory"".",,kitora_naoki,Major,Closed,Fixed,30/Nov/11 23:48,20/Nov/15 11:55
Bug,HBASE-4922,12533397,[packaging] Assembly tars up hbase in a subdir; i.e. after untar hbase-0.92.0 has a subdir named 0.92.0,Reported by Roman.,rvs,stack,Blocker,Closed,Fixed,01/Dec/11 02:39,20/Nov/15 11:52
Bug,HBASE-4923,12533398,[packaging] Assembly should make only executables executable (docs should not be executable!),Reported by Roman.,,stack,Major,Closed,Fixed,01/Dec/11 02:40,12/Jun/22 19:49
Bug,HBASE-4927,12533501,"CatalogJanior:SplitParentFirstComparator doesn't sort as expected, for the last region when the endkey is empty","When reviewing HBASE-4238 backporting, Jon found this issue.

What happens if the split points are  (empty end key is the last key, empty start key is the first key)

Parent     [A,)
L daughter [A,B), 
R daughter [B,)

When sorted, we gets to end key comparision which results in this incorrector order:
[A,B), [A,), [B,) 

we wanted:
[A,), [A,B), [B,)",jxiang,jxiang,Minor,Closed,Fixed,01/Dec/11 20:10,20/Nov/15 11:53
Bug,HBASE-4931,12533537,CopyTable instructions could be improved.,"The book and the usage instructions could be improved to include more details, things caveats and to better explain usage.

One example in particular, could be updated to refer to ReplicationRegionInterface and ReplicationRegionServer in thier current locations (o.a.h.h.client.replication and o.a.h.h.replication.regionserver), and better explain why one would use particular arguments.

{code}
$ bin/hbase org.apache.hadoop.hbase.mapreduce.CopyTable
--rs.class=org.apache.hadoop.hbase.ipc.ReplicationRegionInterface
--rs.impl=org.apache.hadoop.hbase.regionserver.replication.ReplicationRegionServer
--starttime=1265875194289 --endtime=1265878794289
--peer.adr=server1,server2,server3:2181:/hbase TestTable
{code}",misty,jmhsieh,Major,Closed,Fixed,01/Dec/11 23:36,06/Apr/18 17:51
Bug,HBASE-4932,12533572,Block cache can be mistakenly instantiated by tools,Map Reduce tasks that create a writer to write HFiles inadvertently end up creating block cache.,khemani,khemani,Major,Closed,Fixed,02/Dec/11 06:35,26/Feb/13 08:12
Bug,HBASE-4935,12533634,"hbase 0.92.0 doesn't work going against 0.20.205.0, its packaged hadoop","See this Mikhail thread up on the list: http://search-hadoop.com/m/WMUZR24EAJ1/%2522SequenceFileLogReader+uses+a+reflection+hack+resulting+in+runtime+failures%2522&subj=Re+SequenceFileLogReader+uses+a+reflection+hack+resulting+in+runtime+failures

Dig into it.",stack,stack,Major,Closed,Fixed,02/Dec/11 14:52,20/Nov/15 11:53
Bug,HBASE-4936,12533636,Cached HRegionInterface connections crash when getting UnknownHost exceptions,"This isssue is unlikely to come up in a cluster test case. However, for development, the following thing happens: 

1. Start the HBase cluster locally, on network A (DNS A, etc)
2. The region locations are cached using the hostname (mycomputer.company.com, 211.x.y.z - real ip)
3. Change network location (go home)
4. Start the HBase cluster locally. My hostname / ips are not different (mycomputer, 192.168.0.130 - new ip)

If the region locations have been cached using the hostname, there is an UnknownHostException in CatalogTracker.getCachedConnection(ServerName sn), uncaught in the catch statements. The server will crash constantly. 

The error should be caught and not rethrown, so that the cached connection expires normally. ",adragomir,adragomir,Major,Closed,Fixed,02/Dec/11 14:56,12/Oct/12 05:35
Bug,HBASE-4937,12533647,Error in Quick Start Shell Exercises,"The shell exercises in the Quick Start (http://hbase.apache.org/book/quickstart.html) starts

{code}
hbase(main):003:0> create 'test', 'cf'
0 row(s) in 1.2200 seconds
hbase(main):003:0> list 'table'
test
1 row(s) in 0.0550 seconds
{code}

It looks like the second command is wrong. Running it, the actual output is

{code}
hbase(main):001:0> create 'test', 'cf'
0 row(s) in 0.3630 seconds

hbase(main):002:0> list 'table'
TABLE                                                                                                                                                                                                                 
0 row(s) in 0.0100 seconds
{code}

The argument to list should be 'test', not 'table', and the output in the example is missing the {{TABLE}} line.",stack,rberdeen,Major,Closed,Fixed,02/Dec/11 16:05,12/Oct/12 05:34
Bug,HBASE-4942,12533751,HMaster is unable to start of HFile V1 is used,"This was reported by HH Zhu (zhh200910@gmail.com)
If the following is specified in hbase-site.xml：
{code}
    <property>
        <name>hfile.format.version</name>
        <value>1</value>
    </property>
{code}
Clear the hdfs directory ""hbase.rootdir"" so that MasterFileSystem.bootstrap() is executed.
You would see:
{code}
java.lang.NullPointerException
    at org.apache.hadoop.hbase.io.hfile.HFileReaderV1.close(HFileReaderV1.java:358)
    at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.close(StoreFile.java:1083)
    at org.apache.hadoop.hbase.regionserver.StoreFile.closeReader(StoreFile.java:570)
    at org.apache.hadoop.hbase.regionserver.Store.close(Store.java:441)
    at org.apache.hadoop.hbase.regionserver.HRegion.doClose(HRegion.java:782)
    at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:717)
    at org.apache.hadoop.hbase.regionserver.HRegion.close(HRegion.java:688)
    at org.apache.hadoop.hbase.master.MasterFileSystem.bootstrap(MasterFileSystem.java:390)
    at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:356)
    at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:128)
    at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:113)
    at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:435)
    at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:314)
    at java.lang.Thread.run(Thread.java:619)
{code}
The above exception would lead to:
{code}
java.lang.RuntimeException: HMaster Aborted
    at org.apache.hadoop.hbase.master.HMasterCommandLine.startMaster(HMasterCommandLine.java:152)
    at org.apache.hadoop.hbase.master.HMasterCommandLine.run(HMasterCommandLine.java:103)
    at org.apache.hadoop.util.ToolRunner.run(ToolRunner.java:65)
    at org.apache.hadoop.hbase.util.ServerCommandLine.doMain(ServerCommandLine.java:76)
    at org.apache.hadoop.hbase.master.HMaster.main(HMaster.java:1512)
{code}

In org.apache.hadoop.hbase.master.HMaster.HMaster(Configuration conf), we have:
{code}
this.conf.setFloat(CacheConfig.HFILE_BLOCK_CACHE_SIZE_KEY, 0.0f);
{code}
When CacheConfig is instantiated, the following is called:
{code}
org.apache.hadoop.hbase.io.hfile.CacheConfig.instantiateBlockCache(Configuration conf)
{code}
Since ""hfile.block.cache.size"" is 0.0, instantiateBlockCache() would return null, resulting in blockCache field of CacheConfig to be null.
When master closes Root region, org.apache.hadoop.hbase.io.hfile.HFileReaderV1.close(boolean evictOnClose) would be called. cacheConf.getBlockCache() returns null, leading to master abort.

The following should be called in HFileReaderV1.close(), similar to the code in HFileReaderV2.close():
{code}
if (evictOnClose && cacheConf.isBlockCacheEnabled())
{code}",zhh,yuzhihong@gmail.com,Major,Closed,Fixed,03/Dec/11 21:19,12/Oct/12 05:34
Bug,HBASE-4945,12533762,NPE in HRegion.bulkLoadHFiles(...),"Was playing with ""completebulkload"", and ran into an NPE.
The problem is here (HRegion.bulkLoadHFiles(...)).

{code}
Store store = getStore(familyName);
if (store == null) {
  IOException ioe = new DoNotRetryIOException(
      ""No such column family "" + Bytes.toStringBinary(familyName));
  ioes.add(ioe);
  failures.add(p);
}

try {
  store.assertBulkLoadHFileOk(new Path(path));
} catch (WrongRegionException wre) {
  // recoverable (file doesn't fit in region)
  failures.add(p);
} catch (IOException ioe) {
  // unrecoverable (hdfs problem)
  ioes.add(ioe);
}
{code}

This should be 
{code}
Store store = getStore(familyName);
if (store == null) {
...
} else {
  try {
    store.assertBulkLoadHFileOk(new Path(path));
...
}
{code}
",larsh,larsh,Minor,Closed,Fixed,04/Dec/11 03:52,20/Nov/15 11:53
Bug,HBASE-4946,12533767,"HTable.coprocessorExec (and possibly coprocessorProxy) does not work with dynamically loaded coprocessors (from hdfs or local system), because the RPC system tries to deserialize an unknown class. ","Loading coprocessors jars from hdfs works fine. I load it from the shell, after setting the attribute, and it gets loaded:

{noformat}
INFO org.apache.hadoop.hbase.regionserver.HRegion: Setting up tabledescriptor config now ...
INFO org.apache.hadoop.hbase.coprocessor.CoprocessorHost: Class com.MyCoprocessorClass needs to be loaded from a file - hdfs://localhost:9000/coproc/rt-      >0.0.1-SNAPSHOT.jar.
INFO org.apache.hadoop.hbase.coprocessor.CoprocessorHost: loadInstance: com.MyCoprocessorClass
INFO org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost: RegionEnvironment createEnvironment
DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Registered protocol handler: region=t1,,1322572939753.6409aee1726d31f5e5671a59fe6e384f. protocol=com.MyCoprocessorClassProtocol
INFO org.apache.hadoop.hbase.regionserver.RegionCoprocessorHost: Load coprocessor com.MyCoprocessorClass from HTD of t1 successfully.
{noformat}

The problem is that this coprocessors simply extends BaseEndpointCoprocessor, with a dynamic method. When calling this method from the client with HTable.coprocessorExec, I get errors on the HRegionServer, because the call cannot be deserialized from writables. 

The problem is that Exec tries to do an ""early"" resolve of the coprocessor class. The coprocessor class is loaded, but it is in the context of the HRegionServer / HRegion. So, the call fails:

{noformat}
2011-12-02 00:34:17,348 ERROR org.apache.hadoop.hbase.io.HbaseObjectWritable: Error in readFields
java.io.IOException: Protocol class com.MyCoprocessorClassProtocol not found
  at org.apache.hadoop.hbase.client.coprocessor.Exec.readFields(Exec.java:125)
  at org.apache.hadoop.hbase.io.HbaseObjectWritable.readObject(HbaseObjectWritable.java:575)
  at org.apache.hadoop.hbase.ipc.Invocation.readFields(Invocation.java:105)
  at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.processData(HBaseServer.java:1237)
  at org.apache.hadoop.hbase.ipc.HBaseServer$Connection.readAndProcess(HBaseServer.java:1167)
  at org.apache.hadoop.hbase.ipc.HBaseServer$Listener.doRead(HBaseServer.java:703)
  at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.doRunLoop(HBaseServer.java:495)
  at org.apache.hadoop.hbase.ipc.HBaseServer$Listener$Reader.run(HBaseServer.java:470)
  at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
  at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
  at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.ClassNotFoundException: com.MyCoprocessorClassProtocol
  at java.net.URLClassLoader$1.run(URLClassLoader.java:202)
  at java.security.AccessController.doPrivileged(Native Method)
  at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:306)
  at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
  at java.lang.ClassLoader.loadClass(ClassLoader.java:247)
  at java.lang.Class.forName0(Native Method)
  at java.lang.Class.forName(Class.java:247)
  at org.apache.hadoop.conf.Configuration.getClassByName(Configuration.java:943)
  at org.apache.hadoop.hbase.client.coprocessor.Exec.readFields(Exec.java:122)
  ... 10 more
{noformat}

Probably the correct way to fix this is to make Exec really smart, so that it knows all the class definitions loaded in CoprocessorHost(s).

I created a small patch that simply doesn't resolve the class definition in the Exec, instead passing it as string down to the HRegion layer. This layer knows all the definitions, and simply loads it by name. 

",adragomir,adragomir,Major,Closed,Fixed,04/Dec/11 07:22,12/Oct/12 05:35
Bug,HBASE-4950,12533824,"[book] book.xml, corrected BigTable link throughout paper.  added a few more links to additional resources","book.xml
* Corrected the BigTable link in 3 places in the paper.  It was actually producing a 404 from Google Labs!!  I have no idea how long that link has been wrong.
* Minor formatting in additional resource in appendix (moved one presentation to slides from video since that's what it actually was)",dmeil,dmeil,Minor,Closed,Fixed,05/Dec/11 00:12,12/Jun/22 19:56
Bug,HBASE-4969,12534129,tautology in HRegionInfo.readFields,"In HRegionInfo.readFields() the following looks wrong to me

    } else if (getVersion() == VERSION) {

it is always true.

Should it have been

    } else if (getVersion() == version) {

version is a local variable where the deserialized-version is stored.

(I am struggling with another issue where after applying some patches - including ""HBASE-4388 Second start after migration from 90 to trunk crashes"" my version of hbase-92 HRegionInfo.readFields() tries to find HTD in HRegionInfo and fails)


",khemani,khemani,Major,Closed,Fixed,07/Dec/11 01:33,20/Nov/15 11:55
Bug,HBASE-4976,12534236,Add compaction/flush queue size metrics mistakenly removed by HFile v2,"Upping priority, and putting it against 0.92 since J-D fingered it as blocker.  Which metrics in particular are missing?  Hard to patch?",mikhail,mikhail,Blocker,Closed,Fixed,07/Dec/11 19:21,20/Nov/15 11:54
Bug,HBASE-4980,12534298,Null pointer exception in HBaseClient receiveResponse,"Relevant Stack trace: 

2011-11-30 13:10:26,557 [IPC Client (47) connection to xx.xx.xx/172.22.4.68:60020 from an unknown user] WARN  org.apache.hadoop.ipc.HBaseClient - Unexpected exception receiving call responses
java.lang.NullPointerException
>-at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.receiveResponse(HBaseClient.java:583)
>-at org.apache.hadoop.hbase.ipc.HBaseClient$Connection.run(HBaseClient.java:511)


{code}
  if (LOG.isDebugEnabled())
          LOG.debug(getName() + "" got value #"" + id);
        Call call = calls.remove(id);
        // Read the flag byte
        byte flag = in.readByte();
        boolean isError = ResponseFlag.isError(flag);
        if (ResponseFlag.isLength(flag)) {
          // Currently length if present is unused.
          in.readInt();
        }
        int state = in.readInt(); // Read the state.  Currently unused.
        if (isError) {
          //noinspection ThrowableInstanceNeverThrown
          call.setException(new RemoteException( WritableUtils.readString(in),
              WritableUtils.readString(in)));
        } else {
{code}

This line {code}Call call = calls.remove(id);{code}  may return a null 'call'. It is so because if you have rpc timeout enable, we proactively clean up other calls which have expired their lifetime along with the call for which socket timeout exception happend.",shrijeet,shrijeet,Major,Closed,Fixed,08/Dec/11 04:00,20/Nov/15 11:52
Bug,HBASE-4982,12534309,graceful_stop.sh does not pass on the --config its passed to its internal invocations of other hbase scripts,"Means, unless conf is in default location, we mess up asking zk for state (we'll be pointed at wrong ensemble), etc.",stack,stack,Major,Closed,Fixed,08/Dec/11 06:22,20/Nov/15 11:55
Bug,HBASE-4986,12534407,index.html - found another grammatical error,"""modeled after Google'"" should be ""modeled after Google's"" - it was missing the ""s""

",dmeil,dmeil,Trivial,Closed,Fixed,08/Dec/11 21:48,12/Jun/22 19:56
Bug,HBASE-4987,12534423,wrong use of incarnation var in SplitLogManager,@Ramakrishna found and analyzed an issue in SplitLogManager. But I don't think that the fix is correct. Will upload a patch shortly.,khemani,khemani,Major,Closed,Fixed,09/Dec/11 02:14,20/Nov/15 11:55
Bug,HBASE-4992,12534504,Use a UUID for HFileOutputFormat partition path name,"HFileOutputFormat uses timestamps for the path name to avoid collision.
It is possible the path name still collide and causing MR job fails.

Caused by: java.io.IOException: File: hdfs://ubasedev001-snc3-dfs-nn.data.facebook.com:9000/user/hadoop/partitions_1316142976586#_\
partition.lst has changed on HDFS since job started
at org.apache.hadoop.filecache.DistributedCache.ifExistsAndFresh(DistributedCache.java:572)
at org.apache.hadoop.filecache.DistributedCache.checkCacheStatusValidity(DistributedCache.java:465)
at org.apache.hadoop.filecache.DistributedCache.getLocalCache(DistributedCache.java:230)
at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:208)
",,schen,Minor,Closed,Fixed,09/Dec/11 17:09,20/Nov/15 11:53
Bug,HBASE-4993,12534510,Performance regression in minicluster creation,"Side effect of 4610: the mini cluster needs 4,5 seconds to start",nkeywal,nkeywal,Major,Closed,Fixed,09/Dec/11 17:35,12/Oct/12 05:35
Bug,HBASE-4994,12534515,TestHeapSize broke in trunk,"This commit added Map to HRegion

{code}
commit 888d73a9f5fe907f7c616211322fff339eeaa446
Author: Zhihong Yu <tedyu@apache.org>
Date:   Fri Dec 9 06:01:58 2011 +0000

    HBASE-4946  HTable.coprocessorExec (and possibly coprocessorProxy) does not work with
                   dynamically loaded coprocessors (Andrei Dragomir)
{code}",stack,stack,Major,Closed,Fixed,09/Dec/11 17:43,20/Nov/15 11:53
Bug,HBASE-4996,12534525,"hadoopqa not running long category tests, just short and medium",,,stack,Major,Closed,Fixed,09/Dec/11 18:54,12/Jun/22 19:56
Bug,HBASE-4997,12534534,SplitLogManager can have a race on batch.installed,This is a continuation of HBASE-4987. Will put up a patch shortly.,khemani,khemani,Major,Closed,Fixed,09/Dec/11 19:51,20/Nov/15 11:54
Bug,HBASE-5003,12534565,"If the master is started with a wrong root dir, it gets stuck and can't be killed","Reported by a new user on IRC who tried to set hbase.rootdir to file:///~/hbase, the master gets stuck and cannot be killed. I tried something similar on my machine and it spins while logging:

{quote}
2011-12-09 16:11:17,002 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at file:/bin/hbase, retrying: Mkdirs failed to create file:/bin/hbase
2011-12-09 16:11:27,002 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at file:/bin/hbase, retrying: Mkdirs failed to create file:/bin/hbase
2011-12-09 16:11:37,003 WARN org.apache.hadoop.hbase.util.FSUtils: Unable to create version file at file:/bin/hbase, retrying: Mkdirs failed to create file:/bin/hbase
{quote}

The reason it cannot be stopped is that the master's main thread is stuck in there and will never be notified:

{quote}
""Master:0;su-jdcryans-01.local,51116,1323475535684"" prio=5 tid=7f92b7a3c000 nid=0x1137ba000 waiting on condition [1137b9000]
   java.lang.Thread.State: TIMED_WAITING (sleeping)
	at java.lang.Thread.sleep(Native Method)
	at org.apache.hadoop.hbase.util.FSUtils.setVersion(FSUtils.java:297)
	at org.apache.hadoop.hbase.util.FSUtils.setVersion(FSUtils.java:268)
	at org.apache.hadoop.hbase.master.MasterFileSystem.checkRootDir(MasterFileSystem.java:339)
	at org.apache.hadoop.hbase.master.MasterFileSystem.createInitialFileSystemLayout(MasterFileSystem.java:128)
	at org.apache.hadoop.hbase.master.MasterFileSystem.<init>(MasterFileSystem.java:113)
	at org.apache.hadoop.hbase.master.HMaster.finishInitialization(HMaster.java:435)
	at org.apache.hadoop.hbase.master.HMaster.run(HMaster.java:314)
	at org.apache.hadoop.hbase.master.HMasterCommandLine$LocalHMaster.run(HMasterCommandLine.java:218)
	at java.lang.Thread.run(Thread.java:680)
{quote}

It seems we should do a better handling of the exceptions we get in there, and die if we need to. It would make a better user experience.

Maybe also do a check on hbase.rootdir before even starting the master.",smanek,jdcryans,Critical,Closed,Fixed,10/Dec/11 00:15,05/Aug/14 20:11
Bug,HBASE-5007,12534639,HBaseAdmin.stopRegionServer do not stop the region server,"Please running this example:

public class Test {
  public static void main(String[] args) throws Exception {
    HBaseAdmin admin = new HBaseAdmin(HBaseConfiguration.create());
    admin.stopRegionServer(""your.rs.hostname:60020"");
  }
}

then, you can see:

Exception in thread ""main"" java.lang.RuntimeException: The interface org.apache.hadoop.hbase.Stoppable
    at org.apache.hadoop.hbase.ipc.Invocation.<init>(Invocation.java:61)
    at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Invoker.invoke(WritableRpcEngine.java:151)
    at $Proxy2.stop(Unknown Source)
    at org.apache.hadoop.hbase.client.HBaseAdmin.stopRegionServer(HBaseAdmin.java:1492)
    at Test.main(Test.java:7)
Caused by: java.lang.NoSuchFieldException: VERSION
    at java.lang.Class.getField(Class.java:1520)
    at org.apache.hadoop.hbase.ipc.Invocation.<init>(Invocation.java:57)
    ... 4 more





When invoking the ""HBaseAdmin.stopRegionServer"" method，
we obtain a ""proxy"" for org.apache.hadoop.hbase.ipc.HRegionInterface，
(HRegionInterface extends org.apache.hadoop.hbase.Stoppable)
but the ""stop"" method declared in Stoppable.

In the constructor of ""org.apache.hadoop.hbase.ipc.Invocation""，
the ""method"" argument is ""public abstract void org.apache.hadoop.hbase.Stoppable.stop(java.lang.String)""，
so, ""method.getDeclaringClass()"" is ""org.apache.hadoop.hbase.Stoppable""，
but, the ""Stoppable"" interface no ""VERSION"" field.



[fix suggestion]:

Override the ""stop"" method in org.apache.hadoop.hbase.ipc.HRegionInterface as follows:
==================================
@Override
public void stop(String why);


of courese, another attempt is ok.
(e.g. declare ""VERSION"" field in Stoppable interface，
then modify some code fragment of Invocation and org.apache.hadoop.hbase.ipc.WritableRpcEngine.Server)
",,zhh,Major,Closed,Fixed,11/Dec/11 05:54,20/Nov/15 11:54
Bug,HBASE-5008,12534653,The clusters can't  provide services because Region can't flush.,"Hbase version 0.90.4 + patches

My analysis is as follows:

//Started splitting region b24d8ccb852ff742f2a27d01b7f5853e and closed region.

2011-12-10 17:32:48,653 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Starting split of region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.
2011-12-10 17:32:49,759 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: disabling compactions & flushes
2011-12-10 17:32:49,759 INFO org.apache.hadoop.hbase.regionserver.HRegion: Running close preflush of Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.

//Processed a flush request and skipped , But flushRequested had set to true
2011-12-10 17:33:06,963 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e., current region memstore size 12.6m
2011-12-10 17:33:17,277 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Skipping flush on Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e. because closing

//split region b24d8ccb852ff742f2a27d01b7f5853 failed and rolled back, flushRequested flag was true, So all handle was blocked 

2011-12-10 17:34:01,293 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Cleaned up old failed split transaction detritus: hdfs://193.195.18.121:9000/hbase/Htable_UFDR_004/b24d8ccb852ff742f2a27d01b7f5853e/splits
2011-12-10 17:34:01,294 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.; next sequenceid=15494173
2011-12-10 17:34:01,295 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: Successful rollback of failed split of Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.
2011-12-10 17:43:10,147 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 19 on 20020' on region 
Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size


// All handles had been blocked. The clusters could not provide services

2011-12-10 17:34:01,295 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: Successful rollback of failed split of Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.
2011-12-10 17:43:10,147 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 19 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:10,192 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 34 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:10,193 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 51 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:10,196 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 85 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:10,199 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 88 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:10,202 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 44 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:11,663 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 2 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:11,665 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 10 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:11,670 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 75 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:11,671 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 98 on 20020' on region Htable_UFDR_004,09781,1323508582833.b24d8ccb852ff742f2a27d01b7f5853e.: memstore size 384.0m is >= than blocking 384.0m size
2011-12-10 17:43:11,680 INFO org.apache.hadoop.hbase.regionserver.HRegion: Blocking updates for 'IPC Server handler 11 on 20020' on region 



",sunnygao,sunnygao,Blocker,Closed,Fixed,11/Dec/11 12:37,20/Nov/15 11:55
Bug,HBASE-5009,12534770,Failure of creating split dir if it already exists prevents splits from happening further,"The scenario is
-> The split of a region takes a long time
-> The deletion of the splitDir fails due to HDFS problems.
-> Subsequent splits also fail after that.
{code}
private static void createSplitDir(final FileSystem fs, final Path splitdir)
  throws IOException {
    if (fs.exists(splitdir)) throw new IOException(""Splitdir already exits? "" + splitdir);
    if (!fs.mkdirs(splitdir)) throw new IOException(""Failed create of "" + splitdir);
  }
{code}

Correct me if am wrong? If it is an issue can we change the behaviour of throwing exception?
Pls suggest.",ram_krish,ram_krish,Major,Closed,Fixed,12/Dec/11 12:02,20/Nov/15 11:52
Bug,HBASE-5010,12534821,Filter HFiles based on TTL,"In ScanWildcardColumnTracker we have

{code:java}
 
  this.oldestStamp = EnvironmentEdgeManager.currentTimeMillis() - ttl;

  ...

  private boolean isExpired(long timestamp) {
    return timestamp < oldestStamp;
  }
{code}

but this time range filtering does not participate in HFile selection. In one real case this caused next() calls to time out because all KVs in a table got expired, but next() had to iterate over the whole table to find that out. We should be able to filter out those HFiles right away. I think a reasonable approach is to add a ""default timerange filter"" to every scan for a CF with a finite TTL and utilize existing filtering in StoreFile.Reader.passesTimerangeFilter.
",mikhail,mikhail,Major,Closed,Fixed,12/Dec/11 19:09,12/Oct/12 05:34
Bug,HBASE-5015,12534953,Remove some leaks in tests due to lack of HTable.close(),,nkeywal,nkeywal,Minor,Closed,Fixed,13/Dec/11 13:36,12/Oct/12 05:34
Bug,HBASE-5016,12534989,"LoadBalancerFactory - ""maser"" in package name of dynamically loaded balancer class?","
Shouldn't this... ""hbase.maser.loadBalancer.class"" be something like ""hbase.master.loadBalancer.class""?  What's with ""maser""?

{code}

/**
 * The class that creates a load balancer from a conf.
 */
public class LoadBalancerFactory {

  /**
   * Create a loadblanacer from the given conf.
   * @param conf
   * @return A {@link LoadBalancer}
   */
  public static LoadBalancer getLoadBalancer(Configuration conf) {

    // Create the balancer
    Class<? extends LoadBalancer> balancerKlass = conf.getClass(""hbase.maser.loadBalancer.class"",DefaultLoadBalancer.class, LoadBalancer.class);
    return ReflectionUtils.newInstance(balancerKlass, conf);

  }
}
{code}",,dmeil,Major,Closed,Fixed,13/Dec/11 17:47,12/Jun/22 19:53
Bug,HBASE-5020,12535092,MetaReader#fullScan doesn't  stop scanning when vistor returns false in 0.90 version,"In current 0.90 code,
{code}
 public static void fullScan(CatalogTracker catalogTracker,
      final Visitor visitor, final byte [] startrow)
  throws IOException {
    HRegionInterface metaServer =
      catalogTracker.waitForMetaServerConnectionDefault();
    Scan scan = new Scan();
    if (startrow != null) scan.setStartRow(startrow);
    scan.addFamily(HConstants.CATALOG_FAMILY);
    long scannerid = metaServer.openScanner(
        HRegionInfo.FIRST_META_REGIONINFO.getRegionName(), scan);
    try {
      Result data;
      while((data = metaServer.next(scannerid)) != null) {
        if (!data.isEmpty()) visitor.visit(data);
      }
    } finally {
      metaServer.close(scannerid);
    }
    return;
  }
{code}

If visitor.visit(data) return false, the scan will not stop;
However, it is not the same as the description of Visitor
{code}
public interface Visitor {
    /**
     * Visit the catalog table row.
     * @param r A row from catalog table
     * @return True if we are to proceed scanning the table, else false if
     * we are to stop now.
     */
    public boolean visit(final Result r) throws IOException;
  }
{code}


I think it is a miss, and trunk doesn't exist this hole.",zjushch,zjushch,Major,Closed,Fixed,14/Dec/11 03:26,20/Nov/15 11:53
Bug,HBASE-5026,12535178,Add coprocessor hook to HRegionServer.ScannerListener.leaseExpired(),"The RegionObserver's preScannerClose() and postScannerClose()
methods should cover the scanner leaseExpired() situation. ",liujia_ict,liujia_ict,Major,Closed,Fixed,14/Dec/11 16:21,12/Oct/12 05:35
Bug,HBASE-5027,12535191,"HConnection.create(final Connection conf) does not clone, it creates a new Configuration reading *.xmls and then does a merge.",Its more expensive that it should be; its causing TestAdmin to fail after HBASE-4417  went in.,nkeywal,stack,Major,Closed,Fixed,14/Dec/11 18:57,12/Oct/12 05:35
Bug,HBASE-5029,12535202,TestDistributedLogSplitting fails on occasion,"This is how it usually fails: https://builds.apache.org/view/G-L/view/HBase/job/HBase-0.92/lastCompletedBuild/testReport/org.apache.hadoop.hbase.master/TestDistributedLogSplitting/testWorkerAbort/

Assigning mighty Prakash since he offered to take a looksee.",khemani,stack,Critical,Closed,Fixed,14/Dec/11 20:27,15/Sep/22 07:03
Bug,HBASE-5030,12535206,"Some tests do not close the HFile.Reader they use, leaving some file descriptors open",,nkeywal,nkeywal,Trivial,Closed,Fixed,14/Dec/11 20:55,12/Jun/22 19:57
Bug,HBASE-5031,12535213,[89-fb] Remove hard-coded non-existent host name from TestScanner ,TestScanner is failing on 0.89-fb because it has a hard-coded fake host name that it is trying to look up. Replacing this with 127.0.0.1:<random_port> instead.,,mikhail,Minor,Closed,Fixed,14/Dec/11 22:00,12/Jun/22 19:57
Bug,HBASE-5038,12535321,Some tests leak connections,,nkeywal,nkeywal,Minor,Closed,Fixed,15/Dec/11 14:41,12/Oct/12 05:34
Bug,HBASE-5040,12535345,Secure HBase builds fail,"I saw the following in HBase-0.92-security build #39:
{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testCompile (default-testCompile) on project hbase: Compilation failure
[ERROR] <https://builds.apache.org/job/HBase-0.92-security/ws/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java>:[590,4] method does not override or implement a method from a supertype
[ERROR] -> [Help 1]
org.apache.maven.lifecycle.LifecycleExecutionException: Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:testCompile (default-testCompile) on project hbase: Compilation failure
<https://builds.apache.org/job/HBase-0.92-security/ws/trunk/src/test/java/org/apache/hadoop/hbase/regionserver/TestStore.java>:[590,4] method does not override or implement a method from a supertype
{code}

The above was probably introduced by HBASE-5006",stack,zhihyu@ebaysf.com,Major,Closed,Fixed,15/Dec/11 17:59,20/Nov/15 11:54
Bug,HBASE-5041,12535346,Major compaction on non existing table does not throw error ,"Following will not complain even if fubar does not exist

{code}
echo ""major_compact 'fubar'"" | $HBASE_HOME/bin/hbase shell
{code}

The downside for this defect is that major compaction may be skipped due to
a typo by Ops.",shrijeet,shrijeet,Major,Closed,Fixed,15/Dec/11 18:08,12/Oct/12 05:34
Bug,HBASE-5049,12535384,TestHLogSplit.testLogRollAfterSplitStart not working due to HBASE-5006,"java.lang.IllegalStateException: Can't overwrite cause
	at java.lang.Throwable.initCause(Throwable.java:320)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.createWriter(HLog.java:624)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.createWriterInstance(HLog.java:570)
	at org.apache.hadoop.hbase.regionserver.wal.HLog.rollWriter(HLog.java:504)
	at org.apache.hadoop.hbase.regionserver.wal.TestHLogSplit.testLogRollAfterSplitStart(TestHLogSplit.java:880)",jxiang,jxiang,Trivial,Closed,Fixed,15/Dec/11 23:09,20/Nov/15 11:55
Bug,HBASE-5051,12535424,HBaseTestingUtility#getHBaseAdmin() creates a new HBaseAdmin instance at each call,"As it's a new instance, it should be closed. As the function name seems to imply that it's an instance managed by HBaseTestingUtility, most of the users don't close it => leak",nkeywal,nkeywal,Minor,Closed,Fixed,16/Dec/11 08:36,12/Oct/12 05:35
Bug,HBASE-5052,12535427,"The path where a dynamically loaded coprocessor jar is copied on the local file system depends on the region name (and implicitly, the start key)","When loading a coprocessor from hdfs, the jar file gets copied to a path on the local filesystem, which depends on the region name, and the region start key. The name is ""cleaned"", but not enough, so when you have filesystem unfriendly characters (/?:, etc), the coprocessor is not loaded, and an error is thrown",adragomir,adragomir,Major,Closed,Fixed,16/Dec/11 08:44,20/Nov/15 11:55
Bug,HBASE-5053,12535453,HCM Tests leak connections,"There are simple leaks and one more complex.

The complex one comes from the fact fact HConnectionManager.HConnectionImplementation keeps a *reference* to the configuration used for the creation. So if this configuration is updated later, the HConnectionKey created initially will differ from the current one. As a consequence, the close() will not find the connection anymore in the list, and the connection won't be deleted.

I added a warning when a close does not find the connection in the list; but I wonder if we should not copy the HConnectionKey instead of keeping a reference.",nkeywal,nkeywal,Minor,Closed,Fixed,16/Dec/11 12:56,12/Oct/12 05:34
Bug,HBASE-5055,12535480,Build against hadoop 0.22 broken,"I got the following when compiling TRUNK against hadoop 0.22:
{code}
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-compiler-plugin:2.0.2:compile (default-compile) on project hbase: Compilation failure: Compilation failure:
[ERROR] /Users/zhihyu/trunk-hbase/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java:[37,39] cannot find symbol
[ERROR] symbol  : class DFSInputStream
[ERROR] location: class org.apache.hadoop.hdfs.DFSClient
[ERROR] 
[ERROR] /Users/zhihyu/trunk-hbase/src/main/java/org/apache/hadoop/hbase/regionserver/wal/SequenceFileLogReader.java:[109,37] cannot find symbol
[ERROR] symbol  : class DFSInputStream
[ERROR] location: class org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogReader.WALReader.WALReaderFSDataInputStream
{code}",stack,zhihyu@ebaysf.com,Blocker,Closed,Fixed,16/Dec/11 16:51,12/Oct/12 05:34
Bug,HBASE-5060,12535556,HBase client is blocked forever,"Since the client had a temporary network failure, After it recovered.
I found my client thread was blocked. 
Looks below stack and logs, It said that we use a invalid CatalogTracker in function ""tableExists"".

Block stack:
""WriteHbaseThread33"" prio=10 tid=0x00007f76bc27a800 nid=0x2540 in Object.wait() [0x00007f76af4f3000]
   java.lang.Thread.State: TIMED_WAITING (on object monitor)
         at java.lang.Object.wait(Native Method)
         at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMeta(CatalogTracker.java:331)
         - locked <0x00007f7a67817c98> (a java.util.concurrent.atomic.AtomicBoolean)
         at org.apache.hadoop.hbase.catalog.CatalogTracker.waitForMetaServerConnectionDefault(CatalogTracker.java:366)
         at org.apache.hadoop.hbase.catalog.MetaReader.tableExists(MetaReader.java:427)
         at org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:164)
         at com.huawei.hdi.hbase.HbaseFileOperate.checkHtableState(Unknown Source)
         at com.huawei.hdi.hbase.HbaseReOper.reCreateHtable(Unknown Source)
         - locked <0x00007f7a4c5dc578> (a com.huawei.hdi.hbase.HbaseReOper)
         at com.huawei.hdi.hbase.HbaseFileOperate.writeToHbase(Unknown Source)
         at com.huawei.hdi.hbase.WriteHbaseThread.run(Unknown Source)

In ZooKeeperNodeTracker, We don't throw the KeeperException to high level.
So in CatalogTracker level, We think ZooKeeperNodeTracker start success and
continue to process .

[WriteHbaseThread33]2011-12-16 17:07:33,153[WARN ]  | hconnection-0x334129cf6890051-0x334129cf6890051-0x334129cf6890051 Unable to get data of znode /hbase/root-region-server | org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:557)
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/root-region-server
         at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
         at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
         at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:931)
         at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:549)
         at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.start(ZooKeeperNodeTracker.java:73)
         at org.apache.hadoop.hbase.catalog.CatalogTracker.start(CatalogTracker.java:136)
         at org.apache.hadoop.hbase.client.HBaseAdmin.getCatalogTracker(HBaseAdmin.java:111)
         at org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:162)
         at com.huawei.hdi.hbase.HbaseFileOperate.checkHtableState(Unknown Source)
         at com.huawei.hdi.hbase.HbaseReOper.reCreateHtable(Unknown Source)
         at com.huawei.hdi.hbase.HbaseFileOperate.writeToHbase(Unknown Source)
         at com.huawei.hdi.hbase.WriteHbaseThread.run(Unknown Source)
[WriteHbaseThread33]2011-12-16 17:07:33,361[ERROR]  | hconnection-0x334129cf6890051-0x334129cf6890051-0x334129cf6890051 Received unexpected KeeperException, re-throwing exception | org.apache.hadoop.hbase.zookeeper.ZooKeeperWatcher.keeperException(ZooKeeperWatcher.java:385)
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/root-region-server
         at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
         at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
         at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:931)
         at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:549)
         at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.start(ZooKeeperNodeTracker.java:73)
         at org.apache.hadoop.hbase.catalog.CatalogTracker.start(CatalogTracker.java:136)
         at org.apache.hadoop.hbase.client.HBaseAdmin.getCatalogTracker(HBaseAdmin.java:111)
         at org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:162)
         at com.huawei.hdi.hbase.HbaseFileOperate.checkHtableState(Unknown Source)
         at com.huawei.hdi.hbase.HbaseReOper.reCreateHtable(Unknown Source)
         at com.huawei.hdi.hbase.HbaseFileOperate.writeToHbase(Unknown Source)
         at com.huawei.hdi.hbase.WriteHbaseThread.run(Unknown Source)


[WriteHbaseThread33]2011-12-16 17:07:33,361[FATAL]  | Unexpected exception during initialization, aborting | org.apache.hadoop.hbase.client.HConnectionManager$HConnectionImplementation.abort(HConnectionManager.java:1351)
org.apache.zookeeper.KeeperException$ConnectionLossException: KeeperErrorCode = ConnectionLoss for /hbase/root-region-server
         at org.apache.zookeeper.KeeperException.create(KeeperException.java:90)
         at org.apache.zookeeper.KeeperException.create(KeeperException.java:42)
         at org.apache.zookeeper.ZooKeeper.getData(ZooKeeper.java:931)
         at org.apache.hadoop.hbase.zookeeper.ZKUtil.getDataAndWatch(ZKUtil.java:549)
         at org.apache.hadoop.hbase.zookeeper.ZooKeeperNodeTracker.start(ZooKeeperNodeTracker.java:73)
         at org.apache.hadoop.hbase.catalog.CatalogTracker.start(CatalogTracker.java:136)
         at org.apache.hadoop.hbase.client.HBaseAdmin.getCatalogTracker(HBaseAdmin.java:111)
         at org.apache.hadoop.hbase.client.HBaseAdmin.tableExists(HBaseAdmin.java:162)
         at com.huawei.hdi.hbase.HbaseFileOperate.checkHtableState(Unknown Source)
         at com.huawei.hdi.hbase.HbaseReOper.reCreateHtable(Unknown Source)
         at com.huawei.hdi.hbase.HbaseFileOperate.writeToHbase(Unknown Source)
         at com.huawei.hdi.hbase.WriteHbaseThread.run(Unknown Source)
",sunnygao,sunnygao,Critical,Closed,Fixed,17/Dec/11 06:34,20/Nov/15 11:52
Bug,HBASE-5062,12535603,Missing logons if security is enabled,Somehow the attached changes are missing from the security integration. ,apurtell,apurtell,Major,Closed,Fixed,18/Dec/11 00:42,20/Nov/15 11:54
Bug,HBASE-5063,12535606,RegionServers fail to report to backup HMaster after primary goes down.,"# Setup cluster with two HMasters
# Observe that HM1 is up and that all RS's are in the RegionServer list on web page.
# Kill (not even -9) the active HMaster
# Wait for ZK to time out (default 3 minutes).
# Observe that HM2 is now active.  Tables may show up but RegionServers never report on web page.  Existing connections are fine.  New connections cannot find regionservers.

Note: 
* If we replace a new HM1 in the same place and kill HM2, the cluster functions normally again after recovery.  This sees to indicate that regionservers are stuck trying to talk to the old HM1.


",jmhsieh,jmhsieh,Critical,Closed,Fixed,18/Dec/11 02:43,20/Nov/15 11:53
Bug,HBASE-5068,12535749,RC1 can not build its hadoop-0.23 profile,The hadoop .23 version needs to be bumped to 0.23.1-SNAPSHOT,rvs,rvs,Major,Closed,Fixed,19/Dec/11 20:04,12/Oct/12 05:34
Bug,HBASE-5071,12535784,HFile has a possible cast issue.,"HBASE-3040 introduced this line originally in HFile.Reader#loadFileInfo(...):

{code}
int allIndexSize = (int)(this.fileSize - this.trailer.dataIndexOffset - FixedFileTrailer.trailerSize());
{code}

Which on trunk today, for HFile v1 is:

{code}
int sizeToLoadOnOpen = (int) (fileSize - trailer.getLoadOnOpenDataOffset() -
        trailer.getTrailerSize());
{code}

This computed (and casted) integer is then used to build an array of the same size. But if fileSize is very large (>> Integer.MAX_VALUE), then there's an easy chance this can go negative at some point and spew out exceptions such as:

{code}
java.lang.NegativeArraySizeException 
at org.apache.hadoop.hbase.io.hfile.HFile$Reader.readAllIndex(HFile.java:805) 
at org.apache.hadoop.hbase.io.hfile.HFile$Reader.loadFileInfo(HFile.java:832) 
at org.apache.hadoop.hbase.regionserver.StoreFile$Reader.loadFileInfo(StoreFile.java:1003) 
at org.apache.hadoop.hbase.regionserver.StoreFile.open(StoreFile.java:382) 
at org.apache.hadoop.hbase.regionserver.StoreFile.createReader(StoreFile.java:438) 
at org.apache.hadoop.hbase.regionserver.Store.loadStoreFiles(Store.java:267) 
at org.apache.hadoop.hbase.regionserver.Store.<init>(Store.java:209) 
at org.apache.hadoop.hbase.regionserver.HRegion.instantiateHStore(HRegion.java:2088) 
at org.apache.hadoop.hbase.regionserver.HRegion.initialize(HRegion.java:358) 
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2661) 
at org.apache.hadoop.hbase.regionserver.HRegion.openHRegion(HRegion.java:2647) 
{code}

Did we accidentally limit single region sizes this way?

(Unsure about HFile v2's structure so far, so do not know if v2 has the same issue.)",,qwertymaniac,Major,Closed,Fixed,20/Dec/11 05:13,23/Sep/13 18:30
Bug,HBASE-5073,12535788,Registered listeners not getting removed leading to memory leak in HBaseAdmin,"HBaseAdmin apis like tableExists(), flush, split, closeRegion uses catalog tracker.  Every time Root node tracker and meta node tracker are started and a listener is registered.  But after the operations are performed the listeners are not getting removed. Hence if the admin apis are consistently used then it may lead to memory leak.  ",ram_krish,ram_krish,Major,Closed,Fixed,20/Dec/11 06:07,20/Nov/15 11:52
Bug,HBASE-5076,12535856,HBase shell hangs when creating some 'illegal' tables.,"In hbase shell. These commands hang:
{code}
create 'hbase.version','foo'
create 'splitlog','foo'
{code}

Interestingly

{code}
create 'hbase.id','foo'
create existingtablename, 'foo'
create '.META.','foo'
create '-ROOT-','foo'
{code}

are properly rejected.

We should probably either rename to make the files illegal table names (hbase.version to .hbase.version and splitlog to .splitlog) or we could add more special cases.",xieliang007,jmhsieh,Minor,Closed,Fixed,20/Dec/11 15:46,23/Sep/13 18:30
Bug,HBASE-5077,12535910,"SplitLogWorker fails to let go of a task, kills the RS","I hope I didn't break spacetime continuum, I got this while testing 0.92.0:

{quote}
2011-12-20 03:06:19,838 FATAL org.apache.hadoop.hbase.regionserver.SplitLogWorker: logic error - end task /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A9100%2Fhbase%2F.logs%2Fsv4r14s38%2C62023%2C1324345935047-splitting%2Fsv4r14s38%252C62023%252C1324345935047.1324349363814 done failed because task doesn't exist
org.apache.zookeeper.KeeperException$NoNodeException: KeeperErrorCode = NoNode for /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A9100%2Fhbase%2F.logs%2Fsv4r14s38%2C62023%2C1324345935047-splitting%2Fsv4r14s38%252C62023%252C1324345935047.1324349363814
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:111)
        at org.apache.zookeeper.KeeperException.create(KeeperException.java:51)
        at org.apache.zookeeper.ZooKeeper.setData(ZooKeeper.java:1228)
        at org.apache.hadoop.hbase.zookeeper.RecoverableZooKeeper.setData(RecoverableZooKeeper.java:372)
        at org.apache.hadoop.hbase.zookeeper.ZKUtil.setData(ZKUtil.java:654)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.endTask(SplitLogWorker.java:372)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.grabTask(SplitLogWorker.java:280)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.taskLoop(SplitLogWorker.java:197)
        at org.apache.hadoop.hbase.regionserver.SplitLogWorker.run(SplitLogWorker.java:165)
        at java.lang.Thread.run(Thread.java:662)
{quote}

I'll post more logs in a moment. What I can see is that the master shuffled that task around a bit and one of the region servers died on this stack trace while the others were able to interrupt themselves.",jdcryans,jdcryans,Critical,Closed,Fixed,20/Dec/11 22:00,20/Nov/15 11:53
Bug,HBASE-5078,12535912,DistributedLogSplitter failing to split file because it has edits for lots of regions,"Testing 0.92.0RC, ran into interesting issue where a log file had edits for many regions and just opening the file per region was taking so long, we were never updating our progress and so the split of the log just kept failing; in this case, the first 40 edits in a file required our opening 35 files -- opening 35 files took longer than the hard-coded 25 seconds its supposed to take ""acquiring"" the task.

First, here is master's view:

{code}
2011-12-20 17:54:09,184 DEBUG org.apache.hadoop.hbase.master.SplitLogManager: task not yet acquired /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403487679 ver = 0
...
2011-12-20 17:54:09,233 INFO org.apache.hadoop.hbase.master.SplitLogManager: task /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403487679 acquired by sv4r27s44,7003,1324365396664
...
2011-12-20 17:54:35,475 DEBUG org.apache.hadoop.hbase.master.SplitLogManager: task not yet acquired /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403573033 ver = 3
{code}

Master then gives it elsewhere.

Over on the regionserver we see:

{code}
2011-12-20 17:54:09,233 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: worker sv4r27s44,7003,1324365396664 acquired task /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403487679
....
2011-12-20 17:54:10,714 DEBUG org.apache.hadoop.hbase.regionserver.wal.SequenceFileLogWriter: Path=hdfs://sv4r11s38:7000/hbase/splitlog/sv4r27s44,7003,1324365396664_hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403487679/TestTable/6b6bfc2716dff952435ab26f018648b2/recovered.ed
its/0000000000000278862.temp, syncFs=true, hflush=false
....

{code}

.... and so on till:

{code}
2011-12-20 17:54:36,876 INFO org.apache.hadoop.hbase.regionserver.SplitLogWorker: task /hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403487679 preempted from sv4r27s44,7003,1324365396664, current task state and owner=owned sv4r28s44,7003,1324365396678

....

2011-12-20 17:54:37,112 WARN org.apache.hadoop.hbase.regionserver.SplitLogWorker: Failed to heartbeat the task/hbase/splitlog/hdfs%3A%2F%2Fsv4r11s38%3A7000%2Fhbase%2F.logs%2Fsv4r31s44%2C7003%2C1324365396770-splitting%2Fsv4r31s44%252C7003%252C1324365396770.1324403487679
....


{code}

When above happened, we'd only processed 40 edits.  As written, we only heatbeat every 1024 edits.",stack,stack,Critical,Closed,Fixed,20/Dec/11 22:01,20/Nov/15 11:54
Bug,HBASE-5081,12535956,Distributed log splitting deleteNode races against splitLog retry ,"Recently, during 0.92 rc testing, we found distributed log splitting hangs there forever.  Please see attached screen shot.
I looked into it and here is what happened I think:

1. One rs died, the servershutdownhandler found it out and started the distributed log splitting;
2. All three tasks failed, so the three tasks were deleted, asynchronously;
3. Servershutdownhandler retried the log splitting;
4. During the retrial, it created these three tasks again, and put them in a hashmap (tasks);
5. The asynchronously deletion in step 2 finally happened for one task, in the callback, it removed one
task in the hashmap;
6. One of the newly submitted tasks' zookeeper watcher found out that task is unassigned, and it is not
in the hashmap, so it created a new orphan task.
7.  All three tasks failed, but that task created in step 6 is an orphan so the batch.err counter was one short,
so the log splitting hangs there and keeps waiting for the last task to finish which is never going to happen.

So I think the problem is step 2.  The fix is to make deletion sync, instead of async, so that the retry will have
a clean start.

Async deleteNode will mess up with split log retrial.  In extreme situation, if async deleteNode doesn't happen
soon enough, some node created during the retrial could be deleted.

deleteNode should be sync.",khemani,jxiang,Major,Closed,Fixed,21/Dec/11 03:50,20/Nov/15 11:55
Bug,HBASE-5085,12536088,fix test-patch script from setting the ulimit,"test-patch.sh script sets the ulimit -n 1024 just after triggering the patch setting this overrides the underlying systems ulimit and hence failing the hbase tests.

",gkesavan,gkesavan,Major,Closed,Fixed,21/Dec/11 21:32,12/Oct/12 05:35
Bug,HBASE-5088,12536123,A concurrency issue on SoftValueSortedMap,"SoftValueSortedMap is backed by a TreeMap. All the methods in this class are synchronized. If we use this method to add/delete elements, it's ok.
But in HConnectionManager#getCachedLocation, it use headMap to get a view from SoftValueSortedMap#internalMap. Once we operate 
on this view map(like add/delete) in other threads, a concurrency issue may occur.

",larsh,jeason,Critical,Closed,Fixed,22/Dec/11 04:18,12/Oct/12 05:34
Bug,HBASE-5091,12536199,[replication] Update replication doc to reflect current znode structure,"A small nit: The zookeeper node structure in the region server fail over section of the replication document is slightly different than the actual structure. 

The doc shows this:
{noformat}
/hbase/replication/rs/
                      1.1.1.1,60020,123456780/
                        peers/
                              2/
                                1.1.1.1,60020.1234  (Contains a position)
                                1.1.1.1,60020.1265
{noformat}
When in actuality it should be this:
{noformat}
/hbase/replication/rs/
                      1.1.1.1,60020,123456780/
                         2/
                            1.1.1.1,60020.1234  (Contains a position)
                            1.1.1.1,60020.1265
{noformat}
Not a big deal, but it gets confusing when you are going through the code and using the doc as a reference.",ctrezzo,ctrezzo,Trivial,Closed,Fixed,22/Dec/11 19:48,12/Oct/12 05:35
Bug,HBASE-5094,12536368,The META can hold an entry for a region with a different server name from the one actually in the AssignmentManager thus making the region inaccessible.,"{code}
RegionState rit = this.services.getAssignmentManager().isRegionInTransition(e.getKey());
            ServerName addressFromAM = this.services.getAssignmentManager()
                .getRegionServerOfRegion(e.getKey());
            if (rit != null && !rit.isClosing() && !rit.isPendingClose()) {
              // Skip regions that were in transition unless CLOSING or
              // PENDING_CLOSE
              LOG.info(""Skip assigning region "" + rit.toString());
            } else if (addressFromAM != null
                && !addressFromAM.equals(this.serverName)) {
              LOG.debug(""Skip assigning region ""
                    + e.getKey().getRegionNameAsString()
                    + "" because it has been opened in ""
                    + addressFromAM.getServerName());
              }
{code}
In ServerShutDownHandler we try to get the address in the AM.  This address is initially null because it is not yet updated after the region was opened .i.e. the CAll back after node deletion is not yet done in the master side.
But removal from RIT is completed on the master side.  So this will trigger a new assignment.
So there is a small window between the online region is actually added in to the online list and the ServerShutdownHandler where we check the existing address in AM.",ram_krish,ram_krish,Critical,Closed,Fixed,26/Dec/11 06:43,20/Nov/15 11:55
Bug,HBASE-5097,12536410,RegionObserver implementation whose preScannerOpen and postScannerOpen Impl return null can stall the system initialization through NPE,"In HRegionServer.java openScanner()
{code}
      r.prepareScanner(scan);
      RegionScanner s = null;
      if (r.getCoprocessorHost() != null) {
        s = r.getCoprocessorHost().preScannerOpen(scan);
      }
      if (s == null) {
        s = r.getScanner(scan);
      }
      if (r.getCoprocessorHost() != null) {
        s = r.getCoprocessorHost().postScannerOpen(scan, s);
      }
{code}
If we dont have implemention for postScannerOpen the RegionScanner is null and so throwing nullpointer 
{code}
java.lang.NullPointerException
	at java.util.concurrent.ConcurrentHashMap.put(ConcurrentHashMap.java:881)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.addScanner(HRegionServer.java:2282)
	at org.apache.hadoop.hbase.regionserver.HRegionServer.openScanner(HRegionServer.java:2272)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.hadoop.hbase.ipc.WritableRpcEngine$Server.call(WritableRpcEngine.java:364)
	at org.apache.hadoop.hbase.ipc.HBaseServer$Handler.run(HBaseServer.java:1326)

{code}

Making this defect as blocker.. Pls feel free to change the priority if am wrong.  Also correct me if my way of trying out coprocessors without implementing postScannerOpen is wrong.  Am just a learner.
",ram_krish,ram_krish,Major,Closed,Fixed,27/Dec/11 06:41,18/Sep/13 23:21
Bug,HBASE-5099,12536453,ZK event thread waiting for root region assignment may block server shutdown handler for the region sever the root region was on,"A RS died.  The ServerShutdownHandler kicked in and started the logspliting.  SpliLogManager
installed the tasks asynchronously, then started to wait for them to complete.

The task znodes were not created actually.  The requests were just queued.
At this time, the zookeeper connection expired.  HMaster tried to recover the expired ZK session.
During the recovery, a new zookeeper connection was created.  However, this master became the
new master again.  It tried to assign root and meta.

Because the dead RS got the old root region, the master needs to wait for the log splitting to complete.
This waiting holds the zookeeper event thread.  So the async create split task is never retried since
there is only one event thread, which is waiting for the root region assigned.
",jxiang,jxiang,Major,Closed,Fixed,27/Dec/11 23:23,12/Oct/12 05:35
Bug,HBASE-5100,12536473,Rollback of split could cause closed region to be opened again,"If master sending close region to rs and region's split transaction concurrently happen,
it may cause closed region to opened. 

See the detailed code in SplitTransaction#createDaughters
{code}
List<StoreFile> hstoreFilesToSplit = null;
    try{
      hstoreFilesToSplit = this.parent.close(false);
      if (hstoreFilesToSplit == null) {
        // The region was closed by a concurrent thread.  We can't continue
        // with the split, instead we must just abandon the split.  If we
        // reopen or split this could cause problems because the region has
        // probably already been moved to a different server, or is in the
        // process of moving to a different server.
        throw new IOException(""Failed to close region: already closed by "" +
          ""another thread"");
      }
    } finally {
      this.journal.add(JournalEntry.CLOSED_PARENT_REGION);
    }
{code}

when rolling back, the JournalEntry.CLOSED_PARENT_REGION causes this.parent.initialize();

Although this region is not onlined in the regionserver, it may bring some potential problem.

For example, in our environment, the closed parent region is rolled back sucessfully , and then starting compaction and split again.

The parent region is f892dd6107b6b4130199582abc78e9c1

master log
{code}
2011-12-26 00:24:42,693 INFO org.apache.hadoop.hbase.master.HMaster: balance hri=writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1., src=dw87.kgb.sqa.cm4,60020,1324827866085, dest=dw80.kgb.sqa.cm4,60020,1324827865780
2011-12-26 00:24:42,693 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Starting unassignment of region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1. (offlining)
2011-12-26 00:24:42,694 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Sent CLOSE to serverName=dw87.kgb.sqa.cm4,60020,1324827866085, load=(requests=0, regions=0, usedHeap=0, maxHeap=0) for region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.
2011-12-26 00:24:42,699 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling new unassigned node: /hbase-tbfs/unassigned/f892dd6107b6b4130199582abc78e9c1 (region=writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1., server=dw87.kgb.sqa.cm4,60020,1324827866085, state=RS_ZK_REGION_CLOSING)
2011-12-26 00:24:42,699 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_CLOSING, server=dw87.kgb.sqa.cm4,60020,1324827866085, region=f892dd6107b6b4130199582abc78e9c1
2011-12-26 00:24:45,348 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=RS_ZK_REGION_CLOSED, server=dw87.kgb.sqa.cm4,60020,1324827866085, region=f892dd6107b6b4130199582abc78e9c1
2011-12-26 00:24:45,349 DEBUG org.apache.hadoop.hbase.master.handler.ClosedRegionHandler: Handling CLOSED event for f892dd6107b6b4130199582abc78e9c1
2011-12-26 00:24:45,349 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Forcing OFFLINE; was=writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1. state=CLOSED, ts=1324830285347
2011-12-26 00:24:45,349 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: master:60000-0x13447f283f40e73 Creating (or updating) unassigned node for f892dd6107b6b4130199582abc78e9c1 with OFFLINE state
2011-12-26 00:24:45,354 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Handling transition=M_ZK_REGION_OFFLINE, server=dw75.kgb.sqa.cm4:60000, region=f892dd6107b6b4130199582abc78e9c1
2011-12-26 00:24:45,354 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Found an existing plan for writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1. destination server is + serverName=dw80.kgb.sqa.cm4,60020,1324827865780, load=(requests=0, regions=1, usedHeap=0, maxHeap=0)
2011-12-26 00:24:45,354 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Using pre-existing plan for region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.; plan=hri=writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1., src=dw87.kgb.sqa.cm4,60020,1324827866085, dest=dw80.kgb.sqa.cm4,60020,1324827865780
2011-12-26 00:24:45,354 DEBUG org.apache.hadoop.hbase.master.AssignmentManager: Assigning region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1. to dw80.kgb.sqa.cm4,60020,1324827865780
2011-12-26 00:24:46,899 DEBUG org.apache.hadoop.hbase.master.handler.OpenedRegionHandler: Opened region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1. on dw80.kgb.sqa.cm4,60020,1324827865780
{code}

RE_dw87 log
{code}
2011-12-26 00:24:42,694 INFO org.apache.hadoop.hbase.regionserver.HRegionServer: Received close region: writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.
2011-12-26 00:24:42,694 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Processing close of writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.
2011-12-26 00:24:42,694 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x43447f30cb31367 Creating unassigned node for f892dd6107b6b4130199582abc78e9c1 in a CLOSING state
2011-12-26 00:24:42,699 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Closing writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.: disabling compactions & flushes
2011-12-26 00:24:42,699 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: waiting for compaction to complete for region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.
2011-12-26 00:24:43,340 INFO org.apache.hadoop.hbase.regionserver.HRegion: aborted compaction on region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1. after 49sec
2011-12-26 00:24:43,340 INFO org.apache.hadoop.hbase.regionserver.HRegion: Running close preflush of writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.
2011-12-26 00:24:43,340 DEBUG org.apache.hadoop.hbase.regionserver.HRegion: Started memstore flush for writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1., current region memstore size 59.5m
2011-12-26 00:24:43,340 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Starting split of region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.
2011-12-26 00:24:45,347 INFO org.apache.hadoop.hbase.regionserver.HRegion: Closed writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.
2011-12-26 00:24:45,347 WARN org.apache.hadoop.hbase.regionserver.HRegion: Region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1. already closed
2011-12-26 00:24:45,347 WARN org.apache.hadoop.hbase.regionserver.CompactSplitThread: Running rollback of failed split of writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.; Failed to close region: already closed by another thread
2011-12-26 00:24:45,348 DEBUG org.apache.hadoop.hbase.zookeeper.ZKAssign: regionserver:60020-0x43447f30cb31367 Successfully transitioned node f892dd6107b6b4130199582abc78e9c1 from RS_ZK_REGION_CLOSING to RS_ZK_REGION_CLOSED
2011-12-26 00:24:45,348 DEBUG org.apache.hadoop.hbase.regionserver.handler.CloseRegionHandler: Closed region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.
2011-12-26 00:24:46,837 INFO org.apache.hadoop.hbase.regionserver.HRegion: Onlined writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.; next sequenceid=717341809
2011-12-26 00:24:46,841 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: Successful rollback of failed split of writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.
2011-12-26 00:24:46,841 INFO org.apache.hadoop.hbase.regionserver.HRegion: Starting compaction on region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.
2011-12-26 00:25:23,288 INFO org.apache.hadoop.hbase.regionserver.HRegion: completed compaction on region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1. after 36sec
2011-12-26 00:25:23,288 INFO org.apache.hadoop.hbase.regionserver.SplitTransaction: Starting split of region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1.
2011-12-26 00:25:24,847 INFO org.apache.hadoop.hbase.catalog.MetaEditor: Offlined parent region writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1. in META
2011-12-26 00:25:26,165 INFO org.apache.hadoop.hbase.regionserver.CompactSplitThread: Region split, META updated, and report to master. Parent=writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324829936318.f892dd6107b6b4130199582abc78e9c1., new regions: writetest,8ZW417DZP93OU6SZ0QQMKTALTDP4883KW5AXSAFMQ952Y6J6VPPXEXRRPCWBR2PK7DQV3RKK28222JMOJSW3JJ8AB05MIREM1CL6,1324830323288.595f458507cecf208640cb4a1be8e293., writetest,DHZX0CD7A4OE1KRILMWEJL2HTBN6OJVSFOKGU0P938DJ1M44B79C068NCZPXK1Z5OD2RQJ6LMA41TC0D44H05525TO3AHLZ4BZXX,1324830323288.ba9376c83327c34c7926fccb68c3b9e3.. Split took 2sec

{code}",zjushch,zjushch,Major,Closed,Fixed,28/Dec/11 05:51,12/Oct/12 05:35
Bug,HBASE-5103,12536593,Fix improper master znode deserialization,"In ActiveMasterManager#blockUntilBecomingActiveMaster the master znode is created as a versioned serialized version of ServerName
{code}
     if (ZKUtil.createEphemeralNodeAndWatch(this.watcher,
          this.watcher.masterAddressZNode, sn.getVersionedBytes())) {
{code}

There are a few user visible places where it is used but not deserialized properly.",jmhsieh,jmhsieh,Minor,Closed,Fixed,29/Dec/11 16:42,12/Oct/12 05:34
Bug,HBASE-5104,12536610,Provide a reliable intra-row pagination mechanism,"Addendum:

Doing pagination (retrieving at most ""limit"" number of KVs at a particular ""offset"") is currently supported via the ColumnPaginationFilter. However, it is not a very clean way of supporting pagination.  Some of the problems with it are:

* Normally, one would expect a query with (Filter(A) AND Filter(B)) to have same results as (query with Filter(A)) INTERSECT (query with Filter(B)). This is not the case for ColumnPaginationFilter as its internal state gets updated depending on whether or not Filter(A) returns TRUE/FALSE for a particular cell.
* When this Filter is used in combination with other filters (e.g., doing AND with another filter using FilterList), the behavior of the query depends on the order of filters in the FilterList. This is not ideal.
* ColumnPaginationFilter is a stateful filter which ends up counting multiple versions of the cell as separate values even if another filter upstream or the ScanQueryMatcher is going to reject the value for other reasons.

Seems like we need a reliable way to do pagination. The particular use case that prompted this JIRA is pagination within the same rowKey. For example, for a given row key R, get columns with prefix P, starting at offset X (among columns which have prefix P) and limit Y. Some possible fixes might be:

1) enhance ColumnPrefixFilter to support another constructor which supports limit/offset.
2) Support pagination (limit/offset) at the Scan/Get API level (rather than as a filter) [Like SQL].

Original Post:

Thanks Jiakai Liu for reporting this issue and doing the initial investigation. Email from Jiakai below:

Assuming that we have an index column family with the following entries:
""tag0:001:thread1""
...
""tag1:001:thread1""
""tag1:002:thread2""
...
""tag1:010:thread10""
...
""tag2:001:thread1""
""tag2:005:thread5""
...

To get threads with ""tag1"" in range [5, 10), I tried the following code:

    ColumnPrefixFilter filter1 = new ColumnPrefixFilter(Bytes.toBytes(""tag1""));
    ColumnPaginationFilter filter2 = new ColumnPaginationFilter(5 /* limit */, 5 /* offset */);

    FilterList filters = new FilterList(Operator.MUST_PASS_ALL);
    filters.addFilter(filter1);
    filters.addFilter(filter2);

    Get get = new Get(USER);
    get.addFamily(COLUMN_FAMILY);
    get.setMaxVersions(1);
    get.setFilter(filters);

Somehow it didn't work as expected. It returned the entries as if the filter1 were not set.

Turns out the ColumnPrefixFilter returns SEEK_NEXT_USING_HINT in some cases. The FilterList filter does not handle this return code properly (treat it as INCLUDE).",madhuvaidya,kannanm,Major,Closed,Fixed,29/Dec/11 19:49,12/Jun/22 20:02
Bug,HBASE-5105,12536615,TestImportTsv failed with hadoop 0.22,"java.io.FileNotFoundException: File does not exist: /home/henkins/.m2/repository/org/apache/hadoop/hadoop-mapred/0.22-SNAPSHOT/hadoop-mapred-0.22-SNAPSHOT.jar
	at org.apache.hadoop.hdfs.DistributedFileSystem.getFileStatus(DistributedFileSystem.java:742)
	at org.apache.hadoop.mapreduce.filecache.TrackerDistributedCacheManager.getFileStatus(TrackerDistributedCacheManager.java:331)
	at org.apache.hadoop.mapreduce.filecache.TrackerDistributedCacheManager.determineTimestamps(TrackerDistributedCacheManager.java:711)
	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:245)
	at org.apache.hadoop.mapreduce.JobSubmitter.copyAndConfigureFiles(JobSubmitter.java:283)
	at org.apache.hadoop.mapreduce.JobSubmitter.submitJobInternal(JobSubmitter.java:350)
	at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1045)
	at org.apache.hadoop.mapreduce.Job$2.run(Job.java:1042)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1153)
	at org.apache.hadoop.mapreduce.Job.submit(Job.java:1042)
	at org.apache.hadoop.mapreduce.Job.waitForCompletion(Job.java:1062)
	at org.apache.hadoop.hbase.mapreduce.TestImportTsv.doMROnTableTest(TestImportTsv.java:215)
	at org.apache.hadoop.hbase.mapreduce.TestImportTsv.testMROnTable(TestImportTsv.java:165)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
",mingma,mingma,Major,Closed,Fixed,29/Dec/11 20:12,20/Nov/15 11:54
Bug,GROOVY-4613,12816435,static import of method with default parameter value broken again: MissingMethodException,"A static method with a default parameter is not found, when it is imported statically and called without prefixing his class:

{code:title=test.groovy}

import static Settings.*
import static ConsoleUI.*

class Settings
{
    static void initialize() {
        writeln(""working"", 100)
        writeln(""failing"")
    }
}

class ConsoleUI
{
    static void writeln(String s, int delay = 0) {
        sleep delay
        println s
    }
}

Settings.initialize()
{code}

Output:
{noformat}
working
Caught: groovy.lang.MissingMethodException: No signature of method: static Settings.writeln() is applicable for argument types: (java.lang.String) values: {""failing""}
	at Settings.initialize(test.groovy:8)
	at test.run(test.groovy:20)
	at test.main(test.groovy)

Exited: 256
{noformat}
",blackdrag,avalon,Major,Closed,Fixed,03/Jan/11 00:28,01/Feb/11 23:31
Bug,GROOVY-4614,12815421,what does a static outer class declaration mean?,"I'm not sure what is supposed to happen here - you might just need to clarify what it means and close it....

If I write this

{code}
static class C {
}
{code}

groovyc will compile it.  It will produce (as far as I can tell) a classfile identical to:

{code}
class C {
}
{code}

Does static have some special meaning?  Maybe it should be getting policed? I imagine static has only been supported in that position since inner type support was added.  Doing the same thing in Java shows:

{code}
C.java:1: modifier static not allowed here
static class C {
       ^
1 error
{code}

I could imagine static might have forced singleton behaviour in some way but it doesn't - the generated class still has a constructor, I can write my own constructor, etc.",guillaume,aclement,Minor,Closed,Fixed,04/Jan/11 16:21,08/Feb/11 23:25
Bug,GROOVY-4615,12815599,BUG! exception in phase 'conversion' in source unit with NumberFormatException,"{code}
def upperLong = 0
new UUID(upperLong, 0xC000000000000000L)
{code}

The code above fails with
{noformat}
BUG! exception in phase 'conversion' in source unit 'Script1.groovy' For input string: ""C000000000000000""
Caused by: java.lang.NumberFormatException: For input string: ""C000000000000000""
{noformat}

I put the above code in Java after facing this issue and it worked fine.

If I reduce the size to ""0xC0000000000L"", it works fine.",blackdrag,roshandawrani,Major,Closed,Fixed,04/Jan/11 23:58,27/Jan/13 07:13
Bug,GROOVY-4617,12815389,Error when running groovysh with verbose option (-v),"When starting groovysh with the verbose option (-v) ""groovy.lang.MissingPropertyException"" is thrown.
See bellow:
**********
C:\>groovysh -v
Groovy Shell (1.7.6, JVM: 1.6.0_07)
Type 'help' or '\h' for help.
-------------------------------------------------------------------------------
groovy:000> a = """"
ERROR groovy.lang.MissingPropertyException:
No such property: buffer for class: org.codehaus.groovy.tools.shell.Groovysh
Possible solutions: buffers, runner
        at java_lang_Runnable$run.call (Unknown Source)
groovy:000>
**********


",guillaume,eniosp,Minor,Closed,Fixed,05/Jan/11 03:16,12/Feb/12 04:03
Bug,GROOVY-4619,12815473,problem compiling @Delegate to an interface that extends another interface,"The following code compiles with 1.7.3 but fails with 1.7.4, 1.7.5 and 1.8.0-beta-3.

{code:title=SomeInterface.groovy|borderStyle=solid}
interface SomeInterface {
    void doSomething()
}
{code}

{code:title=SomeOtherInterface.groovy|borderStyle=solid}
interface SomeOtherInterface extends SomeInterface {}
{code}

{code:title=SomeClass.groovy|borderStyle=solid}
class SomeClass {
    @Delegate
    SomeOtherInterface someOtherInterface
}
{code}

Compiling with 1.7.3:

{noformat}
 $ groovy -version
Groovy Version: 1.7.3 JVM: 1.6.0_22
 $ groovyc SomeClass.groovy 
 $ 
{noformat}

Compiling with 1.7.4:

{noformat}
 $ groovy -version
Groovy Version: 1.7.4 JVM: 1.6.0_22
 $ groovyc SomeClass.groovy 
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
SomeClass.groovy: 1: Can't have an abstract method in a non-abstract class. The class 'SomeClass' must be declared abstract or the method 'void doSomething()' must be implemented.
 @ line 1, column 1.
   class SomeClass {
   ^

1 error

 $ 
{noformat}

Compiling with 1.7.5:

{noformat}
 $ groovy -version
Groovy Version: 1.7.5 JVM: 1.6.0_22
 $ groovyc SomeClass.groovy 
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
SomeClass.groovy: 1: Can't have an abstract method in a non-abstract class. The class 'SomeClass' must be declared abstract or the method 'void doSomething()' must be implemented.
 @ line 1, column 1.
   class SomeClass {
   ^

1 error

 $ 
{noformat}

Compiling with 1.8.0-beta-3:

{noformat}
 $ groovy -version
Groovy Version: 1.8.0-beta-3 JVM: 1.6.0_22
 $ groovyc SomeClass.groovy 
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
SomeClass.groovy: 1: Can't have an abstract method in a non-abstract class. The class 'SomeClass' must be declared abstract or the method 'void doSomething()' must be implemented.
 @ line 1, column 1.
   class SomeClass {
   ^

1 error

 $ 
{noformat}

This may be related to GROOVY-4163.",paulk,brownj,Major,Closed,Fixed,05/Jan/11 14:10,08/Feb/11 23:25
Bug,GROOVY-4626,12815478,Console launched from groovy-all-1.7.6.jar now has ivy dependancy,"Previous to 1.7.6, I have been launching a Groovy console in my application by a call to:

groovy.ui.Console.main(new String[0]);

My appliction makes use of the emmbedable grovy-all-1.7.X.jar.

However, in 1.7.6, the console is now dependent on Ivy (through the additional code in the Console constructor).  But the org.apache.ivy classes are not in the grovy-all-1.7.6.jar.

I suggest that either:
- The org.apache.ivy classes are also included in the all jar file
- The dependency is documented (it may be already but I have missed it)
- It is made possible to launch the Console without the dependency",paulk,mikevines,Major,Closed,Fixed,13/Jan/11 05:23,15/Feb/11 14:35
Bug,GROOVY-4627,12816895,Binding support for group attribute,"Binding classes do not support the ""group"" attribute.  Here's the relevant code (using Griffon):

bindGroup(id: 'formElements')
textField(text: bind(target: model.currentRecord, 'count',
                     value: '0',
                     id: 'countBinding',
                     group: formElements))

When run, produces ""No such property: group for class: groovy.swing.binding.JTextComponentTextBinding"".  From Andres Almiray of Griffon:

""The problem appears to be in BindFactory.groovy:321, where the group: attribute is set on the binding instance, which may or may nor support he property (in this case it does not). this is certainly a bug in BindFactory ...""",aalmiray,zoerb,Major,Closed,Fixed,13/Jan/11 15:26,22/Dec/12 01:10
Bug,GROOVY-4628,12815480,typo in document comment of Immuable,{#code BigDecimal} should be {@code BigDecimal} ,guillaume,uehaj,Trivial,Closed,Fixed,15/Jan/11 16:38,05/Apr/15 14:44
Bug,GROOVY-4630,12815472,NPE in OptimizingStatementWriter,"With the very latest (but not any earlier) 1.8-beta-4 snapshot, I'm getting the following exception when compiling spock-core (1.8 branch on GitHub). No transforms are involved here.

{noformat}
[10:44:57]: [org.spockframework:spock-core] Caused by: java.lang.NullPointerException
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter$OptVisitor.visitBinaryExpression(OptimizingStatementWriter.java:467)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.ast.expr.BinaryExpression.visit(BinaryExpression.java:49)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.ast.CodeVisitorSupport.visitReturnStatement(CodeVisitorSupport.java:73)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitReturnStatement(ClassCodeVisitorSupport.java:210)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter$OptVisitor.visitReturnStatement(OptimizingStatementWriter.java:370)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.ast.stmt.ReturnStatement.visit(ReturnStatement.java:47)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter$OptVisitor.visitBlockStatement(OptimizingStatementWriter.java:492)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter$OptVisitor.visitClass(OptimizingStatementWriter.java:365)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.setNodeMeta(OptimizingStatementWriter.java:343)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:127)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.control.CompilationUnit$13.call(CompilationUnit.java:754)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:948)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:533)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:511)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:488)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:467)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.gmaven.runtime.v1_7.ClassCompilerFeature$ClassCompilerImpl.compile(ClassCompilerFeature.java:148)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.gmaven.plugin.compile.AbstractCompileMojo.compile(AbstractCompileMojo.java:200)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.gmaven.plugin.compile.AbstractCompileMojo.process(AbstractCompileMojo.java:164)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.gmaven.plugin.ComponentMojoSupport.doExecute(ComponentMojoSupport.java:60)
[10:44:57]: [org.spockframework:spock-core] 	at org.codehaus.gmaven.plugin.MojoSupport.execute(MojoSupport.java:69)
{noformat}",blackdrag,pniederw,Major,Closed,Fixed,18/Jan/11 03:48,29/Jan/11 07:48
Bug,GROOVY-4632,12816485,in eachRow's closures rows do not always respond to getClass() ,"When I try to execute:

{code}
def sqlq = Sql.newInstance(""jdbc:oracle:thin:@${host}:1521:${sid}"", ""user"",
                     ""pw"", ""oracle.jdbc.OracleDriver"")
sqlq.eachRow('select * from dual where rownum<?',[2] ){ row ->
    println (row.getClass())
}
sqlq.close()
{code}

i get:

groovy.lang.MissingMethodException: No signature of method: groovy.sql.GroovyResultSet.getClass() is applicable for argument types: () values: []
Possible solutions: getClass(), metaClass(groovy.lang.Closure), getMetaClass(), getMetaClass(), equals(java.lang.Object), getAt(java.lang.String)


Following code gives:

{code}
def sqlq = Sql.newInstance(""jdbc:oracle:thin:@${host}:1521:${sid}"", ""user"",
                     ""pw"", ""oracle.jdbc.OracleDriver"")
sqlq.eachRow('select * from dual where rownum<?',[2] ){ row ->
    println (row)
}
sqlq.close()
{code}

[DUMMY:X]

I suppose object.getClass() method calls should always work.
",paulk,dariusan,Major,Closed,Fixed,19/Jan/11 04:05,04/Jul/13 00:21
Bug,GROOVY-4633,12815481,@InheritConstructors does not work on inner classes,"When you use the @InheritConstructors to pull all the constructors of a super class into a subclass this fails to work when it is used on an Inner Class. Below is an example of code that fails with error:
{noformat}
groovy.lang.GroovyRuntimeException: Could not find matching constructor for: D$F(D, java.lang.Integer)
{noformat}
when you would expect it to run.
{code}
import groovy.transform.InheritConstructors

class A {
    def arg
    A(int arg) {this.arg=arg}
}

@InheritConstructors //add it here just in case that worked!
class D {
    class E extends A {
        E(int arg) {super(arg)}
    }
    @InheritConstructors //this should make class F identical to class E
    class F extends A{}
   
    public test() {
        def test1=new E(1) //Works FINE.
        println test1.arg  // prints 1 as expected  
        def test2=new F(1) //throws GroovyRuntimeException here due to missing constructor.
        println test2.arg
    }
}

def test3=new D()
test3.test()
{code}
",paulk,owensr,Minor,Closed,Fixed,20/Jan/11 08:47,13/May/12 03:30
Bug,GROOVY-4637,12816683,XmlSlurper Unable to Access Attributes with Same Name but in Different Namespaces,"When using XMLSlurper to parse XML in which there is an element with two attributes having the same name but belonging to different namespaces, one of the attribute values will be unaccessible.
This is due to XMLSlurper using the local attribute name as a key to a map in which data about the attributes are inserted and thus a key-collision will occur.
Adding the following method to the groovy.util.XmlSlurperTest in the Groovy source-code will expose the problem:
{code}
    void testSameNameAttributes() {
        def theInputData = """"""
        <RootElement xmlns=""http://www.ivan.com/ns1"" xmlns:two=""http://www.ivan.com/ns2"">
            <ChildElement ItemId=""FirstItemId"" two:ItemId=""SecondItemId"">Child element data</ChildElement>
        </RootElement>""""""
        def theXml = new MyXmlSlurper().parseText(theInputData).declareNamespace(one:""http://www.ivan.com/ns1"", two: ""http://www.ivan.com/ns2"")
        
        assert theXml.ChildElement.@'ItemId' == ""FirstItemId""
        assert theXml.ChildElement.@'two:ItemId' == ""SecondItemId""
    }
{code}

The following patches will make the above test pass, but I am not entirely sure it is the best solution:
Patching the XmlSlurper.startElement method:
{code}
public void startElement(final String namespaceURI, final String localName,
        final String qName, final Attributes atts) throws SAXException
    {
        addCdata();

        final Map attributes = new HashMap();
        final Map attributeNamespaces = new HashMap();

        for (int i = atts.getLength() - 1; i != -1; i--)
        {
            if (atts.getURI(i).length() == 0)
            {
                attributes.put(atts.getQName(i), atts.getValue(i));
            } else
            {
                // PATCHED START - Use fully qualified attribute names instead of just local attribute names
                attributes.put(atts.getQName(i), atts.getValue(i));
                attributeNamespaces.put(atts.getQName(i), atts.getURI(i));
                // PATCH END

            }

        }

        final Node newElement;
        ...
{code}

Patching groovy.util.slurpersupport.Attributes, method iterator() to use namespace-qualified attribute names when looking up attribute value. This is the patch I am unsure whether it is the best solution.
{code}
    public Iterator iterator() {
        return new NodeIterator(nodeIterator()) {
            protected Object getNextNode(final Iterator iter) {
                while (iter.hasNext()) {
                    final Object next = iter.next();
                    if (next instanceof Attribute) {
                        return next;
                    } else {
                        // PATCH START - Added namespace prefix when looking up attribute.
                        String attributeKey = """";
                        if (Attributes.this.namespacePrefix != null &&
                            !""*"".equals(Attributes.this.namespacePrefix) &&
                            Attributes.this.namespacePrefix.length() > 0) {
                            attributeKey = Attributes.this.namespacePrefix + "":"";
                        }
                        attributeKey += Attributes.this.attributeName;
                        final String value = (String) ((Node) next).attributes().get(attributeKey);
                        // PATCH END
                        if (value != null) {
                            return new Attribute(Attributes.this.attributeName,
                                    value,
                                    new NodeChild((Node) next, Attributes.this.parent.parent, """", Attributes.this.namespaceTagHints),
                                    """",
                                    Attributes.this.namespaceTagHints);
                        }
                    }
                }
                return null;
            }
        };
    }
{code}
",paulk,krizsan,Major,Closed,Fixed,22/Jan/11 04:54,05/Apr/15 14:44
Bug,GROOVY-4641,12815471,Defining an abstract method in enum,"In Java you can define abstract methods for an enum type to override it with a concrete method in a constant-specific class body (see Effective Java, 2nd Ed., page 152). When I try to do the same in Groovy I get a compilation error:

Can't have an abstract method in a non-abstract class. The class 'Day' must be declared abstract or the method 'java.lang.String getAction()' must be implemented. 

This compilation error can be reproduced with this enum:

{code}
enum Day {
   SUNDAY {
      String getAction() {
         'Relax'
      }
   },
   MONDAY {
       String getAction() {
          'Work'
       }
   }

   abstract String getAction()
}
{code}

An enum in Groovy should provide the same behavior for enums as Java.",paulk,subzero66,Major,Closed,Fixed,24/Jan/11 07:58,12/Apr/13 16:55
Bug,GROOVY-4645,12817243,Convariant returns causes compiler to fail when generating property getters,"When attempting to compile the following groovy code I get the compiler error under groovy 1.7.6 and 1.8.3-beta-3.  This has to do with defining an interface (or class) with getters and then defining properties (or final members variables) on a subclass using covariant returns.  I can work around this issue by explicitly defining the required getter in the subclass which is rather annoying b/c I like groovy getter generation and in our codebase we are running into this a lot.  This is also a problem when using ""def"" types.  I would be glad to assist in any way if I can.  Thanks.


For example:
{code}
interface CovariantReturns {
  Foo getGood()
  Object getBad()
}

class Foo {}

class CovariantReturnsImpl implements CovariantReturns {
  //getter is generated by groovy as required by the interface
  final Foo good

  //this variable is defined as the subtype of Object
  final Foo bad

  /*
   I would expect that the following method would be generated by groovy
   which is using Java's covariant returns feature.  Including the method below
   will allow the groovy compiler to succeed but is annoying that it has to be explicitly
   defined in code.

   public Foo getBad() { return bad }
   */
}
{code}

{noformat}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
CovariantReturns.groovy: 8: Can't have an abstract method in a non-abstract class. The class 'CovariantReturns
Impl' must be declared abstract or the method 'java.lang.Object getBad()' must be implemented.
 @ line 8, column 1.
   class CovariantReturnsImpl implements CovariantReturns {
   ^

1 error
{noformat}
",melix,leo.herbie,Major,Closed,Fixed,27/Jan/11 21:07,24/Dec/11 03:07
Bug,GROOVY-4646,12817053,Having a setter with a return value causes the stub generator to generate multiple setters and then fail to compile,"Having a class with a property and then explicitly defining the setter but giving it a return value (common for Builder classes) will compile fine with groovyc without joint compiling.  When using joint compiling the java stubs generated are invalid.  They end up with multiple setters - one normal void setter and one with the return value.  Obviously this is not valid code.  See the following example:

{code}
class SetterWithReturn {
  String foo
  def String bar

  SetterWithReturn setFoo(String foo) { this.foo = foo; return this; }
  SetterWithReturn setBar(String bar) { this.bar = bar; return this; }
}
{code}

{code}
public class SetterWithReturnClient {
    { new SetterWithReturn(); }
}
{code}

Here is the error the compiler gives:

{code}

C:\IdeaProjects\groovy-bugs>groovyc -j src\main\groovy\SetterWithReturn.groovy src\main\java\SetterWithReturnC
lient.java
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
Compile error during compilation with javac.
C:\DOCUME~1\schnee13\LOCALS~1\Temp\groovy-generated-1842144177968937199-java-source\SetterWithReturn.java:22:
setFoo(java.lang.String) is already defined in SetterWithReturn
public  SetterWithReturn setFoo(java.lang.String foo) { return (SetterWithReturn)null;}
                         ^
C:\DOCUME~1\schnee13\LOCALS~1\Temp\groovy-generated-1842144177968937199-java-source\SetterWithReturn.java:23:
setBar(java.lang.String) is already defined in SetterWithReturn
public  SetterWithReturn setBar(java.lang.String bar) { return (SetterWithReturn)null;}
                         ^
2 errors


1 error

{code}",paulk,leo.herbie,Major,Closed,Fixed,27/Jan/11 21:54,21/Jul/11 19:06
Bug,GROOVY-4648,12812025,Pasting into groovy console using shift+insert turns overwrite mode on.,"On windows there are two keyboard shortcut keys for pasting:

Ctrl+V
Shift+Insert

When pasting using the Shift+Insert into the groovy console script area text-overwrite mode is accidentally enabled by the code.

This doesn't happen when using Ctrl+V, I think there may be a bug in the keyboard handling code.

As a side note I'm left handed and thus often use shift+insert to paste so I leave one hand on the mouse (and i don't have to stretch my fingers as far).

Not had the chance to test with the latest version of the console, if it's already fixed please close this bug report.",paulk,dclifton,Minor,Closed,Fixed,28/Jan/11 04:02,22/Dec/12 01:10
Bug,GROOVY-4649,12815504,static inner classes are not being compiled correctly,"The groovy compiler used by eclipse & intellij are both having trouble compiling static inner classes.  The groovyc compiler does not fail but it appears to generate incorrect bytecode.  I filed a issue with intellij and they said that this is a groovy compiler problem not and intellij problem.

The following code triggers the problem:

{code}
//groovy code
class Outer {
  static class Inner {}
}
{code}

{code}
//java code
public class Client {
  { new Outer.Inner(); }
}
{code}

This is a serious issue because without any tool support we cannot use static inner classes.  The only work around for this issue is to not use static inner classes.

Here are the following issues related to this:

http://jira.codehaus.org/browse/GRECLIPSE-983
http://youtrack.jetbrains.net/issue/IDEA-64752
http://youtrack.jetbrains.net/issue/IDEA-50708

I have not looked in detail at the bytecode generated by the groovy compiler but decompiling the class files with JD Decompiler gives the following:
{code}
//notice no reference to the Inner class.  Decompiling a java compiled class like this will show the Inner class in the Outer class.
public class Outer
  implements GroovyObject
{
  public Outer()
  {
    Outer this;
    CallSite[] arrayOfCallSite = $getCallSiteArray();
    MetaClass localMetaClass = $getStaticMetaClass();
    this.metaClass = localMetaClass;
  }

  static
  {
    Long localLong1 = (Long)DefaultTypeTransformation.box(0L);
    __timeStamp__239_neverHappen1296229343062 = DefaultTypeTransformation.longUnbox(localLong1);
    Long localLong2 = (Long)DefaultTypeTransformation.box(1296229343062L);
    __timeStamp = DefaultTypeTransformation.longUnbox(localLong2);
  }
}
{code}
",blackdrag,leo.herbie,Blocker,Closed,Fixed,28/Jan/11 10:00,17/Feb/11 15:52
Bug,GROOVY-4650,12815454,stub generation fails with generic signatures,"The stub generation fails with the following classes.  It appears it may be because of the generic signatures.

{code}
//groovy code
public interface ColContract {
  List<? extends PrimitiveContract> getPrims();
  String getThing();
  List<String> getMoreThings();
}

public interface PrimitiveContract {
  String getFoo();
  String getBar();
}
{code}

{code}
import java.util.List;

public class Client3 {
    { new ColContract() {
        @Override
        public List<? extends PrimitiveContract> getPrims() {
            return null;
        }

        @Override
        public String getThing() {
            return null;
        }

        @Override
        public List<String> getMoreThings() {
            return null;
        }
    }; }
}
{code}",paulk,leo.herbie,Major,Closed,Fixed,28/Jan/11 12:12,21/Jul/11 19:06
Bug,GROOVY-4655,12815508,groovy.transform.EqualsAndHashCode annotation does not handle cycles,"The groovy.transform.EqualsAndHashCode AST transformation does not handle object graphs with cycles. See the attached junit tests.

Some of the test methods I included actually pass - I just included them for completeness.  Basically, the equals method handles cycles just fine.  The hashcode method does not so all the tests related to the hashcode method fail.

This is a pretty serious issue because these object structures are common with some frameworks (like orm tools)",paulk,leo.herbie,Major,Closed,Fixed,01/Feb/11 10:32,12/Apr/13 16:55
Bug,GROOVY-4657,12815507,"Regression: Null element coerced to ""null"" String when containing List is coerced to String array","From http://groovy.329449.n5.nabble.com/Change-in-contains-behavior-in-1-7-6-vs-1-7-5-td3367327.html:

Groovy 1.7.5:

{code}
def x = [null] as String[]
println x[0] == null  // true
println x[0]?.length() // null
{code}

Groovy 1.7.6:

{code}
def x = [null] as String[]
println x[0] == null  // false
println x[0]?.length() // 4
{code}",blackdrag,pniederw,Blocker,Closed,Fixed,01/Feb/11 23:39,03/Feb/11 10:51
Bug,GROOVY-4658,12815488,CLONE -problem compiling @Delegate to an interface that extends another interface (handle default args case),"The following code compiles with 1.7.3 but fails with 1.7.4, 1.7.5 and 1.8.0-beta-3.

{code:title=SomeInterface.groovy|borderStyle=solid}
interface SomeInterface {
    void doSomething()
}
{code}

{code:title=SomeOtherInterface.groovy|borderStyle=solid}
interface SomeOtherInterface extends SomeInterface {}
{code}

{code:title=SomeClass.groovy|borderStyle=solid}
class SomeClass {
    @Delegate
    SomeOtherInterface someOtherInterface
}
{code}

Compiling with 1.7.3:

{noformat}
 $ groovy -version
Groovy Version: 1.7.3 JVM: 1.6.0_22
 $ groovyc SomeClass.groovy 
 $ 
{noformat}

Compiling with 1.7.4:

{noformat}
 $ groovy -version
Groovy Version: 1.7.4 JVM: 1.6.0_22
 $ groovyc SomeClass.groovy 
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
SomeClass.groovy: 1: Can't have an abstract method in a non-abstract class. The class 'SomeClass' must be declared abstract or the method 'void doSomething()' must be implemented.
 @ line 1, column 1.
   class SomeClass {
   ^

1 error

 $ 
{noformat}

Compiling with 1.7.5:

{noformat}
 $ groovy -version
Groovy Version: 1.7.5 JVM: 1.6.0_22
 $ groovyc SomeClass.groovy 
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
SomeClass.groovy: 1: Can't have an abstract method in a non-abstract class. The class 'SomeClass' must be declared abstract or the method 'void doSomething()' must be implemented.
 @ line 1, column 1.
   class SomeClass {
   ^

1 error

 $ 
{noformat}

Compiling with 1.8.0-beta-3:

{noformat}
 $ groovy -version
Groovy Version: 1.8.0-beta-3 JVM: 1.6.0_22
 $ groovyc SomeClass.groovy 
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
SomeClass.groovy: 1: Can't have an abstract method in a non-abstract class. The class 'SomeClass' must be declared abstract or the method 'void doSomething()' must be implemented.
 @ line 1, column 1.
   class SomeClass {
   ^

1 error

 $ 
{noformat}

This may be related to GROOVY-4163.",blackdrag,brownj,Major,Closed,Fixed,02/Feb/11 06:29,08/Nov/11 08:16
Bug,GROOVY-4660,12817269,NPE in groovy.servlet.AbstractHttpServlet if deploy WAR to jetty ,"I have generated a (non-grails) war using gsp's mapped to groovy.servlet.TemplateServlet. If I deploy the War to Tomcat it works. If I _unpack_ the war then it works in Jetty. But it fails with an NPE if I try to deploy it as a War with Jetty:

2011-02-04 11:04:06.030:INFO:data-pump-comparator-04.04.01-dev:GroovyTemplate: Servlet groovy.servlet.TemplateServlet initialized on
 class groovy.text.GStringTemplateEngine
2011-02-04 11:04:06.030:WARN::/data-pump-comparator-04.04.01-dev/
java.lang.NullPointerException
        at java.io.File.<init>(File.java:222)
        at groovy.servlet.AbstractHttpServlet.getScriptUriAsFile(AbstractHttpServlet.java:306)
        at groovy.servlet.TemplateServlet.service(TemplateServlet.java:388)
        at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
        at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:533)
        at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:475)
        at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:119)

I think this is down to a problm in groovy.servlet.AbstractHttpServlet

TemplateServlet.service does this:

    File file = super.getScriptUriAsFile(request);

AbstractHttpServlet.getScriptUriAsFile does this:
    String uri = getScriptUri(request);
    String real = servletContext.getRealPath(uri);
    return new File(real).getAbsoluteFile();

The Javadocs for javax.servlet.ServletContext.getRealPath say:

     * This method returns null
     * if the servlet container cannot translate the virtual path
     * to a real path for any reason (such as when the content is
     * being made available from a .war archive).

I think that is why the failure happens. AFAIK it also happens with WebLogic (http://groovy.329449.n5.nabble.com/Groovlet-GSP-NPE-when-deployed-as-a-war-to-Weblogic-td3320802.html)",guillaume,paulcager,Minor,Closed,Fixed,04/Feb/11 06:19,12/Feb/12 04:03
Bug,GROOVY-4661,12815482,List#getAt(Range) method doesn't work with half-exclusive ranges,"def list = [1, 2, 3, 4]

println list[0..<1]    // [1], correct
println list[0..<2]    // [1, 2], correct
println list[0..<-1]   // [1], should be [1, 2, 3]
println list[0..<-2]   // [1, 2, 3, 4], should be [1, 2]

While this particular behavior is not covered in the Groovy JDK documentation it is intuitive to believe that indexing by such half-exclusive ranges should work just fine. But it doesn't.",,os,Minor,Closed,Fixed,04/Feb/11 08:46,14/Oct/13 16:53
Bug,GROOVY-4663,12815490,Groovy Console shows both full and sanitized stack trace by default,"Groovy Console with default settings (as far as I can tell; don't know how to reset them) shows both full and sanitized stack trace:

{noformat}
groovy> throw new Exception() 
 
Exception thrown
Feb 6, 2011 2:39:14 PM org.codehaus.groovy.runtime.StackTraceUtils sanitize
WARNING: Sanitizing stacktrace:
java.lang.Exception
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.codehaus.groovy.reflection.CachedConstructor.invoke(CachedConstructor.java:77)
	at org.codehaus.groovy.runtime.callsite.ConstructorSite$ConstructorSiteNoUnwrapNoCoerce.callConstructor(ConstructorSite.java:102)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:52)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:190)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:194)
	at ConsoleScript1.run(ConsoleScript1:1)
	at groovy.lang.GroovyShell.runScriptOrMainOrTestOrRunnable(GroovyShell.java:266)
	at groovy.lang.GroovyShell.run(GroovyShell.java:517)
	at groovy.lang.GroovyShell.run(GroovyShell.java:172)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite$PogoCachedMethodSiteNoUnwrapNoCoerce.invoke(PogoMetaMethodSite.java:266)
	at org.codehaus.groovy.runtime.callsite.PogoMetaMethodSite.call(PogoMetaMethodSite.java:63)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:132)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy:927)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:149)
	at groovy.ui.Console$_runScriptImpl_closure17.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at groovy.lang.Closure.call(Closure.java:282)
	at groovy.lang.Closure.call(Closure.java:277)
	at groovy.lang.Closure.run(Closure.java:360)
	at java.lang.Thread.run(Thread.java:680)

java.lang.Exception
	at ConsoleScript1.run(ConsoleScript1:1)
groovy> throw new Exception() 
 
Exception thrown

java.lang.Exception
	at ConsoleScript2.run(ConsoleScript2:1)

{noformat}

Apparently this happens because log output of StackTraceUtils goes to standard err and Groovy Console shows standard err by default. Something should be changed so that only sanitized stack trace is shown by default.",asteingress,pniederw,Major,Closed,Fixed,06/Feb/11 08:58,10/Jul/13 04:42
Bug,GROOVY-4665,12817067,incorrect behavior of array subscript operator with reverse ranges,"reverse ranges like {{X..-Y}} are used in lists to represent forward ranges of the form {{X..list.size()-Y}}.
this is not the case with arrays, as these special cases return a list that match the reverse range.

this can easily be tested with this snippet
{code}
def list = [1,2,3,4]
int[] array = list
assert list[2..-1] == [3,4]
assert array[2..-1] == [3,4]
{code}

which currently yields
{code}
Assertion failed: 

assert array[2..-1] == [3,4]
       |    |       |
       |    |       false
       |    [3, 2, 1, 4]
       [1, 2, 3, 4]
{code}

we can track this to the protected DGM.primitiveArrayGet(Object,Range) that is called from DGM.getAt(int[],Range) and its siblings.

this method just iterates the range to generate the returned list, whereas the list equivalent DGM.getAt(List,Range) makes use of DGMS.subListBorders(int,Range) to detect the reverse ranges.
",,jpertino,Major,Closed,Fixed,06/Feb/11 20:53,14/Oct/13 16:53
Bug,GROOVY-4667,12815483,for-loop type declaration does not work,"The type declaration of the iterator variable in a for-loop seems to be ignored:

{code}
List l = ['aaa'] // contains one String
try {
    for (Integer i in l) { // iterator declared Integer (not assignable from String!)
        // GroovyCastException expected in assignment to i in the previous line
        assert false // we should not get this far, expected GroovyCastException
    }
}
catch (org.codehaus.groovy.runtime.typehandling.GroovyCastException e) {
    assert true // expected exception
}
{code}
 
Exception thrown
07.02.2011 17:33:40 org.codehaus.groovy.runtime.StackTraceUtils sanitize

WARNUNG: Sanitizing stacktrace:

Assertion failed: 

assert false // we should not get this far, expected GroovyCastException

",blackdrag,joerg.schreiner,Minor,Closed,Fixed,07/Feb/11 10:40,07/Feb/11 11:30
Bug,GROOVY-4669,12815514,"""anystring"" as byte[] throws java.lang.NumberFormatException: For input string: ""anystring""","For some reason, since Groovy 1.7.5 I can no longer coerce a String (either "" or ') to a byte[].

I instead get java.lang.NumberFormatException: For input string: ""xxx""",blackdrag,wirah,Major,Closed,Fixed,08/Feb/11 07:30,08/Feb/11 10:21
Bug,GROOVY-4670,12815509,groovy.ui.Console cannot be extended,"A minimal extension of groovy.io.Console throws ""groovy.lang.MissingPropertyException: No such property: scriptRunning"" after successfully evaluating a script.

{code:title=GWLSTConsole.groovy|borderStyle=solid}
package com.middlewareman.mbean.weblogic.shell

class GWLSTConsole extends groovy.ui.Console {

	GWLSTConsole() {
		super()
	}

	GWLSTConsole(Binding binding) {
		super(binding)
	}

	GWLSTConsole(ClassLoader parent, Binding binding) {
		super(parent, binding)
	}

	GWLSTConsole(ClassLoader parent) {
		super(parent)
	}
}
{code}

{noformat}
Exception in thread ""Thread-5"" 
groovy.lang.MissingPropertyException: No such property: scriptRunning for class: com.middlewareman.mbean.weblogic.shell.GWLSTConsole
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:49)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.setGroovyObjectProperty(ScriptBytecodeAdapter.java:533)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:904)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:44)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:149)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:88)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at groovy.lang.Closure.call(Closure.java:276)
	at groovy.lang.Closure.call(Closure.java:271)
	at groovy.lang.Closure.run(Closure.java:354)
	at java.lang.Thread.run(Unknown Source)
{noformat}

In general, it would be great if groovy.ui.Console would be more flexible for adapting to your own DSLs. In my case, for example, I would like to customise the title. It is currently hard-coded in two places, and extending the class would be one way of overriding these.",blackdrag,middlewareman,Minor,Closed,Fixed,08/Feb/11 13:19,12/Jan/13 20:40
Bug,GROOVY-4673,12815486,"java.lang.ClassFormatError: Illegal class name ""groovy/jmx/builder/package-info"" in class file groovy/jmx/builder/package-info","The groovy-all-1.7.7.jar file in the embeddable directory of the binary ZIP file contains an invalid package-info file groovy.jmx.builder.package-info.class. I believe this is due to the src/main/groovy/jmx/builder/package-info.groovy file being incorrectly compiled. In Tomcat 7.0.0 and JBoss AS 6.0.0, this results in an exception like the one below and a deployment failure for any applications that bundle this JAR file.

Tomcat has since released a patch that issues this is a warning and allows the deployment to continue, but JBoss AS has not. Since the problem is ultimately that a legitimately invalid package-info class exists in the library, it seems most logical that the problem be resolved with Groovy and not JBoss.

I was able to resolve my issue for now by manually editing the distributable JAR file and removing the package-info.class file from it, but this is not a good permanent solution.

Here is the exception:

{code:title=Exception}18:02:52,225 WARN  [org.jboss.detailed.classloader.ClassLoaderManager] Unexpected error during load of:groovy.jmx.builder.package-info: java.lang.ClassFormatError: Illegal class name ""groovy/jmx/builder/package-info"" in class file groovy/jmx/builder/package-info
	at java.lang.ClassLoader.defineClass1(Native Method) [:1.6.0_23]
	at java.lang.ClassLoader.defineClassCond(Unknown Source) [:1.6.0_23]
	at java.lang.ClassLoader.defineClass(Unknown Source) [:1.6.0_23]
	at org.jboss.classloader.spi.base.BaseClassLoader.access$200(BaseClassLoader.java:52) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.base.BaseClassLoader$2.run(BaseClassLoader.java:650) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.base.BaseClassLoader$2.run(BaseClassLoader.java:609) [jboss-classloader.jar:2.2.0.GA]
	at java.security.AccessController.doPrivileged(Native Method) [:1.6.0_23]
	at org.jboss.classloader.spi.base.BaseClassLoader.loadClassLocally(BaseClassLoader.java:608) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.base.BaseClassLoader.loadClassLocally(BaseClassLoader.java:585) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.base.BaseDelegateLoader.loadClass(BaseDelegateLoader.java:156) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.filter.FilteredDelegateLoader.doLoadClass(FilteredDelegateLoader.java:141) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.filter.FilteredDelegateLoader.loadClass(FilteredDelegateLoader.java:132) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.base.ClassLoadingTask$ThreadTask.run(ClassLoadingTask.java:461) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.base.ClassLoaderManager.nextTask(ClassLoaderManager.java:262) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.base.ClassLoaderManager.process(ClassLoaderManager.java:161) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.base.BaseClassLoaderDomain.loadClass(BaseClassLoaderDomain.java:260) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.base.BaseClassLoaderDomain.loadClass(BaseClassLoaderDomain.java:1152) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.base.BaseClassLoader.loadClassFromDomain(BaseClassLoader.java:886) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.base.BaseClassLoader.doLoadClass(BaseClassLoader.java:505) [jboss-classloader.jar:2.2.0.GA]
	at org.jboss.classloader.spi.base.BaseClassLoader.loadClass(BaseClassLoader.java:450) [jboss-classloader.jar:2.2.0.GA]
	at java.lang.ClassLoader.loadClass(Unknown Source) [:1.6.0_23]
	at java.lang.Class.forName0(Native Method) [:1.6.0_23]
	at java.lang.Class.forName(Unknown Source) [:1.6.0_23]
	at org.jboss.reflect.plugins.introspection.IntrospectionTypeInfoFactoryImpl.resolveComplexTypeInfo(IntrospectionTypeInfoFactoryImpl.java:458) [jboss-reflect.jar:2.2.0.GA]
	at org.jboss.reflect.plugins.introspection.IntrospectionTypeInfoFactoryImpl.getTypeInfo(IntrospectionTypeInfoFactoryImpl.java:414) [jboss-reflect.jar:2.2.0.GA]
	at org.jboss.reflect.plugins.introspection.IntrospectionTypeInfoFactory.getTypeInfo(IntrospectionTypeInfoFactory.java:54) [jboss-reflect.jar:2.2.0.GA]
	at org.jboss.config.plugins.AbstractConfiguration.getTypeInfo(AbstractConfiguration.java:121) [jboss-reflect.jar:2.2.0.GA]
	at org.jboss.kernel.plugins.config.AbstractKernelConfig.getTypeInfo(AbstractKernelConfig.java:95) [jboss-kernel.jar:2.2.0.GA]
	at org.jboss.kernel.plugins.config.AbstractKernelConfigurator.getTypeInfo(AbstractKernelConfigurator.java:102) [jboss-kernel.jar:2.2.0.GA]
	at org.jboss.scanning.plugins.visitor.ConfiguratorReflectProvider.getTypeInfo(ConfiguratorReflectProvider.java:47) [:1.0.0.GA]
	at org.jboss.scanning.plugins.visitor.CachingReflectProvider.getTypeInfo(CachingReflectProvider.java:52) [:1.0.0.GA]
	at org.jboss.scanning.plugins.visitor.ReflectResourceVisitor.getTypeInfo(ReflectResourceVisitor.java:60) [:1.0.0.GA]
	at org.jboss.scanning.plugins.visitor.ReflectResourceVisitor.getClassInfo(ReflectResourceVisitor.java:72) [:1.0.0.GA]
	at org.jboss.scanning.plugins.visitor.ReflectResourceVisitor.doVisit(ReflectResourceVisitor.java:107) [:1.0.0.GA]
	at org.jboss.scanning.plugins.visitor.ReflectResourceVisitor.visit(ReflectResourceVisitor.java:86) [:1.0.0.GA]
	at org.jboss.scanning.hierarchy.plugins.HierarchyIndexScanningPlugin.visit(HierarchyIndexScanningPlugin.java:91) [:1.0.0.GA]
	at org.jboss.scanning.spi.helpers.ScanningPluginWrapper.visit(ScanningPluginWrapper.java:112) [:1.0.0.GA]
	at org.jboss.classloading.plugins.visitor.FederatedResourceVisitor.visit(FederatedResourceVisitor.java:101) [jboss-classloading.jar:2.2.0.GA]
	at org.jboss.classloading.plugins.vfs.VFSResourceVisitor.visit(VFSResourceVisitor.java:264) [jboss-classloading-vfs.jar:2.2.0.GA]
	at org.jboss.vfs.VirtualFile.visit(VirtualFile.java:408) [jboss-vfs.jar:3.0.0.GA]
	at org.jboss.vfs.VirtualFile.visit(VirtualFile.java:410) [jboss-vfs.jar:3.0.0.GA]
	at org.jboss.vfs.VirtualFile.visit(VirtualFile.java:410) [jboss-vfs.jar:3.0.0.GA]
	at org.jboss.vfs.VirtualFile.visit(VirtualFile.java:410) [jboss-vfs.jar:3.0.0.GA]
	at org.jboss.vfs.VirtualFile.visit(VirtualFile.java:396) [jboss-vfs.jar:3.0.0.GA]
	at org.jboss.classloading.plugins.vfs.VFSResourceVisitor.visit(VFSResourceVisitor.java:102) [jboss-classloading-vfs.jar:2.2.0.GA]
	at org.jboss.deployers.vfs.plugins.classloader.VFSDeploymentClassLoaderPolicyModule.visit(VFSDeploymentClassLoaderPolicyModule.java:181) [:2.2.0.GA]
	at org.jboss.scanning.plugins.DeploymentUnitScanner.scan(DeploymentUnitScanner.java:111) [:1.0.0.GA]
	at org.jboss.scanning.spi.helpers.UrlScanner.scan(UrlScanner.java:96) [:1.0.0.GA]
	at org.jboss.scanning.deployers.ScanningDeployer.deploy(ScanningDeployer.java:95) [:1.0.0.GA]
	at org.jboss.deployers.plugins.deployers.DeployerWrapper.deploy(DeployerWrapper.java:179) [:2.2.0.GA]
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doDeploy(DeployersImpl.java:1832) [:2.2.0.GA]
	at org.jboss.deployers.plugins.deployers.DeployersImpl.doInstallParentFirst(DeployersImpl.java:1550) [:2.2.0.GA]
	at org.jboss.deployers.plugins.deployers.DeployersImpl.install(DeployersImpl.java:1491) [:2.2.0.GA]
	at org.jboss.dependency.plugins.AbstractControllerContext.install(AbstractControllerContext.java:379) [jboss-dependency.jar:2.2.0.GA]
	at org.jboss.dependency.plugins.AbstractController.install(AbstractController.java:2044) [jboss-dependency.jar:2.2.0.GA]
	at org.jboss.dependency.plugins.AbstractController.incrementState(AbstractController.java:1083) [jboss-dependency.jar:2.2.0.GA]
	at org.jboss.dependency.plugins.AbstractController.executeOrIncrementStateDirectly(AbstractController.java:1322) [jboss-dependency.jar:2.2.0.GA]
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1246) [jboss-dependency.jar:2.2.0.GA]
	at org.jboss.dependency.plugins.AbstractController.resolveContexts(AbstractController.java:1139) [jboss-dependency.jar:2.2.0.GA]
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:939) [jboss-dependency.jar:2.2.0.GA]
	at org.jboss.dependency.plugins.AbstractController.change(AbstractController.java:654) [jboss-dependency.jar:2.2.0.GA]
	at org.jboss.deployers.plugins.deployers.DeployersImpl.change(DeployersImpl.java:1983) [:2.2.0.GA]
	at org.jboss.deployers.plugins.deployers.DeployersImpl.process(DeployersImpl.java:1076) [:2.2.0.GA]
	at org.jboss.deployers.plugins.main.MainDeployerImpl.process(MainDeployerImpl.java:679) [:2.2.0.GA]
	at org.jboss.deployment.MainDeployer.deploy(MainDeployer.java:380) [:6.0.0.Final]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) [:1.6.0_23]
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) [:1.6.0_23]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) [:1.6.0_23]
	at java.lang.reflect.Method.invoke(Unknown Source) [:1.6.0_23]
	at org.jboss.mx.interceptor.ReflectedDispatcher.invoke(ReflectedDispatcher.java:157) [:6.0.0.GA]
	at org.jboss.mx.server.Invocation.dispatch(Invocation.java:96) [:6.0.0.GA]
	at org.jboss.mx.server.Invocation.invoke(Invocation.java:88) [:6.0.0.GA]
	at org.jboss.mx.server.AbstractMBeanInvoker.invoke(AbstractMBeanInvoker.java:271) [:6.0.0.GA]
	at org.jboss.mx.server.MBeanServerImpl.invoke(MBeanServerImpl.java:670) [:6.0.0.GA]
	at org.jboss.system.server.jmx.MBeanServerWrapper.invoke(MBeanServerWrapper.java:138) [:6.0.0.Final (Build SVNTag:JBoss_6.0.0.Final date: 20101228)]
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source) [:1.6.0_23]
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source) [:1.6.0_23]
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source) [:1.6.0_23]
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source) [:1.6.0_23]
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source) [:1.6.0_23]
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) [:1.6.0_23]
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source) [:1.6.0_23]
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source) [:1.6.0_23]
	at java.lang.reflect.Method.invoke(Unknown Source) [:1.6.0_23]
	at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source) [:1.6.0_23]
	at sun.rmi.transport.Transport$1.run(Unknown Source) [:1.6.0_23]
	at java.security.AccessController.doPrivileged(Native Method) [:1.6.0_23]
	at sun.rmi.transport.Transport.serviceCall(Unknown Source) [:1.6.0_23]
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source) [:1.6.0_23]
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source) [:1.6.0_23]
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source) [:1.6.0_23]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source) [:1.6.0_23]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source) [:1.6.0_23]
	at java.lang.Thread.run(Unknown Source) [:1.6.0_23]{code}",paulk,beamerblvd,Major,Closed,Fixed,10/Feb/11 20:35,21/Jul/11 19:06
Bug,GROOVY-4678,12815348,high number of generated classes,"Due to a change of the call site meta method logic Groovy now generates an endless number of call site meta method classes, one for each time. The reason for this is one the changed conctructor and second a bytecode error.",blackdrag,blackdrag,Major,Closed,Fixed,15/Feb/11 14:12,15/Feb/11 14:13
Bug,GROOVY-4686,12815489,Varargs are not displayed by GroovyDoc,"This file http://goo.gl/9K3p0 uses varargs arguments.
But its Groovydoc generated http://goo.gl/gKYgH doesn't display them.",paulk,genie,Major,Closed,Fixed,18/Feb/11 15:39,21/Jul/11 19:06
Bug,GROOVY-4691,12815505,Grails 1.3.7 breaks bean-fields plugin,"After upgrading from grails 1.3.6 to 1.3.7 bean-fields plugin render garbage after every <bean:field> tag:
{noformat}
org.codehaus.groovy.grails.web.pages.GroovyPageOutputStack$GroovyPageProxyWriter@602d9d
{noformat}

See http://grails.1312388.n4.nabble.com/Grails-1-3-7-and-bean-fields-td3311784.html

The sample app attached show the problem in view book/index",guillaume,goeh,Major,Closed,Fixed,22/Feb/11 09:28,29/Mar/11 10:49
Bug,GROOVY-4695,12815538,anonymous classes InnerClassNode has -1 for line numbers,"anonymous classes InnerClassNode has -1 for line numbers

new Object() {}

produces an InnerCLassNode. The lineNUmbers are -1

This affects CodeNarc. I'd love to see this rolled back as far as possible. To 1.8 surely. ",hamletdrc,hamletdrc,Major,Closed,Fixed,25/Feb/11 02:33,12/Mar/11 00:48
Bug,GROOVY-4698,12815499,Possible memory leak in Tomcat,"Using Groovy 1.7.8 Groovlets on Tomcat 6.0.28 yields the following in the log files:

{code}
The web application [...] created a ThreadLocal with key of type [null] (value [groovy.util.GroovyScriptEngine$2@68aed52c]) and a value of type [org.codehaus.groovy.tools.gse.StringSetMap] (value [{}]) but failed to remove it when the web application was stopped. This is very likely to create a memory leak.
{code}",jwagenleitner,fxg,Major,Closed,Fixed,25/Feb/11 15:39,22/Feb/16 20:48
Bug,GROOVY-4699,12815530,Observable List misbehaves when using retainAll with closure,"List liste = new ObservableList<String>()


liste.add ""test""
liste.add ""test2""

List andereListe = new LinkedList<String>()


liste.retainAll { elem -> andereListe.contains(elem) }

assert liste.isEmpty()
____________________________________________

This results in:
____________________________________________
Exception in thread ""main"" Assertion failed: 

assert liste.isEmpty()
       |     |
       |     false
       [test]
____________________________________________

If the ObservableList is changed to LinkedList, the testcase works. ",paulk,sharrer,Critical,Closed,Fixed,28/Feb/11 08:07,07/Sep/11 14:13
Bug,GROOVY-4700,12815513,@Field variable can't be referred from closure,"attached code 
{code}
import groovy.transform.Field

@Field def xxx = 3

foo = {
  println xxx
}

foo()

{code}

generate following error:
{quote}
Caught: BUG! exception in phase 'class generation' in source unit '/Users/uehaj/tmp/bug.groovy' tried to get a variable with the name xxx as stack variable, but a variable with this name was not created
{quote}


",paulk,uehaj,Major,Closed,Fixed,28/Feb/11 08:34,13/Apr/11 14:33
Bug,GROOVY-4701,12817703,MissingMethodException#printStackTrace() throws exception if an arguments toString() method fails,"I just saw a somewhat irritating behaviour in MissingMethodException. It throws an exception if the toString() of one of the arguments of the method call fails. 

Code to reproduce the problem:

{code}
class BrokenToString{
  @Override
  public String toString() {
    throw new RuntimeException(""toString broken"")
  }
}

def exception = new MissingMethodException(""methodName"", getClass(), new BrokenToString())

exception.printStackTrace()//Throws Exception
{code}

I've provided two variants of a patch to solve the problem , but to be honest, I'm not completely happy with any of these two variants:
1) The patch for InvokerHelper.java changes a class that seems to be broadly used and I'm not sure if that change is acceptable for all clients
2) The patch for MissingMethod Exception solves the problem by returning an Error String for the whole list of arguments instead of the one argument that caused the problem. But of course this behaviour is better than throwing an Exception:)
",paulk,sieber,Major,Closed,Fixed,02/Mar/11 06:50,10/Jul/13 04:42
Bug,GROOVY-4704,12817713,Error with null parameter in prepared statements with old DB2 driver,"The DB2 JDBC driver throws an exception when setting null as parameter for a prepared statement.

Stack trace (line numbers are wrong):
{code}
Caused by: com.ibm.db2.jcc.c.SqlException: Invalid data conversion: Parameter object type is invalid for requested conversion.
        at com.ibm.db2.jcc.c.uf.setObject(uf.java:1215)
        at groovy.sql.Sql.setObject(Sql.java:2124)
        at groovy.sql.Sql.setParameters(Sql.java:2090)
        at groovy.sql.Sql.getPreparedStatement(Sql.java:2318)
        at groovy.sql.Sql.getPreparedStatement(Sql.java:2324)
        at groovy.sql.Sql.execute(Sql.java:1113)
{code}
Please use PreparedStatement.setNull() instead of setObject() in the last line of Method groovy.sql.Sql.setObject() when value is null.

Also see JavaDoc comment for setObject() about null values.

http://download.oracle.com/javase/6/docs/api/java/sql/PreparedStatement.html#setNull(int, int)
http://download.oracle.com/javase/6/docs/api/java/sql/PreparedStatement.html#setObject(int,%20java.lang.Object)

Thank you!",paulk,kaklakariada,Minor,Closed,Fixed,03/Mar/11 02:41,12/Mar/11 00:47
Bug,GROOVY-4707,12815516,Scripts' class nodes didn't have correct source positions,,guillaume,guillaume,Minor,Closed,Fixed,03/Mar/11 08:48,12/Mar/11 00:47
Bug,GROOVY-4709,12811929,@Interrupt AST transformations should not be applied on abstract methods,"When a class is annotated with @ThreadInterrupt, @ConditionalInterrupt or @TimedInterrupt, a NullPointerException is thrown when visiting an abstract method.

{code}
java.lang.NullPointerException
    at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
    at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:165)
    at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
    at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
    at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
    at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
    at org.codehaus.groovy.transform.ThreadInterruptibleASTTransformation.super$3$visitMethod(ThreadInterruptibleASTTransformation.groovy)
    at sun.reflect.GeneratedMethodAccessor184.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
    at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
    at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1054)
    at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.invokeMethodOnSuperN(ScriptBytecodeAdapter.java:128)
    at org.codehaus.groovy.transform.ThreadInterruptibleASTTransformation.visitMethod(ThreadInterruptibleASTTransformation.groovy:73)
    at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
    at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
    at org.codehaus.groovy.transform.AbstractInterruptibleASTTransformation.visit(AbstractInterruptibleASTTransformation.java:89)
    at org.codehaus.groovy.transform.ASTTransformationVisitor.visitClass(ASTTransformationVisitor.java:129)
    at org.codehaus.groovy.transform.ASTTransformationVisitor$2.call(ASTTransformationVisitor.java:172)
    at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:948)
    at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:533)
    at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:511)
    at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:488)
    at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:283)
    at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:264)
{code}
",guillaume,melix,Major,Closed,Fixed,04/Mar/11 14:43,05/Apr/15 14:43
Bug,GROOVY-4710,12815532,Creating anonymous class instance fails with some legal code structures,"Attached are three small code examples.  One shows that Groovy captures variables fine when using anonymous class creation using new when the expression is part of an assignment.  The second shows that using an as expression also enable proper capture of variables when an expression is the return value used in a collect expression.  The third shows the failure of Groovy to ""do the right thing"" in the case of using a new expression in a collect expression.  The error returned is:

 Caught: groovy.lang.GroovyRuntimeException: Could not find matching constructor for: fails$1(fails, java.lang.Integer)
	at fails$_run_closure1.doCall(fails.groovy:15)
	at fails.run(fails.groovy:14)

Current hypothesis is that the fact that the function is (necessarily) called call may be a factor.",,russel,Major,Closed,Fixed,05/Mar/11 08:47,24/Dec/11 03:08
Bug,GROOVY-4711,12815531,Standalone stub generation task fails resolving classes,"If you create a Java class (Foo) and a Groovy class (Bar extends Foo), and then run the stub generation Ant task (GenerateStubsTask), it'll fail because it now does all the phases up to semantic analysis which attempts to resolve all class references. AFAIU the stub generation is supposed to be run before Javac. So it should be prepared that some Java classes the Groovy code depends on aren't compiled yet.",blackdrag,gromopetr,Major,Closed,Fixed,06/Mar/11 04:55,05/Apr/15 14:44
Bug,GROOVY-4715,12817781,StreamingMarkupBuilder can produce invalid xml,"* From #GROOVY-4115, StreamingMarkupBuilder provides option for using double quotes around attributes. But if we use this option, it can output invalid xml:
** For example:
{code:java}
def builder = new StreamingMarkupBuilder(useDoubleQuotes: true);
builder.bind (
  {
    div(onmouseover:'foo(""some text"")')
  }
}
{code}
** Produce the following output, which is invalid.
{code:xml}
    <div onmouseout=""foo(""some text"")""></div>
{code}
** Another example: 
{code:java}
def builder = new StreamingMarkupBuilder(useDoubleQuotes: true);
builder.bind (
   {
     div(onmouseover:""foo('some text')"")
   }
}
{code}
** Produce the following output
{code:xml}
    <div onmouseout=""foo(&apos;some text&apos;)""></div>
{code}
** However, it should produce
{code:xml}
    <div onmouseout=""foo('some text')""></div>
{code}

* As far as we know, MarkupBuilder will escape the apostrophe if the value is for an attribute, as opposed to element content, and if the builder is configured to surround attribute values with single quotes.
** So if we use the MarkupBuilder to build the above html block: 
{code:java}
def builder = new MarkupBuilder();
builder.setDoubleQuotes(true)
...
{code}
** It will produce the similar xml as the above, except the apostrophe entity
{code:xml}
...
<div onmouseout= ""foo('some text')""></div>
...
{code}

* The single-quote should be displayed instead of the entity ( &amp;apos; ) in both of the builders when attributes are surrounded by double quotes. It appears that StreamingMarkupBuilder is not communicating *useDoubleQuotes* to StreamingMarkupWriter.  One solution we can think of is passing in *useDoubleQuotes* to the StreamingMarkupWriter from the StreamingMarkupBuilder in the constructor and changing StreamingMarkupWriter to use similar logic to MarkupBuilder when checking attribute value for quotes to escape.{code:java|title=StreamingMarkupBuilder.java}
public bind(closure) {
...
out = new StreamingMarkupWriter(out, enc, useDoubleQuotes)
...
}
{code}
{code:java|title=StreamingMarkupWriter}
public void write(final c) throws IOException {
   ...
   else if(c == '\'' && this.writingAttribute && !useDoubleQuotes) {
       this.writer.write(""&apos;"");
   }
   else if(c == '""' && this.writingAttribute && useDoubleQuotes) {
       this.writer.write(""&quot;"");
   }
   ...
}
{code}

Thanks
",paulk,ttrung.vo,Critical,Closed,Fixed,07/Mar/11 16:58,13/Apr/11 14:33
Bug,GROOVY-4716,12815544,Example code in http://groovy.codehaus.org/Groovy+JmxBuilder does not work,"Page http://groovy.codehaus.org/Groovy+JmxBuilder has some sample code for the JMX builder. In a couple of places ""writable"" is misspelt as ""writeable"" so when you copy-and-paste the code it doesn't work (attributes are read-only).",guillaume,paulcager,Minor,Closed,Fixed,08/Mar/11 05:34,14/Aug/13 03:57
Bug,GROOVY-4719,12815539,Groovy build fails in tests - test RedundantCastInStubTest fails,"1 error in UberTestCaseGroovySourceCodehausPackages:

-testAll:
    [mkdir] Created dir: C:\MIKEDATA\java\groovy\groovy-1.7.9\target\test-reports
    [junit] WARNING: multiple versions of ant detected in path for junit
    [junit]          jar:file:/C:/Program%20Filez/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/C:/MIKEDATA/java/groovy/groovy-1.7.9/target/lib/compile/ant-1.8.2.jar!/org/apache/tools/ant/Project.class
    [junit] Running UberTestCaseBugs
    [junit] Tests run: 420, Failures: 0, Errors: 0, Time elapsed: 16.653 sec
    [junit] Running UberTestCaseGroovySourceCodehausPackages
    [junit]
    [junit] Tests run: 461, Failures: 0, Errors: 1, Time elapsed: 85.02 sec
    [junit] Test UberTestCaseGroovySourceCodehausPackages FAILED
    [junit] Running UberTestCaseGroovySourceCodehausPackages_VM6
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.359 sec
    [junit] Running UberTestCaseGroovySourceRootPackage
    [junit] Tests run: 1016, Failures: 0, Errors: 0, Time elapsed: 51.935 sec
    [junit] Running UberTestCaseGroovySourceSubPackages
    [junit] Tests run: 1284, Failures: 0, Errors: 0, Time elapsed: 101.457 sec
    [junit] Running UberTestCaseGroovySourceSubPackages_VM6
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 0.265 sec
    [junit] Running UberTestCaseJavaSourceCodehausPackages
    [junit] Tests run: 610, Failures: 0, Errors: 0, Time elapsed: 42.786 sec
    [junit] Running UberTestCaseJavaSourceGroovyPackagesNonSecurity
    [junit] Tests run: 410, Failures: 0, Errors: 0, Time elapsed: 9.993 sec
    [junit] Running UberTestCaseJavaSourceGroovyPackagesSecurity
    [junit] Tests run: 16, Failures: 0, Errors: 0, Time elapsed: 10.766 sec
    [junit] Running UberTestCaseTCK
    [junit] Tests run: 231, Failures: 0, Errors: 0, Time elapsed: 6.88 sec

Error is on test RedundantCastInStubTest.
",guillaume,mikevines,Major,Closed,Fixed,10/Mar/11 02:41,10/Mar/11 06:02
Bug,GROOVY-4720,12817823,Method overriding with ExpandoMetaClass is partially broken,"There is a regression whereby you cannot override methods using ExpandoMetaClass.

The reason is that ClosureMetaMethod.createMethodList creates an anonymous inner class of type MetaMethod and adds it to the returned List<MetaMethod> that are to be registered

Later the isNonRealMethod(MetaMethod) check in MetaMethodIndex does this check:

{code}
    private boolean isNonRealMethod(MetaMethod method) {
        return method instanceof NewInstanceMetaMethod ||
                method instanceof NewStaticMetaMethod ||
                method instanceof ClosureMetaMethod ||
                method instanceof GeneratedMetaMethod ||
                method instanceof ClosureStaticMetaMethod ||
                method instanceof MixinInstanceMetaMethod;
    }
{code}

Since the anonymous inner MetaMethod defined in ClosureMetaMethod is not an instance of any of these types then  the method is never registered in the MetaMethodIndex:

{code}

                    if (methodC == matchC) {
                        if (isNonRealMethod(method)) {
                            list.set(found, method);
                        }
{code}",guillaume,graemerocher,Critical,Closed,Fixed,11/Mar/11 04:57,13/Apr/11 14:33
Bug,GROOVY-4727,12815534,"Adding ""return"" automatically in nested switch statements","The following does not work. See: http://markmail.org/message/ekm45sghpzh227ga

{code}
assert foo('x1', 'y1') == 'r1'
def foo(x, y) {
    switch(x) {
        case 'x1':
            switch(y) {
                case 'y1':
                    'r1';
                    break;
                case 'y2':
                    'r2';
                    break;
            }
    }
}
{code}",emilles,roshandawrani,Major,Closed,Fixed,15/Mar/11 06:55,27/Aug/21 10:22
Bug,GROOVY-4729,12817888,Object.toString() called instead of coerced Map toString closure,"Sorry to open a new issue, this is very similar to http://jira.codehaus.org/browse/GROOVY-2801

In this instance, I'm mocking the toString method on an Abstract class SocketAddress.

channel = [
    getRemoteAddress: { [toString: remoteAddress] as SocketAddress },
    getLocalAddress: { [toString: localAddress] as SocketAddress }
] as Channel

and during the test, I receive a failure:


org.junit.ComparisonFailure: expected:<[10.10.2.20]> but was:<[SocketAddress_groovyProxy@5a425eb9]>
	at org.junit.Assert.assertEquals(Assert.java:123)
	at org.junit.Assert.assertEquals(Assert.java:145)
	at org.junit.Assert$assertEquals.callStatic(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallStatic(CallSiteArray.java:48)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:165)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callStatic(AbstractCallSite.java:177)


GROOVY-2801 was closed some time ago so I'm opening this new issue.",melix,travishume,Major,Closed,Fixed,16/Mar/11 16:05,14/Oct/11 00:28
Bug,GROOVY-4731,12815557,sample code in javadoc of Map#spreadMap() don't works,"At the GDK document of Map#toSpreadMap(),

http://groovy.codehaus.org/groovy-jdk/java/util/Map.html#toSpreadMap()

says:
{quote}
For examples, if there is defined a function like as

    def fn(a, b, c, d) { return a + b + c + d }
, then all of the following three have the same meaning.
    println fn(a:1, [b:2, c:3].toSpreadMap(), d:4)
    println fn(a:1, *:[b:2, c:3], d:4)
    println fn(a:1, b:2, c:3, d:4)
{quote}

I think there are at least 2 problems here.

1) fn should take Map parameter like:
{quote}
    def fn(Map m) \{ return m.a + m.b + m.c + m.d \}
{quote}

2) I changed it in 1) way but I get following error at the line calling toSpreadMap().
{quote}
Caught: groovy.lang.MissingMethodException: No signature of method: d.fn() is applicable for argument types: (java.util.LinkedHashMap, groovy.lang.SpreadMap) values: [[a:1, d:4], [:]]
Possible solutions: fn(java.util.Map), run(), run(), any(), run(java.io.File, [Ljava.lang.String;), any(groovy.lang.Closure)
	at d.run(d.groovy:3)
{quote}



",paulk,uehaj,Minor,Closed,Fixed,17/Mar/11 08:52,13/Apr/11 14:33
Bug,GROOVY-4732,12817905,Java5 configureClassNode does not take care of parameter annotations,"whenever annotation parameters are specified in byte-code, the Java5 vm-plugin does not include those annotations as AnnotationNode instances when configuring a ClassNode for that particular class.

the following code is from a local test-case: 

{code}
void testParameterAnnotation() { 

        GroovyClassLoader gcl = new GroovyClassLoader() 

        gcl.parseClass """""" 
            import java.lang.annotation.* 

            @Target(ElementType.METHOD) 
            @Retention(RetentionPolicy.RUNTIME) 
            @interface MethodAnnotation {} 

            @Target(ElementType.PARAMETER) 
            @Retention(RetentionPolicy.RUNTIME) 
            @interface ParameterAnnotation {} 

            interface MyInterface { 
                @MethodAnnotation 
                def method(@ParameterAnnotation def param) 
            } 
        """""" 

        GroovyCodeSource codeSource = new GroovyCodeSource("""""" 
            class MyInterfaceImpl implements MyInterface { 
                def method(def param) {} 
            } 
        """""", ""script"" + System.currentTimeMillis() + "".groovy"", ""/groovy/script"") 

        CompilationUnit cu = new CompilationUnit(CompilerConfiguration.DEFAULT, codeSource.codeSource, gcl) 
        cu.addSource(codeSource.getName(), codeSource.scriptText); 
        cu.compile(CompilePhase.FINALIZATION.phaseNumber) 

        def classNode = cu.getClassNode(""MyInterfaceImpl"") 
        def interfaceClassNode = classNode.getInterfaces().find { it.nameWithoutPackage == 'MyInterface' } 

        def methodNode = interfaceClassNode.getDeclaredMethods(""method"")[0] 

        // check if the AnnotationNode for 'MethodAnnotation' has been created 
        assert methodNode.getAnnotations().any { AnnotationNode an -> an.classNode.nameWithoutPackage == 'MethodAnnotation' } 
        
        // this one will fail, since parameter annotations are ignored by Java5Plugin (and above) 
        assert methodNode.getParameters()[0].getAnnotations().any { AnnotationNode an -> an.classNode.nameWithoutPackage == 'ParameterAnnotation' } 
}
{code} 

",blackdrag,andre.steingress,Major,Closed,Fixed,17/Mar/11 10:26,05/Apr/15 14:44
Bug,GROOVY-4734,12815562,Object initializer not being called on object construction,"Perhaps related to GROOVY-4733.

I would expect that this code prints ""99"", but it doesn't.  It prints ""9"":
{code}
class A {
        {
                print f
		}
        def f = 9 //
		def other = { }
		{
        	print f }
}
new A()
{code}

This code, however, prints ""99"":


{code}
class A {
        {
                print f
		}
		{
        	print f }
        def f = 9 //
		def other = { }
}
new A()
{code}
",,werdna,Critical,Closed,Fixed,17/Mar/11 18:50,13/Apr/11 14:33
Bug,GROOVY-4736,12815541,Deadlock when GroovyClassLoader is used in multi-threaded environment,"When multiple threads uses GroovyClassLoader to get Groovy classes or just work with them (instantiation, etc.) and an other(s) threads change the sources (followed by clearCache call), then deadlock can happen related to synchronizations on InnerLoader and HashMap (GroovyClassLoader.sourceCache)

I attached simple test scenation when 3 threads load Groovy classes and instantiate them and 3 other threads replace sources and call GroocyClassLoader.clearCache(). There is a synchronization on writeToFile to be ensure that the same file is not being written at the same time but this synchronization does not take part in a deadlock.

Please just run the test and wait a moment. Deadlock happens usually in 1 second at this test scenario. I hope this test will be also useful in Your future development as a standard test case.",blackdrag,skuklewicz,Blocker,Closed,Fixed,19/Mar/11 07:51,04/Mar/14 16:31
Bug,GROOVY-4741,12818068,VerifyError: Expecting to find integer on stack,"Hi, I have the same issue as in GROOVY-4587.

For the inputs 
{code}
[0.0], [], [], [543]
{code}
the function-code below does throw an Exception which can be fixed by changing
{code}
if (ReWK < 0) {
{code}
to 
{code}
if (((int)ReWK) < 0) {
{code}
Seems like a bug to me or am I doing it wrong? Various other inputs run fine.




{code}
Gesamtbetrag,Direktabzug,Prozent,Einzelbetrag->                                                               

def int ReWK = 0
def arrAbzWK = []; 

Gesamtbetrag.eachWithIndex {
                                                            
	it, index ->                                                                                                          
   
	if(Gesamtbetrag[index] > 0){ 

		                                               
        	if(Prozent[index] != 0 && Prozent[index] != null) {  
			ReWK = Math.ceil( Gesamtbetrag[index] * (100 - Prozent[index]) / 100 ) 

		} else if (Direktabzug[index] > 0 ) {           
         
			ReWK = Math.ceil( Gesamtbetrag[index] - Direktabzug[index] )    
         
			if (ReWK < 0) { 
				ReWK = 0
			}                                     

         	} 
                else if (Einzelbetrag[index] > 0) { 
			ReWK = Math.ceil( Einzelbetrag[index] )                                                                           		
		} else {			
			ReWK = 0	
		}                                                                                     
   	}                                                                                  
                                                                                                           
   	arrAbzWK.add(ReWK)    
                                                                               
}

return arrAbzWK 
{code}",blackdrag,dhansmann,Critical,Closed,Fixed,22/Mar/11 07:07,07/Apr/11 13:42
Bug,GROOVY-4742,12815579,Documentation for Collection.sort(Comparator comparator) is wrong,"The documentation for {{Collection.sort(Comparator comparator)}} at http://groovy.codehaus.org/groovy-jdk/java/util/Collection.html#sort%28java.util.Comparator%29 states

{quote}
Sorts the Collection using the given comparator. The elements are sorted into a new list, and the existing collection is unchanged. 
{quote}

However, it does seem to mutate the original list :-(",paulk,tim_yates,Major,Closed,Fixed,22/Mar/11 10:55,12/Apr/13 16:55
Bug,GROOVY-4743,12815559,Inner classes of inner classes can't find outer class method names.,"Jay Ashworth reported a problem that arose between 1.7.6 and 1.7.7.  His example used Swing events and requires manual clicking to use.  I've reduced it to a simple test case (although not in JUnit form).

The interesting thing is that it takes two levels of Runnable inner classes for the problem to show up.  Turns out threads are not part of the issue.

Demo script attached.  The output for Groovy 1.7.10 is:

Resolved
After
Resolved
Caught: java.lang.NoSuchMethodError: TestInnerRefs$2.this$dist$invoke$2(Ljava/lang/String;Ljava/lang/Object;)Ljava/lang/Object;
	at TestInnerRefs$2$3.methodMissing(TestRunner.groovy)
	at TestInnerRefs$2$3.run(TestRunner.groovy:24)
	at TestInnerRefs$2.run(TestRunner.groovy:27)
	at TestInnerRefs.dontEvent(TestRunner.groovy:31)
	at TestRunner.run(TestRunner.groovy:49)
",blackdrag,jimwhite,Major,Closed,Fixed,23/Mar/11 02:16,12/Apr/11 08:27
Bug,GROOVY-4745,12815578,ClassNode.equals throws ClassCastException for object of other type,"ClassNode.equals(Object o) should check for different types and return false. Current Code:
{code}
 public boolean equals(Object o) {
        if (redirect!=null) return redirect().equals(o);
        ClassNode cn = (ClassNode) o;
        return (cn.getName().equals(getName()));
    }
{code}
is obviously wrong.",paulk,johanneslink,Major,Closed,Fixed,24/Mar/11 08:44,21/Jul/11 19:06
Bug,GROOVY-4746,12815581,broken @link in MetaObjectProtocol.java's javadoc,@link,paulk,uehaj,Trivial,Closed,Fixed,24/Mar/11 17:02,13/Apr/11 14:33
Bug,GROOVY-4747,12817707,ClosureWriter incorrectly changes the accessedVariable field of a VariableExpression,"Since 1.8, there is a ClosureWriter class, which (among other things) corrects the accessed variable field of VariableExpressions inside of closures.  The method {{correctAccessedVariable}} looks at each VariableExpression and tries to map the variable name to a field name inside of a synthetic Closure class generated by the ClosureWriter.  If no such variable name is found, then the accessed variable field is set to null.

This last part is incorrect.  The accessed variable should not be set to null.  The accessed variable may point to a field in the class declaring the closure, or it may point to a closure parameter, or a local variable, etc.  

I am not sure if this has any runtime effect on Groovy code, but it does have an effect on Groovy-Eclipse.  Because of this problem. Field references inside of closures cannot be resolved.  This affects content assist, navigation, hovers, search, refactoring (etc).

A simple null check fixes the problem in Groovy-Eclipse.  Here is the change (from 1.8-rc2):

On line 288, change:
{code}
expression.setAccessedVariable(fn);
{code}

to:
{code}
if (fn != null) // only overwrite if we find something more specific
    expression.setAccessedVariable(fn);
{code}

I'll think about some good test cases for this.",blackdrag,werdna,Major,Closed,Fixed,25/Mar/11 13:30,08/Apr/11 05:29
Bug,GROOVY-4748,12815540,javadoc comment problems which breaks generated HTMLs,"I found some problems:
 - broken links
 - lack of period(summery is broken)
 - unbalanced tags like <pre>..<pre>",paulk,uehaj,Trivial,Closed,Fixed,26/Mar/11 08:31,05/Apr/15 14:44
Bug,GROOVY-4751,12815526,Defensive copying doesn't work for @Immutable classes,"Seems like @Immutable classes do not do defensive copying of the collection provided into constructor:
{code}
@Immutable
final class Person {
    String name
    List<String> address
}

def myAddress = [""Line1"",""Line2""]
def person = new Person(name:""Name"", address:myAddress)
assert person.address == [""Line1"",""Line2""]
myAddress << ""Line3""
assert myAddress == [""Line1"",""Line2"",""Line3""]
assert person.address == [""Line1"",""Line2""] //<- fails
{code}",paulk,denis.solonenko,Major,Closed,Fixed,28/Mar/11 21:23,07/Apr/15 19:12
Bug,GROOVY-4753,12815563,CliBuilder#expandArgumentFiles throws StringIndexOutOfBoundsException on empty arguments,"Pass a CliBuilder instance, in its default state with expandArugmentFiles = true, an argument array containing an empty argument ''.

expected:  the clibuilder to parse the arguments
actual: {code}java.lang.StringIndexOutOfBoundsException: String index out of range: 1
	at java.lang.String.substring(String.java:1934)
        ....
	at groovy.util.CliBuilder.expandArgumentFiles(CliBuilder.groovy:296)
        ....{code}

Attached is code showing how an argument array that parses when expandArgumentFiles is false, throws an exception when it is true.",paulk,jwadamson,Major,Closed,Fixed,30/Mar/11 08:48,05/Apr/15 14:44
Bug,GROOVY-4754,12815550,Error in implements statement,"Following code prints 1 or 2 from time to time I start it. Tried at http://groovyconsole.appspot.com. This code generates error there:
{noformat}
java.lang.NoSuchFieldError: __$stMC
	at I1( or I2!!!!!!!!!!!!!!!!!!!!).<clinit>(Script1.groovy)
	at Script1.run(Script1.groovy:9)
{noformat}
but error differs from time to time too.

{code}
interface I1{
   int VALUE = 1
}
interface I2{
   int VALUE = 2
}
class C implements I1, I2{
}
println(new C().VALUE)
{code}
",,guai,Major,Closed,Fixed,02/Apr/11 06:55,28/Dec/14 08:07
Bug,GROOVY-4756,12815601,wrong usage of _$st in interfaces,"Following code prints 1 or 2 from time to time I start it. Tried at http://groovyconsole.appspot.com. This code generates error there:
java.lang.NoSuchFieldError: __$stMC
	at I1( or I2!!!!!!!!!!!!!!!!!!!!).<clinit>(Script1.groovy)
	at Script1.run(Script1.groovy:9)
but error differs from time to time too.


interface I1{
   int VALUE = 1
}
interface I2{
   int VALUE = 2
}
class C implements I1, I2{
}
println(new C().VALUE)",blackdrag,guai,Blocker,Closed,Fixed,02/Apr/11 10:33,07/Apr/11 10:54
Bug,GROOVY-4757,12815603,Can't compile method with generic signatures and default visibiliy access,"The following script results in a compilation error

{code}
class Foo {
    <T extends Object> void foo(T t) {}
}

f = new Foo()
// compiler output
1 compilation error:

unexpected token: < at line: 2, column: 5
{code}

Adding a visibility accessor makes it work

{code}
class Foo {
    public <T extends Object> void foo(T t) {}
}

f = new Foo()
{code}",daniel_sun,aalmiray,Major,Closed,Fixed,04/Apr/11 04:44,06/Mar/18 23:25
Bug,GROOVY-4759,12815585,compiler NPE with some finally blocks with explicit return,"Not normally a piece of code I would write, but a post in stackoverflow.com brought to my attention this bug.

I don't know the exact trigger, but it seems that a nested try block, with an explicit return on one of the finally clauses, when written as the final statement of a function is throwing a NullPointerException at groovyjarjarasm.asm.MethodWriter#visitMaxs

This script is the shortest example I got.
{code:title=script1.groovy}
try {
  try {} finally {}
} finally {
  return 0
}
{code}

Things to note: 
It's the last statement of the script's run()
There is a nested try block. The blocks could be a try/catch, try/finally or try/catch/finally blocks with identical behavior
The explicit return could be anything, it will fail to compile.
There cannot be anything between the finally clauses and the end of the function.

a little bit longer

{code:title=script2.groovy}
try {
//  1
  try {
//    2
  } finally {
//    3
    return -1
//    4
  }
//  5
} finally {
//  6
 return -2
//  7
}
//8
{code}

There can be anything in positions 1-4,6,7
The script can be compiled when there is a statement in positions 5 or 8
Either of the explicit returns is enough, not necessarily both.

The full stacktrace is
{code}
Caught: java.lang.NullPointerException
java.lang.NullPointerException
        at org.objectweb.asm.MethodWriter.visitMaxs(Unknown Source)
        at org.objectweb.asm.MethodAdapter.visitMaxs(Unknown Source)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:595)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:686)
        at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1039)
        at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
        at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:276)
        at org.codehaus.groovy.control.CompilationUnit$12.call(CompilationUnit.java:748)
        at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:942)
        at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:519)
        at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:497)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:474)
        at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:306)
        at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:283)
        at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:267)
        at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:197)
        at groovy.lang.GroovyShell$2.run(GroovyShell.java:215)
        at groovy.lang.GroovyShell$2.run(GroovyShell.java:213)
        at java.security.AccessController.doPrivileged(Native Method)
        at groovy.lang.GroovyShell.run(GroovyShell.java:213)
        at groovy.lang.GroovyShell.run(GroovyShell.java:159)
        at groovy.ui.GroovyMain.processOnce(GroovyMain.java:496)
        at groovy.ui.GroovyMain.run(GroovyMain.java:311)
        at groovy.ui.GroovyMain.process(GroovyMain.java:297)
        at groovy.ui.GroovyMain.processArgs(GroovyMain.java:112)
        at groovy.ui.GroovyMain.main(GroovyMain.java:93)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:108)
        at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:130)
{code}

",,jpertino,Major,Closed,Fixed,04/Apr/11 12:30,17/Dec/14 13:25
Bug,GROOVY-4761,12815627,Incorrect source location for method call expression,"In this class, the source location for the StaticMethodCallExpression is incorrect.  The start column coincides with the column for '3' and the end includes the whitespace until the comment starts.

{code}
class StaticTrying {
	
	public static Class staticMethod(arg) {
		
	}
	
	def foo() {
		def a = staticMethod 3  // extra whitespace
	}
}
{code}

I'm not too concerned about the invalid end column, but the start column problem is affecting Groovy-Eclipse.

I delved a little bit into this.  Here is when the situation happens:

# when the method declaration takes 1 or more parameters
# the method declaration can be either static or non-static
# the method call takes exactly 1 argument
# does not use parens
# is part of a declaration expression.

Because of parts 4 and 5, this problem can only occur on Groovy 1.8 because on 1.7 it would be a parsing error.

I tracked this down to an invalid source location for the Antlr AST node being passed into {{AntlrParserPlugin.methodCallExpression()}}.

I originally reported this bug in GRECLIPSE-1031.
",daniel_sun,werdna,Major,Closed,Fixed,04/Apr/11 14:54,06/Mar/18 23:25
Bug,GROOVY-4762,12815582,Numbers as properties in command expressions,"It is allowed not to surround numbers with quotes in command expressions if they are used as properties.
Is it bug or feature?
 
{code}
def get123() {2}
def foo(i) {this}

def a = foo(2).'123'
def b = foo 2   123

println a
println b
{code}

But if you rename get123() to get123a() the line will throw an exception.
{code}
def b = foo 2   123a
{code}",daniel_sun,mxm-groovy,Major,Closed,Fixed,05/Apr/11 03:05,06/Mar/18 23:25
Bug,GROOVY-4765,12815602,__$swapInit not quite doing what was expected,"The new method __$swapInit was added (I think) to support reloading, enabling someone to drive re-initialization of some state (like constants, callsite arrays).  Unfortunately it doesn't quite do that - with regards to the callsite array all it does is call $getCallSiteArray.  This doesn't clear it, this just ensures it is initialized - and if initialized in the past it will not be reinitialized.  This means the reloader has to additionally null the callsite array before calling __$swapInit - which is a shame since it is all invoked by reflection.",blackdrag,aclement,Minor,Closed,Fixed,05/Apr/11 11:53,07/Apr/11 16:15
Bug,GROOVY-4767,12815527,Compilation error for generic interface method,"Repro:

Compile

{code}
interface Foo {
    public <N extends Number> void foo()
}
{code}

Result:

{noformat}
unexpected token: public @ line 2, column 5.
       public <N extends Number> void foo()
       ^
{noformat}

Making {{Foo}} an abstract class avoids the problem:

{code}
abstract class Foo {
    public abstract <N extends Number> void foo()
}
{code}",paulk,sukhyun.cho,Major,Closed,Fixed,05/Apr/11 15:45,21/Jul/11 19:06
Bug,GROOVY-4768,12818070,"java stub contains ""AnnotationNode"" string instead of an annotation within an annotation","My groovy method has an annotation that some hibernate users will recognize:
{code:Java}
   @JoinTable(name = ""join_table"",
      inverseJoinColumns = @JoinColumn(name = ""otherEntity_id""),
      joinColumns = @JoinColumn(name = ""myEntity_id""))
   Set<OtherEntity> getOthers() {
      others
   }
{code}
When I compile with just groovy, no problem.  But in joint compilation mode, the java compiler gets confused by the appearance of a String instead of an annotation in the stub:
{code}
  [groovyc] C:\DOCUME~1\cyrus\LOCALS~1\Temp\groovy-generated-5778051423738941844-java-source\MyEntity.java:18: annotation value must be an annotation
  [groovyc] @javax.persistence.OneToMany(targetEntity=OtherEntity.class) @javax.persistence.JoinTable(inverseJoinColumns=""org.codehaus.groovy.ast.AnnotationNode@d4d66b"", name=""join_table"", joinColumns=""org.codehaus.groovy.ast.AnnotationNode@149105b"") public  java.util.Set<OtherEntity> getOthers() { return (java.util.Set<OtherEntity>)null;}
  [groovyc]                                                                          {code}
I've attached a simple ant project to reproduce this problem.  (To save space, I excluded the groovy jar.)",paulk,lgdean,Major,Closed,Fixed,05/Apr/11 16:07,22/Jul/11 16:56
Bug,GROOVY-4774,12815597,Problem with extended command expressions,"Extended command expressions such as:
{code}
a b c d
{code}
Are equivalent to calling:
{code}
a(b).c(d)
{code}
But in the case we have something like:
{code}
a b 1 2
{code}
Which would be supposed to be equivalent to:
{code}
a(b).1(2)
{code}
But the AST represented by that call just represents the 1.call(2) part, and the a and b parts are totally absent.",blackdrag,guillaume,Critical,Closed,Fixed,07/Apr/11 08:40,12/Apr/11 09:51
Bug,GROOVY-4781,12816523,@WithReadLock and @WithWriteLock creates the wrong locks,"The @WithReadLock forces a method to obtain a write lock. 
The @WithWriteLock forces a method to obtain a read lock. 

This is exactly backwards. It should be the other way around. I have a fix.",hamletdrc,hamletdrc,Major,Closed,Fixed,14/Apr/11 03:50,05/Apr/15 14:44
Bug,GROOVY-4784,12815587,"[ null, 'ah' ].findAll() throws a NullPointerException","{code}
[ null, 'ah' ].findAll()
{code}

throws a NullPointerException with no error message which is a bit odd because I can write [][1] and get null back.

Essentially, Groovy is very lax (in a good way) in most parts but not here.

I'd implement a default Closure that just casts each argument to boolean, which i think [ null, 'ah' ].findAll { it } would.

This might apply to more methods than just findAll(). ;-)",paulk,johann,Minor,Closed,Fixed,14/Apr/11 15:39,21/Jul/11 19:06
Bug,GROOVY-4786,12815593,@EqualsAndHashCode : excludes does not work correctly ,"The following code should work. It fails in 1.8-rc4:
{code}
import groovy.transform.EqualsAndHashCode

@EqualsAndHashCode(excludes=""b"")
class X {
    int a
    int b
}

x1 = new X(a:1, b:10)
x2 = new X(a:1, b:100)

assert x1 == x2
assert x1.hashCode() == x2.hashCode() // failure here
{code}",paulk,e.castro,Major,Closed,Fixed,17/Apr/11 04:18,27/Apr/11 19:05
Bug,GROOVY-4787,12818071,BUG! exception in phase 'class generation' generating class from map in closure,"If I have a class like so:

{code}
class Person {
  String name
  int age
}
{code}

Then it is possible to create an instance of {{Person}} using the little known construct:

{code}
def tim = Person [ name:'tim', age:49 ]
{code}

This works fine.

However, if I try to do the following:

{code}
def ppl = [ [ name:'Tim', age:49 ], [ name:'Dave', age:32 ], [ name:'Steve', age:28 ] ]

ppl.collect { Person [ *it ] }
{code}

I get the class generation error

{code}
BUG! exception in phase 'class generation' in source unit 'ConsoleScript10' SpreadExpression should not be visited here
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitSpreadExpression(AsmClassGenerator.java:1871)
	at org.codehaus.groovy.ast.expr.SpreadExpression.visit(SpreadExpression.java:39)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitAndAutoboxBoolean(AsmClassGenerator.java:4122)
	at org.codehaus.groovy.classgen.AsmClassGenerator.makeBinopCallSite(AsmClassGenerator.java:2268)
	at org.codehaus.groovy.classgen.AsmClassGenerator.evaluateBinaryExpression(AsmClassGenerator.java:3902)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBinaryExpression(AsmClassGenerator.java:1662)
	at org.codehaus.groovy.ast.expr.BinaryExpression.visit(BinaryExpression.java:49)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitAndAutoboxBoolean(AsmClassGenerator.java:4122)
	at org.codehaus.groovy.classgen.AsmClassGenerator.evaluateExpression(AsmClassGenerator.java:1447)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitReturnStatement(AsmClassGenerator.java:1408)
	at org.codehaus.groovy.ast.stmt.ReturnStatement.visit(ReturnStatement.java:47)
	at org.codehaus.groovy.ast.CodeVisitorSupport.visitBlockStatement(CodeVisitorSupport.java:35)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitBlockStatement(ClassCodeVisitorSupport.java:165)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:738)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:626)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:601)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:696)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1039)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:276)
	at org.codehaus.groovy.control.CompilationUnit$12.call(CompilationUnit.java:748)
	at org.codehaus.groovy.control.CompilationUnit$12.call(CompilationUnit.java:765)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:942)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:519)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:497)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:474)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:306)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:287)
	at groovy.lang.GroovyShell.parseClass(GroovyShell.java:731)
	at groovy.lang.GroovyShell.run(GroovyShell.java:516)
	at groovy.lang.GroovyShell.run(GroovyShell.java:172)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:904)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:149)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at groovy.lang.Closure.call(Closure.java:282)
	at groovy.lang.Closure.call(Closure.java:277)
	at groovy.lang.Closure.run(Closure.java:360)
	at java.lang.Thread.run(Thread.java:680)
{code}

I know I should be doing:

{code}
ppl.collect { new Person( *:it ) }
{code}

(which works fine), but I just thought you should know about this {{BUG!}} exception",paulk,tim_yates,Minor,Closed,Fixed,19/Apr/11 05:22,02/May/17 02:03
Bug,GROOVY-4790,12818079,Compilation bug with spread operator,"I'm afraid this happened with Grails 1.4.0.BUILD-SNAPSHOT and a Grails application. That means to reproduce, you will have to build Grails 1.4.0 from trunk or pick up [a nightly snapshot|http://hudson.grails.org/job/grails_core_1.4.x/693/artifact/build/distributions/grails-1.4.0.BUILD-SNAPSHOT.zip]. I have attached a sample application.

Here's the stacktrace when running {{grails compile}}:

{noformat}
Welcome to Grails 1.4.0.BUILD-SNAPSHOT - http://grails.org/
Licensed under Apache Standard License 2.0
Grails home is set to: /Users/pledbrook/dev/tools/git/grails-core

Base Directory: /Users/pledbrook/dev/projects/scratch/test-1.4-snap
Running script Compile.groovy
Environment set to development
    [mkdir] Created dir: /Users/pledbrook/dev/projects/scratch/test-1.4-snap/target/plugin-classes
  [groovyc] Compiling 46 source files to /Users/pledbrook/dev/projects/scratch/test-1.4-snap/target/plugin-classes
Error executing script Compile: : BUG! exception in phase 'class generation' in source unit '/Users/pledbrook/dev/projects/scratch/test-1.4-snap/target/plugins/fixtures-1.0.6/src/groovy/grails/plugin/fixtures/files/FixtureFileLoader.groovy' SpreadExpression should not be visited here
: BUG! exception in phase 'class generation' in source unit '/Users/pledbrook/dev/projects/scratch/test-1.4-snap/target/plugins/fixtures-1.0.6/src/groovy/grails/plugin/fixtures/files/FixtureFileLoader.groovy' SpreadExpression should not be visited here
	at gant.Gant$_dispatch_closure5.doCall(Gant.groovy:391)
	at gant.Gant$_dispatch_closure7.doCall(Gant.groovy:415)
	at gant.Gant$_dispatch_closure7.doCall(Gant.groovy)
	at java_util_concurrent_Callable$call.call(Unknown Source)
	at gant.Gant.withBuildListeners(Gant.groovy:427)
	at gant.Gant.this$2$withBuildListeners(Gant.groovy)
	at gant.Gant$this$2$withBuildListeners.callCurrent(Unknown Source)
	at gant.Gant.dispatch(Gant.groovy:415)
	at gant.Gant.this$2$dispatch(Gant.groovy)
	at gant.Gant.invokeMethod(Gant.groovy)
	at gant.Gant.executeTargets(Gant.groovy:590)
	at gant.Gant.executeTargets(Gant.groovy:589)
Caused by: : BUG! exception in phase 'class generation' in source unit '/Users/pledbrook/dev/projects/scratch/test-1.4-snap/target/plugins/fixtures-1.0.6/src/groovy/grails/plugin/fixtures/files/FixtureFileLoader.groovy' SpreadExpression should not be visited here
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:116)
	at _GrailsCompile_groovy$_run_closure3_closure9.doCall(_GrailsCompile_groovy:119)
	at _GrailsCompile_groovy$_run_closure3_closure9.doCall(_GrailsCompile_groovy)
	at _GrailsCompile_groovy$_run_closure3.doCall(_GrailsCompile_groovy:106)
	at java_util_concurrent_Callable$call.call(Unknown Source)
	at _GrailsCompile_groovy$_run_closure2.doCall(_GrailsCompile_groovy:45)
	at java_util_concurrent_Callable$call.call(Unknown Source)
	at java_util_concurrent_Callable$call.call(Unknown Source)
	at gant.Gant$_dispatch_closure5.doCall(Gant.groovy:381)
	... 11 more
Caused by: BUG! exception in phase 'class generation' in source unit '/Users/pledbrook/dev/projects/scratch/test-1.4-snap/target/plugins/fixtures-1.0.6/src/groovy/grails/plugin/fixtures/files/FixtureFileLoader.groovy' SpreadExpression should not be visited here
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitSpreadExpression(AsmClassGenerator.java:575)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeDirectMethodCall(InvocationWriter.java:125)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:166)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:87)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeInvokeMethodCall(InvocationWriter.java:71)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:287)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:661)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:599)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeExpressionStatement(OptimizingStatementWriter.java:320)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:460)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:80)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeBlockStatement(OptimizingStatementWriter.java:151)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:406)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:289)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:268)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:366)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:174)
	at org.codehaus.groovy.control.CompilationUnit$13.call(CompilationUnit.java:763)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:957)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:542)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:520)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:497)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:476)
	at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:67)
	at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:180)
	at org.codehaus.groovy.ant.Groovyc.compile(Groovyc.java:903)
	at org.codehaus.groovy.ant.Groovyc.execute(Groovyc.java:606)
	at org.apache.tools.ant.UnknownElement.execute(UnknownElement.java:291)
	at org.apache.tools.ant.dispatch.DispatchUtils.execute(DispatchUtils.java:106)
	... 19 more
Error executing script Compile: : BUG! exception in phase 'class generation' in source unit '/Users/pledbrook/dev/projects/scratch/test-1.4-snap/target/plugins/fixtures-1.0.6/src/groovy/grails/plugin/fixtures/files/FixtureFileLoader.groovy' SpreadExpression should not be visited here
{noformat}",blackdrag,pledbrook,Critical,Closed,Fixed,21/Apr/11 12:05,18/Jan/13 16:06
Bug,GROOVY-4794,12815349,enum constant initializer parsing,"can't compile enum constant initializer which contains method without explicit type declaration

{code}
enum E {
  enConst {
    def foo(){}
  }
}
{code}

",paulk,mxm-groovy,Major,Closed,Fixed,25/Apr/11 02:43,17/Jun/15 20:09
Bug,GROOVY-4795,12818077,can't specify 'synchronize' parameter to ListenerList AST Transformation.,"I tried this code
{code}
  import groovy.beans.ListenerList
  class C {
    @ListenerList(synchronize=true) List<java.awt.event.ActionListener> listeners;
  }
{code}
following exception occurred.
{code}
$ /tool/groovy-1.8.0-rc-4/bin/groovy  test.groovy
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
General error during canonicalization: No signature of method: groovy.beans.ListenerListASTTransformation.addFireMethods() is applicable for argument types: (org.codehaus.groovy.control.SourceUnit, org.codehaus.groovy.ast.AnnotationNode, org.codehaus.groovy.ast.ClassNode, org.codehaus.groovy.ast.FieldNode, org.codehaus.groovy.ast.expr.ConstantExpression, org.codehaus.groovy.ast.MethodNode) values: [org.codehaus.groovy.control.SourceUnit@5117a20, org.codehaus.groovy.ast.AnnotationNode@71f1235b, C, org.codehaus.groovy.ast.FieldNode@162db19d, ConstantExpression[true], MethodNode@1047055737[void actionPerformed(java.awt.event.ActionEvent)]]
Possible solutions: addFireMethods(org.codehaus.groovy.control.SourceUnit, org.codehaus.groovy.ast.AnnotationNode, org.codehaus.groovy.ast.ClassNode, org.codehaus.groovy.ast.FieldNode, boolean, org.codehaus.groovy.ast.MethodNode)

groovy.lang.MissingMethodException: No signature of method: groovy.beans.ListenerListASTTransformation.addFireMethods() is applicable for argument types: (org.codehaus.groovy.control.SourceUnit, org.codehaus.groovy.ast.AnnotationNode, org.codehaus.groovy.ast.ClassNode, org.codehaus.groovy.ast.FieldNode, org.codehaus.groovy.ast.expr.ConstantExpression, org.codehaus.groovy.ast.MethodNode) values: [org.codehaus.groovy.control.SourceUnit@5117a20, org.codehaus.groovy.ast.AnnotationNode@71f1235b, C, org.codehaus.groovy.ast.FieldNode@162db19d, ConstantExpression[true], MethodNode@1047055737[void actionPerformed(java.awt.event.ActionEvent)]]
Possible solutions: addFireMethods(org.codehaus.groovy.control.SourceUnit, org.codehaus.groovy.ast.AnnotationNode, org.codehaus.groovy.ast.ClassNode, org.codehaus.groovy.ast.FieldNode, boolean, org.codehaus.groovy.ast.MethodNode)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:55)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:78)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:46)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)
	at groovy.beans.ListenerListASTTransformation$_visit_closure2.doCall(ListenerListASTTransformation.groovy:75)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:885)
	at groovy.lang.Closure.call(Closure.java:293)
	at groovy.lang.Closure.call(Closure.java:306)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:1240)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:1216)
	at org.codehaus.groovy.runtime.dgm$124.invoke(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:271)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:53)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at groovy.beans.ListenerListASTTransformation.visit(ListenerListASTTransformation.groovy:74)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.visitClass(ASTTransformationVisitor.java:129)
	at org.codehaus.groovy.transform.ASTTransformationVisitor$2.call(ASTTransformationVisitor.java:172)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:957)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:542)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:520)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:497)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:306)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:283)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:267)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:197)
	at groovy.lang.GroovyShell$2.run(GroovyShell.java:215)
	at groovy.lang.GroovyShell$2.run(GroovyShell.java:213)
	at java.security.AccessController.doPrivileged(Native Method)
	at groovy.lang.GroovyShell.run(GroovyShell.java:213)
	at groovy.lang.GroovyShell.run(GroovyShell.java:159)
	at groovy.ui.GroovyMain.processOnce(GroovyMain.java:514)
	at groovy.ui.GroovyMain.run(GroovyMain.java:329)
	at groovy.ui.GroovyMain.process(GroovyMain.java:315)
	at groovy.ui.GroovyMain.processArgs(GroovyMain.java:112)
	at groovy.ui.GroovyMain.main(GroovyMain.java:93)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:108)
	at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:130)

1 error
{code}
",paulk,uehaj,Major,Closed,Fixed,25/Apr/11 16:41,27/Apr/11 19:05
Bug,GROOVY-4797,12815645,VerifyError when using @ListenerList,"If the generic type (Object below) has methods with primitive args, e.g. {{wait(long timeout)}}, then a Verify Error occurs:
{code}
import groovy.beans.ListenerList
import java.lang.reflect.Modifier

class C {
    @ListenerList List<Object> listeners
}

assert C.class.getMethod('getObjects')
{code}
",paulk,paulk,Major,Closed,Fixed,26/Apr/11 07:19,07/Apr/15 19:07
Bug,GROOVY-4801,12815639,Thrown exception not accessable inside catch block when class has @Category transformation applied,"When implementing a Category to handle exceptions, I find that the thrown exception is not available within the catch block when the class is annotated with the @Category AST transformation. Removing the @Category AST transformation gives the correct behaviour.

Example:

{code}
// Define exception handling category
@Category(Object)
class ExceptionHandler {
  def handled(Closure block) {
    try { block.call() }
    catch (Throwable t) { println t }
  }
}

// Define a class which mixes in this category
@Mixin(ExceptionHandler)
class Caller {
  def thrower() { handled { 1/0 } }
}

// Test the exception handling
new Caller().thrower()
// --> throws ""ERROR groovy.lang.MissingPropertyException: No such property: t for class: Caller""
{code}

However, if the ExceptionHandler category is defined without using the @Category AST transformation:

{code}
class ExceptionHandler {
  static def handled(Object self, Closure block) {
    try { block.call() }
    catch (Throwable t) { println t }
  }
}

@Mixin(ExceptionHandler)
class Caller {
  def thrower() { handled { 1/0 } }
}

new Caller().thrower()
// --> prints ""java.lang.ArithmeticException: Division by zero"", as expected
{code}

This behaviour is observed both in the groovy shell, and in compiled code.  Also, the erroneous behaviour is not due to the @Mixin AST transformation, as I observe the same behaviour when I replace the @Mixin AST transformation with the static initialization block {{{Caller.mixin ExceptionHandler}}} in the Caller class.
",paulk,bunglefeet,Major,Closed,Fixed,28/Apr/11 06:08,07/Apr/15 19:13
Bug,GROOVY-4803,12815624,out of memory error when running groovydoc when there are java files in packages,"reference: i found this bug running the ant task groovydoc; however, i then went to the command line to isolate the issue.  i started with 1.7.10 and then upgraded to 1.8.0 and got the same error.

this bug is repeatable.

follow these steps:
1) create a groovy project
2) add a package with groovy classes
3) run this command: rm -rf /tmp/docs/; mkdir -p /tmp/docs; groovydoc -verbose  -private --destdir /tmp/docs <package name>
4) everything should work fine
5) add a java file to the package
6) run this command: rm -rf /tmp/docs/; mkdir -p /tmp/docs; groovydoc -verbose  -private --destdir /tmp/docs <package name>
7) get an out of memory error when it hits the java file

i think groovy is doing a reclusive call that never ends until it runs out of memory

the project that i created using ant can be found at my github : https://github.com/matthewpurdy/purdyCommonTasks 

project directory structure looks like this:

purdyCommonTasks/
&#9500;&#9472;&#9472; README
&#9500;&#9472;&#9472; build.properties
&#9500;&#9472;&#9472; build.xml
&#9500;&#9472;&#9472; lib/
&#9474;   &#9500;&#9472;&#9472; groovy-all-1.8.0.jar
&#9474;   &#9500;&#9472;&#9472; junit-4.8.1.jar
&#9474;   &#9492;&#9472;&#9472; log4j-1.2.16.jar
&#9500;&#9472;&#9472; purdyCommonTasks.xml
&#9500;&#9472;&#9472; resources/
&#9474;   &#9492;&#9472;&#9472; log4j.xml
&#9500;&#9472;&#9472; src/
&#9474;   &#9492;&#9472;&#9472; somepackage/
&#9474;       &#9500;&#9472;&#9472; Main.groovy
&#9474;       &#9500;&#9472;&#9472; SomeClass.groovy
&#9474;       &#9492;&#9472;&#9472; SomeClass2.java
&#9492;&#9472;&#9472; test/
    &#9492;&#9472;&#9472; somepackage/
        &#9492;&#9472;&#9472; SomeClassTest.groovy


results when i run it from the commandline:

mpurdy-keywcorp:purdyCommonTasks mpurdy$ which groovy
/usr/local/groovy/bin/groovy
mpurdy-keywcorp:purdyCommonTasks mpurdy$ ls -l `which groovy`
-rwxrwxr-x@ 1 root  wheel   1.0K Apr 27 14:55 /usr/local/groovy/bin/groovy*
mpurdy-keywcorp:purdyCommonTasks mpurdy$ ls -l /usr/local/groovy
lrwxr-xr-x  1 root  wheel    12B Apr 29 16:14 /usr/local/groovy@ -> groovy-1.8.0
mpurdy-keywcorp:purdyCommonTasks mpurdy$ groovy -version
Groovy Version: 1.8.0 JVM: 1.6.0_24
mpurdy-keywcorp:purdyCommonTasks mpurdy$ rm -rf /tmp/docs/;mkdir -p /tmp/docs;groovydoc -verbose  -private --destdir /tmp/docs -windowtitle mydocs somepackage `find src/* -type f`
REDERROR [org.codehaus.groovy.tools.groovydoc.GroovyRootDocBuilder] Out of memory while processing: DefaultPackage/SomeClass2.java
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:108)
	at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:130)
Caused by: java.lang.OutOfMemoryError: Java heap space
	at antlr.ANTLRStringBuffer.append(ANTLRStringBuffer.java:36)
	at antlr.CharScanner.append(CharScanner.java:64)
	at antlr.CharScanner.consume(CharScanner.java:82)
	at antlr.CharScanner.match(CharScanner.java:205)
	at org.codehaus.groovy.antlr.java.JavaLexer.mSL_COMMENT(JavaLexer.java:1142)
	at org.codehaus.groovy.antlr.java.JavaLexer.nextToken(JavaLexer.java:401)
	at org.codehaus.groovy.antlr.java.JavaLexer$1.nextToken(JavaLexer.java:98)
	at antlr.TokenBuffer.fill(TokenBuffer.java:69)
	at antlr.TokenBuffer.LT(TokenBuffer.java:86)
	at antlr.LLkParser.LT(LLkParser.java:56)
	at org.codehaus.groovy.antlr.java.JavaRecognizer.classDefinition(JavaRecognizer.java:753)
	at org.codehaus.groovy.antlr.java.JavaRecognizer.typeDefinitionInternal(JavaRecognizer.java:674)
	at org.codehaus.groovy.antlr.java.JavaRecognizer.typeDefinition(JavaRecognizer.java:509)
	at org.codehaus.groovy.antlr.java.JavaRecognizer.compilationUnit(JavaRecognizer.java:348)
	at org.codehaus.groovy.tools.groovydoc.GroovyRootDocBuilder.parseJava(GroovyRootDocBuilder.java:86)
	at org.codehaus.groovy.tools.groovydoc.GroovyRootDocBuilder.getClassDocsFromSingleSource(GroovyRootDocBuilder.java:70)
	at org.codehaus.groovy.tools.groovydoc.GroovyRootDocBuilder.processFile(GroovyRootDocBuilder.java:205)
	at org.codehaus.groovy.tools.groovydoc.GroovyRootDocBuilder.buildTree(GroovyRootDocBuilder.java:162)
	at org.codehaus.groovy.tools.groovydoc.GroovyDocTool.add(GroovyDocTool.java:66)
	at org.codehaus.groovy.tools.groovydoc.GroovyDocTool$add.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at org.codehaus.groovy.tools.groovydoc.Main.execute(Main.groovy:198)
	at org.codehaus.groovy.tools.groovydoc.Main.main(Main.groovy:168)
	... 6 more
mpurdy-keywcorp:purdyCommonTasks mpurdy$ 
",blackdrag,mpurdy1973,Major,Closed,Fixed,29/Apr/11 16:19,15/Feb/12 03:49
Bug,GROOVY-4806,12815484,Sql.eachRow documentation: actual GroovyResultSet vs. documentation GroovyRowResult,"Sql.eachRow actually passes objects of type ""GroovyResultSet"" to the closure. Documentation says ""The row will be a GroovyRowResult"" (wrong).

The documentation example(s) are actually correct. They use ""row.toRowResult()"" to get to the desired ""GroovyRowResult""

http://groovy.codehaus.org/api/groovy/sql/Sql.html#eachRow(java.lang.String, groovy.lang.Closure, groovy.lang.Closure)

In all, six occurences of eachRow() variants show that ""The row will be a GroovyRowResult"" line.",paulk,mgaertner,Trivial,Closed,Fixed,04/May/11 04:33,21/Jul/11 19:06
Bug,GROOVY-4811,12815533,exception while compiling annotation type,"{code}
@interface Inter {
  String[] bar() default ""1""
}
{code}
While compiling this annotation type the compiler throws

java.lang.ClassCastException: org.codehaus.groovy.ast.expr.ConstantExpression cannot be cast to org.codehaus.groovy.ast.expr.ListExpression
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitAnnotationDefaultExpression(AsmClassGenerator.java:316)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitAnnotationDefault(AsmClassGenerator.java:350)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:259)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:366)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:171)
	at org.codehaus.groovy.control.CompilationUnit$13.call(CompilationUnit.java:763)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:957)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:542)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:520)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:497)
	at org.jetbrains.groovy.compiler.rt.GroovyCompilerWrapper.compile(GroovyCompilerWrapper.java:43)
	at org.jetbrains.groovy.compiler.rt.GroovycRunner.main(GroovycRunner.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.intellij.rt.execution.CommandLineWrapper.main(CommandLineWrapper.java:75)",paulk,mxm-groovy,Major,Closed,Fixed,05/May/11 07:21,02/May/17 02:03
Bug,GROOVY-4812,12818085,Problem Parsing Expression With 1.8.0,"The following works with 1.7.x but fails with 1.8.0

{code:title=Demo.groovy|borderStyle=solid}
class Demo  {
   
   void doit() {
       execute new Runnable(){
           void run() {
               println 'hello'
           }
       }
   }
   
   void execute(arg) {
       arg.run()
   }
   
   static void main(args) {
       new Demo().doit()
   }
}
{code}

The code fails with a MissingPropertyException:

{noformat}
$ groovy Demo
Caught: groovy.lang.MissingPropertyException: No such property: execute for class: Demo
        at Demo.doit(Demo.groovy:4)
        at Demo.main(Demo.groovy:16)
{noformat}

If I put parens around the argument to the execute method, then it appears to work:


{code:title=Demo.groovy|borderStyle=solid}
class Demo  {
   
   void doit() {
       execute(new Runnable(){
           void run() {
               println 'hello'
           }
       })
   }
   
   void execute(arg) {
       arg.run()
   }
   
   static void main(args) {
       new Demo().doit()
   }
}
{code}",blackdrag,brownj,Major,Closed,Fixed,05/May/11 15:48,28/Oct/11 15:39
Bug,GROOVY-4813,12818080,GroovyScriptEngine does not use configured ImportCustomizer,"When running a groovy script using {{groovy.util.GroovyScriptEngine}}, the configured {{ImportCustomizer}} is not used. When running the script with {{GroovyShell}}, it works as expected.

The problem can be reproduced using the following groovy script that tries to run a small script using {{groovy.util.GroovyScriptEngine}} and {{GroovyShell}}:

{code}
import org.codehaus.groovy.control.CompilerConfiguration;
import org.codehaus.groovy.control.customizers.ImportCustomizer;

File script = File.createTempFile('test', '.groovy')
script.deleteOnExit()
script.write """"""
println new SimpleDateFormat()
""""""

// Create compiler configuration with import customizer
CompilerConfiguration config = new CompilerConfiguration();
ImportCustomizer importCustomizer = new ImportCustomizer();
importCustomizer.addImports 'java.text.SimpleDateFormat'
config.addCompilationCustomizers importCustomizer

// Run script with groovy shell: this will work
GroovyShell shell = new GroovyShell(config)
shell.run script, []

// Run script with script engine: this will not work, import customizer is not used
GroovyScriptEngine scriptEngine = new GroovyScriptEngine(script.getParent())
scriptEngine.setConfig config
scriptEngine.run script.getName(), new Binding()
{code}

The output of this script looks like this:

{noformat}
java.text.SimpleDateFormat@3dd497a
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
/C:/Users/chpi2491/AppData/Local/Temp/test8494443541360579143.groovy: 2: unable to resolve class SimpleDateFormat 
 @ line 2, column 9.
   println new SimpleDateFormat()
           ^

1 error
{noformat}

The expected output would be something like this:
{noformat}
java.text.SimpleDateFormat@???????
java.text.SimpleDateFormat@???????
{noformat}",melix,kaklakariada,Major,Closed,Fixed,06/May/11 01:37,21/Jul/11 19:06
Bug,GROOVY-4818,12815630,JsonSlurper's methods are package private,"JsonSlurper's methods like parse(),parseText() have package private scope.
There is problems:
(1)
",guillaume,uehaj,Major,Closed,Fixed,07/May/11 16:12,21/Jul/11 19:06
Bug,GROOVY-4820,12815642,Problems using Groovlet / GSP from embedded Jetty,"I'm trying to use Groovlet / GSP from within an embedded Jetty, and it's not working for me.

I want to distribute my web application as a single jar-with-dependencies. So i start an embedded standalone Jetty that serves the webapplication from the jar-with-dependencies build with the assembly plugin.

When I try to load a GSP I get this stacktrace:

2011-05-10 10:50:47.168:WARN::/web/groovy/test.gsp
java.lang.NullPointerException
	at java.io.File.<init>(File.java:222)
	at groovy.servlet.AbstractHttpServlet.getScriptUriAsFile(AbstractHttpServlet.java:306)
	at groovy.servlet.TemplateServlet.service(TemplateServlet.java:377)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:401)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)

When I try to load the Groovlet I get this stacktrace:

2011-05-10 11:00:22.961:INFO:/:GroovyServlet Error:  script: '/web/groovy/test.groovy':  Script processing failed.nulljava.lang.String.startsWith(String.java:1421)
GroovyServlet Error:  script: '/web/groovy/test.groovy':  Script processing failed.nulljava.lang.String.startsWith(String.java:1421)
java.lang.NullPointerException
	at java.lang.String.startsWith(String.java:1421)
	at java.lang.String.startsWith(String.java:1450)
	at groovy.servlet.AbstractHttpServlet.getResourceConnection(AbstractHttpServlet.java:170)
	at groovy.util.GroovyScriptEngine.loadScriptByName(GroovyScriptEngine.java:450)
	at groovy.util.GroovyScriptEngine.createScript(GroovyScriptEngine.java:528)
	at groovy.util.GroovyScriptEngine.run(GroovyScriptEngine.java:515)
	at groovy.servlet.GroovyServlet$1.call(GroovyServlet.java:120)
	at org.codehaus.groovy.runtime.GroovyCategorySupport$ThreadCategoryInfo.use(GroovyCategorySupport.java:106)
	at org.codehaus.groovy.runtime.GroovyCategorySupport$ThreadCategoryInfo.access$400(GroovyCategorySupport.java:64)
	at org.codehaus.groovy.runtime.GroovyCategorySupport.use(GroovyCategorySupport.java:246)
	at groovy.servlet.GroovyServlet.service(GroovyServlet.java:129)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:820)
	at org.mortbay.jetty.servlet.ServletHolder.handle(ServletHolder.java:511)
	at org.mortbay.jetty.servlet.ServletHandler.handle(ServletHandler.java:401)
	at org.mortbay.jetty.security.SecurityHandler.handle(SecurityHandler.java:216)
	at org.mortbay.jetty.servlet.SessionHandler.handle(SessionHandler.java:182)
	at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:766)
	at org.mortbay.jetty.webapp.WebAppContext.handle(WebAppContext.java:450)
	at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)
	at org.mortbay.jetty.handler.HandlerCollection.handle(HandlerCollection.java:114)
	at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)
	at org.mortbay.jetty.Server.handle(Server.java:326)
	at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)
	at org.mortbay.jetty.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:928)
	at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:549)
	at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:212)
	at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)
	at org.mortbay.jetty.bio.SocketConnector$Connection.run(SocketConnector.java:228)
	at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)

When I start the webapplication using the Jetty plugin i works perfectly. 

I have attached a simple project to demonstrate the problem. Build it ""mvn install"" and start jetty either from the web app using the Jetty plugin (cd web; mvn jetty:run) or the standalone (cd standalone; java -jar target/standalone-1.0-SNAPSHOT-jar-with-dependencies.jar). The embedded Jetty listens on port 8080.

URLs to reproduce: 
http://localhost:8080/web/groovy/test.gsp
http://localhost:8080/web/groovy/test.groovy?name=myname

",blackdrag,mortenholm,Major,Closed,Fixed,10/May/11 04:05,22/Dec/12 01:10
Bug,GROOVY-4822,12815648,Label before closure,"While this statement is compiled normally:

{var->} ({->it})

The following one causes the compile error:

label: {var->} ({->it})

expecting EOF, found '(' at line: 1, column: 16

",,zhaber,Major,Closed,Fixed,11/May/11 23:03,17/Dec/14 13:25
Bug,GROOVY-4823,12818096,groovy.lang.MissingMethodException if using class extending BigDecimal,"We have a class extending java.math.BigDecimal, also a method having a BigDecimal argument
The method works fine if we call it with a BigDecimal object. If the method is called with an instance of the extending class, a MissingMethodException is thrown.

groovy.lang.MissingMethodException: No signature of method: ConsoleScript16.f() is applicable for argument types: (ExtendedBigDecimal) values: [1]
Possible solutions: f(java.math.BigDecimal), is(java.lang.Object), run(), run(), any(), any(groovy.lang.Closure)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:54)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:78)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:44)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:149)
	at ConsoleScript16.run(ConsoleScript16:14)
	at groovy.lang.GroovyShell.runScriptOrMainOrTestOrRunnable(GroovyShell.java:266)
	at groovy.lang.GroovyShell.run(GroovyShell.java:517)
	at groovy.lang.GroovyShell.run(GroovyShell.java:172)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:904)
	at sun.reflect.GeneratedMethodAccessor227.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:149)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
	at sun.reflect.GeneratedMethodAccessor226.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:273)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886)
	at groovy.lang.Closure.call(Closure.java:282)
	at groovy.lang.Closure.call(Closure.java:277)
	at groovy.lang.Closure.run(Closure.java:360)
	at java.lang.Thread.run(Thread.java:619)
",blackdrag,uwekirsch,Major,Closed,Fixed,12/May/11 06:47,09/Sep/11 08:08
Bug,GROOVY-4825,12818407,Compiler Errors for @Immutable,"{code}
@Immutable
class X {
  int a	
}
{code}

Leads to (groovy 1.7.10, executed using gradle)

{noformat}
java.lang.RuntimeException: Explicit constructors not allowed for @Immutable class: X
	at org.codehaus.groovy.transform.ImmutableASTTransformation.createConstructor(ImmutableASTTransformation.java:308)
	at org.codehaus.groovy.transform.ImmutableASTTransformation.visit(ImmutableASTTransformation.java:117)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.visitClass(ASTTransformationVisitor.java:129)
	at org.codehaus.groovy.transform.ASTTransformationVisitor$2.call(ASTTransformationVisitor.java:172)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:942)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:519)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:497)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:474)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:453)
	at org.codehaus.groovy.tools.FileSystemCompiler.compile(FileSystemCompiler.java:67)
	at org.codehaus.groovy.tools.FileSystemCompiler.doCompilation(FileSystemCompiler.java:180)
	at org.codehaus.groovy.tools.FileSystemCompiler.commandLineCompile(FileSystemCompiler.java:148)
	at org.codehaus.groovy.tools.FileSystemCompiler.main(FileSystemCompiler.java:162)
{noformat}

{code}
@Immutable
class X {
}
{code}

Leads to (groovy 1.7.8.xx-20110426-1500-e36, eclipse)

{noformat}
Duplicate method X() in type X
{noformat} 


{code}
class Y {
  X x
}
{code}

Leads to (groovy 1.7.8.xx-20110426-1500-e36, eclipse)

{noformat}
General error during canonicalization: @Immutable processor doesn't know how to handle field 'x' of type 'playground.X' while compiling class playground.Y.
@Immutable classes currently only support properties with known immutable types or types where special handling achieves immutable behavior, including:
- Strings, primitive types, wrapper types, BigInteger and BigDecimal, enums
- other @Immutable classes and known immutables (java.awt.Color, java.net.URI)
- Cloneable classes, collections, maps and arrays, and other classes with special handling (java.util.Date)
Other restrictions apply, please see the groovydoc for @Immutable for further details
{noformat}


",paulk,peter.rietzler@smarter-ecommerce.com,Major,Closed,Fixed,16/May/11 04:02,04/Aug/11 15:51
Bug,GROOVY-4827,12818082,@EqualsAndHashCode.excludes doesn't work correctly with class inheritance,"I cannot inherit exclusions of EqualsAndHashCode, I have to duplicate exclusions in every child class.
Please see the tescase, only testA() and testE() pass.

{code}
class EqualsTest {
    @EqualsAndHashCode(excludes = 'a1')
    static class A {
        def a1, a2;
    }

    @Test
    void testA() {
        assert new A(a1: 'same', a2: 'same') == new A(a1: 'same', a2: 'same') // ok
        assert new A(a1: 'diff', a2: 'same') == new A(a1: 'erent', a2: 'same') // ok
    }

    static class B extends A {
        def b
    }

    @Test
    void testB() {
        assert new B(a1: 'diff', a2: 'same', b: 'same') == new B(a1: 'erent', a2: 'same', b: 'same')
        // failed! Shouldn't equals() be inherited from A?
    }

    //ok. let's try to specify explicitly that we call super.equals()
    @EqualsAndHashCode(callSuper = true)
    static class C extends A {
        def c
    }

    @Test
    void testC() {
        assert new C(a1: 'diff', a2: 'same', c: 'same') == new C(a1: 'erent', a2: 'same', c: 'same')
        // failed!
    }

    //hm. what about excluding it again in the child class?
    @EqualsAndHashCode(callSuper = true, excludes = 'a1')
    static class D extends A {
        def d
    }

    @Test
    void testD() {
        assert new D(a1: 'diff', a2: 'same', d: 'same') == new D(a1: 'erent', a2: 'same', d: 'same')
        // failed!
    }

    //hm. should I forget about inheritance??
    @EqualsAndHashCode(excludes = 'a1')
    static class E extends A {
        def e
    }

    @Test
    void testE() {
        assert new E(a1: 'diff', a2: 'same', e: 'same') == new E(a1: 'erent', a2: 'same', e: 'same')
        // YES :(
    }

}
{code} ",paulk,pawlom,Major,Closed,Fixed,16/May/11 13:04,21/Jul/11 19:06
Bug,GROOVY-4831,12815677,New line character is not escaped in attribute when using MarkupBuilder,"For example:
{code}
import groovy.xml.*
def xml = new MarkupBuilder()
xml.test(a:""hello\nworld""){}
{code}

produces output:
{code:xml}
<test a='hello
world' />
{code}

But xml specification says that when parsing, new lines in attribute value should be converted to single space. So they are not preserved.

I suggest this new method implementation (sorry that I don't provide real .patch):
{code}
    private String checkForReplacement(boolean isAttrValue, char ch) {
        switch (ch) {
            case '&':
                return ""&amp;"";
            case '<':
                return ""&lt;"";
            case '>':
                return ""&gt;"";
            case '""':
                // The double quote is only escaped if the value is for
                // an attribute and the builder is configured to output
                // attribute values inside double quotes.
                if (isAttrValue && useDoubleQuotes) return ""&quot;"";
                break;
            case '\'':
                // The apostrophe is only escaped if the value is for an
                // attribute, as opposed to element content, and if the
                // builder is configured to surround attribute values with
                // single quotes.
                if (isAttrValue && !useDoubleQuotes) return ""&apos;"";
                break;
            case '\n':
                if(isAttrValue) return ""&#10;"";
            case '\r':
                if(isAttrValue) return ""&#13;"";
        }
        return null;
    }
{code}
",paulk,jakub.neubauer,Major,Closed,Fixed,18/May/11 06:33,21/Jul/11 19:06
Bug,GROOVY-4832,12818087,"when two java classes extend a groovy class, both subclasses have a metaClass for whichever subclass was loaded first","*Original problem:*

I have an abstract groovy class, and two (or more) concrete java subclasses extend it.  The superclass has an abstract method, which each concrete subclass overrides (of course).

When I use just one subclass, everything works fine: I call a superclass method that calls the abstract method, and I get the behavior I expect.  But when the other subclass has already been loaded, things are different.  Then, I get an IllegalArgumentException: object is not an instance of declaring class.

This problem corresponds to testGenericSubclassWithBafflingSymptom() in the attached junit tests.  See that test for additional details.

*Simpler case that illustrates the likely underlying problem (and does not involve generics):*

I have an abstract groovy class, and two concrete java subclasses extend it.  If I have an instance of just one of the subclasses, then instance.metaClass.theClass returns exactly what I expect.  But if I've already loaded the other subclass, then the metaClass on an instance of *either* subclass is for the class that was used first!

{code:java}
// snippet of the groovy version of testSubclass(), also attached
      OtherConcreteJavaSubclass unrelatedInstance = new OtherConcreteJavaSubclass();
      ConcreteJavaSubclass instance = new ConcreteJavaSubclass();
      assertEquals(""this one works"", OtherConcreteJavaSubclass, unrelatedInstance.metaClass.theClass)
      assertEquals(""but this one is wrong"", ConcreteJavaSubclass, instance.metaClass.theClass)
{code}


This mixture of groovy and java may sound a little odd, but we actually ran into it when converting an existing class from java to groovy, and it stumped us for quite a while.",blackdrag,lgdean,Major,Closed,Fixed,18/May/11 15:34,04/Sep/11 15:00
Bug,GROOVY-4833,12815693,@Field AST Transformation looses annotations,"For reference : http://groovy.329449.n5.nabble.com/Revisiting-guice-injection-with-Groovy-1-8-td4387777.html

The @Field AST transformation creates a FieldNode but forgets to put the other possible annotations on the generated field. For example, in the following code :

{code}
@Awesome @Field def test
{code}

The @Awesome annotation is lost.",melix,melix,Major,Closed,Fixed,18/May/11 17:08,25/Apr/13 05:09
Bug,GROOVY-4838,12815569,Class that extends ClassLoader compiled with JDK 1.5 fails to run on JDK 1.6 with IncompatibleClassChangeError,"We have a class loader like this:


{code}
class ParentOnlyClassLoader extends ClassLoader{

    ParentOnlyClassLoader(ClassLoader parent) {
        super(parent)
    }

    @Override protected Class<?> findClass(String name) {
        parent.findClass(name)
    }

}
{code}

When this class is compiled with JDK 1.5 then executed in JDK 1.6 you get the error:

{code}
java.lang.IncompatibleClassChangeError: the number of constructors during runtime and compile time for java.lang.ClassLoader do not match. Expected 2 but got 3
	at org.grails.plugins.tomcat.ParentOnlyClassLoader.<init>(ParentOnlyClassLoader.groovy:28)
	at org
{code}",melix,graemerocher,Major,Closed,Fixed,23/May/11 03:36,26/Nov/13 14:50
Bug,GROOVY-4840,12818083,LogASTTransformation fails if super class defines a private log field,"Given a Java class which has a private static final Log log = ... field, I find that I cannot apply the @Commons annotation (or, indeed, any other logging annotation) to a subclass without specifying a name other than 'log'. That seems a bit absurd - the sub-classes shouldn't be able to access the super-class log, as it is private. And, indeed, attempting to run code which accesses the private super-class 'log' variable fails with a 'groovy.lang.MissingPropertyException'.

The fix for this would be to check if the owning class of the field (which is acquired on line 81 of LogASTTransformation.groovy) is the one we are currently processing. If so, then fail it. Otherwise, only fail it if the field is non-private - Package, Protected, or Public.",hamletdrc,ipsi,Critical,Closed,Fixed,24/May/11 00:21,21/Jul/11 19:06
Bug,GROOVY-4841,12815699,Null BigDecimals converted to String with add operator,"Using the add operator on null BigDecimal objects concatenates them as Strings

{code:java}
  @Test
  void testNullBigDecimalAddOperator() {
    BigDecimal a = null
    BigDecimal b = null
    assert null == a+b
  }
{code}

{noformat}
Assertion failed: 

assert null == a+b
            |  |||
            |  ||null
            |  |nullnull
            |  null
            false
{noformat}

I would expect a NullPointer would be thrown as the divide operator behaves. 


{code:java}
  @Test(expected=NullPointerException)
  void testNullBigDecimalDivOperator() {
    BigDecimal a = null
    BigDecimal b = null
    assert null == a/b
  }
{code}

{noformat}
java.lang.NullPointerException: Cannot invoke method div() on null object
{noformat}",blackdrag,mcantrell,Major,Closed,Fixed,24/May/11 15:15,07/Feb/12 06:32
Bug,GROOVY-4844,12815686,@ToString should use getX() rather than the field x when getting property values,"I expect the following example to work:
{code}
@TupleConstructor @ToString class Point { int x, y }
def p1 = new Point(1, 2)
def p2 = new Point(1, 1) { int getY() { 2 } }
assert p1.toString() == 'Point(1, 2)'
assert p2.toString() == 'Point(1, 2)'
{code}
",paulk,paulk,Major,Closed,Fixed,25/May/11 22:32,07/Apr/15 19:06
Bug,GROOVY-4851,12815704,NullPointerException on iterator() call,"I got with the following code which use http://sourceforge.net/projects/picard/files/picard-tools/  
{code}
import net.sf.picard.util.Interval 
import net.sf.picard.util.IntervalList 
import net.sf.picard.util.SamLocusIterator 
import net.sf.picard.util.SamLocusIterator.RecordAndOffset 
import net.sf.samtools.SAMFileHeader 
import net.sf.samtools.SAMFileReader 
import net.sf.samtools.SAMFileReader.ValidationStringency 

bamFile = new File(""/media/trx/workspace/Picard/test/ex1.bam"") 
baiFile = new File(bamFile.getAbsolutePath() + "".bai"") 

sam = new SAMFileReader(bamFile, baiFile, true) 

sam.setValidationStringency(ValidationStringency.SILENT) 
if(!sam.hasIndex()) throw new Exception(""Missing index"") 

SAMFileHeader header = sam.getFileHeader() 

sequences = header.getSequenceDictionary().getSequences() 

for ( i in sequences) { 
        println(""${i.getSequenceName()} = ${i.getSequenceLength()}"") 
} 

IntervalList il = new IntervalList(header) 
il.add(new Interval(""chr1"",0, 157)) 
final SamLocusIterator sli = new SamLocusIterator(sam, il, true) 
sli.setEmitUncoveredLoci(false) 

sli.each { li -> 
        println li.getSequenceName() 
} 
{code}
I got the following error: 
{code}
chr1 = 1575 
chr2 = 1584 
WARNING	2011-04-30 12:10:29	SamLocusIterator	SamLocusIterator constructed with samReader that has SortOrder == unsorted.  Assuming SAM is coordinate sorted, but exceptions may occur if it is not. 
Exception in thread ""main"" java.lang.NullPointerException 
        at net.sf.picard.util.SamLocusIterator.samHasMore(SamLocusIterator.java:223) 
        at net.sf.picard.util.SamLocusIterator.hasNext(SamLocusIterator.java:231) 
        at org.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:1219) 
        at org.codehaus.groovy.runtime.DefaultGroovyMethods.each(DefaultGroovyMethods.java:1196) 
        at org.codehaus.groovy.runtime.dgm$110.invoke(Unknown Source) 
        at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:270) 
        at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:52) 
        at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40) 
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116) 
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:124) 
        at pileup.run(pileup.groovy:33) 
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method) 
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57) 
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43) 
        at java.lang.reflect.Method.invoke(Method.java:616) 
        at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90) 
        at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233) 
        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1058) 
        at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:886) 
        at org.codehaus.groovy.runtime.InvokerHelper.invokePogoMethod(InvokerHelper.java:793) 
        at org.codehaus.groovy.runtime.InvokerHelper.invokeMethod(InvokerHelper.java:776) 
        at org.codehaus.groovy.runtime.InvokerHelper.runScript(InvokerHelper.java:394) 
        at org.codehaus.groovy.runtime.InvokerHelper$runScript.call(Unknown Source) 
        at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:40) 
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116) 
        at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:128) 
        at pileup.main(pileup.groovy) 
{code}

The following code is written completely in Java and uses the same input file without any error: 
{code}
import java.io.File; 
import java.util.List; 
import net.sf.picard.util.Interval; 
import net.sf.picard.util.IntervalList; 
import net.sf.picard.util.SamLocusIterator; 
import net.sf.picard.util.SamLocusIterator.LocusInfo; 
import net.sf.picard.util.SamLocusIterator.RecordAndOffset; 
import net.sf.samtools.SAMFileHeader; 
import net.sf.samtools.SAMFileReader; 
import net.sf.samtools.SAMRecord; 
import net.sf.samtools.SAMRecordIterator; 
import net.sf.samtools.SAMSequenceRecord; 
import net.sf.samtools.SAMFileReader.ValidationStringency; 


public class ReadReferenceName { 

        /** 
         * @param args 
         * @throws Exception 
         */ 
        public static void main(String[] args) throws Exception { 
                // TODO Auto-generated method stub 
                
                File bamFile = new File(""/media/trx/workspace/Picard/test/ex1.bam""); 
                File baiFile = new File(bamFile.getAbsolutePath() + "".bai"");	
                
                SAMFileReader sam = new SAMFileReader(bamFile, baiFile, true); 
        sam.setValidationStringency(ValidationStringency.SILENT); 
                if(!sam.hasIndex()) throw new Exception(""Missing index""); 
                
                SAMFileHeader header = sam.getFileHeader(); 
                List<SAMSequenceRecord> sequences = header.getSequenceDictionary().getSequences(); 
                
                for(SAMSequenceRecord rec: sequences) { 
                        System.out.println(rec.getSequenceName() + "" = "" + 
                                        rec.getSequenceLength()); 
                } 
                
                IntervalList il = new IntervalList(header); 
                il.add(new Interval(""chr1"",0, 157)); 
        final SamLocusIterator sli = new SamLocusIterator(sam, il, true); 
        sli.setEmitUncoveredLoci(false); 
      
        for (final SamLocusIterator.LocusInfo li : sli) { 
        String sequenceName = li.getSequenceName(); 
        int pos = li.getPosition(); 
        int coverage = li.getRecordAndPositions().size(); 
        System.out.println(sequenceName + "" coverage at base "" + pos + "" = "" 
        + coverage); 
        for (int i = 0; i< coverage; i++){ 
        RecordAndOffset rec = li.getRecordAndPositions().get(i); 
        char base = (char)rec.getReadBase(); 
        String readName = rec.getRecord().getReadName(); 
        System.out.println(""  base in read "" + readName + "" = "" + base);	
        } 
        } 
        } 
} 
{code}

and it produced this output: 

{code}
chr1 = 1575 
chr2 = 1584 
WARNING	2011-04-30 19:30:07	SamLocusIterator	SamLocusIterator constructed with samReader that has SortOrder == unsorted.  Assuming SAM is coordinate sorted, but exceptions may occur if it is not. 
chr1 coverage at base 100 = 1 
  base in read EAS56_57:6:190:289:82 = A 
chr1 coverage at base 101 = 1 
... 
{code}

Tim Yates wrote the following work around for this problem:

{code}
import net.sf.picard.util.*
import net.sf.picard.util.SamLocusIterator.RecordAndOffset
import net.sf.samtools.*
import net.sf.samtools.SAMFileReader.ValidationStringency
import net.sf.picard.filter.*
import net.sf.samtools.util.CloseableIterator

bamFile = new File(""ex1.bam"")
baiFile = new File(""ex1.bam.bai"")

sam = new SAMFileReader(bamFile, baiFile, true)
sam.validationStringency = ValidationStringency.SILENT
if(!sam.hasIndex()) throw new Exception(""Missing index"")

SAMFileHeader header = sam.fileHeader
sequences = header.sequenceDictionary.sequences

sequences.each { i ->
  println ""$i.sequenceName = $i.sequenceLength""
}

SamLocusIterator.metaClass.iterator = {
  if ( delegate.@samIterator != null ) {
    throw new IllegalStateException(""Cannot call iterator() more than once on SamLocusIterator"");
  }
  CloseableIterator<SAMRecord> tempIterator;
  if (delegate.@intervals != null) {
    tempIterator = new SamRecordIntervalIteratorFactory().makeSamRecordIntervalIterator(delegate.@samReader, delegate.@intervals, delegate.@useIndex);
  } else {
    tempIterator = delegate.@samReader.iterator();
  }
  if (delegate.@samFilters != null) {
    tempIterator = new FilteringIterator(tempIterator, new AggregateFilter(delegate.@samFilters));
  }
  delegate.@samIterator = new PeekableIterator<SAMRecord>( tempIterator )
  delegate 
}

IntervalList il = new IntervalList( header )
il.add( new Interval( ""chr1"", 0, 157 ) )
SamLocusIterator sli = new SamLocusIterator( sam, il, true )
sli.emitUncoveredLoci = false

sli.each { li ->
  println ""$li.sequenceName coverage at base $li.position = ${li.recordAndPositions.size()}""
  (0..<li.recordAndPositions.size()).each { i ->
    rec = li.recordAndPositions.get( i )
	println ""  base in read $rec.record.readName = $rec.readBase""
  }
}
{code}

More information can be found here ( http://groovy.329449.n5.nabble.com/NullPointerException-problem-td4360426.html )",jwagenleitner,trx,Major,Closed,Fixed,28/May/11 19:39,12/Jun/17 00:12
Bug,GROOVY-4852,12815713,Constant Optimization is a little off,"{code}
class Test {
    static main(String[] args) {
        def x = 2.0
        def y = 2.0
        def z = 2.0
    }
}
{code}
For the code above, optimizer generates the code below which has the optimization for first constant wrong (it should also be Object x = $const$0).
{noformat}
  public static Object main(String[] args)
  {
    Object x = new BigDecimal(""2.0"");
    Object y = $const$0;
    Object z = $const$0;
  }
{noformat}",melix,roshandawrani,Minor,Closed,Fixed,30/May/11 04:56,23/Feb/15 01:12
Bug,GROOVY-4854,12815683,MyEnum.values() bombs out on AppEngine,"Calling MyEnum.values() works great on the AppEngine local development server, but once this code runs in production you get:
{noformat}
java.lang.IllegalAccessException: Reflection is not allowed on protected native java.lang.Object java.lang.Object.clone() throws java.lang.CloneNotSupportedException
	at com.google.appengine.runtime.Request.process-d87840548eba2c93(Request.java)
	at java.lang.reflect.Method.invoke(Method.java:43)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1055)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:885)
	at org.codehaus.groovy.runtime.callsite.PojoMetaClassSite.call(PojoMetaClassSite.java:44)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:112)
	at MyEnum.values(Environment.groovy)
	at MyEnum$values.call(Unknown Source)
{noformat}",guillaume,axel.fontaine,Blocker,Closed,Fixed,31/May/11 18:13,21/Jul/11 19:06
Bug,GROOVY-4857,12818089,toString() unsupported on proxy,"toString() and other methods that are expected to be on Object should not generate an UnsupportedOperationException when using the Proxy pattern in groovy.  Many methods expect non-null objects to allow methods that exist on Object to be called without issue.  The example below should throw a MissingMethodException with a nice message that includes the method name and the arguments that were called, but since MissingMethodException.getMessage() calls the proxy's toString() method we get an UnsupportedOperationException on toString() instead, as well as a stack trace that originates from the location that the MissingMethodException's getMessage() is being called, obscuring the actual problem entirely.
{code}
interface A {
  def getValue()
}

class B { }

def test = [
  getValue: {
    'getValue() called'
  }
] as A

def b = new B()

b.call(test)
{code}

Stack trace from 1.8.0:
{code}
java.lang.reflect.InvocationTargetException
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.tools.GroovyStarter.rootLoader(GroovyStarter.java:108)
	at org.codehaus.groovy.tools.GroovyStarter.main(GroovyStarter.java:130)
Caused by: java.lang.UnsupportedOperationException
	at org.codehaus.groovy.runtime.ConvertedMap.invokeCustom(ConvertedMap.java:46)
	at org.codehaus.groovy.runtime.ConversionHandler.invoke(ConversionHandler.java:82)
	at $Proxy4.toString(Unknown Source)
	at org.codehaus.groovy.runtime.InvokerHelper.format(InvokerHelper.java:550)
	at org.codehaus.groovy.runtime.InvokerHelper.format(InvokerHelper.java:504)
	at org.codehaus.groovy.runtime.InvokerHelper.toArrayString(InvokerHelper.java:689)
	at org.codehaus.groovy.runtime.InvokerHelper.toString(InvokerHelper.java:109)
	at groovy.lang.MissingMethodException.getMessage(MissingMethodException.java:55)
	at java.lang.Throwable.getLocalizedMessage(Throwable.java:267)
	at java.lang.Throwable.toString(Throwable.java:343)
	at java.lang.String.valueOf(String.java:2826)
	at java.lang.StringBuilder.append(StringBuilder.java:115)
	at groovy.ui.GroovyMain.run(GroovyMain.java:340)
	at groovy.ui.GroovyMain.process(GroovyMain.java:315)
	at groovy.ui.GroovyMain.processArgs(GroovyMain.java:112)
	at groovy.ui.GroovyMain.main(GroovyMain.java:93)
	... 6 more
{code}

",guillaume,olandere,Major,Closed,Fixed,01/Jun/11 15:16,05/Apr/15 14:44
Bug,GROOVY-4861,12815518,Duplicate entry in InnerClasses,"For this code:
{code}
class Foo {
    static interface A{}
    static interface B{} // (1)
    static class Inner<X> {}
    static Inner<A> method() {} // (2)
}

println new Foo()
{code}
This error is received:
{noformat}
java.lang.ClassFormatError: Duplicate entry in InnerClasses in class file Foo
{noformat}

Removing either the declaration of {{B}} at (1) or the Generics attribute {{<A>}} on the type {{Inner}} at (2) will avoid the error.
",paulk,paulk,Major,Closed,Fixed,02/Jun/11 15:35,22/May/22 16:05
Bug,GROOVY-4863,12815681,1.8 incompatibility with named parameters,"See http://groovy.markmail.org/message/wuskviqggfgzos7r?q=1%2E8+incompatibility&page=1

I have methods that support named parameters via a Map and positional method with the same name. Works on 1.7.x but gives stack overflow error on 1.8.

{noformat}
foo('xxxx')

def foo(final Map map) {
    println map
}

def foo(final name) {
    return foo(name: name)
}
{noformat}

{noformat}
imac:info bob$ groovy test18.groovy
Caught: java.lang.StackOverflowError
	at test18.foo(test18.groovy)
	at test18.foo(test18.groovy:8)
	at test18.foo(test18.groovy:8)
	at test18.foo(test18.groovy:8)

{noformat}",blackdrag,bob.swift,Blocker,Closed,Fixed,06/Jun/11 06:35,29/Aug/11 13:46
Bug,GROOVY-4864,12816223,"HTML builder for ""textarea"" creates empty ""textarea"" element if no text specified, which is not alloed in HTML","Groovy's HTMLBuilder creates an empty ""textarea"" element (i.e. inserts the string ""<textarea />"" instead of ""<textarea></textarea>"") if no text has been specified for the textarea builder.

 The HTML spec is not fully clear on whether an empty element is allowed for ""textarea"":

      http://www.w3.org/TR/html401/interact/forms.html#h-17.7

 But one might want to read this about empty elements in HTML:

      http://www.cs.tut.fi/~jkorpela/html/empty.html


Firefox 3.6.17 deals badly with this (all the stuff following ""<textarea />"" is interpreted as the text of the textarea)


Workaround:

Instead of

   textarea(cols: 40, rows:5, name:'message')       --> <textarea />

Used

   textarea(cols: 40, rows:5, name:'message', """")   --> <textarea></textarea>


",paulk,yatima,Major,Closed,Fixed,06/Jun/11 07:11,21/Jul/11 19:06
Bug,GROOVY-4866,12815818,Documentation incorrect for Object#with,"The documentation for Object#with suggests using it to initialize objects, but the examples it provides are not correct. They assume that #with returns the object, when it actually returns the result of the closure.

The first example works because StringBuilder#append serendipitously returns the object, and the closure returns the result of the last #append.

{code}
def b = new StringBuilder().with {
  append('foo')
  append('bar')
}

assert b instanceof StringBuilder
{code}

The second example does not work correctly, since the closure returns the result of the assignment to the lastName property.

{code}
class Person { def firstName, lastName }

def p = new Person().with {
   firstName = 'John'
   lastName = 'Doe'
}

assert p instanceof Person
{code}

Modifying the examples so that the closures explicitly return the object would make the documentation correct.

{code}
def b = new StringBuilder().with {

  append('foo')
  append('bar')
  
  return it
}

assert b instanceof StringBuilder

class Person { def firstName, lastName }

def p = new Person().with {

  firstName = 'John'
  lastName = 'Doe'
  
  return it
}

assert p instanceof Person
{code}",guillaume,justin.piper@gmail.com,Minor,Closed,Fixed,06/Jun/11 13:24,21/Jul/11 19:06
Bug,GROOVY-4881,12818084,JsonSlurper does not handle backslash,"The JsonSlurper does not appear to handle backslash characters (\) even if they have been properly escaped.

Using this sample pieced together from the Groovy 1.8.0 Release Notes:

{code}
import groovy.json.*

def json = new JsonBuilder()

json.person {
    name ""Guill\\aume""
    age 33
    pets ""Hector"", ""Felix""
}

def jsonstring = json.toString()
println jsonstring

def slurper = new JsonSlurper()
def doc = slurper.parseText(jsonstring)

println doc
{code}

$ groovy json.groovy 
{noformat}
{""person"":{""name"":""Guill\\aume"",""age"":33,""pets"":[""Hector"",""Felix""]}}
Caught: groovy.json.JsonException: Expected a value on line: 1, column: 69.
But got an unterminated object.
        at json.run(json.groovy:15)
{noformat}

Have tried this with 1.8.0, 1.8.1 Snapshot (2011-06-13), 1.9.0 Snapshot (2011-06-13)",guillaume,mattmichalowski,Major,Closed,Fixed,14/Jun/11 00:23,21/Jul/11 19:06
Bug,GROOVY-4882,12815896,Concatenating an empty list to a GString throws a MethodSelectionException ,"A MethodSelectionException is thrown when appending an empty list to a GString that contains a ${} expression.  Note that no exception is thrown if the GString does not contain a ${} expression or if the list is nonempty.  The exception thrown is:

{code}Caught: org.codehaus.groovy.runtime.metaclass.MethodSelectionException: Could not find which method plus() to invoke from this list:
  public groovy.lang.GString groovy.lang.GString#plus(groovy.lang.GString)
  public groovy.lang.GString groovy.lang.GString#plus(java.lang.String)
{code}

No other concatenation I've tried has this behavior.  So the following line throws the exception:

{code}""GString concat ${'a'}"" + []{code}

None of the following throw anything:

{code}
""GString concat ${'a'}"" + ['abc']  // Concatenating a nonempty list
""GString concat "" + []             // Concatenating an empty list, but not using ${} in the GString

// Concatenating various other object types
['abc', '', null, [:], 0, 1, false, new Object[0]].each { ""GString concat ${'a'}"" + it }
{code}

I would expect the behavior of appending an empty list to be consistent with appending any other list (or object type).",paulk,mjjustin,Minor,Closed,Fixed,14/Jun/11 17:33,31/Jan/12 23:04
Bug,GROOVY-4884,12815719,Calling a method overwritten via metaClass from another method uses the original (non-overwritten) method,"Having the following code...

{code}
class A {
    String m1() { println 'called m1'; 'm1' }
    String m2() { println 'called m2'; println m1(); 'm2' }
}
a = new A()                                                                                                   
a.metaClass.m1 = { -> println 'pwned m1'; 'o-m1' }                                                            
{code}

Calling a.m2() shows...
{noformat}
called m2
called m1
m1
===> m2
{noformat}

...but expected is this:
{noformat}
called m2
pwned m1
o-m1
===> m2
{noformat}

The correct output is shown in Groovy 1.7.x, only Groovy 1.8.0 shows the wrong one.",blackdrag,rene.scheibe,Major,Closed,Fixed,16/Jun/11 05:23,26/Jul/11 19:44
Bug,GROOVY-4887,12818091,New Compile failure when upgrading from 1.7.1 to 1.8.0,"When recompiling after switching from 1.7.1 to 1.8.1 I see the following compiler bug error. Do you have suggestions for a fix or a work around?  I don't really understand what the underlying issue is:

{noformat}
BUG! exception in phase 'class generation' in source unit '/home/bidsjagu/java/amsl/eomssim/bids/tools/eomssim/groovy/engine/test/com/dark/eoms/sim/engine/fixture/TestCRDConnector.groovy' Type is null. Most probably you let a transform reuse existing ClassNodes with generics information, that is now used in a wrong context.
	at org.codehaus.groovy.vmplugin.v5.Java5.configureType(Java5.java:96)
	at org.codehaus.groovy.vmplugin.v5.Java5.configureTypeArguments(Java5.java:157)
	at org.codehaus.groovy.vmplugin.v5.Java5.configureParameterizedType(Java5.java:137)
	at org.codehaus.groovy.vmplugin.v5.Java5.configureType(Java5.java:88)
	at org.codehaus.groovy.vmplugin.v5.Java5.makeClassNode(Java5.java:383)
	at org.codehaus.groovy.vmplugin.v5.Java5.makeParameter(Java5.java:402)
	at org.codehaus.groovy.vmplugin.v5.Java5.makeParameters(Java5.java:395)
	at org.codehaus.groovy.vmplugin.v5.Java5.configureClassNode(Java5.java:340)
	at org.codehaus.groovy.ast.ClassNode.lazyClassInit(ClassNode.java:264)
	at org.codehaus.groovy.ast.ClassNode.getUnresolvedSuperClass(ClassNode.java:974)
	at org.codehaus.groovy.ast.ClassNode.getUnresolvedSuperClass(ClassNode.java:969)
	at org.codehaus.groovy.ast.ClassNode.getSuperClass(ClassNode.java:963)
	at org.codehaus.groovy.ast.ClassNode.isDerivedFrom(ClassNode.java:906)
	at org.codehaus.groovy.classgen.asm.OperandStack.doConvertAndCast(OperandStack.java:290)
	at org.codehaus.groovy.classgen.asm.OperandStack.doAsType(OperandStack.java:274)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitCastExpression(AsmClassGenerator.java:630)
	at org.codehaus.groovy.ast.expr.CastExpression.visit(CastExpression.java:66)
	at org.codehaus.groovy.classgen.asm.BinaryExpressionHelper.evaluateEqual(BinaryExpressionHelper.java:279)
	at org.codehaus.groovy.classgen.asm.BinaryExpressionHelper.eval(BinaryExpressionHelper.java:68)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBinaryExpression(AsmClassGenerator.java:526)
	at org.codehaus.groovy.ast.expr.BinaryExpression.visit(BinaryExpression.java:49)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:599)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeExpressionStatement(OptimizingStatementWriter.java:323)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitExpressionStatement(AsmClassGenerator.java:460)
	at org.codehaus.groovy.ast.stmt.ExpressionStatement.visit(ExpressionStatement.java:40)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:80)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeBlockStatement(OptimizingStatementWriter.java:151)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:406)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:289)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:268)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:366)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1056)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:174)
	at org.codehaus.groovy.control.CompilationUnit$13.call(CompilationUnit.java:763)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:957)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:542)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:520)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:497)
	at org.jetbrains.groovy.compiler.rt.GroovyCompilerWrapper.compile(GroovyCompilerWrapper.java:43)
	at org.jetbrains.groovy.compiler.rt.GroovycRunner.main(GroovycRunner.java:128)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.intellij.rt.execution.CommandLineWrapper.main(CommandLineWrapper.java:75)
{noformat}",blackdrag,jay.guidos,Major,Closed,Fixed,17/Jun/11 11:21,01/Apr/13 05:56
Bug,GROOVY-4888,12815742,groovy console script execution output appears in all output windows,"Having two open windows in groovy console. Executing a script in one of the windows will result in having the output in all output windows.
Expected: output appears only in the output window belonging to the script that was executed",pascalschumacher,ingorichter,Minor,Closed,Fixed,17/Jun/11 17:30,22/Feb/16 20:48
Bug,GROOVY-4890,12815574,MapExpression#getText incorrect for empty map,"For an empty map - e.g.
{code}
a = [:]
{code}
MapExpression#getText returns {{[]}} but should return {{[:]}}.

",paulk,rene.scheibe,Trivial,Closed,Fixed,20/Jun/11 06:35,05/Apr/15 14:44
Bug,GROOVY-4891,12818511,groovyConsole throws an exception when opening the AST browser ,"Running ""Inspect Ast"" (Ctrl-T) throws the below exception. The AST browser still opens and looks like usable. This only happens in Groovy 1.8.

{noformat}
Exception in thread ""Thread-4"" 
org.codehaus.groovy.runtime.InvokerInvocationException: java.lang.LinkageError: loader (instance of  org/codehaus/groovy/tools/RootLoader): attempted  duplicate class definition for name: ""groovy/grape/GrabAnnotationTransformation""
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:97)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:885)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:929)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:885)
	at groovy.lang.Closure.call(Closure.java:405)
	at groovy.lang.Closure.call(Closure.java:399)
	at groovy.lang.Closure.run(Closure.java:483)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.LinkageError: loader (instance of  org/codehaus/groovy/tools/RootLoader): attempted  duplicate class definition for name: ""groovy/grape/GrabAnnotationTransformation""
	at java.lang.ClassLoader.defineClass1(Native Method)
	at java.lang.ClassLoader.defineClassCond(ClassLoader.java:632)
	at java.lang.ClassLoader.defineClass(ClassLoader.java:616)
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)
	at java.net.URLClassLoader.defineClass(URLClassLoader.java:283)
	at java.net.URLClassLoader.access$000(URLClassLoader.java:58)
	at java.net.URLClassLoader$1.run(URLClassLoader.java:197)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)
	at org.codehaus.groovy.tools.RootLoader.oldFindClass(RootLoader.java:152)
	at org.codehaus.groovy.tools.RootLoader.loadClass(RootLoader.java:124)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:296)
	at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:696)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.addPhaseOperationsForGlobalTransforms(ASTTransformationVisitor.java:285)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.doAddGlobalTransforms(ASTTransformationVisitor.java:266)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.addGlobalTransforms(ASTTransformationVisitor.java:187)
	at org.codehaus.groovy.transform.ASTTransformationVisitor.addPhaseOperations(ASTTransformationVisitor.java:150)
	at org.codehaus.groovy.control.CompilationUnit.<init>(CompilationUnit.java:188)
	at org.codehaus.groovy.control.CompilationUnit.<init>(CompilationUnit.java:120)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)
	at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at org.codehaus.groovy.reflection.CachedConstructor.invoke(CachedConstructor.java:77)
	at org.codehaus.groovy.runtime.callsite.ConstructorSite$ConstructorSiteNoUnwrapNoCoerce.callConstructor(ConstructorSite.java:102)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:54)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:182)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:198)
	at groovy.inspect.swingui.AstNodeToScriptAdapter.compileToScript(AstNodeToScriptAdapter.groovy:90)
	at groovy.inspect.swingui.AstNodeToScriptAdapter$compileToScript.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at groovy.inspect.swingui.AstBrowser$_decompile_closure6.doCall(AstBrowser.groovy:269)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	... 9 more
{noformat}

I also had problems that the tree view in the AST browser was not showing anything due to NoClassDefFound execptions for Swing classes but I cannot reproduce this at the moment.",,rene.scheibe,Major,Closed,Fixed,20/Jun/11 09:14,25/Aug/13 16:59
Bug,GROOVY-4892,12818115,"GroovyInterceptable / invokeMethod does not correctly in Groovy 1.8.0 when Methods with no arguments are ""delegated""","please compare
http://stackoverflow.com/questions/6419674/groovyinterceptable-does-not-work-on-methods-with-no-arguments

the following class gives the correct output in Groovy < 1.8, in 1.8 it only works if a parameter is added to the nested method.

A workaround is mentioned in the stackoverflow link.
{code:Java}
class SimpleFlow implements GroovyInterceptable {

    def invokeMethod(String name, args) {
        System.out.println(""time before ${name} called: ${new Date()}"")

        def calledMethod = SimpleFlow.metaClass.getMetaMethod(name, args)
        calledMethod?.invoke(this, args)

        System.out.println(""time after ${name} called: ${new Date()}\n"")
    }

    void simpleMethod1(){
        System.out.println(""simpleMethod1() called"")
        simpleMethod2Nested()
    }

    // works well when using an argument
    void simpleMethod2Nested(){
        System.out.println(""simpleMethod2Nested"")
    }

    public static void main(String[] args) {
        def flow = new SimpleFlow()
        flow.simpleMethod1()
    }
}
{code}
actual output:

time before simpleMethod1 called: Tue Jun 21 04:16:41 CEST 2011
simpleMethod1() called
simpleMethod2Nested
time after simpleMethod1 called: Tue Jun 21 04:16:41 CEST 2011

estimated output:
time before simpleMethod1 called: Tue Jun 21 13:32:44 GMT+08:00 2011
simpleMethod1() called
time before simpleMethod2Nested called: Tue Jun 21 13:32:44 GMT+08:00 2011
simpleMethod2Nested
time after simpleMethod2Nested called: Tue Jun 21 13:32:44 GMT+08:00 2011
time after simpleMethod1 called: Tue Jun 21 13:32:44 GMT+08:00 2011",blackdrag,radgon,Major,Closed,Fixed,21/Jun/11 05:16,15/Jul/11 14:38
Bug,GROOVY-4896,12818508,Anonymous inner class failure,"Hi all, 

I found some strange behaviour using anonymous inner classes.

If I declare (and use) an anonymous inner class inside a groovy closure inside a method, AND the anonymous inner class uses at least one of the method's input parameters, I get the following exception:

groovy.lang.GroovyRuntimeException: Could not find matching constructor for: [className]$[closureNumber]([parameters])

{code}
    void doSomethingUsingParam(final String s1){ // This always fails
        logExceptions {
            println ""doing something (expect exception)...""
            Runnable ifA = new Runnable(){
                        void run(){
                            println s1
                        }
                    }
            ifA.run()
        }
    }

    def logExceptions(Closure c){
        try{
            return c.call()
        } catch (Throwable e){
            e.printStackTrace();
        }
    }
}
{code}

I think it's a bug because the same code runs smoothly if, instead of the method input parameter, I use a local variable inside the groovy closure (the local variable doesn't even need to be declared as ""final"").

{code}
void doSomethingUsingParamWorkaround(final String s2){
    logExceptions {
        println ""doing something...""
        String s1=s2 // Too Ungroovy IMHO
        Runnable ifA = new Runnable(){
                    void run(){
                        println s1
                    }
                }
        ifA.run()
    }
}
{code}

A more complete set of variants of the above methods is attached
",blackdrag,mrctrevisan,Major,Closed,Fixed,23/Jun/11 10:35,15/Oct/15 18:16
Bug,GROOVY-4898,12815736,groovydoc fails with MissingPropertyException,"I got the following error when executing ""ant dist"" to build groovy:

    [java] [groovydoc] groovy.lang.MissingPropertyException: No such property: version for class: org.codehaus.groovy.runtime.InvokerHelper


The patch is to use GroovySystem.getVersion() instead of InvokerHelper.getVersion(). ",paulk,nagai_masato,Major,Closed,Fixed,24/Jun/11 07:48,05/Apr/15 14:44
Bug,GROOVY-4902,12815753,Groovy class not compatible with Java integration when @Immutable is used,"The following Groovy class with the {{UsePerson.java}} below works well:
{code}
class Person {
  String firstName
  public Person(first) { firstName = first }
}
{code}
Defining a class with @Immutable annotation seems to result in compilation error when used from Java.

{code:title=Person.groovy}
@Immutable class Person {
  String firstName
}
{code}
{code:title=UsePerson.java}
public class UsePerson {
  public static void main(String[] args) {
    Person person = new Person(""Sam"");
    System.out.println(person.getFirstName());
  }
}
{code}
{noformat}
>groovyc Person.groovy

>ls
Person.class	Person.groovy	UsePerson.java

>javac UsePerson.java 
UsePerson.java:3: cannot access Person
bad class file: ./Person.class
undeclared type variable: K
Please remove or make sure it appears in the correct subdirectory of the classpath.
    Person person = new Person(""Sam"");
    ^
1 error
{noformat}
",paulk,venkats,Major,Closed,Fixed,26/Jun/11 11:05,21/Jul/11 19:06
Bug,GROOVY-4903,12818409,StackOverflowError when parsing JSON text with JsonSlurper().parseText(),"The following code throws java.lang.StackOverflowError

{code}
def jsonData = '{""messageId"":1008,""messages"":[{""id"":""1005"",""urgency"":""info"",""senderName"":""Evgeny Goldin"",""date"":""Saturday, June 25, 2011"",""time"":""09:08"",""text"":""aaaaaaaaaaa"",""sender"":""evgenyg"",""timestamp"":""1309003733376"",""longevity"":168,""sendToAll"":true,""sendToGroups"":[],""sendToUsers"":[],""usersDeleted"":[""evgenyg""]},{""id"":""1006"",""urgency"":""critical"",""senderName"":""Evgeny Goldin"",""date"":""Saturday, June 25, 2011"",""time"":""09:11"",""text"":""eeeeeeeeeeee"",""sender"":""evgenyg"",""timestamp"":""1309003888160"",""longevity"":168,""sendToAll"":true,""sendToGroups"":[],""sendToUsers"":[],""usersDeleted"":[""evgenyg""]},{""id"":""1007"",""urgency"":""warning"",""senderName"":""Evgeny Goldin"",""date"":""Saturday, June 25, 2011"",""time"":""09:14"",""text"":""<p>Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo. Quisque sit amet est et sapien ullamcorper pharetra. Vestibulum erat wisi, condimentum sed, commodo vitae, ornare sit amet, wisi. Aenean fermentum, elit eget tincidunt condimentum, eros ipsum rutrum orci, sagittis tempus lacus enim ac dui. Donec non enim in turpis pulvinar facilisis. Ut felis. Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat. Aliquam erat volutpat. Nam dui mi, tincidunt quis, accumsan porttitor, facilisis luctus, metus</p>"",""sender"":""evgenyg"",""timestamp"":""1309004072795"",""longevity"":168,""sendToAll"":true,""sendToGroups"":[],""sendToUsers"":[],""usersDeleted"":[""evgenyg""]},{""id"":""1008"",""urgency"":""warning"",""senderName"":""Evgeny Goldin"",""date"":""Saturday, June 25, 2011"",""time"":""09:16"",""text"":""<p>Pellentesque habitant morbi tristique senectus et netus et malesuada fames ac turpis egestas. Vestibulum tortor quam, feugiat vitae, ultricies eget, tempor sit amet, ante. Donec eu libero sit amet quam egestas semper. Aenean ultricies mi vitae est. Mauris placerat eleifend leo. Quisque sit amet est et sapien ullamcorper pharetra. Vestibulum erat wisi, condimentum sed, commodo vitae, ornare sit amet, wisi. Aenean fermentum, elit eget tincidunt condimentum, eros ipsum rutrum orci, sagittis tempus lacus enim ac dui. Donec non enim in turpis pulvinar facilisis. Ut felis. Praesent dapibus, neque id cursus faucibus, tortor neque egestas augue, eu vulputate magna eros eu erat. Aliquam erat volutpat. Nam dui mi, tincidunt quis, accumsan porttitor, facilisis luctus, metus</p>"",""sender"":""evgenyg"",""timestamp"":""1309004197803"",""longevity"":168,""sendToAll"":true,""sendToGroups"":[],""sendToUsers"":[],""usersDeleted"":[""evgenyg""]}]}'

new groovy.json.JsonSlurper().parseText( jsonData )

{code}


{code}
Exception thrown
Jun 26, 2011 3:18:23 PM org.codehaus.groovy.runtime.StackTraceUtils sanitize

WARNING: Sanitizing stacktrace:

java.lang.StackOverflowError

	at java.util.regex.Pattern$8.isSatisfiedBy(Pattern.java:4783)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at jav
a.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern
$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.regex.Pattern$CharProperty.match(Pattern.java:3345)

	at java.util.regex.Pattern$Branch.match(Pattern.java:4114)

	at java.util.regex.Pattern$GroupHead.match(Pattern.java:4168)

	at java.util.regex.Pattern$Loop.match(Pattern.java:4295)

	at java.util.regex.Pattern$GroupTail.match(Pattern.java:4227)

	at java.util.regex.Pattern$BranchConn.match(Pattern.java:4078)

	at java.util.re
{code}",guillaume,genie,Major,Closed,Fixed,26/Jun/11 13:22,30/Jun/12 19:00
Bug,GROOVY-4907,12816763,Command line interface of AstNodeToScriptAdapter.groovy don't works,"I tried AstNodeToScriptAdapter#main() but get 
{code}
$ groovy -e 'groovy.inspect.swingui.AstNodeToScriptAdapter.main(args)' a.groovy  1
Caught: groovy.lang.MissingMethodException: No signature of method: static org.codehaus.groovy.control.CompilePhase.fromPhaseNumber() is applicable for argument types: (java.lang.String) values: [1]
Possible solutions: fromPhaseNumber(int), getPhaseNumber()
	at groovy.inspect.swingui.AstNodeToScriptAdapter.main(AstNodeToScriptAdapter.groovy:56)
	at script_from_command_line.run(script_from_command_line:1)
{code}
follwing is a patch:
{code}
$ svn diff AstNodeToScriptAdapter.groovy
Index: AstNodeToScriptAdapter.groovy
===================================================================
--- AstNodeToScriptAdapter.groovy	(revision 22473)
+++ AstNodeToScriptAdapter.groovy	(working copy)
@@ -52,7 +52,7 @@
 and [compilephase] is a valid Integer based org.codehaus.groovy.control.CompilePhase""""""
         } else {
             def file = new File((String) args[0])
-            def phase = CompilePhase.fromPhaseNumber(args[1])
+            def phase = CompilePhase.fromPhaseNumber(args[1] as int)
             if (!file.exists()) {
                 println ""File ${args[0]} cannot be found.""
             } else if (phase == null) {
{code}",paulk,uehaj,Major,Closed,Fixed,30/Jun/11 03:12,05/Apr/15 14:43
Bug,GROOVY-4909,12818406,insufficient compiler error messages relating to interface member visibility,"The following interface illustrates two problems:
{code}
interface Foo {
  private static BAR = 3 // => java.lang.ClassFormatError: Illegal field modifiers in class Foo: 0x1A
  protected foo() // => java.lang.ClassFormatError: Method foo in class Foo has illegal modifiers: 0x404
}
{code}",paulk,paulk,Major,Closed,Fixed,01/Jul/11 18:16,22/May/22 16:05
Bug,GROOVY-4910,12814465,groovy.bat fails when the JAVA_OPTS contains space characters,"groovy.bat fails when the JAVA_OPTS environment variable contains space characters like the following:

> set JAVA_OPTS=-Daprop=""a value""
> bin\groovy -h
value"""" was unexpected at this time.",paulk,nagai_masato,Major,Closed,Fixed,02/Jul/11 06:04,09/Apr/15 13:42
Bug,GROOVY-4914,12815765,FieldNode doesn't collect annotations,"It's in the test case:
{code}
import org.codehaus.groovy.ast.ClassNode
import java.lang.annotation.RetentionPolicy
import java.lang.annotation.Retention


@Retention(RetentionPolicy.RUNTIME)
@interface Annotation1 {}

@Annotation1 class A {
    @Annotation1 def field1    
}


new ClassNode(A).with {
    assert annotations: ""ClassNode for class 'A' has an annotation as it should""
    getField('field1').with {
        assert !annotations: ""Because of a bug FieldNode for field 'field1' doesn't have annotations""
    }
}
{code}",paulk,mojojojo,Major,Closed,Fixed,06/Jul/11 13:29,21/Jul/11 19:06
Bug,GROOVY-4915,12815741,Wrong access modifiers in JsonSlurper,"JsonSlurper.java has parseText() and parse() with package-private instead of public visibility.

IntelliJ rightfully reports a warning when trying to use them.",guillaume,axel.fontaine,Major,Closed,Fixed,07/Jul/11 07:25,21/Jul/11 19:06
Bug,GROOVY-4916,12815749,Set equality fails if null elements are present,"The following assertions work as expected in groovy 1.7.10, but fail in groovy 1.8.0

First case:
{code}
def foo = [ ""a"", null ] as Set
assert foo.equals( foo )
{code}

Second case:
{code}
def foo = [ ""a"", null ] as Set
def bar = [ ""a"", null ] as Set
assert foo == bar
assert foo.equals( bar )
{code}

foo == foo happens to work, which leads me to believe that == tests for object identity first.

I can workaround by testing Set equality using brute force, but it's very surprising.
",paulk,rconner,Major,Closed,Fixed,08/Jul/11 12:32,21/Jul/11 19:06
Bug,GROOVY-4918,12815723,GetEffectivePojoPropertySite#acceptGetProperty may produce a NullPointerException when provided with a null 'receiver' object,"When given a null {{receiver}} object, line 51 of {{org.codehaus.groovy.runtime.callsite.GetEffectivePojoPropertySite}} may produce a {{NullPointerException}} when trying to invoke {{java.lang.Object#getClass}}.
It seems that the commented-out if statement on line 50 handles this case by checking the {{receiver}} object with {{instanceof}}.
Although if fixed, a {{NullPointerException}} will eventually be thrown by the {{NullCallSite}} object when trying to access a property of the null object, the exception thrown by {{NullCallSite}} is much more descriptive.",blackdrag,noamt,Major,Closed,Fixed,11/Jul/11 07:38,23/Feb/15 01:12
Bug,GROOVY-4919,12818094,Categories sometimes not applied,"Since upgrading http://media.io to Groovy 1.8.0, I very rarely get these exceptions:

{code}

groovy.lang.MissingMethodException: No signature of method: java.lang.String.shaHex() is applicable for argument types: () values: []
Possible solutions: size(), charAt(int), isCase(java.lang.Object), isCase(java.lang.Object), sleep(long), sleep(long, groovy.lang.Closure)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:55)
	at org.codehaus.groovy.runtime.callsite.PojoMetaClassSite.call(PojoMetaClassSite.java:46)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:112)
	at media.entities.Info$_getCleanName_closure1.doCall(Info.groovy:87)
{code}

The code in Info.groovy uses categories together with commons-codec and looks like this:

{code}
@EqualsAndHashCode(excludes = 'size,outputName,outputSize,format,progress,downloaded,tags')
class Info implements Serializable {
    
    private static final long serialVersionUID = 3L
    
    String name
...
    String getCleanName(String extension) {
        use (DigestUtils) { extension ? new StringBuilder(49).append(name.shaHex()).append('.').append(extension) : name.shaHex() }
    }
    
}
{code}

My guess is that this happens less frequently than every 50000 calls so it's hard for me to reproduce.

Any ideas? Could this be a thread-safety thing?

",,johann,Minor,Closed,Fixed,11/Jul/11 12:04,16/Nov/14 07:23
Bug,GROOVY-4920,12815785,Default values for primitives error in groovy 1.8.0,"In Groovy 1.8.0, this works fine:
{noformat}
groovy:000>  int x; println x
0
===>  null
{noformat}

Whereas this blows up:

{noformat}
groovy:000>  int x; 1.times { println x }
ERROR org.codehaus.groovy.runtime.typehandling.GroovyCastException:
Cannot cast object 'null' with class 'null' to class 'int'. Try
'java.lang.Integer' instead
        at groovysh_evaluate.run (groovysh_evaluate:2)
        ...
{noformat}
Is that a known issue?

bug and not known, because new in 1.8. Please fill a bug report for this

bye blackdrag",blackdrag,shogusumi,Major,Closed,Fixed,11/Jul/11 12:13,15/Jul/11 11:47
Bug,GROOVY-4922,12815873,StackOverflowError when calling super and overriding a package protected java method,"For reference : http://groovy.329449.n5.nabble.com/StackOverflowError-when-dispatching-to-super-td4572268.html

If a package protected method written in a Java class is overriden in a Groovy class and that method calls super.method(), Groovy throws a stack overflow :

{code:title=Parent.java}
class Parent {
   void someMethod(String param) { ... }
}
{code}

{code:title=Child.groovy}
class Child {
   void someMethod(String param) { super.someMethod(param) }
}
{code}

{code}
java.lang.StackOverflowError
	at java.lang.Exception.<init>(Exception.java:77)
	at java.lang.reflect.InvocationTargetException.<init>(InvocationTargetException.java:54)
	at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1054)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.invokeMethodOnSuperN(ScriptBytecodeAdapter.java:128)
	at groovy.bugs.GroovyStackOverflowBug$Child.someMethod(GroovyStackOverflowBug.groovy:39)
	at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1054)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.invokeMethodOnSuperN(ScriptBytecodeAdapter.java:128)
	at groovy.bugs.GroovyStackOverflowBug$Child.someMethod(GroovyStackOverflowBug.groovy:39)
	at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1054)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.invokeMethodOnSuperN(ScriptBytecodeAdapter.java:128)
	at groovy.bugs.GroovyStackOverflowBug$Child.someMethod(GroovyStackOverflowBug.groovy:39)
	at sun.reflect.GeneratedMethodAccessor1.invoke(Unknown Source)
...
{code}

Adding a public or protected modifier to the super class solves the problem.",melix,melix,Major,Closed,Fixed,13/Jul/11 14:55,13/May/12 03:30
Bug,GROOVY-4923,12815735,Creating Pattern from declared String variable  with pattern = ~stringvariable fails,"An attempt to create a java.util.regex.Pattern from a variable that is declared as a String fails with a java.lang.ClassCastException: java.util.regex.Pattern cannot be cast to java.lang.String

The code below worked with Groovy 1.7.8 but fails with Groovy 1.8.0
String value=""test""
String s = ""^\\S+$value\$""
def p = ~s <==Fails here

A workaround is to change the last statement to:
def p = ~/$s/
but that adds clutter
",blackdrag,ola.mattsson,Major,Closed,Fixed,14/Jul/11 12:03,14/Jul/11 16:20
Bug,GROOVY-4924,12815754,Class implementing a generic interface causes VerifyError,"This script:
{code}
interface Interface<SomeType> {
   public void handle(long a, SomeType x);
}

class InterfaceImpl
implements Interface<String> {
    public void handle(long a, String something) {
        println(something);
    }
}

InterfaceImpl test = new InterfaceImpl()
test.handle(5, ""hi"");
{code}
causes:

{code}
Caught: java.lang.VerifyError: (class: InterfaceImpl, method: handle signature: (JLjava/lang/Object;)V) Register 2 contains wrong type
        at Test.class$(Test.groovy)
        at Test.$get$$class$InterfaceImpl(Test.groovy)
        at Test.run(Test.groovy:12)
{code}",blackdrag,dvekhter,Major,Closed,Fixed,14/Jul/11 13:17,14/Jul/11 15:26
Bug,GROOVY-4934,12816512,incorrect signature attributes in class files for inner class generics,"When an inner type shares state with its outer type, and that shared state is generic, the signature attributes generated inside the inner class are not correct.  This is not normally a problem because once created they are rarely processed again, however, Eclipse checks these things and discovers the inconsistencies, reporting an 'inconsistent class file' error when it sees a type variable referenced that is not in scope.

Here is a simple groovy program:
{code}
import groovy.lang.Closure

import java.io.Serializable
import java.util.Map

class GormInstanceApi<D> {

	void delete(D instance) {
		new VoidSessionCallback() {
			void doInSession() {
				print instance
			}
		}
	}

}

interface VoidSessionCallback {
	void doInSession();
}
{code}

When this gets compiled, a field of type Reference is created in the inner type (in the InnerClassVisitor.visitConstructorCallExpression()) code, and this is added to the constructor.  The signature attribute for the new field and the constructor of the inner type encodes a generic reference that isn't quite right - it refers to a type variable that isn't in scope.  Here is the generic signature attribute for the constructor:

 (LGormInstanceApi<TD;>;Lgroovy/lang/Reference<TT;>;)V;

The first bit is OK, the 'D' referred to is defined by the declaring type and so visible from the inner.  However the 'T' is not visible - I would have expected a reference to the raw type there:

(LGormInstanceApi<TD;>;Lgroovy/lang/Reference;)V;

Or, if it was being used in parameterized form:

(LGormInstanceApi<TD;>;Lgroovy/lang/Reference<Ljava/lang/Object;>;)V;

Similarly it is wrong for the field:

 Lgroovy/lang/Reference<TT;>;;

T is not in scope (not declared by this type or a surrounding type).

To see this problem in Eclipse, define a project containing that code above and then create a pure java project that depends upon it.  Add this file to the java project:
{code}
class Simple<D> extends GormInstanceApi<D> {
	 
	void foo() {	 
		new VoidSessionCallback() {
			public void doInSession() {
			}  
		} ;
	}    
}  
{code}

This code is enough to force JDT to load in the GormInstanceApi inner class and discover the inconsistency in signatures:

Inconsistent classfile encountered: The undefined type parameter T is referenced from within GormInstanceApi<D>.1	

I'd propose the type of the newly created field is set to the raw type for groovy.lang.Reference, but I don't know how to create a raw ClassNode.",blackdrag,aclement,Minor,Closed,Fixed,19/Jul/11 19:53,01/Feb/12 12:47
Bug,GROOVY-4935,12818099,stub generator loses package qualification for generic type argument,"In my groovy class, I have a method that returns a List<Year>, where <Year> is a java class in another package.  (Not sure whether the ""java"" part is relevant; it's what my code happened to do.  I also haven't checked whether this problem happens only with method return types, though I _think_ all the examples I saw had that property.)

{code}
import other_package.Year

class YearRange {
   List<Year> getYears() {
      return null
   }
}
{code}

When groovy creates the stub, however, it includes the full package name for List, but not for Year:

{noformat}
  [groovyc] C:\DOCUME~1\cyrus\LOCALS~1\Temp\groovy-generated-1209606927705701441-java-source\one_package\YearRange.java:18: cannot find symbol
  [groovyc] symbol  : class Year
  [groovyc] location: class one_package.YearRange
  [groovyc] public  java.util.List<Year> getYears() { return (java.util.List<Year>)null;}
  [groovyc]                        ^
  [groovyc] C:\DOCUME~1\cyrus\LOCALS~1\Temp\groovy-generated-1209606927705701441-java-source\one_package\YearRange.java:18: cannot find symbol
  [groovyc] symbol  : class Year
  [groovyc] location: class one_package.YearRange
  [groovyc] public  java.util.List<Year> getYears() { return (java.util.List<Year>)null;}
{noformat}

I have this problem with the following snapshot build:
groovy-all-1.8.1-SNAPSHOT.jar              20-Jul-2011 06:11  5.8M

I do not have this problem with groovy 1.8.0.",paulk,lgdean,Major,Closed,Fixed,20/Jul/11 12:13,22/Jul/11 16:56
Bug,GROOVY-4936,12815529,package protected method in Parent class cannot be called,"assuming we have {code:Java}
public class Parent{ 
  int someMethod(){return 1;}
}{code}written in Java and {code:Java}
public class Child extends Parent{
  int someMethod(){
    return super.someMethod()+2
  }
}{code} written in Groovy, then this results in a StackOverflow.",blackdrag,blackdrag,Major,Closed,Fixed,20/Jul/11 17:53,21/Jul/11 05:01
Bug,GROOVY-4937,12815766,Sorting an ObservableList instance results in IndexOutOfBoundsException,"Reported at the Griffon user ML

{code}
l = [1, 4, 2] as ObservableList
l.sort()
{code}

results in

{code}
java.lang.IndexOutOfBoundsException: Index: 3, Size: 3
	at java.util.ArrayList.RangeCheck(ArrayList.java:547)
	at java.util.ArrayList.set(ArrayList.java:337)
	at groovy.util.ObservableList.set(ObservableList.java:349)
	at groovy.util.ObservableList$ObservableListIterator.set(ObservableList.java:439)
	at java.util.Collections.sort(Collections.java:163)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.sort(DefaultGroovyMethods.java:5918)
	at org.codehaus.groovy.runtime.dgm$595.invoke(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:271)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:53)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:112)
	at ConsoleScript3.run(ConsoleScript3:2)
	at groovy.lang.GroovyShell.runScriptOrMainOrTestOrRunnable(GroovyShell.java:266)
	at groovy.lang.GroovyShell.run(GroovyShell.java:517)
	at groovy.lang.GroovyShell.run(GroovyShell.java:172)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:910)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:885)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:885)
	at groovy.lang.Closure.call(Closure.java:405)
	at groovy.lang.Closure.call(Closure.java:399)
	at groovy.lang.Closure.run(Closure.java:483)
	at java.lang.Thread.run(Thread.java:680)
{code}",paulk,aalmiray,Major,Closed,Fixed,21/Jul/11 07:21,07/Sep/11 14:13
Bug,GROOVY-4938,12815784,VerifyError related to byte arrays,"{noformat}
~ $ groovy --version
Groovy Version: 1.8.1 JVM: 1.6.0_26
~ $ groovy -e ""i = new int[42];println i[0];""
0
~ $ groovy -e ""int[] i = new int[42];println i[0];""
0
~ $ groovy -e ""b = new byte[42]; println b[0];""
0
~ $ groovy -e ""byte[] b = new byte[42]; println b[0];""
Caught: java.lang.VerifyError: (class: script_from_command_line, method: run signature: ()Ljava/lang/Object;) Incompatible argument to function
java.lang.VerifyError: (class: script_from_command_line, method: run signature: ()Ljava/lang/Object;) Incompatible argument to function
{noformat}
",blackdrag,brownj,Major,Closed,Fixed,21/Jul/11 12:29,23/Jul/11 04:54
Bug,GROOVY-4939,12818098,groovyc compilation problem for return values with generics,"Hi I have following code which was ok for groovy 1.8.0 but after updating to groovy 1.8.1 I have failed compilation 

{code:title=MessagingTask.groovy}
class MessagingTask<T extends Serializable> {

  private static final long serialVersionUID = -5533069690405593263L;

  Long id
  String messageId
  String inDestination
  String outDestination
  MessagingTaskType messageType
  T messagePayload

  public String toString() {
    return ""Task{"" +
            ""id="" + id +
            "", messageId='"" + messageId + '\'' +
            "", inDestination='"" + inDestination + '\'' +
            "", outDestination='"" + outDestination + '\'' +
            "", messageType='"" + messageType + '\'' +
            "", messagePayload="" + messagePayload +
            '}';
  }
}
{code}
{code:title=MessagingTaskDatabase.groovy}
class MessagingTaskDatabase {

    public List<MessagingTask<OutMessage>> loadOutMessage() {
        return new ArrayList<MessagingTask<OutMessage>>();
    }

}
{code}
{code:title=SomeClient.java}
List<MessagingTask<OutMessage>> messagingTasks = taskDatabase.loadOutMessage();
{code}
And the compile log:
{noformat}
    [mkdir] Created dir: /home/sargis/projects/twm_v3/trunk/twm_v3/accounting/Server/classes
  [groovyc] Compiling 185 source files to /home/sargis/projects/twm_v3/trunk/twm_v3/accounting/Server/classes
  [groovyc] org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
  [groovyc] Compile error during compilation with javac.
  [groovyc] /home/sargis/projects/twm_v3/trunk/twm_v3/accounting/Server/src/com/webbfontaine/twm/accounting/epaylog/scheduler/SendingScheduler.java:46: incompatible types
  [groovyc] found   : java.util.List<com.webbfontaine.twm.accounting.epaylog.emess.task.MessagingTask>
  [groovyc] required: java.util.List<com.webbfontaine.twm.accounting.epaylog.emess.task.MessagingTask<com.webbfontaine.twm.accounting.epaylog.emess.core.OutMessage>>
  [groovyc]         List<MessagingTask<OutMessage>> messagingTasks = taskDatabase.loadOutMessage();
  [groovyc]                                                                                     ^
  [groovyc] Note: Some input files use unchecked or unsafe operations.
  [groovyc] Note: Recompile with -Xlint:unchecked for details.
  [groovyc] 1 error
  [groovyc] 
  [groovyc] 
  [groovyc] 1 error
{noformat}

But if I remove generic type parameters from return and compilation is ok
{code}
public List loadOutMessage() {
    return new ArrayList<MessagingTask<OutMessage>>();
}
{code}
",paulk,armsargis,Major,Closed,Fixed,21/Jul/11 15:35,07/Sep/11 14:13
Bug,GROOVY-4940,12815771,Groovy Console Icon for OSX still broken,"The long serving bug GROOVY-4306 is still around.

Looking at the icns file using a hex editor (and comparing it to the one I attached to the GROOVY-4306 issue) seems to show lots of extra data being stuffed into the one distributed with the binary distribution...

It's like it's being converted to unicode or something?

The original icns file starts with the following 64 bytes:

{code}
69636E73 0001B17B 696C3332 00000A51
87FF0B53 2D110A05 0B0A0B0C 1E38EC8F
FF10551A 13294760 717A7F73 6948270B
2595C08A FF147F23 1C45718A 9EABB1B4
{code}

But the distributed one starts with:

{code}
69636E73 0001EFBF BD7B696C 33320000
0A51EFBF BDEFBFBD 0B532D11 0A050B0A
0B0C1E38 EFBFBDEF BFBD1055 1A132947
60717A7F 73694827 0B25EFBF BDEFBFBD
{code}

As you can see, the byte {{B1}} at position 7 in the original has been changed to {{EFBFBD}} in the distributed file -- and again, {{87FF}} at position 16 seems to be replaced with {{EFBFBDEFBFBD}}

The file size has increased as well, the original being 110971 bytes in length, but the distributed file is 214052 bytes",guillaume,tim_yates,Minor,Closed,Fixed,22/Jul/11 02:03,22/Jul/11 08:51
Bug,GROOVY-4942,12811912,groovy.bat can't run if PATH contains parentheses ,"JAVA_HOME not defined.

PATH contains "")"" character(Windows 7 default Path).
{code}
C:\Dev\Apps\groovy-1.8.0\bin>echo %PATH%
C:\Program Files\Common Files\Microsoft Shared\Windows Live;C:\Program Files (x8
6)\Common Files\Microsoft Shared\Windows Live;C:\windows\system32;C:\windows;
C:\Program Files\Java\jre7\bin
{code}

groovy and goovysh can't run.
{code}
C:\Dev\Apps\groovy-1.8.0\bin>groovy -v
\Common was unexpected at this time.
{code}

Windows batch command ""for"" can't treat a string contains "")"".
",paulk,aeffy,Major,Closed,Fixed,23/Jul/11 09:26,05/Apr/15 14:43
Bug,GROOVY-4944,12818509,"Presence of ""assert"" unexpectedly changes program semantics (in Groovlet)","In a Groovlet:

We have a function to map ""stuff"" to a boolean:
{code}
boolean mapBoolean(def datum, boolean defaultValue) {
   ......
   return res
}
{code}
Immediately after the above in the Groovlet source, the function is tested:
{code}
assert mapBoolean(null, false) == false
assert mapBoolean(null, null)  == false
{code}
One would expect the compiler to generate an error as 'null' cannot be passed as second parameter.

But the asserts above are accepted and pass!

If one removes the first assert and changes to solely
{code}
assert mapBoolean(null, null)  == false
{code}
THEN the compiler generates an error:
{code}
message GroovyServlet Error: script: 'foo': Script processing failed.No signature of method: com.mplify.interact.InspectMsg.mapBoolean() is applicable for argument types: (null, null) values: [null, null] Possible solutions: mapBoolean(java.lang.Object, boolean), asBoolean()org.codehaus.groovy.runtime.ScriptBytecodeAdapter.unwrap(ScriptBytecodeAdapter.java:54)
{code}

A totally bared-down test ""Simple.groovy"" would be:

{code}
boolean mapBoolean(def datum, boolean defaultValue) {
   return false
}

if (params['run']) {
   assert mapBoolean(null, false)  == false
}

assert mapBoolean(null, null) == false

html.html {
    head { title ""Testing"" }
    body { div ""PASSED"" }
}
{code}

Called with Simple.groovy?run=true ---> PASSED
Called with Simple.groovy          ---> Script processing failed.No signature of method: Simple.mapBoolean() is applicable for argument types: (null, null) ",blackdrag,yatima,Major,Closed,Fixed,25/Jul/11 05:50,25/Jul/11 14:33
Bug,GROOVY-4945,12818416,"Incorrect ""Possible solutions"" when calling a method on super in a non-derived class","h3. Calling an instance method on super in a non-derived class.

{code}
class T { void m() { super.m() } }
new T().m() 
{code}

This results in the below exception.

{noformat}
ERROR groovy.lang.MissingMethodException:
No signature of method: T.m() is applicable for argument types: () values: []
Possible solutions: m(), is(java.lang.Object), dump(), any(), any(groovy.lang.Closure), use([Ljava.lang.Object;)
{noformat}

This can be quite confusion as the called method on super is also listed in the possible solutions. The error message should be a different one.

h3. Calling a static method on super in a non-derived class.

{code}
class T { static void m() { super.m() } }
T.m() 
{code}

This results in the below stack overflow.

{noformat}
ERROR java.lang.StackOverflowError:
null
        at T.$getCallSiteArray (groovysh_evaluate)
        at T.m (groovysh_evaluate)
        at T.m (groovysh_evaluate:2)
        at T.m (groovysh_evaluate:2)
        ...
{noformat}",emilles,rene.scheibe,Minor,Closed,Fixed,25/Jul/11 06:09,23/Jul/20 14:16
Bug,GROOVY-4946,12818095,Groovy getAt cannot be used with lazily initialized lists,"I'm not sure if this should even be logged as a bug, but here goes:

I was playing around with the Apache Commons {{ListUtils.lazyList}}.  This list will automatically create an item if an index is not yet defined (or is {{null}}).  Example code:

{code:title=Example.groovy}
@Grab(group='commons-collections', module='commons-collections', version='[1.3,)')
import org.apache.commons.collections.ListUtils
import org.apache.commons.collections.Factory
import groovy.transform.Canonical

@Canonical class Test {
    String name
    int amount
}

List<Test> t = ListUtils.lazyList([], { new Test() } as Factory)

// UNCOMMENT to make the example work:
// t.get(1)

// Thows NPE here:
t[1].name = 'Jim'
t[0].amount = 6

assert t == [new Test(null, 6), new Test('Jim', 0)]
{code}

However, I thought it was broken, because I had been running with without the commented {{t.get(1)}} line.  The {{getAt}} code checks the size of the dynamic list, and since it is smaller, returns {{null}}.  This causes an NPE to be thrown.

I realize that the lazy list breaks the {{List}} contract (by dynamically changing the list size).  However, it seems like a fairly useful feature to have a lazily created list.

I'm not sure there is an acceptable solution, but returning {{null}} hides the problem in a way that is hard to debug!  Maybe it would be better to have a {{withDefault}} method for lists, too, that works like the one for {{Map}}s.  That would provide a usable solution without breaking the current design.  Plus, you no longer have to include the commons library for that use case.  :-)",paulk,overzealous,Minor,Closed,Fixed,25/Jul/11 14:05,13/May/12 03:30
Bug,GROOVY-4950,12815823,ConfigSlurper broken on AppEngine,"On the production AppEngine environment, ConfigSlurper now breaks in 1.8.1 (This was working in 1.8.0) This makes Groovy 1.8.1 unusable on GAE as it stands!

Exception:
{code}
Caused by: java.lang.SecurityException: Unable to get members for class groovy.util.ConfigObject
	at com.google.appengine.runtime.Request.process-f6979038393db0ef(Request.java)
	at org.codehaus.groovy.reflection.CachedClass$3$1.run(CachedClass.java:84)
	at java.security.AccessController.doPrivileged(AccessController.java:34)
	at org.codehaus.groovy.reflection.CachedClass$3.initValue(CachedClass.java:81)
	at org.codehaus.groovy.reflection.CachedClass$3.initValue(CachedClass.java:79)
	at org.codehaus.groovy.util.LazyReference.getLocked(LazyReference.java:46)
	at org.codehaus.groovy.util.LazyReference.get(LazyReference.java:33)
	at org.codehaus.groovy.reflection.CachedClass.getMethods(CachedClass.java:250)
	at groovy.lang.MetaClassImpl.populateMethods(MetaClassImpl.java:340)
	at groovy.lang.MetaClassImpl.fillMethodIndex(MetaClassImpl.java:290)
	at groovy.lang.MetaClassImpl.initialize(MetaClassImpl.java:2915)
	at org.codehaus.groovy.reflection.ClassInfo.getMetaClassUnderLock(ClassInfo.java:166)
	at org.codehaus.groovy.reflection.ClassInfo.getMetaClass(ClassInfo.java:182)
	at org.codehaus.groovy.runtime.metaclass.MetaClassRegistryImpl.getMetaClass(MetaClassRegistryImpl.java:242)
	at org.codehaus.groovy.runtime.InvokerHelper.getMetaClass(InvokerHelper.java:751)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.createCallConstructorSite(CallSiteArray.java:71)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:54)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:182)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:190)
	at groovy.util.ConfigSlurper.parse(ConfigSlurper.groovy:160)
...
{code}
Note: This error does not occur on the GAE dev server.",guillaume,axel.fontaine,Blocker,Closed,Fixed,27/Jul/11 08:52,28/Dec/12 20:43
Bug,GROOVY-4951,12818116,VerifyError - Expecting to find integer on stack,"The line
result = prime * result + (int) (_tagReservationDate ^ (_tagReservationDate >>> 32))
produces bytecode that fails to run in the accompanying test.


<error message=""(class: com/peersaver/model/Breaking, method: hashCode signature: ()I) Expecting to find integer on stack"" type=""java.lang.VerifyError"">java.lang.VerifyError: (class: com/peersaver/model/Breaking, method: hashCode signature: ()I) Expecting to find integer on stack
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:169)
        at com.peersaver.model.BreakingTest.class$(BreakingTest.groovy)
        at com.peersaver.model.BreakingTest.$get$$class$com$peersaver$model$Breaking(BreakingTest.groovy)
        at com.peersaver.model.BreakingTest.testBreaking(BreakingTest.groovy:55)
</error>

The email conversation that covers drilling down into details can be found here:
http://groovy.329449.n5.nabble.com/VerifyError-when-upgrading-from-Groovy-1-7-6-to-1-8-1-td4632721.html

The relevant bit, quoting BlackDragon:
""
> L13
> ILOAD 3: prime
> [int] ILOAD 5: result
> [int,int] IMUL
> [int] ALOAD 0: this
> [int, this] GETFIELD TrackingTagReservation._tagReservationDate : long
> [int, long] ALOAD 0: this
> [int, long, this] GETFIELD TrackingTagReservation._tagReservationDate : long
> [int, long, long] LDC 32
> [int, long, long, int] I2L
> [int, long, long, long] LUSHR

here is a problem, LUSHR expects [..., long, int], but we have 2 longs
here. The I2L should not have happened. So I can confirm a problem here. ""

Thanks!
Jason",blackdrag,jason@bobberinteractive.com,Major,Closed,Fixed,28/Jul/11 19:32,05/Sep/11 05:03
Bug,GROOVY-4952,12818101,VerifyError - Virtual method call from a static context,"Failing to put a public accessor on the 'name' field of the attached test object causes an error in the bytecode.

The exception is:
    <error message=""(class: com/peersaver/model/Breaking, method: &lt;clinit&gt; signature: ()V) Incompatible object argument for function call"" type=""java.lang.VerifyError"">java.lang.VerifyError: (class: com/peersaver/model/Breaking, method: &lt;clinit&gt; signature: ()V) Incompatible object argument for function call
        at java.lang.Class.forName0(Native Method)
        at java.lang.Class.forName(Class.java:169)
        at com.peersaver.model.BreakingTest.class$(BreakingTest.groovy)
        at com.peersaver.model.BreakingTest.$get$$class$com$peersaver$model$Breaking(BreakingTest.groovy)
        at com.peersaver.model.BreakingTest.testBreaking(BreakingTest.groovy:23)
</error>

The email conversation with details is here:
http://groovy.329449.n5.nabble.com/VerifyError-when-upgrading-to-Groovy-1-8-1-clinit-problem-around-getter-generation-td4640586.html

The relevant bit, again quoting BlackDragon: 
(in the <clinit> bytecode)
""> ALOAD 0
> INVOKEVIRTUAL Quiz.getName() : String
> INVOKEINTERFACE CallSite.call(Object,Object) : Object

it is a pretty bad idea to do a virtual method call from a static
context. Looks like that information is missed on ""  ",blackdrag,jason@bobberinteractive.com,Major,Closed,Fixed,28/Jul/11 19:42,29/Aug/11 14:05
Bug,GROOVY-4953,12818100,NPE possibly related to PojoWrapper,"{code:title=Widget.groovy|borderStyle=solid}
class Widget {
}
{code}

{code:title=testscript.groovy|borderStyle=solid}
Widget.metaClass.invokeMethod = { String methodName, Object[] args ->
    
    args.each { a ->
        println a.class
    }
}

w = new Widget()

w.doSometing([] as Object[])
{code}

With Groovy 1.8.0:

{noformat}
pojoproblem $ groovy -version
Groovy Version: 1.8.0 JVM: 1.6.0_26
pojoproblem $ groovy testscript.groovy 
class [Ljava.lang.Object;
{noformat}

With Groovy 1.8.1:

{noformat}
pojoproblem $ groovy -version
Groovy Version: 1.8.1 JVM: 1.6.0_26
pojoproblem $ groovy testscript.groovy 
Caught: java.lang.NullPointerException
java.lang.NullPointerException
	at testscript$_run_closure1_closure2.doCall(testscript.groovy:4)
	at testscript$_run_closure1.doCall(testscript.groovy:3)
	at testscript.run(testscript.groovy:10)
{noformat}

I think (not 100% sure) the NPE is related to the delegate property in PojoWrapper being null.  PojoWrapper.getProperty(String) is called, which looks like this:

{noformat}
    public Object getProperty(final String property) {
        return this.delegate.getProperty(this.wrapped, property);
    }
{noformat}

Is the wrapper really supposed to be passed into the closure or is it supposed to be the unwrapped object?",blackdrag,brownj,Major,Closed,Fixed,29/Jul/11 15:59,29/Aug/11 14:05
Bug,GROOVY-4954,12815795,Currying closure with null as argument fails with NPE,"Trying to curry a closure with {{null}} as a parameter throws {{NullPointerException}}. With Groovy 1.7.10 it works as expected, i.e. sets the first closure argument to {{null}}.

Example using Groovy 1.8.1:
{code}
{ x, y -> x ?: y }.curry(null)

java.lang.NullPointerException
	at org.codehaus.groovy.runtime.CurriedClosure.<init>(CurriedClosure.java:54)
	at org.codehaus.groovy.runtime.CurriedClosure.<init>(CurriedClosure.java:86)
	at groovy.lang.Closure.curry(Closure.java:527)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:883)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.call(PogoMetaClassSite.java:39)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
[...]

// The workaround is this:
assert { x, y -> x ?: y }.curry([null] as Object[])(3) == 3
{code}

Groovy 1.7.10 works as expected:
{code}
assert { x, y -> x ?: y }.curry(null)(3) == 3
assert { x, y -> x ?: y }.curry([null] as Object[])(3) == [null]
{code}

{{CurriedClosure}} constructor changed between 1.7 and 1.8:
{code}
$ diff 1.7.10/.../CurriedClosure.java 1.8.1/.../CurriedClosure.java
49c49
<     public CurriedClosure(int index, Closure uncurriedClosure, Object[] arguments) {
---
>     public CurriedClosure(int index, Closure<V> uncurriedClosure, Object... arguments) {
{code}

{{arguments}} parameter in 1.8 is set to {{null}} instead of {{[null]}} when invoking {{curry(null)}}, hence the exception on line 54 of {{CurriedClosure}} class.",paulk,dsrkoc,Major,Closed,Fixed,01/Aug/11 16:49,28/Sep/11 10:14
Bug,GROOVY-4958,12815822,ConfigSlurper doesn't parse config files that contain $ signs in property values correctly,"When a ConfigObject is written to file all values are quoted with {{""}}. This leads to problems if a value contains a {{$}} for example.

Consider the following code snipet:
{code}
def config = new ConfigObject()
config.instObject = '1. Dyn. Aktber. ($IO_AKTBER)   # Aktenbereich (V-AKTBER)'

println ""config before writing to file:    ${config}""

def file = new File('C:/Temp/config.groovy')
file.withWriter { writer ->
    config.writeTo(writer)
}

config = new ConfigSlurper().parse(file.getText())
println ""config after reloading from file: ${config}""
{code}

The output is:
{code}
config before writing to file:    [instObject:1. Dyn. Aktber. ($IO_AKTBER)   # Aktenbereich (V-AKTBER)]
config after reloading from file: [IO_AKTBER:[:], instObject:1. Dyn. Aktber. ([:])   # Aktenbereich (V-AKTBER)]
{code}

The according file content:
{code}
instObject=""1. Dyn. Aktber. ($IO_AKTBER)   # Aktenbereich (V-AKTBER)""
{code}

As can be seen the two config objects differ widely. A solution could perhaps be writing properties quoted by {{'}} or writing the properties the way they are quoted in the source code, e. g. {{config.property1 = ""$property1""}} would become {{property1 = ""$property1""}} in the file and {{config.property2 = '$property1'}} would become {{property2 = '$property1'}}. This way the programmer could control the desired behaviour.",roshandawrani,erpl,Minor,Closed,Fixed,03/Aug/11 06:04,24/Dec/11 03:08
Bug,GROOVY-4964,12815838,static import overrides explicit class name when getting property,"When I static import a method, it takes precedence over a call to the method explicitly on another class, if I get a property from the result.

{code}
$ cat Foo.groovy 
class Foo {
    static doIt() { [k: 'foo'] }
}

jbeutel@jbeutel-mac:~/proj/groovySandboxes/staticImports (master *)
$ cat Bar.groovy 
import static Foo.*
class Bar {
    static doIt() { [k: 'bar'] }
    static doAssert() {
        assert doIt().k == 'foo'
        assert doIt() == [k: 'foo']
        assert Bar.doIt() == [k: 'bar']
        assert Bar.doIt().k == 'bar'
    }
}

jbeutel@jbeutel-mac:~/proj/groovySandboxes/staticImports (master *)
$ groovy -e 'Bar.doAssert()'
Caught: Assertion failed: 

assert Bar.doIt().k == 'bar'
                  | |
                  | false
                  foo

Assertion failed: 

assert Bar.doIt().k == 'bar'
                  | |
                  | false
                  foo

	at Bar.doAssert(Bar.groovy:8)
	at Bar$doAssert.call(Unknown Source)
	at script_from_command_line.run(script_from_command_line:1)

jbeutel@jbeutel-mac:~/proj/groovySandboxes/staticImports (master *)
$ groovy --version
Groovy Version: 1.8.1 JVM: 1.6.0_26
{code}

This was a problem for me when statically importing (with wildcards) a bunch of enum classes into another enum.

Work-around:  assign the method result to a temporary variable before getting a property from it.",paulk,david_beutel,Minor,Closed,Fixed,05/Aug/11 00:09,07/Sep/11 14:13
Bug,GROOVY-4965,12815636,shim classes generated incorrectly in 1.8.1 (was fine in 1.8.0) for some static inner-class usages,"Static inner classes that are imported into Groovy scripts just fine in 1.8 are causing compilation issues in 1.8.1

To reproduce:
1) Java class Foo.Bar where Bar is public static within Foo.
2) Groovy script references the Foo.Bar as a type with an arglist to a method.

3) Groovy/Maven/Shim compiler puts a $ in the generated Java source name instead of a dot.

As I said, all that changed between working and not working is the version of Groovy.  This worked in 1.8.0 and at least some recent 1.7.x versions.

",paulk,paul,Minor,Closed,Fixed,05/Aug/11 12:42,16/Sep/11 18:10
Bug,GROOVY-4966,12815774,VeryifyError in multi-dimensional array usage:  Incompatible argument to function,"When running this code, I get the attached error.  Please see: http://pastie.org/2327875.  Dropping back to 1.7 seems to work.  I'm running 1.8.1, but the problem also exists in 1.8.0.",roshandawrani,cfriedline,Major,Closed,Fixed,06/Aug/11 08:58,24/Dec/11 03:08
Bug,GROOVY-4967,12815820,List to LinkedHashSet inconsistent conversion behavior vs List to HashSet,"a), b), and c) all work, but d) fails with ""GroovyCastException: Cannot cast object '[x]' with class 'java.util.ArrayList' to class 'java.util.LinkedHashSet'""

a)
{code}
List x = []
HashSet<String> lhs = x
{code}

b)
{code}
List x = ['x']
HashSet<String> lhs = x
{code}

c)
{code}
List x = []
LinkedHashSet<String> lhs = x
{code}

d)
{code}
List x = ['x']
LinkedHashSet<String> lhs = x
{code}",roshandawrani,roshandawrani,Minor,Closed,Fixed,07/Aug/11 21:35,24/Dec/11 03:08
Bug,GROOVY-4968,12816912,BUG! exception when using spread operator,"Not sure why the author used a spread operator here, but this works fine with Groovy 1.7, and blows up with 1.8.

{code}
class SpreadBug {
  def foo(String... args) {
    bar(*args)
  }
  
  def bar(String... args) {}
}
{code}

Output when run in Groovy Console:

{noformat}
BUG! exception in phase 'class generation' in source unit 'ConsoleScript0' SpreadExpression should not be visited here
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitSpreadExpression(AsmClassGenerator.java:575)
	at org.codehaus.groovy.ast.expr.SpreadExpression.visit(SpreadExpression.java:39)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeDirectMethodCall(InvocationWriter.java:125)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:167)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeCall(InvocationWriter.java:87)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.makeInvokeMethodCall(InvocationWriter.java:71)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeMethod(InvocationWriter.java:288)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethodCallExpression(AsmClassGenerator.java:662)
	at org.codehaus.groovy.ast.expr.MethodCallExpression.visit(MethodCallExpression.java:75)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeReturn(StatementWriter.java:577)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeReturn(OptimizingStatementWriter.java:296)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitReturnStatement(AsmClassGenerator.java:456)
	at org.codehaus.groovy.ast.stmt.ReturnStatement.visit(ReturnStatement.java:47)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeBlockStatement(StatementWriter.java:80)
	at org.codehaus.groovy.classgen.asm.OptimizingStatementWriter.writeBlockStatement(OptimizingStatementWriter.java:160)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBlockStatement(AsmClassGenerator.java:406)
	at org.codehaus.groovy.ast.stmt.BlockStatement.visit(BlockStatement.java:69)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClassCodeContainer(ClassCodeVisitorSupport.java:101)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitConstructorOrMethod(ClassCodeVisitorSupport.java:112)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitStdMethod(AsmClassGenerator.java:289)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorOrMethod(AsmClassGenerator.java:268)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitMethod(ClassCodeVisitorSupport.java:123)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitMethod(AsmClassGenerator.java:366)
	at org.codehaus.groovy.ast.ClassNode.visitContents(ClassNode.java:1058)
	at org.codehaus.groovy.ast.ClassCodeVisitorSupport.visitClass(ClassCodeVisitorSupport.java:50)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitClass(AsmClassGenerator.java:174)
	at org.codehaus.groovy.control.CompilationUnit$13.call(CompilationUnit.java:763)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:957)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:542)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:520)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:497)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:306)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:287)
	at groovy.lang.GroovyShell.parseClass(GroovyShell.java:731)
	at groovy.lang.GroovyShell.run(GroovyShell.java:516)
	at groovy.lang.GroovyShell.run(GroovyShell.java:172)
	at groovy.lang.GroovyShell$run.call(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:124)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy:924)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:883)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:46)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
	at groovy.ui.Console$_runScriptImpl_closure16.doCall(Console.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:883)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:404)
	at groovy.lang.Closure.run(Closure.java:488)
	at java.lang.Thread.run(Thread.java:680)
{noformat}",blackdrag,pniederw,Major,Closed,Fixed,09/Aug/11 08:23,29/Aug/11 14:07
Bug,GROOVY-4970,12815837,Method call expression parsed differently in 1.8 compared to 1.7,"Some method call expressions are parsed differently in 1.8 compared to 1.7. This causes one of Gradle's AST transforms to fail. According to Jochen, this looks like a bug in the 1.8 ANTLR grammar.

{noformat}
task copy(type: Copy) { x = 10 }
g1.7: this.task(this.copy(...))
g1.8: this.task(this.copy(...))

task copy(type: Copy) { println 10 }
g1.7: this.task(this.copy(...))
g1.8: task.copy(...)
{noformat}

The first expression is parsed the same in 1.7 and 1.8, but the second expression is parsed differently.",blackdrag,pniederw,Blocker,Closed,Fixed,10/Aug/11 14:09,18/Oct/11 07:28
Bug,GROOVY-4972,12815798,groovy.codehaus.org gives a 404,"Here's the message:
404 Unknown page Home
/groovy.codehaus.org/Home was not found on this server.

Resin-3.0.14 (built Tue, 05 Jul 2005 11:03:36 PDT) ",,mlzarathustra,Major,Closed,Fixed,12/Aug/11 19:12,13/Aug/11 02:37
Bug,GROOVY-4973,12815830,Inconsistent numeric range types,"I would expect that range literal follow the same type promotion rules as arithmetic operations, yet: 

{code}
assert (1+10).class == Integer   
assert (1L+10).class == Long 
assert (1+10L).class == Long 

assert (1..10).every { it.class == Integer  } 
assert (1L..10).every { it.class == Long } 
assert (1..10L).every { it.class == Integer } // <= weird 
{code}
",roshandawrani,ddimitrov,Major,Closed,Fixed,13/Aug/11 05:44,24/Dec/11 03:08
Bug,GROOVY-4974,12815825,Bounded generics do not work with inheritance,"The following code works in Java, but fails to compile in Groovy - looks like unintentional: 

{code}
public class TestGenerics { 
    static interface Z {} 
    static class X implements Z {} 
    static class Y extends X {} 

    static class A <T extends Z> { void a(T t) { System.out.println(t); } } 
    static class B extends A<Y> {} 
    static class C extends A<X> {} 

    public static void main(String[] args) { 
        new C().a(new Y()); 
    } 
} 
{code}",paulk,ddimitrov,Major,Closed,Fixed,13/Aug/11 05:46,14/Oct/11 00:28
Bug,GROOVY-4975,12815810,GroovyScripeEngine fails to properly reload when dependent class is updated,"Let's say you have 2 scripts : script1 and script2 and they both use class BeanX and they are all loaded via the same GroovyScriptEngine instance.  If I modify BeanX, the script that is the first to get loaded and references BeanX will reflect changes made to BeanX, while the other or second script to reference BeanX will fail to reflect changes made to BeanX.

I'm assuming this is not the proper behavior, although I don't see any test to test for this scenario.  If one is using groovy via Groovlets or something like Gaelyk where the assumption is that I can write 
scripts and hot have to constantly restart to have my changes reflected then I would assume the GSE should reload classes and dependent classes regardless of the order in which they are referenced.

Test attached.",blackdrag,dsmith,Major,Closed,Fixed,14/Aug/11 11:57,04/Oct/12 17:02
Bug,GROOVY-4976,12815659,NPE in AntlrParserPlugin on incorrect array creation expression,"Groovy parser fails on incorrect array creation expression:

{noformat}
$ java -jar groovy-all-1.8.1.jar -d -e ""def foo = new double[][5]""
Caught: BUG! exception in phase 'conversion' in source unit 'script_from_command_line' null
BUG! exception in phase 'conversion' in source unit 'script_from_command_line' null
        at org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:843)
        at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:544)
        at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:520)
        at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:497)
        at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:306)
        at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:287)
        at groovy.lang.GroovyShell.parseClass(GroovyShell.java:731)
        at groovy.lang.GroovyShell.run(GroovyShell.java:516)
        at groovy.lang.GroovyShell.run(GroovyShell.java:172)
        at groovy.ui.GroovyMain.processOnce(GroovyMain.java:526)
        at groovy.ui.GroovyMain.run(GroovyMain.java:332)
        at groovy.ui.GroovyMain.process(GroovyMain.java:318)
        at groovy.ui.GroovyMain.processArgs(GroovyMain.java:115)
        at groovy.ui.GroovyMain.main(GroovyMain.java:96)
Caused by: java.lang.NullPointerException
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expressionSwitch(AntlrParserPlugin.java:1649)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:1633)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:1629)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.arraySizeExpression(AntlrParserPlugin.java:2509)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.arraySizeExpression(AntlrParserPlugin.java:2507)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.constructorCallExpression(AntlrParserPlugin.java:2460)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expressionSwitch(AntlrParserPlugin.java:1670)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:1633)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.expression(AntlrParserPlugin.java:1629)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.declarationExpression(AntlrParserPlugin.java:1437)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.variableDef(AntlrParserPlugin.java:1453)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.statement(AntlrParserPlugin.java:1203)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.convertGroovy(AntlrParserPlugin.java:304)
        at org.codehaus.groovy.antlr.AntlrParserPlugin.buildAST(AntlrParserPlugin.java:241)
        at org.codehaus.groovy.control.SourceUnit.convert(SourceUnit.java:272)
        at org.codehaus.groovy.control.CompilationUnit$10.call(CompilationUnit.java:618)
        at org.codehaus.groovy.control.CompilationUnit.applyToSourceUnits(CompilationUnit.java:839)
        ... 13 more

$ java -jar groovy-all-1.8.1.jar -v
Groovy Version: 1.8.1 JVM: 1.6.0_25
{noformat}
",daniel_sun,edebrev,Minor,Closed,Fixed,15/Aug/11 07:12,21/Feb/20 03:48
Bug,GROOVY-4978,12815796,SecureASTCustomizer blacklist is ignored inside method body,"I'm trying to compile Groovy Scripts while rejecting calls to System.exit() by using using a SecureASTCustomizer like this:

{code}
final SecureASTCustomizer customizer = new SecureASTCustomizer();
customizer.setImportsBlacklist(asList(""java.lang.System"",
		""groovy.lang.GroovyShell"", ""groovy.lang.GroovyClassLoader""));
customizer.setIndirectImportCheckEnabled(true);

CompilerConfiguration configuration = new CompilerConfiguration();
configuration.addCompilationCustomizers(customizer);

ClassLoader parent = ScriptCompiler.class.getClassLoader();
GroovyClassLoader loader = new GroovyClassLoader(parent, configuration);
{code}

The following Script is blocked correctly and I get an exception during parseClass()
{code}
System.exit(1);
{code}

In the following script, System.exit() is called successfully:
{code}
def x() { System.exit(1) }
x()
{code}
",melix,cmj,Major,Closed,Fixed,16/Aug/11 10:27,07/Sep/11 14:13
Bug,GROOVY-4979,12815679,@ToString creates additional unneeded field,"Using {{@ToString(includeNames = true)}} inserts a field called {{$print$names}} into the class. This field is neither transient nor static, which causes havoc when using XStream.

Example XStream output:

{code}
[
  {
    ""name"": ""&#12415; +()[]{}.mp3"",
    ""size"": 690037,
    ""outputSize"": 0,
    ""format"": {
      ""codec"": ""mp3"",
      ""name"": ""mp3"",
      ""bitDepth"": ""s16le"",
      ""samplingRate"": 44100,
      ""channels"": 2,
      ""$print$names"": true
    }
  }
]
{code}

I don't understand why the field is there in the first place since printNames is a compile-time thing.",paulk,johann,Major,Closed,Fixed,19/Aug/11 14:36,13/May/12 03:30
Bug,GROOVY-4980,12815797,Named argument constructor doesn't work for inner classes,"{code}
class Outer {
  class Inner {
    String name
  }

  def doit() {
    new Inner(name: ""fred"")
  }
}

new Outer().doit()
{code}

{noformat}
Caught: groovy.lang.GroovyRuntimeException: Could not find matching constructor for: Outer$Inner(Outer, java.util.LinkedHashMap)
	at Outer.doit(OuterF.groovy:7)
	at OuterF.run(OuterF.groovy:11)
{noformat}",roshandawrani,pniederw,Major,Closed,Fixed,19/Aug/11 17:58,14/Oct/11 00:28
Bug,GROOVY-4981,12815691,Code examples on website are not visible,"The code snippets in the user guide, cookbook, and elsewhere on groovy.codehaus.org are not visible.  Instead of a box containing something like:
println 'Hello World'
A small empty rectangle is displayed instead.
It looks like this syntax highlighter library is being used: http://alexgorbatchev.com/SyntaxHighlighter/manual/installation.html.  However, looking at the source of the page it appears that the javascript and css libraries that it uses aren't actually loaded, nor is the SyntaxHighlighter.all() JavaScript method invoked.
",guillaume,redshadow,Critical,Closed,Fixed,20/Aug/11 20:20,21/Dec/12 09:46
Bug,GROOVY-4983,12818410,Integer multiples of 256 evaluate to false.,"groovy -e ""assert 256""
groovy -e ""assert 512""
and so on will fail.",blackdrag,huxi,Blocker,Closed,Fixed,22/Aug/11 14:53,30/Aug/11 04:26
Bug,GROOVY-4984,12815833,JsonOutput.toJson(object) does not handle char[] correctly,"Currently {quote}println new JsonBuilder(['a','b','c','d'] as char[]).toString(){quote} will output {quote}[{},{},{},{}]{quote} since the primitive class {{char}} has no properties (other than {{class}}).

My solution is to call {{toString}} on the object if there are no properties left. I have attached an updated JsonOutput.groovy and a fixed test.",guillaume,seth.miller,Minor,Closed,Fixed,23/Aug/11 12:30,07/Sep/11 14:13
Bug,GROOVY-4986,12817017,java.sql.Timestamp + int = java.util.Date,"The following code:

{code:Java}
def d = new Date()
def t = new java.sql.Timestamp(d.getTime())
println t.class
t = t + 1
println t.class
{code}
Produces the following output:

class java.sql.Timestamp
class java.util.Date


This behavior is unexpected, and is causing us a small problem.  The fix would be to change the org.codehaus.groovy.runtime.DateGroovyMethods.plus(Date,int) method to be as follows:
{code:Java}
    public static Date plus(Date self, int days) {
        Calendar calendar = (Calendar) Calendar.getInstance().clone();
        calendar.setTime(self);
        calendar.add(Calendar.DAY_OF_YEAR, days);
        if (self instanceof java.sql.Timestamp) {
            java.sql.Timestamp ts = new java.sql.Timestamp(calendar.getTime().getTime());
            ts.setNanos(((java.sql.Timestamp)self).getNanos());
            return ts;
        }
        return calendar.getTime();
    }
{code}

",roshandawrani,driscoll,Minor,Closed,Fixed,24/Aug/11 13:40,05/Apr/15 14:44
Bug,GROOVY-4988,12815816,JsonSlurper fails to correctly parse strings containing some escaped character sequences,It seems JsonSlurper does not successfully parse serialized objects that originally contain character sequences like \n \t etc.,melix,ltpcsucs,Major,Closed,Fixed,25/Aug/11 11:26,24/Dec/11 03:07
Bug,GROOVY-4994,12815684,wrong delegate for ComposedClosure,"{code:Java}
def a = { println foo }
def b = { println bar }

class O {
    def foo = 'foo'
    def bar = 'bar'
}

def ab = a >> b
ab.delegate = new O()
ab()
{code}
throws groovy.lang.MissingPropertyException: No such property: foo for class: ConsoleScript5",blackdrag,blackdrag,Major,Closed,Fixed,30/Aug/11 14:34,30/Aug/11 15:06
Bug,GROOVY-4995,12815843,groovy-all-1.7.8 in Codehaus Maven repository is actually 1.7.9-SNAPSHOT,"See http://groovy.markmail.org/message/tvfmmgieh4sgdkkr

The groovy-all-1.7.8 in the Codehaus Maven repository is actually a 1.7.9-SNAPSHOT. Thus, anyone download groovy-all-1.7.8 from this location is unknowingly getting a 1.7.9-SNAPSHOT.",,kpshek,Critical,Closed,Fixed,31/Aug/11 10:12,07/Sep/11 14:13
Bug,GROOVY-4997,12815703,@Immutable not working for inner classes,"{code}
class A {
    @Immutable
    class B {  
    }
}
{code}
throws
{code}
Explicit constructors not allowed for @Immutable class: A$B
 at line: -1, column: -1
{code}",paulk,codevise,Major,Closed,Fixed,02/Sep/11 08:27,13/May/12 03:30
Bug,GROOVY-4999,12818103,Runtime error on static overloaded mixin method,"I have a class with static utility methods. I have done 'mixin' this class into another class using @Mixin annotation and then while calling static overloaded method I give runtime error.

Utility class:
{code}
class UtilClass {
    public void callClassOverloadedMethod(String s) {
        System.out.println(""callClassOverloadedMethod(String)"");
    }
    public void callClassOverloadedMethod(String s, Object o) {
        System.out.println(""callClassOverloadedMethod(String, Object)"");
    }
    public static void callStaticOverloadedMethod(String s) {
        System.out.println(""callStaticOverloadedMethod(String)"");
    }
    public static void callStaticOverloadedMethod(String s, Object o) {
        System.out.println(""callStaticOverloadedMethod(String, Object)"");
    }
}
{code}
Main class:
{code}
@Mixin(UtilClass)
class MainClass {
    public static void main(String []s) {
        MainClass mc = new MainClass()
        mc.callMixinMethods()
    }

    void callMixinMethods() {
        callClassOverloadedMethod("""")
        callStaticOverloadedMethod("""")
    }
}
{code}
Run command:
{noformat}
groovy -cp . MainClass
{noformat}
Error:
{noformat}
Caught: org.codehaus.groovy.runtime.metaclass.MethodSelectionException: Could not find which method callStaticOverloadedMethod() to invoke from this list:
  public static void UtilClass#callStaticOverloadedMethod(java.lang.String)
  public static void UtilClass#callStaticOverloadedMethod(java.lang.String, java.lang.Object)
        at MainClass.callMixinMethod(MainClass.groovy:20)
        at MainClass.main(MainClass.groovy:12)
{noformat}

Please fix this trouble, it prevent me to write code and tests.",roshandawrani,sehseh,Major,Closed,Fixed,06/Sep/11 05:25,14/Oct/11 00:28
Bug,GROOVY-5000,12817209,Wrong constructor call generated with AIC + closure usage,"This issue is raised based on dev mailing list discussion here: http://markmail.org/message/ksngkezqgqazpz5q

Here are the code snippets that demonstrate the issue with constructor call generation when anonymous inner classes and closure are used together.

1)
{code}
interface X{}

final double delta = 0.1
(0 ..< 1).collect { n ->
    new X () {
        Double foo () {
            delta
        }
    }
}
{code}

fails with
{noformat}
Caught: groovy.lang.GroovyRuntimeException: Could not find matching constructor for: TryGroovy$1(TryGroovy, java.lang.Double)
{noformat}

2) Seems like there is some mix-up with the variables used from the enclosing context. In this snippet, I use 2 variables from enclosing context, and both become part of constructor call that fails.

{code}
interface X{}

final double delta1 = 0.1
final double delta2 = 0.1
(0 ..< 1).collect { n ->
    new X () {
        Double foo () {
            delta1 + delta2
        }
    }
}
{code}

fails with:

{noformat}
Caught: groovy.lang.GroovyRuntimeException: Could not find matching constructor for: TryGroovy$1(TryGroovy, java.lang.Double, java.lang.Double)
{noformat}",blackdrag,roshandawrani,Major,Closed,Fixed,07/Sep/11 10:54,09/Sep/11 06:43
Bug,GROOVY-5001,12817165,Map access is given higher precedence when trying to access fields/properties in classes which implement java.util.Map or extend java.util.HashMap or java.util.Properties,"Using the @Log4j annotation on a groovy class that extends java.util.Properties seems to have odd side effects on closures.

{code:title=testscript}

import groovy.util.logging.*

[new Test1(), new Test2(), new Test3()].each {
    runTest(it)
}

def runTest (obj) {
    try {
         obj.test()
         obj.method()
         println ""${obj.getClass()} success""
    }
    catch (e) {
         println ""${obj.getClass()} failed $e""
    }
}

//   classes

@Log4j
class Test1 {
    def test () {
         println '============================='
         println 'testing plain Groovy class'
         println ""testing class [${this.getClass().getSimpleName()}]""
    }
    def method () {
        log.debug 'method called, not in closure'
        ['a', 'b', 'c'].each {
            log.debug ""inside closure [$it]""
        }
    }
}

@Log4j
class Test2 extends BaseClass {
    def test () {
         println '============================='
         println 'testing Groovy class that extends another Groovy class'
         println ""testing class [${this.getClass().getSimpleName()}]""
    }
    def method () {
        log.debug 'method called, not in closure'
        ['a', 'b', 'c'].each {
            log.debug ""inside closure [$it]""
        }
    }
}

class BaseClass {
}

@Log4j
class Test3 extends java.util.Properties {
    def test () {
         println '============================='
         println 'testing Groovy class that extends java.util.Properties'
         println ""testing class [${this.getClass().getSimpleName()}]""
    }
    def method () {
        log.debug 'method called, not in closure'
        ['a', 'b', 'c'].each {
            log.debug ""inside closure [$it]""
        }
    }
}
{code}

{code:title=log4j.properties}
log4j.rootLogger=DEBUG, CONSOLE
log4j.appender.CONSOLE=org.apache.log4j.ConsoleAppender
log4j.appender.CONSOLE.layout=org.apache.log4j.PatternLayout
log4j.appender.CONSOLE.layout.ConversionPattern=%d [%t] %-5p %c - %m%n
{code}

{noformat}

$ groovy testscript
=============================
testing plain Groovy class
testing class [Test1]
2011-09-08 15:59:45,757 [main] DEBUG Test1 - method called, not in closure
2011-09-08 15:59:45,759 [main] DEBUG Test1 - inside closure [a]
2011-09-08 15:59:45,759 [main] DEBUG Test1 - inside closure [b]
2011-09-08 15:59:45,759 [main] DEBUG Test1 - inside closure [c]
class Test1 success
=============================
testing Groovy class that extends another Groovy class
testing class [Test2]
2011-09-08 15:59:45,768 [main] DEBUG Test2 - method called, not in closure
2011-09-08 15:59:45,769 [main] DEBUG Test2 - inside closure [a]
2011-09-08 15:59:45,769 [main] DEBUG Test2 - inside closure [b]
2011-09-08 15:59:45,769 [main] DEBUG Test2 - inside closure [c]
class Test2 success
=============================
testing Groovy class that extends java.util.Properties
testing class [Test3]
2011-09-08 15:59:45,771 [main] DEBUG Test3 - method called, not in closure
class Test3 failed java.lang.NullPointerException: Cannot invoke method debug() on null object

{noformat}

The problem is on Test3 where the log instance var is null inside the closure, but not outside it.
Only Test 3 has this problem",emilles,ericksn,Major,Resolved,Fixed,08/Sep/11 15:04,15/Aug/22 14:38
Bug,GROOVY-5003,12816985,[PATCH] Fix the problem in handling bridge methods,"Groovy should ignore bridge methods.

This eliminates redundant method calls, although that isn't why
I need it.

My primary reason for this change is bit unusual; I've got a little
byte code post processor to inject synthetic bridge methods
to help me evolve code without breaking existing binaries,
<http://bridge-method-injector.infradna.com/>, and as a part of this
I generate a bridge method whose return type is narrower, instead of
wider.

That is, whereas normally bridge methods are as follows:

{noformat}
interface Base {
    Object foo();
}

class Impl implements Base {
    String foo() {...}

    // the above definition causes javac to insert the following
    // bridge method
    @Synthetic @Bridge
    Object foo() { return <String>foo(); }
}
{noformat}

my byte code post processing would produce this:

{noformat}
class Impl /* no interface needed */ {
    Object foo() {...}

    @Synthetic @Bridge
    String foo() { return (String)<Object>foo(); }
}
{noformat}

This works with javac, in the sense that it'll invoke ""Object foo()"", by preferring non-bridge methods for resolution.

Unfortunately, Grooovy doesn't discreminate against bridge methods, so it can end up calling ""String foo()"" depending on the exact implementation detail of the search.

This fix eliminates this issue by making Groovy ignore all bridge methods. Would you please please include this?",guillaume,kohsuke,Major,Closed,Fixed,08/Sep/11 17:28,05/Apr/15 14:44
Bug,GROOVY-5006,12818106,Inappropriate transformation of type from int to BigDecimal,"In Groovy 1.8.1 the following code compiles fine and runs fine.  Fine here means hanging forever with no output.
{code}

@Grab ( 'org.codehaus.gpars:gpars:0.12' )

import groovyx.gpars.group.DefaultPGroup

final int n = 1000000000i
final int actorCount = 1
final int sliceSize = n / actorCount // The division expression here is crucial to observing the problem.
final group = new DefaultPGroup ( )
final accumulator = group.messageHandler { }
final computors = [ ]  
assert sliceSize.class == Integer
for ( index in 0 ..< actorCount ) {
  assert sliceSize.class == Integer
  computors.add (
    //assert sliceSize.class == Integer // this statement causes a compilation error in the following statement ?????
    group.actor {
      assert sliceSize.class == Integer
    }
  )
}
accumulator.join ( )
{code}
Using 1.8.2 or Groovy trunk with the SHA1 given above, this results in:
{quote}
An exception occurred in the Actor thread Actor Thread 2
Assertion failed: 

assert sliceSize.class == Integer
       |         |     |
       1000000000|     false
                 class java.math.BigDecimal

	at org.codehaus.groovy.runtime.InvokerHelper.assertFailed(InvokerHelper.java:385)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.assertFailed(ScriptBytecodeAdapter.java:658)
	at alt$_run_closure2.doCall(alt.groovy:25)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:882)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallCurrent(CallSiteArray.java:46)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:133)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
	at alt$_run_closure2.doCall(alt.groovy)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:882)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:404)
	at groovyx.gpars.actor.DefaultActor.handleStart(DefaultActor.java:336)
	at groovyx.gpars.actor.AbstractLoopingActor$1.handleMessage(AbstractLoopingActor.java:70)
	at groovyx.gpars.util.AsyncMessagingCore.run(AsyncMessagingCore.java:132)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{quote}
it then also hangs forever, but in this case that can be considered normal and correct.",blackdrag,russel,Critical,Closed,Fixed,09/Sep/11 06:41,09/Sep/11 11:20
Bug,GROOVY-5008,12816260,Integer optimization is not executed,"Integer optimization is not executed because BytecodeInterface8.isOrigZ() which is for boolean optimization and always returns false is called instead of BytecodeInterface8.isOrigInt().

Steps to confirm the problem:
# Save the following script as Test.groovy
{code:java}
int fib(int n) {
    if (n < 2) return n
    return rcfib(n - 1) + rcfib(n - 2)
}
{code}
# Compile Test.groovy using groovyc
{noformat}groovyc Test.groovy{noformat}
# Disassemble the Test.class
{noformat}javap -c Test{noformat}
# You can see BytecodeInterface8.isOrigZ is called
{code}
  public int fib(int);
    Code:
       0: invokestatic  #20                 // Method $getCallSiteArray:()[Lorg/codehaus/groovy/runtime/callsite/CallSite;
       3: astore_2      
       4: invokestatic  #66                 // Method org/codehaus/groovy/runtime/BytecodeInterface8.isOrigZ:()Z
       7: ifeq          25
      10: getstatic     #68                 // Field __$stMC:Z
      13: ifne          25
      16: invokestatic  #71                 // Method org/codehaus/groovy/runtime/BytecodeInterface8.disabledStandardMetaClass:()Z
      19: ifne          25
      :
      :
{code}
 ",blackdrag,nagai_masato,Major,Closed,Fixed,11/Sep/11 02:43,05/Apr/15 14:44
Bug,GROOVY-5009,12815861,Problem dispatching array as argument to a closure,"{code:title=myscript.groovy|borderStyle=solid}
ExpandoMetaClass.enableGlobally()

Object[] myObjectArray = ['a', 'b'] as Object[]

closure = {
	println 'closure running...'
}
	
closure(myObjectArray)
{code}

{noformat}
code $ groovy -version
Groovy Version: 1.8.2 JVM: 1.6.0_26
code $ groovy myscript.groovy 
Caught: groovy.lang.MissingMethodException: No signature of method: myscript$_run_closure1.doCall() is applicable for argument types: (java.lang.String, java.lang.String) values: [a, b]
Possible solutions: doCall(), doCall(java.lang.Object), call(), call([Ljava.lang.Object;), call(java.lang.Object), findAll()
groovy.lang.MissingMethodException: No signature of method: myscript$_run_closure1.doCall() is applicable for argument types: (java.lang.String, java.lang.String) values: [a, b]
Possible solutions: doCall(), doCall(java.lang.Object), call(), call([Ljava.lang.Object;), call(java.lang.Object), findAll()
	at myscript.run(myscript.groovy:9)
{noformat}

I have tested with versions all the way back to 1.7.0 and the exception is thrown for all the versions I tested.

If I remove the ExpandoMetaClass.enableGlobally(), the exception goes away.

I think this is related to http://jira.grails.org/browse/GRAILS-8002, though I am not sure why that problem just showed up in Grails 2.0-M2.  I have not been able to reproduce it with Grails 1.3.7.
",blackdrag,brownj,Major,Closed,Fixed,12/Sep/11 09:12,25/Jan/13 23:35
Bug,GROOVY-5014,12815737,Syntax highlighting problem in groovy.ui.Console text editor on empty string expression entering,Any input of an expression with empty string (see the attachments) does invalid syntax highlighting.,pschumacher,dmovchinn,Minor,Closed,Fixed,13/Sep/11 10:41,26/Nov/13 14:48
Bug,GROOVY-5017,12815876,[PATCH] SimpleTemplateEngine line number is off by one,Line number reported from scripts compiled by SimpleTemplateEngine is off by one because it generates an extra new line that's not found in the original source file. Patch attached.,roshandawrani,kohsuke,Major,Closed,Fixed,13/Sep/11 21:13,05/Apr/15 14:43
Bug,GROOVY-5018,12815819,FileSystemCompiler#generateFileNamesFromOptions is adding filenames multiple times,"See mailing list for further details:
http://groovy.markmail.org/thread/674bikja7ylfejsd",paulk,paulk,Major,Closed,Fixed,13/Sep/11 21:20,07/Apr/15 19:06
Bug,GROOVY-5021,12811713,JavaStubCompilationUnit.addSource() should allow other extensions,"I noticed this while working on a replacement for GMaven.  I think JavaStubCompilationUnit should allow other extensions such as "".gvy"", "".gy"", "".gsh"" to be used when adding sources.  This change in behavior was introduced with https://fisheye.codehaus.org/changelog/groovy?cs=18020.  I think what we might want to do instead is blacklist "".java"" rather than whitelist "".groovy"".  I've attached a patch demonstrating this.",guillaume,keegan,Minor,Closed,Fixed,14/Sep/11 20:16,05/Apr/15 14:44
Bug,GROOVY-5026,12815895,NullPointerException using Open JDK caused problems running Grails unit tests,See http://jira.grails.org/browse/GRAILS-8031,blackdrag,graemerocher,Critical,Closed,Fixed,15/Sep/11 10:34,22/Sep/11 13:20
Bug,GROOVY-5027,12815868,Groovydoc calculateFirstSentence doesn't recognise html tags as breaks,"The Method summary block of the groovydoc pages contains a sentence describing the method.

However, if the javadoc looks like (taken from StreamingJsonBuilder.groovy):

{code:java}
    /**
     * Varargs elements as arguments to the JSON builder create a root JSON array
     * <p>
     * Example:
     * <pre class=""groovyTestCase"">
     * new StringWriter().with { w ->
     *   def json = new groovy.json.StreamingJsonBuilder( w )
     *   def result = json 1, 2, 3
     *
     *   assert result instanceof List
     *   assert w.toString() == ""[1,2,3]""
     * }
     * </pre>
     * @param args an array of values
     * @return a list of values
     */
{code}

Then the summary text becomes:

{code:none}
Varargs elements as arguments to the JSON builder create a root JSON array

Example:

 new StringWriter().with { w ->
   def json = new groovy.json.StreamingJsonBuilder( w )
   def result = json 1, 2, 3
{code}

In this case, it is only confusing (as the {{assert}} is missed off the end, but in other cases, it could result in invalid code being shown in the summary.

The attached patch adds another {{replaceAll}} to the {{calculateFirstSentence}} method of {{SimpleGroovyDoc.java}} which trims the javadoc as soon as an HTML tag is encountered as the first thing on a new line in the javadoc.

The summary text for this same method then becomes simply:

{code:none}
Varargs elements as arguments to the JSON builder create a root JSON array
{code}",guillaume,tim_yates,Trivial,Closed,Fixed,16/Sep/11 04:29,05/Apr/15 14:44
Bug,GROOVY-5029,12816242,"XmlSlurper does not close InputStream, leaks file handles/resources","{code}
  public GPathResult parse(final File file) throws IOException, SAXException {
  final InputSource input = new InputSource(new FileInputStream(file));
    
    input.setSystemId(""file://"" + file.getAbsolutePath());
    
    return parse(input);
    
  }
{code}

I stepped through the SAX code but couldn't see the {{InputSource}} being closed so {{XmlSlurper}} is leaking file descriptors.

I think the {{Reader}} and {{InputStream}} methods' JavaDoc should also mention that the streams aren't closed.

{{GPathResult parse(final String uri)}} also seems to leak though that could also be a Xerces issue because I don't see it closing its input streams, either.

Shameless plug: Use [Resource.close|http://johannburkard.de/blog/programming/java/centralizing-resource-closing.html]

I tried to get the VM to crash. After checking {{ulimit -a}}, I ran
{code}
5000.times { new XmlSlurper().parse(new File('web.xml')) }
{code}

This caused a lot of FileInputStreams to be created but I couldn't reach my limit of 1024 due to the finalizer in {{FileInputStream}}. Still, in the screenshot below you can see how the {{FileInputStream}} instances are accumulating on the heap.

Fixing this is of course easy:

{code}
  public GPathResult parse(final File file) throws IOException, SAXException {
   FileInputStream in = null;
   try {
     in = new FileInputStream(file);
     final InputSource input = new InputSource(in);
     input.setSystemId(""file://"" + file.getAbsolutePath());   
     return parse(input);
   }
   finally {
     close(in); // or if (in != null) try { in.close() } catch (IOException ex) {}
   }   
  }
{code}",paulk,johann,Major,Closed,Fixed,16/Sep/11 16:30,05/Apr/15 14:44
Bug,GROOVY-5030,12815882,Calling a method overwritten via metaClass from another method uses the original (non-overwritten) method if the overridden class extends something,"Hi Everyone

I'm using Groovy 1.8.2, and there's a variant of http://jira.codehaus.org/browse/GROOVY-4884.  This code fails for me:

{code}
import org.junit.Test;

public class BreakingExample_NoMetaclassOverride {
   @Test
   void testNotOverriden() {
      def list = []
      ClassUnderTest cut = new ClassUnderTest()
      cut.metaClass.getRemoteObject = { ->
         return [method: {obj -> 
            list << obj
         }] as RemoteObject
      }
      
      String val = ""Value"" 
      cut.someMethod(val)
      assert list == [val]
   }
}

public class ClassUnderTest extends RemoteObject {
   public def someMethod(String someValue) {
      RemoteObject object = getRemoteObject()
      object.method(someValue)
   }
   protected RemoteObject getRemoteObject() {
      return new RemoteObject()
   }
}

public class RemoteObject {
   public void method(obj) { /* Something */ }
}
{code}

The only difference between this and #4884 is the ""extends RemoteObject"" added to ClassUnderTest.  With it in, the test fails.  Remove it, the test passes.

Please let me know if you need any more info!

Thanks
Jason Griffith",roshandawrani,jason@bobberinteractive.com,Major,Closed,Fixed,19/Sep/11 20:57,14/Oct/11 00:28
Bug,GROOVY-5033,12815866,Annotations on a method with optional parameters do not work,"I found this bug when using Springcache in a Grails project. I have a service method which flushes a cache.

{code}
class MyService { 
    @CacheFlush('MyCache') 
    def update(paramA, paramB, paramC=null) { 
        ... 
    } 
}
{code}

When I call the update method with all three parameters, Springcache flushes the cache correctly. But when I call with only the first two parameters, Springcache will not flush the cache.",roshandawrani,kcheang,Major,Closed,Fixed,21/Sep/11 05:31,14/Oct/11 00:28
Bug,GROOVY-5034,12818075,Groovy is producing incorrect generic type signatures for getters that are created by closures,"Example say you have a method like:

{code}
    def <T> T  mockController(Class<T> controllerClass) {


        doStuff {
            println controllerClass
            ....
        }
    }
{code}

The closure passed to doStuff contains within its byte code a generated getter called ""getControllerClass"" with the following type signature:

{code}
  public Class<T> getControllerClass()
  {
    CallSite[] arrayOfCallSite = $getCallSiteArray();
    return (Class)ScriptBytecodeAdapter.castToType(this.controllerClass.get(), $get$$class$java$lang$Class());
    return null;
  }

{code}

The generic type T is not declared anywhere. This causes exceptions on OpenJDK such as:

{code}
java.lang.NullPointerException
at com.sun.beans.TypeResolver.resolve(TypeResolver.java:321)
at com.sun.beans.TypeResolver.resolve(TypeResolver.java:351)
at com.sun.beans.TypeResolver.resolve(TypeResolver.java:310)
at com.sun.beans.TypeResolver.resolve(TypeResolver.java:157)
at com.sun.beans.TypeResolver.resolveInClass(TypeResolver.java:78)
at java.beans.FeatureDescriptor.getReturnType(FeatureDescriptor.java:368)
at java.beans.Introspector.getTargetEventInfo(Introspector.java:1020)
at java.beans.Introspector.getBeanInfo(Introspector.java:424)
at java.beans.Introspector.getBeanInfo(Introspector.java:189)
at grails.test.mixin.web.ControllerUnitTestMixin.mockController(ControllerUnitTestMixin.groovy:268)

{code}

This issue is the cause of http://jira.grails.org/browse/GRAILS-8031",blackdrag,graemerocher,Critical,Closed,Fixed,21/Sep/11 10:49,20/Oct/11 04:51
Bug,GROOVY-5036,12815890,Date and Calendar JSON serialization yields stackoverflow errors,"Date and Calendar contain references to themselves, and the standard object properties serialization logic tries to serialize an infinite chains of such references, yielding a stack overflow error.

The idea is to serialize those dates and calendars into ISO-8601 compliant date strings, that are also parseable from JavaScript with Date.parse(...).",guillaume,guillaume,Major,Closed,Fixed,22/Sep/11 07:26,14/Oct/11 00:28
Bug,GROOVY-5037,12815831,Grails functional tests failing due to VerifyError loading classes,"Exception:

{code}
Caused by: java.lang.VerifyError: (class: org/jsecurity/grails/RealmWrapper, method: isPermitted signature: (Lorg/jsecurity/subject/PrincipalCollection;Ljava/util/List;)[Z) Expecting to find object/array on stack
	at java.lang.Class.forName0(Native Method)
	at java.lang.Class.forName(Class.java:169)
	at JsecurityGrailsPlugin$_closure7.class$(JsecurityGrailsPlugin.groovy)
	at JsecurityGrailsPlugin$_closure7.$get$$class$org$jsecurity$grails$RealmWrapper(JsecurityGrailsPlugin.groovy)
	at JsecurityGrailsPlugin$_closure7.doCall(JsecurityGrailsPlugin.groovy:338)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokePropertyOrMissing(MetaClassImpl.java:1093)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1056)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:884)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:704)
	at Jsec
{code}

Console output from functional tests:

http://hudson.grails.org/job/grails_functional_tests_2.0.x/1009/console

Code that is causing the exception looks like:

{code}
       def wrapperName = ""${realmName}Wrapper"".toString()
        ""${wrapperName}""(RealmWrapper) {
            realm = ref(""${realmName}Instance"")
            tokenClass = GrailsClassUtils.getStaticPropertyValue(grailsClass.clazz, 'authTokenClass')
        }
{code}",blackdrag,graemerocher,Blocker,Closed,Fixed,22/Sep/11 08:58,22/Sep/11 13:19
Bug,GROOVY-5040,12815898,"Variables in annotation closures should never be bound to declarations in ""enclosing"" scopes","For example, in the following code the closure variable is incorrectly bound to the method parameter, resulting in the absence of an (Object, Object) constructur and the presence of an (Object, Object, Reference) constructor for the closure:

{code}
@Foo({ value })
def doit(value) {}
{code}",blackdrag,pniederw,Major,Closed,Fixed,23/Sep/11 00:05,29/Sep/11 17:05
Bug,GROOVY-5041,12815884,Anonymous inner class constructor call referencing a getter caused NPE at compile time,"Example code:

{code}
import java.sql.Connection
import groovy.sql.Sql

class GrailsPrecondition {

    Connection getConnection() { database?.connection?.wrappedConnection }
    /**
     * Called from the check closure. Creates a <code>Sql</code> instance from the current connection.
     *
     * @return the sql instance
     */
    Sql getSql() {
        if (!connection) return null

        if (!sql) {
            sql = new Sql(connection) {
                protected void closeResources(Connection c) {
                    // do nothing, let Liquibase close the connection
                }
            }
        }

        sql
    }
}
{code}

Exception:

{code}
java.lang.NullPointerException
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitFieldExpression(AsmClassGenerator.java:1081)
	at org.codehaus.groovy.classgen.asm.ClosureWriter.loadReference(ClosureWriter.java:123)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.loadVariableWithReference(InvocationWriter.java:433)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeAICCall(InvocationWriter.java:417)
	at org.codehaus.groovy.classgen.asm.InvocationWriter.writeInvokeConstructor(InvocationWriter.java:385)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitConstructorCallExpression(AsmClassGenerator.java:914)
	at org.codehaus.groovy.ast.expr.ConstructorCallExpression.visit(ConstructorCallExpression.java:43)
	at org.codehaus.groovy.classgen.asm.BinaryExpressionHelper.evaluateEqual(BinaryExpressionHelper.java:283)
	at org.codehaus.groovy.classgen.asm.BinaryExpressionHelper.eval(BinaryExpressionHelper.java:72)
	at org.codehaus.groovy.classgen.AsmClassGenerator.visitBinaryExpression(AsmClassGenerator.java:527)
	at org.codehaus.groovy.ast.expr.BinaryExpression.visit(BinaryExpression.java:49)
	at org.codehaus.groovy.classgen.asm.StatementWriter.writeExpressionStatement(StatementWriter.java:599)
	at org.cod
{code}

The issue appears to be that a FieldExpression is created with a null FieldNode when calling the constructor new Sql(connection)",blackdrag,graemerocher,Blocker,Closed,Fixed,23/Sep/11 08:53,23/Sep/11 10:24
Bug,GROOVY-5044,12818478,Ant Groovyc compilation error in Gradle,"I ran the [Groovyc Ant task|http://groovy.codehaus.org/The+groovyc+Ant+Task] to compile my classes within Gradle. My code uses the {{@Singleton}} annotation. The version I set in the classpath is 1.8.1. The following snippet shows the code I use:

{code}
def groovycClasspath = getGroovyClasspath().asPath + System.getProperty('path.separator') + getTestRuntimeClasspath().asPath
ant.taskdef(name: 'groovyc', classname: 'org.codehaus.groovy.ant.Groovyc', classpath: getGroovyClasspath().asPath)

ant.groovyc(destdir: getClassesDir().canonicalPath, includeAntRuntime: false, classpath: groovycClasspath) {
    getSrcDirs().each { srcDir ->
        src(path: srcDir)
    }
}
{code}

When I run this without the attribute {{fork: true}} I get this compilation error:

{code}
Not an ASTTransformation: org.codehaus.groovy.transform.SingletonASTTransformation declared by groovy.lang.Singleton
{code}

Running Groovyc as forked process works fine. The compilation finished without an issue. Somebody on the [mailing list|http://groovy.329449.n5.nabble.com/Delegate-annotation-doesn-t-work-when-building-in-NetBeans-td3270943.html] reported the same error except that he uses Groovyc in Ant.",melix,subzero66,Major,Closed,Fixed,25/Sep/11 21:40,13/May/12 03:30
Bug,GROOVY-5045,12818117,SwingBuilder edt breaks with anonymous subclassing,"When within SwingBuilder.edt, the instantiation of anonymous inner classes to appears to be broken.

Example code:
{code}
import groovy.swing.SwingBuilder
import javax.swing.SwingWorker;
import javax.swing.JFrame
import java.awt.BorderLayout

class TestEDT {
    public static void main( String[] args )
    {
        def myLabel
        def swing = new SwingBuilder()
        swing.edt {
            frame(pack:true, show:true, defaultCloseOperation:JFrame.EXIT_ON_CLOSE){
                borderLayout()
                myLabel = label(""Hello"", constraints:BorderLayout.CENTER)
            }

            def sw = new SwingWorker() {
                protected Object doInBackground() throws Exception
                {
                    return 10
                }
                protected void done()
                {
                    myLabel.text = get()
                }
            }.execute();
        }
    }
}
{code}

This throws:
{code}Exception in thread ""main"" groovy.lang.GroovyRuntimeException: Could not find matching constructor for: TestEDT$1(java.lang.Class, groovy.lang.Reference)
	at groovy.lang.MetaClassImpl.invokeConstructor(MetaClassImpl.java:1481)
	at groovy.lang.MetaClassImpl.invokeConstructor(MetaClassImpl.java:1397)
	at org.codehaus.groovy.runtime.callsite.MetaClassConstructorSite.callConstructor(MetaClassConstructorSite.java:46)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCallConstructor(CallSiteArray.java:52)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:190)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callConstructor(AbstractCallSite.java:202)
	at TestEDT$_main_closure1.doCall(TestEDT.groovy:17)
{code}",,gubespam,Major,Resolved,Fixed,26/Sep/11 11:06,13/Jul/23 19:50
Bug,GROOVY-5046,12815902,incorrect result when using += operator in closure,"I get incorrect result running the following script with groovy 1.8.2:{code:Java}
a=[3:5]

class B {
        int v;
}

B b = new B();
b.v = 3

clos = {
        if (it!=null)
        {
                a[it.v] += 3
        }
}
clos.call(b)

println 'b.v = ' + b.v
println 'a = ' + a
{code}
> groovy test.groovy
b.v = 8
a = 8
",blackdrag,shumin,Major,Closed,Fixed,26/Sep/11 14:01,29/Sep/11 14:30
Bug,GROOVY-5049,12815711,File.getText() should close the streams,"Performing a lot of system calls causes Groovy to die on Linux with an IOException for ""Too many open files"". The cause is streams which are left open by system calls:

(1..n).each { ""ls"".execute().text }

This method should be changed to close all streams after it gets the text from them.",paulk,balor123,Major,Closed,Fixed,27/Sep/11 09:20,14/Oct/11 00:28
Bug,GROOVY-5052,12815893,CharsetToolkit javadoc references private guessEncoding() method in the example usage,"CharsetToolkit javadocs (http://groovy.codehaus.org/api/groovy/util/CharsetToolkit.html) for the class has the following code example:

{code}
...
// guess the encoding
Charset guessedCharset = CharsetToolkit.guessEncoding(file, 4096);
...
{code}

The instance method guessEncoding() is private and is also referenced in other public method javadoc.
",guillaume,jwagenleitner,Trivial,Closed,Fixed,29/Sep/11 01:15,14/Oct/11 00:28
Bug,GROOVY-5056,12818114,?TreeSet? Comparators not correctly comparing everything - Compares only a subset,"Hi everyone

This fails with the latest 1.8.3 snapshot - But works in 1.7.10

{code}
import java.util.logging.Logger;
import org.junit.*;

class Breaker {
   static Logger log = Logger.getLogger(getName())
   
   @Test
   void testBreaking() {
      def comparator = [compare:
         {a,b-> 
            def retVal = a.x.compareTo(b.x)
            log.info(""Comparing ${a.x} to ${b.x} and returning ${retVal}"")
            return retVal }
      ] as Comparator
   
      def ts1 = new TreeSet(comparator)
      ts1.addAll([
         new ToCompare(x:""1""),
         new ToCompare(x:""2""),
         new ToCompare(x:""3"")
      ])
       
      def ts2 = new TreeSet(comparator)
      ts2.addAll([
         new ToCompare(x:""1""),
         new ToCompare(x:""2""),
         new ToCompare(x:""3"")
      ])
      
      def difference = ts1 - ts2
      assert difference.size() == 0
   }
}

class ToCompare {
   String x
}
{code}

The test works if you pass the same list of the same objects to ts1 and ts2.  Passing in different objects with the same content fails.

For 1.8.x, the logs spit out:
{code}
Sep 29, 2011 9:03:57 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 2 to 1 and returning 1
Sep 29, 2011 9:03:57 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 3 to 1 and returning 2
Sep 29, 2011 9:03:57 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 3 to 2 and returning 1
Sep 29, 2011 9:03:57 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 2 to 1 and returning 1
Sep 29, 2011 9:03:57 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 3 to 1 and returning 2
Sep 29, 2011 9:03:57 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 3 to 2 and returning 1
{code}

For 1.7.10, the logs spit out:
{code}

Sep 29, 2011 9:07:09 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 2 to 1 and returning 1
Sep 29, 2011 9:07:09 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 3 to 1 and returning 2
Sep 29, 2011 9:07:09 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 3 to 2 and returning 1
Sep 29, 2011 9:07:09 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 2 to 1 and returning 1
Sep 29, 2011 9:07:09 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 3 to 1 and returning 2
Sep 29, 2011 9:07:09 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 3 to 2 and returning 1
Sep 29, 2011 9:07:09 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 1 to 2 and returning -1
Sep 29, 2011 9:07:09 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 1 to 1 and returning 0
Sep 29, 2011 9:07:09 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 2 to 2 and returning 0
Sep 29, 2011 9:07:09 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 3 to 2 and returning 1
Sep 29, 2011 9:07:09 PM sun.reflect.NativeMethodAccessorImpl invoke0
INFO: Comparing 3 to 3 and returning 0
{code}

Please let me know if there's anything else ya'll need!

Cheers
Jason Griffith",roshandawrani,jason@bobberinteractive.com,Major,Closed,Fixed,29/Sep/11 23:11,02/Nov/11 15:34
Bug,GROOVY-5061,12815859,Incorrect translation of class property which type has array as enclosed type argument,"When compiling via Ant Groovyc task, the compiler generates erroneous Java from the two following files:

{code:title=B.groovy}
class B {
    Map<String, Map<String, Integer[]>> columnsMap = [:]
}
{code}


{code:title=C.java}
public class C {
    public void f(B b) {
        System.out.println(b.getColumnsMap());
    }
}
{code}

Ant compilation results in many lines with the same error (full log attached):

{noformat}
  [groovyc] /tmp/groovy-generated-2531065055182916850-java-source/B.java:16: ';' expected
  [groovyc] public  java.util.Map<java.lang.String, java.util.Map<java.lang.String, [Ljava.lang.Integer;>> getColumnsMap() { return (java.util.Map<java.lang.String, java.util.Map<java.lang.String, [Ljava.lang.Integer;>>)null;}
{noformat}

Notes:
* if C.java is excluded from build, everything compiles fines.
* Eclipse plug-in compiles code fine too.

And if it's needed, the Ant task:

{code:xml}
    <target name=""compile2"" depends=""check_java_version,prepare_libs"">
        <mkdir dir=""temp_src/classes""/>

        <path id=""groovy.class.path"">
            <fileset dir=""${lib.dir}/groovy"" includes=""*.jar""/>
        </path>

        <taskdef name=""groovyc""
                 classname=""org.codehaus.groovy.ant.Groovyc""
                 classpathref=""groovy.class.path"" />

        <groovyc srcdir=""temp_src""
                 destdir=""temp_src/classes""
                 listfiles=""yes""
                 stacktrace=""yes""
                 verbose=""yes"">
            <classpath refid=""arms.class.path""/>
            <javac source=""1.6"" target=""1.6""/>
        </groovyc>
    </target>
{code}",melix,almo,Minor,Closed,Fixed,03/Oct/11 03:49,24/Dec/11 03:08
Bug,GROOVY-5063,12815706,"Cannot set ""-Werror"" javac option when using Groovyc ant task.","When trying to configure the joint compiler to treat java compile warnings as errors I tried to set the following javac args:

* -Xlint:all
* -Werror

I found that -Xlint:all worked, but -Werror gave me an error saying that it was an unrecognised option. After some digging it seems that the joint compiler prefixes all options bound for javac with -F, which means the lint option was transformed to ""-FXlint:all"" but the -Werror option didn't get this same transformation. If I manually added the ""-F"" making it ""-FWerror"" everything worked as it should.

I went looking for whereabouts this prefixing gets applied to try and determine why it wasn't happening to ""-Werror"" but couldn't find it.",roshandawrani,ldaley,Minor,Closed,Fixed,04/Oct/11 05:46,24/Dec/11 03:08
Bug,GROOVY-5068,12815915,SecureASTCustomizer receiver needs documentation,"SecureASTCustomizer offers the ability to blacklist receiver methods.  Without at least a minor bit of documentation, I'm concerned that this will result in the naive user creating ""secure"" environments that are anything but.

In particular, in order to stop execution of a method on a class (I'll assume System.exit() for this example), the following configuration is required:

{code}
                receiversClassesBlackList = [
                    Object,
                    Script,
                    GroovyShell,
                    Eval,
                    System,
                ].asImmutable()
                
                expressionsBlacklist = [MethodPointerExpression].asImmutable()
{code}

It's pretty certain that this would be a surprise to most users.

We should fix this in documentation, at a minimum, and I'd also propose that we provide a new example class using Blacklists.

We should also detail that in the event of using whitelist receivers, the above classes must also not be specified.",melix,driscoll,Minor,Closed,Fixed,09/Oct/11 13:09,10/Jul/13 04:42
Bug,GROOVY-5074,12814689,SwingBuilder never shuts down the default executor service,"If you use the swingbuilder.doOutside() method, the execution is delegated to an executor service which is set globally (static variable). The problem is that when application exits, the executor service is never shutdown, so the VM won't exit.

Steps to reproduce :

1. Open Groovy Console
2. any code like ""println 'hello'"" will do
3. Open AST Browser (which internally calls doOutside)
4. Exit the console with the menu. The application won't return
5. kill -3 <pid of the groovy console> to obtain a thread dump

The thread dump shows threads waiting from the executor service :

{code}
""pool-1-thread-2"" prio=10 tid=0x0000000040cbe800 nid=0x1204 waiting on condition [0x00007f1a06478000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000784fe14b0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:662)

""pool-1-thread-1"" prio=10 tid=0x0000000041c6b800 nid=0x1203 waiting on condition [0x00007f1a06579000]
   java.lang.Thread.State: WAITING (parking)
	at sun.misc.Unsafe.park(Native Method)
	- parking to wait for  <0x0000000784fe14b0> (a java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject)
	at java.util.concurrent.locks.LockSupport.park(LockSupport.java:156)
	at java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject.await(AbstractQueuedSynchronizer.java:1987)
	at java.util.concurrent.LinkedBlockingQueue.take(LinkedBlockingQueue.java:399)
	at java.util.concurrent.ThreadPoolExecutor.getTask(ThreadPoolExecutor.java:947)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:662)

{code}

",aalmiray,melix,Major,Closed,Fixed,12/Oct/11 14:23,24/Jan/14 13:18
Bug,GROOVY-5076,12815710,the shorthand notation for Grab isn't working correctly,"The following should work:
{code}
@Grab('commons-digester:commons-digester:2.1;transitive=false')
{code}
Instead, a compilation error is thrown because the 'false' value is left as a String not converted to a boolean.",paulk,paulk,Major,Closed,Fixed,14/Oct/11 01:14,07/Apr/15 19:07
Bug,GROOVY-5077,12815850,GroovyCastException after upgrade from Groovy 1.7.10 to 1.8.3,"Have a catetegory with
{code}
  public static Object asType(Byte p_byteSelf, Class<?> p_class)
  {
    //...
    return new MyClass(p_byteSelf);
  }
{code}

With Groovy 1.7.10 the script
{code}
  def v
  v = (byte)15 as MyClass  
{code}
worked fine.

With Groovy 1.8.3 a org.codehaus.groovy.runtime.typehandling.GroovyCastException is thrown.

Has the operator precedence been changed?

",,veita,Major,Closed,Fixed,14/Oct/11 04:12,08/Mar/22 12:16
Bug,GROOVY-5078,12815922,Compiler should not allow 2 methods with same name and no argument to return different types,"This compiles fine, but it shouldn't be allowed : 
{code}
class A {
   int foo() { 1 }
   long foo() {2L }
}
new A().foo()
{code}

(returns 1)
",blackdrag,melix,Major,Closed,Fixed,14/Oct/11 04:16,14/Oct/13 16:53
Bug,GROOVY-5080,12818118,Inner type default constructors incorrectly tagged synthetic,"Possibly groovy is doing this by design, but I suspect not.  When compiling this code:

{code}
class Outer {
  static class Inner { }
}
{code}

groovy creates a public constructor for the Outer.Inner type (because it doesn't have one) but it marks it synthetic.  Synthetic members cannot be called from user written code.  So once compiled I can't write this java code:

{code}
class Foo {
  public void m() {
    new Outer.Inner();
  }
}
{code}

As it will complain the constructor isn't there (it is, but it is synthetic).  Javac does not make these constructors synthetic, I can't think of a good reason why groovy should either (?)

To fix it, change the InnerClassVisitor.visitClass method.  Where it reads (line 73 for me):

{code}
innerClass.addConstructor(PUBLIC_SYNTHETIC, new Parameter[0], null, null);
{code}

change it to:

{code}
innerClass.addConstructor(ACC_PUBLIC, new Parameter[0], null, null);
{code}

This change appears to continue to pass all the tests.

Note you won't see the problem if you compile all the code together (both java and groovy) - I haven't looked but of course the stubs cannot indicate syntheticness before calling javac (and if the stubs contained no ctor in that inner type, it would get the javac behaviour of a non synthetic one).  In eclipse the problem won't happen on a full build because default constructors are not marked synthetic by the conversion layer (between groovy and eclipse types).  It can be seen on an incremental build in eclipse (of the java code) because it then uses the bytecode form of the groovy types to satisfy dependencies.
",roshandawrani,aclement,Minor,Closed,Fixed,16/Oct/11 14:14,24/Dec/11 03:08
Bug,GROOVY-5082,12818120,Sometimes invalid inner class reference left in .class files produced for interfaces,"Compile this:

{code}
interface X {
  public String compute();
}
{code}

Upon javap'ing the result we see this InnerClass attribute:

{code}
public interface X
  SourceFile: ""X.groovy""
  InnerClass: 
   #10= #9 of #2; //""1""=class X$1 of class X
  minor version: 0
  major version: 47
  Constant pool:
{code}

to X$1.  But there is no X$1 produced on disk.  Some environments (e.g. eclipse) are attempting to make sense of this and failing with a 'cant find type' message as they can't find the class file.  groovy shouldn't be including these references.  I believe a decision is made up front that one of these types will be created and then later a decision is taken that there is no need to produce one, but the original reference is left in the interface type (and so it is captured in the attribute).

I changed two things to fix this:

1. Added a method to ClassNode so that it could be told to forget about an interface like this:
{code}
    public void forgetInnerClass(InnerClassNode icn) {
        if (innerClasses!=null) {
        innerClasses.remove(icn);
        }
    }
{code}

2. And then in the code that decides it doesn't need one (in AsmClassGenerator), forget is called so that the InnerClass attribute isn't added for it:
{code}
protected void createInterfaceSyntheticStaticFields() {
    if (referencedClasses.isEmpty()) {
        controller.getClassNode().forgetInnerClass(controller.getInterfaceClassLoadingClass()); // my new line
        return;
    }
    ...
{code}

this appears to fix it and continues to pass all the groovy tests.",blackdrag,aclement,Minor,Closed,Fixed,17/Oct/11 13:45,01/Feb/12 11:10
Bug,GROOVY-5084,12815946,Groovydoc can't handle @link tags in package-info.java,"A @link tag in package-info.java abruptly stops the comment shown in the generated Groovydoc. Compare http://gradle.org/releases/latest/docs/javadoc/index.html to http://gradle.org/releases/latest/docs/groovydoc/index.html.

Would it help to provide a package-info.groovy as well?",paulk,pniederw,Major,Closed,Fixed,18/Oct/11 04:14,12/Feb/12 04:03
Bug,GROOVY-5087,12815924,Variables from a static import scope used in GStrings resolve to null under certain conditions,"I would expect ""$logDir/foo.txt"" to be ""foo/foo.txt"", since I just assigned logDir; however, this doesn't happen. I do not understand why this happens.

Currently, the 2nd and 3rd assertions fail (comment out the 2nd to see the third fail)",paulk,dgrnbrg,Minor,Closed,Fixed,18/Oct/11 10:51,13/May/12 03:30
Bug,GROOVY-5089,12818088,Unnecessary unboxing and casting in boolean handling in generated bytecode,"This sample code:
{code}
class GroovyBooleanTest {
	public boolean someCall() {
		return true;
	}
	
	public void somecode() {
		boolean val = someCall()
		println val
	}

}
{code}

produces very redundant bytecode (decompiled with jd-gui):
{code}
import groovy.lang.GroovyObject;
import groovy.lang.MetaClass;
import org.codehaus.groovy.runtime.BytecodeInterface8;
import org.codehaus.groovy.runtime.ScriptBytecodeAdapter;
import org.codehaus.groovy.runtime.callsite.CallSite;
import org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation;

public class GroovyBooleanTest implements GroovyObject {
    public GroovyBooleanTest()
  {
    GroovyBooleanTest this;
    CallSite[] arrayOfCallSite = $getCallSiteArray();
    MetaClass localMetaClass = $getStaticMetaClass();
    this.metaClass = localMetaClass;
  }

    public boolean someCall() {
        CallSite[] arrayOfCallSite = $getCallSiteArray();
        return DefaultTypeTransformation.booleanUnbox(Boolean.TRUE);
        return DefaultTypeTransformation.booleanUnbox((Integer)DefaultTypeTransformation.box(0));
    }

    public void somecode() {
        CallSite[] arrayOfCallSite = $getCallSiteArray();
        boolean val = 0;
        Object localObject;
        boolean bool1;
        if ((!BytecodeInterface8.isOrigZ()) || (__$stMC) || (BytecodeInterface8.disabledStandardMetaClass())) {
            localObject = arrayOfCallSite[0].callCurrent(this);
            val = DefaultTypeTransformation.booleanUnbox((Boolean)ScriptBytecodeAdapter.castToType(localObject,
                    $get$$class$java$lang$Boolean()));
        }
        else {
            bool1 = someCall();
            val = DefaultTypeTransformation.booleanUnbox((Boolean)ScriptBytecodeAdapter.castToType(
                    (Boolean)DefaultTypeTransformation.box(bool1), $get$$class$java$lang$Boolean()));
        }
        arrayOfCallSite[1].callCurrent(this, (Boolean)DefaultTypeTransformation.box(val));
    }

    static {
        __$swapInit();
        Long localLong1 = (Long)DefaultTypeTransformation.box(0L);
        __timeStamp__239_neverHappen1319001147656 = DefaultTypeTransformation.longUnbox(localLong1);
        Long localLong2 = (Long)DefaultTypeTransformation.box(1319001147656L);
        __timeStamp = DefaultTypeTransformation.longUnbox(localLong2);
    }
}
{code}


This part of the bytecode is interesting:
{code}

            bool1 = someCall();
            val = DefaultTypeTransformation.booleanUnbox((Boolean)ScriptBytecodeAdapter.castToType(
                    (Boolean)DefaultTypeTransformation.box(bool1), $get$$class$java$lang$Boolean()));
{code}
bool1 and val are both already booleans. It goes through many unnecessary layers before assigning val to bool1.

",blackdrag,lhotari,Critical,Closed,Fixed,19/Oct/11 00:20,09/Nov/11 10:17
Bug,GROOVY-5090,12815932,Groovy compiler generates invalid byte code for local boolean variables that later on are referenced in a closure,"Some sample code to show the bug:

{code}
class GroovyBooleanTest {
	public boolean someCall() {
		return true;
	}
	
	public void somecode() {
		boolean val = someCall()
		println val
		def c = {
			val
		}
		boolean val2 = c.call()
		println val2
	}

}
{code}

decompiled with jd-gui:
{code}
import groovy.lang.Closure;
import groovy.lang.GroovyObject;
import groovy.lang.MetaClass;
import groovy.lang.Reference;
import org.codehaus.groovy.runtime.BytecodeInterface8;
import org.codehaus.groovy.runtime.GeneratedClosure;
import org.codehaus.groovy.runtime.ScriptBytecodeAdapter;
import org.codehaus.groovy.runtime.callsite.CallSite;
import org.codehaus.groovy.runtime.typehandling.DefaultTypeTransformation;

public class GroovyBooleanTest implements GroovyObject {
	public GroovyBooleanTest()
  {
    GroovyBooleanTest this;
    CallSite[] arrayOfCallSite = $getCallSiteArray();
    MetaClass localMetaClass = $getStaticMetaClass();
    this.metaClass = localMetaClass;
  }

	public boolean someCall() {
		CallSite[] arrayOfCallSite = $getCallSiteArray();
		return DefaultTypeTransformation.booleanUnbox(Boolean.TRUE);
		return DefaultTypeTransformation
				.booleanUnbox((Integer) DefaultTypeTransformation.box(0));
	}

	public void somecode() {
		CallSite[] arrayOfCallSite = $getCallSiteArray();
		boolean val = new Reference((Boolean) DefaultTypeTransformation.box(0));
		Object localObject1;
		boolean bool1;
		if ((__$stMC) || (BytecodeInterface8.disabledStandardMetaClass())) {
			localObject1 = arrayOfCallSite[0].callCurrent(this);
			((Reference) val).set((Boolean) ScriptBytecodeAdapter.castToType(
					localObject1, $get$$class$java$lang$Boolean()));
		} else {
			bool1 = someCall();
			((Reference) val).set((Boolean) ScriptBytecodeAdapter.castToType(
					(Boolean) DefaultTypeTransformation.box(bool1),
					$get$$class$java$lang$Boolean()));
		}
		arrayOfCallSite[1]
				.callCurrent(this, (Boolean) DefaultTypeTransformation
						.box(DefaultTypeTransformation.booleanUnbox(val.get())));
		GroovyBooleanTest._somecode_closure1 local_somecode_closure1 = new GroovyBooleanTest._somecode_closure1(
				this, val);
		Object c = local_somecode_closure1;

		Object localObject2 = arrayOfCallSite[2].call(c);
		boolean val2 = DefaultTypeTransformation.booleanUnbox(localObject2);
		arrayOfCallSite[3].callCurrent(this,
				(Boolean) DefaultTypeTransformation.box(val2));
	}

	static {
		__$swapInit();
		Long localLong1 = (Long) DefaultTypeTransformation.box(0L);
		__timeStamp__239_neverHappen1319016363968 = DefaultTypeTransformation
				.longUnbox(localLong1);
		Long localLong2 = (Long) DefaultTypeTransformation.box(1319016363968L);
		__timeStamp = DefaultTypeTransformation.longUnbox(localLong2);
	}

	class _somecode_closure1 extends Closure implements GeneratedClosure {
		public _somecode_closure1(Object _thisObject, Reference val) {
			super(_thisObject);
			boolean bool = val;
			this.val = bool;
		}

		public Object doCall(Object it) {
			CallSite[] arrayOfCallSite = $getCallSiteArray();
			return this.val.get();
			return null;
		}

		public Boolean getVal() {
			CallSite[] arrayOfCallSite = $getCallSiteArray();
			return (Boolean) ScriptBytecodeAdapter.castToType(this.val.get(),
					$get$$class$java$lang$Boolean());
			return null;
		}

		public Object doCall() {
			CallSite[] arrayOfCallSite = $getCallSiteArray();
			return arrayOfCallSite[0].callCurrent(this, ScriptBytecodeAdapter
					.createPojoWrapper(null, $get$$class$java$lang$Object()));
			return null;
		}

		static {
			__$swapInit();
		}
	}
}
{code}

Invalid byte code is shown in this line:
{code}
		boolean val = new Reference((Boolean) DefaultTypeTransformation.box(0));
{code}

JVM accepts this, but the debugger will show always ""false"" for such variables.


The type information looks invalid also in the closure class:
{code}
	class _somecode_closure1 extends Closure implements GeneratedClosure {
		public _somecode_closure1(Object _thisObject, Reference val) {
			super(_thisObject);
			boolean bool = val;
			this.val = bool;
		}
{code}",blackdrag,lhotari,Blocker,Closed,Fixed,19/Oct/11 04:40,09/Nov/11 10:17
Bug,GROOVY-5097,12818121,Proxy unwrapping causes UndeclaredThrowableExceptions,"When upgrading to Grails 1.3.7, we started having trouble with UndeclaredThrowableExceptions popping out of our withTransaction blocks. Normally, throwing checked exceptions out of these blocks has worked perfectly well. We traced it down to Groovy 1.7.6's Git commit e53591b122bbfd039a03d37f2868ff95b5202c6b (http://svn.codehaus.org/groovy/branches/GROOVY_1_7_X@21327) which changes src/main/org/codehaus/groovy/runtime/ConversionHandler.java to unwrap GroovyRuntimeExceptions coming out of invokeCustom(). This takes what is normally an InvokerInvocationException containing our real checked exception - which would normally bounce all the way out to user code - and instead throws the real checked exception underneath, which promptly tries to propagate through a proxy which doesn't allow any checked exceptions through at all, and turns into an UndeclaredThrowableException.

Immediate fix would be to revert the commit.

2 sample apps attached (one is for Grails 1.3.6 [bundles Groovy 1.7.5], other is for Grails 1.3.7 [bundles Groovy 1.7.8], they are otherwise identical). Get a copy of Grails 1.3.6 and Grails 1.3.7.

Extract the 1.3.6 app and type 'grails console'. Run:

try {
   test.Test.withTransaction {
      throw new Exception(""Testing throwing exceptions out of withTransaction"")
   }
} catch (Throwable t) {
   println t
}

Expected output: 

java.lang.Exception: Testing throwing exceptions out of withTransaction

Now, switch to Grails 1.3.7, and run grails console for the 1.3.7 app. Same code:

try {
   test.Test.withTransaction {
      throw new Exception(""Testing throwing exceptions out of withTransaction"")
   }
} catch (Throwable t) {
   println t
}

Expected (undesirable) output:

java.lang.reflect.UndeclaredThrowableException",,jstoneham,Major,Closed,Fixed,23/Oct/11 22:18,10/Aug/13 05:02
Bug,GROOVY-5098,12815942,Using log variable created with @Log causes compiler error if used in static method,"If I have a class with it and use the 'log' variable in a static method I get a compilation error. 


Log4jTest.groovy:
import groovy.util.logging.Log4j

@Log4j
class TestLog4j {

  public static void main(String[] args) {
    log.info ""Hello World""
  }
}

Error Message:
Apparent variable 'log' was found in a static scope but doesn't refer to a local variable, static field or class. Possible causes:
You attempted to reference a variable in the binding or an instance variable from a static context.
You misspelled a classname or statically imported field. Please check the spelling.
You attempted to use a method 'log' but left out brackets in a place not allowed by the grammar. ",hamletdrc,smartini,Major,Closed,Fixed,24/Oct/11 05:36,24/Dec/11 03:08
Bug,GROOVY-5101,12818108,GroovyCastException occurs since 1.8.3,"GroovyServ 0.9 doesn't work with Groovy 1.8.3. Surely, it still works well with Groovy 1.8.2.

Stacktrace on starting-up of GroovyServ:
{code}
 2011/10/25 18:17:01.121 ---> org.codehaus.groovy.runtime.typehandling.GroovyCastException: Cannot cast object 'groovy.lang.Reference@464693f9' with class 'groovy.lang.Reference' to class 'java.lang.Runnable'
 2011/10/25 18:17:01.121         at org.jggug.kobo.groovyserv.RequestWorker$2.<init>(RequestWorker.groovy)
 2011/10/25 18:17:01.121         at org.jggug.kobo.groovyserv.RequestWorker.newTaskFor(RequestWorker.groovy:85)
 2011/10/25 18:17:01.121         at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:77)
 2011/10/25 18:17:01.121         at java_util_concurrent_ExecutorService$submit.callCurrent(Unknown Source)
 2011/10/25 18:17:01.121         at org.jggug.kobo.groovyserv.RequestWorker.start(RequestWorker.groovy:67)
 ...(snip)...
{code}

At line 85 of RequestWorker:
{code}
new FutureTask(runnable, defaultValue) { // anonymous inner class
    String toString() { runnable.id }
}
{code}

When I tried to fix it as following, it worked well even with groovy 1.8.3.
{code}
        new FutureTask(runnable, defaultValue)
{code}

I'm sorry. I wanted to report the sample code which is independent on GroovyServ, but the problem didn't occur on the simplified sample code.

This might be the same problem as http://jira.codehaus.org/browse/GROOVY-5077.


GroovyServ: http://kobo.github.com/groovyserv/
RequestWorker's full source code: https://github.com/kobo/groovyserv/blob/master/src/main/groovy/org/jggug/kobo/groovyserv/RequestWorker.groovy",blackdrag,nobeans,Blocker,Closed,Fixed,25/Oct/11 04:50,20/Dec/11 09:32
Bug,GROOVY-5102,12815949,Unrelated changes cause BigDecimal division to return Double,"In Groovy 1.8.3 the first test passes while the second fails.  The return type of the second operation (and third, if it were to get to it) becomes a Double.

The tests both pass in 1.8.2.

{code}
class BigDecimalTest extends GroovyTestCase{
    public void testMath1() {
        assert BigDecimal == (3/2).getClass()
        assert BigDecimal == (7.0/8.0).getClass()
        assert BigDecimal == (new BigDecimal(3.0)/new BigDecimal(2.0)).getClass()
        true
    }

    public void testMath2() {
        assert BigDecimal == (3/2).getClass()
        assert BigDecimal == (7.0/8.0).getClass()
        assert BigDecimal == (new BigDecimal(3.0)/new BigDecimal(2.0)).getClass()
    }
}
{code}",blackdrag,sjurgemeyer,Blocker,Closed,Fixed,25/Oct/11 10:05,03/Nov/11 14:16
Bug,GROOVY-5103,12814743,groovy compiler ignores import * for static nested classes in java sources,"(using Groovy 1.8.3, JVM 1.6.0_23 OpenJDK)

Given this class:
{code:title=Foo.java}
package org.test;

public class Foo {
    public static class Bar {
        String message;
        
        public Bar(String message) {
            this.message = message;
        }
    }
    public static class Cat {
        Integer amount;

        public Cat(Integer amount) {
            this.amount = amount;
        }
    }
}
{code}

Groovy code can import both of these static nested classes like this:
{code}
import static org.test.Foo.*
{code}

However, in Java they are imported like this:
{code}
import org.test.Foo.*;
{code}

Using the Groovy compiler to compile any Java files which do the above will result in the following error if any of the static nested classes are used:
{code}
App.java: 9: unable to resolve class Bar 
 @ line 9, column 13.
   		Bar bar = new Bar(""Hello"");
{code}

Is this expected? If so, is there a known workaround (or maybe I'm just doing it wrong...)?",emilles,clarkdave,Minor,Closed,Fixed,26/Oct/11 16:45,19/Mar/22 13:49
Bug,GROOVY-5106,12815789,Same interface can be implemented with two different generic types,"Given the following code :
{code}

interface One<T> {
    void echo(T obj);
}

interface Two extends One<String> {}

class OneImpl implements Two,One<Integer> {

    public void echo(String obj) { println obj }
}

def o = new OneImpl()
{code}

The ""One"" interface is implemented twice with two different generic types, which should not be allowed. Here's another example, more complex :

{code}
interface Transcoder<T, U> {
    T transcode(U obj);
}

interface TranscoderToString<V> extends Transcoder<String, V>{
}

abstract class AbstractToStringTranscoder<V> implements TranscoderToString<V> {

    public String transcode(final V obj) {
        return obj.toString();
    }
}

class IntegerToStringTranscoder extends AbstractToStringTranscoder<Integer> implements TranscoderToString<Boolean> {

}
{code}
",emilles,melix,Major,Closed,Fixed,28/Oct/11 15:52,03/Feb/22 22:35
Bug,GROOVY-5109,12817836,inner class inheritance with outer class inheritance can't find constructor,"Instantiation fails at runtime if I have an inner class extending an inner class defined in the outer class's parent I get a {{NoSuchMethodError}} on instantiation.  In other words, *{{C2 extends C1}}* and *{{C2.B extends C1.A}}* yields *{{C1$A: method <init>()V not found}}* in *{{C2.B}}*'s constructor.  A repeatable test case is below:

{code}
class FailingInnerClassInheritance {
	
	public static void main(String[] args) {
		new C2()
	}
	
}

class C1 {
	class A {}
}

class C2 extends C1 {
	{ new B() }
	class B extends C1.A {}
}
{code}

The analogous example works in Java. It fails in Groovy even if we construct {{B}} later (after {{C2}} construction) and try to supply various explicit constructors.
",melix,alex.heneveld,Major,Closed,Fixed,30/Oct/11 21:23,24/Dec/11 03:08
Bug,GROOVY-5110,12818412,Groovy examples on the codehaus site don't appear to work on some versions of IE,"When using IE8 (version: 8.0.7601.17514) some (most?) of the code examples don't appear to work.

On the ""Installing Groovy"" page which doesn't display (screen snapshot attached) the html used is:
{code}
<div class=""code panel"" style=""border-width: 1px;""><div class=""codeContent panelContent"">
<script type=""syntaxhighlighter"" class=""theme: Confluence; brush: java; gutter: false""><![CDATA[groovysh]]></script>
</div></div>
{code}
but on the home page which has html like:
{code}
<div class=""preformatted panel preformattedContent panelContent null3""><pre>
...
</pre></div>
{code}
it does seem to display correctly.
",guillaume,paulk,Major,Closed,Fixed,31/Oct/11 00:50,07/Apr/15 19:07
Bug,GROOVY-5111,12815972,Named parameter support breaks with postgres casting syntax,"Postgres supports a casting syntax like ""SELECT '2011-01-01'::date"". I recently wrote a query like: ""SELECT * FROM table WHERE my_date BETWEEN ?-'1 week'::interval AND ?"" and was surprised to receive strange errors about java.sql.Date not supporting the ""interval"" property. It would appear that the regex parsing that identifies named parameters is a bit broken in this instance.",paulk,hollandlef,Minor,Closed,Fixed,01/Nov/11 11:58,15/Jun/12 22:56
Bug,GROOVY-5112,12818127,"""A transform used a generics containing ClassNode"" error messages for @Delegate","I had a quick look but didn't see this already raised.  This program:

{code}
class ListWrapper {  
  @Delegate
  List myList
  
  @Delegate
  URL homepage
}
{code}

causes this error:

{code}
org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
Code.groovy: -1: A transform used a generics containing ClassNode java.lang.Class <T extends java.lang.Object -> java.lang.Object> for the method public final java.lang.Object getContent([Ljava.lang.Class; param0) throws java.io.IOException { ... } directly. You are not suppposed to do this. Please create a new ClassNode refering to the old ClassNode and use the new ClassNode instead of the old one. Otherwise the compiler will create wrong descriptors and a potential NullPointerException in TypeResolver in the OpenJDK. If this is not your own doing, please report this bug to the writer of the transform.
 @ line -1, column -1.
1 error

{code}
",melix,aclement,Major,Closed,Fixed,01/Nov/11 12:41,24/Dec/11 03:08
Bug,GROOVY-5114,12815939,String[] to Set coercion seems broken in some cases,See http://jira.grails.org/browse/GRAILS-8194 for a description of the problem,melix,graemerocher,Blocker,Closed,Fixed,04/Nov/11 11:56,31/May/23 14:49
Bug,GROOVY-5119,12815880,Node.depthFirst() 'forgets' text nodes,"Node.depthFirst() is expected to recursively include all Node.children() elements. The attached program demonstrates a counter example, where Node.depthFirst() 'forgets' all 3 text chldren resulting in a list of 4 instead of 7 elements.

The correct semantics of ""Node.depthFirst()"" is defined by ""depthFirst( node )"" in the attached program.",paulk,zroh,Major,Closed,Fixed,10/Nov/11 08:09,13/May/12 03:30
Bug,GROOVY-5122,12818371,"static fields on interface not getting initialized, when being initialized to an anonymous class instance","The following code shows a _static_ field on an _interface_ not initialized at the point when the constructor of an _instance_ of an implementing class is being run.

{code}

package ignored.alex.failingcallsite

import java.lang.reflect.Field;

class FailingCallSiteLocalClass {
	public static void main(String[] args) {
		new B()		
	}
}

interface A {
	public static X x = new X() {
		public void foo() {}
	}
}

interface X {
	void foo()
}

class B implements A {
	public B() {
        for (Field f in getClass().getFields()) {
			println f
			println f.get(this)
		}
	}
}
{code}

Throws {{java.lang.IllegalAccessError}} when {{x}} is accessed via {{f.get(this)}}.

I would expect {{x}} to have been initialized by this point. Is there something subtle in when interfaces are initialized?

There is no problem if {{A}} is a _class_ extended by {{B}}, and more curiously, no problem if {{x}} is set equal to a named outer class (e.g. a class {{X2 implements X}}).
",melix,alex.heneveld,Major,Closed,Fixed,14/Nov/11 07:28,29/Mar/17 09:48
Bug,GROOVY-5125,12815793,GroovyScriptEngine load from  jarURL fail,"GroovyScriptEngine loadScriptByName from jarURL, first time is ok, second time fail, GroovyScriptEngine attempts to load from a fileURL.
For fix this, change this:
{code}
String path = conn.getURL().getPath();
{code}
to
{code}
String path = conn.getURL().toString();
{code}",blackdrag,wonder365,Major,Closed,Fixed,15/Nov/11 08:09,11/Sep/12 01:15
Bug,GROOVY-5126,12815879,"Arrays.asList(2L,2L) returns incorrect inferred generic type","The inferred return type for {{Arrays.asList(2L,2L)}} is {{List<Long>}} but should be {{List<? extends Long>}}. If not, then the following code will fail, though it is valid:

{code}
Set<Number> s4 = new HashSet<Number>(Arrays.asList(0L,0L))
{code}",melix,melix,Major,Closed,Fixed,15/Nov/11 10:14,24/Dec/11 03:08
Bug,GROOVY-5127,12815925,Assignement to interface is wrongly marked as an error,"The following code throws an incompatible assignement type error, though the RHS implements the interface shown on the LHS.

{code}
Serializable ser = 'String'
{code}
",melix,melix,Major,Closed,Fixed,15/Nov/11 10:47,24/Dec/11 03:08
Bug,GROOVY-5129,12815974,groovy.json.JsonSlurper failes to parse decimal numbers correctly,"{code:title=Test.groovy|borderStyle=solid}
slurper = new groovy.json.JsonSlurper()
assert slurper.parseText('{""number"": 123456.123456789}') == [number: 123456.123456789]
{code}
{code:title=Output}
assert slurper.parseText('{""number"": 123456.123456789}') == [number: 123456.123456789]
       |       |                                         |
       |       [number:123456.125]                       false
       groovy.json.JsonSlurper@11346f77
{code}",guillaume,jonny,Major,Closed,Fixed,18/Nov/11 05:36,24/Dec/11 03:08
Bug,GROOVY-5130,12815954,Documentation: Error for one of the examples from JN0025-Starting,"One of the examples from http://groovy.codehaus.org/JN0025-Starting is incorrect.

def x= ['a', 'b', 'c'] as Integer[] //convert each item in list to an Integer
assert x[0] == 97 && x[1] == 98 && x[2] == 99 //access each element individually

It causes java.lang.NumberFormatException: For input string: ""a""

There's also a related ticket already: GROOVY-4602",paulk,defascat,Trivial,Closed,Fixed,18/Nov/11 09:03,12/Feb/12 04:03
Bug,GROOVY-5137,12815930,MockFor.getInstance() fails for abstract classes,"Trying to mock or stub an abstract class fails as of groovy 1.8.2 and later (this
works under 1.7.10, 1.8.0, and 1.8.1). This code

{code:java}
import groovy.mock.interceptor.MockFor

MockFor.getInstance(URLConnection, new URL('http://foo'))
{code}

results in

{noformat}
Caught: BUG! exception in phase 'class generation' in source unit 'Script1.groovy' SpreadExpression should not be visited here
BUG! exception in phase 'class generation' in source unit 'Script1.groovy' SpreadExpression should not be visited here
{noformat}

The breakage was possibly introduced by the fix for GROOVY-4968.

Also note that this may be related to GROOVY-4255, though I'd expect MockFor and StubFor
to work irrespective of the status of that bug.",melix,roadrunner,Major,Closed,Fixed,22/Nov/11 21:01,24/Dec/11 03:08
Bug,GROOVY-5140,12818126,ASTTransformationCustomizer uses wrong classloader to find transformer class from annotation,"ASTTransformationCustomizer.

In this method:
{code}
    private static Class<ASTTransformation> findASTTranformationClass(Class<? extends Annotation> anAnnotationClass) {
        final GroovyASTTransformationClass annotation = anAnnotationClass.getAnnotation(GroovyASTTransformationClass)
        if (annotation==null) throw new IllegalArgumentException(""Provided class doesn't look like an AST @interface"")

        Class[] classes = annotation.classes()
        String[] classesAsStrings = annotation.value()
        if (classes.length+classesAsStrings.length>1) {
            throw new IllegalArgumentException(""AST transformation customizer doesn't support AST transforms with multiple classes"")
        }
        return classes?classes[0]:Class.forName(classesAsStrings[0])
    }
{code}

{{Class.forName}} has no classloader, which means it using the loader associated with {{ASTTransformationCustomizer}}, but when I use it from inside STS that loader is associated with the Eclipse/STS infrastructure and the transform isn't on its classpath (since it is part of the compiled project, not the compiler infrastructure).

It would seem more logical/correct if {{Class.forName}} here should use an explicit classloader and pass it the classloader from the annotation instead. After all, I think the annotation class should refer to the transformer class, so the annotation's classloader should be able to find the transform class.

On the other hand, there doesn't seem to be a logical connection between {{ASTTransformationCustomizer}} and some random transform class attached to a random annotation that guarantees the class can be found by the {{ASTTransformationCustomizer}}'s classloader.
",melix,kdvolder,Minor,Closed,Fixed,23/Nov/11 18:01,24/Dec/11 03:08
Bug,GROOVY-5141,12815812,"The static type checker may be unable to choose between two ""identical"" methods from DGM","The following code:
{code}
[1,2,3].collect { it.toString() }
{code}

causes the static type checker to fail with a cryptic message :
{code}Reference to method is ambiguous. Cannot choose between [MethodNode@481404130[java.util.List collect(groovy.lang.Closure)], MethodNode@1289411108[java.util.List collect(groovy.lang.Closure)]]{code}
",melix,melix,Major,Closed,Fixed,24/Nov/11 05:48,24/Dec/11 03:08
Bug,GROOVY-5142,12818414,collectMany should not use createSimilarCollection and instead behave like collect,,paulk,paulk,Major,Closed,Fixed,24/Nov/11 06:29,07/Apr/15 19:07
Bug,GROOVY-5144,12815827,JsonSlurper does not handle backslashes at the end of a String,"Whenever I try to deserialize a JSON Object with a String that ends with a correctly escaped backslash ""\\"", it throws an exception:

Caught: groovy.json.JsonException: Expected a value on line: 1, column: 25.
But got an unterminated object.
groovy.json.JsonException: Expected a value on line: 1, column: 25.
But got an unterminated object.
	at JsonSlurperBug.run(JsonSlurperBug.groovy:9)

I've attached a small repro.",guillaume,gabrielsz,Major,Closed,Fixed,24/Nov/11 08:25,12/Feb/12 04:03
Bug,GROOVY-5145,12815990,Return type inference doesn't handle closures properly,"The following code :
{code}
[1,2,3].collect { it.toString() }
{code}
has an inferred type of {{List<Closure>}} instead of {{List<String>}}.",melix,melix,Major,Closed,Fixed,24/Nov/11 09:59,24/Dec/11 03:08
Bug,GROOVY-5147,12815813,Static type checker marks null assignments as invalid,"The following code raises a compilation error:
{code}
@groovy.transform.TypeChecked
void test() {
   Integer o = null
}
{code}
",melix,melix,Critical,Closed,Fixed,25/Nov/11 11:50,24/Dec/11 03:08
Bug,GROOVY-5148,12815814,Static type checker should allow casts to character from single char strings,"The following code is not allowed by the static type checker:
{code}
char c = 'c'
{code}

nor this one:


{code}
Character c = 'c'
{code}
",melix,melix,Major,Closed,Fixed,25/Nov/11 11:52,24/Dec/11 03:08
Bug,GROOVY-5152,12815961,object instanceof MyObject[] causes ClassDefNotFound exception within Closure,"The following code causes ClassDefNotFound exception:
{code}
public Map<String, Object> toMap() {
        Map<String, Object> params = getPropertiesWithoutClassData()
        Map<String, Object> params = new HashMap<String,Object>();
        
        params.each { 
           if(it.value instanceof MyObject[]){ 
params.add(it.key, it.value)
 }else{
params.add(""test"",""test"")
           } 
        }

   return params
}
{code}
The equivalent java code works normally:
{code}
public Map<String, Object> toMap() {
        Map<String, Object> params = getPropertiesWithoutClassData()
        Map<String, Object> params = new HashMap<String,Object>();

for(String param : params)
{
 if(params.get(param) instanceof MyObject[]){ 
params.add(""works"", ""works"")
 }else{
params.add(""test"",""test"")
           } 
}

   return params
}
{code} 
Running MacOSX 10.6.6",melix,teammcs,Minor,Closed,Fixed,29/Nov/11 10:35,24/Dec/11 03:08
Bug,GROOVY-5154,12815992,Invalid Static Type Checking report on a method call with type parameters,"The following code produces static type checking error:

{code}
import groovy.transform.*

@TypeChecked 
class Foo {
    def say() {
        FooWithGenerics f
        FooBound fb
        f.say(fb)
    }
}

class FooWithGenerics {
    def <T extends FooBound> void say(T p) {
    }
}
class FooBound {
}
{code}",melix,vns,Major,Closed,Fixed,30/Nov/11 08:09,24/Dec/11 03:08
Bug,GROOVY-5156,12815971,Strange method name in the StaticTypeCheckingVisitor class,"Is the method name valid ?
{color:red}
chooseBestBethod
{color}

May be {color:blue}
chooseBestMethod
{color}
",melix,vns,Trivial,Closed,Fixed,30/Nov/11 12:05,24/Dec/11 03:08
Bug,GROOVY-5158,12816004,Encoding issue with groovy.xml.XmlUtil.serialize(),"http://groovy.329449.n5.nabble.com/Encoding-issue-with-groovy-xml-XmlUtil-serialize-td5038513.html
 

I think it should be possible to put an 'ü' charter in the xml element. 


def xml =""""""<?xml version=""1.0"" encoding=""UTF-8""?> 
<Schlüssel>
 text content 
</Schlüssel>"""""" 

groovy.util.slurpersupport.GPathResult s  = new XmlSlurper().parseText(xml) 

println groovy.xml.XmlUtil.serialize(s) // results in empty xml, why? 

----- OUTPUT 
<?xml version=""1.0"" encoding=""UTF-8""?> 



WORKAROUND
def outxml = new groovy.xml.StreamingMarkupBuilder().with {
  encoding = 'UTF-8'
  '<?xml version=""1.0"" encoding=""UTF-8""?>\n' + bindNode( s )
}
println outxml",paulk,citron,Minor,Closed,Fixed,01/Dec/11 17:40,13/May/12 03:30
Bug,GROOVY-5160,12815851,Assignments in if/else/for/while/ternary are not type checked properly,"The following code will pass, though the return type of x after the if/else statement may differ:

{code}
def x
if (cond) {
  x = new Date()
} else {
  x = 123
}
x.toInteger()
{code}

In a similar manner:
{code}
def x = '123'
for (int i=0; i<5;i++) { x = new HashSet() }
x.toInteger()
{code}

{code}
def x = '123'
while (false) { x = new HashSet() }
x.toInteger()
{code}

{code}
def x = '123'
def cond = false
cond?(x = new HashSet()):3
x.toInteger()
{code}",melix,melix,Major,Closed,Fixed,02/Dec/11 08:49,24/Dec/11 03:08
Bug,GROOVY-5161,12815959,"MetaObjectProtocol.getMetaMethod doc for ""args"" is ambiguous","http://groovy.codehaus.org/api/groovy/lang/MetaObjectProtocol.html#getMetaMethod(java.lang.String, java.lang.Object[])
---
The args parameter is described as ""The argument types"", but the method definition says ""Retrieves an instance MetaMethod for the given name and argument values, using the types of the argument values to establish the chosen MetaMethod"".

For the former, I'd expect to pass in something like [String.class]; from the latter I'd expect to have to pass in [""""]

---
The code actually works with both!
Looks like it eventually calls MetaClassHelper.castArgumentsToClassArray, which uses the arg if it's a Class, or uses argType.getClass() otherwise.

If this is guaranteed to be the case in future version, then I'd suggest the wording:
    @param args The argument types: can be the Class objects, or example argument values for inferring the types
",roshandawrani,aled.sage@gmail.com,Trivial,Closed,Fixed,02/Dec/11 11:31,24/Dec/11 03:07
Bug,GROOVY-5162,12816003,JsonOutput fails with uninformative NPE when map has null key,"{code}
import groovy.json.JsonOutput

def group = [1, 2, null].countBy{it}
def result = use (JsonOutput) {
    group.toJson()
}
{code}
throws NullPointerException due to the presence of a null a as key to map entry.
The stacktrace is rather unhelpful, so it would be nice if the JsonOutput would translate this to indicate a problem with the map content.
http://groovyconsole.appspot.com/script/601001",roshandawrani,jwb,Minor,Closed,Fixed,02/Dec/11 11:58,24/Dec/11 03:07
Bug,GROOVY-5163,12815996,IncompatibleClassChangeError when running groovysh,"Wondering if this bug is new in Groovy 1.8.4; it seems it work better in 1.8.2.

The problem is that groovysh is throwing IncompatibleClassChangeError in a few cases:
import com.not.Found // import that doesn't resolve to class
show // command doesn't run at all
purge // command doesn't run at all

Groovy 1.8.4 appears to be working well, otherwise.

Please let me know if you need further info. It seems reproducible, here.

Thanks!
John
",melix,jwb,Major,Closed,Fixed,02/Dec/11 19:46,12/Feb/12 04:03
Bug,GROOVY-5165,12815977,ASTNode#removeNodeMetaData may nullify metaData field,"The {{org.codehaus.groovy.ast.ASTNode#removeNodeMetaData}} method nullifies the {{metadata}} field if the map is empty after removal. However, the map is not supposed to be null, which will cause a NPE if you try to add metadata afterwards.",melix,melix,Major,Closed,Fixed,05/Dec/11 03:22,24/Dec/11 03:08
Bug,GROOVY-5166,12815981,Static type checker should handle closure shared variables properly,"See discussion at http://groovy.329449.n5.nabble.com/Closure-shared-variables-and-flow-typing-tp5041259p5041259.html

Copy of original post:


Yesterday, Guillaume & I had a face-to-face working session in Paris. One of our discussion subjects was centered on static type checking, and especially a corner case with closure shared variables. In this email, I will expose the problem and the solutions we had in mind. Let's start with a sample code:

{code}
def x = '123'
def cl = { x = new Date() }
x.toInteger()
{code}

The current implementation of the static type checked is totally wrong in that case (this is a bug I know about for long). Especially, it complains with the following error message:

Cannot find matching method java.util.Date#toInteger()

This is because the closure is visited before the call to x.toInteger() so the inferred type of x is changed although the closure is *not* called. This example illustrates the case of a closure shared variable, ""x"", and flow typing. In flow typing, we want this not to throw an error:

{code}
def x = new Date()
x = '123'
x.toInteger() // should work because the compiler can infer that x is of string type at this point
{code}

Now, when ""x"" is closure shared, we are facing a dangerous situation. Back to our first example:

{code}
def x = '123'
def cl = { x = new Date() }
x.toInteger()
{code}

The workaround seems to be easy: ""hey, we don't call the closure, you must know that x is still a string at line 3!"". If we do that, then we must also keep track of closure calls which may alter the shared variable:

{code}
def x = '123'
def cl = { x = new Date() }
cl()
x.toInteger() // now, this must throw an error because x has changed type !
{code}

In that case, this is a very problematic issue. Tracking shared variables is doable, but tracking closure calls depends on runtime execution and seems impossible. For example, we could have nastier code like this:

{code}
def x = '123'
def cl = { foo -> x = new Date() }
def cl2 = cl.curry('If you ever visit Nantes, we could have a talk')
cl2()
x.toInteger() // now, this must throw an error because x has changed type !
{code}

or even more problematic :

{code}
class A {
   Closure action
   def foo() { action() }
}

def x = '123'
def cl = { x = new Date() }
def a = new A(action:cl)
a.foo()
x.toInteger() // now, this must throw an error because x has changed type !
{code}

Basically, the latter example shows it is rather impossible to track closure calls implying shared variables at compile time. The first solution is to disable flow typing. We don't really like that idea, as it is definitely not in the ""Groovy"" spirit. Though flow typing may be seen as bad style, we still think things like this are groovier:

{code}
class A {}
class B extends A { void foo() {} }

A a = new B()
a.foo() // should be allowed in static mode
{code}

The first option, then, is not the one we want to promote. The second option is to go ""Java style"", and disallow closure shared variables, meaning each variable used in a closure should be final. The code you saw would therefore be invalid. But we don't like this idea because it would remove a large part of the interest of using ""lightweight"" closures.

The 3rd solution, first suggested by Guillaume, was to ignore tracking of closure execution, and let the program fail at runtime. For example, this would fail with a class cast exception:

{code}
def x = '123'
def cl = { x = new Date() }
cl()
x.toInteger()
{code}

But it would fail *at runtime*. Dynamic groovy would fail with a ""No signature of method Date#toInteger"". I don't really like this option for two reasons:
    1. it beats the concept of static type checking, which is IMHO interesting if errors are found at compile time rather than runtime
    2. it is conceptually wrong

After a short brainstorming session, where we discussed about possible warnings or disallowing assignments of shared variables in closures, I suggested an alternative option, which requires some trickery in the type checker, but seems conceptually correct: throwing an error on ""x.toInteger()"", knowing that if you assign a shared variable in a closure with an ""incompatible"" type, this is not bad style, but very bad style. The question is how can we throw an error here, without throwing an error when the variable is *not* closure shared (that is to say in the classical flow typing mode). My idea is to use the ""lowest upper bound"" algorithm to compute, for closure shared variables, the lowest common type of all assignments of a closure shared variables. Direct method calls (I mean, without explicit casts), in that case, should only be allowed on methods belonging to that common super type. This means that in that case:

{code}
def x = '123'
def cl = { x = new Date() }
cl()
x.toInteger()
{code}

We know that 'x' is assigned a string and a date. We compute the LUB of those types. Then method calls on 'x' would be checked against this type. Here, the LUB is an Object implementing Serializable. Serializable doesn't have a ""toInteger"" method, so ""toInteger"" would fail here. Guillaume, then, came with another example:

{code}
def x = '123'
def cl = { x = new Date() }
cl()
x = '456'
x.toInteger()
{code}

Using our algorithm, 'x.toInteger()' would throw a compilation error, so Guillaume said this would violate the principle of least surprise. At first, I thought we could be smarter, letting this pass if no method call was made between an assignment and the method call on the shared variable. For example, this would pass:

{code}
def x = '123'
def cl = { x = new Date() }
cl()
x = '456'
x.toInteger()
{code}

But this would not:

{code}
def x = '123'
def cl = { x = new Date() }
cl()
x = '456'
logger.debug('info')
x.toInteger() // would fail, because we don't know what logger.debug does. Potentially, it could lead to using the ""cl"" closure. If you as a human know that it would not, the compiler cannot know, so it must invalidate the call
{code}

However, Guillaume came with an excellent counter-example. What if ""x"" is used in another thread? For example :

{code}
def x = '123'
Thread.start { x = new Date() }
x = '456'
x.toInteger()
{code}

There is absolutely no guarantee that when ""x.toInteger()"" will be called, ""x"" will be of type 'String'. There are chances that it will be of type Date, depending on when the assignement is executed... We all agree that using a shared variable in this use case is a very, ugly, fool code style, but it demonstrates that at compile time, we cannot make any better hint that ""x"" would be of the LUB type.

This is why I think the solution of throwing an error systematically when such a method call is found is the right way to go. This would allow the classical ""flow typing"" to work. This would also encourage good style because using a shared variable to store whatever you want is not a good idea.

Eventually, we must think about this use case :

{code}
def x = 1
def cl = { x.toInteger() }
x = '123'
cl()
{code}

This is a similar problem. Here, to be able to statically check the closure, we must know the type of the shared variable. There is no assignment in the closure, but still, we can statically check it if we use the very same algorithm. A call to "".toInteger()"" would only be allowed if we know that this method belongs to all types that have been assigned to x. This won't be fun to implement, but I still think this is the most promising way to do this.
",melix,melix,Major,Closed,Fixed,05/Dec/11 06:54,21/Jul/22 21:23
Bug,GROOVY-5167,12815998,GDK date methods not recognized by the static type checker,"I was playing with the following snippet:
{code}
import groovy.transform.TypeChecked
@TypeChecked void m() {
    def s = ""1234""
    println s.toInteger()
    s = 1234
    println s * 3
    s = new Date()
    println s.year
    println s.format(""yyyyMMdd"")
}
m()
{code}
And the static type checker was complaining on the s.format() call.
The format() method is part of DateGroovyMethods, not directly DefaultGroovyMethods, for modularity sake, and hence it wasn't recognized.",melix,guillaume,Major,Closed,Fixed,05/Dec/11 09:36,24/Dec/11 03:08
Bug,GROOVY-5168,12816005,Static type checker should not complain on methods w/ implicit object return type when last expression is void,"For the script below, when I annotate a method with @TypeChecked, and it has an implicit Object return type (no type, no def, nothing), the checker will complain because the last expression is actually a void statement. This kind of methods, just like plain Groovy, should be valid and return null.

{code}
import groovy.transform.TypeChecked

@TypeChecked m() {
    println ""hello""
}

m()
{code}",melix,guillaume,Minor,Closed,Fixed,05/Dec/11 09:43,24/Dec/11 03:08
Bug,GROOVY-5169,12815877,JsonOutput.toJson(object) is not returning expected results,"Consider the attached Groovy script. It defines two classes. Each has public properties explicitly defined. When I send the resulting objects to JsonOutput.toJson(), I expect the public properties of the object to be serialized into the JSON output. This is not working as expected. The only properties that get serialized are those created via ""def propName"" and those that seem to have getter methods (this in not actually the case).

Attached is a screenshot of GroovyConsole running the attached script.",emilles,jsumners,Minor,Closed,Fixed,05/Dec/11 12:38,03/Feb/22 22:35
Bug,GROOVY-5170,12816012, GroovyRowResult and GroovyResultSet are Inconsistent with each other when using a postgres driver,"Basically, the root problem seams to be that the postgres (driver?) folds unquoted object identifiers to lower case.  This behavior is transparent to the api when using sql.eachRow, but not when sql.firstRow. This appears to be related to how eachRow uses a GroovyResultSet which delegates the getObject(property) to the driver, but firstRow uses a GroovyRowResult which does not.

	String query = ""SELECT * FROM $table"";
	sql.eachRow(query){ row ->
		def eValue = row[column]
		println ""each row found $eValue"";
	}
	def fValue = sql.firstRow(query)[column]
	println ""firstRow found $fValue""

I saw code in GroovyRowResult#getProperty(String) currently checks the exact case, then tries folding the property name to upper-case (as is more common).  This is not adequate for postgres behavior which folds to lower-case.

Attached is a script which demonstrates this, an example invocation would be
`bug.groovy --url jdbc:postgresql://example.com:5432/catalog --user postgres --pass postgres --schema public --driver org.postgresql.Driver TABLE ID`
It requires a postgres database with a table and column (create using unquoted and/or lowercase names) exist with at least one row.",paulk,jwadamson,Major,Closed,Fixed,05/Dec/11 15:21,12/Feb/12 04:03
Bug,GROOVY-5171,12815801,Change execution phase of static type checking,"The current phase for static type checking is {{CANONICALIZATION}}. However, some AST xforms may be triggered at this phase too. As we cannot make sure that STC will be the last transform to be started, we must delay its execution to the {{INSTRUCTION_SELECTION}} phase.",melix,melix,Major,Closed,Fixed,06/Dec/11 04:32,24/Dec/11 03:07
Bug,GROOVY-5172,12815999,Inferred generics parameter types could contain primitive types,"In some cases like Groovy classes extending Java classes, inferred generics parameter types could be primitives instead of boxed types:

{code:title=GroovyPage.java}
public class GroovyPage {
        public final void printHtmlPart(final int partNumber) {}
        public final void createTagBody(int bodyClosureIndex, Closure<?> bodyClosure) {}
    }
{code}

{code:title=Child.groovy}
class Child extends GroovyPage {
                void foo() {
                    createTagBody(1) { ->
                        printHtmlPart(2)
                    }
                }
            }
{code}
",melix,melix,Major,Closed,Fixed,06/Dec/11 08:59,24/Dec/11 03:08
Bug,GROOVY-5173,12815888,BigDecimals are incorrectly converted to Double after arithmetic operation (appears to be the same as [GROOVY-5102] - Unrelated changes cause BigDecimal division to return Double),"It appears that the above bug was not correctly fixed.  Addition of BigDecimals results in a type coercion of the result to Double.

I have attached a GroovyTestCase which should be self explanatory.",blackdrag,davidclark,Blocker,Closed,Fixed,06/Dec/11 20:07,12/Dec/11 09:59
Bug,GROOVY-5175,12818125,Calling a method with a null argument's value when the parameter is an array,"STC Error when calling a method with an argument of null when the parameter is an array

{code:title=Null parameter with array argument}
import groovy.transform.*

@TypeChecked 
class Foo {
    def say() {
        methodWithArrayParam(null) // STC Error
    }
    def methodWithArrayParam(String[] s) {
        
    }
}

{code}

Other cases tested too:

{code:title=Simple method call with null argument}
class Foo {
    def say() {
        methodWithArrayParam(null)
    }

    def methodWithArrayParam(Date date) {

    }
}
{code}

{code:title=Multiple parameters where one of them is null}
class Foo {
    def say() {
        methodWithArrayParam(null, new Date())
    }
    def methodWithArrayParam(Date date1, Date date2) {

    }
}
{code}

{code:title=Ambiguous method call due to null parameters}
class Foo {
    def say() {
        methodWithArrayParam(null, new Date())
    }
    def methodWithArrayParam(Date date1, Date date2) {

    }
    def methodWithArrayParam(String o, Date date2) {

    }
}
{code}

{code:title=Disambiguated method call}
class Foo {
    def say() {
        methodWithArrayParam((Date)null, new Date())
    }
    def methodWithArrayParam(Date date1, Date date2) {

    }
    def methodWithArrayParam(String o, Date date2) {

    }
}
{code}",melix,vns,Major,Closed,Fixed,08/Dec/11 06:07,24/Dec/11 03:08
Bug,GROOVY-5176,12816015,incorrect handling of generics placeholders in JavaStubGenerator,"The {{JavaStubGenerator}} incorrectly handles generics signatures of the form:
{code}
java.util.List<? extends T>
{code}
instead producing
{code}
java.util.List<? extends java.lang.Object<T>>
{code}
which doesn't compile as {{java.lang.Object}} does not take parameters.
",paulk,paulk,Major,Closed,Fixed,08/Dec/11 07:05,07/Apr/15 19:07
Bug,GROOVY-5177,12815962,STC allows assign an array to any type which is not an array,"STC allows assign an array to any type which is not an array
{code}
import groovy.transform.*

@TypeChecked 
class Foo {
    def say() {
        FooAnother foo1 = new Foo[13] // but FooAnother foo1 = new Foo() reports a STC                        Error
    }
}
class FooAnother {
    
}
{code}",melix,vns,Major,Closed,Fixed,08/Dec/11 07:34,24/Dec/11 03:07
Bug,GROOVY-5178,12815799,STC allows assign an array to any type which is not an array,"STC allows assign an array to any type which is not an array
{code}
import groovy.transform.*

@TypeChecked 
class Foo {
    def say() {
        FooAnother foo1 = new Foo[13] // but FooAnother[] foo1 = new Foo() reports a STC                        Error
    }
}
class FooAnother {
    
}
{code}",melix,vns,Major,Closed,Fixed,08/Dec/11 07:36,05/Apr/15 14:44
Bug,GROOVY-5180,12816014,Enum Comparison results in incorrect map behavior when using add assignment operator,"In the attached test case you will see four methods which do the same three things.  Each initializes a map, increments a value in the map, and then returns the map.  The method called ""works()"" uses the add assignment operator to do this.  The ""buggy()"" method is the same, but adds an enum comparison.

There is some interaction between the enum comparison and the += operator.  The method ""numericComparison()"" is just like ""buggy(),"" but performs an integer comparison.  The ""workaround()"" method avoids the += operator.  Both of these methods return a Map as expected.",,davidclark,Blocker,Closed,Fixed,08/Dec/11 17:15,09/Dec/11 02:23
Bug,GROOVY-5181,12815892,"""static final"" constructors should be a Compile-time error","Some modifiers on the constructor should be disallowed instead of being passed to the init method.  For example ""static"":

class Foo {
  static final Foo() {
    println 'it works!'
  }
}

This creates a method <init> with modifiers of 0x19, which are static public and final.  Not valid for a constructor.  This should bomb at compile time instead of generating bytecode.
",roshandawrani,shemnon,Major,Closed,Fixed,08/Dec/11 22:13,24/Dec/11 03:08
Bug,GROOVY-5184,12815869,Static type checker doesn't handle divisions properly,"In the following code:
{code}
def res = 1 / 2
{code}

The inferred type is a {{Number}} because of the return type of DGM#div.
",melix,melix,Major,Closed,Fixed,12/Dec/11 03:25,24/Dec/11 03:08
Bug,GROOVY-5185,12815871,Cast operator precedence is incorrect,"The cast operator precedence is incorrect:

{code}
def i = (int)1/(int)2
assert i.class==BigDecimal // fails
{code}

To have proper casts, we need extra parenthesis which should not be necessary:

{code}
def i = ((int)1)/((int)2)
assert i.class==BigDecimal // ok
{code}
",paulk,melix,Major,Closed,Fixed,12/Dec/11 03:34,10/Oct/15 06:34
Bug,GROOVY-5187,12815976,Groovy Script Engine creates new classes for each script and doesn't get garbage collected,"New classes are created for each script:

https://github.com/groovy/groovy-core/blob/master/src/main/org/codehaus/groovy/jsr223/GroovyScriptEngineImpl.java#L109

Which are added to the classMap, which grows and old classes don't get garbage collected:

https://github.com/groovy/groovy-core/blob/master/src/main/org/codehaus/groovy/jsr223/GroovyScriptEngineImpl.java#L338

This results in PermGen errors:

Rexster Groovy Script Engine
https://github.com/tinkerpop/rexster/issues/143

Neo4j Server Groovy Script Engine
http://neo4j-community-discussions.438527.n3.nabble.com/Neo4j-Feedback-after-evaluation-tp3569774p3574520.html

There is no public way of removing older scripts and the HashMap is not a LinkedHashMap with LRU enabled for storing them. Right now the Neo4j workaround is to recreated the GSE every 500 requests.




",blackdrag,espeed,Major,Closed,Fixed,12/Dec/11 16:36,22/Dec/12 01:10
Bug,GROOVY-5189,12815994,Character XOR behaves differently depending on JVM,"If you run this piece of code:

{code}
println GroovySystem.version
println( ('a' as char) ^ ('b' as char) )
{code}

On OS X running with the Apple JVM, then you get:

{code}
1.8.4
3
{code}

If you're running on the Java 6 OpenJDK (for example with Ubuntu, or the Groovy Web Console), you get:

{code}
1.8.4
groovy.lang.MissingMethodException: No signature of method: java.lang.Character.xor() is applicable for argument types: (java.lang.Character) values: [b]
Possible solutions: div(java.lang.Character), any(), use([Ljava.lang.Object;), plus(java.lang.Character), is(java.lang.Object), any(groovy.lang.Closure)
	at Script1.run(Script1.groovy:2)
{code}

Is there anything that can be done to get the behaviour the same across the board?

Jochen [said on the mailing list|http://groovy.markmail.org/thread/keyqmeyfdztgzfxz] that it was an issue to do with the native type optimisations.

Maybe just adding {{xor}} to Character would get rid of the OpenJDK issue?",blackdrag,tim_yates,Major,Closed,Fixed,13/Dec/11 09:25,22/Dec/12 01:10
Bug,GROOVY-5191,12815984,Running script with '--enoding' param and some script parameters,"Create simple script 'main.groovy':
{code}
args.each {println it}
{code}

and try to run it by the following command:
{{groovy --encoding=UTF-8 main.groovy -script -param}}

The output is:
{{script}}
{{-p}}
{{-a}}
{{ram}}

while the expected output is:
{{-script}}
{{-param}}",paulk,mxm-groovy,Critical,Closed,Fixed,14/Dec/11 03:30,17/Jun/15 20:09
Bug,GROOVY-5193,12812066,Compilation should always fail when two methods exist in the same class with the same name but differing access levels,"Currently, the check only fails under certain circumstatnces, but according to Jochen, it should always fail.

The following passes, but apparently should not:
{code}class Repository {
  def find(String id) {}
  private <T> T find(Class<T> type, String id, boolean suppressNotFoundExceptions) { }
}{code}

The following fails, as expected:

{code}class Repository {
  def find(String id) {}
  private <T> T find(Class<T> type, String id, boolean suppressNotFoundExceptions = true) { }
}{code}
",roshandawrani,bcarr,Minor,Closed,Fixed,14/Dec/11 08:08,12/Feb/12 04:03
Bug,GROOVY-5196,12815963,Map#drop Javadoc breaks all following Javadoc,"The Map#drop Javadoc has some invalid HTML in it, making all following Javacod preformatted as well.",paulk,wujek,Trivial,Closed,Fixed,15/Dec/11 14:29,24/Dec/11 03:08
Bug,GROOVY-5197,12815872,Source location incorrect for statement after a label,"Take this code and put it into the groovy console:
{code}
def meth() {
  label:
    assert i == 9
}
{code}

Now inspect the AST and navigate to the assert statement.  You will see that the assert statement has a lineNumber and lastLineNumber are 2, but it should be 3.  This is incorrect.  It doesn't seem like this affects Groovy itself too much, but this is causing an exception in Groovy-Eclipse.  See GRECLIPSE-1270.

The fix is simple enough.",paulk,werdna,Major,Closed,Fixed,15/Dec/11 17:23,12/Feb/12 04:03
Bug,GROOVY-5198,12818130,"using ""as"" to coerce a String to an Enum value results in a groovy.lang.MissingMethodException under high contention","I've provided a test case in the form of a simple script that can be run in GroovyConsole.

It creates 5000 threads -- each thread converts strings to enums in a loop.
I tried three different methods of converting strings to Enums in the test.

These two seem to work fine:

Foo f = Foo.valueOf(key)
Foo f = Enum.valueOf(Foo, key)

However, under load, this method:

Foo f = key as Foo

will occasionally throw an exception like the following:

{noformat}
Exception in thread ""Thread-127349"" 
org.codehaus.groovy.runtime.InvokerInvocationException: groovy.lang.MissingMethodException: No signature of method: static Foo.valueOf() is applicable for argument types: (java.lang.String) values: [bar]
Possible solutions: valueOf(java.lang.String), valueOf(java.lang.Class, java.lang.String), values()
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:97)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:883)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:404)
	at groovy.lang.Closure.run(Closure.java:488)
	at java.lang.Thread.run(Thread.java:680)
Caused by: groovy.lang.MissingMethodException: No signature of method: static Foo.valueOf() is applicable for argument types: (java.lang.String) values: [bar]
Possible solutions: valueOf(java.lang.String), valueOf(java.lang.Class, java.lang.String), values()
	at groovy.lang.MetaClassImpl.invokeStaticMissingMethod(MetaClassImpl.java:1349)
	at groovy.lang.MetaClassImpl.invokeStaticMethod(MetaClassImpl.java:1335)
	at org.codehaus.groovy.runtime.InvokerHelper.invokeMethod(InvokerHelper.java:767)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.asType(DefaultGroovyMethods.java:17327)
	at org.codehaus.groovy.runtime.dgm$58.doMethodInvoke(Unknown Source)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:1053)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:883)
	at org.codehaus.groovy.runtime.InvokerHelper.invokePojoMethod(InvokerHelper.java:781)
	at org.codehaus.groovy.runtime.InvokerHelper.invokeMethod(InvokerHelper.java:772)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.invokeMethodN(ScriptBytecodeAdapter.java:164)
	at org.codehaus.groovy.runtime.ScriptBytecodeAdapter.asType(ScriptBytecodeAdapter.java:587)
	at TestCase$_run_closure1_closure4_closure5.doCall(TestCase.groovy:12)
	at sun.reflect.GeneratedMethodAccessor682.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:883)
	at groovy.lang.Closure.call(Closure.java:410)
	at groovy.lang.Closure.call(Closure.java:423)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.times(DefaultGroovyMethods.java:12140)
	at org.codehaus.groovy.runtime.dgm$753.invoke(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite$PojoMetaMethodSiteNoUnwrapNoCoerce.invoke(PojoMetaMethodSite.java:271)
	at org.codehaus.groovy.runtime.callsite.PojoMetaMethodSite.call(PojoMetaMethodSite.java:53)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at TestCase$_run_closure1_closure4.doCall(TestCase.groovy:10)
	at sun.reflect.GeneratedMethodAccessor672.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	at groovy.lang.MetaMethod.doMethodInvoke(MetaMethod.java:233)
	at org.codehaus.groovy.runtime.metaclass.ClosureMetaClass.invokeMethod(ClosureMetaClass.java:272)
	at groovy.lang.MetaClassImpl.invokeMethod(MetaClassImpl.java:883)
	at org.codehaus.groovy.runtime.callsite.PogoMetaClassSite.callCurrent(PogoMetaClassSite.java:66)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.callCurrent(AbstractCallSite.java:141)
	at TestCase$_run_closure1_closure4.doCall(TestCase.groovy)
	at sun.reflect.GeneratedMethodAccessor665.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.codehaus.groovy.reflection.CachedMethod.invoke(CachedMethod.java:90)
	... 7 more
{noformat}

It's a concurrency issue, so the test isn't 100% reliable, but on my hardware this test can make the ""as"" conversion fail at least once in a test run most of the time. The other two methods seem to always work.
",melix,sreilly@equalexperts.com,Major,Closed,Fixed,16/Dec/11 05:58,24/Dec/11 03:08
Bug,GROOVY-5199,12815829,Expression#transformExpression should copy node metadata,"When an expression is transformed through {{Expression#transformExpression}}, we must make sure to copy node metadata, otherwise this information can be lost for further processing.

It is important for static compilation which relies on node metadata computed by the static type checker. This inferrence information is lost after the OptimizerVisitor has transformed expressions.",melix,melix,Major,Closed,Fixed,16/Dec/11 07:57,24/Dec/11 03:08
Bug,GROOVY-5200,12816018,Anonymous class with elipsis type in constructor,"{code:title='main.groovy'}
abstract class Abc {
    Abc(String... s) {}
    
    abstract def run()
}

print new Abc() {
    def run(){}
}
{code}

Try to run this code sample and get the following exception :
{{Caught: java.lang.ArrayIndexOutOfBoundsException: 0}}
{{java.lang.ArrayIndexOutOfBoundsException: 0}}
{{at main$1.<init>(main.groovy)}} 
{{at main.run(main.groovy:7)}}",,mxm-groovy,Major,Closed,Fixed,17/Dec/11 11:53,17/Jun/15 20:09
Bug,GROOVY-5202,12815987,inherited non public listener structure causing NPE,"the following script works with groovy 1.8.2 but not with 1.8.4 oci connection has an error (oci_error.txt) -- thin connection works
{code}
import groovy.sql.Sql
import java.sql.SQLException
def dbc = Sql.newInstance('jdbc:oracle:oci:@b003', 'system', 'system', ""oracle.jdbc.driver.OracleDriver"")
//def dbc = Sql.newInstance('jdbc:oracle:thin:@//server:1521/b003', 'system', 'system', ""oracle.jdbc.driver.OracleDriver"")
println dbc.properties
def row = dbc.firstRow(""SELECT count(*) FROM all_users WHERE username = 'SYSTEM'"")
println row
dbc.connection.autoCommit = false
{code}",blackdrag,lwolter,Major,Closed,Fixed,18/Dec/11 07:35,19/Feb/12 08:39
Bug,GROOVY-5204,12816029,@Delegate to a method with optional params,"Groovyc fails to compile this code with a message like 'method foo is already defined in B'
{code}
class A {
  def foo(a = ''){}
}

class B {
  @Delegate A a = new A()
  def foo(){}
}
{code}

while these snippets are compiled pretty well
{code}
class A {
  def foo(a){}
  def foo(){}
}

class B {
  @Delegate A a = new A()
  def foo(){}
}
{code}

{code}
class A {
  def foo(a=''){}
}

class B extends A{
  def foo(){}
}
{code}

IMHO there should be a warning (or nothing at all) instead of the compilation error.",emilles,mxm-groovy,Major,Closed,Fixed,19/Dec/11 01:47,22/Feb/22 03:12
Bug,GROOVY-5207,12815685,@Field access within closure fails with 'BUG! exception in phase 'class generation' in source unit',"Looks like accessing @Field fields inside closures has a problem. Please file a bug.

---------------------------------------------
import groovy.transform.Field

@Field pomProperties = [:]

"""".each {
    pomProperties[1] = 2
}
---------------------------------------------
fails with the same error.
---------------------------------------------
Caught: BUG! exception in phase 'class generation' in source unit 'C:\Temp\m2g.groovy' tried to get a variable with the name pomProperties
 as stack variable, but a variable with this name was not created
---------------------------------------------",paulk,ray@suliteanu.com,Major,Closed,Fixed,20/Dec/11 10:10,12/Feb/12 04:03
Bug,GROOVY-5208,12816011,Incorrect line numbers in code using optimized primitive operations,The visible problem is that debugger doesn't stop on the lines containing only primitive value operations. See more details in http://youtrack.jetbrains.net/issue/IDEA-77107.,blackdrag,gromopetr,Major,Closed,Fixed,21/Dec/11 06:54,11/May/14 06:01
Bug,GROOVY-5210,12818132,Problem converting primitive array to a Set,"When I try to convert a primitive array to java.util.Set using the ""as"" operator, it looks like Groovy tries to create an instance of java.util.Set (as opposed to creating an instance of some class which implements java.util.Set).

{code:title=demo.groovy|borderStyle=solid}
def intArray = [1, 2, 3] as int[]
def setOfInt = intArray as Set
println setOfInt
{code}

{noformat}
conversion_problem $ groovy -version
Groovy Version: 1.8.4 JVM: 1.6.0_29
conversion_problem $ groovy demo.groovy 
Caught: org.codehaus.groovy.runtime.typehandling.GroovyCastException: Could not instantiate instance of: java.util.Set. Reason: java.lang.InstantiationException: java.util.Set
org.codehaus.groovy.runtime.typehandling.GroovyCastException: Could not instantiate instance of: java.util.Set. Reason: java.lang.InstantiationException: java.util.Set
	at demo.run(demo.groovy:2)
{noformat}
",melix,brownj,Major,Closed,Fixed,22/Dec/11 18:20,24/Dec/11 03:08
Bug,GROOVY-5211,12816000,Method dispatch error with @Delegate,"Looks like there is some mix-up of the method parameter 'a' in A.foo() with the field 'a' in class B. In the example below if I rename the method parameter or the field, the code goes through.
{code}
class A {
	def foo(a){}
}

class B {
  @Delegate A a = new A()
}

new B().foo(10)
{code}

fails with:
{noformat}
Caught: groovy.lang.MissingMethodException: No signature of method: java.lang.Integer.foo() is applicable for argument types: (java.lang.Integer) values: [10]
{noformat}",paulk,roshandawrani,Major,Closed,Fixed,24/Dec/11 21:57,12/Apr/13 16:55
Bug,GROOVY-5212,12818131,Compilation problem for java enums defined as groovy classes,"Hi, I have following groovy class TransactionStatus.groovy with code:

package com.webbfontaine.twm.accounting.epaylog.emess.model.response

public enum TransactionStatus {
  OK, ERROR
}

and compilation of this class using groovy 1.8.4 was ok but after  migrating to 1.8.5 I have following:

groovy.compile:
    [mkdir] Created dir: /home/sargis/projects/twm3.git/modules/accounting/Server/classes
  [groovyc] Compiling 186 source files to /home/sargis/projects/twm3.git/modules/accounting/Server/classes
  [groovyc] org.codehaus.groovy.control.MultipleCompilationErrorsException: startup failed:
  [groovyc] Compile error during compilation with javac.
  [groovyc] warning: [options] bootstrap class path not set in conjunction with -source 1.6
  [groovyc] /tmp/groovy-generated-3682106614822967278-java-source/com/webbfontaine/twm/accounting/epaylog/emess/model/response/TransactionStatus.java:10: error: modifier final not allowed here
  [groovyc] public final enum TransactionStatus
  [groovyc]              ^
  [groovyc] Note: Some input files use or override a deprecated API.
  [groovyc] Note: Recompile with -Xlint:deprecation for details.
  [groovyc] Note: Some input files use unchecked or unsafe operations.
  [groovyc] Note: Recompile with -Xlint:unchecked for details.
  [groovyc] 1 error
  [groovyc] 1 warning
  [groovyc] 
  [groovyc] 
  [groovyc] 1 error


I am using

        <taskdef name=""groovyc"" classname=""org.codehaus.groovy.ant.Groovyc"">
 
        </taskdef>

to compile groovy codes of my project.",melix,armsargis,Major,Closed,Fixed,26/Dec/11 00:24,12/Feb/12 04:03
Bug,GROOVY-5213,12812067,Inner classes are not type checked,"The static type checker forgets to visit inner classes, which are therefore not type checked.",melix,melix,Major,Closed,Fixed,26/Dec/11 10:30,13/May/12 03:30
Bug,GROOVY-5214,12816008,Source location incorrect for enums,"As per:
https://github.com/groovy/groovy-core/pull/10",paulk,paulk,Minor,Closed,Fixed,26/Dec/11 23:22,07/Apr/15 19:07
Bug,GROOVY-5215,12816046,Linenumber information is missing for Enums in AST,"In the AST for enums, the linenumber information is not available. This causes some errors in CodeNarc, which uses that information to report on.",paulk,bodiam,Major,Closed,Fixed,27/Dec/11 07:04,05/Apr/15 14:44
Bug,GROOVY-5216,12815966,"groovy.sql.Sql.newInstance(Map<String, Object>) remove params","method groovy.sql.Sql.newInstance(Map<String, Object>) can be nicely used with ConfigSlurper, the problem is that it removes parameters from method call argument, so source code:
{code}
def res = new ConfigSlurper().parse(new File('resource/Resources.groovy').toURI().toURL())

files.each {
		def sql = Sql.newInstance(res.db)
		//do something
		sql.close()
}
{code}
fails on second iteration due to parameters had been removed from res.db map and param ""url"" can not be found

the solution may be changing args.remove to args.get in procedure groovy.sql.Sql.newInstance(Map<String, Object>)",paulk,aaaia,Trivial,Closed,Fixed,27/Dec/11 07:34,12/Feb/12 04:03
Bug,GROOVY-5217,12816017,Calls to closures declared as fields are not type checked properly,"If a class contains a field declared as a closure and that you use that closure as a method, the type checker throws a missing method error.

{code}
class FibUtil {
                private Closure<Integer> fibo
                FibUtil() {
                    fibo = { int x-> x<1?x:fibo(x-1)+fibo(x-2) }
                }

                int fib(int n) { fibo(n) }
            }
{code}",melix,melix,Major,Closed,Fixed,27/Dec/11 12:01,13/May/12 03:30
Bug,GROOVY-5219,12816038,FactoryBuilderSupport doesn't register methods in a predictable order,"Many build failures on jdk 1.7.0_02 are due to the fact that this version of the JDK doesn't return declared methods in a predictable order, while other JDKs do. This leads to random build failures, as the {{Class#getDeclaredMethods}} method _may_ return the methods in the expected order, but not always.",melix,melix,Major,Closed,Fixed,28/Dec/11 04:37,12/Feb/12 04:03
Bug,GROOVY-5221,12818139,Problem with @InheritConstructors with multiple level of inheritance (part 1 of 2 - document the current limitation),"See also GROOVY-5274 which will be the actual fix, changing the focus of this issue to updating the doco to outline the current limitations.

=======================

See test eclipse project attached.

- Class Hierarchy : A <-- inherit -- B <-- inherit -- C

- A define constructor public A(String dummy){...}

- B and C use  @InheritConstructors.

- In the main(...) : C.class.newInstance(""tata"")

- Result : Exception in thread ""main"" groovy.lang.GroovyRuntimeException: Could not find matching constructor for: data.C(java.lang.String)
	at groovy.lang.MetaClassImpl.invokeConstructor(MetaClassImpl.java:1474)
	at groovy.lang.MetaClassImpl.invokeConstructor(MetaClassImpl.java:1390)
	at org.codehaus.groovy.runtime.InvokerHelper.invokeConstructorOf(InvokerHelper.java:824)
	at org.codehaus.groovy.runtime.DefaultGroovyMethods.newInstance(DefaultGroovyMethods.java:17689)
	at org.codehaus.groovy.runtime.dgm$511.doMethodInvoke(Unknown Source)
	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.invoke(StaticMetaMethodSite.java:43)
	at org.codehaus.groovy.runtime.callsite.StaticMetaMethodSite.call(StaticMetaMethodSite.java:88)
	at org.codehaus.groovy.runtime.callsite.CallSiteArray.defaultCall(CallSiteArray.java:42)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:108)
	at org.codehaus.groovy.runtime.callsite.AbstractCallSite.call(AbstractCallSite.java:116)
	at Main.MainLaunch.main(MainLaunch.groovy:7)
",paulk,ronan_michaux,Blocker,Closed,Fixed,28/Dec/11 12:03,12/Feb/12 04:03
Bug,GROOVY-5222,12816059,Casts to subclasses should not be caught as errors by the type checker,"The following expression causes an incompatible type error at compile time, although it's valid:

{code}
Object o = null
try {
   ((Integer)o).intValue()
} catch (NullPointerException e) {
}
{code}",melix,melix,Major,Closed,Fixed,29/Dec/11 10:21,13/May/12 03:30
Bug,CASSANDRA-1927,12494462,Hadoop Integration doesn't work when one node is down,"using the same directives in the sample code:

When I start the CFInputFormat to read a CF in a keyspace of RF=3 on a 4-node cluster:
- If all the nodes are all up, everything works fine and I don't have any problems walking through the all data in the CF, however
- If there's a node down, the hadoop job does not even start, just dies without any errors or exceptions.

So I'm really sorry for not being able to post any errors or exceptions, though it's really easy to reproduce. Just startup a cluster and take one node down and you're there :)",mck,uctopcu,Normal,Resolved,Fixed,03/Jan/11 01:15,16/Apr/19 09:33
Bug,CASSANDRA-1931,12494562,Internal error processing insert java.lang.AssertionError  at org.apache.cassandra.service.StorageProxy.sendMessages(StorageProxy.java:219),"ERROR [pool-1-thread-137] 2011-01-03 18:22:21,751 Cassandra.java (line 2960) Internal error processing insert
java.lang.AssertionError
        at org.apache.cassandra.service.StorageProxy.sendMessages(StorageProxy.java:219)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:174)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:412)
        at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:349)
        at org.apache.cassandra.thrift.Cassandra$Processor$insert.process(Cassandra.java:2952)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2555)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)",tjake,kmueller,Normal,Resolved,Fixed,04/Jan/11 05:29,16/Apr/19 09:33
Bug,CASSANDRA-1934,12494667,Update token metadata for NORMAL state,"The handleStateNormal() method in StorageService.java doesn't update the tokenmetadata. This means if you try to decommission a node but for some reason it fails, and then you bring the node back up, all other nodes will see it in a 'Leaving' state. When the state jumps back to normal they should update the token metadata to reflect that.

This also means you won't be able to call 'removetoken' on that node, unless you restart another node in the cluster in order to put it back in a 'normal' state.",brandon.williams,nickmbailey,Low,Resolved,Fixed,05/Jan/11 02:06,16/Apr/19 09:33
Bug,CASSANDRA-1939,12494734,Misuses of ByteBuffer absolute get (wrongfully adding arrayOffset to the index),ByteBuffer.arrayOffset() should not be added to the argument of an absolute get. ,slebresne,slebresne,Low,Resolved,Fixed,05/Jan/11 16:21,16/Apr/19 09:33
Bug,CASSANDRA-1943,12494805,Addition of internode buffering broke Streaming,Adding internode buffering broke StreamingTransferTest in the 0.7 branch. Bisected to r1055313,,stuhood,Urgent,Resolved,Fixed,06/Jan/11 03:18,16/Apr/19 09:33
Bug,CASSANDRA-1959,12495158,java.lang.ArrayIndexOutOfBoundsException while executing repair on a freshly added node (0.7.0),"Hi,

I added a node to the cluster (20 nodes in total) and ran repair on it after a while.

The repair still runs, but there are errors in the log file (see below).

Some of the data in the clsuter has been filled with rc-4. The cluster runs on version 0.7.0 (the release linked on the main cassandra web site).

Any ideas what might cause this?
(PS. 0.7.0 turns up as unreleased version in Affects Version/s:) 


INFO [CompactionExecutor:1] 2011-01-10 19:55:20,684 SSTableReader.java (line 158) Opening /hd2/cassandra_md5/data/table_x/table_x-e-4
 INFO [CompactionExecutor:1] 2011-01-10 19:55:20,775 SSTableReader.java (line 158) Opening /hd2/cassandra_md5/data/table_x/table_x_meta-e-14
ERROR [RequestResponseStage:3] 2011-01-10 19:55:20,856 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.fastRemove(ArrayList.java:441)
        at java.util.ArrayList.remove(ArrayList.java:424)
        at com.google.common.collect.AbstractMultimap.remove(AbstractMultimap.java:219)
        at com.google.common.collect.ArrayListMultimap.remove(ArrayListMultimap.java:60)
        at org.apache.cassandra.net.MessagingService.responseReceivedFrom(MessagingService.java:436)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:40)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [RequestResponseStage:3] 2011-01-10 19:55:20,856 AbstractCassandraDaemon.java (line 91) Fatal exception in thread Thread[RequestResponseStage:3,5,main]
java.lang.ArrayIndexOutOfBoundsException: -1
        at java.util.ArrayList.fastRemove(ArrayList.java:441)
        at java.util.ArrayList.remove(ArrayList.java:424)
        at com.google.common.collect.AbstractMultimap.remove(AbstractMultimap.java:219)
        at com.google.common.collect.ArrayListMultimap.remove(ArrayListMultimap.java:60)
        at org.apache.cassandra.net.MessagingService.responseReceivedFrom(MessagingService.java:436)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:40)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:63)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
",jbellis,tbritz,Normal,Resolved,Fixed,10/Jan/11 19:03,16/Apr/19 09:33
Bug,CASSANDRA-1962,12495222,Equality problem in schema updates,"CFMetaData.apply uses equals to compare objects that are not the same class: this may work now, but we shouldn't rely on that behaviour.",stuhood,stuhood,Low,Resolved,Fixed,11/Jan/11 09:53,16/Apr/19 09:33
Bug,CASSANDRA-1964,12495250,MutationTest of the distributed-test suite fails,"MutationTest of the distributed-test test suite causes errors on trunk.

To reproduce, issue:

ant distributed-test -Dwhirr.config=<path_to_whirr_config_file>

from the project root.

relevant whirr configuration settings used:

whirr.service-name=cassandra
whirr.cluster-name=cassandra_test
whirr.instance-templates=4 cassandra
whirr.version=0.3.0-incubating-SNAPSHOT
whirr.location-id=us-west-1
whirr.image-id=us-west-1/ami-16f3a253
whirr.hardware-id=m1.large
whirr.blobstore.provider=s3
whirr.blobstore.container=tawamuducassandratests
whirr.provider=ec2
whirr.run-url-base=http://hoodidge.net/scripts/

Traceback:

distributed-test:   
     [echo] running distributed tests
    [junit] WARNING: multiple versions of ant detected in path for junit 
    [junit]          jar:file:/usr/share/ant/lib/ant.jar!/org/apache/tools/ant/Project.class
    [junit]      and jar:file:/Users/mallen/Desktop/cassandra-trunk/build/lib/jars/ant-1.6.5.jar!/org/apache/tools/ant/Project.class
    [junit] Testsuite: org.apache.cassandra.MovementTest
    [junit] Tests run: 1, Failures: 0, Errors: 0, Time elapsed: 446.65 sec
    [junit] 
    [junit] ------------- Standard Error -----------------
    [junit] SLF4J: Class path contains multiple SLF4J bindings.
    [junit] SLF4J: Found binding in [jar:file:/Users/mallen/Desktop/cassandra-trunk/lib/slf4j-log4j12-1.6.1.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    [junit] SLF4J: Found binding in [jar:file:/Users/mallen/Desktop/cassandra-trunk/build/test/lib/jars/whirr-cli-0.3.0-incubating-SNAPSHOT.jar!/org/slf4j/impl/StaticLoggerBinder.class]
    [junit] SLF4J: See http://www.slf4j.org/codes.html#multiple_bindings for an explanation.
    [junit]  WARN 12:12:46,654 over limit 471283/262144: wrote temp file
    [junit]  WARN 12:12:48,572 over limit 374979/262144: wrote temp file
    [junit]  WARN 12:12:50,701 over limit 892174/262144: wrote temp file
    [junit]  WARN 12:12:54,442 over limit 612358/262144: wrote temp file
    [junit] ------------- ---------------- ---------------
    [junit] Testsuite: org.apache.cassandra.MutationTest
    [junit] Tests run: 4, Failures: 0, Errors: 3, Time elapsed: 110.971 sec
    [junit] 
    [junit] Testcase: testInsert(org.apache.cassandra.MutationTest):    Caused an ERROR
    [junit] null
    [junit] NotFoundException()
    [junit]     at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6900)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:568)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:541)
    [junit]     at org.apache.cassandra.MutationTest.getColumn(MutationTest.java:210)
    [junit]     at org.apache.cassandra.MutationTest.testInsert(MutationTest.java:66)
    [junit] 
    [junit] 
    [junit] Testcase: testWriteAllReadOne(org.apache.cassandra.MutationTest):   Caused an ERROR
    [junit] null
    [junit] NotFoundException()
    [junit]     at org.apache.cassandra.thrift.Cassandra$get_result.read(Cassandra.java:6900)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.recv_get(Cassandra.java:568)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.get(Cassandra.java:541)
    [junit]     at org.apache.cassandra.MutationTest.getColumn(MutationTest.java:210)
    [junit]     at org.apache.cassandra.MutationTest.testWriteAllReadOne(MutationTest.java:87)
    [junit] 
    [junit] 
    [junit] Testcase: testWriteOneReadAll(org.apache.cassandra.MutationTest):   Caused an ERROR
    [junit] null
    [junit] TimedOutException()
    [junit]     at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15392)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:907)
    [junit]     at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:879)
    [junit]     at org.apache.cassandra.MutationTest.insert(MutationTest.java:202)
    [junit]     at org.apache.cassandra.MutationTest.testWriteOneReadAll(MutationTest.java:185)
    [junit] 
    [junit] 
    [junit] TEST org.apache.cassandra.MutationTest FAILED
    [junit] Tests FAILED

BUILD FAILED
/Users/mallen/Desktop/cassandra-trunk/build.xml:557: The following error occurred while executing this line:
/Users/mallen/Desktop/cassandra-trunk/build.xml:540: Some distributed test(s) failed.

Total time: 10 minutes 15 seconds
",stuhood,tawamudu,Low,Resolved,Fixed,11/Jan/11 14:08,16/Apr/19 09:33
Bug,CASSANDRA-1972,12495338,Default concurrency values are improperly proportioned,"The ""default""/""suggested"" {{concurrent_reads}} value is much too low. It assumes that CPU will be the bottleneck, rather than IO, and for most deployments, this will not be the case. Additionally it is better to be queued for IO in the kernel or on your device than in user space, because the former work to optimize queue order.

Additionally, reads are much cheaper than writes in terms of CPU time (since writes can experience contention due to retries), so while {{concurrent_writes}} should probably factor in the number of cores on the machine, {{concurrent_reads}} should probably be calculated purely by number of spindles.",stuhood,stuhood,Low,Resolved,Fixed,12/Jan/11 07:04,16/Apr/19 09:33
Bug,CASSANDRA-1973,12495398,stress.java -k doesn't keep going,"stress.java's -k option doesn't work correctly.  In the face of many errors, it ends up printing 'null' a bunch and then exiting.",xedin,brandon.williams,Low,Resolved,Fixed,12/Jan/11 18:27,16/Apr/19 09:33
Bug,CASSANDRA-1976,12495423,SSTableWriter.writeStatistics is serializing incorrect data.,it is serializing rowSizes twice instead of serializing rowSizes and columnCounts.,jbellis,gdusbabek,Low,Resolved,Fixed,12/Jan/11 21:45,16/Apr/19 09:33
Bug,CASSANDRA-1979,12495504,CassandraServiceDataCleaner.prepare() fails with IOException.,"CassandraServiceDataCleaner.prepare() fails with an IOException if run in isolation.  It seems that initializing the DataDescriptor creates a new CommitLog file, and then the cleaner tries to delete this file and fails.

16:06:07.204 [main] INFO  o.a.c.config.DatabaseDescriptor - Loading settings from file:/C:/workspace/sandbox/target/classes/cassandra.yaml
16:06:07.282 [main] DEBUG o.a.c.config.DatabaseDescriptor - Syncing log with a period of 10000
16:06:07.282 [main] INFO  o.a.c.config.DatabaseDescriptor - DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
16:06:07.797 [main] DEBUG o.a.c.config.DatabaseDescriptor - setting auto_bootstrap to false
16:06:07.797 [main] INFO  o.a.c.db.commitlog.CommitLogSegment - Creating new commitlog segment target/var/lib/cassandra/commitlog\CommitLog-1294934767797.log
16:06:07.813 [main] DEBUG o.apache.cassandra.io.util.FileUtils - Deleting CommitLog-1294934767797.log
Exception in thread ""main"" java.io.IOException: Failed to delete C:\workspace\sandbox\target\var\lib\cassandra\commitlog\CommitLog-1294934767797.log
	at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:54)
	at org.apache.cassandra.io.util.FileUtils.deleteRecursive(FileUtils.java:201)
	at org.apache.cassandra.contrib.utils.service.CassandraServiceDataCleaner.cleanDir(CassandraServiceDataCleaner.java:99)
	at org.apache.cassandra.contrib.utils.service.CassandraServiceDataCleaner.cleanupDataDirectories(CassandraServiceDataCleaner.java:53)
	at org.apache.cassandra.contrib.utils.service.CassandraServiceDataCleaner.prepare(CassandraServiceDataCleaner.java:44)
	at cng.sandbox.App.main(App.java:15)

This also seems to leave a bunch of threads running in the background, so the process has to be manually killed.

This was tested with the javautils in the 0.7.0 branch.",zznate,cng1066,Normal,Resolved,Fixed,13/Jan/11 16:09,16/Apr/19 09:33
Bug,CASSANDRA-1981,12495531,The writing of statistics in SSTableWrite.Builder has been mistakenly removed by #1072,Everything's in the summary,slebresne,slebresne,Low,Resolved,Fixed,13/Jan/11 19:27,16/Apr/19 09:33
Bug,CASSANDRA-1985,12495562,read repair on CL.ONE regression,"read repair w/ CL.ONE had a regression.

The RepairCallback was dropped (in the background for CL.ONE), so ReadResponseResolver : resolve() was never called.",jbellis,kelvin,Normal,Resolved,Fixed,14/Jan/11 00:32,16/Apr/19 09:33
Bug,CASSANDRA-1986,12495612,write CL > ONE regression,write CL > ONE regression by the DC refactor.,kelvin,kelvin,Normal,Resolved,Fixed,14/Jan/11 18:16,16/Apr/19 09:33
Bug,CASSANDRA-1989,12495657,Cannot parse generation after restart,"Looks like CASSANDRA-1714 broke some parsing of the generation on some restarts: haven't tracked it down yet, but this is likely to be obvious for someone who worked on that issue.
{code}java.lang.UnsupportedOperationException
        at java.nio.ByteBuffer.array(ByteBuffer.java:940)
        at org.apache.cassandra.utils.FBUtilities.byteBufferToInt(FBUtilities.java:212)
        at org.apache.cassandra.db.SystemTable.incrementAndGetGeneration(SystemTable.java:286)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:356)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:184)
        at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:54)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:240)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:133){code}
",xedin,stuhood,Normal,Resolved,Fixed,15/Jan/11 05:43,16/Apr/19 09:33
Bug,CASSANDRA-1992,12495685,"Bootstrap breaks data stored (missing rows, extra rows, column values modified)","Scenario:
Two fresh (empty /data /commitog /saved_caches dirs) cassandra installs.
Start first one.
Run data inserting program [1],  run again in verify mode - all data intact.
Bootstrap 2nd node.
Run verification again, now it fails.

Issue is very strange to me as cassandra works perfectly for me when cluster nodes stay the same for days now but any bootstrap ( 1 -> 2 nodes, 2 -> 3 nodes, 2->3 nodes RF=2) breaks data.

I am running cassandra with 1GB heap size, 32bit userland on 64bit kernels, not sure what else could matter there.
Any hints ?
Thanks in advance, regards.

[1] simple program generating data and later verifying data.
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/test.py

[2] Logs from 1st node:
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/system-3.4.log

[3] Logs from 2nd (bootstraping node)
http://beauty.ant.gliwice.pl/bugs/cassandra-bootstrap/system-3.8.log

",brandon.williams,matkor,Normal,Resolved,Fixed,15/Jan/11 21:47,16/Apr/19 09:33
Bug,CASSANDRA-1993,12495691,Word count example doesn't output the words correctly to cassandra.  It outputs spurious data past the length of the byte array.,"To reproduce:
# start a local cassandra server e.g. sudo bin/cassandra -f
cd contrib/word_count
ant
bin/word_count_setup
bin/word_count

# check the data in cassandra, all looks fine because the words are all of the same length.
# change the data in cassandra to real words, rerun the mapreduce and you'll see some words have spurious characters written past their length
# this is because the word bytes are not terminated at their length",jesseshieh,jesseshieh,Low,Resolved,Fixed,16/Jan/11 01:38,16/Apr/19 09:33
Bug,CASSANDRA-1995,12495723,cassandra-cli doesn't accept 'name' as a column name in column metadata when creating a column family,"This fails:

create column family Countries with comparator=UTF8Type and column_metadata=[ {column_name: name, validation_class: UTF8Type} ];

This works:

create column family Countries with comparator=UTF8Type and column_metadata=[ {column_name: fooname, validation_class: UTF8Type} ];",xedin,thobbs,Low,Resolved,Fixed,16/Jan/11 20:03,16/Apr/19 09:33
Bug,CASSANDRA-1999,12495809,Fix misuses of ByteBufferUtil.string(),"ByteBufferUtil.string() takes a start offset and a length. It is however used as if taking
a start and end offset.",slebresne,slebresne,Normal,Resolved,Fixed,17/Jan/11 17:48,16/Apr/19 09:33
Bug,CASSANDRA-2000,12495843,"Table comments indicates expiry checking happens 10x times per minimum interval, but doesn't","Minor point, the there is a comment in the body of the Table constructor that claims it checks 10x as often to not miss the deadline by more than 10%. It seems to me that either the comment should be removed, or a change is necessary to make it true (trivial patch attached).",scode,scode,Low,Resolved,Fixed,18/Jan/11 01:24,16/Apr/19 09:33
Bug,CASSANDRA-2001,12495898,0.7 migrations/schema serializations are incompatible with trunk,"Two problems:
1. inserting replicate_on_write into the middle of the CfDef members created a problem with serialization.  
2. merging the genavro files created a strange namespacing problem.",gdusbabek,gdusbabek,Low,Resolved,Fixed,18/Jan/11 14:03,16/Apr/19 09:33
Bug,CASSANDRA-2010,12496038,Error when read repair is disabled,,jbellis,jbellis,Low,Resolved,Fixed,19/Jan/11 18:54,16/Apr/19 09:33
Bug,CASSANDRA-2014,12496104,Can't delete whole row from Hadoop MapReduce,"ColumnFamilyRecordWriter.java doesn't support Mutation with Deletion without slice_predicat and super_column to delete whole row. The other way I tried is to specify SlicePredicate with empty start and finish and I got:

{code}
java.io.IOException: InvalidRequestException(why:Deletion does not yet support SliceRange predicates.)
        at org.apache.cassandra.hadoop.ColumnFamilyRecordWriter$RangeClient.run(ColumnFamilyRecordWriter.java:355)
{code}

I tryied to patch the ColumnFamilyRecordWriter.java like this:
{code}
--- a/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java
+++ b/src/java/org/apache/cassandra/hadoop/ColumnFamilyRecordWriter.java
@@ -166,10 +166,17 @@ implements org.apache.hadoop.mapred.RecordWriter<ByteBuffer,List<org.apache.cass
             // deletion
             Deletion deletion = new Deletion(amut.deletion.timestamp);
             mutation.setDeletion(deletion);
+
             org.apache.cassandra.avro.SlicePredicate apred = amut.deletion.predicate;
-            if (amut.deletion.super_column != null)
+            if (apred == null && amut.deletion.super_column == null)
+            {
+                // epmty; delete whole row
+            }
+            else if (amut.deletion.super_column != null)
+            {
                 // super column
                 deletion.setSuper_column(copy(amut.deletion.super_column));
+            }
             else if (apred.column_names != null)
             {
                 // column names
{code}

but that didn't work as well.",patrik.modesto,patrik.modesto,Normal,Resolved,Fixed,20/Jan/11 08:55,16/Apr/19 09:33
Bug,CASSANDRA-2016,12496139,Files added with missing license headers,"       src/java/org/apache/cassandra/utils/BloomFilterSerializer.java
       src/java/org/apache/cassandra/utils/LegacyBloomFilterSerializer.java
       src/java/org/apache/cassandra/service/RepairCallback.java
       src/java/org/apache/cassandra/io/util/ColumnSortedMap.java

are all missing license headers... ASLv2 I assume",stephenc,stephenc,Urgent,Resolved,Fixed,20/Jan/11 15:35,16/Apr/19 09:33
Bug,CASSANDRA-2022,12496185,Twisted driver for CQL,"In-tree CQL drivers should be reasonably consistent with one another (wherever possible/practical), and implement a minimum of:

• Query compression
• Keyspace assignment on connection
• Connection pooling / load-balancing

The goal is not to supplant the idiomatic libraries, but to provide a consistent, stable base for them to build upon.",brandon.williams,urandom,Low,Resolved,Fixed,20/Jan/11 21:58,16/Apr/19 09:33
Bug,CASSANDRA-2023,12496207,fix regression in 1968 (young gen sizing logic),"1968 introduced a regression (there was still cleanup to do). In particular it broke when an explicit MAX_HEAP_SIZE was set. Attaching *draft* patch (needs more testing).

Allowing automatic newsize calculation in the face of a manually specified MAX_HEAP_SIZE was problematic. Either one has to duplicate JVM parsing of MAX_HEAP_SIZE or ask the user to set MAX_HEAP_SIZE_IN_MB (or similar) instead.

In this patch (consider it a draft) i opted for the latter + picking up MAX_HEAP_SIZE for backwards compatibility (but with the effect that it disables new size calculation). I tried to make it slightly more posixly correct, but as usual no guarantees given that I have no posix shell to test it on.

I'm not really happy about the shell acrobatics and my confidence that there is not some left-over issue is not high. Should we just not worry about MAX_HEAP_SIZE compatibility and remove all that compatibility cruft? Plenty of acrobatics left still, but it would remove the more hideous parts.",scode,scode,Normal,Resolved,Fixed,21/Jan/11 00:57,16/Apr/19 09:33
Bug,CASSANDRA-2031,12496345,CLI chokes on whitespace after semicolon when using -f,"The CLI chokes on whitespace after the semicolon when a file is passed with -f

""... missing EOF at""",xedin,mdennis,Low,Resolved,Fixed,21/Jan/11 22:06,16/Apr/19 09:33
Bug,CASSANDRA-2036,12496455,cassandra-topology.properties cannot reside inside jar file,"PropertyFileSnitch cannot load the cassandra-topology.properties if it is located inside a jar file.

At startup cassandra will print and exit
[ERROR] 20:50:01  Fatal error: Unable to read cassandra-topology.properties
Bad configuration; unable to start server

It seems FBUtilities.resourceToFIle(..) can only be used for loading plain files.

The attached patch solves the problem. It uses the standard java approach for loading a resource stream...
",mck,mck,Normal,Resolved,Fixed,23/Jan/11 20:19,16/Apr/19 09:33
Bug,CASSANDRA-2039,12496518,LazilyCompactedRow doesn't add CFInfo to digest,"LazilyCompactedRow.update doesn't add the CFInfo or columnCount to the digest, so the hash value in the Merkle tree does not include this data.  However, PrecompactedRow does include this.  Two consequences of this are:
* Row-level tombstones are not compared when using LazilyCompactedRow so could remain inconsistent
* LazilyCompactedRow and PrecompactedRow produce different hashes of the same row, so if two nodes have differing in_memory_compaction_limit_in_mb values, rows of size in between the two limits will have different hashes so will always be repaired even when they are the same.",richardlow,richardlow,Low,Resolved,Fixed,24/Jan/11 10:59,16/Apr/19 09:33
Bug,CASSANDRA-2044,12496600,CLI should loop on describe_schema until agreement or fatel exit with stacktrace/message if no agreement after X seconds,"see CASSANDRA-2026 for brief background.

It's easy to enter statements into the CLI before the schema has settled, often causing problems where it is no longer possible to get the nodes in agreement about the schema without removing the system directory.

The alleviate the most common problems with this, the CLI should issue the modification statement and loop on describe_schema until all nodes agree or until X seconds has passed.  If the timeout has been exceeded, the CLI should exit with an error and inform the user that the schema has not settled and further migrations are ill-advised until it does.

number_of_nodes/2+1 seconds seems like a decent wait time for schema migrations to start with.

Bonus points for making the value configurable.",xedin,mdennis,Normal,Resolved,Fixed,24/Jan/11 20:36,16/Apr/19 09:33
Bug,CASSANDRA-2046,12496610,ivy.jar is included in the binary distribution,"The build currently copys ivy.xml into the bin.tar.gz

according to Eric, this is a bug


-rw-r--r-- 0/0          910990 2011-01-06 16:46 apache-cassandra-0.7.0/lib/ivy-2.1.0.jar
",urandom,stephenc,Normal,Resolved,Fixed,24/Jan/11 22:15,16/Apr/19 09:33
Bug,CASSANDRA-2049,12496687,"On the CLI, creating or updating a keyspace to use the NetworkTopologyStrategy breaks ""show keyspaces;""","To reproduce:
- Start fresh.
- Run ""show keyspaces;""
- Run ""create keyspace Keyspace1 with placement_strategy='org.apache.cassandra.locator.NetworkTopologyStrategy';""
- Run ""show keyspaces;""

Note how before it showed the system keyspace.  After it shows just:
Keyspace: Keyspace1:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
null

If you have multiple keyspaces, it will hide those as well.  Also, if you create the keyspace and then update it with NetworkTopologyStrategy, the same thing will happen.",xedin,jeromatron,Normal,Resolved,Fixed,25/Jan/11 15:23,16/Apr/19 09:33
Bug,CASSANDRA-2051,12496702,Fixes for multi-datacenter writes,"Copied from CASSANDRA-982:

    * Message::removeHeader
      message.setHeader(RowMutation.FORWARD_HEADER, null) throws NullPointerException

    * db/RowMutationVerbHandler::forwardToLocalNodes
      set correct destination address for sendOneWay

    * response(ReadResponse result) added to DatacenterReadCallback
      otherwise ReadCallback will process local results and condition will be never signaled in DatacenterReadCallback

    * FORWARD header removed in StorageProxy::sendMessages if dataCenter equals to localDataCenter
      (if a non local DC processed before local DC FORWARD header will be set when unhintedMessage used in sendToHintedEndpoints. one instance of Message used for unhintedMessage)
",ivancso,jbellis,Normal,Resolved,Fixed,25/Jan/11 16:57,16/Apr/19 09:33
Bug,CASSANDRA-2053,12496713,Make cache saving less contentious,"The current default for saving key caches is every hour.  Additionally the default timeout for flushing memtables is every hour.  I've seen situations where both of these occuring at the same time every hour causes enough pressure on the node to have it drop messages and other nodes mark it dead.  This happens across the cluster and results in flapping.

We should do something to spread this out. Perhaps staggering cache saves/flushes that occur due to timeouts.",jbellis,nickmbailey,Normal,Resolved,Fixed,25/Jan/11 19:15,16/Apr/19 09:33
Bug,CASSANDRA-2057,12496751,overflow in NodeCmd,We aggregate the long read/write counts across CFs into an int.,kingryan,kingryan,Low,Resolved,Fixed,26/Jan/11 00:54,16/Apr/19 09:33
Bug,CASSANDRA-2058,12496753,Load spikes due to MessagingService-generated garbage collection,"(Filing as a placeholder bug as I gather information.)

At ~10p 24 Jan, I upgraded our 20-node cluster from 0.6.8->0.6.10, turned on the DES, and moved some CFs from one KS into another (drain whole cluster, take it down, move files, change schema, put it back up). Since then, I've had four storms whereby a node's load will shoot to 700+ (400% CPU on a 4-cpu machine) and become totally unresponsive. After a moment or two like that, its neighbour dies too, and the failure cascades around the ring. Unfortunately because of the high load I'm not able to get into the machine to pull a thread dump to see wtf it's doing as it happens.

I've also had an issue where a single node spikes up to high load, but recovers. This may or may not be the same issue from which the nodes don't recover as above, but both are new behaviour",jbellis,ketralnis,Normal,Resolved,Fixed,26/Jan/11 01:20,16/Apr/19 09:33
Bug,CASSANDRA-2059,12496836,SSTableDeletingReference only deletes data files,"Ching-Cheng Chen reports on the mailing list:
	

In SSTableDeletingReference, it try this operation

components.remove(Component.DATA);

before

STable.delete(desc, components);

However, the components was reference to the components object which was created inside SSTable by

this.components = Collections.unmodifiableSet(dataComponents);

As you can see, you can't try the remove operation on that components object.",jbellis,jbellis,Normal,Resolved,Fixed,26/Jan/11 15:28,16/Apr/19 09:33
Bug,CASSANDRA-2061,12496895,Missing logging for some exceptions,"{quote}Since you are using ScheduledThreadPoolExecutor.schedule(), the exception was swallowed by the FutureTask.

You will have to perform a get() method on the ScheduledFuture, and you will get ExecutionException if there was any exception occured in run().{quote}",jbellis,stuhood,Low,Resolved,Fixed,27/Jan/11 04:29,16/Apr/19 09:33
Bug,CASSANDRA-2063,12496928,bug with test,"when executing nosetests (e.g: nosetests test/system/test_avro_system.py), you get the following error:

    mod = load_module(part_fqname, fh, filename, desc)
  File ""/tmp/apache-cassandra-0.7.0-src/test/system/test_avro_system.py"", line 19
    from . import AvroTester
         ^
SyntaxError: invalid syntax

All *.py scripts should be changed to be ""from __init__ import (AvroTester)""    instead of ""from . import AvroTester""


",urandom,amit71,Low,Resolved,Fixed,27/Jan/11 11:34,16/Apr/19 09:33
Bug,CASSANDRA-2066,12496957,2 (more) Misuses of ByteBuffer relative gets,In RandomPartitioner and SerDeUtils,slebresne,slebresne,Normal,Resolved,Fixed,27/Jan/11 16:28,16/Apr/19 09:33
Bug,CASSANDRA-2067,12496972,refactor o.a.c.utils.UUIDGen to allow creating type 1 UUIDs for a given time,"CASSANDRA-2027 creates the need to generate type 1 UUIDs using arbitrary date/times.  IMO, this would be a good opportunity to replace o.a.c.utils.UUIDGen with the class that Gary Dusbabek wrote for Flewton (https://github.com/flewton/flewton/blob/master/src/com/rackspace/flewton/util/UUIDGen.java), which is better/more comprehensive.  We can even eliminate the dependency on JUG.

Patches to follow.",urandom,urandom,Normal,Resolved,Fixed,27/Jan/11 19:16,16/Apr/19 09:33
Bug,CASSANDRA-2069,12496983,Read repair causes tremendous GC pressure,"To reproduce: start a three node cluster, insert 1M rows with stress.java and rf=2.  Take one down, delete its data, then bring it back up and issue 1M reads against it.  After the run is done you will see at least 1 STW long enough to mark the node as dead, often 4 or 5.",jbellis,brandon.williams,Normal,Resolved,Fixed,27/Jan/11 21:14,16/Apr/19 09:33
Bug,CASSANDRA-2071,12496999,RP.describeOwnership() does some bad math,"If the input isn't sorted correctly for some reason, then describeOwnership() fails to calculate the ownership %ages correctly.

Repro is 2 nodes with these tokens, you get these fractions:
49000620740128447720217646403197156812 : 0.7615167
770141183460469231731687303715884105727 : 4.2384834

423% ownership is obviously broken.",jhermes,jhermes,Low,Resolved,Fixed,27/Jan/11 23:23,16/Apr/19 09:33
Bug,CASSANDRA-2072,12497003,Race condition during decommission,"Occasionally when decommissioning a node, there is a race condition that occurs where another node will never remove the token and thus propagate it again with a state of down.  With CASSANDRA-1900 we can solve this, but it shouldn't occur in the first place.

Given nodes A, B, and C, if you decommission B it will stream to A and C.  When complete, B will decommission and receive this stacktrace:

ERROR 00:02:40,282 Fatal exception in thread Thread[Thread-5,5,main]
java.util.concurrent.RejectedExecutionException: ThreadPoolExecutor has shut down
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:62)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at org.apache.cassandra.net.MessagingService.receive(MessagingService.java:387)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:91

At this point A will show it is removing B's token, but C will not and instead its failure detector will report that B is dead, and nodetool ring on C shows B in a leaving/down state.  In another gossip round, C will propagate this state back to A.",brandon.williams,brandon.williams,Low,Resolved,Fixed,28/Jan/11 00:06,16/Apr/19 09:33
Bug,CASSANDRA-2073,12497092,Streaming occasionally makes gossip back up,"Streaming occasionally makes gossip back up, causing nodes to mark each other as down even though the network is ok.  This appears to happen just after streaming has finished.  I noticed this in the course of working on CASSANDRA-2072, so decommission is one way to reproduce.  It seems to happen maybe one of fifteen or twenty tries, so it's fairly rare.",jbellis,brandon.williams,Low,Resolved,Fixed,28/Jan/11 20:47,16/Apr/19 09:33
Bug,CASSANDRA-2076,12497183,Not restarting due to Invalid saved cache,"This occured on two nodes on me (running 0.7.1 from svn)

One node was killed by the kernel due to a OOM and the other node was haning and I had to kill it manually with kill -9 (kill didn't work). (maybe these were faulty hardware nodes, I don't know)

The saved_cache was corrupt afterwards and I couldn't start the nodes. 

After deleting the saved_caches directory I could start the nodes again. 

Instead of not starting when an error occurs, cassandra could simply delete the errornous file and continue to start?




 INFO 22:31:11,570 reading saved cache
/hd1/cassandra_md5/saved_caches/table_attributes-table_attributes-KeyCache
ERROR 22:31:11,595 Exception encountered during startup.
java.lang.RuntimeException: The provided key was not UTF8 encoded.
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
       at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:281)
       at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:218)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:458)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:440)
       at org.apache.cassandra.db.Table.initCf(Table.java:360)
       at org.apache.cassandra.db.Table.<init>(Table.java:290)
       at org.apache.cassandra.db.Table.open(Table.java:107)
       at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
       at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:312)
       at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.nio.charset.MalformedInputException: Input length = 1
       at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
       at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
       at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
       ... 11 more
Exception encountered during startup.
java.lang.RuntimeException: The provided key was not UTF8 encoded.
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:159)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.decorateKey(OrderPreservingPartitioner.java:44)
       at org.apache.cassandra.db.ColumnFamilyStore.readSavedCache(ColumnFamilyStore.java:281)
       at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:218)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:458)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:440)
       at org.apache.cassandra.db.Table.initCf(Table.java:360)
       at org.apache.cassandra.db.Table.<init>(Table.java:290)
       at org.apache.cassandra.db.Table.open(Table.java:107)
       at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:167)
       at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:312)
       at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:81)
Caused by: java.nio.charset.MalformedInputException: Input length = 1
       at java.nio.charset.CoderResult.throwException(CoderResult.java:260)
       at java.nio.charset.CharsetDecoder.decode(CharsetDecoder.java:781)
       at org.apache.cassandra.utils.FBUtilities.decodeToUTF8(FBUtilities.java:403)
       at org.apache.cassandra.dht.OrderPreservingPartitioner.getToken(OrderPreservingPartitioner.java:155)
       ... 11 more",mdennis,tbritz,Urgent,Resolved,Fixed,31/Jan/11 09:15,16/Apr/19 09:33
Bug,CASSANDRA-2081,12497245,Consistency QUORUM does not work anymore (hector:Could not fullfill request on this host),"I'm using apache-cassandra-2011-01-28_20-06-01.jar and hector 7.0.25.

Using consistency level Quorum won't work anymore (tested it on read). Consisteny level ONE still works though

I have tried this with one dead node in my cluster.

If I restart cassandra with an older svn revision (apache-cassandra-2011-01-28_20-06-01.jar), I can access the cluster with consistency level QUORUM again, while still using apache-cassandra-2011-01-28_20-06-01.jar and hector 7.0.25 in my application.


11/01/31 19:54:38 ERROR connection.CassandraHostRetryService: Downed intr1n18(192.168.0.18):9160 host still appears to be down: Unable to open transport to intr1n18(192.168.0.18):9160 , java.net.NoRouteToHostException: No route to host
11/01/31 19:54:38 INFO connection.CassandraHostRetryService: Downed Host retry status false with host: intr1n18(192.168.0.18):9160
11/01/31 19:54:45 ERROR connection.HConnectionManager: Could not fullfill request on this host CassandraClient<intr1n11:9160-483>

intr1n11 is marked as up however and I can also access the node through the cassandra cli.


192.168.0.1     Up     Normal  8.02 GB         5.00%   0cc
192.168.0.2     Up     Normal  7.96 GB         5.00%   199
192.168.0.3     Up     Normal  8.24 GB         5.00%   266
192.168.0.4     Up     Normal  4.94 GB         5.00%   333
192.168.0.5     Up     Normal  5.02 GB         5.00%   400
192.168.0.6     Up     Normal  5 GB            5.00%   4cc
192.168.0.7     Up     Normal  5.1 GB          5.00%   599
192.168.0.8     Up     Normal  5.07 GB         5.00%   666
192.168.0.9     Up     Normal  4.78 GB         5.00%   733
192.168.0.10    Up     Normal  4.34 GB         5.00%   7ff
192.168.0.11    Up     Normal  5.01 GB         5.00%   8cc
192.168.0.12    Up     Normal  5.31 GB         5.00%   999
192.168.0.13    Up     Normal  5.56 GB         5.00%   a66
192.168.0.14    Up     Normal  5.82 GB         5.00%   b33
192.168.0.15    Up     Normal  5.57 GB         5.00%   c00
192.168.0.16    Up     Normal  5.03 GB         5.00%   ccc
192.168.0.17    Up     Normal  4.77 GB         5.00%   d99
192.168.0.18    Down   Normal  ?               5.00%   e66
192.168.0.19    Up     Normal  4.78 GB         5.00%   f33
192.168.0.20    Up     Normal  4.83 GB         5.00%   ffffffffffffffff




",amorton,tbritz,Urgent,Resolved,Fixed,31/Jan/11 19:40,16/Apr/19 09:33
Bug,CASSANDRA-2082,12497246,Saved row cache continues to be read past max cache size,"Scenario:
 - Node has a saved row cache of size n
 - node OOMs
 - Make row cache size = .5n to prevent OOM while we debug, restart node.
 - n items are still read from the row cache, making startup take twice as long as needed.


(This is intended as a straightforward bug, not as a hackish CASSANDRA-1966.)",cburroughs,cburroughs,Low,Resolved,Fixed,31/Jan/11 19:46,16/Apr/19 09:33
Bug,CASSANDRA-2083,12497250,Hinted Handoff and schema race,"If a node is down while a keyspace/cf is created and then data is inserted into the CF causing other nodes to hint, when the down node recovers it will lose some hints until the schema propagates:

{noformat}
ERROR 19:59:28,264 Error in row mutation
org.apache.cassandra.db.UnserializableColumnFamilyException: Couldn't find cfId=1000
        at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:117)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:377)
        at org.apache.cassandra.db.RowMutationVerbHandler.doVerb(RowMutationVerbHandler.java:50)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:70)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 19:59:28,356 Applying migration 28e2e7a4-2d74-11e0-9b6b-cdc89135952c
{noformat}",brandon.williams,brandon.williams,Low,Resolved,Fixed,31/Jan/11 19:57,16/Apr/19 09:33
Bug,CASSANDRA-2085,12497283,digest latencies are not included in snitch calculations,"ResponseVerbHandler calls

        MessagingService.instance.maybeAddLatency(cb, message.getFrom(), age);

but maybeAddLatency needs to include DigestResponseHandler (it was ported from 0.7 where that no longer exists)",jbellis,jbellis,Normal,Resolved,Fixed,01/Feb/11 04:19,16/Apr/19 09:33
Bug,CASSANDRA-2088,12497286,Clean up after failed (repair) streaming operation,,slebresne,stuhood,Low,Resolved,Fixed,01/Feb/11 05:49,16/Apr/19 09:33
Bug,CASSANDRA-2094,12497408,fix regression in CL.ALL read,"regression:
- digest message object re-used across multiple hosts.

problem:
- shared message id, so the first digest response received will remove the callback for all others.",kelvin,kelvin,Normal,Resolved,Fixed,02/Feb/11 01:14,16/Apr/19 09:33
Bug,CASSANDRA-2095,12497410,SST counter repair,"When creating an SST for AES of a commutative/counter CF, do not ""clean"" non-commutative/counter columns.  i.e. deleted columns.",kelvin,kelvin,Normal,Resolved,Fixed,02/Feb/11 01:24,16/Apr/19 09:33
Bug,CASSANDRA-2096,12497417,InternalException on system_update_column_family if column_metadata is not assigned,"Steps to reproduce:

Execute system_update_column_family without passing in column_metadata in CfDef object.

Error:


java.lang.NullPointerException
	at org.apache.cassandra.config.CFMetaData.convertToAvro(CFMetaData.java:827)
	at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:882)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:4518)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:3227)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
",lenn0x,lenn0x,Low,Resolved,Fixed,02/Feb/11 03:44,16/Apr/19 09:33
Bug,CASSANDRA-2099,12497499,UUID generation when specifying date-times is broken,"o.a.c.utils.UUIDGen properly guards against a clock moving backward, but this makes the creation of UUIDs based on arbitrary date-times problematic.",urandom,urandom,Low,Resolved,Fixed,02/Feb/11 20:06,16/Apr/19 09:33
Bug,CASSANDRA-2100,12497510,Restart required to change cache_save_period,"The cache_save_period is set in the schema for each column family.  However this value is only checked when a node starts up so changing this value isn't really dynamic.

We should actually change this when the schema changes instead of having to restart.",jhermes,nickmbailey,Low,Resolved,Fixed,02/Feb/11 21:46,16/Apr/19 09:33
Bug,CASSANDRA-2101,12497514,support deletes in counters,Obey timestampOfLastDelete during reconciliation.,kelvin,kelvin,Normal,Resolved,Fixed,02/Feb/11 22:05,16/Apr/19 09:33
Bug,CASSANDRA-2102,12497520,saved row cache doesn't save the cache,"saving row caches works by periodically iterating of the keySet() on the caches and writing the keys for the cached contents to disk.  The cache keys are DecoratedKeys.  DecoratedKeys contain a Token token and a ByteBuffer key.  The underlying buffer on the key gets reused so the contents change.  This means that all the cache entries have distinct tokens but only a handful of distinct key values.  This means that when the cache is loaded you only end up loading a handful of keys instead of the ones actually in your cache.

",mdennis,mdennis,Normal,Resolved,Fixed,02/Feb/11 22:42,16/Apr/19 09:33
Bug,CASSANDRA-2104,12497548,IndexOutOfBoundsException during lazy row compaction of supercolumns,"I ran into an exception when lazily compacting wide rows of TimeUUID columns.
It seems to trigger when a row is larger than {{in_memory_compaction_limit_in_mb}}.

Traceback:
{noformat}
 INFO [CompactionExecutor:1] 2011-02-03 10:59:59,262 CompactionIterator.java (line 135) Compacting large row XXXXXXXXXXXXX (76999384 bytes) incrementally
 ERROR [CompactionExecutor:1] 2011-02-03 10:59:59,266 AbstractCassandraDaemon.java (line 114) Fatal exception in thread T
 hread[CompactionExecutor:1,1,main]
 java.lang.IndexOutOfBoundsException
         at java.nio.Buffer.checkIndex(Buffer.java:514)
         at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)
         at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(TimeUUIDType.java:56)
         at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:45)
         at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.java:29)
         at java.util.concurrent.ConcurrentSkipListMap$ComparableUsingComparator.compareTo(ConcurrentSkipListMap.java:606
 )
         at java.util.concurrent.ConcurrentSkipListMap.findPredecessor(ConcurrentSkipListMap.java:685)
         at java.util.concurrent.ConcurrentSkipListMap.doPut(ConcurrentSkipListMap.java:864)
         at java.util.concurrent.ConcurrentSkipListMap.putIfAbsent(ConcurrentSkipListMap.java:1893)
         at org.apache.cassandra.db.SuperColumn.addColumn(SuperColumn.java:170)
         at org.apache.cassandra.db.SuperColumn.putColumn(SuperColumn.java:195)
         at org.apache.cassandra.db.ColumnFamily.addColumn(ColumnFamily.java:221)
         at org.apache.cassandra.io.LazilyCompactedRow$LazyColumnIterator.reduce(LazilyCompactedRow.java:204)
         at org.apache.cassandra.io.LazilyCompactedRow$LazyColumnIterator.reduce(LazilyCompactedRow.java:185)
         at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:62)
         at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
         at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
         at com.google.common.collect.Iterators$7.computeNext(Iterators.java:604)
         at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
         at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
         at org.apache.cassandra.db.ColumnIndexer.serializeInternal(ColumnIndexer.java:76)
         at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:50)
         at org.apache.cassandra.io.LazilyCompactedRow.<init>(LazilyCompactedRow.java:88)
         at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:137)
         at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:108)
         at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:43)
         at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:73)
         at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
         at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
         at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
         at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
         at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:426)
         at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:122)
         at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:92)
         at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
         at java.util.concurrent.FutureTask.run(FutureTask.java:138)
         at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
         at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
         at java.lang.Thread.run(Thread.java:662)

{noformat}",slebresne,dln,Normal,Resolved,Fixed,03/Feb/11 10:08,16/Apr/19 09:33
Bug,CASSANDRA-2105,12497550,Fix the read race condition in CFStore for counters ,"There is a (known) race condition during counter read. Indeed, for standard
column family there is a small time during which a memtable is both active and
pending flush and similarly a small time during which a 'memtable' is both
pending flush and an active sstable. For counters that would imply sometime
reconciling twice during a read the same counterColumn and thus over-counting.

Current code changes this slightly by trading the possibility to count twice a
given counterColumn by the possibility to miss a counterColumn. Thus it trades
over-counts for under-counts.

But this is no fix and there is no hope to offer clients any kind of guarantee
on reads unless we fix this.
",slebresne,slebresne,Normal,Resolved,Fixed,03/Feb/11 10:51,16/Apr/19 09:33
Bug,CASSANDRA-2111,12497714,cassandra-cli 'use Keyspace user pass' breaks with SimpleAuth,"If SimpleAuth is used and the -Daccess.properties... JVM options are passed in, the CLI's ""use Keyspace user 'password'"" command breaks.  However, if the --username and --password options are used, you can still authenticate.",xedin,thobbs,Low,Resolved,Fixed,04/Feb/11 20:41,16/Apr/19 09:33
Bug,CASSANDRA-2121,12497851,stress.java cardinality option parsing typo,"Session.java
{noformat} 
            if (cmd.hasOption(""C""))
                cardinality = Integer.parseInt(cmd.getOptionValue(""t""));
{noformat} 

",cburroughs,cburroughs,Low,Resolved,Fixed,07/Feb/11 14:49,16/Apr/19 09:33
Bug,CASSANDRA-2123,12497861,nodetool cfhistograms write/read latency columns are reversed,"As first reported by Oleg Proudnikov in the thread http://www.mail-archive.com/user@cassandra.apache.org/msg09607.html the columns for read and write latency are reversed in the output of cfhistograms.  The Mbean values are correct.

Example output during stress.java insert test.
{noformat}
Keyspace1/Standard1 histograms
Offset      SSTables     Write Latency      Read Latency          Row Size      Column Count
1                  0                 0                 0                 0                 0
2                  0                 0                 1                 0                 0
3                  0                 0               998                 0                 0
4                  0                 0              7729                 0                 0
5                  0                 0             22844                 0                 0
6                  0                 0             44439                 0           6524792
7                  0                 0             64576                 0                 0
8                  0                 0             79000                 0                 0
10                 0                 0            139338                 0                 0
12                 0                 0             84675                 0                 0
14                 0                 0             36928                 0                 0
17                 0                 0             16547                 0                 0
20                 0                 0              3926                 0                 0
24                 0                 0              1681                 0                 0
29                 0                 0               776                 0                 0
35                 0                 0               357                 0                 0
42                 0                 0               172                 0                 0
50                 0                 0                51                 0                 0
60                 0                 0                15                 0                 0
72                 0                 0                10                 0                 0
86                 0                 0                 4                 0                 0
103                0                 0                 6                 0                 0
124                0                 0                 3                 0                 0
149                0                 0                 1                 0                 0
179                0                 0                 0                 0                 0
215                0                 0                 1                 0                 0
258                0                 0                 1                 0                 0
310                0                 0                 0                 0                 0
372                0                 0                 1           6524792                 0
446                0                 0                 2                 0                 0
535                0                 0                 0                 0                 0
642                0                 0                 0                 0                 0
770                0                 0                 0                 0                 0
924                0                 0                 0                 0                 0
1109               0                 0                 1                 0                 0
1331               0                 0                 0                 0                 0
{noformat}",jbellis,cburroughs,Low,Resolved,Fixed,07/Feb/11 15:53,16/Apr/19 09:33
Bug,CASSANDRA-2128,12497895,Corrupted Commit logs,"Two of our nodes had a hard failure.

They both came up with a corrupted commit log.

On startup we get this:
{quote}
011-02-07_19:34:03.95124 INFO - Finished reading /var/lib/cassandra/commitlog/CommitLog-1297099954252.log
2011-02-07_19:34:03.95400 ERROR - Exception encountered during startup.
2011-02-07_19:34:03.95403 java.io.EOFException
2011-02-07_19:34:03.95403 	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
2011-02-07_19:34:03.95404 	at java.io.DataInputStream.readUTF(DataInputStream.java:572)
2011-02-07_19:34:03.95405 	at java.io.DataInputStream.readUTF(DataInputStream.java:547)
2011-02-07_19:34:03.95406 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:363)
2011-02-07_19:34:03.95407 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:318)
2011-02-07_19:34:03.95408 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:240)
2011-02-07_19:34:03.95409 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
2011-02-07_19:34:03.95409 	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:115)
2011-02-07_19:34:03.95410 	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
2011-02-07_19:34:03.95422 Exception encountered during startup.
2011-02-07_19:34:03.95436 java.io.EOFException
2011-02-07_19:34:03.95447 	at java.io.DataInputStream.readUnsignedShort(DataInputStream.java:323)
2011-02-07_19:34:03.95458 	at java.io.DataInputStream.readUTF(DataInputStream.java:572)
2011-02-07_19:34:03.95468 	at java.io.DataInputStream.readUTF(DataInputStream.java:547)
2011-02-07_19:34:03.95478 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:363)
2011-02-07_19:34:03.95489 	at org.apache.cassandra.db.RowMutationSerializer.deserialize(RowMutation.java:318)
2011-02-07_19:34:03.95499 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:240)
2011-02-07_19:34:03.95510 	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:172)
2011-02-07_19:34:03.95521 	at org.apache.cassandra.thrift.CassandraDaemon.setup(CassandraDaemon.java:115)
2011-02-07_19:34:03.95531 	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:224)
{quote}

On node A, the commit log in question is 100mb.

On node B, the commit log in question is 60mb.

An ideal resolution would be if EOF is hit early, log something, but don't stop the startup.  Instead process everything that we have done so far, and keep going.
",jbellis,pquerna,Normal,Resolved,Fixed,07/Feb/11 19:41,16/Apr/19 09:33
Bug,CASSANDRA-2129,12497902,removetoken after removetoken rf error fails to work,"2 node cluster, a keyspace existed with rf=2.  Tried removetoken and got:

mbulman@ripcord-maverick1:/usr/src/cassandra/tags/cassandra-0.7.0$ bin/nodetool -h localhost removetoken 159559397954378837828954138596956659794
Exception in thread ""main"" java.lang.IllegalStateException: replication factor (2) exceeds number of endpoints (1)

Deleted the keyspace, and tried again:

mbulman@ripcord-maverick1:/usr/src/cassandra/tags/cassandra-0.7.0$ bin/nodetool -h localhost removetoken 159559397954378837828954138596956659794
Exception in thread ""main"" java.lang.UnsupportedOperationException: This node is already processing a removal. Wait for it to complete.",brandon.williams,mbulman,Low,Resolved,Fixed,07/Feb/11 20:56,16/Apr/19 09:33
Bug,CASSANDRA-2131,12497914,Illegal file mode when saving caches,"The following error is logged when trying to save caches


DEBUG [CompactionExecutor:1] 2011-02-08 07:30:03,647 CacheWriter.java (line 45) Saving /var/lib/cassandra/saved_caches/Keyspace1-ascii-KeyCache
ERROR [CompactionExecutor:1] 2011-02-08 07:30:03,725 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: Illegal mode ""w"" must be one of ""r"", ""rw"", ""rws"", or ""rwd""
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.IllegalArgumentException: Illegal mode ""w"" must be one of ""r"", ""rw"", ""rws"", or ""rwd""
	at java.io.RandomAccessFile.<init>(RandomAccessFile.java:197)
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:116)
	at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:48)
	at org.apache.cassandra.db.CompactionManager$9.runMayThrow(CompactionManager.java:746)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 6 more
",amorton,amorton,Normal,Resolved,Fixed,07/Feb/11 22:01,16/Apr/19 09:33
Bug,CASSANDRA-2134,12497954,Error in ThreadPoolExecutor,"On my two-node test setup I get repeatedly following error:

The 10.0.18.129 server log:
{noformat} 
 INFO 14:10:37,707 Node /10.0.18.99 has restarted, now UP again
 INFO 14:10:37,708 Checking remote schema before delivering hints
 INFO 14:10:37,708 Sleeping 45506ms to stagger hint delivery
 INFO 14:10:37,709 Node /10.0.18.99 state jump to normal
 INFO 14:11:23,215 Started hinted handoff for endpoint /10.0.18.99
ERROR 14:11:23,884 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
ERROR 14:11:23,885 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
{noformat} 

The 10.0.18.99 server log:
{noformat} 
 INFO 14:10:37,691 Binding thrift service to /0.0.0.0:9160
 INFO 14:10:37,693 Using TFastFramedTransport with a max frame size of
15728640 bytes.
 INFO 14:10:37,695 Listening for thrift clients...
 INFO 14:10:38,337 GC for ParNew: 954 ms, 658827608 reclaimed leaving
966732432 used; max is 4265607168
 INFO 14:11:27,142 Started hinted handoff for endpoint /10.0.18.129
ERROR 14:11:27,370 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
ERROR 14:11:27,371 Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException
       at java.nio.Buffer.position(Buffer.java:218)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:117)
       at org.apache.cassandra.utils.ByteBufferUtil.string(ByteBufferUtil.java:111)
       at org.apache.cassandra.db.HintedHandOffManager.getTableAndCFNames(HintedHandOffManager.java:237)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:306)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:385)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
{noformat}

It happen durring batch_mutate test or after restart, when there are commitlogs to replay. Using current 0.7.1 from cassandra-0.7 branch.",slebresne,patrik.modesto,Normal,Resolved,Fixed,08/Feb/11 06:57,16/Apr/19 09:33
Bug,CASSANDRA-2136,12497984,CLI does not use sub-comparator on Super CF `get`.,"[default@foo] get foo[page-field];
=> (super_column=20110208,
    (column=82f4c650-2d53-11e0-a08b-58b035f3f60d, value=msg1,
timestamp=1297159430471000)
    (column=82f4c650-2d53-11e0-a08b-58b035f3f60e, value=msg2,
timestamp=1297159437423000)
    (column=82f4c650-2d53-11e0-a08b-58b035f3f60f, value=msg3,
timestamp=1297159439855000))
Returned 1 results.

[default@foo] get foo[page-field][20110208];
, value=msg1, timestamp=1297159430471000)
=> (column=???P-S???X?5??, value=msg2, timestamp=1297159437423000)
=> (column=???P-S???X?5??, value=msg3, timestamp=1297159439855000)
Returned 3 results.

[default@foo] get
foo[page-field][20110208][82f4c650-2d53-11e0-a08b-58b035f3f60e];
=> (column=???P-S???X?5??, value=msg2, timestamp=1297159437423000)",xedin,xedin,Low,Resolved,Fixed,08/Feb/11 14:55,16/Apr/19 09:33
Bug,CASSANDRA-2140,12498014,versioning isn't going to work when forwarding messages,SP.sendToHintedEndpoints needs to take care to create properly versioned messages that get forwarded to other nodes.,gdusbabek,gdusbabek,Normal,Resolved,Fixed,08/Feb/11 18:06,16/Apr/19 09:33
Bug,CASSANDRA-2146,12498154,cli read_repair_chance input not validated,"{noformat}
put(ColumnFamilyArgument.READ_REPAIR_CHANCE, ""Probability (0.0-1.0) with which to perform read repairs on CL.ONE reads"");
{noformat}

The input range is not enforced so 
{noformat}
create column family ... with ... read_repair_chance = 25;
{noformat}

Will result in
{noformat}
Keyspace: ks1:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
    Replication Factor: 3
  Column Families:
    ColumnFamily: cf1
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
...
      Read repair chance: 25.0
{noformat}


I am unsure if in practice this means RR chance 100%, or something surprising.  (I ran into this because read_repair_chance requires a leading 0 and ommiting it results in an unhelpful ""Command not found:"" message).
",xedin,cburroughs,Low,Resolved,Fixed,09/Feb/11 18:28,16/Apr/19 09:33
Bug,CASSANDRA-2147,12498161,stress.java doesn't read more unique rows than 2x the number of threads,"This can be observed by watching how much the row/key cache grows on each run.  I'm not sure when this started or if it was always the case, but it's actually useful behavior when you want to benchmark just the cache, so it'd be nice to preserve as an option.",xedin,brandon.williams,Low,Resolved,Fixed,09/Feb/11 19:02,16/Apr/19 09:33
Bug,CASSANDRA-2148,12498177,system CFs default to large memtable throughputs on large heaps,"The default memtableThroughputInMB is calculated now based on the heap size.  Most people running with a large heap in production explicitly set it for their memtable(s).  However, the the CFs in the system keyspace still default to the calculated value which on a large heap can be quite large.  HintsColumnFamily is really the only problematic one though as the others are flushed afters changes to them.

we should:

1) set the throughput on the hints CF to a reasonable max and min value - min(256, max(32, normalDefault/2))
2) set the throughput on the other system CFs to some small constant value (just as a safety); 8M sounds good",mdennis,mdennis,Normal,Resolved,Fixed,09/Feb/11 21:09,16/Apr/19 09:33
Bug,CASSANDRA-2150,12498191,sstablekeys silently ignores extra arguments,"sstablekeys only passes arg $1 to SSTableExporter instead of passing all arguments, like sstable2json.  Only one SSTable is allowed as an argument, but this is normally detected in SSTableExporter.java.  By only passing the one argument, we end up silently ignoring the remaining arguments.",thobbs,thobbs,Low,Resolved,Fixed,10/Feb/11 04:47,16/Apr/19 09:33
Bug,CASSANDRA-2152,12498269,Encryption options are not validated correctly in DatabaseDescriptor,Missing configuration for encryption_options introduced via CASSANDRA-1567 result in an obtuse NPE from MessagingService,zznate,zznate,Normal,Resolved,Fixed,10/Feb/11 17:24,16/Apr/19 09:33
Bug,CASSANDRA-2155,12498391,Fix counter bug (regression from svn commit r1068504),A line was mistakenly removed by the merge from 0.7 at r1068504,slebresne,slebresne,Low,Resolved,Fixed,11/Feb/11 15:45,16/Apr/19 09:33
Bug,CASSANDRA-2158,12498478,memtable_throughput_in_mb can not support sizes over 2.2 gigs because of an integer overflow.,"If memtable_throughput_in_mb is set past 2.2 gigs, no errors are thrown.  However, as soon as data starts being written it is almost immediately being flushed.  Several hundred SSTables are created in minutes.  I am almost positive that the problem is that when memtable_throughput_in_mb is being converted into bytes the result is stored in an integer, which is overflowing.

From memtable.java:

    private final int THRESHOLD;
    private final int THRESHOLD_COUNT;

...
this.THRESHOLD = cfs.getMemtableThroughputInMB() * 1024 * 1024;
this.THRESHOLD_COUNT = (int) (cfs.getMemtableOperationsInMillions() * 1024 * 1024);


NOTE:
I also think currentThroughput also needs to be changed from an int to a long.  I'm not sure if it is as simple as this or if this also is used in other places.",jbellis,trisk,Low,Resolved,Fixed,13/Feb/11 06:54,16/Apr/19 09:33
Bug,CASSANDRA-2159,12498528,Fix build of stress.java in trunk,Some lines in stress (java) seems to have missed a merge,,slebresne,Low,Resolved,Fixed,14/Feb/11 13:19,16/Apr/19 09:33
Bug,CASSANDRA-2162,12498565,cassandra-cli --keyspace option doesn't work properly when used with authentication,"The logic to select the keyspace is applied before authentication credentials are processed in cassandra-cli. This leads to a ""Keyspace FOO not found"" message at login for a keyspace that exists.
",jancona,jancona,Low,Resolved,Fixed,14/Feb/11 19:30,16/Apr/19 09:33
Bug,CASSANDRA-2163,12498566,UnavailableException after bootstrapping a previously decommissioned node,"In trunk (not in 0.7), if I boostrap a node, decommission it and boostrap it back (after having clean all data directory but on same ip), I get UnavailableException on read. 
I've bisected the regression to r1067508.",brandon.williams,slebresne,Normal,Resolved,Fixed,14/Feb/11 19:31,16/Apr/19 09:33
Bug,CASSANDRA-2164,12498643,debian build dep on ant-optional is missing,"Without the ant-optional package installed in Debian, builds fail (on lenny) with:

    Could not create type regexpmapper due to No supported regular expression matcher found: java.lang.ClassNotFoundException: org.apache.tools.ant.util.regexp.Jdk14RegexpMatcher

The attached patch makes it build. Tested on lenny and squeeze.
",scode,scode,Low,Resolved,Fixed,15/Feb/11 14:46,16/Apr/19 09:33
Bug,CASSANDRA-2165,12498670,EOFException during name query,"As reported by Jonas Borgstrom on the mailing list:

{quote}
While testing the new 0.7.1 release I got the following exception:

ERROR [ReadStage:11] 2011-02-15 16:39:18,105
DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.io.IOError: java.io.EOFException
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:75)
       at
org.apache.cassandra.db.filter.NamesQueryFilter.getSSTableColumnIterator(NamesQueryFilter.java:59)
       at
org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
       at
org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1274)
       at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1166)
       at
org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
       at org.apache.cassandra.db.Table.getRow(Table.java:384)
       at
org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:60)
       at
org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:473)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       at
java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
       at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
       at java.lang.Thread.run(Thread.java:636)
Caused by: java.io.EOFException
       at java.io.DataInputStream.readInt(DataInputStream.java:392)
       at
org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:48)
       at
org.apache.cassandra.utils.BloomFilterSerializer.deserialize(BloomFilterSerializer.java:30)
       at
org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:108)
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.read(SSTableNamesIterator.java:106)
       at
org.apache.cassandra.db.columniterator.SSTableNamesIterator.<init>(SSTableNamesIterator.java:71)
       ... 12 more

{quote}",jbellis,slebresne,Normal,Resolved,Fixed,15/Feb/11 18:11,16/Apr/19 09:33
Bug,CASSANDRA-2168,12498678,SSTable2Json tool returns a different key when a querying for a specific key in an SSTable that does not exist,"bin/sstable2json storage/core/data/Foo/BAR-1-Data.db -k NonExistantKey

returns

{ ""ExistantKey"" } ",bcoverston,bcoverston,Low,Resolved,Fixed,15/Feb/11 19:19,16/Apr/19 09:33
Bug,CASSANDRA-2169,12498679,user created with debian packaging is unable to increase memlock,"To reproduce:
- Install a fresh copy of ubuntu 10.04.
- Install sun's java6 jdk.
- Install libjna-java 3.2.7 into /usr/share/java.
- Install cassandra 0.7.0 from the apache debian packages.
- Start cassandra using /etc/init.d/cassandra
In the output.log there will be the following error:
{quote}
Unable to lock JVM memory (ENOMEM). This can result in part of the JVM being swapped out, especially with mmapped I/O enabled. Increase RLIMIT_MEMLOCK or run Cassandra as root.
{quote}
This shouldn't be as the debian package creates /etc/security/limits.d/cassandra.conf and sets the cassandra user's memlock limit to 'unlimited'.

I tried a variety of things including making the memlock unlimited for all users in /etc/security/limits.conf.  I was able to run cassandra using root with jna symbolically linked into /usr/share/cassandra from /usr/share/java, but I could never get the init.d script to work and get beyond that error.

Based on all the trial and error, I think it might have to do with the cassandra user itself, but my debian/ubuntu fu isn't as good as others'.",jeromatron,jeromatron,Low,Resolved,Fixed,15/Feb/11 19:36,16/Apr/19 09:33
Bug,CASSANDRA-2170,12498681,Load spikes,"as reported on CASSANDRA-2058, some users are still seeing load spikes on 0.6.11, even with fairly low-volume read workloads.",brandon.williams,jbellis,Normal,Resolved,Fixed,15/Feb/11 19:46,16/Apr/19 09:33
Bug,CASSANDRA-2172,12498711,Saved-cache files are created for empty caches,This results in a harmless EOFException on startup.,mdennis,jbellis,Low,Resolved,Fixed,16/Feb/11 02:10,16/Apr/19 09:33
Bug,CASSANDRA-2174,12498718,saved caches written with BufferedRandomAccessFile cannot be read by ObjectInputStream,The CacheWriter is currently writing with BufferedRandomAccessFile which is incompatible with ObjectInputStream resulting in stack traces about corrupted stream headers when loading a saved cache.,mdennis,mdennis,Normal,Resolved,Fixed,16/Feb/11 04:35,16/Apr/19 09:33
Bug,CASSANDRA-2178,12498818,Memtable Flush writers doesn't actually flush in parallel,"The flushWriter JMXEnabledThreadPoolExecutor sets the core pool min to 1, and sets the LBQ to DatabaseDescriptor.getFlushWriters(). Increasing memtable_flush_writers should allow us to flush more in parallel. The pool will not grow until LBQ fills up to DatabaseDescriptor.getFlushWriters(). ",lenn0x,lenn0x,Low,Resolved,Fixed,17/Feb/11 01:45,16/Apr/19 09:33
Bug,CASSANDRA-2182,12498911,Cassandra doesn't startup on single core boxes.,"I happened to run cassandra in a VM and got the following error, caused by the single core:

ERROR 10:47:30,304 Exception encountered during startup.
java.lang.AssertionError: multi-threaded stages must have at least 2 threads
        at org.apache.cassandra.concurrent.StageManager.multiThreadedStage(StageManager.java:60)
        at org.apache.cassandra.concurrent.StageManager.<clinit>(StageManager.java:53)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:303)
        at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:159)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:175)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)",tjake,tjake,Low,Resolved,Fixed,17/Feb/11 16:13,16/Apr/19 09:33
Bug,CASSANDRA-2183,12498922,memtable_flush_after_mins setting not working,"We have observed the behavior that memtable_flush_after_mins setting not working occasionally.   After some testing and code digging, we finally figured out what going on.
The memtable_flush_after_mins won't work on certain condition with current implementation in Cassandra.

In org.apache.cassandra.db.Table,  the scheduled flush task is setup by the following code during construction.

------------------------------------------------------------------------------------------------------------------
int minCheckMs = Integer.MAX_VALUE;
       
for (ColumnFamilyStore cfs : columnFamilyStores.values())  
{
    minCheckMs = Math.min(minCheckMs, cfs.getMemtableFlushAfterMins() * 60 * 1000);
}

Runnable runnable = new Runnable()
{
   public void run()
   {
       for (ColumnFamilyStore cfs : columnFamilyStores.values())
       {
           cfs.forceFlushIfExpired();
       }
   }
};
flushTask = StorageService.scheduledTasks.scheduleWithFixedDelay(runnable, minCheckMs, minCheckMs, TimeUnit.MILLISECONDS);
------------------------------------------------------------------------------------------------------------------------------

Now for our application, we will create a keyspacewithout without any columnfamily first.  And only add needed columnfamily later depends on request.

However, when keyspacegot created (without any columnfamily ), the above code will actually schedule a fixed delay flush check task with Integer.MAX_VALUE ms
since there is no columnfamily yet.

Later when you add columnfamily to this empty keyspace, the initCf() method in Table.java doesn't check whether the scheduled flush check task interval need
to be updated or not.   To fix this, we'd need to restart the Cassandra after columnfamily added into the keyspace. 

I would suggest that add additional logic in initCf() method to recreate a scheduled flush check task if needed.
",jbellis,chenyy,Low,Resolved,Fixed,17/Feb/11 16:51,16/Apr/19 09:33
Bug,CASSANDRA-2184,12498930,Returning split length of 0 confuses Pig,"Matt Kennedy reports on the user list,

bq. There is a new feature in Pig 0.8 that will try to reduce the number of splits used to speed up the whole job.  Since the ColumnFamilyInputFormat lists the input size as zero, this feature eliminates all of the splits except for one. 
bq. The workaround is to disable this feature for jobs that use CassandraStorage by setting -Dpig.splitCombination=false in the pig_cassandra script.
{noformat}

bq. However, we wanted to keep splitCombination on because it is a useful optimization for a lot of our use cases, so I went digging for the least intrusive way to keep the split combiner on, but also prevent it from combining splits that read from Cassandra.  My solution, which you are welcome to critique, is to change line 65 of http://svn.apache.org/viewvc/cassandra/trunk/src/java/org/apache/cassandra/hadoop/ColumnFamilySplit.java such that it returns Long.MAX_VALUE instead of zero.

I looked into actually returning the number of keys in the split but Hadoop javadoc says ""Get the size of the split, so that the input splits can be sorted by size"" so since our splits should be very very close in size this doesn't sound like it's worth doing an extra round trip to the host servers to get super accurate numbers on.  Returning MAX_VALUE seems like it's good enough.",brandon.williams,jbellis,Low,Resolved,Fixed,17/Feb/11 18:35,16/Apr/19 09:33
Bug,CASSANDRA-2187,12498988,Cassandra Cli hangs forever if schema does not settle within timeout window,"validateSchemaIsSettled will hang in the while loop since we never update start if migrations never settle.
",lenn0x,lenn0x,Low,Resolved,Fixed,18/Feb/11 02:24,16/Apr/19 09:33
Bug,CASSANDRA-2188,12498989,"sstable2json generates invalid json for ""paged"" rows","I have a json file created with sstable2json for a column family of super column type. But json2sstable failed to create sstable from the file. It's because file format is wrong. 

 WARN 11:41:55,141 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
org.codehaus.jackson.JsonParseException: Unexpected character ('""' (code 34)): was expecting comma to separate OBJECT entries
 at [Source: dump.json; line: 2, column: 739439661]
        at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:929)
        at org.codehaus.jackson.impl.JsonParserBase._reportError(JsonParserBase.java:632)
        at org.codehaus.jackson.impl.JsonParserBase._reportUnexpectedChar(JsonParserBase.java:565)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:128)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:93)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:65)
        at org.codehaus.jackson.map.deser.MapDeserializer._readAndBind(MapDeserializer.java:197)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:145)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:23)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:1261)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:517)
        at org.codehaus.jackson.JsonParser.readValueAs(JsonParser.java:897)
        at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:208)
        at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:197)
        at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:421)
ERROR: Unexpected character ('""' (code 34)): was expecting comma to separate OBJECT entries
 at [Source: dump.json; line: 2, column: 739439661]

When I looked at the file, I found that a comma is missing between super columns. The part of data is like this: 

[""756e697473"",
 ""32"",
 1297926692097000, false]]}""32303036303830373135303030302f313030303030303030302d32303036313030322d303030303030303639382d612f30"": {
""deletedAt"": -9223372036854775808,
 ""subColumns"": [[""5f64656c"",
 """",
 1297926692097000,
 false],

You'll see no comma between } and "". 

",xedin,skamio,Normal,Resolved,Fixed,18/Feb/11 02:43,16/Apr/19 09:33
Bug,CASSANDRA-2189,12498991,json2sstable fails due to OutOfMemory,"I have a json file created with sstable2json for a column family of super column type. Its size is about 1.9GB. (It's a dump of all keys because I cannot find out how to specify keys to dump in sstable2json.)
When I tried to create sstable from the json file, it failed with OutOfMemoryError as follows.

 WARN 00:31:58,595 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
Exception in thread ""main"" java.lang.OutOfMemoryError: PermGen space
        at java.lang.String.intern(Native Method)
        at org.codehaus.jackson.util.InternCache.intern(InternCache.java:40)
        at org.codehaus.jackson.sym.BytesToNameCanonicalizer.addName(BytesToNameCanonicalizer.java:471)
        at org.codehaus.jackson.impl.Utf8StreamParser.addName(Utf8StreamParser.java:893)
        at org.codehaus.jackson.impl.Utf8StreamParser.findName(Utf8StreamParser.java:773)
        at org.codehaus.jackson.impl.Utf8StreamParser.parseLongFieldName(Utf8StreamParser.java:379)
        at org.codehaus.jackson.impl.Utf8StreamParser.parseMediumFieldName(Utf8StreamParser.java:347)
        at org.codehaus.jackson.impl.Utf8StreamParser._parseFieldName(Utf8StreamParser.java:304)
        at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:140)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.mapObject(UntypedObjectDeserializer.java:93)
        at org.codehaus.jackson.map.deser.UntypedObjectDeserializer.deserialize(UntypedObjectDeserializer.java:65)
        at org.codehaus.jackson.map.deser.MapDeserializer._readAndBind(MapDeserializer.java:197)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:145)
        at org.codehaus.jackson.map.deser.MapDeserializer.deserialize(MapDeserializer.java:23)
        at org.codehaus.jackson.map.ObjectMapper._readValue(ObjectMapper.java:1261)
        at org.codehaus.jackson.map.ObjectMapper.readValue(ObjectMapper.java:517)
        at org.codehaus.jackson.JsonParser.readValueAs(JsonParser.java:897)
        at org.apache.cassandra.tools.SSTableImport.importUnsorted(SSTableImport.java:208)
        at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:197)
        at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:421)

So, what I had to is that split the json file with ""split"" command and modify them to be correct json file. Create sstable for each small json files.

Could you change json2sstable to avoid OutOfMemory?",jbellis,skamio,Low,Resolved,Fixed,18/Feb/11 02:53,16/Apr/19 09:33
Bug,CASSANDRA-2196,12499061,Hyphenated index names cause problems,"When inserting a large number of entries with batch_insert (100000) using thrift compiled into C# there's a NumberFormatException that occurs.

The first logged entry that tipped me off was this:
 INFO 10:53:52,171 Writing Memtable-TransactionLogs.client-hostname@350930888(1171371 bytes, 32787 o
perations)
ERROR 10:53:52,171 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.NumberFormatException: For input string: ""tmp""
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NumberFormatException: For input string: ""tmp""
        at java.lang.NumberFormatException.forInputString(NumberFormatException.java:48)
        at java.lang.Integer.parseInt(Integer.java:449)
        at java.lang.Integer.parseInt(Integer.java:499)
        at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:154)
        at org.apache.cassandra.io.sstable.Descriptor.fromFilename(Descriptor.java:119)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:67)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:156)
        at org.apache.cassandra.db.Memtable.access$000(Memtable.java:49)
        at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:174)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more

 Which points to the suspect piece of code in Descriptor.java:154 (browse at https://github.com/apache/cassandra/blob/trunk/src/java/org/apache/cassandra/io/sstable/Descriptor.java)

 The file I believe it's trying to parse is mentioned in my logs as:

INFO 10:51:31,231 Compacted to C:\cassandra\apache-cassandra-0.7.2\bin\..\Storage\data\system\Index
Info-tmp-f-6-Data.db.  384 to 225 (~58% of original) bytes for 1 keys.  Time: 281ms.

 I'm new here, so I'm not sure what needs fixing here (the filename, or the parsing of it).",jbellis,sgpope,Low,Resolved,Fixed,18/Feb/11 16:16,16/Apr/19 09:33
Bug,CASSANDRA-2200,12499092,stress.java doesn't insert the correct amount of rows,"For example, if you pass -n 2000000 you only get 1999800 (with 300 threads at least, didn't check if it was related)",xedin,brandon.williams,Low,Resolved,Fixed,18/Feb/11 22:29,16/Apr/19 09:33
Bug,CASSANDRA-2206,12499221,Startup fails due to cassandra trying to delete nonexisting file,"Hi,

On one of our nodes, startup fails due to cassandra trying to delete a nonexistant ""Data"" file (see below).

Why that file is missing is another mistery... The log file entries don't show any ERROR messages before cassandra restarted (for reasons I don't know) and this error occured.

Directory listing:

total 109M
-rw-r--r-- 1 root root  51M 2011-02-21 05:25 table_task-f-1666-Data.db
-rw-r--r-- 1 root root 243K 2011-02-21 05:25 table_task-f-1666-Filter.db
-rw-r--r-- 1 root root 6.1M 2011-02-21 05:25 table_task-f-1666-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 05:25 table_task-f-1666-Statistics.db
-rw-r--r-- 1 root root 9.8M 2011-02-21 11:36 table_task-f-1703-Data.db
-rw-r--r-- 1 root root  57K 2011-02-21 11:36 table_task-f-1703-Filter.db
-rw-r--r-- 1 root root 1.3M 2011-02-21 11:36 table_task-f-1703-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:36 table_task-f-1703-Statistics.db
-rw-r--r-- 1 root root 292K 2011-02-21 11:42 table_task-f-1704-Data.db
-rw-r--r-- 1 root root 1.7K 2011-02-21 11:42 table_task-f-1704-Filter.db
-rw-r--r-- 1 root root  42K 2011-02-21 11:42 table_task-f-1704-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:42 table_task-f-1704-Statistics.db
-rw-r--r-- 1 root root 364K 2011-02-21 11:52 table_task-f-1705-Data.db
-rw-r--r-- 1 root root 2.0K 2011-02-21 11:52 table_task-f-1705-Filter.db
-rw-r--r-- 1 root root  50K 2011-02-21 11:52 table_task-f-1705-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:52 table_task-f-1705-Statistics.db
-rw-r--r-- 1 root root 535K 2011-02-21 12:10 table_task-f-1706-Data.db
-rw-r--r-- 1 root root 2.8K 2011-02-21 12:10 table_task-f-1706-Filter.db
-rw-r--r-- 1 root root  70K 2011-02-21 12:10 table_task-f-1706-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 12:10 table_task-f-1706-Statistics.db
-rw-r--r-- 1 root root  11M 2011-02-21 12:11 table_task-f-1707-Data.db
-rw-r--r-- 1 root root  18M 2011-02-21 09:47 table_task_meta-f-417-Data.db
-rw-r--r-- 1 root root 271K 2011-02-21 09:47 table_task_meta-f-417-Filter.db
-rw-r--r-- 1 root root 6.7M 2011-02-21 09:47 table_task_meta-f-417-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 09:47 table_task_meta-f-417-Statistics.db
-rw-r--r-- 1 root root 1.2M 2011-02-21 10:47 table_task_meta-f-418-Data.db
-rw-r--r-- 1 root root  18K 2011-02-21 10:47 table_task_meta-f-418-Filter.db
-rw-r--r-- 1 root root 460K 2011-02-21 10:47 table_task_meta-f-418-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 10:47 table_task_meta-f-418-Statistics.db
-rw-r--r-- 1 root root 791K 2011-02-21 11:47 table_task_meta-f-419-Data.db
-rw-r--r-- 1 root root  13K 2011-02-21 11:47 table_task_meta-f-419-Filter.db
-rw-r--r-- 1 root root 311K 2011-02-21 11:47 table_task_meta-f-419-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 11:47 table_task_meta-f-419-Statistics.db
-rw-r--r-- 1 root root  57K 2011-02-21 12:11 table_task-tmp-f-1707-Filter.db
-rw-r--r-- 1 root root 1.4M 2011-02-21 12:11 table_task-tmp-f-1707-Index.db
-rw-r--r-- 1 root root 4.2K 2011-02-21 12:11 table_task-tmp-f-1707-Statistics.db


Cassandra log:

/software/cassandra/bin/cassandra
rm: cannot remove `/software/cassandra/lib/jna.jar': No such file or directory
root@intr1n3:/cassandra/data/table_task#  INFO 12:47:29,020 Logging initialized
 INFO 12:47:29,030 Heap size: 2614493184/2614493184
 INFO 12:47:29,031 JNA not found. Native methods will be disabled.
 INFO 12:47:29,038 Loading settings from file:/software/cassandra/conf/cassandra.yaml
 INFO 12:47:29,320 DiskAccessMode is standard, indexAccessMode is mmap
 INFO 12:47:29,332 Creating new commitlog segment /hd1/cassandra_md5/commitlog/CommitLog-1298288849332.log
 INFO 12:47:29,422 Opening /cassandra/data/system/Schema-f-244
 INFO 12:47:29,434 Opening /cassandra/data/system/Migrations-f-244
 INFO 12:47:29,437 Opening /cassandra/data/system/LocationInfo-f-137
 INFO 12:47:29,440 Opening /cassandra/data/system/HintsColumnFamily-f-352
 INFO 12:47:29,441 Opening /cassandra/data/system/HintsColumnFamily-f-353
 INFO 12:47:29,465 Loading schema version 54bc134e-2229-11e0-9159-fdf0b6b4b562
 WARN 12:47:29,623 Schema definitions were defined both locally and in cassandra.yaml. Definitions in cassandra.yaml were ignored.
ERROR 12:47:29,638 Exception encountered during startup.
java.io.IOError: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:145)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:468)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:153)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
Caused by: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:51)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:41)
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:133)
        ... 4 more
Exception encountered during startup.
java.io.IOError: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:145)
        at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:468)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:153)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)
Caused by: java.io.IOException: Failed to delete /cassandra/data/table_task/table_task-tmp-f-1707-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:51)
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:41)
        at org.apache.cassandra.io.sstable.SSTable.delete(SSTable.java:133)
        ... 4 more
",jbellis,tbritz,Normal,Resolved,Fixed,21/Feb/11 11:57,16/Apr/19 09:33
Bug,CASSANDRA-2207,12499223,Error saving cache on Windows,"I launch clean cassandra 7.2 instalation, and after few days i look at system.log follow error (more then 10 times):


ERROR [CompactionExecutor:1] 2011-02-19 02:56:17,965 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.io.IOException: Unable to rename cache to F:\Cassandra\7.2\saved_caches\system-LocationInfo-KeyCache
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
    at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to rename cache to F:\Cassandra\7.2\saved_caches\system-LocationInfo-KeyCache
    at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:85)
    at org.apache.cassandra.db.CompactionManager$9.runMayThrow(CompactionManager.java:746)
    at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
    ... 6 more
",jbellis,tantra,Low,Resolved,Fixed,21/Feb/11 12:14,16/Apr/19 09:33
Bug,CASSANDRA-2211,12499284,Cleanup can create sstables whose contents do not match their advertised version,"Since cleanup switched to per-sstable operation (CASSANDRA-1916), the main loop looks like this:

{code}
                    if (Range.isTokenInRanges(row.getKey().token, ranges))
                    {
                        writer = maybeCreateWriter(sstable, compactionFileLocation, expectedBloomFilterSize, writer);
                        writer.append(new EchoedRow(row));
                        totalkeysWritten++;
                    }
                    else
                    {
                        while (row.hasNext())
                        {
                            IColumn column = row.next();
                            if (indexedColumns.contains(column.name()))
                                Table.cleanupIndexEntry(cfs, row.getKey().key, column);
                        }
                    }
{code}

... that is, rows that haven't changed we copy to the new sstable without deserializing, with EchoedRow.  But, the new sstable is created with CURRENT_VERSION which may not be what the old data consisted of.

(This could cause symptoms similar to CASSANDRA-2195 but I do not think it is the cause of that bug; IIRC the cluster in question there was not upgraded from an older Cassandra.)",jbellis,jbellis,Normal,Resolved,Fixed,21/Feb/11 23:28,16/Apr/19 09:33
Bug,CASSANDRA-2212,12499291,Cannot get range slice of super columns in reversed order,"I cannot get range slice of super columns in reversed order.  These data are stored in Cassandra in advance.  On the other hand, range slice of these data in normal order can be acquired.

You can reproduce the bug by executing attached programs.  
- 1. Start Cassandra daemon on localhost (number of thrift port is 9160)
- 2. Create keyspace and column family, according to ""create_table.cli"", 
- 3. Execute ""cassandra_sample_insert.py"", storing pairs of row keys and super columns
- 4. Execute ""cassandra_sample_rangeslice.py"" and get range slice of stored super columns
""cassandra_sample_insert.py"" and ""cassandra_sample_rangeslice.py"" require pycassa.  

You will need to execute 4.""cassandra_sample_rangeslice.py"" with following options so that you get range slice of super columns in reversed order.  

 % python cassandra_sample_rangeslice.py -r 00082 00083

On the other hand, to get range slice in normal order, you will need to use following options.  

 % python cassandra_sample_rangeslice.py -f 00082 00083

00082 and 00083 are the specified key range.  Range slice can be acquired in normal order but, I cannot get it in reversed order.  

I assume that there may be a bug within the code for acquiring the index block of specified range.  In fact, 00083 is included in gap between lastName of index block and firstName of next index block.   ",slebresne,muga_nishizawa,Normal,Resolved,Fixed,22/Feb/11 02:36,16/Apr/19 09:33
Bug,CASSANDRA-2213,12499317,A bug in BufferedRandomAccessFile,"The first line of BufferedRandomAccessFile.readAtMost is
{code}if (length >= bufferEnd && hitEOF){code}

I think It should be "">"" instead of "">="",
Here is a test for this:{code}
    @Test
    public void testRead() throws IOException {
        File tmpFile = File.createTempFile(""readtest"", ""bin"");
        tmpFile.deleteOnExit();

        // Create the BRAF by filename instead of by file.
        BufferedRandomAccessFile rw = new BufferedRandomAccessFile(tmpFile.getPath(), ""rw"");
        rw.write(new byte[] {1});

        rw.seek(0);
        byte[] buffer = new byte[1];
        assert rw.read(buffer) == 1;
        assert buffer[0] == 1;
}
{code}",leojay,leojay,Normal,Resolved,Fixed,22/Feb/11 09:15,16/Apr/19 09:33
Bug,CASSANDRA-2216,12499332,Compaction can echo data which breaks upon sstable format changes,"While compaction, if for a row we have only 1 sstable holding data, we echo this data. This breaks when we change the data format, creating mixed (corrupted) sstable.

(I suspect this is the cause of CASSANDRA-2195, but opening a new ticket until we can confirm that hunch)",slebresne,slebresne,Urgent,Resolved,Fixed,22/Feb/11 12:17,16/Apr/19 09:33
Bug,CASSANDRA-2218,12499383,Performance regression caused by cache-avoiding code in BRAF,"As reported by Ivan Georgiev on the mailing list, BRAF.reBuffer unnecessarily does extra read + fadvise when seeking to the end of the file.",jbellis,jbellis,Normal,Resolved,Fixed,22/Feb/11 18:19,16/Apr/19 09:33
Bug,CASSANDRA-2223,12499415,ClientOnly mode is creating directories,,gdusbabek,gdusbabek,Low,Resolved,Fixed,22/Feb/11 22:36,16/Apr/19 09:33
Bug,CASSANDRA-2228,12499491,Race conditions when reinitialisating nodes (OOM + Nullpointer),"I had a corrupt system table which wouldn't compact anymore and I deleted the files and restarted cassandra and let it take the same token/ip address.

I experienced the same errors when I'm adding a newly installed node under the same token/ip address before calling repair.

1)
After a few seconds/minutes, I get a OOM error:


 INFO [FlushWriter:1] 2011-02-23 16:40:28,958 Memtable.java (line 164) Completed flushing /cassandra/data/system/Schema-f-15-Data.db (8037 bytes)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,965 Migration.java (line 133) Applying migration 3e30e76b-1e3f-11e0-8369-5a9c1faed4ae Add keyspace: table_userentriesrep factor:3rep strategy:SimpleStrategy{org.apache.cassandra.config.CFMetaData@58925d9[cfId=1024,tableName=table_userentries,cfName=table_userentries,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType@b44dff0,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=200000.0,readRepairChance=0.0,gcGraceSeconds=86400,defaultValidator=org.apache.cassandra.db.marshal.BytesType@b44dff0,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=3600,memtableFlushAfterMins=60,memtableThroughputInMb=64,memtableOperationsInMillions=10.0,column_metadata={}], org.apache.cassandra.config.CFMetaData@11ab7246[cfId=1025,tableName=table_userentries,cfName=table_userentries_meta,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType@b44dff0,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=200000.0,readRepairChance=0.0,gcGraceSeconds=86400,defaultValidator=org.apache.cassandra.db.marshal.BytesType@b44dff0,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=3600,memtableFlushAfterMins=60,memtableThroughputInMb=64,memtableOperationsInMillions=10.0,column_metadata={}]}
 INFO [MigrationStage:1] 2011-02-23 16:40:28,965 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for Migrations at CommitLogContext(file='/cassandra/commitlog/CommitLog-1298475572022.log', position=226075)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,966 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-Migrations@2121008793(12529 bytes, 1 operations)
 INFO [FlushWriter:1] 2011-02-23 16:40:28,966 Memtable.java (line 157) Writing Memtable-Migrations@2121008793(12529 bytes, 1 operations)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,966 ColumnFamilyStore.java (line 666) switching in a fresh Memtable for Schema at CommitLogContext(file='/cassandra/commitlog/CommitLog-1298475572022.log', position=226075)
 INFO [MigrationStage:1] 2011-02-23 16:40:28,967 ColumnFamilyStore.java (line 977) Enqueuing flush of Memtable-Schema@139610466(8370 bytes, 15 operations)
 INFO [ScheduledTasks:1] 2011-02-23 16:40:28,972 StatusLogger.java (line 89) table_sourcedetection.table_sourcedetection                 0,0                 0/0            0/200000
ERROR [FlushWriter:1] 2011-02-23 16:41:01,240 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[FlushWriter:1,5,main]
java.lang.OutOfMemoryError: Java heap space
        at java.nio.HeapByteBuffer.<init>(HeapByteBuffer.java:39)
        at java.nio.ByteBuffer.allocate(ByteBuffer.java:312)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:126)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:75)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:158)
        at org.apache.cassandra.db.Memtable.access$000(Memtable.java:51)
        at org.apache.cassandra.db.Memtable$1.runMayThrow(Memtable.java:176)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)





2) If I restart then, I'm getting an Nullpointer exception. The OOM error will only appear once.

ERROR [main] 2011-02-23 16:42:32,782 AbstractCassandraDaemon.java (line 333) Exception encountered during startup.
java.lang.NullPointerException
        at java.util.concurrent.ConcurrentHashMap.get(ConcurrentHashMap.java:768)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:925)
        at org.apache.cassandra.service.MigrationManager.passiveAnnounce(MigrationManager.java:105)
        at org.apache.cassandra.service.MigrationManager.applyMigrations(MigrationManager.java:161)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:185)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:316)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:79)


Killing and restarting the node multiple times will eventually ""fix"" these errors.


Steps to reproduce. Remove complete data directory and restart node with same token/ip.

",gdusbabek,tbritz,Normal,Resolved,Fixed,23/Feb/11 15:50,16/Apr/19 09:33
Bug,CASSANDRA-2235,12499563,Counter AES performance issue,"We noticed tonight when trying out AES for Counters in trunk, there is a serious performance issue when inlining the SSTables. We found that the way we are seeking in the file, BRAF keeps flushing out its buffer of 8MB, and we call dfile.sync() on every row. We are finalizing a patch to write a new SSTable on rebuild, instead of inlining. ",lenn0x,lenn0x,Normal,Resolved,Fixed,24/Feb/11 08:08,16/Apr/19 09:33
Bug,CASSANDRA-2236,12499565,Cli does not support updating replicate_on_write,Add support for updating a column families replicate on write setting.,lenn0x,lenn0x,Low,Resolved,Fixed,24/Feb/11 08:46,16/Apr/19 09:33
Bug,CASSANDRA-2237,12499597,cassandra.bat does fail when CASSANDRA_HOME contains a whitespace,"If you try to start cassandra from a directory with whitespaces you will see a stacktrace similar to this:

Starting Cassandra Server
Exception in thread ""main"" java.lang.NoClassDefFoundError: and
Caused by: java.lang.ClassNotFoundException: and
        at java.net.URLClassLoader$1.run(URLClassLoader.java:200)
        at java.security.AccessController.doPrivileged(Native Method)
        at java.net.URLClassLoader.findClass(URLClassLoader.java:188)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:303)
        at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:301)
        at java.lang.ClassLoader.loadClass(ClassLoader.java:248)
        at java.lang.ClassLoader.loadClassInternal(ClassLoader.java:316)
Could not find the main class: and.  Program will exit.
",norman,norman,Normal,Resolved,Fixed,24/Feb/11 14:24,16/Apr/19 09:33
Bug,CASSANDRA-2240,12499601,nodetool scrub hangs or throws an exception,"trying to run nodetool scrub hung or (only happened one time) threw the following exception:

ERROR [CompactionExecutor:1] 2011-02-28 10:26:26,620 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
        at org.apache.cassandra.dht.RandomPartitioner.convertFromDiskFormat(RandomPartitioner.java:62)
        at org.apache.cassandra.io.sstable.SSTableReader.decodeKey(SSTableReader.java:627)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:538)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:55)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:194)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
",jbellis,kunda,Normal,Resolved,Fixed,24/Feb/11 15:32,16/Apr/19 09:33
Bug,CASSANDRA-2241,12499622,BRAF read can loop infinitely instead of detecting EOF,"(marking this Minor since normally we never try to read past the end of an SSTable, but CASSANDRA-2240 is running into it.)",jbellis,jbellis,Low,Resolved,Fixed,24/Feb/11 18:53,16/Apr/19 09:33
Bug,CASSANDRA-2243,12499624,"fix ""ant codecoverage""",,stephenc,jbellis,Normal,Resolved,Fixed,24/Feb/11 19:05,16/Apr/19 09:33
Bug,CASSANDRA-2244,12499629,secondary indexes aren't created on pre-existing or streamed data,"The repaired node neither receives indexes from the replicas, nor does it generate them afterwards.  The same bug prevents generation of new indexes against existing data.",jbellis,brandon.williams,Normal,Resolved,Fixed,24/Feb/11 19:29,16/Apr/19 09:33
Bug,CASSANDRA-2248,12499705,ant javadoc fails on windows,"When try to run ""ant javadoc"" (or any task that include javadoc) on windows it fails with the error:

Javadoc failed: java.io.IOException: Cannot run program ""c:\Program Files\Java\jdk1.6.0_17\bin\javadoc.exe"": CreateProcess error=87, The parameter is incorrect",norman,norman,Normal,Resolved,Fixed,25/Feb/11 12:47,16/Apr/19 09:33
Bug,CASSANDRA-2251,12499749,unhelpful exception when failing to set keyspace,"If you fail to set the keyspace, {{ThriftValidation.validateColumnFamily()}} raises an {{AssertionError}}, which remotely results in a {{TApplicationException}}. ",urandom,urandom,Normal,Resolved,Fixed,25/Feb/11 19:27,16/Apr/19 09:33
Bug,CASSANDRA-2253,12499862,Gossiper Starvation,"Gossiper periodic task will get into starvation in case large sstable files need to be deleted.
Indeed the SSTableDeletingReference uses the same scheduledTasks pool (from StorageService) as the Gossiper and other periodic tasks, but the gossiper tasks should run each second to assure correct cluster status (liveness of nodes). In case of large sstable files to be deleted (several GB) the delete operation can take more than 30 sec, thus making the whole cluster going into a wrong state where nodes are marked as not living while they are!
This will lead to unneeded additional load like hinted hand off, wrong cluster state, increase in latency.

One of the possible solution is to use a separate pool for periodic and non periodic tasks. 
I've implemented such change and it resolves the problem. 
I can provide a patch ",mikaels,mikaels,Normal,Resolved,Fixed,27/Feb/11 16:03,16/Apr/19 09:33
Bug,CASSANDRA-2254,12499882,assert when using CL.EACH_QUORUM,"When using the NetworkTopology strategy, I am able to write using CL.LOCAL_QUORUM. When I attempt to write using CL.EACH_QUORUM, an assert is hit in DatacenterSyncWriteResponseHandler. Tracing the call through to the NetworkTopology code, it seems that this particular handler is only used when CL = EACH_QUORUM, yet the code asserts. ",segy,segy,Normal,Resolved,Fixed,28/Feb/11 00:42,16/Apr/19 09:33
Bug,CASSANDRA-2255,12499973,ColumnFamilyOutputFormat drops mutations when batches fill up.,"queue.poll() takes a mutation,
but then the batch is already full,
so the while loop exits, ant the mutation we just got is dropped.",jeromatron,eldondev,Normal,Resolved,Fixed,28/Feb/11 21:06,16/Apr/19 09:33
Bug,CASSANDRA-2256,12499978,BRAF assertion error,"While investigating CASSANDRA-2240 I ran into this:

{noformat}
java.lang.AssertionError
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.read(BufferedRandomAccessFile.java\
:230)
        at java.io.RandomAccessFile.readByte(RandomAccessFile.java:589)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:273)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:284)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:539)
{noformat}",xedin,jbellis,Low,Resolved,Fixed,28/Feb/11 21:29,16/Apr/19 09:33
Bug,CASSANDRA-2258,12500103,service.SerializationsTest failes under cobertura,"ant codecoverage -Dtest.name=SerializationsTest gives

{noformat}
    [junit] Testcase: testTreeResponseRead(org.apache.cassandra.service.SerializationsTest):	Caused an ERROR
    [junit] java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124
    [junit] java.lang.RuntimeException: java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.deserialize(AntiEntropyService.java:634)
    [junit] 	at org.apache.cassandra.service.SerializationsTest.testTreeResponseRead(SerializationsTest.java:90)
    [junit] Caused by: java.io.InvalidClassException: org.apache.cassandra.dht.BigIntegerToken; local class incompatible: stream classdesc serialVersionUID = -5833589141319293006, local class serialVersionUID = 2280189098581028124
    [junit] 	at java.io.ObjectStreamClass.initNonProxy(ObjectStreamClass.java:562)
    [junit] 	at java.io.ObjectInputStream.readNonProxyDesc(ObjectInputStream.java:1582)
    [junit] 	at java.io.ObjectInputStream.readClassDesc(ObjectInputStream.java:1495)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1731)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.defaultReadFields(ObjectInputStream.java:1946)
    [junit] 	at java.io.ObjectInputStream.readSerialData(ObjectInputStream.java:1870)
    [junit] 	at java.io.ObjectInputStream.readOrdinaryObject(ObjectInputStream.java:1752)
    [junit] 	at java.io.ObjectInputStream.readObject0(ObjectInputStream.java:1328)
    [junit] 	at java.io.ObjectInputStream.readObject(ObjectInputStream.java:350)
    [junit] 	at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.deserialize(AntiEntropyService.java:630)
{noformat}",gdusbabek,jbellis,Low,Resolved,Fixed,01/Mar/11 21:39,16/Apr/19 09:33
Bug,CASSANDRA-2259,12500108,column values are only being validated in insert(),insert() is the only code path that currently results in validate() being called for column values; it is possible to write invalid column values using batch_mutate(),mishravivek,urandom,Normal,Resolved,Fixed,01/Mar/11 22:06,16/Apr/19 09:33
Bug,CASSANDRA-2263,12500226,cql driver jar,"Work was done in CASSANDRA-1848  to create a jar for the CQL Java driver.  The generated Thrfit code was broken out into it's own jar as well, since that is a dependency for both servers and clients. However, based on the work currently happening in CASSANDRA-2262 and CASSANDRA-2124, it seems that additional dependencies will exist, and new jar(s) will need to be created.

The easiest way to fix this will probably be to put copies of all of {{o.a.c.db.marshal}} and {{o.a.c.utils}}, and a copy of {{o.a.c.config.ConfigurationException}} into the CQL driver jar (a split along those lines to create another jar doesn't make sense IMO).",urandom,urandom,Low,Resolved,Fixed,02/Mar/11 20:59,16/Apr/19 09:33
Bug,CASSANDRA-2269,12500381,OOM in the Thrift thread doesn't shut down server,"Example:

{noformat}
java.lang.OutOfMemoryError: Java heap space
        at org.cliffc.high_scale_lib.NonBlockingHashMap$CHM.resize(NonBlockingHashMap.java:849)
        at org.cliffc.high_scale_lib.NonBlockingHashMap$CHM.access$200(NonBlockingHashMap.java:699)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:634)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:339)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.put(NonBlockingHashMap.java:302)
        at org.apache.cassandra.utils.ExpiringMap.put(ExpiringMap.java:112)
        at org.apache.cassandra.net.MessagingService.addCallback(MessagingService.java:237)
        at org.apache.cassandra.net.MessagingService.sendRR(MessagingService.java:305)
        at org.apache.cassandra.service.StorageProxy.weakRead(StorageProxy.java:386)
        at org.apache.cassandra.service.StorageProxy.readProtocol(StorageProxy.java:347)
        at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:92)
        at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:175)
        at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:254)
        at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:215)
        at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:1272)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:1166)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:167)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
{noformat}",jbellis,jbellis,Low,Resolved,Fixed,03/Mar/11 23:46,16/Apr/19 09:33
Bug,CASSANDRA-2270,12500386,nodetool info NPE when node isn't fully booted,"Running ""nodetool -h 127.0.0.1 info"" when the node is not yet ready throw a NPE.

Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.gms.Gossiper.getCurrentGenerationNumber(Gossiper.java:313)
        at org.apache.cassandra.service.StorageService.getCurrentGenerationNumber(StorageService.java:1239)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:57)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)
        at java.lang.reflect.Method.invoke(Method.java:616)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:111)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:45)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:226)
        at com.sun.jmx.mbeanserver.PerInterface.getAttribute(PerInterface.java:83)
        at com.sun.jmx.mbeanserver.MBeanSupport.getAttribute(MBeanSupport.java:205)
",brandon.williams,wajam,Low,Resolved,Fixed,04/Mar/11 01:16,16/Apr/19 09:33
Bug,CASSANDRA-2279,12500654,Tombstones not collected post-repair,"The keys would only show up in sstables2json and look like this:

(root@aps4):/opt/cassandra/storage/queue/data/Panama Wed Feb 23 07:24:34am 
===> /opt/cassandra/bin/sstable2json Queue-2527-Data.db -k waq:publicMessageIndexingWorkArea:PUM8a65ce95-9d35-4941-928c-dd5965e8b29b 
2011-02-23 07:24:43,710 INFO [org.apache.cassandra.config.DatabaseDescriptor] - DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap 
2011-02-23 07:24:43,972 INFO [org.apache.cassandra.io.SSTableReader] - Opening /opt/cassandra/storage/queue/data/Panama/Queue-2527-Data.db 
{ 
""waq:publicMessageIndexingWorkArea:PUM8a65ce95-9d35-4941-928c-dd5965e8b29b"": [] 
} 
(root@aps4):/opt/cassandra/storage/queue/data/Panama Wed Feb 23 07:24:44am 
===>

The steps that I took to reproduce it were:
Create a keyspace, column family, and a key
Delete the key on Node 1 using the cli (del cf['key'];)
Flush 
Repair on a cluster with more than 1 node
Wait GCSeconds 
Compact
And the empty row would appear on Node 2

However, when I was able to get rid of the empty rows, I was following these steps on a single machine: 
Create a keyspace, column family, and a key
Delete the key
Flush
Sample write (writing to some temporary key)
Deleting the attribute to that temporary key (not the entire key)
Flush
Compact

or these steps:
Create a keyspace, column family, and a key
Delete the key
Flush 
Wait GCseconds
Compact

",slebresne,j.casares,Low,Resolved,Fixed,07/Mar/11 17:42,16/Apr/19 09:33
Bug,CASSANDRA-2282,12500665,ReadCallback AssertionError: resolver.getMessageCount() <= endpoints.size(),"In a three node cluster with RF=2, when trying to page through all rows with get_range_slices() at CL.ONE, I get timeouts on the client side.  Looking at the Cassandra logs, all of the nodes show the following AssertionError repeatedly:

{noformat}
ERROR [RequestResponseStage:2] 2011-03-07 19:10:27,527 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.AssertionError
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
ERROR [RequestResponseStage:2] 2011-03-07 19:10:27,529 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[RequestResponseStage:2,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
        at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{noformat}

The nodes are all running 0.7.3.  The cluster was at size 3 before any data was inserted, and everything else appears perfectly healthy.",jbellis,thobbs,Normal,Resolved,Fixed,07/Mar/11 19:34,16/Apr/19 09:33
Bug,CASSANDRA-2283,12500677,Streaming Old Format Data Fails in 0.7.3 after upgrade from 0.6.8,"After successfully upgrading a 0.6.8 ring to 0.7.3, we needed to bootstrap in a new node relatively quickly. When starting the new node with an assigned token in auto bootstrap mode, we see the following exceptions on the new node:

INFO [main] 2011-03-07 10:37:32,671 StorageService.java (line 505) Joining: sleeping 30000 ms for pending range setup
 INFO [main] 2011-03-07 10:38:02,679 StorageService.java (line 505) Bootstrapping
 INFO [HintedHandoff:1] 2011-03-07 10:38:02,899 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.211.14.200
 INFO [HintedHandoff:1] 2011-03-07 10:38:02,900 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.211.14.200
 INFO [CompactionExecutor:1] 2011-03-07 10:38:04,924 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuff-f-1
 INFO [CompactionExecutor:1] 2011-03-07 10:38:05,390 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuff-f-2
 INFO [CompactionExecutor:1] 2011-03-07 10:38:05,768 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-1
 INFO [CompactionExecutor:1] 2011-03-07 10:38:06,389 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-2
 INFO [CompactionExecutor:1] 2011-03-07 10:38:06,581 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-3
ERROR [CompactionExecutor:1] 2011-03-07 10:38:07,056 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [CompactionExecutor:1] 2011-03-07 10:38:08,480 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid-f-5
 INFO [CompactionExecutor:1] 2011-03-07 10:38:08,582 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid_reg_idx-f-1
ERROR [CompactionExecutor:1] 2011-03-07 10:38:08,635 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [CompactionExecutor:1] 2011-03-07 10:38:08,666 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.io.EOFException
        at org.apache.cassandra.io.sstable.IndexHelper.skipIndex(IndexHelper.java:65)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:303)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:923)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:916)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO [CompactionExecutor:1] 2011-03-07 10:38:08,855 SSTableReader.java (line 154) Opening /mnt/services/cassandra/var/data/0.7.3/data/Stuff/stuffid_reg_idx-f-4

Two attempts to bootstrap in the new node both exhibited this behavior. On the node owning the tokens being migrated, stream activity is visible but doesn't update any progress I think due to the issues on the receiving host.







Lastly, just case it's relevant, we had an EC2 node die underneath us during the upgrade so not all nodes were drained. This didn't affect the upgrade but I wanted to note it her to be thorough.",jbellis,eonnen,Normal,Resolved,Fixed,07/Mar/11 21:00,16/Apr/19 09:33
Bug,CASSANDRA-2285,12500768,Reading an empty commit log throw an exception,"Start a one node cluster, shutdown within 10 seconds but after the node is started and the location infos has been flushed. Restart node, you'll get a 'EOFException: unable to seek past the end of the file in read-only mode.'",slebresne,slebresne,Low,Resolved,Fixed,08/Mar/11 14:47,16/Apr/19 09:33
Bug,CASSANDRA-2286,12500783,range queries don't respect snitch for local replicas,,jbellis,jbellis,Low,Resolved,Fixed,08/Mar/11 16:37,16/Apr/19 09:33
Bug,CASSANDRA-2289,12500798,Replicate on write NPE for empty row,Replicate on write will throw a NPE for the first write to a row.,stuhood,stuhood,Low,Resolved,Fixed,08/Mar/11 19:00,16/Apr/19 09:33
Bug,CASSANDRA-2290,12500799,Repair hangs if one of the neighbor is dead,"Repair don't cope well with dead/dying neighbors. There is 2 problems:

  # Repair don't check if a node is dead before sending a TreeRequest; this is easily fixable.
  # If a neighbor dies mid-repair, the repair will also hang forever.

The second point is not easy to deal with. The best approach is probably CASSANDRA-1740 however. That is, if we add a way to query the state of a repair, and that this query correctly check all neighbors and also add a way to cancel a repair, this would probably be enough.
",slebresne,slebresne,Low,Resolved,Fixed,08/Mar/11 19:09,16/Apr/19 09:33
Bug,CASSANDRA-2291,12500802,Ant build script in contrib/stress fails.,"Build fails in contrib/stress with following message:
/mnt/hgfs/workspace/cassandra-source/contrib/stress/build.xml:38: javac doesn't support the nested ""path"" element.

Fix:
Needs to move <path refid=""cassandra.classes"" /> inside path element.

SVN copy:
            <path refid=""cassandra.classes"" />
            <classpath>
                <path>
                    <fileset dir=""${cassandra.lib}"">
                        <include name=""**/*.jar"" />
                    </fileset>
                </path>
            </classpath>

To be changed to:
            <classpath>
                <path>
                    <path refid=""cassandra.classes"" />
                    <fileset dir=""${cassandra.lib}"">
                        <include name=""**/*.jar"" />
                    </fileset>
                </path>
            </classpath>",jbellis,jahangir,Low,Resolved,Fixed,08/Mar/11 19:12,16/Apr/19 09:33
Bug,CASSANDRA-2292,12500808,Connections are not reset if a node is restarted but we had not marked it down,,slebresne,slebresne,Normal,Resolved,Fixed,08/Mar/11 19:49,16/Apr/19 09:33
Bug,CASSANDRA-2296,12500831,"Scrub resulting in ""bloom filter claims to be longer than entire row size"" error","Doing a scrub on a node which I upgraded from 0.7.1 (was previously 0.6.8) to 0.7.3. Getting this error multiple times:
{code}
 WARN [CompactionExecutor:1] 2011-03-08 18:33:52,513 CompactionManager.java (line 625) Row is unreadable; skipping to next
 WARN [CompactionExecutor:1] 2011-03-08 18:33:52,514 CompactionManager.java (line 599) Non-fatal error reading row (stacktrace follows)
java.io.IOError: java.io.EOFException: bloom filter claims to be longer than entire row size
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:117)
        at org.apache.cassandra.db.CompactionManager.doScrub(CompactionManager.java:590)
        at org.apache.cassandra.db.CompactionManager.access$600(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$3.call(CompactionManager.java:195)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.EOFException: bloom filter claims to be longer than entire row size
        at org.apache.cassandra.io.sstable.IndexHelper.defreezeBloomFilter(IndexHelper.java:113)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:87)
        ... 8 more
 WARN [CompactionExecutor:1] 2011-03-08 18:33:52,515 CompactionManager.java (line 625) Row is unreadable; skipping to next
 INFO [CompactionExecutor:1] 2011-03-08 18:33:53,777 CompactionManager.java (line 637) Scrub of SSTableReader(path='/cassandra/data/reddit/Hide-f-671-Data.db') complete: 254709 rows in new sstable
 WARN [CompactionExecutor:1] 2011-03-08 18:33:53,777 CompactionManager.java (line 639) Unable to recover 1630 that were skipped.  You can attempt manual recovery from the pre-scrub snapshot.  You can also run nodetool repair to transfer the data from a healthy replica, if any
{code}",jbellis,alienth,Normal,Resolved,Fixed,09/Mar/11 01:40,16/Apr/19 09:33
Bug,CASSANDRA-2297,12500876,UnsupportedOperationException: Overflow in bytesPastMark(..),"I hit the following exception on a row that was more than 60GB.  
The row has column families of super column type.

This problem is discussed by the following thread.  
http://www.mail-archive.com/dev@cassandra.apache.org/msg01881.html

{code}
ERROR [HintedHandoff:1] 2011-02-26 18:49:35,708 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.UnsupportedOperationException: Overflow: 2147484294
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.UnsupportedOperationException: Overflow: 2147484294
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.bytesPastMark(BufferedRandomAccessFile.java:477)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.getNextBlock(IndexedSliceReader.java:179)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:120)
        at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:1)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:68)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:118)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:142)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1290)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1167)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1095)
        at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:138)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:313)
        at org.apache.cassandra.db.HintedHandOffManager.access$1(HintedHandOffManager.java:262)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:391)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
{code}",jbellis,muga_nishizawa,Normal,Resolved,Fixed,09/Mar/11 11:41,16/Apr/19 09:33
Bug,CASSANDRA-2301,12500936,OOM on repair with many inconsistent ranges,"Repair can OOM when lots of ranges are inconsistent, causing many sstables to be streamed.

I replicated the error by making 1264 3MB sstables on one node, added a second node, changed the replication factor to 2, and ran a repair.

Looking at the heap dump of the original failure, there were 2.4GB of FutureTasks, each taking up 8MB of space. I tracked down the BufferedRandomAccessFile and made sure that it was cleared every time it was closed inside of src/java/org/apache/cassandra/io/sstable/SSTableWriter.java.

Attached is the patch I used which stopped the error when I was trying to replicate it.",j.casares,j.casares,Normal,Resolved,Fixed,09/Mar/11 19:46,16/Apr/19 09:33
Bug,CASSANDRA-2304,12500973,"sstable2json dies with ""Too many open files"", regardless of ulimit","Running sstable2json on the attached sstable eventually results in the following:

{code}
Exception in thread ""main"" java.io.IOError: java.io.FileNotFoundException: /var/lib/cassandra/data/reddit/CommentSortsCache-f-9764-Data.db (Too many open files)
        at org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:68)
        at org.apache.cassandra.io.sstable.SSTableReader.getFileDataInput(SSTableReader.java:567)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.<init>(SSTableSliceIterator.java:49)
        at org.apache.cassandra.db.filter.SliceQueryFilter.getSSTableColumnIterator(SliceQueryFilter.java:68)
        at org.apache.cassandra.db.filter.QueryFilter.getSSTableColumnIterator(QueryFilter.java:80)
        at org.apache.cassandra.tools.SSTableExport.serializeRow(SSTableExport.java:187)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:355)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:377)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:390)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:448)
Caused by: java.io.FileNotFoundException: /var/lib/cassandra/data/reddit/CommentSortsCache-f-9764-Data.db (Too many open files)
        at java.io.RandomAccessFile.open(Native Method)
        at java.io.RandomAccessFile.<init>(RandomAccessFile.java:233)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:111)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:106)
        at org.apache.cassandra.io.util.BufferedRandomAccessFile.<init>(BufferedRandomAccessFile.java:91)
        at org.apache.cassandra.io.util.BufferedSegmentedFile.getSegment(BufferedSegmentedFile.java:62)
{code}

Set my ulimit -n to 60000 and got the same result. Leaking file descriptors?",jbellis,alienth,Low,Resolved,Fixed,10/Mar/11 00:54,16/Apr/19 09:33
Bug,CASSANDRA-2305,12500984,Tombstoned rows not purged from cache after gcgraceseconds,"From email to list:

I was wondering if this is the expected behavior of deletes (0.7.0). Let's say I have a 1-node cluster with a single CF which has gc_grace_seconds = 0. The following sequence of operations happens (in the given order):

insert row X with timestamp T
delete row X with timestamp T+1
force flush + compaction
insert row X with timestamp T

My understanding is that the tombstone created by the delete (and row X) will disappear with the flush + compaction which means the last insertion should show up. My experimentation, however, suggests otherwise (the last insertion does not show up).

I believe I have traced this to the fact that the markedForDeleteAt field on the ColumnFamily does not get reset after a compaction (after gc_grace_seconds has passed); is this desirable? I think it introduces an inconsistency in how tombstoned columns work versus tombstoned CFs. Thanks.",slebresne,paladin8,Low,Resolved,Fixed,10/Mar/11 04:39,16/Apr/19 09:33
Bug,CASSANDRA-2310,12501053,"CassandraStorage for pig checks for environment variable on mappers/reducers, but it should only need to be set on the machine launching pig.","Only error out if necessary pig settings have not previously been set in job config. CassandraStorage checks for environment variables on mappers/reducers, but it should only need to be set on the machine launching the pig jobs.",,eldondev,Low,Resolved,Fixed,10/Mar/11 19:14,16/Apr/19 09:33
Bug,CASSANDRA-2312,12501067,Stress.java columns are bigger than advertised,"Converting from bytes to hex makes the columns 4x larger than they should be.  (2x for conversion to hex, then another 2x for converting to UTF-16 which is the default String encoding.)
",jbellis,jbellis,Low,Resolved,Fixed,10/Mar/11 21:33,16/Apr/19 09:33
Bug,CASSANDRA-2313,12501164,CommutativeRowIndexer always read full row in memory,"CommutativeRowIndexer use CFSerializer.deserializeColumns() that read the full row in memory. We should use PreCompactedRow/LazilyCompactedRow instead to avoid this on huge row.

As an added benefit, using PreCompactedRow will avoid a current seek back to write the row size.",slebresne,slebresne,Normal,Resolved,Fixed,11/Mar/11 16:37,16/Apr/19 09:33
Bug,CASSANDRA-2316,12501231,NoSuchElement exception on node which is streaming a repair,"Running latest SVN snapshot of 0.7.

When I ran a repair on a node, that node's neighbor threw the following exception. Let me know what other info could be helpful.

{code}
 INFO 23:43:44,358 Streaming to /10.251.166.15
ERROR 23:50:21,321 Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.util.NoSuchElementException
        at com.google.common.collect.AbstractIterator.next(AbstractIterator.java:146)
        at org.apache.cassandra.service.AntiEntropyService$Validator.add(AntiEntropyService.java:366)
        at org.apache.cassandra.db.CompactionManager.doValidationCompaction(CompactionManager.java:825)
        at org.apache.cassandra.db.CompactionManager.access$800(CompactionManager.java:56)
        at org.apache.cassandra.db.CompactionManager$6.call(CompactionManager.java:358)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:334)
        at java.util.concurrent.FutureTask.run(FutureTask.java:166)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
{code}",jbellis,alienth,Normal,Resolved,Fixed,12/Mar/11 07:12,16/Apr/19 09:33
Bug,CASSANDRA-2317,12501241,Column family deletion time is not always reseted after gc_grace,"Follow up of CASSANDRA-2305.
Reproducible (thanks to Jeffrey Wang) by: 

Create a CF with gc_grace_seconds = 0 and no row cache.
Insert row X, col A with timestamp 0.
Insert row X, col B with timestamp 2.
Remove row X with timestamp 1 (expect col A to disappear, col B to stay).
Wait 1 second.
Force flush and compaction.
Insert row X, col A with timestamp 0.
Read row X, col A (see nothing).",slebresne,slebresne,Low,Resolved,Fixed,12/Mar/11 11:19,16/Apr/19 09:33
Bug,CASSANDRA-2320,12501288,Dropping an index leaves index in Built state in system table,,jbellis,jbellis,Low,Resolved,Fixed,13/Mar/11 04:43,16/Apr/19 09:33
Bug,CASSANDRA-2321,12501289,disallow to querying a counter CF with non-counter operation,"CounterColumnType.getString() returns hexString.

{code}
public String getString(ByteBuffer bytes)
{ 
       return ByteBufferUtil.bytesToHex(bytes);
}
{code}
and python stress.py reader returns


[ColumnOrSuperColumn(column=None, super_column=SuperColumn(name='19', columns=[Column(timestamp=1299984960277, name='56', value='\x7f\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00\x08\x00\x00\x00\x00\x00\x00\x00,', ttl=None), Column(timestamp=1299985019923, name='57', value='\x7f\x00\x00\x01\x00\x00\x00\x00\x00\x00\x00;\x00\x00\x00\x00\x00\x00\x08\xfd', ttl=None))]",slebresne,mubarak.seyed,Low,Resolved,Fixed,13/Mar/11 05:27,16/Apr/19 09:33
Bug,CASSANDRA-2323,12501378,stress.java should not allow arbitrary arguments,"This doesn't seem like a big deal, until you accidentally insert a space between a dash and it's flag, and it's at the point where the line wraps in your terminal.",xedin,brandon.williams,Low,Resolved,Fixed,14/Mar/11 17:01,16/Apr/19 09:33
Bug,CASSANDRA-2324,12501390,Repair transfers more data than necessary,"To repro: 3 node cluster, stress.java 1M rows with -x KEYS and -l 2.  The index is enough to make some mutations drop (about 20-30k total in my tests).  Repair afterwards will repair a large amount of ranges the first time.  However, each subsequent run will repair the same set of small ranges every time.  INDEXED_RANGE_SLICE in stress never fully works.  Counting rows with sstablekeys shows there are 2M rows total as expected, however when trying to count the indexed keys, I get exceptions like:
{noformat}
Exception in thread ""main"" java.io.IOException: Key out of order! DecoratedKey(101571366040797913119296586470838356016, 0707ab782c5b5029d28a5e6d508ef72f0222528b5e28da3b7787492679dc51b96f868e0746073e54bc173be927049d0f51e25a6a95b3268213b8969abf40cea7d7) > DecoratedKey(12639574763031545147067490818595764132, 0bc414be3093348a2ad389ed28f18f0cc9a044b2e98587848a0d289dae13ed0ad479c74654900eeffc6236)
        at org.apache.cassandra.tools.SSTableExport.enumeratekeys(SSTableExport.java:206)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:388)
{noformat}",slebresne,brandon.williams,Normal,Resolved,Fixed,14/Mar/11 19:43,16/Apr/19 09:33
Bug,CASSANDRA-2326,12501404,stress.java indexed range slicing is broken,"I probably broke it when I fixed the build that CASSANDRA-2312 broke.  Now it compiles, but never works.",xedin,brandon.williams,Low,Resolved,Fixed,14/Mar/11 21:26,16/Apr/19 09:33
Bug,CASSANDRA-2328,12501429,Index predicate values used in get_indexed_slice() are not validated,"If a client makes a get_indexed_slice() request with malformed predicate values we get an assertion failing rather than InvalidRequestException.

{noformat}
ERROR 14:47:56,842 Error in ThreadPoolExecutor
java.lang.RuntimeException: java.lang.IndexOutOfBoundsException: 6
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVer
bHandler.java:51)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.
java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.IndexOutOfBoundsException: 6
        at java.nio.HeapByteBuffer.get(HeapByteBuffer.java:121)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compareTimestampBytes(Ti
meUUIDType.java:56)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.jav
a:45)
        at org.apache.cassandra.db.marshal.TimeUUIDType.compare(TimeUUIDType.jav
a:29)
        at org.apache.cassandra.db.ColumnFamilyStore.satisfies(ColumnFamilyStore
.java:1608)
        at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java
:1552)
        at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVer
bHandler.java:42)
        ... 4 more
{noformat}",amorton,amorton,Low,Resolved,Fixed,15/Mar/11 06:53,16/Apr/19 09:33
Bug,CASSANDRA-2333,12501491,Clean up thread pool and queue sizes,"Most of Cassandra assumes that ThreadPoolExecutor handles tasks by starting with Core threads, adding threads up to Max as tasks arrive, then queuing any additional.  This is not correct:

{noformat}
    If fewer than corePoolSize threads are running, the Executor always prefers adding a new thread rather than queuing.
    If corePoolSize or more threads are running, the Executor always prefers queuing a request rather than adding a new thread.
    If a request cannot be queued, a new thread is created unless this would exceed maximumPoolSize, in which case, the task will be rejected.
{noformat}

CASSANDRA-2178 fixed this in one place but made it worse by default since most people run with a single data dir, meaning as soon as you have multiple CFs flushing (or a single one with indexes) then you will start blocking writes.",jbellis,jbellis,Low,Resolved,Fixed,15/Mar/11 18:11,16/Apr/19 09:33
Bug,CASSANDRA-2337,12501536,Windows: CliTest broken because of /r/n,Somebody thought that windows should emulate a telex machine and we ended up with /r/n.,bcoverston,bcoverston,Normal,Resolved,Fixed,16/Mar/11 01:53,16/Apr/19 09:33
Bug,CASSANDRA-2347,12501647,index scan uses incorrect comparator on non-indexed expressions,"When multiple index expressions are specified, the column name comparator is used when evaluating secondary (non-indexed) expressions after an indexed expression match.",rjtg,jbellis,Normal,Resolved,Fixed,17/Mar/11 03:35,16/Apr/19 09:33
Bug,CASSANDRA-2349,12501688,Expring columns can expire between the two phase of LazilyCompactedRow.,"LazilyCompactedRow reads the columns to compact twice. First to create the index, bloom filter and calculate the data size, and then another phase to actually write the columns. But a column can expire between those two phase, which will result in a bad data size in the sstable (and a possibly corrupted row index).",slebresne,slebresne,Urgent,Resolved,Fixed,17/Mar/11 14:37,16/Apr/19 09:33
Bug,CASSANDRA-2350,12501718,Races between schema changes and StorageService operations,"I only tested this on 0.7.0, but it judging by the 0.7.3 code (latest I've looked at) the same thing should happen.

The case in particular that I ran into is this: I force a compaction for all CFs in a keyspace, and while the compaction is happening I add another CF to the keyspace. I get the following exception because the underlying set of CFs has changed while being iterated over.

{noformat}
java.util.ConcurrentModificationException
        at java.util.HashMap$HashIterator.nextEntry(Unknown Source)
        at java.util.HashMap$ValueIterator.next(Unknown Source)
        at java.util.Collections$UnmodifiableCollection$1.next(Unknown Source)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1140)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(Unknown Source)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(Unknown Source)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(Unknown Source)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(Unknown Source)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(Unknown Source)
        at sun.reflect.GeneratedMethodAccessor84.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
        at java.lang.reflect.Method.invoke(Unknown Source)
        at sun.rmi.server.UnicastServerRef.dispatch(Unknown Source)
        at sun.rmi.transport.Transport$1.run(Unknown Source)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(Unknown Source)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source) 
{noformat}

The problem is a little more fundamental than that, though, as I believe any schema change of CFs in the keyspace during one of these operations (e.g. flush, compaction, etc) has the potential to cause a race. I'm not sure what would happen if the set of CFs to compact was acquired and one of them was dropped before it had been compacted.",jbellis,paladin8,Low,Resolved,Fixed,17/Mar/11 18:30,16/Apr/19 09:33
Bug,CASSANDRA-2351,12501730,Null CF comments should be allowed,"Prior to 1906, cassandra tolerated null CF comments.  They were converted to empty quotes when the CFM was created.

",jhermes,gdusbabek,Low,Resolved,Fixed,17/Mar/11 20:29,16/Apr/19 09:33
Bug,CASSANDRA-2352,12501733,zero-length strings should result in zero-length ByteBuffers,"The {{o.a.c.db.marshal.AbstractType.fromString()}} methods should return an empty {{ByteBuffer}} when passed a zero-length string, (empty bytes do {{validate()}} properly).",urandom,urandom,Normal,Resolved,Fixed,17/Mar/11 20:55,16/Apr/19 09:33
Bug,CASSANDRA-2358,12501895,CLI doesn't handle inserting negative integers,"The CLI raises a syntax error when trying to insert negative integers:

{noformat}
[default@Keyspace1] set StandardInteger['key'][-12] = 'val';
Syntax error at position 28: mismatched character '1' expecting '-'
{noformat}",xedin,thobbs,Low,Resolved,Fixed,20/Mar/11 06:36,16/Apr/19 09:33
Bug,CASSANDRA-2360,12501914,help in schema-sample uses wrong file name,"As described in CASSANDRA-2007 

Wasn't sure about re-opening a resolved issue and wanted to make sure it was not lost. 
",amorton,amorton,Low,Resolved,Fixed,21/Mar/11 02:42,16/Apr/19 09:33
Bug,CASSANDRA-2365,12502073,ByteBufferUtil.read(byte[]) returns 0 when the end of the stream has been reached.,"read(byte[], int, int) doesn't return -1 when the end of the stream is reached. Instead, it returns 0. 

len = Math.min(len, copy.remaining());
copy.get(bytes, off, len);

return len;

copy.remaining() returns 0 when the end of the stream is reached. ",jbellis,julie.zhang10,Low,Resolved,Fixed,22/Mar/11 18:23,16/Apr/19 09:33
Bug,CASSANDRA-2367,12502184,Cleanup conversions between bytes and strings,"There is a bit of inconsistency in our conversions between ByteBuffers and Strings.
For instance, ByteBufferUtil.string() uses as a default the java default charset, while ByteBufferUtil.bytes(String) assumes UTF8. Moreover, a number of places in the code don't use those functions and uses getBytes() directly. There again, we often encode with the default charset but decode in UTF8 or the contrary.

Using the default charset is probably a bad idea anyway, since this depends on the actual system the node is running on and could lead to a stupid bug when running in heterogeneous systems.

This ticket proposes to always assume UTF8 all over the place (and tries to use the ByteBufferUtil as much as possible to help with that).",slebresne,slebresne,Low,Resolved,Fixed,23/Mar/11 14:22,16/Apr/19 09:33
Bug,CASSANDRA-2370,12502220,unstable repo has disappeared from http://www.apache.org/dist/cassandra/debian/dists/,,urandom,brandon.williams,Normal,Resolved,Fixed,23/Mar/11 19:39,16/Apr/19 09:33
Bug,CASSANDRA-2371,12502232,Removed/Dead Node keeps reappearing,"The removetoken option does not seem to work. The original node 10.240.50.63 comes back into the ring, even after the EC2 instance is no longer in existence. Originally I tried to add a new node 10.214.103.224 with the same token, but there were some complications with that. I have pasted below all the INFO log entries found with greping the system log files.

Seems to be a similar issue seen with http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/Ghost-node-showing-up-in-the-ring-td6198180.html 

INFO [GossipStage:1] 2011-03-16 00:54:31,590 StorageService.java (line 745) Nodes /10.214.103.224 and /10.240.50.63 have the same token 95704415696513900000000000000000000000.  /10.214.103.224 is the new owner
 INFO [GossipStage:1] 2011-03-16 17:26:51,083 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:27:24,767 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:29:30,191 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:31:35,609 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-19 17:33:39,440 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.214.103.224
 INFO [GossipStage:1] 2011-03-23 17:22:55,520 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.240.50.63


 INFO [GossipStage:1] 2011-03-10 03:52:37,299 Gossiper.java (line 608) Node /10.240.50.63 is now part of the cluster
 INFO [GossipStage:1] 2011-03-10 03:52:37,545 Gossiper.java (line 600) InetAddress /10.240.50.63 is now UP
 INFO [HintedHandoff:1] 2011-03-10 03:53:36,168 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.240.50.63
 INFO [HintedHandoff:1] 2011-03-10 03:53:36,169 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.240.50.63
 INFO [GossipStage:1] 2011-03-15 23:23:43,770 Gossiper.java (line 623) Node /10.240.50.63 has restarted, now UP again
 INFO [GossipStage:1] 2011-03-15 23:23:43,771 StorageService.java (line 726) Node /10.240.50.63 state jump to normal
 INFO [HintedHandoff:1] 2011-03-15 23:28:48,957 HintedHandOffManager.java (line 304) Started hinted handoff for endpoint /10.240.50.63
 INFO [HintedHandoff:1] 2011-03-15 23:28:48,958 HintedHandOffManager.java (line 360) Finished hinted handoff of 0 rows to endpoint /10.240.50.63
 INFO [ScheduledTasks:1] 2011-03-15 23:37:25,071 Gossiper.java (line 226) InetAddress /10.240.50.63 is now dead.
 INFO [GossipStage:1] 2011-03-16 00:54:31,590 StorageService.java (line 745) Nodes /10.214.103.224 and /10.240.50.63 have the same token 95704415696513900000000000000000000000.  /10.214.103.224 is the new owner
 WARN [GossipStage:1] 2011-03-16 00:54:31,590 TokenMetadata.java (line 115) Token 95704415696513900000000000000000000000 changing ownership from /10.240.50.63 to /10.214.103.224
 INFO [GossipStage:1] 2011-03-18 23:37:09,158 Gossiper.java (line 610) Node /10.240.50.63 is now part of the cluster
 INFO [GossipStage:1] 2011-03-21 23:37:10,421 Gossiper.java (line 610) Node /10.240.50.63 is now part of the cluster
 INFO [GossipStage:1] 2011-03-21 23:37:10,421 StorageService.java (line 726) Node /10.240.50.63 state jump to normal
 INFO [GossipStage:1] 2011-03-23 17:22:55,520 StorageService.java (line 865) Removing token 95704415696513900000000000000000000000 for /10.240.50.63
 INFO [ScheduledTasks:1] 2011-03-23 17:22:55,521 HintedHandOffManager.java (line 210) Deleting any stored hints for 10.240.50.63
",brandon.williams,acctech,Low,Resolved,Fixed,23/Mar/11 22:03,16/Apr/19 09:33
Bug,CASSANDRA-2376,12502254,Both name an index iterators cast block offset to int,This means that performing random access to the end of a large row will not work.,jbellis,jbellis,Low,Resolved,Fixed,24/Mar/11 04:55,16/Apr/19 09:33
Bug,CASSANDRA-2377,12502261,NPE During Repair In StreamReplyVerbHandler,"ERROR [MiscStage:4] 2011-03-24 02:45:05,172 DebuggableThreadPoolExecutor.java (line 103) Error in ThreadPoolExecutorjava.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:62)        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
ERROR [MiscStage:4] 2011-03-24 02:45:05,172 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[MiscStage:4,5,main]java.lang.NullPointerException
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:62)        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)",brandon.williams,bcoverston,Normal,Resolved,Fixed,24/Mar/11 06:57,16/Apr/19 09:33
Bug,CASSANDRA-2381,12502323,orphaned data files may be created during migration race,"We try to prevent creating orphans by locking Table.flusherLock in maybeSwitchMemtable and the Migration process, but since the actual writing is done asynchronously in Memtable.writeSortedContents there is a race window, where we acquire lock in maybeSwitch, we're not dropped so we queue the flush and release the lock, Migration does the drop, then Memtable writes itself out.",jbellis,jbellis,Low,Resolved,Fixed,24/Mar/11 20:55,16/Apr/19 09:33
Bug,CASSANDRA-2382,12502344,statistics component not fsynced,The statistics file is prone to getting lost during a hard reset since it is not fsynced like the other sstable components.,jbellis,jbellis,Low,Resolved,Fixed,25/Mar/11 03:28,16/Apr/19 09:33
Bug,CASSANDRA-2383,12502347,log4j unable to load properties file from classpath,"when cassandra home folder is placed inside a folder which has space characters in its name,
log4j settings are not properly loaded and warning messages are shown.",dallsopp,iecanfly,Low,Resolved,Fixed,25/Mar/11 04:07,16/Apr/19 09:33
Bug,CASSANDRA-2388,12502417,"ColumnFamilyRecordReader fails for a given split because a host is down, even if records could reasonably be read from other replica.",ColumnFamilyRecordReader only tries the first location for a given split. We should try multiple locations for a given split.,pauloricardomg,eldondev,Low,Resolved,Fixed,25/Mar/11 20:40,16/Apr/19 09:33
Bug,CASSANDRA-2390,12502469,MarshalException is thrown when cassandra-cli creates the example Keyspace specified by conf/schema-sample.txt,"Use the following steps to recreate the bug:

1. Checkout the source code from trunk. For my case, revision is 1085753.
2. Run ""ant"" to build cassandra.
3. Run ""bin/cassandra -f"" to start cassandra.
4. Run ""bin/cassandra-cli -host localhost --file conf/schema-sample.txt"".

Then there is the following message:
{quote}
... schemas agree across the cluster
Line 9 => org.apache.cassandra.db.marshal.MarshalException: cannot parse 'birthdate' as hex bytes
{quote}
The root cause is BytesType's fromString method. FBUtilities's hexToBytes method is invoked with ""birthdate"". NumberFormatException is thrown since ""birthdate"" is not a hex string.

{code:title=BytesType.java|borderStyle=solid}

    public ByteBuffer fromString(String source)
    {
        try
        {
            return ByteBuffer.wrap(FBUtilities.hexToBytes(source));
        }
        catch (NumberFormatException e)
        {
            throw new MarshalException(String.format(""cannot parse '%s' as hex bytes"", source), e);
        }
    }
{code}",jbellis,yaojingguo,Low,Resolved,Fixed,26/Mar/11 16:31,16/Apr/19 09:33
Bug,CASSANDRA-2401,12502672,"getColumnFamily() return null, which is not checked in ColumnFamilyStore.java scan() method, causing Timeout Exception in query","ColumnFamilyStore.java, line near 1680, ""ColumnFamily data = getColumnFamily(new QueryFilter(dk, path, firstFilter))"", the data is returned null, causing NULL exception in ""satisfies(data, clause, primary)"" which is not captured. The callback got timeout and return a Timeout exception to Hector.

The data is empty, as I traced, I have the the columns Count as 0 in removeDeletedCF(), which return the null there. (I am new and trying to understand the logics around still). Instead of crash to NULL, could we bypass the data?

About my test:
A stress-test program to add, modify and delete data to keyspace. I have 30 threads simulate concurrent users to perform the actions above, and do a query to all rows periodically. I have Column Family with rows (as File) and columns as index (e.g. userID, fileType).

No issue on the first day of test, and stopped for 3 days. I restart the test on 4th day, 1 of the users failed to query the files (timeout exception received). Most of the users are still okay with the query.
",jbellis,karshiang,Normal,Resolved,Fixed,29/Mar/11 07:15,16/Apr/19 09:33
Bug,CASSANDRA-2406,12502972,Secondary index and index expression problems,"When I iteratively get data with secondary index and index clause, result of data acquired by consistency level ""one"" is different from the one by consistency level ""quorum"".  The one by consistecy level ""one"" is correct result.  But the one by consistecy level ""quorum"" is incorrect and is dropped by Cassandra.  

You can reproduce the bug by executing attached programs.

- 1. Start Cassandra cluster.  It consists of 3 cassandra nodes and distributes data by ByteOrderedPartitioner.  Initial tokens of those nodes are [""31"", ""32"", ""33""].  
- 2. Create keyspace and column family, according to ""create_table.cli"",
- 3. Execute ""secondary_index_insertv2.py"", inserting a few hundred columns to cluster
- 4. Execute ""secondary_index_checkv2.py"" and get data with secondary index and index clause iteratively.  ""secondary_index_insertv2.py"" and ""secondary_index_checkv2.py"" require pycassa.

You will be able to execute  4th ""secondary_index_checkv2.py"" script with following option so that 
you get data with consistency level ""one"".  

% python ""secondary_index_checkv2.py"" -one

On the other hand, to acquire data with consistency level ""quorum"", you will need to use following option.  

% python ""secondary_index_checkv2.py"" -quorum

You can check that result of data acquired by consistency level ""one"" is different from one by consistency level ""quorum"".  ",xedin,muga_nishizawa,Normal,Resolved,Fixed,31/Mar/11 03:18,16/Apr/19 09:33
Bug,CASSANDRA-2409,12503053,Add INSERT support to CQL,"There are two reasons to support INSERT:

- It helps new users feel comfortable (everyone's first statement will be to try to INSERT something, we should make that a positive experience instead of slapping them)
- Even though it is synonymous with update it is still useful in your code to have both to help communicate intent, similar to choosing good variable names

The only downside is explaining how INSERT isn't a ""true"" insert because it doesn't error out if the row already exists -- but we already have to explain that same concept for UPDATE; the cognitive load is extremely minor.",xedin,jbellis,Normal,Resolved,Fixed,31/Mar/11 18:13,16/Apr/19 09:33
Bug,CASSANDRA-2410,12503071,JDBC ResultSet does not honor column value typing for the CF and uses default validator for all column value types.,"Assume a CF declared in CQL as :
{code}
CREATE COLUMNFAMILY TestCF(KEY utf8 PRIMARY KEY,description utf8, anumber int)
  WITH comparator = ascii AND default_validation = long;
{code}

If the {{ResultSet}} is fetched thusly:


{code}
Statement stmt = con.createStatement();
ResultSet rs = stmt.executeQuery(query);

String description;
Integer anumber;

    while (rs.next())
    {
      description = rs.getString(1);
      System.out.print(""description : ""+ description);
      anumber = rs.getInt(2);
      System.out.print(""anumber     : ""+ anumber);
    }
{code}

It will immediately fail with a message of: 

{code}
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 16
	at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:66)
	at org.apache.cassandra.cql.jdbc.TypedColumn.<init>(TypedColumn.java:45)
	at org.apache.cassandra.cql.jdbc.ColumnDecoder.makeCol(ColumnDecoder.java:158)
	at org.apache.cassandra.cql.jdbc.CassandraResultSet.next(CassandraResultSet.java:1073)
	at da.access.testing.TestJDBC.selectAll(TestJDBC.java:83)
         ...
{code}


It appears that the {{makeCol}} method of {{ColumnDecoder.java}} chooses NOT to use the {{CfDef}} to look up the possible occurrence of a column? That's not right. Right? 

{code}
    /** constructs a typed column */
    public TypedColumn makeCol(String keyspace, String columnFamily, byte[] name, byte[] value)
    {
        CfDef cfDef = cfDefs.get(String.format(""%s.%s"", keyspace, columnFamily));
        AbstractType comparator = getComparator(keyspace, columnFamily, Specifier.Comparator, cfDef);
        AbstractType validator = getComparator(keyspace, columnFamily, Specifier.Validator, null);
        return new TypedColumn(comparator, name, validator, value);
    }
{code}



",gdusbabek,ardot,Low,Resolved,Fixed,31/Mar/11 20:55,16/Apr/19 09:33
Bug,CASSANDRA-2413,12503152,Reduce default memtable size,"I'm going to wimp out on targeting CASSANDRA-2006 for 0.7.5 so to mitigate OOMing by newcomers let's reduce the default memtable size -- what we have now predates indexes, which can dramatically increase memory requirements.",jbellis,jbellis,Low,Resolved,Fixed,01/Apr/11 16:51,16/Apr/19 09:33
Bug,CASSANDRA-2416,12503379,NullPointerException in CacheWriter.saveCache(),"I've seen NullPointerException of CacheWriter in our cluster (replication 3).

ERROR [CompactionExecutor:1] 2011-04-05 09:57:42,968 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithLength(ByteBufferUtil.java:275)
        at org.apache.cassandra.io.sstable.CacheWriter.saveCache(CacheWriter.java:84)
        at org.apache.cassandra.db.CompactionManager$10.runMayThrow(CompactionManager.java:960)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more
",jbellis,skamio,Low,Resolved,Fixed,05/Apr/11 09:28,16/Apr/19 09:33
Bug,CASSANDRA-2417,12503397,convert mmap assertion to if/throw,"This will allow scrub to catch this:

{noformat}
java.lang.AssertionError: mmap segment underflow; remaining is 73936639 but 1970430821 requested

                at org.apache.cassandra.io.util.MappedFileDataInput.readBytes(MappedFileDataInput.java:119)

                at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:315)

                at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:272)

                at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:76)
{noformat}",jbellis,jbellis,Low,Resolved,Fixed,05/Apr/11 14:06,16/Apr/19 09:33
Bug,CASSANDRA-2418,12503444,default gc log settings overwrite previous log,"For those spoiled by nice rolling and appending syslogs log4js etc the JVM gc log can be jarring:

{noformat} 
# GC logging options -- uncomment to enable
# JVM_OPTS=""$JVM_OPTS -XX:+PrintGCDetails""
# JVM_OPTS=""$JVM_OPTS -XX:+PrintGCTimeStamps""
# JVM_OPTS=""$JVM_OPTS -XX:+PrintClassHistogram""
# JVM_OPTS=""$JVM_OPTS -XX:+PrintTenuringDistribution""
# JVM_OPTS=""$JVM_OPTS -XX:+PrintGCApplicationStoppedTime""
# JVM_OPTS=""$JVM_OPTS -Xloggc:/var/log/cassandra/gc.log""
{noformat} 

Will result in gc.log with days of data being overwritten on restart, which leads to sad faces.

The simplest change would be along these lines:
{noformat} 
GC_LOG_TS=`date +%s`
JVM_OPTS=""$JVM_OPTS -XX:+PrintGCDetails""
JVM_OPTS=""$JVM_OPTS -XX:+PrintGCTimeStamps""
JVM_OPTS=""$JVM_OPTS -XX:+PrintClassHistogram""
JVM_OPTS=""$JVM_OPTS -XX:+PrintTenuringDistribution""
JVM_OPTS=""$JVM_OPTS -XX:+PrintGCApplicationStoppedTime""
JVM_OPTS=""$JVM_OPTS -Xloggc:/var/log/cassandra/gc-$GC_LOG_TS.log""
{noformat} 

There are probably prettier approaches.",cburroughs,cburroughs,Low,Resolved,Fixed,05/Apr/11 20:16,16/Apr/19 09:33
Bug,CASSANDRA-2419,12503450,Risk of counter over-count when recovering commit log,"When a memtable was flush, there is a small delay before the commit log replay position gets updated. If the node fails during this delay, all the updates of this memtable will be replay during commit log recovery and will end-up being over-counts.",slebresne,slebresne,Normal,Resolved,Fixed,05/Apr/11 20:49,16/Apr/19 09:33
Bug,CASSANDRA-2420,12503457,row cache / streaming aren't aware of each other,"SSTableWriter.Builder.build() takes tables that resulted from streaming, repair, bootstrapping, et cetera and builds the indexes and bloom filters before ""adding"" it so the current node is aware of it.

However, if there is data present in the cache for a row that is also present in the streamed table the row cache can over shadow the data in the newly built table.  In other words, until the row in row cache is removed from the cache (e.g. because it's pushed out because of size, the node is restarted, the cache is manually cleared) the data in the newly built table will never be returned to clients.

The solution that seems most reasonable at this point is to have SSTableWriter.Builder.build() (or something below it) update the row cache if the row key in the table being built is also present in the cache.
",slebresne,mdennis,Low,Resolved,Fixed,05/Apr/11 21:26,16/Apr/19 09:33
Bug,CASSANDRA-2421,12503461,make sure we get the output schema when we output to cassandra from pig,see summary,jeromatron,jeromatron,Normal,Resolved,Fixed,05/Apr/11 21:44,16/Apr/19 09:33
Bug,CASSANDRA-2422,12503468,SSTables per read metric is incorrect,"The sstables per read metric is currently recording the count of live sstables, rather than the number that are being read.",stuhood,stuhood,Low,Resolved,Fixed,05/Apr/11 22:25,16/Apr/19 09:33
Bug,CASSANDRA-2428,12503560,Running cleanup on a node with join_ring=false removes all data,"If you need to bring up a node with join_ring=false for operator maintenance, and this node already has data, it will end up removing the data on the node. We noticed this when we were calling cleanup on a specific CF.
",slebresne,lenn0x,Urgent,Resolved,Fixed,06/Apr/11 21:58,16/Apr/19 09:33
Bug,CASSANDRA-2431,12503617,Try harder to close scanners after a failed compaction,Forked from 2191.,stuhood,stuhood,Normal,Resolved,Fixed,07/Apr/11 09:30,16/Apr/19 09:33
Bug,CASSANDRA-2432,12503630,add key_validation_class support to cli,Also update README to include utf8type key validator.,xedin,erny1803,Low,Resolved,Fixed,07/Apr/11 12:42,16/Apr/19 09:33
Bug,CASSANDRA-2433,12503646,Failed Streams Break Repair,"Running repair in cases where a stream fails we are seeing multiple problems.

1. Although retry is initiated and completes, the old stream doesn't seem to clean itself up and repair hangs.
2. The temp files are left behind and multiple failures can end up filling up the data partition.

These issues together are making repair very difficult for nearly everyone running repair on a non-trivial sized data set.

This issue is also being worked on w.r.t CASSANDRA-2088, however that was moved to 0.8 for a few reasons. This ticket is to fix the immediate issues that we are seeing in 0.7.",slebresne,bcoverston,Normal,Resolved,Fixed,07/Apr/11 15:40,16/Apr/19 09:33
Bug,CASSANDRA-2434,12503655,range movements can violate consistency,"My reading (a while ago) of the code indicates that there is no logic involved during bootstrapping that avoids consistency level violations. If I recall correctly it just grabs neighbors that are currently up.

There are at least two issues I have with this behavior:

* If I have a cluster where I have applications relying on QUORUM with RF=3, and bootstrapping complete based on only one node, I have just violated the supposedly guaranteed consistency semantics of the cluster.

* Nodes can flap up and down at any time, so even if a human takes care to look at which nodes are up and things about it carefully before bootstrapping, there's no guarantee.

A complication is that not only does it depend on use-case where this is an issue (if all you ever do you do at CL.ONE, it's fine); even in a cluster which is otherwise used for QUORUM operations you may wish to accept less-than-quorum nodes during bootstrap in various emergency situations.

A potential easy fix is to have bootstrap take an argument which is the number of hosts to bootstrap from, or to assume QUORUM if none is given.

(A related concern is bootstrapping across data centers. You may *want* to bootstrap to a local node and then do a repair to avoid sending loads of data across DC:s while still achieving consistency. Or even if you don't care about the consistency issues, I don't think there is currently a way to bootstrap from local nodes only.)

Thoughts?

",tjake,scode,Normal,Resolved,Fixed,07/Apr/11 17:23,16/Apr/19 09:33
Bug,CASSANDRA-2435,12503656,auto bootstrap happened on already bootstrapped nodes,"I believe the following was observed on 0.7.2. I meant to dig deeper, but never had the time, and now I want to at least file this even if I don't have extremely helpful information.

A piece of background is that we consciously made the decision to have the default configuration on nodes have auto_bootstrap set to true. The logic was that if one accidentally were to start a new node, we'd rather have it join with data than join *without* data and cause bogus read results in the cluster.

We executed this policy (by way of having the puppet managed config have auto_bootstrap set to true).

On one of our clusters with 5 nodes, we did some moves. All looked well; the moves completed. For unrelated reasons, we wanted to restart nodes after they had been moved. When we did, three of the 5, specifically those 3 that were *NOT* seed nodes, initiated a bootstrap procedure! Before the moves the cluster had been running for several days at least.

The logs indicated the automatic token selection, and they joined the ring under a new automatically selected token.

Presumably, this violated consistency but at the time there was no live traffic to the cluster and we didn't confirm (put traffic on it after repair+cleanup).

I did look a little bit at the code in light of this but didn't see anything obvious, so I don't really know what the likely culprit is.

A potential complication was that seed nodes were moved without using the correct procedure of de-seeding them first. This was clearly wrong, but it is not obvious to me that it would cause other nodes to incorrectly bootstrap since a node should *never* bootstrap more than once if the local system tables say it's been bootstrapped.
",jbellis,scode,Urgent,Resolved,Fixed,07/Apr/11 17:29,16/Apr/19 09:33
Bug,CASSANDRA-2437,12503667,ClusterTool throws exception for get_endpoints,"ByteBuffer is not serializable over JMX.

Exception in thread ""main"" java.lang.reflect.UndeclaredThrowableException
	at $Proxy0.getNaturalEndpoints(Unknown Source)
	at org.apache.cassandra.tools.NodeProbe.getEndpoints(NodeProbe.java:446)
	at org.apache.cassandra.tools.ClusterCmd.printEndpoints(ClusterCmd.java:146)
	at org.apache.cassandra.tools.ClusterCmd.main(ClusterCmd.java:240)
Caused by: java.io.NotSerializableException: java.nio.HeapByteBuffer
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1156)
	at java.io.ObjectOutputStream.writeArray(ObjectOutputStream.java:1338)
	at java.io.ObjectOutputStream.writeObject0(ObjectOutputStream.java:1146)
	at java.io.ObjectOutputStream.writeObject(ObjectOutputStream.java:326)
	at java.rmi.MarshalledObject.<init>(MarshalledObject.java:101)
	at javax.management.remote.rmi.RMIConnector$RemoteMBeanServerConnection.invoke(RMIConnector.java:990)
	at javax.management.MBeanServerInvocationHandler.invoke(MBeanServerInvocationHandler.java:288)
	... 4 more",lenn0x,lenn0x,Normal,Resolved,Fixed,07/Apr/11 19:21,16/Apr/19 09:33
Bug,CASSANDRA-2441,12503787,Cassandra crashes with segmentation fault on Debian 5.0 and Ubuntu 10.10,"Last working commit is c8d1984bf17cab58f40069e522d074c7b0077bc1 (merge from 0.7), branch: trunk.

What I did is cloned git://git.apache.org/cassandra.git and did git reset each commit with `ant clean && ant && ./bin/cassandra -f` until I got cassandra started",jbellis,xedin,Urgent,Resolved,Fixed,08/Apr/11 21:35,16/Apr/19 09:33
Bug,CASSANDRA-2444,12503814,Remove checkAllColumnFamilies on startup,"We've ran into many times where we do not want compaction to run right away against CFs when booting up a node. If the node needs to compact, it will do so at the first flush",lenn0x,lenn0x,Low,Resolved,Fixed,09/Apr/11 03:50,16/Apr/19 09:33
Bug,CASSANDRA-2445,12503843,Support SQL data types in CQL,"We should support SQL data types where possible:

(sql -> cassandra)
varint -> int
bigint -> long
varchar, text -> utf8
ascii -> ascii (not strictly correct -- sql defines collations for this -- but close enough for a 1.0)
uuid -> uuid (see CASSANDRA-2233)
bytea -> bytes

IMO the right thing to do is _only_ support the SQL types in CQL CREATE statements, because there is ambiguity otherwise (in particular with ""int"").
",jbellis,jbellis,Normal,Resolved,Fixed,09/Apr/11 21:40,16/Apr/19 09:33
Bug,CASSANDRA-2448,12503953,Remove loadbalance command,"With the update to how the move command works, the loadbalance command is even less useful that it was previously.  The loadbalance command now calculates the token it is going to move to before it leaves which means it isn't considering the load it is giving away. Given that, I think we should just remove the loadbalance command entirely. Anyone who wants to do an old style loadbalance can just do decommission then bootstrap.

This is a minor change, and honestly I think it might count as a 'bug' so I think we should squeeze it into 0.8, post-freeze. ",nickmbailey,nickmbailey,Low,Resolved,Fixed,11/Apr/11 17:00,16/Apr/19 09:33
Bug,CASSANDRA-2450,12503968,incompatibility w/ 0.7 schemas,"If you create a SimpleStrategy keyspace under 0.7, then switch to 0.8, you will get this error on startup:

{noformat}
ERROR 14:31:41,725 Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:277)
	at org.apache.cassandra.db.Table.open(Table.java:109)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:75)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:327)
	at org.apache.cassandra.db.Table.<init>(Table.java:273)
	... 4 more
{noformat}",jbellis,jbellis,Normal,Resolved,Fixed,11/Apr/11 19:32,16/Apr/19 09:33
Bug,CASSANDRA-2454,12503982,Possible deadlock for counter mutations,"{{StorageProxy.applyCounterMutation}} is executed on the mutation stage, but it also submits tasks to the mutation stage, and then blocks for them. If there are more than a few concurrent mutations, this can lead to deadlock.",kelvin,stuhood,Normal,Resolved,Fixed,11/Apr/11 21:38,16/Apr/19 09:33
Bug,CASSANDRA-2457,12504037,Batch_mutate is broken for counters,"CASSANDRA-2384 allowed for batch_mutate to take counter and non counter operation, but the code was not updated correctly to handle that case. As it is, the code will use the first mutation in the batch list to decide whether to apply the write code path of counter or not, and will thus break if those are mixed.",slebresne,slebresne,Normal,Resolved,Fixed,12/Apr/11 11:29,16/Apr/19 09:33
Bug,CASSANDRA-2458,12504040,cli divides read repair chance by 100,cli incorrectly divides the read_repair chance by 100 when creating / updating CF's,amorton,amorton,Low,Resolved,Fixed,12/Apr/11 11:40,16/Apr/19 09:33
Bug,CASSANDRA-2461,12504066,LongCompactionSpeedTest fails,"ant long-test -Dtest.name=LongCompactionSpeedTest fails.

There are several errors. Here is the first:

{noformat}
    [junit] java.lang.IllegalArgumentException
    [junit] 	at java.nio.ByteBuffer.allocate(ByteBuffer.java:311)
    [junit] 	at org.apache.cassandra.db.context.CounterContext.clearAllDelta(CounterContext.java:444)
    [junit] 	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:100)
    [junit] 	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:36)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:158)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableIdentityIterator.next(SSTableIdentityIterator.java:41)
    [junit] 	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:284)
    [junit] 	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
    [junit] 	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
    [junit] 	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
    [junit] 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    [junit] 	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    [junit] 	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
    [junit] 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    [junit] 	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    [junit] 	at org.apache.cassandra.db.ColumnIndexer.serializeInternal(ColumnIndexer.java:76)
    [junit] 	at org.apache.cassandra.db.ColumnIndexer.serialize(ColumnIndexer.java:50)
    [junit] 	at org.apache.cassandra.io.LazilyCompactedRow.<init>(LazilyCompactedRow.java:87)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableWriter$CommutativeRowIndexer.doIndexing(SSTableWriter.java:462)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
    [junit] 	at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:317)
    [junit] 	at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1089)
    [junit] 	at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1080)
{noformat}",slebresne,jbellis,Normal,Resolved,Fixed,12/Apr/11 15:02,16/Apr/19 09:33
Bug,CASSANDRA-2462,12504104,Fix build for distributed and stress tests,Distributed and stress tests are not compiling for trunk.,stuhood,stuhood,Low,Resolved,Fixed,12/Apr/11 20:46,16/Apr/19 09:33
Bug,CASSANDRA-2463,12504105,Flush and Compaction Unnecessarily Allocate 256MB Contiguous Buffers,"Currently, Cassandra 0.7.x allocates a 256MB contiguous byte array at the beginning of a memtable flush or compaction (presently hard-coded as Config.in_memory_compaction_limit_in_mb). When several memtable flushes are triggered at once (as by `nodetool flush` or `nodetool snapshot`), the tenured generation will typically experience extreme pressure as it attempts to locate [n] contiguous 256mb chunks of heap to allocate. This will often trigger a promotion failure, resulting in a stop-the-world GC until the allocation can be made. (Note that in the case of the ""release valve"" being triggered, the problem is even further exacerbated; the release valve will ironically trigger two contiguous 256MB allocations when attempting to flush the two largest memtables).

This patch sets the buffer to be used by BufferedRandomAccessFile to Math.min(bytesToWrite, BufferedRandomAccessFile.DEFAULT_BUFFER_SIZE) rather than a hard-coded 256MB. The typical resulting buffer size is 64kb.

I've taken some time to measure the impact of this change on the base 0.7.4 release and with this patch applied. This test involved launching Cassandra, performing four million writes across three column families from three clients, and monitoring heap usage and garbage collections. Cassandra was launched with 2GB of heap and the default JVM options shipped with the project. This configuration has 7 column families with a total of 15GB of data.

Here's the base 0.7.4 release:
http://cl.ly/413g2K06121z252e2t10

Note that on launch, we see a flush + compaction triggered almost immediately, resulting in at least 7x very quick 256MB allocations maxing out the heap, resulting in a promotion failure and a full GC. As flushes proceeed, we see that most of these have a corresponding CMS, consistent with the pattern of a large allocation and immediate collection. We see a second promotion failure and full GC at the 75% mark as the allocations cannot be satisfied without a collection, along with several CMSs in between. In the failure cases, the allocation requests occur so quickly that a standard CMS phase cannot completed before a ParNew attempts to promote the surviving byte array into the tenured generation. The heap usage and GC profile of this graph is very unhealthy.

Here's the 0.7.4 release with this patch applied:
http://cl.ly/050I1g26401B1X0w3s1f

This graph is very different. At launch, rather than a immediate spike to full allocation and a promotion failure, we see a slow allocation slope reaching only 1/8th of total heap size. As writes begin, we see several flushes and compactions, but none result in immediate, large allocations. The ParNew collector keeps up with collections far more ably, resulting in only one healthy CMS collection with no promotion failure. Unlike the unhealthy rapid allocation and massive collection pattern we see in the first graph, this graph depicts a healthy sawtooth pattern of ParNews and an occasional effective CMS with no danger of heap fragmentation resulting in a promotion failure.

The bottom line is that there's no need to allocate a hard-coded 256MB write buffer for flushing memtables and compactions to disk. Doing so results in unhealthy rapid allocation patterns and increases the probability of triggering promotion failures and full stop-the-world GCs which can cause nodes to become unresponsive and shunned from the ring during flushes and compactions.",cscotta,cscotta,Normal,Resolved,Fixed,12/Apr/11 21:23,16/Apr/19 09:33
Bug,CASSANDRA-2465,12504126,Pig load/storefunc loads only one schema and BytesType validation class needs fix,"With a recent optimization, it appears that the Pig load/store func gets only one schema from Cassandra and tries to apply it to all CFs in the pig script.  Also, the BytesType validation tries to cast the object in putNext as a DataByteArray and wrap it as a ByteBuffer.  Instead it should just call objToBB which should take care of it.",jeromatron,jeromatron,Normal,Resolved,Fixed,13/Apr/11 01:39,16/Apr/19 09:33
Bug,CASSANDRA-2466,12504130,bloom filters should avoid huge array allocations to avoid fragmentation concerns,"The fact that bloom filters are backed by single large arrays of longs is expected to interact badly with promotion of objects into old gen with CMS, due to fragmentation concerns (as discussed in CASSANDRA-2463).

It should be less of an issue than CASSANDRA-2463 in the sense that you need to have a lot of rows before the array sizes become truly huge. For comparison, the ~ 143 million row key limit implied by the use of 'int' in BitSet prior to the switch to OpenBitSet translates roughly to 238 MB (assuming the limitation factor there was the addressability of the bits with a 32 bit int, which is my understanding).

Having a preliminary look at OpenBitSet with an eye towards replacing the single long[] with multiple arrays, it seems that if we're willing to drop some of the functionality that is not used for bloom filter purposes, the bits[i] indexing should be pretty easy to augment with modulo to address an appropriate smaller array. Locality is not an issue since the bloom filter case is the worst possible case for locality anyway, and it doesn't matter whether it's one huge array or a number of ~ 64k arrays.

Callers may be affected like BloomFilterSerializer which cares about the underlying bit array.

If the full functionality of OpenBitSet is to be maintained (e.g., xorCount) some additional acrobatics would be necessary and presumably at a noticable performance cost if such operations were to be used in performance critical places.

An argument against touching OpenBitSet is that it seems to be pretty carefully written and tested and has some non-trivial details and people have seemingly benchmarked it quite carefully. On the other hand, the improvement would then apply to other things as well, such as the bitsets used to keep track of in-core pages (off the cuff for scale, a 64 gig sstable should imply a 2 mb bit set, with one bit per 4k page).


",m0nstermind,scode,Low,Resolved,Fixed,13/Apr/11 03:45,16/Apr/19 09:33
Bug,CASSANDRA-2467,12504200,key validator not getting set when adding a keyspace,needs to be applied CassandraServer.convertToCFMetaData(),gdusbabek,gdusbabek,Low,Resolved,Fixed,13/Apr/11 19:05,16/Apr/19 09:33
Bug,CASSANDRA-2468,12504212,Clean up after failed compaction,(Started in CASSANDRA-2088.),amorton,jbellis,Low,Resolved,Fixed,13/Apr/11 21:05,16/Apr/19 09:33
Bug,CASSANDRA-2481,12504307,C* .deb installs C* init.d scripts such that C* comes up before mdadm and related,the C* .deb packages install the init.d scripts at S20 which is before mdadm and various other services.  This means that when a node reboots that C* is started before the RAID sets are up and mounted causing C* to think it has no data and attempt bootstrapping again.,thepaul,mdennis,Low,Resolved,Fixed,14/Apr/11 18:51,16/Apr/19 09:33
Bug,CASSANDRA-2482,12504311,Make compaction type an enum,"Compaction type should be an enum, half of the places we set it we use it as a message and include the keyspace/cf although that is already included in the compaction info object.

I realize this is minor and a pedantic but from the standpoint of someone writing a monitoring application its kind of annoying.",nickmbailey,nickmbailey,Low,Resolved,Fixed,14/Apr/11 19:33,16/Apr/19 09:33
Bug,CASSANDRA-2483,12504322,Stress.java fails to run if the keyspace already exists,,,nickmbailey,Low,Resolved,Fixed,14/Apr/11 23:00,16/Apr/19 09:33
Bug,CASSANDRA-2484,12504329,CassandraStorage LoadPushDown implementation causes heisenbugs,"After pulling hair out about why weird errors were happening loading data from cassandra with seemingly irrelevant changes to the pig scripts (mostly changing the script trying to debug other problems), it looks like the weird errors were because of the implementation we currently have for LoadPushDaown in CassandraStorage.  Unless there is a good reason to implement it, I feel like we should just remove the few lines that are in there until we can spend some serious time doing an implementation of it.",jeromatron,jeromatron,Normal,Resolved,Fixed,15/Apr/11 00:49,16/Apr/19 09:33
Bug,CASSANDRA-2486,12504380,preserve KsDef backwards compatibility for Thrift clients,CASSANDRA-1263 broke client compatibility; we can't preserve it entirely (we'll continue to resturn replication_factor in strategy option rather than try to guess somehow if client is an old one) but we can accommodate old clients on write and leave the KsDef signature compatible which (I think) will make it easier for client authors.,jbellis,jbellis,Normal,Resolved,Fixed,15/Apr/11 15:07,16/Apr/19 09:33
Bug,CASSANDRA-2487,12504385,Pig example script no longer working,"There is a strange error given when trying to run the example-script.pig.

java.io.IOException: Type mismatch in key from map: expected org.apache.pig.impl.io.NullableBytesWritable, recieved org.apache.pig.impl.io.NullableText
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:870)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:573)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:116)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:238)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:231)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:646)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:322)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:210)

Looks like it has to do with PIG-919 and PIG-1277.  For now we can just cast the var as a chararray and it works though.  Will attach a patch.",jeromatron,jeromatron,Low,Resolved,Fixed,15/Apr/11 16:34,16/Apr/19 09:33
Bug,CASSANDRA-2488,12504418,cqlsh errors on comments that end with a semicolon,"Commented-out lines that end in a semicolon cause an error.

Examples:

cqlsh> -- CREATE KEYSPACE ELE WITH replication_factor = 3 AND strategy_class = SimpleStrategy AND strategy_options:replication_factor=3;
Bad Request: line 0:-1 no viable alternative at input '<EOF>'
cqlsh> -- CREATE KEYSPACE ELE WITH replication_factor = 3 AND strategy_class = SimpleStrategy AND strategy_options:replication_factor=3
   ... 
   ... 
   ... ;
Bad Request: line 2:0 no viable alternative at input ';'
cqlsh> -- ;
Bad Request: line 0:-1 no viable alternative at input '<EOF>'
cqlsh> --;
Bad Request: line 0:-1 no viable alternative at input '<EOF>'

As long as there's a line with valid CQL before the semicolon, things work fine though.

I'm pretty sure the problem is on line 75 of cqlsh:
        if not line.endswith("";""):
            self.set_prompt(Shell.continue_prompt)
            return None

A quick workaround would be to kill the pretty continue prompt. A more involved fix would detect whether or not the semicolon was in a comment. This is harder than it sounds, since /* and */ allow multi-line comments.",urandom,angryparsley,Low,Resolved,Fixed,15/Apr/11 22:25,20/Aug/20 12:03
Bug,CASSANDRA-2490,12504433,DatabaseDescriptor.defsVersion should be volatile,"(Probably affects other versions, but I am on 0.7.3).

DatabaseDescriptor.defsVersion should be protected by volatile since it is written to and read by multiple threads from unsynchronized methods. This can manifest itself in schema agreement never occurring due to a node broadcasting the wrong schema version.",paladin8,paladin8,Low,Resolved,Fixed,16/Apr/11 04:57,16/Apr/19 09:33
Bug,CASSANDRA-2492,12504465,add an escapeSQLString function and fix unescapeSQLString,"CliUtils.unescapeSqlString repeats the escape character e.g. 
{noformat}""my \\t tab"" becomes ""my \tt""{noformat}
because {{i}} is not bumped when an escape is processed.
 
Also for Cassandra-2221 I need a function to escape strings back so they will work if processed by the cli again. 

There are a number of non [standard escapes|http://java.sun.com/docs/books/jls/second_edition/html/lexical.doc.html#101089] which I assume is a hang over from is original source https://github.com/apache/cassandra/blob/1aeca2b6257b0ad6680080b1756edf7ee9acf8c8/src/java/org/apache/cassandra/cli/CliUtils.java

Will change to use the [StringEscapeUtils|http://commons.apache.org/lang/api-2.5/org/apache/commons/lang/StringEscapeUtils.html] class  ",amorton,amorton,Low,Resolved,Fixed,17/Apr/11 02:37,16/Apr/19 09:33
Bug,CASSANDRA-2493,12504467,CQL does not preserve column order in select statement,,jbellis,jbellis,Low,Resolved,Fixed,17/Apr/11 03:36,16/Apr/19 09:33
Bug,CASSANDRA-2494,12504486,Quorum reads are not monotonically consistent,"As discussed in this thread,

http://www.mail-archive.com/user@cassandra.apache.org/msg12421.html

Quorum reads should be consistent.  Assume we have a cluster of 3 nodes (X,Y,Z) and a replication factor of 3. If a write of N is committed to X, but not Y and Z, then a read from X should not return N unless the read is committed to at  least two nodes.  To ensure this, a read from X should wait for an ack of the read repair write from either Y or Z before returning.

Are there system tests for cassandra?  If so, there should be a test similar to the original post in the email thread.  One thread should write 1,2,3... at consistency level ONE.  Another thread should read at consistency level QUORUM from a random host, and verify that each read is >= the last read.",jbellis,sbridges,Low,Resolved,Fixed,17/Apr/11 14:29,04/Dec/19 19:38
Bug,CASSANDRA-2496,12504582,Gossip should handle 'dead' states,"For background, see CASSANDRA-2371",brandon.williams,brandon.williams,Normal,Resolved,Fixed,18/Apr/11 18:57,16/Apr/19 09:33
Bug,CASSANDRA-2497,12504586,CLI: issue with keys being interpreted as hex and causing SET statement to fail,"*Original Summary*: Issues with Update Column Family and adding a key_validation_class
_Changed summary because the issue repros on drop/create.  see comment._

*Reproduction Steps*
{code}
create column family users with comparator = UTF8Type 
and column_metadata = [{column_name: password, validation_class: UTF8Type}];

update column family users with key_validation_class=UTF8Type;

set users['jsmith']['password']='ch@ngem3';          
{code}


*EXPECTED RESULT:* After the UPDATE statement, the SET statement should go through successfully.


*ACTUAL RESULT:*  The SET statement gives the same error message, regardless of the UPDATE statement: 
{code}
org.apache.cassandra.db.marshal.MarshalException: cannot parse 'jsmith' as hex bytes
{code}


*Output from describe keyspace*
{code}
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.29062499999999997/62/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type

{code}
",xedin,cdaw,Low,Resolved,Fixed,18/Apr/11 19:40,16/Apr/19 09:33
Bug,CASSANDRA-2499,12504595,cassandra-env.sh pattern matching for OpenJDK broken in some cases,"With bash version 4.1.5, the section of cassandra-env that tries to match the JDK distribution seems to have some kind of syntax error.  I get the following message when running bin/cassandra:

{noformat}
bin/../conf/cassandra-env.sh: 99: [[: not found
{noformat}",cywjackson,thobbs,Low,Resolved,Fixed,18/Apr/11 20:37,16/Apr/19 09:33
Bug,CASSANDRA-2501,12504612,Error running cqlsh from .tar file -- global name 'SchemaDisagreementException' is not defined,"*Error when running cqlsh*
{code}
[cassandra@cdaw-qa1 cql-1.0.0]$ cqlsh cdaw-qa1
Traceback (most recent call last):
  File ""/usr/bin/cqlsh"", line 212, in <module>
    password=options.password)
  File ""/usr/bin/cqlsh"", line 55, in __init__
    self.conn = cql.connect(hostname, port, user=username, password=password)
  File ""/usr/lib/python2.6/site-packages/cql/__init__.py"", line 51, in connect
    return connection.Connection(host, port, keyspace, user, password)
  File ""/usr/lib/python2.6/site-packages/cql/connection.py"", line 53, in __init__
    c.execute('USE %s;' % keyspace)
  File ""/usr/lib/python2.6/site-packages/cql/cursor.py"", line 126, in execute
    except SchemaDisagreementException, sde:
NameError: global name 'SchemaDisagreementException' is not defined
{code}


*Build*
* Install the cassandra binary from the nightly build
wget https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/apache-cassandra-2011-04-18_11-02-29-bin.tar.gz

* Install cql from .tar file on nightly build
wget https://builds.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/cql-1.0.0.tar.gz

*CQL Install Output*
{code}
[cassandra@cdaw-qa1 cql-1.0.0]$ sudo python2.6 ./setup.py install
[sudo] password for cassandra: 
running install
running build
running build_py
running build_scripts
creating build/scripts-2.6
copying and adjusting cqlsh -> build/scripts-2.6
changing mode of build/scripts-2.6/cqlsh from 644 to 755
running install_lib
creating /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/results.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/marshal.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/connection.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/cursor.py -> /usr/lib/python2.6/site-packages/cql
creating /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/__init__.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/Cassandra.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/constants.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/cassandra/ttypes.py -> /usr/lib/python2.6/site-packages/cql/cassandra
copying build/lib/cql/decoders.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/__init__.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/errors.py -> /usr/lib/python2.6/site-packages/cql
copying build/lib/cql/connection_pool.py -> /usr/lib/python2.6/site-packages/cql
byte-compiling /usr/lib/python2.6/site-packages/cql/results.py to results.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/marshal.py to marshal.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/connection.py to connection.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cursor.py to cursor.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/__init__.py to __init__.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/Cassandra.py to Cassandra.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/constants.py to constants.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/cassandra/ttypes.py to ttypes.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/decoders.py to decoders.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/__init__.py to __init__.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/errors.py to errors.pyc
byte-compiling /usr/lib/python2.6/site-packages/cql/connection_pool.py to connection_pool.pyc
running install_scripts
copying build/scripts-2.6/cqlsh -> /usr/bin
changing mode of /usr/bin/cqlsh to 755
running install_egg_info
Writing /usr/lib/python2.6/site-packages/cql-1.0.0-py2.6.egg-info

{code}
",jbellis,cdaw,Normal,Resolved,Fixed,18/Apr/11 22:51,16/Apr/19 09:33
Bug,CASSANDRA-2505,12504678,cqlsh can't decode column names/values,"The way results are accessed, cqlsh is displaying the raw thrift results and not the decoded values.",urandom,urandom,Normal,Resolved,Fixed,19/Apr/11 16:27,16/Apr/19 09:33
Bug,CASSANDRA-2507,12504681,Python CQL driver does not decode most values,"Most keys, and column name/values are not decoded properly.  The attached CQL input can be used to demonstrate:

_Note: requires the patch from CASSANDRA-2505 to be applied_

{noformat}
$ drivers/py/cqlsh localhost 9170 < repro.cql 
 | '\x00\x00\x00\x00\x00\x00\x00\x01','\x00\x00\x00\x00\x00\x00\x00\x01' | '\x00\x00\x00\x00\x00\x00\x00\x02','\x00\x00\x00\x00\x00\x00\x00\x02'
e�#j������ | 'e\xe2#\x01j\xa2\x11\xe0\x00\x00\xfe\x8e\xbe\xea\xd9\xff','e\xe2#\x02j\xa2\x11\xe0\x00\x00\xfe\x8e\xbe\xea\xd9\xff'
{noformat}

For all practical purposes, this renders the driver useless for everything but strings.",thobbs,urandom,Normal,Resolved,Fixed,19/Apr/11 16:33,16/Apr/19 09:33
Bug,CASSANDRA-2508,12504694,missing imports in CQL Python driver,"Try:

bq. cd drivers/py && python -c 'from cql import DateFromTicks; DateFromTicks(1)'

Also:
{{cql.connection}} is missing an import of {{AuthenticationRequest}} from {{ttypes}}, and the exceptions {{NotSupportedError}}, and {{InternalError}}.

Also:
{{marshal.unmarshal_long}} has a NameError waiting to happen in the form of ""unpack""",jbellis,urandom,Normal,Resolved,Fixed,19/Apr/11 17:44,16/Apr/19 09:33
Bug,CASSANDRA-2509,12504708,Update py_test to use strategy_options,"KsDef was changed in cassandra.thrift to accept a hash of options as strategy_options.  py_test/stress.py needs to be updated with the new method arguments.

CASSANDRA-1263 changed the parameters to KsDef.
CASSANDRA-2462 fixed this issue in the native Java stress package.",rwjblue,rwjblue,Normal,Resolved,Fixed,19/Apr/11 20:19,16/Apr/19 09:33
Bug,CASSANDRA-2510,12504713,Word count example won't compile,"On the 0.8 branch, the word count stuff isn't compiling.",jeromatron,jeromatron,Low,Resolved,Fixed,19/Apr/11 21:02,16/Apr/19 09:33
Bug,CASSANDRA-2511,12504715,Need to forward merge parts of r1088800 to make the pig CassandraStorage build,"Parts of revision 1088800 weren't forward merged into 0.8/trunk.  So the 0.8/trunk version of CassandraStorage doesn't currently build.  Specifically, it needs the decompose method in the AbstractType hierarchy.",jeromatron,jeromatron,Normal,Resolved,Fixed,19/Apr/11 21:22,16/Apr/19 09:33
Bug,CASSANDRA-2512,12504720,Updating a column's validation class from AsciiType to UTF8Type does not actually work,"Please note this is reproducible on both Cassandra 0.74 and the April 18th trunk build.

*Reproduction Steps*
{code}
create column family users with comparator = UTF8Type
and column_metadata = [{column_name: password, validation_class: UTF8Type},
{column_name: gender, validation_class: AsciiType}];

update column family users with comparator = UTF8Type
and column_metadata = [{column_name: password, validation_class: UTF8Type}
{column_name: gender, validation_class: UTF8Type}];
{code}

*Before & After quitting cassandra-cli:  Notice the validation class for the gender client still shows AsciiType*
{code}
[default@demo] describe keyspace demo;
Keyspace: demo:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [datacenter1:1]
  Column Families:
    ColumnFamily: users
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.29062499999999997/62/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: gender
          Validation Class: org.apache.cassandra.db.marshal.AsciiType
        Column Name: password
          Validation Class: org.apache.cassandra.db.marshal.UTF8Type

{code}
",jbellis,cdaw,Normal,Resolved,Fixed,19/Apr/11 21:59,16/Apr/19 09:33
Bug,CASSANDRA-2514,12504738,batch_mutate operations with CL=LOCAL_QUORUM throw TimeOutException when there aren't sufficient live nodes,"We have a 2 DC setup with RF = 4. There are 2 nodes in each DC. Following is the keyspace definition:
<snip>
keyspaces:
    - name: KeyspaceMetadata
      replica_placement_strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
      strategy_options:
        DC1 : 2
        DC2 : 2
      replication_factor: 4
</snip>

I shutdown all except one node and waited for the live node to recognize that other nodes are dead. Following is the nodetool ring output on the live node:
Address         Status State   Load            Owns    Token                                       
                                                       169579575332184635438912517119426957796     
10.17.221.19    Down   Normal  ?               29.20%  49117425183422571410176530597442406739      
10.17.221.17    Up     Normal  81.64 KB        4.41%   56615248844645582918169246064691229930      
10.16.80.54     Down   Normal  ?               21.13%  92563519227261352488017033924602789201      
10.17.221.18    Down   Normal  ?               45.27%  169579575332184635438912517119426957796     

I expect UnavailableException when I send batch_mutate request to node that is up. However, it returned TimeOutException:
TimedOutException()
    at org.apache.cassandra.thrift.Cassandra$batch_mutate_result.read(Cassandra.java:16493)
    at org.apache.cassandra.thrift.Cassandra$Client.recv_batch_mutate(Cassandra.java:916)
    at org.apache.cassandra.thrift.Cassandra$Client.batch_mutate(Cassandra.java:890)

Following is the cassandra-topology.properties
# Cassandra Node IP=Data Center:Rack
10.17.221.17=DC1:RAC1
10.17.221.19=DC1:RAC2

10.17.221.18=DC2:RAC1
10.16.80.54=DC2:RAC2
",nar3ndra,nar3ndra,Low,Resolved,Fixed,20/Apr/11 00:29,16/Apr/19 09:33
Bug,CASSANDRA-2516,12504742,"Allow LOCAL_QUORUM, EACH_QUORUM CLs to work w/ any Strategy class",,jbellis,jbellis,Low,Resolved,Fixed,20/Apr/11 02:00,16/Apr/19 09:33
Bug,CASSANDRA-2517,12504752,Update distributed tests for optional Column fields,,stuhood,stuhood,Normal,Resolved,Fixed,20/Apr/11 03:49,16/Apr/19 09:33
Bug,CASSANDRA-2519,12504779,row deletions do not add to memtable op count,"from discussion http://www.mail-archive.com/user@cassandra.apache.org/msg12531.html

Memtable.resolve() uses the count of columns in the CF to bump the op count however RowMutation.delete() does not add any columns to the CF when an entire row is deleted. If a super column or column is deleted it adds 1 towards the op count. Deleting many named columns will add many to the op count. ",amorton,amorton,Low,Resolved,Fixed,20/Apr/11 10:45,16/Apr/19 09:33
Bug,CASSANDRA-2523,12504822,Distributed test scripts not working with Whirr 0.4.0,"I suspect that our runurl based script execution is not working with Whirr 0.4.0, which is causing distributed tests that kill/wipe nodes to timeout. See [this FAQ entry|http://incubator.apache.org/whirr/faq.html#how-can-i-modify-the-instance-installation-and-configuration-scripts] for a description of the change.",mallen,stuhood,Normal,Resolved,Fixed,20/Apr/11 19:58,16/Apr/19 09:33
Bug,CASSANDRA-2525,12504840,CQL: create keyspace does not the replication factor argument and allows invalid sql to pass thru,"I ran the following SQL in cqlsh and immediately received a socket closed error.  After that point, I couldn't run nodetool, and then got an exception starting up the cluster.

Please Note:  The following syntax is valid in 0.74 but invalid in 0.8.
The 0.8 cassandra-cli errors on the following statement, so the resolution of the bug is to have cqlsh block this incorrect syntax.

{code}
create keyspace testcli with replication_factor=1
and placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy';
{code}

{code}
CREATE KEYSPACE testcql with replication_factor = 1 and strategy_class = 'org.apache.cassandra.locator.SimpleStrategy';	
{code}


{code}
ERROR 01:29:26,989 Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 4 more
Exception encountered during startup.
java.lang.RuntimeException: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.db.Table.<init>(Table.java:278)
	at org.apache.cassandra.db.Table.open(Table.java:110)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:160)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: SimpleStrategy requires a replication_factor strategy option.
	at org.apache.cassandra.locator.SimpleStrategy.validateOptions(SimpleStrategy.java:79)
	at org.apache.cassandra.locator.AbstractReplicationStrategy.createReplicationStrategy(AbstractReplicationStrategy.java:262)
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:328)
	at org.apache.cassandra.db.Table.<init>(Table.java:274)
	... 4 more
{code}",jbellis,cdaw,Urgent,Resolved,Fixed,21/Apr/11 01:35,16/Apr/19 09:33
Bug,CASSANDRA-2528,12504909,NPE from PrecompactedRow,"received a NPE from trunk (0.8) on PrecompactedRow:

ERROR [CompactionExecutor:2] 2011-04-21 17:21:31,610 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:2,1,main]
java.lang.NullPointerException
        at org.apache.cassandra.io.PrecompactedRow.<init>(PrecompactedRow.java:86)
        at org.apache.cassandra.io.CompactionIterator.getCompactedRow(CompactionIterator.java:167)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:124)
        at org.apache.cassandra.io.CompactionIterator.getReduced(CompactionIterator.java:44)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:553)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:146)
        at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:112)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


size of data in /var/lib/cassandra is 11G on this, but there is also report that 1.7G also see the same.

data was previously populated from 0.7.4 cassandra

added debug logging, not sure how much this help (this is logged before the exception.)

 INFO [CompactionExecutor:2] 2011-04-21 17:21:31,588 CompactionManager.java (line 534) Compacting Major: [SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-10-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-7-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-6-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-8-Data.db'), SSTableReader(path='/var/lib/cassandra/data/cfs/inode.path-f-9-Data.db')]
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-10-Data.db   : 256
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-7-Data.db   : 512
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,588 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-6-Data.db   : 768
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,589 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-8-Data.db   : 1024
DEBUG [CompactionExecutor:2] 2011-04-21 17:21:31,589 SSTableReader.java (line 132) index size for bloom filter calc for file  : /var/lib/cassandra/data/cfs/inode.path-f-9-Data.db   : 1280
 INFO [CompactionExecutor:2] 2011-04-21 17:21:31,609 CompactionIterator.java (line 185) Major@1181554512(cfs, inode.path, 523/10895) now compacting at 16777 bytes/ms.
",jbellis,cywjackson,Normal,Resolved,Fixed,21/Apr/11 17:55,16/Apr/19 09:33
Bug,CASSANDRA-2532,12504922,Log read timeouts at the StorageProxy level,"We log successful reads, but not timeouts (although we have a lovely TimeoutException message, it doesn't look like we are printing it).",stuhood,stuhood,Normal,Resolved,Fixed,21/Apr/11 21:26,16/Apr/19 09:33
Bug,CASSANDRA-2533,12504923,CQL documentation missing INSERT syntax,"Both documents are missing syntax for the INSERT statement:

https://github.com/apache/cassandra/blob/trunk/doc/cql/CQL.textile
https://github.com/apache/cassandra/raw/trunk/doc/cql/CQL.html",xedin,cdaw,Low,Resolved,Fixed,21/Apr/11 21:44,16/Apr/19 09:33
Bug,CASSANDRA-2536,12504930,Schema disagreements when using connections to multiple hosts,"If you have two thrift connections open to different nodes and you create a KS using the first, then a CF in that KS using the second, you wind up with a schema disagreement even if you wait/sleep after creating the KS.

The attached script reproduces the issue using pycassa (1.0.6 should work fine, although it has the 0.7 thrift-gen code).  It's also reproducible by hand with two cassandra-cli sessions.",thobbs,thobbs,Low,Resolved,Fixed,21/Apr/11 22:55,16/Apr/19 09:33
Bug,CASSANDRA-2538,12504935,CQL: NPE running SELECT with an IN clause,"*Test Case to Run*
{noformat}
cqlsh> select * from users where key in ('user2', 'user3');
Internal application error
{noformat}


*Test Setup*
{noformat}
CREATE COLUMNFAMILY users (
  KEY varchar PRIMARY KEY,
  password varchar);

INSERT INTO users (KEY, password) VALUES ('user1', 'ch@ngem3a');
{noformat}


*Log Files*
{noformat}
ERROR [RequestResponseStage:17] 2011-04-21 23:36:41,600 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[RequestResponseStage:17,5,main]
java.lang.AssertionError
	at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR [RequestResponseStage:17] 2011-04-21 23:36:41,600 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[RequestResponseStage:17,5,main]
java.lang.AssertionError
	at org.apache.cassandra.service.ReadCallback.response(ReadCallback.java:127)
	at org.apache.cassandra.net.ResponseVerbHandler.doVerb(ResponseVerbHandler.java:49)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
ERROR [pool-2-thread-5] 2011-04-21 23:37:12,026 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.NullPointerException
	at org.apache.cassandra.cql.WhereClause.and(WhereClause.java:59)
	at org.apache.cassandra.cql.WhereClause.<init>(WhereClause.java:44)
	at org.apache.cassandra.cql.CqlParser.whereClause(CqlParser.java:816)
	at org.apache.cassandra.cql.CqlParser.selectStatement(CqlParser.java:502)
	at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:191)
	at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:834)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:463)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1134)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{noformat}",urandom,cdaw,Low,Resolved,Fixed,21/Apr/11 23:41,16/Apr/19 09:33
Bug,CASSANDRA-2539,12504937,"CQL: cqlsh does shows Exception, but not error message when running truncate while a node is down.","This is really just a usability bug, but it would nice to bubble the error message that is printed in the log file up to the interface.

*cqlsh output*
{noformat}
cqlsh> truncate users;
Exception: UnavailableException()
{noformat}

*log file error*
{noformat}
 INFO [pool-2-thread-5] 2011-04-21 23:53:30,466 StorageProxy.java (line 1021) Cannot perform truncate, some hosts are down
{noformat}",urandom,cdaw,Low,Resolved,Fixed,22/Apr/11 00:28,16/Apr/19 09:33
Bug,CASSANDRA-2545,12505019,CQL: cqlsh error running batch update commands,"*CQL Test Case*
{code}
//TEST CASE #1
BEGIN BATCH
UPDATE users SET gender = 'm', birth_year = '1981' WHERE KEY = 'user1';
UPDATE users SET gender = 'm', birth_year = '1982' WHERE KEY = 'user2';
UPDATE users SET gender = 'm', birth_year = '1983' WHERE KEY = 'user3';
APPLY BATCH	

//TEST CASE #2
BEGIN BATCH USING CONSISTENCY ZERO
UPDATE users SET state = 'TX' WHERE KEY = 'user1';
UPDATE users SET state = 'TX' WHERE KEY = 'user2';
UPDATE users SET state = 'TX' WHERE KEY = 'user3';
APPLY BATCH	


//ERROR
Bad Request: line 0:-1 mismatched input '<EOF>' expecting K_APPLY
{code}

*Test Setup*
{code}
CREATE COLUMNFAMILY users (
  KEY varchar PRIMARY KEY,
  password varchar,
  gender varchar,
  session_token varchar,
  state varchar,
  birth_year bigint);

INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');
{code}

*Documented Syntax*
{panel}
BEGIN BATCH [USING <CONSISTENCY>]
UPDATE CF1 SET name1 = value1, name2 = value2 WHERE KEY = keyname1;
UPDATE CF1 SET name3 = value3 WHERE KEY = keyname2;
UPDATE CF2 SET name4 = value4, name5 = value5 WHERE KEY = keyname3;
APPLY BATCH
{panel}",urandom,cdaw,Normal,Resolved,Fixed,23/Apr/11 00:55,16/Apr/19 09:33
Bug,CASSANDRA-2549,12505024,Start up of 0.8-beta1 on Ubuntu,"root@home:/home/drew# cassandra -f
 INFO 14:06:03,261 Logging initialized
 INFO 14:06:03,323 Heap size: 1543831552/1543831552
 INFO 14:06:03,332 JNA not found. Native methods will be disabled.
 INFO 14:06:03,379 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 14:06:03,899 DiskAccessMode 'auto' determined to be standard, indexAccessMode is standard
ERROR 14:06:04,028 Exception encountered during startup.
java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/UnavailableException
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2444)
	at java.lang.Class.privateGetPublicMethods(Class.java:2564)
	at java.lang.Class.getMethods(Class.java:1427)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.initMaps(MBeanAnalyzer.java:126)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.<init>(MBeanAnalyzer.java:116)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.analyzer(MBeanAnalyzer.java:104)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.getAnalyzer(StandardMBeanIntrospector.java:66)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.getPerInterface(MBeanIntrospector.java:181)
	at com.sun.jmx.mbeanserver.MBeanSupport.<init>(MBeanSupport.java:136)
	at com.sun.jmx.mbeanserver.StandardMBeanSupport.<init>(StandardMBeanSupport.java:64)
	at com.sun.jmx.mbeanserver.Introspector.makeDynamicMBean(Introspector.java:174)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:330)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:516)
	at org.apache.cassandra.service.StorageService.<init>(StorageService.java:231)
	at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:171)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:78)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:429)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:294)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:98)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.UnavailableException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
	... 23 more
Exception encountered during startup.
java.lang.NoClassDefFoundError: org/apache/cassandra/thrift/UnavailableException
	at java.lang.Class.getDeclaredMethods0(Native Method)
	at java.lang.Class.privateGetDeclaredMethods(Class.java:2444)
	at java.lang.Class.privateGetPublicMethods(Class.java:2564)
	at java.lang.Class.getMethods(Class.java:1427)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.initMaps(MBeanAnalyzer.java:126)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.<init>(MBeanAnalyzer.java:116)
	at com.sun.jmx.mbeanserver.MBeanAnalyzer.analyzer(MBeanAnalyzer.java:104)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.getAnalyzer(StandardMBeanIntrospector.java:66)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.getPerInterface(MBeanIntrospector.java:181)
	at com.sun.jmx.mbeanserver.MBeanSupport.<init>(MBeanSupport.java:136)
	at com.sun.jmx.mbeanserver.StandardMBeanSupport.<init>(StandardMBeanSupport.java:64)
	at com.sun.jmx.mbeanserver.Introspector.makeDynamicMBean(Introspector.java:174)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerObject(DefaultMBeanServerInterceptor.java:936)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.registerMBean(DefaultMBeanServerInterceptor.java:330)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.registerMBean(JmxMBeanServer.java:516)
	at org.apache.cassandra.service.StorageService.<init>(StorageService.java:231)
	at org.apache.cassandra.service.StorageService.<clinit>(StorageService.java:171)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.<init>(DynamicEndpointSnitch.java:78)
	at org.apache.cassandra.config.DatabaseDescriptor.createEndpointSnitch(DatabaseDescriptor.java:429)
	at org.apache.cassandra.config.DatabaseDescriptor.<clinit>(DatabaseDescriptor.java:294)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:98)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:314)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: java.lang.ClassNotFoundException: org.apache.cassandra.thrift.UnavailableException
	at java.net.URLClassLoader$1.run(URLClassLoader.java:217)
	at java.security.AccessController.doPrivileged(Native Method)
	at java.net.URLClassLoader.findClass(URLClassLoader.java:205)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:321)
	at sun.misc.Launcher$AppClassLoader.loadClass(Launcher.java:294)
	at java.lang.ClassLoader.loadClass(ClassLoader.java:266)
	... 23 more
",selam,drewbroadley,Normal,Resolved,Fixed,23/Apr/11 02:09,16/Apr/19 09:33
Bug,CASSANDRA-2550,12505058,nodetool setcompactionthroughput requiring wrong number of arguments?,"---
            case SETCOMPACTIONTHROUGHPUT :
                if (arguments.length != 2) { badUse(""Missing value argument.""); }
                probe.setCompactionThroughput(Integer.valueOf(arguments[1]));
                break;
---

I would think arguments.length should be just 1?

",terjem,terjem,Low,Resolved,Fixed,23/Apr/11 17:58,16/Apr/19 09:33
Bug,CASSANDRA-2552,12505091,ReadResponseResolver Race,"When receiving a response, ReadResponseResolver uses a 3 step process to decide whether to trigger the condition that enough responses have arrived:
# Add new response
# Check response set size
# Check that data is present

I think that these steps must have been reordered by the compiler in some cases, because I was able to reproduce a case for a QUORUM read where the condition is not properly triggered:
{noformat}
INFO [RequestResponseStage:15] 2011-04-25 00:26:53,514 ReadResponseResolver.java (line 87) post append for 1087367065: hasData=false in 2 messages
INFO [RequestResponseStage:8] 2011-04-25 00:26:53,514 ReadResponseResolver.java (line 87) post append for 1087367065: hasData=true in 1 messages
INFO [pool-1-thread-54] 2011-04-25 00:27:03,516 StorageProxy.java (line 623) Read timeout: java.util.concurrent.TimeoutException: ReadResponseResolver@1087367065(/10.34.131.109=false,/10.34.132.122=true,)
{noformat}
The last line shows that both results were present, and that one of them was holding data.",jbellis,stuhood,Normal,Resolved,Fixed,25/Apr/11 01:21,16/Apr/19 09:33
Bug,CASSANDRA-2554,12505126,Move gossip heartbeats [back] to its own thread,"Gossip heartbeat *really* needs to run every 1s or other nodes may mark us down. But gossip currently shares an executor thread with other tasks.

I see at least two of these could cause blocking: hint cleanup post-delivery and flush-expired-memtables, both of which call forceFlush which will block if the flush queue + threads are full.

We've run into this before (CASSANDRA-2253); we should move Gossip back to its own dedicated executor or it will keep happening whenever someone accidentally puts something on the ""shared"" executor that can block.",jbellis,jbellis,Normal,Resolved,Fixed,25/Apr/11 17:20,16/Apr/19 09:33
Bug,CASSANDRA-2556,12505139,DatacenterReadResolver not triggering repair,DatacenterReadResolver only calls maybeResolveForRepair for local reads.,jbellis,stuhood,Low,Resolved,Fixed,25/Apr/11 21:39,16/Apr/19 09:33
Bug,CASSANDRA-2557,12505158,StorageProxy sends same message multiple times,"A cassandra node gets multiple mutation messages (in number of times of replication factor at maximum) for an insert. It may cause high load on the node. The mutation should be only once for each insert.

This bug is visible via MutationStage count in nodetool tpstats.
For instance, if you have 6 node cluster (initial keys are 31, 32, 33, 34, 35 and 36) with replication factor = 4 and a single data (for example, key='2') is inserted, MutationStage count will be as follows:

node 1: MutationStage 0 0 4
node 2: MutationStage 0 0 3
node 3: MutationStage 0 0 2
node 4: MutationStage 0 0 1
node 5: MutationStage 0 0 0
node 6: MutationStage 0 0 0

As you can see, the counts are different in each node.
",,skamio,Normal,Resolved,Fixed,26/Apr/11 06:45,16/Apr/19 09:33
Bug,CASSANDRA-2562,12505194,Parent POM does not get deployed to the maven repository,"The parent pom does not get deployed to the Maven Central

(for 0.7.5 I am fixing this by hand)",stephenc,stephenc,Normal,Resolved,Fixed,26/Apr/11 13:48,16/Apr/19 09:33
Bug,CASSANDRA-2563,12505230,Error starting up a cassandra cluster after creating a table in the system keyspace: Attempt to assign id to existing column family.,"*Repro Steps*
* rm -rf /var/lib/cassandra/*
* rm -rf /var/log/cassandra/*
* Start Cassandra
* In cqlsh, create a column family and insert data
{noformat}
cqlsh> CREATE COLUMNFAMILY users (
   ...   KEY varchar PRIMARY KEY,
   ...   password varchar,
   ...   gender varchar,
   ...   session_token varchar,
   ...   state varchar,
   ...   birth_year bigint);

cqlsh> INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');
cqlsh> INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');
cqlsh> INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');
{noformat}

* Quit cqlsh
* Kill Cassandra
* Startup Cassandra and get error

{noformat}
 INFO 18:38:24,509 Loading schema version 087af100-7034-11e0-0000-242d50cf1fde
ERROR 18:38:24,774 Exception encountered during startup.
java.io.IOError: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:489)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.CFMetaData.map(CFMetaData.java:126)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:485)
	... 3 more
Exception encountered during startup.
java.io.IOError: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:489)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:138)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Caused by: org.apache.cassandra.config.ConfigurationException: Attempt to assign id to existing column family.
	at org.apache.cassandra.config.CFMetaData.map(CFMetaData.java:126)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:485)
	... 3 more
{noformat}



*UPDATE:  This issue happens if I create the CF in the default keyspace.*

*Workaround*
{noformat}
cqlsh> CREATE KEYSPACE cqldb with 
   ...   strategy_class =  
   ...     'org.apache.cassandra.locator.SimpleStrategy' 
   ...   and strategy_options:replication_factor=1;
cqlsh> use cqldb;

The create the table and insert data.
{noformat}
",jbellis,cdaw,Urgent,Resolved,Fixed,26/Apr/11 18:50,16/Apr/19 09:33
Bug,CASSANDRA-2565,12505235,Nodes are not reported as alive after being marked down,"To reproduce: start two nodes in foreground mode, then suspend (^Z) one of them.  Once it is marked down, resume the process (fg) and it will not be marked up again.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,26/Apr/11 19:36,16/Apr/19 09:33
Bug,CASSANDRA-2566,12505240,CQL: Batch Updates: some consistency levels not working,"Testing the batch updates, and running into some issues with different consistency levels

+*Summary*+
* UNTESTED: CONSISTENCY ANY
* PASS: CONSISTENCY  ONE
* PASS: CONSISTENCY  QUORUM
* PASS: CONSISTENCY  ALL
* CQL ERROR: CONSISTENCY  LOCAL_QUORUM
* CQL ERROR: CONSISTENCY  EACH_QUORUM

 
+*Test Setup*+
{code}
CREATE KEYSPACE cqldb with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy'  
and strategy_options:replication_factor=1;

use cqldb;

CREATE COLUMNFAMILY users (KEY varchar PRIMARY KEY, password varchar, gender varchar, 
session_token varchar, state varchar, birth_year bigint);

INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3', 'f', 'CA', '1971');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user2', 'ch@ngem3', 'f', 'CA', '1972');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user3', 'ch@ngem3', 'f', 'CA', '1973');
{code}


+*Bug Details*+

*CONSISTENCY LOCAL_QUORUM*
{code}
BEGIN BATCH USING CONSISTENCY  LOCAL_QUORUM
UPDATE users SET state = 'UT' WHERE KEY = 'user1';
UPDATE users SET state = 'UT' WHERE KEY = 'user2';
UPDATE users SET state = 'UT' WHERE KEY = 'user3';
APPLY BATCH

cqlsh>  Bad Request: line 1:31 mismatched input 'LOCAL_QUORUM' expecting K_LEVEL
{code}

*CONSISTENCY EACH_QUORUM*
{code}
BEGIN BATCH USING CONSISTENCY  EACH_QUORUM
UPDATE users SET state = 'TX' WHERE KEY = 'user1';
UPDATE users SET state = 'TX' WHERE KEY = 'user2';
UPDATE users SET state = 'TX' WHERE KEY = 'user3';
APPLY BATCH

cqlsh> Bad Request: line 1:31 mismatched input 'EACH_QUORUM' expecting K_LEVEL
{code}
",xedin,cdaw,Low,Resolved,Fixed,26/Apr/11 20:00,16/Apr/19 09:33
Bug,CASSANDRA-2567,12505242,CQL: DELETE documentation uses UPDATE examples,"
{panel}
h2. DELETE

_Synopsis:_

bc. 
DELETE [COLUMNS] FROM <COLUMN FAMILY> [USING <CONSISTENCY>] WHERE KEY = keyname1
DELETE [COLUMNS] FROM <COLUMN FAMILY> [USING <CONSISTENCY>] WHERE KEY IN (keyname1, keyname2);

A @DELETE@ is used to perform the removal of one or more columns from one or more rows.

h3. Specifying Columns

bc. 
DELETE [COLUMNS] ...

Following the @DELETE@ keyword is an optional comma-delimited list of column name terms. When no column names are specified, the remove applies to the entire row(s) matched by the ""WHERE clause"":#deleterows

h3. Column Family

bc. 
DELETE ... FROM <COLUMN FAMILY> ...

The column family name follows the list of column names.

h3. Consistency Level

bc. 
UPDATE ... [USING <CONSISTENCY>] ...

Following the column family identifier is an optional ""consistency level specification"":#consistency.

h3(#deleterows). Specifying Rows

bc. 
UPDATE ... WHERE KEY = keyname1
UPDATE ... WHERE KEY IN (keyname1, keyname2)

The @WHERE@ clause is used to determine which row(s) a @DELETE@ applies to.  The first form allows the specification of a single keyname using the @KEY@ keyword and the @=@ operator.  The second form allows a list of keyname terms to be specified using the @IN@ notation and a parenthesized list of comma-delimited keyname terms.
     
{panel}",,cdaw,Low,Resolved,Fixed,26/Apr/11 20:15,16/Apr/19 09:33
Bug,CASSANDRA-2570,12505248,CQL: incorrect error message running truncate on CF that does not exist,"Run truncate on a CF that does not exist. The error message is misleading.

*CQLSH*
{code}
cqlsh> truncate aaaa;
Unable to complete request: one or more nodes were unavailable.
{code}

*cassandra-cli*
{code}
[default@cqldb] truncate aaaaaaaaa;
aaaaaaaaa not found in current keyspace.
{code}",xedin,cdaw,Low,Resolved,Fixed,26/Apr/11 21:23,16/Apr/19 09:33
Bug,CASSANDRA-2571,12505255,Check for null super column for SC CF in ThriftValidation (and always validate the sc key),"Run the following via cli:
{noformat}
[default@test] use test;
Authenticated to keyspace: test
[default@test] create column family super with column_type=Super and default_validation_class=CounterColumnType;
d41df8e0-7055-11e0-0000-242d50cf1fbf
Waiting for schema agreement...
... schemas agree across the cluster
[default@test] incr super['0']['0'];
Value incremented.
[default@test] incr super['0']['0']['0'];
null
{noformat}

Obviously the first incr call is invalid, even though it reports otherwise, as well as generates this exception:
{noformat}
ERROR 17:38:05,871 Fatal exception in thread Thread[COMMIT-LOG-WRITER,5,main]
java.lang.RuntimeException: java.lang.ClassCastException: org.apache.cassandra.db.CounterColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.ClassCastException: org.apache.cassandra.db.CounterColumn cannot be cast to org.apache.cassandra.db.SuperColumn
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:353)
        at org.apache.cassandra.db.SuperColumnSerializer.serialize(SuperColumn.java:337)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:88)
        at org.apache.cassandra.db.ColumnFamilySerializer.serialize(ColumnFamilySerializer.java:74)
        at org.apache.cassandra.db.RowMutation$RowMutationSerializer.serialize(RowMutation.java:353)
        at org.apache.cassandra.db.RowMutation.getSerializedBuffer(RowMutation.java:236)
        at org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:480)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 1 more
{noformat}

But the second, proper incr call results in a bunch of exceptions and not a real increment.",slebresne,mbulman,Normal,Resolved,Fixed,26/Apr/11 22:53,16/Apr/19 09:33
Bug,CASSANDRA-2572,12505258,cassandra_cli: CREATE CF HELP should list option as key_cache_save_period instead of keys_cached_save_period,"*cassandra-cli: output from create cf command*
{noformat}
[default@cqldb]  create column family supa_dupa2 with keys_cached_save_period = 124000;

No enum const class org.apache.cassandra.cli.CliClient$ColumnFamilyArgument.KEYS_CACHED_SAVE_PERIOD
{noformat}

*cassandra-cli: help create column family*
{noformat}
- keys_cached_save_period: Duration in seconds after which Cassandra should
  safe the keys cache. Caches are saved to saved_caches_directory as
  specified in conf/Cassandra.yaml. Default is 14400 or 4 hours.

  Saved caches greatly improve cold-start speeds, and is relatively cheap in
  terms of I/O for the key cache. Row cache saving is much more expensive and
  has limited use.
{noformat}

*cqlsh: documentation for create column family options*
{noformat}
key_cache_save_period_in_seconds	14400	Number of seconds between saving key caches.
{noformat}

*cqlsh: this actually works*
{noformat}
cqlsh>  CREATE COLUMNFAMILY cf1 (KEY varchar PRIMARY KEY) WITH key_cache_save_period_in_seconds=10000;
{noformat}

*cassandra-cli: CF definition via show keyspace*
{noformat}
    ColumnFamily: cf1
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.UTF8Type
      Columns sorted by: org.apache.cassandra.db.marshal.UTF8Type
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/10000
      Memtable thresholds: 0.140625/30/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
{noformat}
",xedin,cdaw,Low,Resolved,Fixed,27/Apr/11 00:36,16/Apr/19 09:33
Bug,CASSANDRA-2575,12505353,make forceUserDefinedCompaction actually do what it says,See http://www.mail-archive.com/user@cassandra.apache.org/msg12621.html for motivation,jbellis,jbellis,Low,Resolved,Fixed,27/Apr/11 21:07,16/Apr/19 09:33
Bug,CASSANDRA-2578,12505731,stress performance is artificially limited,"With stress I only get about 7k inserts/s against a single server, and the load and cpu usage from stress is higher than the server.  Pystress gets 15-20k inserts/s against the same machine.  Stress isn't cpu-limited however, so there must be something else holding it back.",xedin,brandon.williams,Normal,Resolved,Fixed,28/Apr/11 22:09,16/Apr/19 09:33
Bug,CASSANDRA-2579,12505738,apache-cassandra-cql-*.jar as a separate artifact,"The CQL jar should not be stored in the -bin.tar.gz artifact, but created separately with its own checksum files.",urandom,urandom,Low,Resolved,Fixed,28/Apr/11 23:52,16/Apr/19 09:33
Bug,CASSANDRA-2580,12505741,stress will not use existing keyspaces,"cassandra-3:/srv/cassandra# tools/stress/bin/stress -n 1 -d cassandra-2 -i 1 -o insert
Created keyspaces. Sleeping 1s for propagation.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
1,1,1,0.049,0
cassandra-3:/srv/cassandra# tools/stress/bin/stress -n 1 -d cassandra-2 -i 1 -o insert
Unable to create stress keyspace: Keyspace already exists.
",xedin,brandon.williams,Normal,Resolved,Fixed,29/Apr/11 01:43,16/Apr/19 09:33
Bug,CASSANDRA-2581,12505751,Rebuffer called excessively during seeks,"When doing an strace tonight, I noticed during memtable flushes that we were only writing 1KB per every write() system call...After diving more into it, it's because of a bug in the seek() code. 

if (newPosition >= bufferOffset + validBufferBytes || newPosition < bufferOffset)

vs.

if (newPosition > (bufferOffset + validBufferBytes) || newPosition < bufferOffset)

Two things I noticed, we shouldn't need to rebuffer if newPosition is equal to bufferOffset + validBufferBytes, second the evaluation was doing (newPosition >= bufferOffset) + validBufferBytes which always seemed to be true.
",lenn0x,lenn0x,Low,Resolved,Fixed,29/Apr/11 04:34,16/Apr/19 09:33
Bug,CASSANDRA-2590,12505899,row delete breaks read repair,"related to CASSANDRA-2589 

Working at CL ALL can get inconsistent reads after row deletion. Reproduced on the 0.7 and 0.8 source. 

Steps to reproduce:

# two node cluster with rf 2 and HH turned off
# insert rows via cli 
# flush both nodes 
# shutdown node 1
# connect to node 2 via cli and delete one row
# bring up node 1
# connect to node 1 via cli and issue get with CL ALL 
# first get returns the deleted row, second get returns zero rows.

RowRepairResolver.resolveSuperSet() resolves a local CF with the old row columns, and the remote CF which is marked for deletion. CF.resolve() does not pay attention to the deletion flags and the resolved CF has both markedForDeletion set and a column with a lower timestamp. The return from resolveSuperSet() is used as the return for the read without checking if the cols are relevant. 

Also when RowRepairResolver.mabeScheduleRepairs() runs it sends two mutations. Node 1 is given the row level deletation, and Node 2 is given a mutation to write the old (and now deleted) column from node 2. I have some log traces for this if needed. 

A quick fix is to check for relevant columns in the RowRepairResolver, will attach shortly.    ",amorton,amorton,Low,Resolved,Fixed,02/May/11 03:44,16/Apr/19 09:33
Bug,CASSANDRA-2592,12505959,CQL greater-than and less-than operators (> and <) result in key ranges that are inclusive of the terms,"This affects range queries against keys, but not index queries.

One possible solution: let the coordinator strip out the extra row in QueryProcessor.",xedin,jbellis,Normal,Resolved,Fixed,02/May/11 17:42,16/Apr/19 09:33
Bug,CASSANDRA-2593,12505999,"CQL: Errors when running unqualified ""select column"" statement (no where clause)","*Seed Data*
{code}
CREATE KEYSPACE cqldb with strategy_class = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=2;

USE cqldb;

CREATE COLUMNFAMILY users (KEY varchar PRIMARY KEY, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint);


INSERT INTO users (KEY, password) VALUES ('user0', 'ch@ngem3');
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3a', 'f', 'TX', '1968');
INSERT INTO users (KEY, password) VALUES ('user2', 'ch@ngem3b');
INSERT INTO users (KEY, password) VALUES ('user3', 'ch@ngem3c');
{code}

*Query #1 - select varchar column*
{code}
cqlsh> select state from users;
u'user1' | u'state',u'TX'
Exception: 'NoneType' object has no attribute 'decode'

cqlsh> select state from users where KEY='user1';
u'user1' | u'state',u'TX'
{code}

*Query #2 - select bigint column*
{code}
cqlsh> select birth_year from users;
Exception: unpack requires a string argument of length 8

cqlsh> select birth_year from users where KEY='user1';
u'user1' | u'birth_year',1968
{code}

*A simple 'SELECT *' with no WHERE clause works fine*
{code}
cqlsh> select * from users;
u'user1' | u'birth_year',1968 | u'gender',u'f' | u'password',u'ch@ngem3a' | u'state',u'TX'
u'user0' | u'password',u'ch@ngem3'
u'user3' | u'password',u'ch@ngem3c'
u'user2' | u'password',u'ch@ngem3b'
{code}
",xedin,cdaw,Low,Resolved,Fixed,03/May/11 00:49,16/Apr/19 09:33
Bug,CASSANDRA-2595,12506077,Tame excessive logging during repairs,"PendingFile.toString is called from logging (i.e. StreamOut:173) which lists all sections in the pending file.

This leads to (in our case multi mb ) ... (59352638,59354005),(59373477,59379520),(59381952,59385368) ... in the log.",doubleday,doubleday,Low,Resolved,Fixed,03/May/11 17:02,16/Apr/19 09:33
Bug,CASSANDRA-2596,12506088,include indexes in snapshots,CFS.snapshot should include index sstables as well.  Since flushing the parent CF (which we do as part of snapshot) also flushes index CFs consistently w/ the parent data this should work as expected.,jbellis,jbellis,Low,Resolved,Fixed,03/May/11 19:18,16/Apr/19 09:33
Bug,CASSANDRA-2597,12506089,inconsistent implementation of 'cumulative distribution function' for Exponential Distribution,"As reported on the mailing list (http://mail-archives.apache.org/mod_mbox/cassandra-dev/201104.mbox/%3CAANLkTimdMSLE8-z0x+0kvzqp7za3AEMLaOFXvd4Z=tvc@mail.gmail.com%3E),

{quote}
I just found there are two implementations of 'cumulative distribution
function' for Exponential Distribution and there are inconsistent :

*FailureDetector*
{code:java}
org.apache.cassandra.gms.ArrivalWindow.p(double)
   double p(double t)
   {
       double mean = mean();
       double exponent = (-1)*(t)/mean;
       return *Math.pow(Math.E, exponent)*;
   }
{code}

*DynamicEndpointSnitch*
{code:java}
org.apache.cassandra.locator.AdaptiveLatencyTracker.p(double)
   double p(double t)
   {
       double mean = mean();
       double exponent = (-1) * (t) / mean;
       return *1 - Math.pow( Math.E, exponent);*
   }
{code}

According to the  Exponential Distribution cumulative distribution function
definition<http://en.wikipedia.org/wiki/Exponential_distribution#Cumulative_distribution_function>,
the later one is correct
{quote}

... however FailureDetector has been working as advertised for some time now.  Does this mean the Snitch version is actually wrong?",thepaul,jbellis,Low,Resolved,Fixed,03/May/11 19:28,16/Apr/19 09:33
Bug,CASSANDRA-2598,12506094,incremental_backups and snapshot_before_compaction duplicate hard links,"See discussion @ http://thread.gmane.org/gmane.comp.db.cassandra.user/15933/

Enabling both incremental_backups and snapshot_before_compaction leads to the same hard links trying to be created.

This gives stacktraces like 

java.io.IOError: java.io.IOException: Unable to create hard link from
/cassandra-data/<keyspace>/<cf>-f-3875-Data.db
to
/cassandra-data/<keyspace>/snapshots/compact-<cf>/<cf>-f-3875-Data.db
(errno 17)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1629)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshot(ColumnFamilyStore.java:1654)
	at org.apache.cassandra.db.Table.snapshot(Table.java:198)
	at org.apache.cassandra.db.CompactionManager.doCompaction(CompactionManager.java:504)
	at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:146)
	at org.apache.cassandra.db.CompactionManager$1.call(CompactionManager.java:112)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.io.IOException: Unable to create hard link from
/cassandra-data/<keyspace>/<cf>-f-3875-Data.db
to
/cassandra-data/<keyspace>/snapshots/compact-<cf>/<cf>-f-3875-Data.db
(errno 17)
	at org.apache.cassandra.utils.CLibrary.createHardLink(CLibrary.java:155)
	at org.apache.cassandra.io.sstable.SSTableReader.createLinks(SSTableReader.java:713)
	at org.apache.cassandra.db.ColumnFamilyStore.snapshotWithoutFlush(ColumnFamilyStore.java:1622)
	... 10 more
",jbellis,mck,Low,Resolved,Fixed,03/May/11 20:19,16/Apr/19 09:33
Bug,CASSANDRA-2599,12506103,Command Line Client has Memtable thresholds: (millions of ops/minutes/MB) last two parameters reversed,"Here's a patch

Index: src/java/org/apache/cassandra/cli/CliClient.java
===================================================================
--- src/java/org/apache/cassandra/cli/CliClient.java	(revision 1099262)
+++ src/java/org/apache/cassandra/cli/CliClient.java	(working copy)
@@ -1325,7 +1325,7 @@
                 sessionState.out.printf(""      Row cache size / save period in seconds: %s/%s%n"", cf_def.row_cache_size, cf_def.row_cache_save_period_in_seconds);
                 sessionState.out.printf(""      Key cache size / save period in seconds: %s/%s%n"", cf_def.key_cache_size, cf_def.key_cache_save_period_in_seconds);
                 sessionState.out.printf(""      Memtable thresholds: %s/%s/%s (millions of ops/minutes/MB)%n"",
-                                cf_def.memtable_operations_in_millions, cf_def.memtable_throughput_in_mb, cf_def.memtable_flush_after_mins);
+                                cf_def.memtable_operations_in_millions, cf_def.memtable_flush_after_mins, cf_def.memtable_throughput_in_mb);
                 sessionState.out.printf(""      GC grace seconds: %s%n"", cf_def.gc_grace_seconds);
                 sessionState.out.printf(""      Compaction min/max thresholds: %s/%s%n"", cf_def.min_compaction_threshold, cf_def.max_compaction_threshold);
                 sessionState.out.printf(""      Read repair chance: %s%n"", cf_def.read_repair_chance);
",,joelastpass,Low,Resolved,Fixed,03/May/11 21:31,16/Apr/19 09:33
Bug,CASSANDRA-2600,12506117,CQL: Range query throws errors when run thru cqlsh but passes in system test,"*It appears the following nose test breaks when run via cqlsh*
{code}
CREATE COLUMNFAMILY StandardLongA (KEY text PRIMARY KEY) WITH comparator = bigint AND default_validation = ascii;

UPDATE StandardLongA SET 1='1', 2='2', 3='3', 4='4' WHERE KEY='aa';
UPDATE StandardLongA SET 5='5', 6='6', 7='8', 9='9' WHERE KEY='ab';
UPDATE StandardLongA SET 9='9', 8='8', 7='7', 6='6' WHERE KEY='ac';
UPDATE StandardLongA SET 5='5', 4='4', 3='3', 2='2' WHERE KEY='ad';
UPDATE StandardLongA SET 1='1', 2='2', 3='3', 4='4' WHERE KEY='ae';
UPDATE StandardLongA SET 1='1', 2='2', 3='3', 4='4' WHERE KEY='af';
UPDATE StandardLongA SET 5='5', 6='6', 7='8', 9='9' WHERE KEY='ag';

cqlsh> SELECT 4 FROM StandardLongA WHERE KEY > 'ad' AND KEY < 'ag';
Internal application error

cqlsh> SELECT * FROM StandardLongA WHERE KEY > 'ad' AND KEY < 'ag';
Internal application error
{code}


{code}

ERROR 21:43:16,880 Internal error processing execute_cql_query
java.lang.AssertionError: [109302822465993666080409141220504733189,104027502549504462599318918375258179002]
	at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:40)
	at org.apache.cassandra.dht.Bounds.<init>(Bounds.java:33)
	at org.apache.cassandra.cql.QueryProcessor.multiRangeSlice(QueryProcessor.java:142)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:507)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1127)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
	at java.lang.Thread.run(Thread.java:636)
{code}

*This test case runs nightly in the system tests and passes*
[http://173.203.89.16:8080/job/CassandraSystem/]
{code}
jenkins@mallen2:~/jobs/Cassandra/workspace$ nosetests test/system/test_cql.py
..................................
----------------------------------------------------------------------
Ran 34 tests in 147.040s

OK

{code}
",xedin,cdaw,Low,Resolved,Fixed,03/May/11 22:54,16/Apr/19 09:33
Bug,CASSANDRA-2604,12506154,EOFException on commitlogs,"I have seen this occasionally since we started testing 0.8.

It happens when reading commitlogs on startups.

However, I have seen it a lot less on 0.8 beta2 (although this is from beta 2)

ERROR [main] 2011-05-04 18:02:38,134 AbstractCassandraDaemon.java (line 330) Exception encountered during startup.
java.io.EOFException
	at java.io.DataInputStream.readByte(DataInputStream.java:250)
	at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:357)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:368)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:252)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:43)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserializeColumns(ColumnFamilySerializer.java:136)
	at org.apache.cassandra.db.ColumnFamilySerializer.deserialize(ColumnFamilySerializer.java:126)
	at org.apache.cassandra.db.RowMutation$RowMutationSerializer.deserialize(RowMutation.java:368)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:256)
	at org.apache.cassandra.db.commitlog.CommitLog.recover(CommitLog.java:157)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:173)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:313)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)

Note that the line numbers on columnserializer may be off due to some local changes, but those changes are in code not executed in this case and I am 100% sure they do not trigger this problem.

I looked on this in the debugger in eclipse on a trunk from 0.8 2 weeks ago, and the interesting thing I saw was that according to the debugger, the offset of the inputstream to the deserializer was already at the end (very last byte) of the underlying bytebuffer but according to the stack, it was trying to read the length of the column name (first read done in the deserialized).
",slebresne,terjem,Normal,Resolved,Fixed,04/May/11 10:00,16/Apr/19 09:33
Bug,CASSANDRA-2605,12506185,Merkle tree splitting can exit early,"There was a small bug introduced by CASSANDRA-2324 that, depending on the key sample token, can make the merkle tree splitting process exit early, potentially resulting in a unnecessary imprecise tree.",slebresne,slebresne,Low,Resolved,Fixed,04/May/11 15:51,16/Apr/19 09:33
Bug,CASSANDRA-2613,12506419,CQL test failures,"{noformat}
FAIL: delete columns from a row
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 360, in test_delete_columns
    assert ['kd', None, None] == r, r
AssertionError: [u'kd']

======================================================================
FAIL: delete columns from multiple rows
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 379, in test_delete_columns_multi_rows
    assert ['kc', None] == r, r
AssertionError: [u'kc']

======================================================================
FAIL: delete entire rows
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 397, in test_delete_rows
    assert ['kd', None, None] == r, r
AssertionError: [u'kd']

======================================================================
FAIL: retrieve multiple columns
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 149, in test_select_columns
    assert ['Row Key', 'ca1', 'col', 'cd1'] == [col_dscptn[0] for col_dscptn in d], d
AssertionError: [('Row Key', 'org.apache.cassandra.db.marshal.UTF8Type', None, None, None, None, None, False), ('col', 'org.apache.cassandra.db.marshal.AsciiType', None, None, None, None, True), ('cd1', 'org.apache.cassandra.db.marshal.AsciiType', None, None, None, None, True)]

======================================================================
FAIL: range should not fail when keys were not set
----------------------------------------------------------------------
Traceback (most recent call last):
  File ""/Library/Python/2.6/site-packages/nose-0.11.3-py2.6.egg/nose/case.py"", line 186, in runTest
    self.test(*self.arg)
  File ""/Users/jonathan/projects/cassandra/svn-0.8/test/system/test_cql.py"", line 252, in test_select_range_with_single_column_results
    assert len(r) == 2
AssertionError
{noformat}",xedin,jbellis,Urgent,Resolved,Fixed,06/May/11 14:53,16/Apr/19 09:33
Bug,CASSANDRA-2615,12506456,"in cassandra-cli, the help command output on validation types should be updated","from cassandra-cli, say type ""help assume""

you will find:
  Supported values are:
    - AsciiType
    - BytesType
    - CounterColumnType (distributed counter column)
    - IntegerType (a generic variable-length integer type)
    - LexicalUUIDType
    - LongType
    - UTF8Type


ok now:
[default@cfs] assume inode comparator as UTF8Type;   
Type 'UTF8Type' was not found. Available: bytes, integer, long, lexicaluuid, timeuuid, utf8, ascii.


so looks like the ""supported type list should be update by taking away the ""Type"" post-fix..

however, on the other hand, you can't really use it:

[default@cfs] update column family inode;                         
Unable to find abstract-type class 'org.apache.cassandra.db.marshal.utf8'

looks like from the update, you still need the ""Type"" (case insensitive?)",xedin,cywjackson,Low,Resolved,Fixed,06/May/11 20:45,16/Apr/19 09:33
Bug,CASSANDRA-2618,12506465,DynamicSnitch race in adding latencies,"ERROR 15:33:48,614 Fatal exception in thread Thread[ReadStage:264,5,main]
java.lang.RuntimeException: java.util.NoSuchElementException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.NoSuchElementException
	at java.util.concurrent.LinkedBlockingDeque.removeFirst(LinkedBlockingDeque.java:401)
	at java.util.concurrent.LinkedBlockingDeque.remove(LinkedBlockingDeque.java:621)
	at org.apache.cassandra.locator.AdaptiveLatencyTracker.add(DynamicEndpointSnitch.java:288)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.receiveTiming(DynamicEndpointSnitch.java:202)
	at org.apache.cassandra.net.MessagingService.addLatency(MessagingService.java:152)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:642)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
ERROR 15:33:48,615 Fatal exception in thread Thread[ReadStage:264,5,main]
java.lang.RuntimeException: java.util.NoSuchElementException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.util.NoSuchElementException
	at java.util.concurrent.LinkedBlockingDeque.removeFirst(LinkedBlockingDeque.java:401)
	at java.util.concurrent.LinkedBlockingDeque.remove(LinkedBlockingDeque.java:621)
	at org.apache.cassandra.locator.AdaptiveLatencyTracker.add(DynamicEndpointSnitch.java:288)
	at org.apache.cassandra.locator.DynamicEndpointSnitch.receiveTiming(DynamicEndpointSnitch.java:202)
	at org.apache.cassandra.net.MessagingService.addLatency(MessagingService.java:152)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:642)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more

What is happening that AdaptiveLatencyTracker.add is trying to add a latency, but the deque is full, so it makes a second effort to remove an entry from the deque and then try to add again.  However, when it tries to remove, the deque has already been emptied by DES.reset call clear() on all the ALTs.  This bug has existed for a long time, but it's very rare and difficult to trigger.",brandon.williams,brandon.williams,Low,Resolved,Fixed,06/May/11 22:27,16/Apr/19 09:32
Bug,CASSANDRA-2619,12506473,secondary index not dropped until restart,"when dropping the secondary index (via cassandra-cli), the describe keyspace still shows the Built index entry. Only after a restart of the CassandraDaemon then the Built Index entry is gone. This seems indicate a problem with the index not really been dropped completed.

to test, use a single node, create an index, then drop it from the cli (issue an update column family ... with metadata fields but not the index info)

below is the original:

  Column Families:
    ColumnFamily: inode
    ""Stores file meta data""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 0.0/14400
      Memtable thresholds: 0.103125/22/1440 (millions of ops/MB/minutes)
      GC grace seconds: 60
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      {color:red}Built indexes: [inode.path, inode.sentinel]{color}
      Column Metadata:
        Column Name: path (70617468)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
          {color:red}Index Name: path
          Index Type: KEYS{color}
        Column Name: sentinel (73656e74696e656c)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
          {color:red}Index Name: sentinel
          Index Type: KEYS{color}

issue an update:
{noformat}

[default@unknown] use cfs;
Authenticated to keyspace: cfs
[default@cfs] update column family inode with comparator=BytesType and column_metadata=[{column_name:70617468, validation_class:BytesType}, {column_name:73656e74696e656c,validation_class:BytesType}];
fca46d00-783c-11e0-0000-242d50cf1fff
Waiting for schema agreement...
... schemas agree across the cluster
{noformat}

describe the keyspace again:
Keyspace: cfs:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [Brisk:1, Cassandra:0]
  Column Families:
    ColumnFamily: inode
    ""Stores file meta data""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 0.0/14400
      Memtable thresholds: 0.103125/22/1440 (millions of ops/MB/minutes)
      GC grace seconds: 60
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      {color:red}Built indexes: [inode.path, inode.sentinel]{color}
      Column Metadata:
        Column Name: path (70617468)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
        Column Name: sentinel (73656e74696e656c)
          Validation Class: org.apache.cassandra.db.marshal.BytesType

*notice the red line on Built Indexes*

restart CassandraDaemon, describe again:

Keyspace: cfs:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [Brisk:1, Cassandra:0]
  Column Families:
    ColumnFamily: inode
    ""Stores file meta data""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 0.0/14400
      Memtable thresholds: 0.103125/22/1440 (millions of ops/MB/minutes)
      GC grace seconds: 60
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      {color:red}Built indexes: []{color}
      Column Metadata:
        Column Name: path (70617468)
          Validation Class: org.apache.cassandra.db.marshal.BytesType
        Column Name: sentinel (73656e74696e656c)
          Validation Class: org.apache.cassandra.db.marshal.BytesType


on another note, upon re-create the index, it does not appear the index is actually rebuilt. There is no need to restart CassandraDaemon for the Built Index to show up from the describe. But the update goes very fast. We could tell the index is not being rebuilt because we were getting NPE from:

{noformat}
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:51)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.satisfies(ColumnFamilyStore.java:1647)
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1594)
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
{noformat}
and after re-create the index, the exception resurface (the exception does not surface upon drop).

If we drop the index files and remove them, then re-create the index, the NPE is resolved: 

{noformat}
$ find /var/lib/cassandra/data/cfs -name ""*path*"" -o -name ""*sentinel* -exec rm {} \;""
{noformat}",jbellis,cywjackson,Normal,Resolved,Fixed,07/May/11 00:36,16/Apr/19 09:32
Bug,CASSANDRA-2622,12506497,Select * doesn't include row key,,jbellis,jbellis,Normal,Resolved,Fixed,07/May/11 13:08,16/Apr/19 09:32
Bug,CASSANDRA-2623,12506513,CLI escaped single quote parsing gives errors,"Escaping quotes in cli commands causes parsing errors.


some examples::::
No need to create columns etc, it doesn't get through parsing the expression::

cassandra-cli

1. 
set column['KEY+vals'][VALUE] = 'VAL\'' ;
Syntax error at position 41: mismatched character '<EOF>' expecting '''

2.
set column['KEY+val\'s'][VALUE] = 'VAL' ;
Syntax error at position 41: mismatched character '<EOF>' expecting '''

3.
set column['KEY+vals\''][VALUE] = 'VAL\'' ;
Syntax error at position 38: unexpected ""\"" for `set column['KEY+vals\''][VALUE] = 'VAL\'' ;`.
",xedin,rday,Low,Resolved,Fixed,07/May/11 21:21,16/Apr/19 09:32
Bug,CASSANDRA-2624,12506525,SimpleStrategy w/o replication_factor,"It is possible to create a new keyspace using {{SimpleStrategy}} _without_ specifying the {{replication_factor}} option.  Things get more interesting if you shut the node down, since it will refuse to restart (throwing a {{ConfigurationException}}).",jbellis,urandom,Normal,Resolved,Fixed,08/May/11 02:10,16/Apr/19 09:32
Bug,CASSANDRA-2625,12506550,auto bootstrapping a node into a cluster without a schema silently fails,"from http://www.mail-archive.com/user@cassandra.apache.org/msg13001.html

StorageService.joinRing() aborts the auto bootstrap process if the cluster does not have a schema defined. It looks like the node is left in the ""Joining"" mode and there is no logging. 

There could be a schema defined and no data loaded, so just having a schema does not make the token selection any better. And BootStrapper.bootstrap() handles their been no non system tables.

Can we let the bootstrap process continue ?",slebresne,amorton,Low,Resolved,Fixed,08/May/11 22:38,16/Apr/19 09:32
Bug,CASSANDRA-2626,12506563,stack overflow while compacting,"This is a trunk build from May 3.

After adding  CASSANDRA-2401, I have gotten the following on several nodes.
I am not 100% sure right now if it is related to 2401 but it may seem likely.

Unfortunately, as often is the case with stack overflows, I don't see the start of the stack

ERROR [CompactionExecutor:17] 2011-05-09 07:56:32,479 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[CompactionExecutor:17,1,main]
java.lang.StackOverflowError
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
        at java.util.Collections$UnmodifiableCollection.size(Collections.java:998)
",skamio,terjem,Normal,Resolved,Fixed,09/May/11 05:04,16/Apr/19 09:32
Bug,CASSANDRA-2627,12506574,Don't allow {LOCAL|EACH}_QUORUM unless strategy is NTS,"There is not check when {LOCAL|EACH}_QUORUM is used than we do use NTS, hence using such CL with simpleStrategy for instance result in
{noformat}
ERROR [pool-1-thread-1] 2011-05-09 10:44:29,728 Cassandra.java (line 2960) Internal error processing insert
java.lang.ClassCastException: org.apache.cassandra.locator.SimpleStrategy cannot be cast to org.apache.cassandra.locator.NetworkTopologyStrategy
...
{noformat}",slebresne,slebresne,Low,Resolved,Fixed,09/May/11 08:48,16/Apr/19 09:32
Bug,CASSANDRA-2628,12506607,"Empty Result with Secondary Index Queries with ""limit 1""","Empty result is returned by secondary index queries with ""limit 1"".  Cassandra returns correct result for other numbers than ""1"" (e.g. limit 2, limit 3, etc.).  

You can reproduce the problem with programs attached on CASSANDRA-2406.  

- 1. Start Cassandra cluster. It consists of 3 cassandra nodes and distributes data by ByteOrderedPartitioner. Initial tokens of those nodes are [""31"", ""32"", ""33""].
- 2. Create keyspace and column family, according to ""create_table.cli"",
- 3. Execute ""secondary_index_insertv2.py"", inserting a few hundred columns to cluster
- 4. Here, when you first use cassandra-cli and execute following lines, you can get correct result.  

{quote}
% bin/cassandra-cli
[default@unknown] connect localhost/9160;
[default@unknown] use SampleKS;
[default@SampleKS] get SampleCF where up = 'up' limit 3;               
-------------------
RowKey: 150
=> (column=date, value=150, timestamp=1304937931)
=> (column=up, value=up, timestamp=1304937931)
-------------------
RowKey: 151
=> (column=date, value=151, timestamp=1304937932)
=> (column=up, value=up, timestamp=1304937932)
-------------------
RowKey: 152
=> (column=date, value=152, timestamp=1304937932)
=> (column=up, value=up, timestamp=1304937932)
3 Rows Returned.  
{quote}

On the other hand, if you set limit to ""1"", you can reproduce the problem.

{quote}
[default@SampleKS] get SampleCF where up = 'up' and date > 150 limit 1;
0 Row Returned.
{quote}

There are two factors to cause this problem:
- 1. scanned first column doesn't match at specified clause like ""date > 150"".
- 2. ""limit 1""

Only one factor doesn't cause problem.  For example, I can get correct data when I specify as following:

- ""limit 1"" -> ""limit 2""
{quote}
[default@SampleKS] get SampleCF where up = 'up' and date > 150 limit 2;
-------------------
RowKey: 151
=> (column=date, value=151, timestamp=1304937932)
=> (column=up, value=up, timestamp=1304937932)
-------------------
RowKey: 152
=> (column=date, value=152, timestamp=1304937932)
=> (column=up, value=up, timestamp=1304937932)
2 Rows Returned.
{quote}

- ""date > 150"" -> ""date >= 150""
{quote}
[default@SampleKS] get SampleCF where up = 'up' and date >= 150 limit 1;
-------------------
RowKey: 150
=> (column=date, value=150, timestamp=1304937931)
=> (column=up, value=up, timestamp=1304937931)
1 Row Returned.
{quote}",slebresne,muga_nishizawa,Normal,Resolved,Fixed,09/May/11 13:29,16/Apr/19 09:32
Bug,CASSANDRA-2631,12506778,Replaying a commitlog entry from a dropped keyspace will cause an error,,jbellis,jbellis,Normal,Resolved,Fixed,10/May/11 23:35,16/Apr/19 09:32
Bug,CASSANDRA-2632,12506792,ConfigurationException when starting a node after deleting LocationInfo SStables,"from http://www.mail-archive.com/user@cassandra.apache.org/msg13170.html

SystemTable.checkHealth() assumes that if the LOCATION_KEY row is not in the STATUS system CF their should be no other files in the system data directory. If it's safe to delete the LocationInfo sstables this stops the server restarting.

I think the intention of the check is to assert that the reason the row was not found is that there is no data in the STATUS CF. ",amorton,amorton,Low,Resolved,Fixed,11/May/11 01:19,16/Apr/19 09:32
Bug,CASSANDRA-2633,12506829,Keys get lost in bootstrap,"When bootstrapping a new node, the key at the upper end of the new node's range can get lost.  To reproduce:

* Set up one cassandra node, create a keyspace and column family and perform some inserts
* Read every row back
* Bootstrap a second node
* Read every row back

You find one row is missing, whose row key is exactly equal to the token the new node gets (for OPP - for RP it's the key whose hash is equal to the token).  If you don't do the reads after the inserts, the key is not lost.  I tracked the problem down to o.a.c.io.sstable.SSTableReader in getPosition.  The problem is that the cached position is used if it is there (so only if the reads were performed).  But this is incorrect because the cached position is the start of the row, not the end.  This means the end row itself is not transferred.  This causes the last key in the range to get lost.

Although I haven't seen it, this may occur during antientropy repairs too.

The attached patch (against the 0.7 branch) fixes it by not using the cache for Operator.GT.  I haven't tested with 0.8 but from looking at the code I think the problem is present.

This might be related to CASSANDRA-1992",richardlow,richardlow,Urgent,Resolved,Fixed,11/May/11 10:36,16/Apr/19 09:32
Bug,CASSANDRA-2637,12506865,bloom filter true positives not counted unless key cache is enabled,,jbellis,jbellis,Low,Resolved,Fixed,11/May/11 15:42,16/Apr/19 09:32
Bug,CASSANDRA-2638,12506881,Migrations announce on startup attempts to set local gossip state before gossiper is started.,"AbstractCassandraDemon calls MigrationManager.applyMigrations() before the gossiper is initialized (via SS.initServer()).

MM.applyMigrations tries to set the local gossip state before it is initialized via G.start().",gdusbabek,gdusbabek,Normal,Resolved,Fixed,11/May/11 17:37,16/Apr/19 09:32
Bug,CASSANDRA-2642,12506986,CounterColumn Increments lost after restart,"While testing the 0.8.0-rc1; 

I've come across this problem. In order to reproduce please follow the steps:

- create a ColumnFamily named Counters
- do a few increments on a column
- get column value
- kill cassandra
- start cassandra
- get the column value

please see the cli-history.txt or pastebin http://pastebin.com/9jYdDiRY",slebresne,uctopcu,Urgent,Resolved,Fixed,12/May/11 13:47,16/Apr/19 09:32
Bug,CASSANDRA-2643,12507010,read repair/reconciliation breaks slice based iteration at QUORUM,"In short, I believe iterating over columns is impossible to do reliably with QUORUM due to the way reconciliation works.

The problem is that the SliceQueryFilter is executing locally when reading on a node, but no attempts seem to be made to consider limits when doing reconciliation and/or read-repair (RowRepairResolver.resolveSuperset() and ColumnFamily.resolve()).

If a node slices and comes up with 100 columns, and another node slices and comes up with 100 columns, some of which are unique to each side, reconciliation results in > 100 columns in the result set. In this case the effect is limited to ""client gets more than asked for"", but the columns still accurately represent the range. This is easily triggered by my test-case.

In addition to the client receiving ""too many"" columns, I believe some of them will not be satisfying the QUORUM consistency level for the same reasons as with deletions (see discussion below).

Now, there *should* be a problem for tombstones as well, but it's more subtle. Suppose A has:

  1
  2
  3
  4
  5
  6

and B has:

  1
  del 2
  del 3
  del 4
  5
  6 

If you now slice 1-6 with count=3 the tombstones from B will reconcile with those from A - fine. So you end up getting 1,5,6 back. This made it a bit difficult to trigger in a test case until I realized what was going on. At first I was ""hoping"" to see a ""short"" iteration result, which would mean that the process of iterating until you get a short result will cause spurious ""end of columns"" and thus make it impossible to iterate correctly.

So; due to 5-6 existing (and if they didn't, you legitimately reached end-of-columns) we do indeed get a result of size 3 which contains 1,5 and 6. However, only node B would have contributed columns 5 and 6; so there is actually no QUORUM consistency on the co-ordinating node with respect to these columns. If node A and C also had 5 and 6, they would not have been considered.

Am I wrong?

In any case; using script I'm about to attach, you can trigger the over-delivery case very easily:

(0) disable hinted hand-off to avoid that interacting with the test
(1) start three nodes
(2) create ks 'test' with rf=3 and cf 'slicetest'
(3) ./slicetest.py hostname_of_node_C insert # let it run for a few seconds, then ctrl-c
(4) stop node A
(5) ./slicetest.py hostname_of_node_C insert # let it run for a few seconds, then ctrl-c
(6) start node A, wait for B and C to consider it up
(7) ./slicetest.py hostname_of_node_A slice # make A co-ordinator though it doesn't necessarily matter

You can also pass 'delete' (random deletion of 50% of contents) or 'deleterange' (delete all in [0.2,0.8]) to slicetest, but you don't trigger a short read by doing that (see discussion above).
",byronclark,scode,Urgent,Resolved,Fixed,12/May/11 16:27,16/Apr/19 09:32
Bug,CASSANDRA-2644,12507049,Make bootstrap retry,"We ran into a situation where we had rpc_timeout set to 1 second, and the node needing to compute the token took over a second (1.6 seconds). The bootstrapping node hangs forever without getting a token because the expiring map removes it before the reply comes back.",lenn0x,lenn0x,Normal,Resolved,Fixed,12/May/11 22:22,16/Apr/19 09:32
Bug,CASSANDRA-2647,12507147,Cassandra can't find jamm on startup,"I installed the Debian package (from http://www.apache.org/dist/cassandra/debian unstable) of Cassandra 0.8beta2 on Ubuntu 10.04 with the sun jdk over a working copy of 0.7.2. It broke on restart.
On startup it gives this:
{code}
Error occurred during initialization of VM
agent library failed to init: instrument
Error opening zip file or JAR manifest missing : /lib/jamm-0.2.2.jar
{code}
/etc/cassandra/cassandra-env.sh contains this:
{code}
# add the jamm javaagent
check_openjdk=$(java -version 2>&1 | awk '{if (NR == 2) {print $1}}')
if [ ""$check_openjdk"" != ""OpenJDK"" ]
then
    JVM_OPTS=""$JVM_OPTS -javaagent:$CASSANDRA_HOME/lib/jamm-0.2.2.jar""
fi
{code}
By default, CASSANDRA_HOME is not set, so it's looking in /lib for this jar. It seems CASSANDRA_HOME should be set to /usr/share/cassandra, since that's where jamm-0.2.2.jar is installed, but that means the path is still wrong.

I set CASSANDRA_HOME to /usr/share/cassandra and changed the JVM_OPTS line to this:
{code}
    JVM_OPTS=""$JVM_OPTS -javaagent:$CASSANDRA_HOME/jamm-0.2.2.jar""
{code}

and then cassandra started ok.

Is this a bug or did I miss something?

I also noticed that this line appears to be the only use of CASSANDRA_HOME.",urandom,synchrom,Low,Resolved,Fixed,13/May/11 16:47,16/Apr/19 09:32
Bug,CASSANDRA-2649,12507173,work-around schema disagreements from cqlsh,"It is handy to be able to put CQL statements in a flat-file and load them by redirecting to {{cqlsh}} stdin, but this can fail on a cluster when executing statements that modify schema.

The attached patch works around this problem by retrying up to 3 times, with a progressive delay after each attempt.  A better solution would probably be to compare schema versions, but this seems to work well enough, and is better than _not_ handling it at all.",urandom,urandom,Low,Resolved,Fixed,13/May/11 20:16,16/Apr/19 09:32
Bug,CASSANDRA-2651,12507209,Inferred Rack and DC Values Should be Unsigned,RackInferringSnitch formats IP address octets as signed byte values when inferring rack and data center values.,,jpisk,Low,Resolved,Fixed,14/May/11 08:27,16/Apr/19 09:32
Bug,CASSANDRA-2652,12507223,Hinted handoff needs to adjust page size for lage columns to avoid OOM,"Example OOM:
{noformat}
java.lang.OutOfMemoryError: Java heap space
	at org.apache.cassandra.io.util.BufferedRandomAccessFile.readBytes(BufferedRandomAccessFile.java:269)
	at org.apache.cassandra.utils.ByteBufferUtil.read(ByteBufferUtil.java:356)
	at org.apache.cassandra.utils.ByteBufferUtil.readWithLength(ByteBufferUtil.java:318)
	at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:99)
	at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:248)
	at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:268)
	at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:227)
	at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(ConcurrentSkipListMap.java:1493)
	at java.util.concurrent.ConcurrentSkipListMap.<init>(ConcurrentSkipListMap.java:1443)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:379)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:362)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:322)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader$IndexedBlockFetcher.getNextBlock(IndexedSliceReader.java:179)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:121)
	at org.apache.cassandra.db.columniterator.IndexedSliceReader.computeNext(IndexedSliceReader.java:49)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:116)
	at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1267)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1195)
	at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:138)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:331)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
{noformat}",jbellis,jbellis,Low,Resolved,Fixed,14/May/11 14:52,16/Apr/19 09:32
Bug,CASSANDRA-2653,12507224,index scan errors out when zero columns are requested,"As reported by Tyler Hobbs as an addendum to CASSANDRA-2401,

{noformat}
ERROR 16:13:38,864 Fatal exception in thread Thread[ReadStage:16,5,main]
java.lang.AssertionError: No data found for SliceQueryFilter(start=java.nio.HeapByteBuffer[pos=10 lim=10 cap=30], finish=java.nio.HeapByteBuffer[pos=17 lim=17 cap=30], reversed=false, count=0] in DecoratedKey(81509516161424251288255223397843705139, 6b657931):QueryPath(columnFamilyName='cf', superColumnName='null', columnName='null') (original filter SliceQueryFilter(start=java.nio.HeapByteBuffer[pos=10 lim=10 cap=30], finish=java.nio.HeapByteBuffer[pos=17 lim=17 cap=30], reversed=false, count=0]) from expression 'cf.626972746864617465 EQ 1'
	at org.apache.cassandra.db.ColumnFamilyStore.scan(ColumnFamilyStore.java:1517)
	at org.apache.cassandra.service.IndexScanVerbHandler.doVerb(IndexScanVerbHandler.java:42)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}",slebresne,jbellis,Low,Resolved,Fixed,14/May/11 15:02,16/Apr/19 09:32
Bug,CASSANDRA-2660,12507480,BRAF.sync() bug can cause massive commit log write magnification,"This was discovered, fixed and tested on 0.7.5. Cursory examination shows it should still be an issue on trunk/0.8. If people otherwise agree with the patch I can rebase if necessary.

Problem:

BRAF.flush() is actually broken in the sense that it cannot be called without close co-operation with the caller. rebuffer() does the co-op by adjusting bufferOffset and validateBufferBytes appropriately, by sync() doesn't. This means sync() is broken, and sync() is used by the commit log.

The attached patch moves the bufferOffset/validateBufferBytes handling out into resetBuffer() and has both flush() and rebuffer() call that. This makes sync() safe.

What happened was that for batched commit log mode, every time sync() was called the data buffered so far would get written to the OS and fsync():ed. But until rebuffer() is called for other reasons as part of the write path, all subsequent sync():s would result in the very same data (plus whatever was written since last time) being re-written and fsync():ed again. So first you write+fsync N bytes, then N+N1, then N+N1+N2... (each N being a batch), until at some point you trigger a rebuffer() and it starts all over again.

The result is that you see *a lot* more writes to the commit log than are in fact written to the BRAF. And these writes translate into actual real writes to the underlying storage device due to fsync(). We had crazy numbers where we saw spikes upwards of 80 mb/second where the actual throughput was more like ~ 1 mb second of data to the commit log.

(One can make a possibly weak argument that it is also functionally incorrect as I can imagine implementations where re-writing the same blocks does copy-on-write in such a way that you're not necessarily guaranteed to see before-or-after data, particularly in case of partial page writes. However that's probably not a practical issue.)

Worthy of noting is that this probably causes added difficulties in fsync() latencies since the average fsync() will contain a lot more data. Depending on I/O scheduler and underlying device characteristics, the extra writes *may* not have a detrimental effect, but I think it's pretty easy to point to cases where it will be detrimental - in particular if the commit log is on a non-battery backed drive. Even with a nice battery backed RAID with the commit log on, the size of the writes probably contributes to difficulty in making the write requests propagate down without being starved by reads (but this is speculation, not tested, other than that I've observed commit log writer starvation that seemed excessive).

This isn't the first subtle BRAF bug. What are people's thoughts on creating separate abstractions for streaming I/O that can perhaps be a lot more simple, and use BRAF only for random reads in response to live traffic? (Not as part of this JIRA, just asking in general.)
",scode,scode,Low,Resolved,Fixed,17/May/11 12:22,16/Apr/19 09:32
Bug,CASSANDRA-2662,12507590,Nodes get ignored by dynamic snitch when read repair chance is zero,"DynamicEndpointSnitch falls back to subsnitch when one of the scores of the endpoints being compared is missing.

This leads to a stable order of hosts until reads will lead to recorded scores. 
If setting read repair chance to 0 and reads are performed with quorum then (rf - # quorum nodes) will never get reads.",brandon.williams,doubleday,Low,Resolved,Fixed,18/May/11 10:55,16/Apr/19 09:32
Bug,CASSANDRA-2668,12507780,don't perform HH to client-mode nodes,,jbellis,jbellis,Low,Resolved,Fixed,19/May/11 16:54,16/Apr/19 09:32
Bug,CASSANDRA-2669,12507784,Scrub does not close files,"After scrubbing I find that cassandra process still holds file handles to the deleted sstables:

{noformat}
root@blnrzh047:/mnt/cassandra# jps
6932 Jps
32359 CassandraDaemon
32398 CassandraJmxHttpServer

root@blnrzh047:/mnt/cassandra# du -sh .
315G	.

root@blnrzh047:/mnt/cassandra# df -h .
Filesystem            Size  Used Avail Use% Mounted on
/dev/md0              1.1T  626G  420G  60% /mnt/cassandra


root@blnrzh047:/mnt/cassandra# lsof | grep /mnt
java      32359        root  356r      REG                9,0           24    4194599 /mnt/cassandra/data/system/Migrations-f-13-Index.db (deleted)
java      32359        root  357r      REG                9,0       329451    4194547 /mnt/cassandra/data/system/HintsColumnFamily-f-588-Data.db (deleted)
java      32359        root  358r      REG                9,0           22    4194546 /mnt/cassandra/data/system/HintsColumnFamily-f-588-Index.db (deleted)
java      32359        root  359r      REG                9,0       313225    4194534 /mnt/cassandra/data/system/HintsColumnFamily-f-587-Data.db (deleted)
java      32359        root  360r      REG                9,0           22    4194494 /mnt/cassandra/data/system/HintsColumnFamily-f-587-Index.db (deleted)
java      32359        root  361r      REG                9,0        30452    4194636 /mnt/cassandra/data/system/Schema-f-13-Data.db (deleted)
java      32359        root  362r      REG                9,0          484    4194635 /mnt/cassandra/data/system/Schema-f-13-Index.db (deleted)
{noformat}

I guess there's a missing dataFile.close() in CompactionManager:648
",jbellis,doubleday,Low,Resolved,Fixed,19/May/11 17:14,16/Apr/19 09:32
Bug,CASSANDRA-2672,12507821,The class o.a.c.cql.jdbc.TypedColumn needs to be declared public,"The implementation of {{ResultSet}} in the JDBC package provides a method: {{unwrap( Class<T> interfaceName)}} in order to allow some of the methods in the {{ResultSet}} implementation {{Class}} to be exposed.

The implementation currently restricts the access to only one acceptable interface: {{CassandraResultSet}}.

Two of the getters in that interface return {{TypedColumn}} which cleverly contains the ""Cassandra"" details of the desired column such as raw column details, and the {{AbstractType}} of the validator and the comparator among others. (Nice!) Unfortunately the {{TypedColumn}} class is not public so is it is not accessible to the callers code.


",ardot,ardot,Low,Resolved,Fixed,19/May/11 22:14,16/Apr/19 09:32
Bug,CASSANDRA-2673,12507836,AssertionError post truncate,"I had 3 nodes with about 100G in a CF. I run truncate on that CF from cassandra-cli. Then I run cleanup for that CF. I saw this exception shortly after.

 INFO [FlushWriter:5] 2011-05-20 02:56:42,699 Memtable.java (line 157) Writing Memtable-body@1278535630(26722 bytes, 1 operations)
 INFO [FlushWriter:5] 2011-05-20 02:56:42,706 Memtable.java (line 172) Completed flushing /var/lib/cassandra/data/dnet/body-f-1892-Data.db (26915 bytes)
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,981 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1892
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,982 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1889
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,983 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1890
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,983 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1888
 INFO [NonPeriodicTasks:1] 2011-05-20 02:59:55,984 SSTable.java (line 147) Deleted /var/lib/cassandra/data/dnet/body-f-1887
 INFO [CompactionExecutor:1] 2011-05-20 03:02:08,724 CompactionManager.java (line 750) Cleaned up to /var/lib/cassandra/data/dnet/body-tmp-f-1891-Data.db.  25,629,365,173 to 25,629,365,173 (~100% of original) bytes for 884,546 keys.  Time: 1,165,900ms.
ERROR [CompactionExecutor:1] 2011-05-20 03:02:08,727 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[CompactionExecutor:1,1,main]
java.lang.AssertionError
	at org.apache.cassandra.io.sstable.SSTableTracker.replace(SSTableTracker.java:108)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:1037)
	at org.apache.cassandra.db.CompactionManager.doCleanupCompaction(CompactionManager.java:769)
	at org.apache.cassandra.db.CompactionManager.access$500(CompactionManager.java:56)
	at org.apache.cassandra.db.CompactionManager$2.call(CompactionManager.java:173)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",jbellis,ithkuil,Low,Resolved,Fixed,20/May/11 01:06,16/Apr/19 09:32
Bug,CASSANDRA-2675,12507853,java.io.IOError: java.io.EOFException with version 0.7.6,"I use the following data-model

column_metadata: []
name: Customers
column_type: Super
gc_grace_seconds: 60

I have a super-column-family with a single row.
Within this row I have a single super-column.
Within this super-column, I concurrently create, read and delete columns.

I have three threads:

- Do in a loop: add a column to the super-column.
- Do in a loop: delete a random column from the super-column.
- Do in a loop: read the super-column (with all columns).

After running the above threads concurrently, I always receive one of the following errors:

ERROR 17:09:57,036 Fatal exception in thread Thread[ReadStage:81,5,main]
java.io.IOError: java.io.EOFException
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:252)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:268)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:227)
        at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(Unknown Source)
        at java.util.concurrent.ConcurrentSkipListMap.<init>(Unknown Source)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:379)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:362)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:322)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:116)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1267)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1195)
        at org.apache.cassandra.db.Table.getRow(Table.java:324)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:451)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readByte(Unknown Source)
        at org.apache.cassandra.utils.ByteBufferUtil.readShortLength(ByteBufferUtil.java:324)
        at org.apache.cassandra.utils.ByteBufferUtil.readWithShortLength(ByteBufferUtil.java:335)
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:71)
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:248)
        ... 30 more

java.io.IOError: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:252)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:268)
        at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:227)
        at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(Unknown Source)
        at java.util.concurrent.ConcurrentSkipListMap.<init>(Unknown Source)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:379)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:362)
        at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:322)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
        at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
        at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
        at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
        at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
        at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:116)
        at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
        at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1385)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1262)
        at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1190)
        at org.apache.cassandra.db.Table.getRow(Table.java:324)
        at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
        at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:451)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
        at java.lang.Thread.run(Unknown Source)
Caused by: org.apache.cassandra.db.ColumnSerializer$CorruptColumnException: invalid column name length 0
        at org.apache.cassandra.db.ColumnSerializer.deserialize(ColumnSerializer.java:73)
        at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:248)
        ... 30 more 

ERROR 11:02:19,824 Fatal exception in thread Thread[ReadStage:3404,5,main]
java.io.IOError: java.io.IOException: mmap segment underflow; remaining is 660267 but 758592100 requested
	at org.apache.cassandra.io.util.ColumnIterator.deserializeNext(ColumnSortedMap.java:252)
	at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:268)
	at org.apache.cassandra.io.util.ColumnIterator.next(ColumnSortedMap.java:227)
	at java.util.concurrent.ConcurrentSkipListMap.buildFromSorted(Unknown Source)
	at java.util.concurrent.ConcurrentSkipListMap.<init>(Unknown Source)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:379)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:362)
	at org.apache.cassandra.db.SuperColumnSerializer.deserialize(SuperColumn.java:322)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:79)
	at org.apache.cassandra.db.columniterator.SimpleSliceReader.computeNext(SimpleSliceReader.java:40)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.columniterator.SSTableSliceIterator.hasNext(SSTableSliceIterator.java:108)
	at org.apache.commons.collections.iterators.CollatingIterator.set(CollatingIterator.java:283)
	at org.apache.commons.collections.iterators.CollatingIterator.least(CollatingIterator.java:326)
	at org.apache.commons.collections.iterators.CollatingIterator.next(CollatingIterator.java:230)
	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:69)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:136)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:131)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:116)
	at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1390)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1267)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1195)
	at org.apache.cassandra.db.Table.getRow(Table.java:324)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:63)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:451)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
	at java.lang.Thread.run(Unknown Source)",slebresne,kochen,Low,Resolved,Fixed,20/May/11 09:09,16/Apr/19 09:32
Bug,CASSANDRA-2678,12507930,Incorrect NetworkTopolgyStrategy Options on upgrade from 0.7.5,"After an upgrade from 0.7.5 to 0.8.0-rc1 on a 10 node, single DC ring configured with NTS, operations fail due to an inability to reach replicas in the 'second datacenter':

ERROR [pool-2-thread-8] 2011-05-17 12:15:23,145 Cassandra.java (line 3294) Internal error processing insert
java.lang.IllegalStateException: datacenter (replication_factor) has no more endpoints, (3) replicas still needed
        at org.apache.cassandra.locator.NetworkTopologyStrategy.calculateNaturalEndpoints(NetworkTopologyStrategy.java:118)
        at org.apache.cassandra.locator.AbstractReplicationStrategy.getNaturalEndpoints(AbstractReplicationStrategy.java:100)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1611)
        at org.apache.cassandra.service.StorageService.getNaturalEndpoints(StorageService.java:1599)
        at org.apache.cassandra.service.StorageProxy.getWriteEndpoints(StorageProxy.java:217)
        at org.apache.cassandra.service.StorageProxy.performWrite(StorageProxy.java:202)
        at org.apache.cassandra.service.StorageProxy.mutate(StorageProxy.java:154)
        at org.apache.cassandra.thrift.CassandraServer.doInsert(CassandraServer.java:557)
        at org.apache.cassandra.thrift.CassandraServer.internal_insert(CassandraServer.java:434)
        at org.apache.cassandra.thrift.CassandraServer.insert(CassandraServer.java:442)
        at org.apache.cassandra.thrift.Cassandra$Processor$insert.process(Cassandra.java:3286)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
DEBUG [ScheduledTasks:1] 2011-05-17 12:15:33,975 StorageLoadBalancer.java (line 334) Disseminating load info ...

On checking the keyspace definition with cassandra-cli, it appears that 0.8.0-rc1 considered the 'replication_factor:3' configuration in the older version as a DC name in part of the DC replication strategy:

  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
    Options: [replication_factor:3, DC1:3]

I attempted to remove replication_factor as a DC using the 'update keyspace' command, but it would persist.  I was able to remove the DC1:3 and use:

update keyspace MyKeyspace with strategy_options=[{replication_factor:3}];

then changed the topology properties file, renamed DC1 to replication_factor, and it worked - so there is a workaround. ",jbellis,ctrahman,Low,Resolved,Fixed,20/May/11 20:57,16/Apr/19 09:32
Bug,CASSANDRA-2680,12507952,range scan doesn't repair missing rows,"Range scans do not do digest queries but they do compare all the replicas they receive and repair any discrepancies in the background.  (Thus, to get comparable behavior to normal read repair, CL.ALL must be used.)

The bug is that currently, replicas that omit a row entirely will be ignored and that row will not be sent to them.  ",jbellis,jbellis,Low,Resolved,Fixed,21/May/11 02:08,16/Apr/19 09:32
Bug,CASSANDRA-2682,12507976,UUIDType assumes ByteBuffer has an accessible backing array,"I'm very embarrassed to say this got left out in the UUIDType, but it's not doing a hasArray() check on the bytebuffers passed to it, causing it to break.  I'll make a patch to fix it.",edanuff,edanuff,Normal,Resolved,Fixed,21/May/11 17:08,16/Apr/19 09:32
Bug,CASSANDRA-2684,12508014,IntergerType uses Thrift method that attempts to unsafely access backing array of ByteBuffer and fails,"I get the following exception:

{noformat}
ERROR 13:27:38,153 Fatal exception in thread Thread[ReadStage:36,5,main]
java.lang.RuntimeException: java.lang.UnsupportedOperationException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.lang.UnsupportedOperationException
	at java.nio.ByteBuffer.array(ByteBuffer.java:940)
	at org.apache.thrift.TBaseHelper.byteBufferToByteArray(TBaseHelper.java:264)
	at org.apache.thrift.TBaseHelper.byteBufferToByteArray(TBaseHelper.java:251)
	at org.apache.cassandra.db.marshal.IntegerType.getString(IntegerType.java:136)
	at org.apache.cassandra.db.marshal.AbstractCompositeType.getString(AbstractCompositeType.java:131)
	at org.apache.cassandra.db.Column.getString(Column.java:228)
	at org.apache.cassandra.db.filter.SliceQueryFilter.collectReducedColumns(SliceQueryFilter.java:123)
	at org.apache.cassandra.db.filter.QueryFilter.collectCollatedColumns(QueryFilter.java:130)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1303)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1188)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1145)
	at org.apache.cassandra.db.Table.getRow(Table.java:385)
	at org.apache.cassandra.db.SliceFromReadCommand.getRow(SliceFromReadCommand.java:61)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:641)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}

Tracing it down, I find that IntegerType's getString method() looks like this:

{code:title=IntegerType.java|borderStyle=solid}
    public String getString(ByteBuffer bytes)
    {
        if (bytes == null)
            return ""null"";
        if (bytes.remaining() == 0)
            return ""empty"";

        return new java.math.BigInteger(TBaseHelper.byteBufferToByteArray(bytes)).toString(10);
    }
{code} 
    
TBaseHelper.byteBufferToByteArray() looks like this:

{code:title=TBaseHelper.java|borderStyle=solid}
  public static byte[] byteBufferToByteArray(ByteBuffer byteBuffer) {
    if (wrapsFullArray(byteBuffer)) {
      return byteBuffer.array();
    }
    byte[] target = new byte[byteBuffer.remaining()];
    byteBufferToByteArray(byteBuffer, target, 0);
    return target;
  }

  public static boolean wrapsFullArray(ByteBuffer byteBuffer) {
    return byteBuffer.hasArray()
      && byteBuffer.position() == 0
      && byteBuffer.arrayOffset() == 0
      && byteBuffer.remaining() == byteBuffer.capacity();
  }

  public static int byteBufferToByteArray(ByteBuffer byteBuffer, byte[] target, int offset) {
    int remaining = byteBuffer.remaining();
    System.arraycopy(byteBuffer.array(),
        byteBuffer.arrayOffset() + byteBuffer.position(),
        target,
        offset,
        remaining);
    return remaining;
  }
{code} 

The second overloaded implementation of byteBufferToByteArray is calling the bytebuffer's array() method.

Suggested fixes:

1) Don't use TBaseHelper in IntegerType.getString(), use ByteBufferUtil.getArray()

2) Report problem upstream to Thrift.

3) Find a better way to deserialize BigIntegers that doesn't require an array copy.






",edanuff,edanuff,Low,Resolved,Fixed,22/May/11 20:43,16/Apr/19 09:32
Bug,CASSANDRA-2685,12508047,NPE in Table.createReplicationStrategy during sends from HintedHandOffManager,"After about 800k inserts in a column family with RF=1, I get this exception:

{code}
ERROR [HintedHandoff:2] 2011-05-20 18:38:25,089 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[HintedHandoff:2,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.Table.createReplicationStrategy(Table.java:266)
	at org.apache.cassandra.db.Table.<init>(Table.java:212)
	at org.apache.cassandra.db.Table.open(Table.java:106)
	at org.apache.cassandra.db.HintedHandOffManager.sendMessage(HintedHandOffManager.java:131)
	at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:331)
	at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:88)
	at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:409)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{code}",jbellis,ithkuil,Low,Resolved,Fixed,23/May/11 11:59,16/Apr/19 09:32
Bug,CASSANDRA-2687,12508074,generate-eclipse-files ant target throws StackOverflowError in eclipse,,tjake,tjake,Low,Resolved,Fixed,23/May/11 16:08,16/Apr/19 09:32
Bug,CASSANDRA-2694,12508119,stop JDBC driver from needing access to cassandra.yaml,"The JDBC driver uses CFMetaData.fromThrift(), it was calling validateMemtableSettings() which used static methods on  DatabaseDescriptor. This causes cassandra.yaml to be loaded and means the client side needs access to the file. 

I think this needs to be fixed for 0.8, I have the patch. 

**Updated** changed title from ""remove references to DatabaseDescriptor in CFMetaData""",amorton,amorton,Low,Resolved,Fixed,23/May/11 23:31,16/Apr/19 09:32
Bug,CASSANDRA-2696,12508130,Exception adding validators to non-string columns,"Adding column metadata to a column with a non-string name causes an Exception to be raised:

{noformat}
org.apache.cassandra.db.marshal.MarshalException: Expected 8 or 0 byte long (3)
	at org.apache.cassandra.db.marshal.LongType.validate(LongType.java:106)
	at org.apache.cassandra.config.CFMetaData.validateAliasCompares(CFMetaData.java:977)
	at org.apache.cassandra.config.CFMetaData.apply(CFMetaData.java:699)
	at org.apache.cassandra.db.migration.UpdateColumnFamily.<init>(UpdateColumnFamily.java:59)
	at org.apache.cassandra.thrift.CassandraServer.system_update_column_family(CassandraServer.java:968)
	at org.apache.cassandra.thrift.Cassandra$Processor$system_update_column_family.process(Cassandra.java:4032)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{noformat}

For example, if the comparator type is LongType, adding a validator or index to column int(3) will cause this.",thobbs,thobbs,Low,Resolved,Fixed,24/May/11 03:47,16/Apr/19 09:32
Bug,CASSANDRA-2706,12508335,Pig output not working with 0.8.0 branch,"For some reason running a simple column family copy with pig is not writing out, though pig reports that it is successful.
Steps to reproduce on a local node:
1. Create the schema:
http://aep.appspot.com/display/VgbvdtP6QExc3OTY3HBry9ncC3k/
2. Run the following pig script (I did it with pig 0.8.0 from cdh3) using contrib/pig/bin/pig_cassandra -x local:
http://aep.appspot.com/display/PaWJkCqRGbp7CRgjt7qoyx9izN8/",brandon.williams,jeromatron,Normal,Resolved,Fixed,25/May/11 17:22,16/Apr/19 09:32
Bug,CASSANDRA-2708,12508349,memory leak in CompactionManager's estimatedCompactions,"CompactionManager's estimatedCompactions map seems to hold all or most ColumnFamilyStores in the system as keys.  Keys are never removed from estimatedCompactions.

I have a project that embeds Cassandra as a storage backend.  Some of my integration tests create and drop a single keyspace and pair of column families a hundred or 150 times in one JVM.  These tests always OOM'd.  Loading some near-death heapdumps in mat suggested CompactionManager's estimatedCompactions held over 80% of total heap via its ColumnFamilyStore keys.  estimatedCompactions had the only inbound reference to these CFSs, and the CFSs themselves had invalid = true.

As a workaround, I changed estimatedCompactions to a WeakReference-keyed map (using Guava MapMaker).  My integration tests no longer OOM.

I'm generally unfamiliar with Cassandra's guts.  I don't know whether weak referencing the keys of estimatedCompactions is correct (or ideal).  But, that did seem to confirm my guess that retained references to dead CFSs in estimatedCompactions were swamping my heap after lots of Keyspace+ColumnFamily drops.",dalaro,dalaro,Low,Resolved,Fixed,25/May/11 20:51,16/Apr/19 09:32
Bug,CASSANDRA-2713,12508463,Null strategy_options on a KsDef leads to an NPE.,"For add/update keyspace, a KsDef with null strategy_options will cause an NPE.",jhermes,jhermes,Low,Resolved,Fixed,26/May/11 19:14,16/Apr/19 09:32
Bug,CASSANDRA-2717,12508504,duplicate rows returned from SELECT where KEY term is duplicated,"Noticed while working on CASSANDRA-2268 when random keys generated during a mutli_get test contain duplicate keys. 

The thrift multiget_slice() returns only the unique rows because of the map generated for the result. 

CQL will return a row for each KEY term in the SELECT. 

I could make QueryProcessor.getSlice() only create commands for the unique keys if we wanted to. 

Not sure it's a bug and it's definitely not something that should come up to often, reporting it because it's different to the thrift mutli_get operation. 

Happy to close if it's by design. 

",jancona,amorton,Low,Resolved,Fixed,27/May/11 02:48,16/Apr/19 09:32
Bug,CASSANDRA-2718,12508550,NPE in SSTableWriter when no ReplayPosition availible,"The following NPE occurs when durable_writes is set to false

{noformat}
ERROR 09:20:30,378 Fatal exception in thread Thread[FlushWriter:11,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.commitlog.ReplayPosition$ReplayPositionSerializer.serialize(ReplayPosition.java:127)
	at org.apache.cassandra.io.sstable.SSTableWriter.writeMetadata(SSTableWriter.java:209)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:187)
	at org.apache.cassandra.io.sstable.SSTableWriter.closeAndOpenReader(SSTableWriter.java:173)
	at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:253)
	at org.apache.cassandra.db.Memtable.access$400(Memtable.java:49)
	at org.apache.cassandra.db.Memtable$3.runMayThrow(Memtable.java:270)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{noformat}",tjake,tjake,Low,Resolved,Fixed,27/May/11 13:40,16/Apr/19 09:32
Bug,CASSANDRA-2721,12508683,nodetool statusthrift exception while node starts up,"We noticed when calling nodetool statusthrift, while a node is starting up, it throws an exception. I think the proper behavior should be just return false, instead of throwing an exception if RPC server hasn't started yet. That way this stack trace won't have to be thrown in nodetool:

Exception in thread ""main"" 

java.lang.IllegalStateException: No configured RPC daemon
",lenn0x,lenn0x,Low,Resolved,Fixed,30/May/11 07:16,16/Apr/19 09:32
Bug,CASSANDRA-2723,12508780,Rows that don't exist get cached,"We noticed that rows that don't exist were getting cached anyway. We end up storing an empty CF in cache.


",lenn0x,lenn0x,Low,Resolved,Fixed,31/May/11 01:54,16/Apr/19 09:32
Bug,CASSANDRA-2727,12508874,examples/hadoop_word_count reducer to cassandra doesn't output into the output_words cf,"I tried the examples/hadoop_word_count example and could output to the filesystem but when I output to cassandra (the default), nothing shows up in output_words.  I can output to cassandra using pig so I think the problem is isolated to this example.",tjake,jeromatron,Low,Resolved,Fixed,31/May/11 21:41,16/Apr/19 09:32
Bug,CASSANDRA-2730,12509001,exception generate when using same index names,"when using cqlsh tool to generate indexes, for example, suppose we have a column family Tuser, which has two columns: name and state.
cqlsh> create index name_key on Tuser(name);
cqlsh> create index name_key on Tuser(state);
note that name_key is used twice by mistake, then a javax.management.InstanceAlreadyExistsException will be thrown and this exception will prevent cassandra service from starting any more.
",,wubn2000,Low,Resolved,Fixed,01/Jun/11 17:13,16/Apr/19 09:32
Bug,CASSANDRA-2736,12509063,"The link for ""Latest Builds"" in the download page is incorrect","The link in the download page (http://cassandra.apache.org/download/) is outdated, pointing to 
{noformat}http://hudson.zones.apache.org/hudson/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/{noformat}
and should probably replaced by
{noformat}http://builds.apache.org/job/Cassandra/lastSuccessfulBuild/artifact/cassandra/build/{noformat}

In addition, the wording ""Latest Builds"" is incorrect per the link, and should be changed to ""Latest Build"" or more precisely ""Latest Trunk Build"".
Alternatively, the link could instead point to either {noformat}https://builds.apache.org/job/Cassandra/changes{noformat} or {noformat}https://builds.apache.org/job/Cassandra{noformat}, both including a list of the latest trunk builds.

Furthermore, it might be sensible to have additional links to 0.6, 0.7 & 0.8 builds - as it's more probable users are running those rather than the trunk version.

Finally (I didn't think I'd right so much about a link...), I believe the text ""(Hudson)"" next to the link is not up-to-date, and should be replaced by ""(Jenkins)"" or removed altogether.",,kunda,Low,Resolved,Fixed,02/Jun/11 06:56,16/Apr/19 09:32
Bug,CASSANDRA-2740,12509253,nodetool decommission should throw an error when there are extra params,"removetoken takes a token parameter, but decommission works against the node where the call is issued.  This allows confusion such as 'nodetool -h localhost decommission <ip or token>' actually decommissioning the local node, instead of whatever was passed to it.",jhermes,brandon.williams,Low,Resolved,Fixed,04/Jun/11 03:24,16/Apr/19 09:32
Bug,CASSANDRA-2744,12509377,stress.jar is not executable,"If you build stress.jar by running 'ant jar' from tools/stress/ and try to execute it with 'java -jar stress.jar', you get the following error:

{noformat}
Failed to load Main-Class manifest attribute from
stress.jar
{noformat}",xedin,thobbs,Low,Resolved,Fixed,06/Jun/11 18:43,16/Apr/19 09:32
Bug,CASSANDRA-2746,12509492,CliClient does not log root cause exception when catch it from executeCLIStatement,"When executing a statement from the cassandra-cli (with --debug) , if an exception is thrown from one of the cases in side the executeCLIStatement method, the root cause is swallowed. For specific case such as the InvalidRequestException or the SchemaDisagreementException, just the message itself maybe enough, but for the general Exception case, without the root cause, it could be difficult to debug the issue. 

For example, we have seen exception like:
{noformat}
null
java.lang.RuntimeException
at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:209)
at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:223)
at org.apache.cassandra.cli.CliMain.main(CliMain.java:351)
{noformat}

the null there would most likely indicate this is a NPE (though it could still be any Exception with null message). By adding a initCause to the caught exception, we could see the root cause, eg:

{noformat}
null
java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:212)
        at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:223)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:351)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.cli.CliClient.describeKeySpace(CliClient.java:1336)
        at org.apache.cassandra.cli.CliClient.executeShowKeySpaces(CliClient.java:1166)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:170)
        ... 2 more
{noformat}

submitting a patch here that would add the initCause to all caught exceptions here. But the most important one is the general Exception case.",cywjackson,cywjackson,Low,Resolved,Fixed,07/Jun/11 18:01,16/Apr/19 09:32
Bug,CASSANDRA-2752,12509699,repair fails with java.io.EOFException,"Issuing repair on node 1  (1.10.42.81) in a cluster quickly fails with
INFO [AntiEntropyStage:1] 2011-06-09 19:02:47,999 AntiEntropyService.java (line 234) Queueing comparison #<Differencer #<TreeRequest manual-repair-0c17c5f9-583f-4a31-a6d4-a9e7306fb46e, /1
.10.42.82, (JP,XXX), (Token(bytes[6e]),Token(bytes[313039])]>>
 INFO [AntiEntropyStage:1] 2011-06-09 19:02:48,026 AntiEntropyService.java (line 468) Endpoints somewhere/1.10.42.81 and /1.10.42.82 have 2 range(s) out of sync for (JP,XXX) on (Token(bytes[6e]),Token(bytes[313039])]
 INFO [AntiEntropyStage:1] 2011-06-09 19:02:48,026 AntiEntropyService.java (line 485) Performing streaming repair of 2 ranges for #<TreeRequest manual-repair-0c17c5f9-583f-4a31-a6d4-a9e7306
fb46e, /1.10.42.82, (JP,XXX), (Token(bytes[6e]),Token(bytes[313039])]>
 INFO [AntiEntropyStage:1] 2011-06-09 19:02:48,030 StreamOut.java (line 173) Stream context metadata [/data/cassandra/node0/data/JP/XXX-g-3-Data.db sections=1 progress=0/36592 - 0%], 1 sstables.
 INFO [AntiEntropyStage:1] 2011-06-09 19:02:48,031 StreamOutSession.java (line 174) Streaming to /1.10.42.82
ERROR [CompactionExecutor:9] 2011-06-09 19:02:48,970 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[CompactionExecutor:9,1,main]
java.io.EOFException
        at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.doIndexing(SSTableWriter.java:457)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1099)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1090)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)


On .82
ERROR [CompactionExecutor:12] 2011-06-09 19:02:48,051 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[CompactionExecutor:12,1,main]
java.io.EOFException
        at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.doIndexing(SSTableWriter.java:457)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1099)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1090)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR [Thread-132] 2011-06-09 19:02:48,051 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-132,5,main]
java.lang.RuntimeException: java.util.concurrent.ExecutionException: java.io.EOFException
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:152)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
Caused by: java.util.concurrent.ExecutionException: java.io.EOFException
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:136)
        ... 3 more
Caused by: java.io.EOFException
        at java.io.RandomAccessFile.readInt(RandomAccessFile.java:725)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.doIndexing(SSTableWriter.java:457)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:364)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1099)
        at org.apache.cassandra.db.CompactionManager$9.call(CompactionManager.java:1090)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Looks to me like the receiving side fails first.

",jbellis,terjem,Urgent,Resolved,Fixed,09/Jun/11 11:24,16/Apr/19 09:32
Bug,CASSANDRA-2755,12509754,ColumnFamilyRecordWriter fails to throw a write exception encountered after the user begins to close the writer,"There appears to be a race condition in {{ColumnFamilyRecordWriter}} that can result in the loss of an exception. Here is how it can happen (W stands for the {{RangeClient}}'s worker thread; U stands for the {{ColumnFamilyRecordWriter}} user's thread):

# W: {{RangeClient}}'s {{run}} method catches an exception originating in the Thrift client/socket, but doesn't get a chance to set it on the {{lastException}} field before it the thread is preempted.
# U: The user calls {{close}} which calls {{stopNicely}}. Because the {{lastException}} field is null, {{stopNicely}} does not throw anything. {{close}} then joins on the worker thread.
# W: The {{RangeClient}}'s {{run}} method sets the {{lastException}} field and exits.
# U: Although the thread in {{close}} is waiting for the worker thread to exit, it has already checked the {{lastException}} field so it doesn't detect the presence of the last exception. Instead, {{close}} returns without throwing anything.

This race condition means that intermittently write failures will go undetected.",mck,gregkatz,Low,Resolved,Fixed,09/Jun/11 19:40,16/Apr/19 09:32
Bug,CASSANDRA-2758,12509829,nodetool repair never finishes. Loops forever through merkle trees?,"I am not sure all steps here is needed, but as part of testing something else, I set up
node1: initial_token: 1
node2: initial_token: 5

Then:
{noformat}
create keyspace myks 
 with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
 with strategy_options = [{ replication_factor:2 }];

use myks;

create column family test with comparator = AsciiType and column_metadata=[ {column_name: 'up_', validation_class: LongType, index_type: 0}, {column_name: 'del_', validation_class: LongType, index_type: 0} ]
 and keys_cached = 100000 and rows_cached = 10000 and min_compaction_threshold = 2;
quit;
{noformat}

Doing nodetool repair after this gets both nodes busy looping forever.

A quick look at one node in eclipse makes me guess its having fun spinning through  merkle trees, but I have to admit I have not look at it for a long time.





",slebresne,terjem,Low,Resolved,Fixed,10/Jun/11 13:39,16/Apr/19 09:32
Bug,CASSANDRA-2759,12509845,Scrub could lose increments and replicate that loss,"If scrub cannot 'repair' a corrupted row, it will skip it. On node A, if the row contains some sub-count for A id, those will be lost forever since A is the source of truth on it's current id. We should thus renew node A id when that happens to avoid this (not unlike we do in cleanup).",slebresne,slebresne,Normal,Resolved,Fixed,10/Jun/11 16:20,16/Apr/19 09:32
Bug,CASSANDRA-2760,12509854,NPE in sstable2json,"./sstable2json /var/lib/cassandra/data/test/snapshots/1307649033076/User-g-4-Data.db 
{
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamily.<init>(ColumnFamily.java:82)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:70)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:142)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:90)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:74)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:179)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:144)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:136)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:313)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:344)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:357)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:415)
",jasobrown,daningaddr,Low,Resolved,Fixed,10/Jun/11 17:48,16/Apr/19 09:32
Bug,CASSANDRA-2761,12509880,JDBC driver does not build,"Need a way to build (and run tests for) the Java driver.

Also: still some vestigal references to drivers/ in trunk build.xml.

Should we remove drivers/ from the 0.8 branch as well?",ardot,jbellis,Normal,Resolved,Fixed,10/Jun/11 21:36,16/Apr/19 09:32
Bug,CASSANDRA-2762,12509893,Token cannot contain comma (possibly non-alpha/non-numeric too?) in OrderPreservingPartitioner,"It'd appear that when the token contain comma in the OrderPreservingPartitioner case, C* will fail with assert error:

ERROR [GossipStage:1] 2011-06-09 16:01:05,063 AbstractCassandraDaemon.java (line 112) Fatal exception in thread Thread[GossipStage:1,5,main]
java.lang.AssertionError
    at org.apache.cassandra.service.StorageService.handleStateBootstrap(StorageService.java:685)
    at org.apache.cassandra.service.StorageService.onChange(StorageService.java:648)
    at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:772)
    at org.apache.cassandra.gms.Gossiper.applyApplicationStateLocally(Gossiper.java:737)
    at org.apache.cassandra.gms.Gossiper.applyStateLocally(Gossiper.java:679)
    at org.apache.cassandra.gms.GossipDigestAck2VerbHandler.doVerb(GossipDigestAck2VerbHandler.java:60)
    at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
    at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:662)
",jbellis,cywjackson,Low,Resolved,Fixed,11/Jun/11 01:54,16/Apr/19 09:32
Bug,CASSANDRA-2764,12510076,generate-eclipse-files still referencing drivers/ source,"In trunk, running ant generate-eclipse-files will reference the old drivers top-level directory. The result is that the generated project, once loaded into Eclipse causes errors about the non-existent source directories.",kirktrue,kirktrue,Low,Resolved,Fixed,13/Jun/11 05:00,16/Apr/19 09:32
Bug,CASSANDRA-2765,12510131,DataTracker.View.MarkCompacting adds ALL sstables and marks them as compacting,"At some point if the list isn't cleaned up with this symptom compactions will stop until the server is restarted.
",bcoverston,bcoverston,Normal,Resolved,Fixed,13/Jun/11 19:17,16/Apr/19 09:32
Bug,CASSANDRA-2766,12510276,ConcurrentModificationException during node recovery,"Testing some node recovery operations.

In this case:
1. Data is being added/updated as it would in production
2. repair is running on other nodes in the cluster
3. we wiped data on this node and started up again, but before repair was actually started on this node (but it had gotten data through the regular data feed) we got this error.

I see no indication in the logs that outgoing streams has been started, but the node have finished one incoming stream before this (I guess from some other node doing repair).

 INFO [CompactionExecutor:11] 2011-06-14 14:15:09,078 SSTableReader.java (line 155) Opening /data/cassandra/node1/data/JP/test-g-8
 INFO [CompactionExecutor:13] 2011-06-14 14:15:09,079 SSTableReader.java (line 155) Opening /data/cassandra/node1/data/JP/test-g-10
 INFO [HintedHandoff:1] 2011-06-14 14:15:26,623 HintedHandOffManager.java (line 302) Started hinted handoff for endpoint /1.10.42.216
 INFO [HintedHandoff:1] 2011-06-14 14:15:26,623 HintedHandOffManager.java (line 358) Finished hinted handoff of 0 rows to endpoint /1.10.42.216
 INFO [CompactionExecutor:9] 2011-06-14 14:15:29,417 SSTableReader.java (line 155) Opening /data/cassandra/node1/data/JP/Datetest-g-2
ERROR [Thread-84] 2011-06-14 14:15:36,755 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-84,5,main]
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:132)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
ERROR [Thread-79] 2011-06-14 14:15:36,755 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-79,5,main]
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:132)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
ERROR [Thread-83] 2011-06-14 14:15:36,755 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-83,5,main]
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:132)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
ERROR [Thread-85] 2011-06-14 14:15:36,755 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[Thread-85,5,main]
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:132)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:63)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:155)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:93)
",jbellis,terjem,Normal,Resolved,Fixed,14/Jun/11 07:14,16/Apr/19 09:32
Bug,CASSANDRA-2767,12510277,ConcurrentModificationException in AntiEntropyService.getNeighbors(),,slebresne,slebresne,Normal,Resolved,Fixed,14/Jun/11 08:01,16/Apr/19 09:32
Bug,CASSANDRA-2769,12510353,Cannot Create Duplicate Compaction Marker,"Concurrent compaction can trigger the following exception when two threads compact the same sstable. DataTracker attempts to prevent this but apparently not successfully.

java.io.IOError: java.io.IOException: Unable to create compaction marker
	at org.apache.cassandra.io.sstable.SSTableReader.markCompacted(SSTableReader.java:638)
	at org.apache.cassandra.db.DataTracker.removeOldSSTablesSize(DataTracker.java:321)
	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:294)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:255)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:932)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:173)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:119)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:102)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
Caused by: java.io.IOException: Unable to create compaction marker
	at org.apache.cassandra.io.sstable.SSTableReader.markCompacted(SSTableReader.java:634)
	... 12 more",slebresne,bcoverston,Normal,Resolved,Fixed,14/Jun/11 20:15,16/Apr/19 09:32
Bug,CASSANDRA-2773,12510393,"""Index manager cannot support deleting and inserting into a row in the same mutation""","I use hector 0.8.0-1 and cassandra 0.8.

1. create mutator by using hector api, 
2. Insert a few columns into the mutator for key ""key1"", cf ""standard"". 
3. add a deletion to the mutator to delete the record of ""key1"", cf ""standard"".
4. repeat 2 and 3
5. execute the mutator.

the result: the connection seems to be held by the sever forever, it never returns. when I tried to restart the cassandra I saw unsupportedexception : ""Index manager cannot support deleting and inserting into a row in the same mutation"". and the cassandra is dead forever, unless I delete the commitlog. 

I would expect to get an exception when I execute the mutator, not after I restart the cassandra.",jbellis,yulinyen,Urgent,Resolved,Fixed,15/Jun/11 03:10,16/Apr/19 09:32
Bug,CASSANDRA-2778,12510510,Unable to set compaction strategy in cli using create column family command,"The following command does not set compaction strategy and its options:
{code}
create column family Standard1
    with comparator = BytesType
    and compaction_strategy = 'org.apache.cassandra.db.compaction.TimestampBucketedCompactionStrategy'
    and compaction_strategy_options = [{max_sstable_size:504857600, retention_in_seconds:60}];
{code}",alanliang,alanliang,Normal,Resolved,Fixed,16/Jun/11 00:10,16/Apr/19 09:32
Bug,CASSANDRA-2780,12510538,sstable2json needs to escape quotes,"[default@foo] set transactions[test][data]='{""foo"":""bar""}'; 

$ cat /tmp/json
{
""74657374"": [[""data"", ""{""foo"":""bar""}"", 1308209845388000]]
}

$ ./json2sstable -s -c transactions -K foo /tmp/json /tmp/ss-g-1-Data.db
Counting keys to import, please wait... (NOTE: to skip this use -n <num_keys>)
org.codehaus.jackson.JsonParseException: Unexpected character ('f' (code 102)): was expecting comma to separate ARRAY entries
 at [Source: /tmp/json2; line: 2, column: 27]
	at org.codehaus.jackson.JsonParser._constructError(JsonParser.java:929)
	at org.codehaus.jackson.impl.JsonParserBase._reportError(JsonParserBase.java:632)
	at org.codehaus.jackson.impl.JsonParserBase._reportUnexpectedChar(JsonParserBase.java:565)
	at org.codehaus.jackson.impl.Utf8StreamParser.nextToken(Utf8StreamParser.java:128)
	at org.codehaus.jackson.impl.JsonParserBase.skipChildren(JsonParserBase.java:263)
	at org.apache.cassandra.tools.SSTableImport.importSorted(SSTableImport.java:328)
	at org.apache.cassandra.tools.SSTableImport.importJson(SSTableImport.java:252)
	at org.apache.cassandra.tools.SSTableImport.main(SSTableImport.java:476)
ERROR: Unexpected character ('f' (code 102)): was expecting comma to separate ARRAY entries
 at [Source: /tmp/json2; line: 2, column: 27]

http://www.mail-archive.com/user@cassandra.apache.org/msg14257.html
",xedin,tcn,Low,Resolved,Fixed,16/Jun/11 07:49,16/Apr/19 09:32
Bug,CASSANDRA-2781,12510580,regression: exposing cache size through MBean,"Looks like it was part of CASSANDRA-1969.  A method called size, as opposed to getSize, won't be exposed through jmx.",cburroughs,cburroughs,Low,Resolved,Fixed,16/Jun/11 14:39,16/Apr/19 09:32
Bug,CASSANDRA-2784,12510617,Fix build for removal of commons-collections,,stuhood,stuhood,Low,Resolved,Fixed,16/Jun/11 20:42,16/Apr/19 09:32
Bug,CASSANDRA-2785,12510628,should export JAVA variable in the bin/cassandra and use that in the cassandra-env.sh when check for the java version,"I forgot which jira we add this java -version check in the cassandra-env.sh (for adding jamm to the javaagent), but we should probably use the variable JAVA set in bin/cassandra (will need export) and use $JAVA instead of ""java"" in the cassandra-env.sh

In a situation where JAVA_HOME may have been properly set as the Sun's java but the PATH still have the OpenJDK's java in front, the check will fail to add the jamm.jar, even though the cassandra jvm is properly started via the Sun's java.",thepaul,cywjackson,Low,Resolved,Fixed,17/Jun/11 00:59,16/Apr/19 09:32
Bug,CASSANDRA-2786,12510655,"After a minor compaction, deleted key-slices are visible again","After a minor compaction, deleted key-slices are visible again.

Steps to reproduce:

1) Insert a row named ""test"".
2) Insert 500000 rows. During this step, row ""test"" is included in a major compaction:
   file-1, file-2, file-3 and file-4 compacted to file-5 (includes ""test"").
3) Delete row named ""test"".
4) Insert 500000 rows. During this step, row ""test"" is included in a minor compaction:
   file-6, file-7, file-8 and file-9 compacted to file-10 (should include tombstoned ""test"").
After step 4, row ""test"" is live again.

Test environment:

Single node with empty database.

Standard configured super-column-family (I see this behavior with several gc_grace settings (big and small values):
create column family Customers with column_type = 'Super' and comparator = 'BytesType;

In Cassandra 0.7.6 I observe the expected behavior, i.e. after step 4, the row is still deleted.

I've included a .NET program to reproduce the problem. I will add a Java version later on.",slebresne,kochen,Normal,Resolved,Fixed,17/Jun/11 12:42,16/Apr/19 09:32
Bug,CASSANDRA-2787,12510656,java agent option missing in cassandra.bat file,"This option must be included in cassandra.bat:

-javaagent:%CASSANDRA_HOME%\lib\jamm-0.2.2.jar

Otherwise you see the following warnings in cassandra log:

WARN 12:02:32,478 MemoryMeter uninitialized (jamm not specified as java agent); assuming liveRatio of 10.0. Usually this means cassandra-env.sh disabled jamm because you are using a buggy JRE; upgrade to the Sun JRE instead
",kochen,kochen,Low,Resolved,Fixed,17/Jun/11 12:49,16/Apr/19 09:32
Bug,CASSANDRA-2792,12510760,Bootstrapping node stalls. Bootstrapper thinks it is still streaming some sstables. The source nodes do not. Caused by IllegalStateException on source nodes.,"I am bootstrapping a node into a 4 node cluster with RF3 (1 node is currently down due to sstable issues, but the cluster is running without issues). 

There are two keyspaces FightMyMonster and FMM_Studio. The first keyspace successfully streams and the whole operation is probably at 99% when it stalls on some sstables in the much smaller FMM_Studio keyspace.

Netstats on the bootstrapping node reports it is still streaming:

Mode: Bootstrapping
Not sending any streams.
Streaming from: /192.168.1.4
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-101-Data.db sections=1 progress=0/76453 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-103-Data.db sections=1 progress=0/90475 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-102-Data.db sections=1 progress=0/4304182 - 0%
Streaming from: /192.168.1.3
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-158-Data.db sections=2 progress=0/146990 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/AuthorClasses-f-81-Data.db sections=1 progress=0/3992 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/Studio-f-70-Data.db sections=1 progress=0/1776 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-159-Data.db sections=2 progress=0/136829 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/PartsData-f-157-Data.db sections=2 progress=0/5779597 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/AuthorClasses-f-82-Data.db sections=1 progress=0/161 - 0%
   FMM_Studio: /var/opt/cassandra/data/FMM_Studio/Studio-f-71-Data.db sections=1 progress=0/135 - 0%
Pool Name                    Active   Pending      Completed
Commands                        n/a         0            334
Responses                       n/a         0         421957

However, running netstats on the source nodes reports they are not streaming:

Mode: Normal
 Nothing streaming to /192.168.1.9
Not receiving any streams.
Pool Name                    Active   Pending      Completed
Commands                        n/a         0        1949476
Responses                       n/a         1        1778768

Examination of the logs on the source nodes show an IllegalStateException that has likely interrupted/broken the streaming process.

17 22:27:05,924 StreamOut.java (line 126) Beginning transfer to /192.168.1.9
 INFO [StreamStage:1] 2011-06-17 22:27:05,925 StreamOut.java (line 100) Flushing memtables for FMM_Studio...
 INFO [StreamStage:1] 2011-06-17 22:27:06,004 StreamOut.java (line 173) Stream context metadata [/var/opt/cassandra/data/FMM_Studio/Classes-f-107-Data.db sections=1 progress=0/1585378 - 0%, /var/opt/cas
sandra/data/FMM_Studio/PartsData-f-100-Data.db sections=1 progress=0/76453 - 0%, /var/opt/cassandra/data/FMM_Studio/PartsData-f-98-Data.db sections=1 progress=0/4309514 - 0%, /var/opt/cassandra/data/FMM
_Studio/PartsData-f-99-Data.db sections=1 progress=0/90475 - 0%], 11 sstables.
 INFO [StreamStage:1] 2011-06-17 22:27:06,005 StreamOutSession.java (line 174) Streaming to /192.168.1.9
 INFO [StreamStage:1] 2011-06-17 22:27:06,006 StreamOut.java (line 126) Beginning transfer to /192.168.1.9
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 StreamOut.java (line 100) Flushing memtables for FightMyMonster...
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-MonsterMarket_1@1054909557(338 bytes, 24 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-UserFights@239934867(1124836 bytes, 965 operations)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,007 Memtable.java (line 157) Writing Memtable-MonsterMarket_1@1054909557(338 bytes, 24 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,007 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-Users_CisIndex@1758504250(242 bytes, 8 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,008 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-Tribes@1510979736(18318 bytes, 703 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,008 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-ColumnViews_TimeUUID@864545260(2073 bytes, 63 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,008 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-MonsterMarket_0@537829218(2600 bytes, 129 operations)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,069 Memtable.java (line 172) Completed flushing /var/opt/cassandra/data/FightMyMonster/MonsterMarket_1-f-3799-Data.db (1774 bytes)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,069 Memtable.java (line 157) Writing Memtable-UserFights@239934867(1124836 bytes, 965 operations)
 INFO [StreamStage:1] 2011-06-17 22:27:06,070 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-UserSigninLog@1692186117(4043 bytes, 137 operations)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,161 Memtable.java (line 172) Completed flushing /var/opt/cassandra/data/FightMyMonster/UserFights-f-8192-Data.db (1179202 bytes)
 INFO [FlushWriter:409] 2011-06-17 22:27:06,161 Memtable.java (line 157) Writing Memtable-Users_CisIndex@1758504250(242 bytes, 8 operations)
 INFO [CompactionExecutor:1] 2011-06-17 22:27:06,161 CompactionManager.java (line 395) Compacting [SSTableReader(path='/var/opt/cassandra/data/FightMyMonster/UserFights-f-8189-Data.db'),SSTableReader(pa
th='/var/opt/cassandra/data/FightMyMonster/UserFights-f-8190-Data.db'),SSTableReader(path='/var/opt/cassandra/data/FightMyMonster/UserFights-f-8191-Data.db'),SSTableReader(path='/var/opt/cassandra/data/
FightMyMonster/UserFights-f-8192-Data.db')]
 INFO [StreamStage:1] 2011-06-17 22:27:06,162 ColumnFamilyStore.java (line 1065) Enqueuing flush of Memtable-TribeFights@321579649(138 bytes, 3 operations)
ERROR [MiscStage:1] 2011-06-17 22:27:06,168 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[MiscStage:1,5,main]
java.lang.IllegalStateException: target reports current file is /var/opt/cassandra/data/FMM_Studio/Classes-f-107-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:166)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
ERROR [MiscStage:1] 2011-06-17 22:27:06,168 AbstractCassandraDaemon.java (line 114) Fatal exception in thread Thread[MiscStage:1,5,main]
java.lang.IllegalStateException: target reports current file is /var/opt/cassandra/data/FMM_Studio/Classes-f-107-Data.db but is null
        at org.apache.cassandra.streaming.StreamOutSession.validateCurrentFile(StreamOutSession.java:166)
        at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:58)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:72)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619) 

There are two problems. Firstly the source nodes should report to the bootstrapping node that there has been a problem, and/or the bootstrapping node should timeout and report the the issue. 

Secondly, there is an issue with what is causing IllegalStateException.",slebresne,dccwilliams,Normal,Resolved,Fixed,18/Jun/11 13:53,16/Apr/19 09:32
Bug,CASSANDRA-2797,12510951,Repair hangs if a neighbor has nothing to send ,"This is actually a streaming problem. If a StreamOutSession has nothing to transfer (i.e, no sstables have the requested ranges), it will not even initiate the transfer and simply close the session right away. The problem is that if the session was initiated by a remote end (through a StreamRequestMessage), the remote end will never be notified and never run his callback.
",slebresne,slebresne,Low,Resolved,Fixed,20/Jun/11 12:33,16/Apr/19 09:32
Bug,CASSANDRA-2800,12511003,OPP#describeOwnership reports incorrect ownership,"OPP#describeOwnership relies on StorageService#getSplits and counts the received tokens as its basis of ownership.

When the number of result keys is less than the number of splits, the full count is omitted (to save work?). However, we don't care if a split would end up fractional in this case, we just need the full count.

The logic here is:
{code}
int splits = keycount * DatabaseDescriptor.getIndexInterval() / keysPerSplit;
if (keycount >= splits) { ... add count to result set }
{code}
We were passing in 1 key per split (since we just care about the count), but splits=keycount*IndexInterval is guaranteed to be > keycount, so the result set is not completely formed.
The better ""unit keysPerSplit"" to use is IndexInterval itself, which gives splits=keycount*II/II=keycount, so the logic runs correctly.",jhermes,jhermes,Low,Resolved,Fixed,20/Jun/11 21:07,16/Apr/19 09:32
Bug,CASSANDRA-2801,12511068,Tombstone are not purged when the row is in only one sstable,"We messed up the refactor of compactionController. It echoes rows if they are present in only one sstable, even if they could be purged.",slebresne,slebresne,Normal,Resolved,Fixed,21/Jun/11 09:57,16/Apr/19 09:32
Bug,CASSANDRA-2805,12511159,Clean up mbeans that return Internal Cassandra types,"We need to clean up wherever we return internal cassandra objects over jmx. Namely CompactionInfo objects as well as Tokens. There may be a few other examples.

This is bad for two reasons

1. You have to load the cassandra jar when querying these mbeans, which sucks.
2. Stuff breaks between versions when things are moved. For example, CASSANDRA-1610 moves the compaction related classes around. Any code querying those jmx mbeans in 0.8.0 is now broken in 0.8.2. (assuming those moves stay in the 0.8 branch)

For things like CompactionInfo we should just expose more mbean methods or serialize to something standard like json.

I'd like to target this for 0.8.2. Since we've already broken compatibility between 0.8.0 and 0.8.1, I'd say just fix this everywhere now.",nickmbailey,nickmbailey,Low,Resolved,Fixed,21/Jun/11 20:46,16/Apr/19 09:32
Bug,CASSANDRA-2809,12511233,"In the Cli, update column family <cf> with comparator; create Column metadata","Using cassandra-cli, I can't update the comparator of a column family with the type I want and when I did it with BytesType, Column metadata appear for each of my existing columns.
Step to reproduce:
{code}
[default@unknown] create keyspace Test
    with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
    and strategy_options = [{replication_factor:1}];

[default@unknown] use Test;
Authenticated to keyspace: Test

[default@Test] create column family test;

[default@Test] describe keyspace;
...
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
...

[default@Test] update column family test with comparator = 'LongType';
comparators do not match.
{code}
why?? the CF is empty
{code}
[default@Test] update column family test with comparator = 'BytesType';
f8e4dcb0-9cca-11e0-0000-d0583497e7ff
Waiting for schema agreement...
... schemas agree across the cluster

[default@Test] describe keyspace;
...
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
...

[default@Test] set test[ascii('row1')][long(1)]=integer(35);
set test[ascii('row1')][long(2)]=integer(36);
set test[ascii('row1')][long(3)]=integer(38);
set test[ascii('row2')][long(1)]=integer(45);
set test[ascii('row2')][long(2)]=integer(42);
set test[ascii('row2')][long(3)]=integer(33);

[default@Test] list test;
Using default limit of 100
-------------------
RowKey: 726f7731
=> (column=0000000000000001, value=35, timestamp=1308744931122000)
=> (column=0000000000000002, value=36, timestamp=1308744931124000)
=> (column=0000000000000003, value=38, timestamp=1308744931125000)
-------------------
RowKey: 726f7732
=> (column=0000000000000001, value=45, timestamp=1308744931127000)
=> (column=0000000000000002, value=42, timestamp=1308744931128000)
=> (column=0000000000000003, value=33, timestamp=1308744932722000)

2 Rows Returned.

[default@Test] update column family test with comparator = 'LongType';
comparators do not match.
{code}
same question than before, my columns contains only long, why I can't?

{code}
[default@Test] update column family test with comparator = 'BytesType';

[default@Test] describe keyspace;                                      
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name:  (0000000000000001)
          Validation Class: org.apache.cassandra.db.marshal.IntegerType
        Column Name:  (0000000000000003)
          Validation Class: org.apache.cassandra.db.marshal.IntegerType
        Column Name:  (0000000000000002)
          Validation Class: org.apache.cassandra.db.marshal.IntegerType
{code}
Column Metadata appear from nowhere. I don't think that it's expected.
",xedin,silvere,Low,Resolved,Fixed,22/Jun/11 12:54,16/Apr/19 09:32
Bug,CASSANDRA-2810,12511243,"RuntimeException in Pig when using ""dump"" command on column name","This bug was previously report on [Brisk bug tracker|https://datastax.jira.com/browse/BRISK-232].

In cassandra-cli:
{code}
[default@unknown] create keyspace Test
    with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy'
    and strategy_options = [{replication_factor:1}];

[default@unknown] use Test;
Authenticated to keyspace: Test

[default@Test] create column family test;

[default@Test] set test[ascii('row1')][long(1)]=integer(35);
set test[ascii('row1')][long(2)]=integer(36);
set test[ascii('row1')][long(3)]=integer(38);
set test[ascii('row2')][long(1)]=integer(45);
set test[ascii('row2')][long(2)]=integer(42);
set test[ascii('row2')][long(3)]=integer(33);

[default@Test] list test;
Using default limit of 100
-------------------
RowKey: 726f7731
=> (column=0000000000000001, value=35, timestamp=1308744931122000)
=> (column=0000000000000002, value=36, timestamp=1308744931124000)
=> (column=0000000000000003, value=38, timestamp=1308744931125000)
-------------------
RowKey: 726f7732
=> (column=0000000000000001, value=45, timestamp=1308744931127000)
=> (column=0000000000000002, value=42, timestamp=1308744931128000)
=> (column=0000000000000003, value=33, timestamp=1308744932722000)

2 Rows Returned.

[default@Test] describe keyspace;
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
{code}
In Pig command line:
{code}
grunt> test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS (rowkey:chararray, columns: bag {T: (name:long, value:int)});

grunt> value_test = foreach test generate rowkey, columns.name, columns.value;

grunt> dump value_test;
{code}
In /var/log/cassandra/system.log, I have severals time this exception:
{code}
INFO [IPC Server handler 3 on 8012] 2011-06-22 15:03:28,533 TaskInProgress.java (line 551) Error from attempt_201106210955_0051_m_000000_3: java.lang.RuntimeException: Unexpected data type -1 found in stream.
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:478)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeBag(BinInterSedes.java:522)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:361)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
	at org.apache.pig.impl.io.InterRecordWriter.write(InterRecordWriter.java:73)
	at org.apache.pig.impl.io.InterStorage.putNext(InterStorage.java:87)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:138)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:97)
	at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:638)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.collect(PigMapOnly.java:48)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:239)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:232)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:763)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:369)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:259)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.Child.main(Child.java:253)
{code}
and the request failed.

{code}
grunt> test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS (rowkey:chararray, columns: bag {T: (name:long, value:int)});

grunt> value_test = foreach test generate rowkey, columns.value;

grunt> dump value_test;
{code}

This time, without the column name, it's work (but the value are displayed as char instead of integer). Result:
{code}
(row1,{(#),($),(&)})
(row2,{(-),(*),(!)})
{code}

Now we do the same test but we set a comparator to the CF.
{code}
[default@Test] create column family test with comparator = 'LongType';

[default@Test] set test[ascii('row1')][long(1)]=integer(35);
set test[ascii('row1')][long(2)]=integer(36);
set test[ascii('row1')][long(3)]=integer(38);
set test[ascii('row2')][long(1)]=integer(45);
set test[ascii('row2')][long(2)]=integer(42);
set test[ascii('row2')][long(3)]=integer(33);

[default@Test] list test;
Using default limit of 100
-------------------
RowKey: 726f7731
=> (column=1, value=35, timestamp=1308748643506000)
=> (column=2, value=36, timestamp=1308748643508000)
=> (column=3, value=38, timestamp=1308748643509000)
-------------------
RowKey: 726f7732
=> (column=1, value=45, timestamp=1308748643510000)
=> (column=2, value=42, timestamp=1308748643512000)
=> (column=3, value=33, timestamp=1308748645138000)

2 Rows Returned.

[default@Test] describe keyspace;
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
{code}
{code}
grunt> test = LOAD 'cassandra://Test/test' USING CassandraStorage() AS (rowkey:chararray, columns: bag {T: (name:long, value:int)});

grunt> value_test = foreach test generate rowkey, columns.name, columns.value;

grunt> dump value_test;
{code}
This time it's work as expected (appart from the value displayed as char). Result:
{code}
(row1,{(1),(2),(3)},{(#),($),(&)})
(row2,{(1),(2),(3)},{(-),(*),(!)})
{code}
",brandon.williams,silvere,Normal,Resolved,Fixed,22/Jun/11 14:49,16/Apr/19 09:32
Bug,CASSANDRA-2816,12511343,Repair doesn't synchronize merkle tree creation properly,"Being a little slow, I just realized after having opened CASSANDRA-2811 and CASSANDRA-2815 that there is a more general problem with repair.

When a repair is started, it will send a number of merkle tree to its neighbor as well as himself and assume for correction that the building of those trees will be started on every node roughly at the same time (if not, we end up comparing data snapshot at different time and will thus mistakenly repair a lot of useless data). This is bogus for many reasons:
* Because validation compaction runs on the same executor that other compaction, the start of the validation on the different node is subject to other compactions. 0.8 mitigates this in a way by being multi-threaded (and thus there is less change to be blocked a long time by a long running compaction), but the compaction executor being bounded, its still a problem)
* if you run a nodetool repair without arguments, it will repair every CFs. As a consequence it will generate lots of merkle tree requests and all of those requests will be issued at the same time. Because even in 0.8 the compaction executor is bounded, some of those validations will end up being queued behind the first ones. Even assuming that the different validation are submitted in the same order on each node (which isn't guaranteed either), there is no guarantee that on all nodes, the first validation will take the same time, hence desynchronizing the queued ones.

Overall, it is important for the precision of repair that for a given CF and range (which is the unit at which trees are computed), we make sure that all node will start the validation at the same time (or, since we can't do magic, as close as possible).

One (reasonably simple) proposition to fix this would be to have repair schedule validation compactions across nodes one by one (i.e, one CF/range at a time), waiting for all nodes to return their tree before submitting the next request. Then on each node, we should make sure that the node will start the validation compaction as soon as requested. For that, we probably want to have a specific executor for validation compaction and:
* either we fail the whole repair whenever one node is not able to execute the validation compaction right away (because no thread are available right away).
* we simply tell the user that if he start too many repairs in parallel, he may start seeing some of those repairing more data than it should.
",slebresne,slebresne,Normal,Resolved,Fixed,23/Jun/11 11:35,16/Apr/19 09:32
Bug,CASSANDRA-2817,12511377,Expose number of threads blocked on submitting a memtable for flush,"Writes can be blocked by a thread trying to submit a memtable while the flush queue is full. While this is the expected behavior (the goal being to prevent OOMing), it is worth exposing when that happens so that people can monitor it and modify settings accordingly if that happens too often.",slebresne,slebresne,Low,Resolved,Fixed,23/Jun/11 16:11,16/Apr/19 09:32
Bug,CASSANDRA-2818,12511410,0.8.0 is unable to participate with nodes using a _newer_ protocol version,"When a 0.8.1 node tries to join a 0.8.0 ring, we see an endless supply of these in system.log:

INFO [Thread-4] 2011-06-23 21:14:04,149 IncomingTcpConnection.java (line 103) Received connection from newer protocol version. Ignorning message.

and the node never joins the ring.",brandon.williams,mallen,Low,Resolved,Fixed,23/Jun/11 21:22,16/Apr/19 09:32
Bug,CASSANDRA-2821,12511478,CLI remove ascii column,"[default@sdo] incr counters[ascii('EU')][ascii('null')];
Value incremented.
[default@sdo] list counters;
Using default limit of 100
-------------------
RowKey: 4555
=> (counter=6e756c6c, value=1)

1 Row Returned.
[default@sdo] del counters[ascii('EU')][ascii('null')];
org.apache.cassandra.db.marshal.MarshalException: cannot parse
'FUNCTION_CALL' as hex bytes
[default@sdo]

Suggested workaround, although not tested:

assume counters comparator as bytes;
del counters['EU'][0];",xedin,sdolgy,Low,Resolved,Fixed,24/Jun/11 14:44,16/Apr/19 09:32
Bug,CASSANDRA-2822,12511479,NullPointerException after upgrade to 0.8.0,"I'm getting NullPointerException on a node upgraded from 0.7 to 0.8.0 (Debian package). The exception is thrown quickly several times after start. Then the Cassandra node is unresponsive. The Stack trace is:

ERROR 14:36:49,712 Fatal exception in thread Thread[WRITE-/10.228.243.191,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)

ERROR 14:36:49,758 Fatal exception in thread Thread[WRITE-/10.227.101.171,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)

ERROR 14:36:49,797 Fatal exception in thread Thread[WRITE-/10.228.243.191,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)

ERROR 14:36:50,756 Fatal exception in thread Thread[WRITE-/10.226.194.239,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.OutboundTcpConnection.connect(OutboundTcpConnection.java:168)
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:89)",jbellis,vilda,Low,Resolved,Fixed,24/Jun/11 14:46,16/Apr/19 09:32
Bug,CASSANDRA-2823,12511484,NPE during range slices with rowrepairs,"Doing some heavy testing of relatively fast feeding (5000+ mutations/sec) + repair on all node + range slices.
Then occasionally killing a node here and there and restarting it.

Triggers the following NPE
 ERROR [pool-2-thread-3] 2011-06-24 20:56:27,289 Cassandra.java (line 3210) Internal error processing get_range_slices
java.lang.NullPointerException
	at org.apache.cassandra.service.RowRepairResolver.maybeScheduleRepairs(RowRepairResolver.java:109)
	at org.apache.cassandra.service.RangeSliceResponseResolver$2.getReduced(RangeSliceResponseResolver.java:112)
	at org.apache.cassandra.service.RangeSliceResponseResolver$2.getReduced(RangeSliceResponseResolver.java:83)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:161)
	at org.apache.cassandra.utils.MergeIterator.computeNext(MergeIterator.java:88)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:120)
	at org.apache.cassandra.service.RangeSliceResponseResolver.resolve(RangeSliceResponseResolver.java:43)

Looking at the code in getReduced:

{noformat}
                ColumnFamily resolved = versions.size() > 1
                                      ? RowRepairResolver.resolveSuperset(versions)
                                      : versions.get(0);
{noformat}
seems like resolved becomes null when this happens and versions.size is larger than 1.

RowRepairResolver.resolveSuperset() does actually return null if it cannot resolve anything, so there is definately a case here which can occur and is not handled.

It may also be an interesting question if it is guaranteed that                
versions.add(current.left.cf);
can never return null?

Jonathan suggested on IRC that maybe 
{noformat}
                ColumnFamily resolved = versions.size() > 1
                                      ? RowRepairResolver.resolveSuperset(versions)
                                      : versions.get(0);
                if (resolved == null)
                      return new Row(key, resolved);
{noformat}

could be a fix.
",slebresne,terjem,Normal,Resolved,Fixed,24/Jun/11 15:39,16/Apr/19 09:32
Bug,CASSANDRA-2824,12511491,assert err on SystemTable.getCurrentLocalNodeId during a cleanup,"when running nodetool cleanup the following happened:

$ ./bin/nodetool cleanup --host localhost
Exception in thread ""main"" java.lang.AssertionError
at org.apache.cassandra.db.SystemTable.getCurrentLocalNodeId(SystemTable.java:383)
at org.apache.cassandra.utils.NodeId$LocalNodeIdHistory.<init>(NodeId.java:179)
at org.apache.cassandra.utils.NodeId.<clinit>(NodeId.java:38)
at org.apache.cassandra.utils.NodeId$OneShotRenewer.<init>(NodeId.java:159)
at org.apache.cassandra.service.StorageService.forceTableCleanup(StorageService.java:1317)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
at java.lang.reflect.Method.invoke(Method.java:597)
at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
at sun.rmi.transport.Transport$1.run(Transport.java:159)
at java.security.AccessController.doPrivileged(Native Method)
at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:662) 

",slebresne,cywjackson,Low,Resolved,Fixed,24/Jun/11 16:34,16/Apr/19 09:32
Bug,CASSANDRA-2825,12511498,"Auto bootstrapping the 4th node in a 4 node cluster doesn't work, when no token explicitly assigned in config.","This was done in sequence.  A, B, C, and D.  Node A with token 0 explicitly set in config.  The rest with auto_bootstrap: true and no token explicitly assigned.  B and C work as expected. D ends up stealing C's token.  

from system.log on C:

INFO [GossipStage:1] 2011-06-24 16:40:41,947 Gossiper.java (line 638) Node /10.171.47.226 is now part of the cluster
INFO [GossipStage:1] 2011-06-24 16:40:41,947 Gossiper.java (line 606) InetAddress /10.171.47.226 is now UP
INFO [GossipStage:1] 2011-06-24 16:42:09,432 StorageService.java (line 769) Nodes /10.171.47.226 and /10.171.55.77 have the same token 61078635599166706937511052402724559481.  /10.171.47.226 is the new owner
WARN [GossipStage:1] 2011-06-24 16:42:09,432 TokenMetadata.java (line 120) Token 61078635599166706937511052402724559481 changing ownership from /10.171.55.77 to /10.171.47.226


",brandon.williams,mallen,Normal,Resolved,Fixed,24/Jun/11 18:41,16/Apr/19 09:32
Bug,CASSANDRA-2826,12511523,Debian Package for 0.8 is missing,"In file

http://www.apache.org/dist/cassandra/debian/dists/08x/InRelease

Codename: sid

Should be changed to

Codename: 08x",slebresne,smitchell360,Low,Resolved,Fixed,25/Jun/11 04:30,16/Apr/19 09:32
Bug,CASSANDRA-2829,12511589,memtable with no post-flush activity can leave commitlog permanently dirty,"Only dirty Memtables are flushed, and so only dirty memtables are used to discard obsolete commit log segments. This can result it log segments not been deleted even though the data has been flushed.  

Was using a 3 node 0.7.6-2 AWS cluster (DataStax AMI's) with pre 0.7 data loaded and a running application working against the cluster. Did a rolling restart and then kicked off a repair, one node filled up the commit log volume with 7GB+ of log data, there was about 20 hours of log files. 

{noformat}
$ sudo ls -lah commitlog/
total 6.9G
drwx------ 2 cassandra cassandra  12K 2011-06-24 20:38 .
drwxr-xr-x 3 cassandra cassandra 4.0K 2011-06-25 01:47 ..
-rw------- 1 cassandra cassandra 129M 2011-06-24 01:08 CommitLog-1308876643288.log
-rw------- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308876643288.log.header
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 01:36 CommitLog-1308877711517.log
-rw-r--r-- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308877711517.log.header
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 02:20 CommitLog-1308879395824.log
-rw-r--r-- 1 cassandra cassandra   28 2011-06-24 20:47 CommitLog-1308879395824.log.header
...
-rw-r--r-- 1 cassandra cassandra 129M 2011-06-24 20:38 CommitLog-1308946745380.log
-rw-r--r-- 1 cassandra cassandra   36 2011-06-24 20:47 CommitLog-1308946745380.log.header
-rw-r--r-- 1 cassandra cassandra 112M 2011-06-24 20:54 CommitLog-1308947888397.log
-rw-r--r-- 1 cassandra cassandra   44 2011-06-24 20:47 CommitLog-1308947888397.log.header
{noformat}

The user KS has 2 CF's with 60 minute flush times. System KS had the default settings which is 24 hours. Will create another ticket see if these can be reduced or if it's something users should do, in this case it would not have mattered. 

I grabbed the log headers and used the tool in CASSANDRA-2828 and most of the segments had the system CF's marked as dirty.

{noformat}
$ bin/logtool dirty /tmp/logs/commitlog/

Not connected to a server, Keyspace and Column Family names are not available.

/tmp/logs/commitlog/CommitLog-1308876643288.log.header
Keyspace Unknown:
	Cf id 0: 444
/tmp/logs/commitlog/CommitLog-1308877711517.log.header
Keyspace Unknown:
	Cf id 1: 68848763
...
/tmp/logs/commitlog/CommitLog-1308944451460.log.header
Keyspace Unknown:
	Cf id 1: 61074
/tmp/logs/commitlog/CommitLog-1308945597471.log.header
Keyspace Unknown:
	Cf id 1000: 43175492
	Cf id 1: 108483
/tmp/logs/commitlog/CommitLog-1308946745380.log.header
Keyspace Unknown:
	Cf id 1000: 239223
	Cf id 1: 172211

/tmp/logs/commitlog/CommitLog-1308947888397.log.header
Keyspace Unknown:
	Cf id 1001: 57595560
	Cf id 1: 816960
	Cf id 1000: 0
{noformat}

CF 0 is the Status / LocationInfo CF and 1 is the HintedHandof CF. I dont have it now, but IIRC CFStats showed the LocationInfo CF with dirty ops. 

I was able to repo a case where flushing the CF's did not mark the log segments as obsolete (attached unit-test patch). Steps are:

1. Write to cf1 and flush.
2. Current log segment is marked as dirty at the CL position when the flush started, CommitLog.discardCompletedSegmentsInternal()
3. Do not write to cf1 again.
4. Roll the log, my test does this manually. 
5. Write to CF2 and flush.
6. Only CF2 is flushed because it is the only dirty CF. cfs.maybeSwitchMemtable() is not called for cf1 and so log segment 1 is still marked as dirty from cf1.

Step 5 is not essential, just matched what I thought was happening. I thought SystemTable.updateToken() was called which does not flush, and this was the last thing that happened.  

The expired memtable thread created by Table uses the same cfs.forceFlush() which is a no-op if the cf or it's secondary indexes are clean. 
    
I think the same problem would exist in 0.8. ",slebresne,amorton,Urgent,Resolved,Fixed,27/Jun/11 01:54,16/Apr/19 09:32
Bug,CASSANDRA-2831,12511644,Creating or updating CF key_validation_class with the CLI doesn't works,"In the command line:
{code}
create column family test with key_validation_class = 'AsciiType' and comparator = 'LongType' and default_validation_class = 'IntegerType';
describe keyspace;
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
{code}
The ""Default column value validator"" is BytesType instead of IntegerType. Also tested with other types or with the ""update column family"" command, same problem occur.

{code}
[default@Test] update column family test with default_validation_class = 'LongType';
51a37430-a0bb-11e0-0000-ef8993101fdf
Waiting for schema agreement...
... schemas agree across the cluster
[default@Test] describe keyspace;                                                   
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: test
      Key Validation Class: org.apache.cassandra.db.marshal.AsciiType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.571875/122/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: false
      Built indexes: []
{code}

Btw, they are a typo in file src/resources/org/apache/cassandra/cli/CliHelp.yaml line 642: key_valiation_class > key_validation_class
Very annoying for people like me who stupidly copy/paste the help.",,silvere,Normal,Resolved,Fixed,27/Jun/11 12:54,16/Apr/19 09:32
Bug,CASSANDRA-2835,12511983,CFMetadata don't set the default for Replicate_on_write correctly,"Replicate_on_write *must* default to true (defaulting to false is very dangerous and imho, the option of setting it to false shouldn't exist in the first place) and CFMetadata.DEFAULT_REPLICATE_ON_WRITE is correctly true. But it doesn't get set correctly. Instead, the code force the value of the cf_def even if it wasn't defined, resulting in setting it to false since that is the default value for a boolean in JAVA.",slebresne,slebresne,Normal,Resolved,Fixed,28/Jun/11 12:47,16/Apr/19 09:32
Bug,CASSANDRA-2837,12512038,Setting RR Chance via CliClient results in chance being too low,"running a command like 

{noformat}
""update column family shorturls with read_repair_chance=0.4;""
{noformat}

results in the value being set to 0.0040. Was expecting it to be 0.4.

Affects 0.7.6.-2; seems to be fixed on trunk/0.8.",jhermes,dehora,Low,Resolved,Fixed,28/Jun/11 20:04,16/Apr/19 09:32
Bug,CASSANDRA-2839,12512152,Small typos in the cli,"Memtable thresholds: %s/%s/%s (millions of ops/minutes/MB) was displaying ops/MB/minutes.

placement_strategy: the fully qualified class used to place replicas in
                        this keyspace. Valid values are
                        org.apache.cassandra.locator.SimpleStrategy,
                        org.apache.cassandra.locator.NetworkTopologyStrategy,
                        and org.apache.cassandra.locator.OldNetworkTopologyStrategy
was being displayed but would only accept SimpleStrategy.

",j.casares,j.casares,Low,Resolved,Fixed,29/Jun/11 00:08,16/Apr/19 09:32
Bug,CASSANDRA-2842,12512391,Hive JDBC connections fail with InvalidUrlException when both the C* and Hive JDBC drivers are loaded,"Hive connections fail with InvalidUrlException when both the C* and Hive JDBC drivers are loaded, and it seems the URL is being interpreted as a C* url.

{code}
	Caused an ERROR
    [junit] Invalid connection url:jdbc:hive://127.0.0.1:10000/default. should start with jdbc:cassandra
    [junit] org.apache.cassandra.cql.jdbc.InvalidUrlException: Invalid connection url:jdbc:hive://127.0.0.1:10000/default. should start with jdbc:cassandra
    [junit] 	at org.apache.cassandra.cql.jdbc.CassandraDriver.connect(CassandraDriver.java:90)
    [junit] 	at java.sql.DriverManager.getConnection(DriverManager.java:582)
    [junit] 	at java.sql.DriverManager.getConnection(DriverManager.java:185)
    [junit] 	at com.datastax.bugRepros.repro_connection_error.test1_runHiveBeforeJdbc(repro_connection_error.java:34)

{code}

*Code Snippet: intended to illustrate the connection issues* 
* Copy file to test directory
* Change package declaration
* run:  ant test -Dtest.name=repro_conn_error
{code}

package com.datastax.bugRepros;

import java.sql.DriverManager;
import java.sql.Connection;
import java.sql.SQLException;

import java.util.Enumeration;

import org.junit.Test;

public class repro_conn_error
{
    @Test
    public void jdbcConnectionError() throws Exception 
    {  
        // Create Hive JDBC Connection - will succeed if      
        try 
        {
            // Uncomment loading C* driver to reproduce bug
            Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
            
            // Load Hive driver and connect
            Class.forName(""org.apache.hadoop.hive.jdbc.HiveDriver"");
            Connection hiveConn = DriverManager.getConnection(""jdbc:hive://127.0.0.1:10000/default"", """", """");
            hiveConn.close();  
            System.out.println(""successful hive connection"");

        } catch (SQLException e) {
            System.out.println(""unsuccessful hive connection"");
            e.printStackTrace();
        }
        
        // Create C* JDBC Connection
        try 
        {
            Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
            Connection jdbcConn = DriverManager.getConnection(""jdbc:cassandra:root/root@127.0.0.1:9160/default"");     
            jdbcConn.close();    
            System.out.println(""successful c* connection"");

        } catch (SQLException e) {
            System.out.println(""unsuccessful c* connection"");

            e.printStackTrace();
        }
        
        // Print out all loaded JDBC drivers.
        Enumeration d = java.sql.DriverManager.getDrivers();
        
        while (d.hasMoreElements()) {
            Object driverAsObject = d.nextElement();
            System.out.println(""JDBC driver="" + driverAsObject);
        }
    }
}

{code}",ardot,cdaw,Low,Resolved,Fixed,30/Jun/11 23:01,16/Apr/19 09:32
Bug,CASSANDRA-2846,12512469,"Changing replication_factor using ""update keyspace"" not working","Unless I've misunderstood the new way to do this with 0.8 I think ""update keyspace"" is broken:

{code}
[default@unknown] create keyspace Test with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = [{replication_factor:1}];
37f70d40-a3e9-11e0-0000-242d50cf1fbf
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] describe keyspace Test;
Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
[default@unknown] update keyspace Test with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = [{replication_factor:2}];
489fe220-a3e9-11e0-0000-242d50cf1fbf
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] describe keyspace Test;                                                                                                                   Keyspace: Test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
{code}

Isn't the second ""describe keyspace"" supposed to to say ""replication_factor:2""?

Relevant bits from system.log:
{code}
Migration.java (line 116) Applying migration 489fe220-a3e9-11e0-0000-242d50cf1fbf Update keyspace Testrep strategy:SimpleStrategy{}durable_writes: true to Testrep strategy:SimpleStrategy{}durable_writes: true
UpdateKeyspace.java (line 74) Keyspace updated. Please perform any manual operations
{code}
",jbellis,jborgstrom,Low,Resolved,Fixed,01/Jul/11 14:15,16/Apr/19 09:32
Bug,CASSANDRA-2849,12512664,InvalidRequestException when validating column data includes entire column value,"If the column value fails to validate, then ThriftValidation.validateColumnData() calls bytesToHex() on the entire column value and puts this string in the Exception. Since the value may be up to 2GB, this is potentially a lot of extra memory. The value is likely to be logged (and presumably returned to the thrift client over the network?). This could cause a lot of slowdown or an unnecessary OOM crash, and is unlikely to be useful (the client has access to the full value anyway if required for debugging).

Also, the reason for the exception (extracted from the MarshalException) is printed _after_ the data, so if there's any truncation in the logging system at any point, the reason will be lost. 

The reason should be displayed before the column value, and the column value should be truncated in the Exception message.",dallsopp,dallsopp,Low,Resolved,Fixed,03/Jul/11 11:41,16/Apr/19 09:32
Bug,CASSANDRA-2851,12512673,hex-to-bytes conversion accepts invalid inputs silently,"FBUtilities.hexToBytes() has a minor bug - it copes with single-character inputs by prepending ""0"", which is OK - but it does this for any input with an odd number of characters, which is probably incorrect.

{noformat}
if (str.length() % 2 == 1)
    str = ""0"" + str;
{noformat}

Given 'fff' as an input, can we really assume that this should be '0fff'? Isn't this just an error?

Add the following to FBUtilitiesTest to demonstrate:

{noformat}
String[] badvalues = new String[]{""000"", ""fff""};
       
for (int i = 0; i < badvalues.length; i++)
    try
    {
        FBUtilities.hexToBytes(badvalues[i]);
        fail(""Invalid hex value accepted""+badvalues[i]);
    } catch (Exception e){}
{noformat}",slebresne,dallsopp,Low,Resolved,Fixed,03/Jul/11 18:04,16/Apr/19 09:32
Bug,CASSANDRA-2852,12512674,Cassandra CLI - Import Keyspace Definitions from File - Comments do partitially interpret characters/commands,"Hello, 

using: bin/cassandra-cli -host localhost --file conf/schema-sample.txt

with schema-sample.txt having contents like this:

/* here are a lot of comments,
like this sample create keyspace;
and so on
*/

Will result in an error: 
Line 1 => Syntax Error at Position 323: mismatched charackter '<EOF>' expecting '*'

The Cause is the keyspace; statement => the semicolon "";"" causes the error.

However:

Writing the word ""keyspace;"" with quotes, does NOT lead to the error.
so this works: 
/* here are a lot of comments,
like this sample create ""keyspace;""
and so on
*/

From my point of view this is an error. Everyting between the ""Start Comment"" => /* and ""End Comment"" => */ Should be treated as a comment and not be interpreted in any way. Thats the definition of a comment, to be not interpreted at all. 

Or this must be documented somewhere very prominently, otherwise this will lead to unnecessary wasting of time searching for this odd behavoiur. And it makes ""commenting out"" statements much more cumbersome.

Plattform: Windows Vista

thanks
",xedin,jensmueller,Low,Resolved,Fixed,03/Jul/11 18:23,16/Apr/19 09:32
Bug,CASSANDRA-2853,12512678,cassandra-cli has backwards index status message,"When a secondary index is building, the total bytes and processed bytes are swapped in the message.  Example:
Currently building index cf1, completed 12052040551 of 18047343 bytes.

The problem is a call to CompactionInfo constructor with swapped parameters.  Patch to follow.",stinkymatt,stinkymatt,Low,Resolved,Fixed,03/Jul/11 20:25,16/Apr/19 09:32
Bug,CASSANDRA-2854,12512681,Java Build Path is broken in Eclipse after running generate-eclipse-files Ant target,"Following the instructions in http://wiki.apache.org/cassandra/RunningCassandraInEclipse, but checking out v0.8.1:

After running the 'generate-eclipse-files' Ant target, the build path is set up to include all the libraries in build/lib/jars, but each library has ;C (semicolon, letter C) appended to it, so Eclipse can't find the libraries - there are about 55 errors like:

Project 'cassandra-0.8.1' is missing required library: '\Users\David\eclipse_workspace\cassandra-0.8.1\build\lib\jars\commons-cli-1.2.jar;C'
",dallsopp,dallsopp,Low,Resolved,Fixed,03/Jul/11 22:37,16/Apr/19 09:32
Bug,CASSANDRA-2857,12512883,initialize log4j correctly in EmbeddedCassandraService,"Currently, ECS.cleanUpOldStuff calls CleanupHelper.cleanupAndLeaveDirs(), which initialized DatabaseDescriptor which does some logging.  When we go to initialize log4j later in AbstractCassandraService, it's too late.",jbellis,jbellis,Low,Resolved,Fixed,05/Jul/11 19:51,16/Apr/19 09:32
Bug,CASSANDRA-2860,12512897,Versioning works *too* well,"The scenario goes something like this: you upgrade from 0.7 to 0.8, but all the nodes remember that the remote side is 0.7, so they in turn speak 0.7, causing the local node to also think the remote is 0.7, even though both are really 0.8.",jbellis,brandon.williams,Normal,Resolved,Fixed,05/Jul/11 21:28,16/Apr/19 09:32
Bug,CASSANDRA-2863,12512969,NPE when writing SSTable generated via repair,"A NPE is generated during repair when closing an sstable generated via SSTable build. It doesn't happen always. The node had been scrubbed and compacted before calling repair.

 INFO [CompactionExecutor:2] 2011-07-06 11:11:32,640 SSTableReader.java (line 158) Opening /d2/cassandra/data/sbs/walf-g-730
ERROR [CompactionExecutor:2] 2011-07-06 11:11:34,327 AbstractCassandraDaemon.java (line 113) Fatal exception in thread Thread[CompactionExecutor:2,1,main] 
java.lang.NullPointerException
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.close(SSTableWriter.java:382)
	at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:370)
	at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:315)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1103)
	at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1094)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",slebresne,hector,Normal,Resolved,Fixed,06/Jul/11 11:32,16/Apr/19 09:32
Bug,CASSANDRA-2867,12513118,Starting 0.8.1 after upgrade from 0.7.6-2 fails,"After upgrading the binaries to 0.8.1 I get an exception when starting cassandra:

{noformat}
[root@bserv2 local]#  INFO 12:51:04,512 Logging initialized
 INFO 12:51:04,523 Heap size: 8329887744/8329887744
 INFO 12:51:04,524 JNA not found. Native methods will be disabled.
 INFO 12:51:04,531 Loading settings from file:/usr/local/apache-cassandra-0.8.1/conf/cassandra.yaml
 INFO 12:51:04,621 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 12:51:04,707 Global memtable threshold is enabled at 2648MB
 INFO 12:51:04,708 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,713 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,714 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,716 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,717 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,719 Removing compacted SSTable files (see http://wiki.apache.org/cassandra/MemtableSSTable)
 INFO 12:51:04,770 reading saved cache /vm1/cassandraDB/saved_caches/system-IndexInfo-KeyCache
 INFO 12:51:04,776 Opening /vm1/cassandraDB/data/system/IndexInfo-f-9
 INFO 12:51:04,792 reading saved cache /vm1/cassandraDB/saved_caches/system-Schema-KeyCache
 INFO 12:51:04,794 Opening /vm1/cassandraDB/data/system/Schema-f-194
 INFO 12:51:04,797 Opening /vm1/cassandraDB/data/system/Schema-f-195
 INFO 12:51:04,802 Opening /vm1/cassandraDB/data/system/Schema-f-193
 INFO 12:51:04,811 Opening /vm1/cassandraDB/data/system/Migrations-f-193
 INFO 12:51:04,814 reading saved cache /vm1/cassandraDB/saved_caches/system-LocationInfo-KeyCache
 INFO 12:51:04,815 Opening /vm1/cassandraDB/data/system/LocationInfo-f-292
 INFO 12:51:04,843 Loading schema version 586e70fd-a332-11e0-828e-34b74a661156
ERROR 12:51:04,996 Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 15
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.config.CFMetaData.getDefaultIndexName(CFMetaData.java:971)
        at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:381)
        at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:172)
        at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:479)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:139)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:315)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
Exception encountered during startup.
org.apache.cassandra.db.marshal.MarshalException: A long is exactly 8 bytes: 15
        at org.apache.cassandra.db.marshal.LongType.getString(LongType.java:72)
        at org.apache.cassandra.config.CFMetaData.getDefaultIndexName(CFMetaData.java:971)
        at org.apache.cassandra.config.CFMetaData.inflate(CFMetaData.java:381)
        at org.apache.cassandra.config.KSMetaData.inflate(KSMetaData.java:172)
        at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
        at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:479)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:139)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:315)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:80)
{noformat}

It seems this has something to do with indexes, and I do have a CF with an index on it, but it is not used.
I can try and remove the index with 0.7.x binaries, but I will wait a bit to see if anyone needs it to reproduce the bug.",jbellis,kunda,Normal,Resolved,Fixed,07/Jul/11 10:20,16/Apr/19 09:32
Bug,CASSANDRA-2868,12513152,Native Memory Leak,"We have memory issues with long running servers. These have been confirmed by several users in the user list. That's why I report.

The memory consumption of the cassandra java process increases steadily until it's killed by the os because of oom (with no swap)

Our server is started with -Xmx3000M and running for around 23 days.

pmap -x shows

Total SST: 1961616 (mem mapped data and index files)
Anon  RSS: 6499640
Total RSS: 8478376

This shows that > 3G are 'overallocated'.

We will use BRAF on one of our less important nodes to check wether it is related to mmap and report back.",brandon.williams,doubleday,Low,Resolved,Fixed,07/Jul/11 14:05,16/Apr/19 09:32
Bug,CASSANDRA-2869,12513172,CassandraStorage does not function properly when used multiple times in a single pig script due to UDFContext sharing issues,"CassandraStorage appears to have threading issues along the lines of those described at http://pig.markmail.org/message/oz7oz2x2dwp66eoz due to the sharing of the UDFContext.

I believe the fix lies in implementing
{code}
public void setStoreFuncUDFContextSignature(String signature)
    {
    }
{code}

and then using that signature when getting the UDFContext.

From the Pig manual:
{quote}
setStoreFunc!UDFContextSignature(): This method will be called by Pig both in the front end and back end to pass a unique signature to the Storer. The signature can be used to store into the UDFContext any information which the Storer needs to store between various method invocations in the front end and back end. The default implementation in StoreFunc has an empty body. This method will be called before other methods.
{quote}",jeromatron,gsingers,Normal,Resolved,Fixed,07/Jul/11 18:26,16/Apr/19 09:32
Bug,CASSANDRA-2870,12513176,dynamic snitch + read repair off can cause LOCAL_QUORUM reads to return spurious UnavailableException,"When Read Repair is off, we want to avoid doing requests to more nodes than necessary to satisfy the ConsistencyLevel.  ReadCallback does this here:

{code}
        this.endpoints = repair || resolver instanceof RowRepairResolver
                       ? endpoints
                       : endpoints.subList(0, Math.min(endpoints.size(), blockfor)); // min so as to not throw exception until assureSufficient is called
{code}

You can see that it is assuming that the ""endpoints"" list is sorted in order of preferred-ness for the read.

Then the LOCAL_QUORUM code in DatacenterReadCallback checks to see if we have enough nodes to do the read:

{code}
        int localEndpoints = 0;
        for (InetAddress endpoint : endpoints)
        {
            if (localdc.equals(snitch.getDatacenter(endpoint)))
                localEndpoints++;
        }

        if (localEndpoints < blockfor)
            throw new UnavailableException();
{code}

So if repair is off (so we truncate our endpoints list) AND dynamic snitch has decided that nodes in another DC are to be preferred over local ones, we'll throw UE even if all the replicas are healthy.",jbellis,jbellis,Low,Resolved,Fixed,07/Jul/11 19:22,16/Apr/19 09:32
Bug,CASSANDRA-2872,12513235,"While dropping and recreating an index, incremental snapshotting can hang","When creating a hard link (at list with JNA), link() hang if the target of the
link already exists. In theory though, we should not hit that situation
because we use a new directory for each manual snapshot and the generation
number of the sstables should prevent this from hapenning with increment
snapshot.

However, when you drop, then recreate a secondary index, if the sstables are
deleted after the drop and before we recreate the index, the recreated index
sstables will start with a generation to 0. Thus, when we start backuping them
incrementally, it will conflict with the sstables of the previously dropped
index.

First, we should check for the target existance because calling link() to at
least avoid hanging. But then we must make sure that when we drop, then
recreate an index, we will either not name the sstables the same way or the
incremental snapshot use a different directory.
",jbellis,slebresne,Low,Resolved,Fixed,08/Jul/11 08:59,16/Apr/19 09:32
Bug,CASSANDRA-2873,12513260,Typo in src/java/org/apache/cassandra/cli/CliClient  ,"I have read your documentation about syntax for creating column family and parameters that I can pass.
According to documentation i can use parameter :

"" - keys_cache_save_period: Duration in seconds after which Cassandra should
  safe the keys cache. Caches are saved to saved_caches_directory as
  specified in conf/Cassandra.yaml. Default is 14400 or 4 hours. ""

but then i was receiving error: ""No enum const class org.apache.cassandra.cli.CliClient$ColumnFamilyArgument.KEYS_CACHE_SAVE_PERIOD""


In class mentioned in title we have:

protected enum ColumnFamilyArgument
115 	{
116 	COLUMN_TYPE,
117 	COMPARATOR,
118 	SUBCOMPARATOR,
119 	COMMENT,
120 	ROWS_CACHED,
121 	ROW_CACHE_SAVE_PERIOD,
122 	KEYS_CACHED,
123 	KEY_CACHE_SAVE_PERIOD,   <---- TYPO !
124 	READ_REPAIR_CHANCE,
125 	GC_GRACE,
126 	COLUMN_METADATA,
127 	MEMTABLE_OPERATIONS,
128 	MEMTABLE_THROUGHPUT,
129 	MEMTABLE_FLUSH_AFTER,
130 	DEFAULT_VALIDATION_CLASS,
131 	MIN_COMPACTION_THRESHOLD,
132 	MAX_COMPACTION_THRESHOLD,
133 	REPLICATE_ON_WRITE,
134 	ROW_CACHE_PROVIDER,
135 	KEY_VALIDATION_CLASS
136 	} ",jbellis,e1n,Low,Resolved,Fixed,08/Jul/11 13:25,16/Apr/19 09:32
Bug,CASSANDRA-2880,12513647,example configuration of commitlog_sync: batch,"There is no example of commitlog_sync: batch configuration in default config file, and one have to guess that commitlog_sync_batch_window_in_ms should be configured instead of CommitLogSyncBatchWindowInMS.",wmeler,wmeler,Low,Resolved,Fixed,11/Jul/11 09:17,16/Apr/19 09:32
Bug,CASSANDRA-2881,12513657,RPM classpath evaluation include current directory (-cp:),"/usr/share/cassandra/cassandra.in.sh builds CLASSPATH in a way that cause current directory inclusion (-cp:).
This should be avioded as can effect in config file change if one is present in current directory.",wmeler,wmeler,Low,Resolved,Fixed,11/Jul/11 12:12,16/Apr/19 09:32
Bug,CASSANDRA-2898,12513978,Escape characters in CQL,"When trying to get all the columns named ""fmd:"" in cqlsh you can not escape : or ;

As per Jonathan Ellis:
You can escape quotes but I don't think you can escape semicolons.

Try:
sqlsh> select 'fmd:'..'fmd;' from feeds;",brandon.williams,bcvisin,Normal,Resolved,Fixed,13/Jul/11 20:07,16/Apr/19 09:32
Bug,CASSANDRA-2899,12514105,cli silently fails when classes are quoted,"For example: CREATE COLUMN FAMILY autocomplete_meta WITH comparator = 'UTF8Type' AND default_validation_class = 'UTF8Type' AND key_validation_class = 'UTF8Type'

Neither validation class is actually set, but if you remove the quotes everything works.",xedin,brandon.williams,Low,Resolved,Fixed,14/Jul/11 18:38,16/Apr/19 09:32
Bug,CASSANDRA-2902,12514246,"""count"" doesn't accept UUIDs in CLI even though ""get"" does","[default@V360HC_SCHEMA1] get RawValues[7dc75c1c-8af0-462a-a920-bc1dafc44f31] limit 1;
=> (column=1310593550317, value=aced00057709053fe9cc17a95b9093, timestamp=1310593550583438)
Returned 1 results.

[default@V360HC_SCHEMA1] count RawValues[7dc75c1c-8af0-462a-a920-bc1dafc44f31];
UUIDs must be exactly 16 bytes
",xedin,mdennis,Low,Resolved,Fixed,15/Jul/11 16:28,16/Apr/19 09:32
Bug,CASSANDRA-2906,12514348,Streaming SSTable build does not use cleanupIfNecessary,"The new streaming sstable rebuilding in IncomingStreamReader needs to wrap things in with {{try, finally, cleanupIfNecessary}} to ensure that the writer is cleaned up properly.",yukim,stuhood,Low,Resolved,Fixed,17/Jul/11 06:22,16/Apr/19 09:32
Bug,CASSANDRA-2907,12514395,durable_writes flag cannot be changed via the CLI (system does not process KsDef.durable_writes option properly).,"I am unable to change the durable_writes option in the CLI. Here are the commands to replicate the problem on a clean install:

create keyspace test;
update keyspace test with durable_writes=false;
show keyspaces;

It will still say:

Keyspace: test:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [datacenter1:1]
  Column Families:


PS: I looked in the tests of the CLI code of CASSANDRA-2683 and saw that the feature actually is not properly tested: the flag is set, but never tested.",xedin,benschrauwen,Low,Resolved,Fixed,18/Jul/11 03:53,16/Apr/19 09:32
Bug,CASSANDRA-2908,12514404,Fix bulkload JMX call,"The bulkload JMX call is supposed to simplify bulkloading when done from a Cassandra node (so you don't have to configure the bulkloading client to not conflict with the node itself), but that call doesn't work (it forgets to add the ranges to stream).",slebresne,slebresne,Low,Resolved,Fixed,18/Jul/11 07:23,16/Apr/19 09:32
Bug,CASSANDRA-2912,12514451,CQL ignores client timestamp for full row deletion,,slebresne,slebresne,Low,Resolved,Fixed,18/Jul/11 14:46,16/Apr/19 09:32
Bug,CASSANDRA-2916,12514491,Streaming estimatedKey calculation should never be 0,"The new in-streaming SSTable rebuild uses the sender's estimated key calculation to determine which codepath to take: in some cases, samples can result in an estimated key count of 0.",stuhood,stuhood,Low,Resolved,Fixed,18/Jul/11 19:41,16/Apr/19 09:32
Bug,CASSANDRA-2928,12514860,Fix Hinted Handoff replay,"Broken in CASSANDRA-2668. Brandon explains:

bq. the Ack and Ack2 verb handlers are applying a new ep state every time there is a generation change via Gossiper.applyStateLocally, so it's always unset initially when the node starts up. state.hasToken() is set in the Gossiper's status check, which won't have happened when the onAlive event is sent to SS.",jbellis,jbellis,Normal,Resolved,Fixed,21/Jul/11 03:10,16/Apr/19 09:32
Bug,CASSANDRA-2929,12514884,Don't include tmp files as sstable when create column families,"When we open a column family and populate the SSTableReader, we happen to include -tmp files. This has no change to actually happen in a real life situation, but that is what was triggering a race in the unit tests triggering spurious assertion failure in estimateRowsFromIndex.",slebresne,slebresne,Low,Resolved,Fixed,21/Jul/11 09:51,16/Apr/19 09:32
Bug,CASSANDRA-2933,12514940,nodetool hangs (doesn't return prompt) if you specify a table that doesn't exist or a KS that has no CF's,"Invalid CF
{code}
ERROR 02:18:18,904 Fatal exception in thread Thread[AntiEntropyStage:3,5,main]
java.lang.IllegalArgumentException: Unknown table/cf pair (StressKeyspace.StressStandard)
	at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:147)
	at org.apache.cassandra.service.AntiEntropyService$TreeRequestVerbHandler.doVerb(AntiEntropyService.java:601)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}


Empty KS
{code}
 INFO 02:19:21,483 Waiting for repair requests: []
 INFO 02:19:21,484 Waiting for repair requests: []
 INFO 02:19:21,484 Waiting for repair requests: []
{code}",yukim,cdaw,Low,Resolved,Fixed,21/Jul/11 16:02,16/Apr/19 09:32
Bug,CASSANDRA-2937,12514978,certain generic type causes compile error in eclipse,"the code ColumnFamily and AbstractColumnContainer uses code similar to the following (substitute Blah with AbstractColumnContainer.DeletionInfo):



import java.util.concurrent.atomic.AtomicReference;
public class TestPrivateAtomicRef {
    protected final AtomicReference<Blah> b = new AtomicReference<Blah>(new Blah());
    // the following form would work for eclipse
//    protected final AtomicReference b = new AtomicReference(new Blah());

    private static class Blah {
    }
}


class Child extends TestPrivateAtomicRef {    
    public void aaa() {
        Child c = new Child();
        c.b.set(
        b.get()  //<==== eclipse shows error here
        );
    }
}


in eclipse, the above code generates compile error, but works fine under java command line. since many people use eclipse, it's better to 
make a temporary compromise and make DeletionInfo protected",,yangyangyyy,Low,Resolved,Fixed,21/Jul/11 23:22,16/Apr/19 09:32
Bug,CASSANDRA-2939,12515057,CQL regex to match column family in query,"In the file cursor.py (https://svn.apache.org/repos/asf/cassandra/drivers/py/cql/cursor.py)

{code}Line 37: _cfamily_re = re.compile(""\s*SELECT\s+.+\s+FROM\s+[\']?(\w+)"", re.I | re.M){code}

The Regex will improperly match anything after the word  'from' even if it is in the WHERE clause

I believe the fix is:

{code}_cfamily_re = re.compile(""\s*SELECT\s+.+?\s+FROM\s+[\']?(\w+)"", re.I | re.M){code}

Added the ? so the regex is not so greedy

use this query to reproduce the results:

SELECT key FROM column_family WHERE key = 'break from chores'""",bcvisin,bcvisin,Low,Resolved,Fixed,22/Jul/11 19:33,16/Apr/19 09:32
Bug,CASSANDRA-2942,12515146,Dropped columnfamilies can leave orphaned data files that do not get cleared on restart,"* Bring up 3 node cluster
* From node1: Run Stress Tool
{code} stress --num-keys=10 --columns=10 --consistency-level=ALL --average-size-values --replication-factor=3 --nodes=node1,node2 {code}
* Shutdown node3
* From node1: drop the Standard1 CF in Keyspace1
* Shutdown node2 and node3
* Bring up node1 and node2. Check that the Standard1 files are gone.
{code}
ls -al /var/lib/cassandra/data/Keyspace1/
{code}
* Bring up node3. The log file shows the drop column family occurs
{code}
 INFO 00:51:25,742 Applying migration 9a76f880-b4c5-11e0-0000-8901a7c5c9ce Drop column family: Keyspace1.Standard1
{code}
* Restart node3 to clear out dropped tables from the filesystem
{code}
root@cathy3:~/cass-0.8/bin# ls -al /var/lib/cassandra/data/Keyspace1/
total 36
drwxr-xr-x 3 root root 4096 Jul 23 00:51 .
drwxr-xr-x 6 root root 4096 Jul 23 00:48 ..
-rw-r--r-- 1 root root    0 Jul 23 00:51 Standard1-g-1-Compacted
-rw-r--r-- 2 root root 5770 Jul 23 00:51 Standard1-g-1-Data.db
-rw-r--r-- 2 root root   32 Jul 23 00:51 Standard1-g-1-Filter.db
-rw-r--r-- 2 root root  120 Jul 23 00:51 Standard1-g-1-Index.db
-rw-r--r-- 2 root root 4276 Jul 23 00:51 Standard1-g-1-Statistics.db
drwxr-xr-x 3 root root 4096 Jul 23 00:51 snapshots
{code}
*Bug:  The files for Standard1 are orphaned on node3*

",jbellis,cdaw,Low,Resolved,Fixed,23/Jul/11 00:57,16/Apr/19 09:32
Bug,CASSANDRA-2946,12515285,HintedHandoff fails with could not reach schema agreement,"To reproduce, have two nodes A and B.

1. On node A, create a keyspace with replication factor 1 and add a column family
2. Ensure node B has created the keyspace and column family
3. Take down node B
4. Insert some keys to A at CL.ANY, ensuring some keys should be written to B
5. Bring up node B
6. When hints are delivered, I get the error:

ERROR [HintedHandoff:1] 2011-07-25 17:19:14,729 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.RuntimeException: Could not reach schema agreement with /10.2.129.9 in 60000ms
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.RuntimeException: Could not reach schema agreement with /10.2.129.9 in 60000ms
        at org.apache.cassandra.db.HintedHandOffManager.waitForSchemaAgreement(HintedHandOffManager.java:290)
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:301)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:89)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:394)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more

If I use DatabaseDescriptor.getDefsVersion() instead of gossiper.getEndpointStateForEndpoint(FBUtilities.getLocalAddress()).getApplicationState(ApplicationState.SCHEMA) then the error goes away, and the hints are correctly delivered.

This may be the same issue as Aaron saw here: http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/ApplicationState-Schema-has-drifted-from-DatabaseDescriptor-td6006576.html, and may be related to CASSANDRA-2083.",jbellis,richardlow,Normal,Resolved,Fixed,25/Jul/11 16:38,16/Apr/19 09:32
Bug,CASSANDRA-2948,12515417,Nodetool move fails to stream out data from moved node to new endpoint.,"When moving a node in the ring with nodetool move, that node streams its data to itself instead of to the new endpoint responsible for its old range.

Steps to reproduce:
* Create a cluster (A,B,C,D) with tokens (0,4,8,C) using ByteOrderedPartitioner
* Create a keyspace and CF with RF=1
* Insert keys (2,6,A,E). This should put one key on each node.
* Move node A to token 7. This should cause:
  + node C streams key 6 to node A
  + node A streams key E to node B
  However instead, node A streams key E to itself.

Selected log messages from node A:
 INFO [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:17,075 StorageService.java (line 1878) Moving miles/10.2.129.41 from Token(bytes[00]) to Token(bytes[07]).
DEBUG [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:17,080 StorageService.java (line 1941) Table ks: work map {/10.2.129.16=[(Token(bytes[04]),Token(bytes[07])]]}.
 INFO [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:17,080 StorageService.java (line 1946) Sleeping 30000 ms before start streaming/fetching ranges.
...
 INFO [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:46,728 StorageService.java (line 522) Moving: fetching new ranges and streaming old ranges
DEBUG [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:46,728 StorageService.java (line 1960) [Move->STREAMING] Work Map: {ks={(Token(bytes[0c]),Token(bytes[00])]=[miles/10.2.129.41]}}
DEBUG [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:46,729 StorageService.java (line 1965) [Move->FETCHING] Work Map: {ks={/10.2.129.16=[(Token(bytes[04]),Token(bytes[07])]]}}
DEBUG [RMI TCP Connection(6)-10.2.129.41] 2011-07-26 16:29:46,730 StorageService.java (line 2411) Requesting from /10.2.129.16 ranges (Token(bytes[04]),Token(bytes[07])]
...
 INFO [StreamStage:1] 2011-07-26 16:29:46,737 StreamOut.java (line 90) Beginning transfer to miles/10.2.129.41
DEBUG [StreamStage:1] 2011-07-26 16:29:46,737 StreamOut.java (line 91) Ranges are (Token(bytes[0c]),Token(bytes[00])]

This appears to be caused because in StorageService.move we call
    Gossiper.instance.addLocalApplicationState(ApplicationState.STATUS, valueFactory.moving(newToken));
and then get the new token metadata in order to calculate where the new endpoint is that we should stream to
    TokenMetadata tokenMetaClone = tokenMetadata_.cloneAfterAllSettled();
however, in addLocalApplicationState there is no notification broadcast for the change in local state, so tokenMetadata_ never updates the list of moving nodes, and the tokenMetaClone is still the state of the ring from before the move.

",soverton,soverton,Urgent,Resolved,Fixed,26/Jul/11 15:42,16/Apr/19 09:32
Bug,CASSANDRA-2949,12515424,Batch mutation of counters in multiple supercolumns throws an exception during replication.,"Steps to reproduce:
* Perform a batch mutation of more than one counter in more than one super-column in the same column-family.
* The following exception is thrown during replication:

DEBUG [MutationStage:63] 2011-07-26 17:05:12,653 CounterMutationVerbHandler.java (line 52) Applying forwarded CounterMutation(RowMutation(keyspace='ks1', key='4ae71336e44bf9bf', modifications=[ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:8@1311696312648,]),SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:8@1311696312648,]),])]), QUORUM)
DEBUG [MutationStage:63] 2011-07-26 17:05:12,653 StorageProxy.java (line 432) insert writing local & replicate CounterMutation(RowMutation(keyspace='ks1', key='4ae71336e44bf9bf', modifications=[cf1]), QUORUM)
DEBUG [MutationStage:63] 2011-07-26 17:05:12,654 Table.java (line 398) applying mutation of row 4ae71336e44bf9bf
ERROR [ReplicateOnWriteStage:125] 2011-07-26 17:05:12,655 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[ReplicateOnWriteStage:125,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: ColumnFamily ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),]) already has modifications in this mutation: ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),])
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
Caused by: java.lang.IllegalArgumentException: ColumnFamily ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c31 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),]) already has modifications in this mutation: ColumnFamily(cf1 [SuperColumn(302c7375706572636f6c30 [302c636f6c30:false:[{cad93dc0-b7a0-11e0-0000-123f813dd5df, 3, 3}*]@1311696312648!-9223372036854775808,]),])
        at org.apache.cassandra.db.RowMutation.add(RowMutation.java:123)
        at org.apache.cassandra.db.CounterMutation.makeReplicationMutation(CounterMutation.java:120)
        at org.apache.cassandra.service.StorageProxy$5$1.runMayThrow(StorageProxy.java:455)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
",slebresne,soverton,Urgent,Resolved,Fixed,26/Jul/11 16:19,16/Apr/19 09:32
Bug,CASSANDRA-2950,12515455,Data from truncated CF reappears after server restart,"* Configure 3 node cluster
* Ensure the java stress tool creates Keyspace1 with RF=3

{code}
// Run Stress Tool to generate 10 keys, 1 column
stress --operation=INSERT -t 2 --num-keys=50 --columns=20 --consistency-level=QUORUM --average-size-values --replication-factor=3 --create-index=KEYS --nodes=cathy1,cathy2

// Verify 50 keys in CLI
use Keyspace1; 
list Standard1; 

// TRUNCATE CF in CLI
use Keyspace1;
truncate counter1;
list counter1;

// Run stress tool and verify creation of 1 key with 10 columns
stress --operation=INSERT -t 2 --num-keys=1 --columns=10 --consistency-level=QUORUM --average-size-values --replication-factor=3 --create-index=KEYS --nodes=cathy1,cathy2

// Verify 1 key in CLI
use Keyspace1; 
list Standard1; 

// Restart all three nodes

// You will see 51 keys in CLI
use Keyspace1; 
list Standard1; 
{code}


",jbellis,cdaw,Normal,Resolved,Fixed,26/Jul/11 20:56,16/Apr/19 09:32
Bug,CASSANDRA-2951,12515459,FreeableMemory can be accessed after it is invalid,"SerializingCache.get looks like this:

{code}
    public V get(Object key)
    {
        FreeableMemory mem = map.get(key);
        if (mem == null)
            return null;
        return deserialize(mem);
    }
{code}

If a cache object is evicted or replaced after the get happens, but before deserialize completes, we will trigger an assertion failure (if asserts are enabled) or segfault (if they are not).",jbellis,jbellis,Low,Resolved,Fixed,26/Jul/11 22:02,16/Apr/19 09:32
Bug,CASSANDRA-2952,12515482,"cassandra.bat fails when CASSANDRA_HOME contains a whitespace, again","I installed cassandra into C:\Program Files\apache-cassandra and tried to start cassandra. But cassandra.bat fails with following error.

{code} 
C:\Program Files\apache-cassandra>bin\cassandra.bat
Starting Cassandra Server
Error opening zip file or JAR manifest missing : C:\Program
Error occurred during initialization of VM
agent library failed to init: instrument
{code}

This problem is similar to CASSANDRA-2237. I'll post a patch to fix the problem later.",miau,miau,Low,Resolved,Fixed,27/Jul/11 07:24,16/Apr/19 09:32
Bug,CASSANDRA-2956,12515544,JDBC ResultSet improperly handles null column values,"JDBC {{ResultSet}} getters return built-in datatypes such as {{int, long, short, byte, boolean}} that are not capable of handling a null value. As a consequence, it provides a method: {{wasNull}} which returns true if the value was null. The spec requires a zero numeric value (or false in the case of {{boolean}} ) is returned by the getter. This was being mis-handled and a null value was being cast (boxed) to the return value. An NPE would result.
",ardot,ardot,Low,Resolved,Fixed,27/Jul/11 15:56,16/Apr/19 09:32
Bug,CASSANDRA-2957,12515546,JDBC Unit test failed to run due to spurious character in text and bad YAML entry,"Problem #1:

Bad character in the text (line 294, col 1):
{code}
˜        PreparedStatement stmt = con.prepareStatement(""update JdbcInteger set ?=?, ?=? where key = ?"");
{code}

Problem #2:
Outdated YAML directive (line 17):

{code}
commitlog_rotation_threshold_in_mb: 128
{code}
",ardot,ardot,Low,Resolved,Fixed,27/Jul/11 16:08,16/Apr/19 09:32
Bug,CASSANDRA-2958,12515554,Flush memtables on shutdown when durable writes are disabled,"Memtables need to be flushed on shutdown when durable_writes is set to false, otherwise data loss occurs as the data is not available to be replayed from the commit log. ",jbellis,electrum,Normal,Resolved,Fixed,27/Jul/11 17:26,16/Apr/19 09:32
Bug,CASSANDRA-2959,12515557,long-test fails to build,"build-test:
    [javac] /var/lib/jenkins/jobs/Cassandra/workspace/build.xml:910: warning: 'includeantruntime' was not set, defaulting to build.sysclasspath=last; set to false for repeatable builds
    [javac] Compiling 125 source files to /var/lib/jenkins/jobs/Cassandra/workspace/build/test/classes
    [javac] /var/lib/jenkins/jobs/Cassandra/workspace/test/unit/org/apache/cassandra/service/RemoveTest.java:172: removingNonlocal(org.apache.cassandra.dht.Token) in org.apache.cassandra.gms.VersionedValue.VersionedValueFactory cannot be applied to (org.apache.cassandra.dht.Token,org.apache.cassandra.dht.Token)
    [javac]                     valueFactory.removingNonlocal(endpointTokens.get(1), removaltoken));
    [javac]                                 ^
    [javac] /var/lib/jenkins/jobs/Cassandra/workspace/test/unit/org/apache/cassandra/service/RemoveTest.java:189: removedNonlocal(org.apache.cassandra.dht.Token) in org.apache.cassandra.gms.VersionedValue.VersionedValueFactory cannot be applied to (org.apache.cassandra.dht.Token,org.apache.cassandra.dht.Token)
    [javac]                     valueFactory.removedNonlocal(endpointTokens.get(1), removaltoken));
    [javac]                                 ^
    [javac] Note: Some input files use or override a deprecated API.
    [javac] Note: Recompile with -Xlint:deprecation for details.
    [javac] Note: Some input files use unchecked or unsafe operations.
    [javac] Note: Recompile with -Xlint:unchecked for details.
    [javac] 2 errors

BUILD FAILED
/var/lib/jenkins/jobs/Cassandra/workspace/build.xml:910: Compile failed; see the compiler error output for details.
",,mallen,Low,Resolved,Fixed,27/Jul/11 18:30,16/Apr/19 09:32
Bug,CASSANDRA-2960,12515563,replication_factor > 1 always causes cassandra to return null,"On a brand new cluster:
	
[default@SimpleTest] create keyspace SimpleTest2 with strategy_options = [{replication_factor:3}];              
16babc60-b886-11e0-0000-c9ff69cb2dfb
Waiting for schema agreement...
... schemas agree across the cluster

[default@SimpleTest] use SimpleTest2;
Authenticated to keyspace: SimpleTest2

[default@SimpleTest2] create column family CFTest with comparator=UTF8Type and default_validation_class=UTF8Type;
1f108660-b886-11e0-0000-c9ff69cb2dfb
Waiting for schema agreement...
... schemas agree across the cluster

[default@SimpleTest2] set CFTest['1']['text'] = 'test';
null

[default@SimpleTest2] get CFTest['1'];
null

[default@SimpleTest2] list CFTest;
Using default limit of 100
null

[default@SimpleTest2] describe cluster;
Cluster Information:
   Snitch: org.apache.cassandra.locator.SimpleSnitch
   Partitioner: org.apache.cassandra.dht.RandomPartitioner
   Schema versions: 
	1f108660-b886-11e0-0000-c9ff69cb2dfb: [10.60.98.20, 10.60.98.24, 10.60.98.26]",jbellis,stevecorona,Low,Resolved,Fixed,27/Jul/11 19:37,16/Apr/19 09:32
Bug,CASSANDRA-2964,12515619,IndexRangeSliceQuery results include index column even though it is not in SliceRange,When an IndexSlicwQuery is done the result contains the index column even though it was not in the slice range.,jbellis,rjtg,Normal,Resolved,Fixed,28/Jul/11 09:19,16/Apr/19 09:32
Bug,CASSANDRA-2968,12515766,AssertionError during compaction of CF with counter columns,"Having upgraded from 0.8.0 to 0.8.2 we ran nodetool compact and got

Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.AssertionError
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager.performMajor(CompactionManager.java:277)
        at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1762)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1358)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.AssertionError                                                                                                                                                                                                          
        at org.apache.cassandra.db.context.CounterContext.removeOldShards(CounterContext.java:593)                                                                                                                                           
        at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:237)                                                                                                                                                     
        at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:256)                                                                                                                                                     
        at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:88)                                                                                                                                                
        at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:140)                                                                                                                            
        at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:123)                                                                                                                                     
        at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:43)                                                                                                                                      
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)                                                                                                                                                 
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)                                                                                                                                            
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)                                                                                                                                                     
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)                                                                                                                                    
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)                                                                                                                                           
        at org.apache.cassandra.db.compaction.CompactionManager.doCompactionWithoutSizeEstimation(CompactionManager.java:569)                                                                                                                
        at org.apache.cassandra.db.compaction.CompactionManager.doCompaction(CompactionManager.java:506)                                                                                                                                     
        at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:319)                                                                                                                                           
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)                                                                                                                                                                
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)                                                                                                                                                                          
        ... 3 more",slebresne,tarasp,Normal,Resolved,Fixed,29/Jul/11 12:00,16/Apr/19 09:32
Bug,CASSANDRA-2972,12515826,nodetool netstats progress does not update on receiving side,"when you add/remove node to cluster, nodetool netstats show correct results only on sending side - on receiving side you can see only 0% progress",yukim,wmeler,Low,Resolved,Fixed,30/Jul/11 03:42,16/Apr/19 09:32
Bug,CASSANDRA-2983,12516133,CliClient print memtable threshold in incorrect order,"as a continuation from #2839 looks like it was incorrectly merged into 0.8 as well, hence affecting > 0.8.2.

for trunk, this is also changed (time is taken out). So I guess format wise, we would stick with the fixed format in 0.7.7 per #2839 , which is:

{code}
sessionState.out.printf(""      Memtable thresholds: %s/%s/%s (millions of ops/minutes/MB)%n"",
    cf_def.memtable_operations_in_millions, cf_def.memtable_flush_after_mins, cf_def.memtable_throughput_in_mb);
{code}",,cywjackson,Low,Resolved,Fixed,01/Aug/11 23:15,16/Apr/19 09:32
Bug,CASSANDRA-2990,12517821,We should refuse query for counters at CL.ANY,"We currently do not reject writes for counters at CL.ANY, even though this is not supported (and rightly so).",slebresne,slebresne,Low,Resolved,Fixed,03/Aug/11 18:10,16/Apr/19 09:32
Bug,CASSANDRA-2992,12517928,Cassandra doesn't start on Red Hat Linux due to hardcoded JAVA_HOME,"On CentOS /etc/init.d/cassandra has
bq. export JAVA_HOME=/usr/lib/jvm/java-1.6.0-openjdk-1.6.0.0/

While there is no such a directory on our server it was ok for 0.8.2, because /usr/sbin/cassandra checked the executable
{quote}
if [ -x $JAVA_HOME/bin/java ]; then
    JAVA=$JAVA_HOME/bin/java
else
    JAVA=`which java`
fi
{quote}

But 0.8.3 builds replaced the above code with one that doesn't check if JAVA_HOME is set correctly.
{quote}
if [ -n ""$JAVA_HOME"" ]; then
    JAVA=""$JAVA_HOME/bin/java""
else
    JAVA=java
fi
{quote}

That's why cassandra doesn't start anymore.


The correct fix would be to remove ""export JAVA_HOME"" from /etc/init.d/cassandra or set it only to correct path and only if it hasn't already been set.

It would also be nice to revert to ""[ -x $JAVA_HOME/bin/java ]"" in /usr/sbin/cassandra
",thepaul,tarasp,Low,Resolved,Fixed,04/Aug/11 14:26,16/Apr/19 09:32
Bug,CASSANDRA-2993,12517966,Issues with parameters being escaped correctly in Python CQL,"When using parameterised queries in Python CQL strings are not being escaped correctly.


Query and Parameters:
{code}
'UPDATE sites SET :col = :val WHERE KEY = :site_id'

{'col': 'feed_stats:1312493736688033024',
 'site_id': '899d15e8-bd4a-11e0-bc8c-001fe14cba06',
 'val': ""(dp0\nS'1'\np1\n(lp2\nI1\naI2\naI3\naI4\nasS'0'\np3\n(lp4\nI1\naI2\naI3\naI4\nasS'3'\np5\n(lp6\nI1\naI2\naI3\naI4\nasS'2'\np7\n(lp8\nI1\naI2\naI3\naI4\nas.""}
{code}

Query trying to be executed after processing parameters
{code}     
""UPDATE sites SET 'feed_stats:1312493736688033024' = '(dp0\nS''1''\np1\n(lp2\nI1\naI2\naI3\naI4\nasS''0''\np3\n(lp4\nI1\naI2\naI3\naI4\nasS''3''\np5\n(lp6\nI1\naI2\naI3\naI4\nasS''2''\np7\n(lp8\nI1\naI2\naI3\naI4\nas.' WHERE KEY = '899d15e8-bd4a-11e0-bc8c-001fe14cba06'""

{code}",thobbs,bcvisin,Normal,Resolved,Fixed,04/Aug/11 21:54,16/Apr/19 09:32
Bug,CASSANDRA-2994,12517968,OutOfBounds in CompressedSequentialWriter.flushData,"Near the beginning of a wide row test with CASSANDRA-47 compression enabled on a counter column family, I see the following exception:

{code:java} WARN [CompactionExecutor:5] 2011-08-04 21:50:14,558 FileUtils.java (line 95) Failed closing org.apache.cassandra.io.compress.CompressedSequentialWriter@28f01347
java.lang.IndexOutOfBoundsException
	at java.io.RandomAccessFile.writeBytes(Native Method)
	at java.io.RandomAccessFile.write(RandomAccessFile.java:466)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.flushData(CompressedSequentialWriter.java:88)
	at org.apache.cassandra.io.util.SequentialWriter.flushInternal(SequentialWriter.java:174)
	at org.apache.cassandra.io.util.SequentialWriter.syncInternal(SequentialWriter.java:150)
	at org.apache.cassandra.io.util.SequentialWriter.close(SequentialWriter.java:283)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.close(CompressedSequentialWriter.java:159)
	at org.apache.cassandra.io.util.FileUtils.closeQuietly(FileUtils.java:91)
	at org.apache.cassandra.io.sstable.SSTableWriter.cleanupIfNecessary(SSTableWriter.java:201)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:176)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:120)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:103)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [CompactionExecutor:5] 2011-08-04 21:50:14,561 AbstractCassandraDaemon.java (line 146) Fatal exception in thread Thread[CompactionExecutor:5,1,main]
java.lang.IndexOutOfBoundsException
	at java.io.RandomAccessFile.writeBytes(Native Method)
	at java.io.RandomAccessFile.write(RandomAccessFile.java:466)
	at org.apache.cassandra.io.compress.CompressedSequentialWriter.flushData(CompressedSequentialWriter.java:88)
	at org.apache.cassandra.io.util.SequentialWriter.flushInternal(SequentialWriter.java:174)
	at org.apache.cassandra.io.util.SequentialWriter.reBuffer(SequentialWriter.java:226)
	at org.apache.cassandra.io.util.SequentialWriter.writeAtMost(SequentialWriter.java:117)
	at org.apache.cassandra.io.util.SequentialWriter.write(SequentialWriter.java:101)
	at java.io.DataOutputStream.write(DataOutputStream.java:90)
	at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:105)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:150)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:153)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:120)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:103)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
{code}",slebresne,stuhood,Normal,Resolved,Fixed,04/Aug/11 22:07,16/Apr/19 09:32
Bug,CASSANDRA-2996,12518032,Fix NPE in getRangeToRpcaddressMap,"DatabaseDescriptor.getRpcAddress() can be null, which getRangeToRpcaddressMap doesn't take into account",slebresne,slebresne,Low,Resolved,Fixed,05/Aug/11 12:49,16/Apr/19 09:32
Bug,CASSANDRA-3000,12518136,Ec2MultiRegionSnitch throws AssertionError on EC2,"I found Ec2MultiRegionSnitch patch at https://issues.apache.org/jira/browse/CASSANDRA-2452 

However, I could not find any documentation on how to get it working, which address to use as seed, listen and thrift addresses. I used the following, 

seed_address     = Public DNS of the seed node 
listen_address   = Public DNS of the cluster node
rpc_address      = 0.0.0.0
endpoint_snitch: org.apache.cassandra.locator.Ec2MultiRegionSnitch


When I try to start cassandra, I get the following error: 

 INFO 14:44:19,822 Ec2Snitch adding ApplicationState ec2region=eu-west ec2zone=1c
 INFO 14:44:19,831 Starting Messaging Service on ec2-46-137-139-124.eu-west-1.compute.amazonaws.com/10.227.143.202:7000
 INFO 14:44:19,851 Using saved token 162732122844140653649170199706439942449
 INFO 14:44:19,852 Enqueuing flush of Memtable-LocationInfo@550579946(53/66 serialized/live bytes, 2 ops)
 INFO 14:44:19,852 Writing Memtable-LocationInfo@550579946(53/66 serialized/live bytes, 2 ops)
 INFO 14:44:19,908 Completed flushing /var/lib/cassandra/data/system/LocationInfo-h-20-Data.db (163 bytes)
 INFO 14:44:19,913 Compacting Major: [SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-20-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-19-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-17-Data.db'), SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-18-Data.db')]
ERROR 14:44:19,922 Exception encountered during startup.
java.lang.AssertionError
        at org.apache.cassandra.gms.Gossiper.compareEndpointStartup(Gossiper.java:620)
        at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:803)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:706)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:839)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:986)
        at org.apache.cassandra.service.StorageService.setToken(StorageService.java:219)
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:520)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:434)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:213)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:335)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:91)
Exception encountered during startup.
java.lang.AssertionError
        at org.apache.cassandra.gms.Gossiper.compareEndpointStartup(Gossiper.java:620)
        at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:803)
        at org.apache.cassandra.service.StorageService.onChange(StorageService.java:706)
        at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:839)
        at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:986)
        at org.apache.cassandra.service.StorageService.setToken(StorageService.java:219)
        at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:520)
        at org.apache.cassandra.service.StorageService.initServer(StorageService.java:434)
        at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:213)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:335)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:91)

",vijay2win@yahoo.com,initcron,Normal,Resolved,Fixed,07/Aug/11 14:47,16/Apr/19 09:32
Bug,CASSANDRA-3003,12518240,Trunk single-pass streaming doesn't handle large row correctly,"For normal column family, trunk streaming always buffer the whole row into memory. In uses
{noformat}
  ColumnFamily.serializer().deserializeColumns(in, cf, true, true);
{noformat}
on the input bytes.
We must avoid this for rows that don't fit in the inMemoryLimit.

Note that for regular column families, for a given row, there is actually no need to even recreate the bloom filter of column index, nor to deserialize the columns. It is enough to filter the key and row size to feed the index writer, but then simply dump the rest on disk directly. This would make streaming more efficient, avoid a lot of object creation and avoid the pitfall of big rows.

Counters column family are unfortunately trickier, because each column needs to be deserialized (to mark them as 'fromRemote'). However, we don't need to do the double pass of LazilyCompactedRow for that. We can simply use a SSTableIdentityIterator and deserialize/reserialize input as it comes.",yukim,slebresne,Urgent,Resolved,Fixed,08/Aug/11 19:52,16/Apr/19 09:32
Bug,CASSANDRA-3006,12518299,Enormous counter,"I have two-node cluster with the following keyspace and column family settings.

Cluster Information:
   Snitch: org.apache.cassandra.locator.SimpleSnitch
   Partitioner: org.apache.cassandra.dht.RandomPartitioner
   Schema versions: 
	63fda700-c243-11e0-0000-2d03dcafebdf: [172.17.19.151, 172.17.19.152]

Keyspace: test:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [datacenter1:2]
  Column Families:
    ColumnFamily: testCounter (Super)
    ""APP status information.""
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.CounterColumnType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType/org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 1.1578125/1440/247 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []

Then, I use a test program based on hector to add a counter column (testCounter[sc][column]) 1000 times. In the middle the adding process, I intentional shut down the node 172.17.19.152. In addition to that, the test program is smart enough to switch the consistency level from Quorum to One, so that the following adding actions would not fail. 

After all the adding actions are done, I start the cassandra on 172.17.19.152, and I use cassandra-cli to check if the counter is correct on both nodes, and I got a result 1001 which should be reasonable because hector will retry once. However, when I shut down 172.17.19.151 and after 172.17.19.152 is aware of 172.17.19.151 is down, I try to start the cassandra on 172.17.19.151 again. Then, I check the counter again, this time I got a result 481387 which is so wrong.

I use 0.8.3 to reproduce this bug, but I think this also happens on 0.8.2 or before also. ",slebresne,yulinyen,Normal,Resolved,Fixed,09/Aug/11 09:41,16/Apr/19 09:32
Bug,CASSANDRA-3007,12518303,NullPointerException in MessagingService.java:420,"I'm getting large quantity of exceptions during streaming. It is always in MessagingService.java:420. The streaming appears to be blocked.

 INFO 10:11:14,734 Streaming to /10.235.77.27
ERROR 10:11:14,734 Fatal exception in thread Thread[StreamStage:2,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.net.MessagingService.stream(MessagingService.java:420)
        at org.apache.cassandra.streaming.StreamOutSession.begin(StreamOutSession.java:176)
        at org.apache.cassandra.streaming.StreamOut.transferRangesForRequest(StreamOut.java:148)
        at org.apache.cassandra.streaming.StreamRequestVerbHandler.doVerb(StreamRequestVerbHandler.java:54)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:636)
",jbellis,vilda,Low,Resolved,Fixed,09/Aug/11 10:22,16/Apr/19 09:32
Bug,CASSANDRA-3011,12518462,Error upgrading when replication_factor is stored in strategy_options,"[from the ML]

{noformat}
When I upgraded from 0.8.2 to 0.8.3 I encountered a exception during startup:
...
Caused by: org.apache.cassandra.config.ConfigurationException:
replication_factor is an option for SimpleStrategy, not
NetworkTopologyStrategy
       at org.apache.cassandra.locator.NetworkTopologyStrategy.<init>(NetworkTopologyStrategy.java:70)
...
{noformat}",jbellis,jbellis,Low,Resolved,Fixed,10/Aug/11 14:54,16/Apr/19 09:32
Bug,CASSANDRA-3013,12518477,CQL counter support is undocumented,"When counter support was added to CQL, the documentation wasn't updated.",urandom,urandom,Low,Resolved,Fixed,10/Aug/11 16:42,16/Apr/19 09:32
Bug,CASSANDRA-3021,12518719,Null pointer dereference of m in org.apache.cassandra.db.commitlog.CommitLogSegment.dirtyString(),,fantayeneh,fantayeneh,Low,Resolved,Fixed,12/Aug/11 16:24,16/Apr/19 09:32
Bug,CASSANDRA-3022,12518732,Failures in cassandra long test: LongCompactionSpeedTest,"*The failing test case*
{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
{code}


*The following error is repeated in the console output*
{code}
    [junit] ERROR 04:02:20,654 Error in ThreadPoolExecutor
    [junit] java.util.MissingFormatArgumentException: Format specifier 's'
    [junit] 	at java.util.Formatter.format(Formatter.java:2432)
    [junit] 	at java.util.Formatter.format(Formatter.java:2367)
    [junit] 	at java.lang.String.format(String.java:2769)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:136)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:123)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionIterator.getReduced(CompactionIterator.java:43)
    [junit] 	at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
    [junit] 	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
    [junit] 	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
    [junit] 	at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
    [junit] 	at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager.doCompactionWithoutSizeEstimation(CompactionManager.java:559)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager.doCompaction(CompactionManager.java:506)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:141)
    [junit] 	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:107)
    [junit] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
    [junit] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit] 	at java.lang.Thread.run(Thread.java:662)
{code}

*Cassandra Revision List at time of failure
{code}
Summary
* log ks and cf of large rows being compacted patch by Ryan King; reviewed by jbellis for CASSANDRA-3019
* revert r1156772
* cache invalidate removes saved cache files patch by Ed Capriolo; reviewed by jbellis for CASSANDRA-2325
* make sure truncate clears out the commitlog patch by jbellis; reviewed by slebresne for CASSANDRA-2950
* include column name in validation failure exceptions patch by jbellis; reviewed by David Allsopp for CASSANDRA-2849
* fix NPE when encryption_options is unspecified patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3007
* update CHANGES
* update CHANGES

Revision 1156830 by jbellis: 
log ks and cf of large rows being compacted
patch by Ryan King; reviewed by jbellis for CASSANDRA-3019
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/compaction/CompactionController.java

Revision 1156791 by jbellis: 
revert r1156772
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java

Revision 1156772 by jbellis: 
cache invalidate removes saved cache files
patch by Ed Capriolo; reviewed by jbellis for CASSANDRA-2325
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java

Revision 1156763 by jbellis: 
make sure truncate clears out the commitlog
patch by jbellis; reviewed by slebresne for CASSANDRA-2950
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/SystemTable.java
	/cassandra/branches/cassandra-0.8/test/unit/org/apache/cassandra/db/RecoveryManagerTruncateTest.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/commitlog/CommitLog.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/db/ColumnFamilyStore.java

Revision 1156753 by jbellis: 
include column name in validation failure exceptions
patch by jbellis; reviewed by David Allsopp for CASSANDRA-2849
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java

Revision 1156749 by jbellis: 
fix NPE when encryption_options is unspecified
patch by jbellis; reviewed by brandonwilliams for CASSANDRA-3007
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/net/MessagingService.java

Revision 1156695 by jbellis: 
update CHANGES
	/cassandra/branches/cassandra-0.8/CHANGES.txt

Revision 1156694 by jbellis: 
update CHANGES
	/cassandra/branches/cassandra-0.8/CHANGES.txt
{code}

",jbellis,cdaw,Normal,Resolved,Fixed,12/Aug/11 19:44,16/Apr/19 09:32
Bug,CASSANDRA-3023,12518752,NPE in describe_ring,"Not sure how much of the following is relevant besides the stack trace, but here I go:

I have a 2 DC, 2 node per DC cluster. DC1 had it's seed replaced but I hadn't restarted. I upgraded to 0.8.4 in the following fashion:

-edited seeds
-stopped both DC1 nodes
-upgraded jars
-started both nodes at the same time

The non-seed node came up first and showed the following error. Then when the seed node came up, the error went away on the non-seed node but started occurring on the seed node:

ERROR [pool-2-thread-15] 2011-08-12 22:32:27,438 Cassandra.java (line 3668) Internal error processing describe_ring
java.lang.NullPointerException
	at org.apache.cassandra.service.StorageService.getRangeToRpcaddressMap(StorageService.java:623)
	at org.apache.cassandra.thrift.CassandraServer.describe_ring(CassandraServer.java:731)
	at org.apache.cassandra.thrift.Cassandra$Processor$describe_ring.process(Cassandra.java:3664)
	at org.apache.cassandra.thrift.Brisk$Processor.process(Brisk.java:464)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
",brandon.williams,efalcao,Normal,Resolved,Fixed,12/Aug/11 22:47,16/Apr/19 09:32
Bug,CASSANDRA-3034,12518824,"[patch] BufferedInputStream.skip only skips bytes that are in the buffer, so keep skipping until done","code calls skip(remaining) without checking result. Skip isn't guaranteed to skip what you requested, especially BufferedInputStream, so keep skipping until the remaining bytes is 0.",dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,14/Aug/11 15:43,16/Apr/19 09:32
Bug,CASSANDRA-3036,12518838,Vague primary key references in CQL,"create columnfamily wonk (id 'utf8' primary key, id int)
update wonk set id=1 where id='test'
create index wonk_id on wonk (id)

This does what you would expect but then the results are unclear when using 'id' in a where clause.

""select * from wonk where id=1"" returns nothing and ""select * from wonk where id='test'"" works fine.

Perhaps secondary indexes should not be allowed on columns that have the same name as the key_alias? At least a warning or something should be thrown to indicate you've just made a useless index.
",xedin,kreynolds,Low,Resolved,Fixed,14/Aug/11 19:24,16/Apr/19 09:32
Bug,CASSANDRA-3038,12518904,nodetool snapshot does not handle keyspace arguments correctly,"Given the following invocation:
./bin/nodetool snapshot Keyspace1 -t keyspace1_snapshot -h localhost

Snapshots are made for all keyspaces

Given a multi-keyspace invocation:
./bin/nodetool snapshot Keyspace1 Keyspace2 Keyspac3 -t keyspace1_snapshot -h localhost

Snapshots will be made for Keyspace2 and Keyspace3 but not Keyspace1. 

It appears there is just some antiquated command argument noodling in NodeCmd#handleSnapshots",zznate,zznate,Low,Resolved,Fixed,15/Aug/11 17:40,16/Apr/19 09:32
Bug,CASSANDRA-3039,12518911,AssertionError on nodetool cleanup,"While doing a cleanup I got the following AssertionError, I have tried a scrub and a major compaction before the cleanup which has not helped.

ST:

 INFO 18:49:58,540 Scrubbing SSTableReader(path='/vol/cassandra/data/system/LocationInfo-g-93-Data.db')
 INFO 18:49:58,834 Scrub of SSTableReader(path='/vol/cassandra/data/system/LocationInfo-g-93-Data.db') complete: 4 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:58,913 Scrubbing SSTableReader(path='/vol/cassandra/data/system/Migrations-g-56-Data.db')
 INFO 18:49:59,218 Scrub of SSTableReader(path='/vol/cassandra/data/system/Migrations-g-56-Data.db') complete: 1 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:59,256 Scrubbing SSTableReader(path='/vol/cassandra/data/system/Schema-g-58-Data.db')
 INFO 18:49:59,323 Scrub of SSTableReader(path='/vol/cassandra/data/system/Schema-g-58-Data.db') complete: 34 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:49:59,416 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5074-Data.db')
 INFO 18:50:50,137 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5074-Data.db') complete: 91735 rows in new sstable and 32 empty (tombstoned) rows dropped
 INFO 18:50:50,137 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5075-Data.db')
 INFO 18:50:53,075 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5075-Data.db') complete: 27940 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:50:53,089 Scrubbing SSTableReader(path='/vol/cassandra/data/SpiderServices/Content-g-238-Data.db')

 INFO 18:51:10,302 Scrub of SSTableReader(path='/vol/cassandra/data/SpiderServices/Content-g-238-Data.db') complete: 70815 rows in new sstable and 0 empty (tombstoned) rows dropped
 INFO 18:53:05,420 Cleaning up SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5078-Data.db')
 INFO 18:53:13,266 Cleaned up to /vol/cassandra/data/SpiderServices/Content2-tmp-g-5079-Data.db.  198,705,176 to 198,705,176 (~100% of original) bytes for 27,940 keys.  Time: 7,846ms.
 INFO 18:53:13,267 Cleaning up SSTableReader(path='/vol/cassandra/data/SpiderServices/Content2-g-5077-Data.db')
ERROR 18:53:33,913 Fatal exception in thread Thread[CompactionExecutor:21,1,RMI Runtime]
java.lang.AssertionError
	at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:107)
	at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:132)
	at org.apache.cassandra.db.compaction.CompactionManager.doCleanupCompaction(CompactionManager.java:866)
	at org.apache.cassandra.db.compaction.CompactionManager.access$500(CompactionManager.java:65)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:204)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)",jbellis,rays,Normal,Resolved,Fixed,15/Aug/11 19:18,16/Apr/19 09:32
Bug,CASSANDRA-3043,12519054,SSTableImportTest fails on Windows because of malformed file path,SSTableImportTest uses URL.getPath() to create path to JSON files. This fails on Windows in many cases (for example if there are spaces in path which get encoded as %20 which Windows doesn't like). Trick is to create URI from URL which satisfies all platforms.,vloncar,vloncar,Low,Resolved,Fixed,16/Aug/11 22:34,16/Apr/19 09:32
Bug,CASSANDRA-3044,12519055,Hector NodeAutoDiscoverService fails to resolve hosts due to / being part of the IP address,"Didn't get this problem with Cassandra 0.8.2- started happening under 0.8.4.  Temporary work around was to disable node auto discovery.  Seems to be related to:

http://svn.apache.org/viewvc?view=rev&revision=1155157
http://issues.apache.org/jira/browse/CASSANDRA-1777

LOG:

240514 [pool-2-thread-1] INFO me.prettyprint.cassandra.connection.NodeAutoDiscoverService - using existing hosts [10.255.255.176(10.255.255.176):9160, 10.255.255.175(10.255.255.175):9160]
240553 [pool-2-thread-1] ERROR me.prettyprint.cassandra.service.CassandraHost - Unable to resolve host /10.255.255.176
240553 [pool-2-thread-1] INFO me.prettyprint.cassandra.connection.NodeAutoDiscoverService - Found a node we don't know about /10.255.255.176(/10.255.255.176):9160 for TokenRange TokenRange(start_token:33370589793653380361461751202224080323, end_token:93518639523624865529944734322199113946, endpoints:[/10.255.255.176])
240553 [pool-2-thread-1] INFO me.prettyprint.cassandra.connection.NodeAutoDiscoverService - Found 1 new host(s) in Ring
240553 [pool-2-thread-1] INFO me.prettyprint.cassandra.connection.NodeAutoDiscoverService - Addding found host /10.255.255.176(/10.255.255.176):9160 to pool
240554 [pool-2-thread-1] ERROR me.prettyprint.cassandra.connection.HConnectionManager - General exception host to HConnectionManager: /10.255.255.176(/10.255.255.176):9160
java.lang.IllegalArgumentException: protocol = socket host = null
        at sun.net.spi.DefaultProxySelector.select(DefaultProxySelector.java:151)
        at java.net.SocksSocketImpl.connect(SocksSocketImpl.java:358)
        at java.net.Socket.connect(Socket.java:529)
        at org.apache.thrift.transport.TSocket.open(TSocket.java:178)
        at org.apache.thrift.transport.TFramedTransport.open(TFramedTransport.java:81)
        at me.prettyprint.cassandra.connection.HThriftClient.open(HThriftClient.java:123)
        at me.prettyprint.cassandra.connection.ConcurrentHClientPool.<init>(ConcurrentHClientPool.java:43)
        at me.prettyprint.cassandra.connection.RoundRobinBalancingPolicy.createConnection(RoundRobinBalancingPolicy.java:68)
        at me.prettyprint.cassandra.connection.HConnectionManager.addCassandraHost(HConnectionManager.java:103)
        at me.prettyprint.cassandra.connection.NodeAutoDiscoverService.doAddNodes(NodeAutoDiscoverService.java:68)
        at me.prettyprint.cassandra.connection.NodeAutoDiscoverService$QueryRing.run(NodeAutoDiscoverService.java:53)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)",brandon.williams,synfinatic,Normal,Resolved,Fixed,16/Aug/11 23:03,16/Apr/19 09:32
Bug,CASSANDRA-3051,12519195,On Disk Compression breaks SSL Encryption,"Encryption depends on FileStreamTask.write [1] protected member to be called because the SSLFileStreamTask.write overrides this to write back to the server.

When enabled, compression circumvents the call and the client does not communicate using an SSL socket back to the server.

[1]
protected long write(FileChannel fc, Pair<Long, Long> section, long length, long bytesTransferred) throws IOException

",xedin,bcoverston,Normal,Resolved,Fixed,17/Aug/11 22:06,16/Apr/19 09:32
Bug,CASSANDRA-3052,12519225,CQL: ResultSet.next() gives NPE when run after an INSERT or CREATE statement,"This test script used to work until I upgraded the jdbc driver to 1.0.4.

*CQL 1.0.4*: apache-cassandra-cql-1.0.4-SNAPSHOT.jar build at revision 1158979

*Repro Script*: 
* drop in test directory, change package declaration and run:  ant test -Dtest.name=resultSetNPE
* The script gives you a NullPointerException when you uncomment out the following lines after a CREATE or INSERT statement.
{code}
colCount = res.getMetaData().getColumnCount();

res.next();
{code}
* Please note that there is no need to comment out those lines if a SELECT statement was run prior.


{code}
package com.datastax.bugs;

import java.sql.DriverManager;
import java.sql.Connection;
import java.sql.ResultSet;
import java.sql.SQLException;
import java.sql.Statement;

import org.junit.Test;

public class resultSetNPE {
    
    @Test
    public void createKS() throws Exception {   
        Connection initConn = null;
        Connection connection = null;

        ResultSet res;
        Statement stmt;
        int colCount = 0;
        
        Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
        
        // Check create keyspace
        initConn = DriverManager.getConnection(""jdbc:cassandra://127.0.0.1:9160/default"");     
        stmt = initConn.createStatement();

        try {
          System.out.println(""Running DROP KS Statement"");  
          res = stmt.executeQuery(""DROP KEYSPACE ks1"");  
          // res.next();
          
        } catch (SQLException e) {
            if (e.getMessage().startsWith(""Keyspace does not exist"")) 
            {
                // Do nothing - this just means you tried to drop something that was not there.
                // res = stmt.executeQuery(""CREATE KEYSPACE ks1 with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=1"");  
            } 
        }   
          
        System.out.println(""Running CREATE KS Statement"");
        res = stmt.executeQuery(""CREATE KEYSPACE ks1 with strategy_class =  'org.apache.cassandra.locator.SimpleStrategy' and strategy_options:replication_factor=1"");  
        // res.next();

        initConn.close();    
    }  
 
    @Test
    public void createCF() throws Exception 
    {   

        Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
        int colCount = 0;

        Connection connection = DriverManager.getConnection(""jdbc:cassandra://127.0.0.1:9160/ks1"");     
        Statement stmt = connection.createStatement();

        System.out.print(""Running CREATE CF Statement"");
        ResultSet res = stmt.executeQuery(""CREATE COLUMNFAMILY users (KEY varchar PRIMARY KEY, password varchar, gender varchar, session_token varchar, state varchar, birth_year bigint)"");    
        
        //colCount = res.getMetaData().getColumnCount();
        System.out.println("" -- Column Count: "" + colCount); 
        //res.next();
        
        connection.close();               
    }  
    
    @Test
    public void simpleSelect() throws Exception 
    {   
        Class.forName(""org.apache.cassandra.cql.jdbc.CassandraDriver"");
        int colCount = 0;

        Connection connection = DriverManager.getConnection(""jdbc:cassandra://127.0.0.1:9160/ks1"");     
        Statement stmt = connection.createStatement();
        
        System.out.print(""Running INSERT Statement"");
        ResultSet res = stmt.executeQuery(""INSERT INTO users (KEY, password) VALUES ('user1', 'ch@nge')"");  
        //colCount = res.getMetaData().getColumnCount();
        System.out.println("" -- Column Count: "" + colCount); 
        //res.next();
        
        System.out.print(""Running SELECT Statement"");
        res = stmt.executeQuery(""SELECT KEY, gender, state FROM users"");  
        colCount = res.getMetaData().getColumnCount();
        System.out.println("" -- Column Count: "" + colCount); 
        res.getRow();
        res.next();
            
        connection.close(); 
    }  
}
{code}
",ardot,cdaw,Normal,Resolved,Fixed,18/Aug/11 01:32,16/Apr/19 09:32
Bug,CASSANDRA-3053,12519235,[patch] EstimatedHIstogram doesn't overide equals correctly,"EstimatedHistogram declares equals with an EstimatedHistogram parameter, instead of Object, thus only working correctly for statically typed EstimatedHistogram references. Fixed to take Object parm.",dbrosius,dbrosius@apache.org,Low,Resolved,Fixed,18/Aug/11 04:28,16/Apr/19 09:32
Bug,CASSANDRA-3054,12519305,"CLI ""drop index"" doesn't handle numeric-only hex column identifiers properly","CLI drop index doesn't accept requests to drop columns whose hex names include only numeric characters.  The 617070 column name below is col2.

[default@Host] use MyKeyspace;
Authenticated to keyspace: MyKeyspace
[default@Host] drop index on MyCF.617070; 
Syntax error at position 22: mismatched input '617070' expecting Identifier

While drop index seems to parse correctly with alpha chars included:

[default@Host] drop index on MyCF.617070x;
Column '617070x' definition was not found in ColumnFamily 'MyCF'.
[default@Host] drop index on MyCF.col2;
Column 'col2' definition was not found in ColumnFamily 'MyCF'.

cassandra-user thread: http://mail-archives.apache.org/mod_mbox/cassandra-user/201108.mbox/%3CB2D1533B-C69E-467A-9653-1D086E33227C@thelastpickle.com%3E",xedin,dkuebric,Low,Resolved,Fixed,18/Aug/11 15:39,16/Apr/19 09:32
Bug,CASSANDRA-3057,12519349,secondary index on a column that has a value of size > 64k will fail on flush,"exception seen on flush when an indexed column contain size > 64k:

granted that having a value > 64k possibly mean something that shouldn't be indexed as it most likely would have a high cardinality, but i think there would still be some valid use case for it.

test case:
simply run the stress test with 
-n 1 -u 0 -c 2  -y Standard  -o INSERT  -S 65536 -x KEYS

then call a flush

exception:
 INFO [FlushWriter:8] 2011-08-18 21:49:33,214 Memtable.java (line 218) Writing Memtable-Standard1.Idx1@1652462853(16/20 serialized/live bytes, 1 ops)
Standard1@980087547(196659/245823 serialized/live bytes, 3 ops)
ERROR [FlushWriter:8] 2011-08-18 21:49:33,230 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[FlushWriter:8,5,RMI Runtime]
java.lang.AssertionError: 65536
        at org.apache.cassandra.utils.ByteBufferUtil.writeWithShortLength(ByteBufferUtil.java:330)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:164)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:245)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:49)
        at org.apache.cassandra.db.Memtable$3.runMayThrow(Memtable.java:270)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

",xedin,cywjackson,Low,Resolved,Fixed,18/Aug/11 22:02,16/Apr/19 09:32
Bug,CASSANDRA-3059,12519354,sstable2json on an index sstable failed with NPE,"$ ./bin/sstable2json /var/lib/cassandra-trunk/data/Keyspace1/Standard1.Idx1-h-1-Data.db 
{
Exception in thread ""main"" java.lang.NullPointerException
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:74)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:69)
        at org.apache.cassandra.db.ColumnFamily.create(ColumnFamily.java:64)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:147)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:87)
        at org.apache.cassandra.io.sstable.SSTableIdentityIterator.<init>(SSTableIdentityIterator.java:71)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:177)
        at org.apache.cassandra.io.sstable.SSTableScanner$KeyScanningIterator.next(SSTableScanner.java:142)
        at org.apache.cassandra.io.sstable.SSTableScanner.next(SSTableScanner.java:134)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:304)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:335)
        at org.apache.cassandra.tools.SSTableExport.export(SSTableExport.java:348)
        at org.apache.cassandra.tools.SSTableExport.main(SSTableExport.java:406)


cfm is null for Index CF?",jbellis,cywjackson,Low,Resolved,Fixed,19/Aug/11 00:15,16/Apr/19 09:32
Bug,CASSANDRA-3066,12519538,Creating a keyspace SYSTEM cause issue,"It's possible to create a keyspace SYSTEM but impossible to do anything with it after.

I know naming a keyspace SYSTEM is probably not a good idea but I was testing something on a test cluster and found this bug. Step to reproduce:

connect localhost/9160;
create keyspace SYSTEM;
use SYSTEM;
create column family test
with comparator = UTF8Type and subcomparator = UTF8Type
and default_validation_class = UTF8Type
and column_metadata = [{column_name: title, validation_class: UTF8Type},
    {column_name: publisher, validation_class: UTF8Type}];

And you get:

system keyspace is not user-modifiable

Although SYSTEM keyspace have been created and is a different keyspace as system.",jbellis,wajam,Low,Resolved,Fixed,21/Aug/11 16:33,16/Apr/19 09:32
Bug,CASSANDRA-3071,12519848,Gossip state is not removed after a new IP takes over a token,"When a new node takes over a token, the endpoint state in the gossiper is never removed for the old node.  ",brandon.williams,brandon.williams,Low,Resolved,Fixed,23/Aug/11 16:59,16/Apr/19 09:32
Bug,CASSANDRA-3074,12520009,comments and documentation for index_interval are misleading,"The comments and documentation for index_interval are misleading.  They state the larger the *sampling* the more effective the index as at the cost of space.  This is true, but in the context of the configuration variable it implies the larger the *setting* is the larger the index is while in fact it's the opposite of that.",mdennis,mdennis,Low,Resolved,Fixed,24/Aug/11 19:24,16/Apr/19 09:32
Bug,CASSANDRA-3075,12520071,"Cassandra CLI unable to use list command with INTEGER column names, resulting in syntax error","I have a Column Family named 1105115.

I have inserted the CF with Hector, and it did not
throw any exception concerning the name of the
column.

If I am issuing the command

list 1105115;

I incur the following error:

[default@unknown] list 1105115;
Syntax error at position 5: mismatched input '1105115' expecting Identifier

I presume we are not to name CFs as integers?

 Or is there something I am missing from
the bellow help content:

[default@unknown] help list;
list <cf>;
list <cf>[<startKey>:];
list <cf>[<startKey>:<endKey>];
list <cf>[<startKey>:<endKey>] limit <limit>;

List a range of rows, and all of their columns, in the specified column
family.

The order of rows returned is dependant on the Partitioner in use.

Required Parameters:
- cf: Name of the column family to list rows from.

Optional Parameters:
- endKey: Key to end the range at. The end key will be included
in the result. Defaults to an empty byte array.

- limit: Number of rows to return. Default is 100.

- startKey: Key start the range from. The start key will be
included in the result. Defaults to an empty byte array.

Examples:
list Standard1;
list Super1[j:];
list Standard1[j:k] limit 40;

================================================

Column Family Info:

    ColumnFamily: 1105115
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.AsciiType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.5203125/111/1440 (millions of ops/MB/minutes)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
",xedin,renatodasil,Low,Resolved,Fixed,25/Aug/11 08:05,16/Apr/19 09:32
Bug,CASSANDRA-3076,12520117,AssertionError in new GCInspector log,Small regression from CASSANDRA-2868,tjake,tjake,Low,Resolved,Fixed,25/Aug/11 14:26,16/Apr/19 09:32
Bug,CASSANDRA-3082,12520268,Operation with CL=EACH_QUORUM doesn't succeed when a replica is down (RF=3),"{code}  DatacenterSyncWriteResponseHandler#assureSufficientLiveNodes()
     ...
     ...
        // Throw exception if any of the DC doesn't have livenodes to accept write.
        for (String dc: strategy.getDatacenters())
        {
        	if (dcEndpoints.get(dc).get() != responses.get(dc).get())
                throw new UnavailableException();
        }
{code}

should be:
 
{code}
      if (dcEndpoints.get(dc).get() < responses.get(dc).get())
{code}",patricioe,patricioe,Low,Resolved,Fixed,26/Aug/11 16:17,16/Apr/19 09:32
Bug,CASSANDRA-3084,12520286,o.a.c.dht.Range.differenceToFetch() doesn't handle all cases correctly,"It's possible that differenceToFetch is making implicit assumptions about the relationship between the two ranges, but the following cases are not handled correctly (the old range is (A, B], the new is (C, D]:

{noformat}
--C--A-----B--D--
{noformat}

Here, the result will be (C, A] and (D, B], instead of (C, A] and (B, D].

{noformat}
--C--A-----D--B--
{noformat}

The result will be (C, D] instead of just (C, A].

{noformat}
--A--C-----D--B--
{noformat}

The result will be (B, D] when nothing needs to be transfered.

If there is some kind of implicit assumption that these cases won't arise, it either needs to be explicit (assertions, exceptions) or the cases need to be handled.  It should be easy to cover this with unit tests.",thobbs,thobbs,Normal,Resolved,Fixed,26/Aug/11 19:06,16/Apr/19 09:32
Bug,CASSANDRA-3085,12520293,Race condition in sstable reference counting,"DataTracker gives us an atomic View of memtable/sstables, but acquiring references is not atomic.  So it is possible to acquire references to an SSTableReader object that is no longer valid, as in this example:

View V contains sstables {A, B}.  We attempt a read in thread T using this View.
Meanwhile, A and B are compacted to {C}, yielding View W.  No references exist to A or B so they are cleaned up.
Back in thread T we acquire references to A and B.  This does not cause an error, but it will when we attempt to read from them next.",jbellis,jbellis,Urgent,Resolved,Fixed,26/Aug/11 19:22,16/Apr/19 09:32
Bug,CASSANDRA-3087,12520318,Leveled compaction allows multiple simultaneous compaction Tasks,"CASSANDRA-1608 attempts to restrict itself to one compaction task per CF (see discussion there for why this is necessary) by synchronizing LCS.getBackgroundTasks but this is not sufficient.  Consider this sequence of events:

1. getBackgroundTasks returns a Task for compacting some L0 sstables.  this Task is scheduled.
2. Another SSTable for this CF is flushed, so CompactionManager.submitBackground is called.  getBT is not currently in-progress so the synchronization does not stop another Task from being returned and scheduled.",jbellis,jbellis,Normal,Resolved,Fixed,26/Aug/11 22:54,16/Apr/19 09:32
Bug,CASSANDRA-3096,12520496,Test RoundRobinScheduler timeouts,"CASSANDRA-3079 was very hasty, and introduced two bugs that would: 1) cause the scheduler to busywait after a timeout, 2) never actually throw timeouts. This calls for a test.",stuhood,stuhood,Normal,Resolved,Fixed,28/Aug/11 22:56,16/Apr/19 09:32
Bug,CASSANDRA-3099,12520576,Counters are not always hinted,"CASSANDRA-2892 mistakenly removed some hints for counters, namely the hints that were supposed to be stored on the local node (that is, instead of removing from the hintedEndpoints multimap only the local write (since it has been already applied), we were removing everything having the local node as destination).",slebresne,slebresne,Low,Resolved,Fixed,29/Aug/11 14:29,16/Apr/19 09:32
Bug,CASSANDRA-3101,12520615,Should check for errors when calling /bin/ln,"It looks like cassandra.utils.CLibrary.createHardLinkWithExec() does not check for any errors in the execution of the hard-link-making utility. This could be bad if, for example, the user has put the snapshot directory on a different filesystem from the data directory. The hard linking would fail and the sstable snapshots would not exist, but no error would be reported.

It does look like errors with the more direct JNA link() call are handled correctly- an exception is thrown. The WithExec version should probably do the same thing.

Definitely it would be enough to check the process exit value from /bin/ln for nonzero in the *nix case, but I don't know whether 'fsutil hardlink create' or 'cmd /c mklink /H' return nonzero on failure.

For bonus points, use any output from the Process's error stream in the text of the exception, to aid in debugging problems.",vijay2win@yahoo.com,thepaul,Low,Resolved,Fixed,29/Aug/11 18:39,16/Apr/19 09:32
Bug,CASSANDRA-3102,12520679,catch invalid key_validation_class before instantiating UpdateColumnFamily,,jbellis,jbellis,Low,Resolved,Fixed,30/Aug/11 03:05,16/Apr/19 09:32
Bug,CASSANDRA-3106,12520793,getRangeToEndpointMap() method removed,"When getRangeToRPCAddress was added, getRangeToEndpointMap was removed, however, both are useful. We should add it back.",nickmbailey,nickmbailey,Low,Resolved,Fixed,30/Aug/11 19:31,16/Apr/19 09:32
Bug,CASSANDRA-3108,12520830,Make Range and Bounds objects client-safe,"From Mck's comment on CASSANDRA-1125:

Something broke here in production once we went out with 0.8.2. It may have been some poor testing, i'm not entirely sure and a little surprised.

CFIF:135 breaks because inside dhtRange.intersects(jobRange) there's a call to new Range(token, token) which calls StorageService.getPartitioner() and StorageService is null as we're not inside the server.

A quick fix is to change Range:148 from new Range(token, token) to new Range(token, token, partitioner) making the presumption that the partitioner for the new Range will be the same as this Range.
",mck,jbellis,Normal,Resolved,Fixed,30/Aug/11 20:43,16/Apr/19 09:32
Bug,CASSANDRA-3110,12520904,SSTables iterators are closed and before being used,"Seems there is misplaced finally blocks in CollationController: we close the sstable iterators and release the sstable references *before* having actually used said iterators/sstables.
Note: this cause a lot of tests to fail in `ant test-compression` in particular.",slebresne,slebresne,Normal,Resolved,Fixed,31/Aug/11 10:22,16/Apr/19 09:32
Bug,CASSANDRA-3111,12520907,Bug in ReversedType comparator,"Scenario :
 * create a cf with a reversed comparator
 * insert a few columns in a row
 * try to read data with : SliceRange(start='', finish='', reversed=true)
 ** no data is returned, but some columns are expected
 * try to read data with : SliceRange(start='', finish='', reversed=false)
 ** if not flushed on disk : no data is returned


",,frousseau,Low,Resolved,Fixed,31/Aug/11 10:35,16/Apr/19 09:32
Bug,CASSANDRA-3114,12520973,After Choosing EC2Snitch you can't migrate off w/o a full cluster restart,"Once you choose the Ec2Snitch the gossip messages will trigger this exception if you try to move (for example) to the property file snitch:

ERROR [pool-2-thread-11] 2011-08-30 16:38:06,935 Cassandra.java (line 3041) Internal error processing get_slice 
java.lang.NullPointerException 
at org.apache.cassandra.locator.Ec2Snitch.getDatacenter(Ec2Snitch.java:84) 
at org.apache.cassandra.locator.DynamicEndpointSnitch.getDatacenter(DynamicEndpointSnitch.java:122) 
at org.apache.cassandra.service.DatacenterReadCallback.assureSufficientLiveNodes(DatacenterReadCallback.java:77) 
at org.apache.cassandra.service.StorageProxy.fetchRows(StorageProxy.java:516) 
at org.apache.cassandra.service.StorageProxy.read(StorageProxy.java:480) 
at org.apache.cassandra.thrift.CassandraServer.readColumnFamily(CassandraServer.java:109) 
at org.apache.cassandra.thrift.CassandraServer.getSlice(CassandraServer.java:263) 
at org.apache.cassandra.thrift.CassandraServer.multigetSliceInternal(CassandraServer.java:345) 
at org.apache.cassandra.thrift.CassandraServer.get_slice(CassandraServer.java:306) 
at org.apache.cassandra.thrift.Cassandra$Processor$get_slice.process(Cassandra.java:3033) 
at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889) 
at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187) 
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
at java.lang.Thread.run(Thread.java:662)",,bcoverston,Normal,Resolved,Fixed,31/Aug/11 20:40,16/Apr/19 09:32
Bug,CASSANDRA-3115,12520975,LongCompactionSpeedTest running longer starting with builds on Aug31,"The Long tests started consistently timing out as this build of cassandra: [https://jenkins.qa.datastax.com/job/CassandraLong/131/console]

The regression server shows pretty consistent run times for this test, and then consistent timeouts from this point forward.
{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 6, Failures: 0, Errors: 0, Time elapsed: 111.379 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 1637 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: 6144 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 2379 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=500000: 15690 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=500000 colsper=1: 20953 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=1000 colsper=5: 5672 ms
{code}

After increasing the timeout, the run time shows are now:
{code}
    [junit] Testsuite: org.apache.cassandra.db.compaction.LongCompactionSpeedTest
    [junit] Tests run: 6, Failures: 0, Errors: 0, Time elapsed: 409.486 sec
    [junit] 
    [junit] ------------- Standard Output ---------------
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 2465 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: 29407 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 2456 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=500000: 14588 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=500000 colsper=1: 100794 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=1000 colsper=5: 19266 ms
{code}


*Single node local run:  Build 1056 / on Aug 30 / Macbook Pro w/ 8 GB ram (all apps shutdown)*
{panel}
+Run 1: Fresh install with no log or lib dir+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 850 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *3004 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 767 ms
    
+Run 2: Invoke test without restarting the server+
	[junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 826 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *3030 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 776 ms

+Run 3: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 830 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *2964 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 635 ms

+Run 4: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 931 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *2987 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 910 ms
{panel}

*Singled node local run: Build 1062 / on Aug 31 / Macbook pro w/ 8GB ram (all apps shutdown)*
{panel}
+Run 1: Fresh restart with no log or lib dir+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 802 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *17649 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 713 ms

+Run 2: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 832 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *16875 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 868 ms

+Run 3: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 809 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *16818 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 807 ms

+Run 4: Invoke test without restarting the server+
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=1 colsper=200000: 834 ms
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=2 rowsper=200000 colsper=1: *16997 ms*
    [junit] org.apache.cassandra.db.compaction.LongCompactionSpeedTest: sstables=100 rowsper=800 colsper=5: 873 ms
{panel}


*Cassandra Build at time of test failure*
[https://jenkins.qa.datastax.com/job/Cassandra/168/changes]
{panel}
Revision: 1163394

* make Range and Bounds objects client-safe patch by Mck SembWever and jbellis for CASSANDRA-3108
* Catch invalid key_validation_class before instantiating UpdateColumnFamily patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3102
* Add validation that Keyspace names are case-insensitively unique patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3066
* merge from 0.7
* merge from 0.7
* merge from 0.7
* update CHANGES for #3023 and #3044

Revision 1163394 by jbellis: 
make Range and Bounds objects client-safe
patch by Mck SembWever and jbellis for CASSANDRA-3108
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/dht/Range.java
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/dht/Bounds.java

Revision 1163291 by xedin: 
Catch invalid key_validation_class before instantiating UpdateColumnFamily
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3102
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java

Revision 1163289 by xedin: 
Add validation that Keyspace names are case-insensitively unique
patch by Jonathan Ellis; reviewed by Pavel Yaskevich for CASSANDRA-3066
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/ClientState.java
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/ThriftValidation.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/cql/QueryProcessor.java
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/thrift/CassandraServer.java

Revision 1163281 by jbellis: 
merge from 0.7
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
	/cassandra/branches/cassandra-0.8/conf/cassandra.yaml
	/cassandra/branches/cassandra-0.8/contrib
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
	/cassandra/branches/cassandra-0.8
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java

Revision 1163268 by jbellis: 
merge from 0.7
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
	/cassandra/branches/cassandra-0.8/conf/cassandra.yaml
	/cassandra/branches/cassandra-0.8/contrib
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
	/cassandra/branches/cassandra-0.8
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java

Revision 1163235 by jake: 
merge from 0.7
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Cassandra.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/InvalidRequestException.java
	/cassandra/branches/cassandra-0.8/contrib
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/SuperColumn.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/NotFoundException.java
	/cassandra/branches/cassandra-0.8
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/service/GCInspector.java
	/cassandra/branches/cassandra-0.8/interface/thrift/gen-java/org/apache/cassandra/thrift/Column.java

Revision 1163205 by jbellis: 
update CHANGES for #3023 and #3044
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/utils/BloomFilter.java
	/cassandra/branches/cassandra-0.8/CHANGES.txt
	/cassandra/branches/cassandra-0.8/src/java/org/apache/cassandra/utils/BloomCalculations.java
{panel}

",brandon.williams,cdaw,Low,Resolved,Fixed,31/Aug/11 20:57,16/Apr/19 09:32
Bug,CASSANDRA-3116,12521007,Compactions can (seriously) delay schema migrations,A compaction lock is acquired when dropping keyspaces or column families which will cause the schema migration to block if a compaction is in progress.,jbellis,urandom,Normal,Resolved,Fixed,01/Sep/11 02:42,16/Apr/19 09:32
Bug,CASSANDRA-3117,12521008,StorageServiceMBean is missing a getCompactionThroughputMbPerSec() method,"Without a getter, you can assign a new value but not query the existing one (which is strange).",urandom,urandom,Low,Resolved,Fixed,01/Sep/11 02:47,16/Apr/19 09:32
Bug,CASSANDRA-3119,12521061,Cli syntax for creating keyspace is inconsistent in 1.0,"In 0.8, to create a keyspace you could do:
{noformat}
create keyspace test with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = [{replication_factor:3}]
{noformat}

In current trunk, if you try that, you get back ""null"". Turns out this is because the syntax for strategy_options has changed and you should not use the brackets, i.e:
{noformat}
strategy_options = {replication_factor:3}
{noformat}
(and note that reversely, this syntax doesn't work in 0.8).

I'm not sure what motivated that change but this is very user unfriendly. The help does correctly mention the new syntax, but it is the kind of changes that takes you 5 minutes to notice. It will also break people scripts for no good reason that I can see.

We should either:
# revert to the old syntax
# support both the new and old syntax
# at least print a meaningful error message when the old syntax is used

Imho, the last solution is by far the worst solution.
",tjake,slebresne,Low,Resolved,Fixed,01/Sep/11 13:46,16/Apr/19 09:32
Bug,CASSANDRA-3123,12521174,Don't try to build secondary indexes when there is none,"buildSecondaryIndexes() is sometimes called without checking the cfs has secondary indexes. Has a result, it prints a useless message and will trigger a bunch of useless action (among which, a full scan of the indexed column family). This is not a huge problem in 0.8 because only the fairly new loadNewSSTables() call does this (which doesn't mean we should fix it). But in trunk, it does this after every streamIn session. ",slebresne,slebresne,Low,Resolved,Fixed,02/Sep/11 12:01,16/Apr/19 09:32
Bug,CASSANDRA-3126,12521207,unable to remove column metadata via CLI,"I cant find way how to remove all columns definitions without CF import/export.

[default@int4] update column family sipdb with column_metadata = [];
Syntax error at position 51: required (...)+ loop did not match anything at input ']'

[default@int4] update column family sipdb with column_metadata = [{}];
Command not found: `update column family sipdb with column_metadata = [{}];`. Type 'help;' or '?' for help.
[default@int4]
",xedin,hsn,Low,Resolved,Fixed,02/Sep/11 16:39,16/Apr/19 09:32
Bug,CASSANDRA-3129,12521215,"""show schema"" in CLI outputs invalid text structure that cannot be replayed (easily tweakable though)","Log explaining the problem. Trouble happens at the ""and replication_factor = 1"" string

[default@unknown] connect cassandra2/9160;                                       
Connected to: ""Lab1"" on cassandra2/9160

* Create a keyspace with a pretty simple definition
[default@unknown] create keyspace foo
...	  with placement_strategy = 'SimpleStrategy'
...	  and strategy_options = [{replication_factor : 1}];
f9e13340-d58f-11e0-0000-e3f60146f2df
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] use foo;
Authenticated to keyspace: foo

* Copy the schema so we can paste it later
[default@foo] show schema; 
create keyspace foo
  and replication_factor = 1
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = [{replication_factor : 1}];

use foo;


* Remove the keyspace, so we can paste the exact same text above
[default@foo] drop keyspace foo;
07c93a70-d590-11e0-0000-e3f60146f2df
Waiting for schema agreement...
... schemas agree across the cluster

* Paste the schema shown above as result of the 'show schema' command
[default@unknown] create keyspace foo
...	  and replication_factor = 1
...	  with placement_strategy = 'SimpleStrategy'
...	  and strategy_options = [{replication_factor : 1}];
No enum constant org.apache.cassandra.cli.CliClient.AddKeyspaceArgument.REPLICATION_FACTOR
* Presented an error that should not occur if show schema generated valid text
",xedin,udontknow,Low,Resolved,Fixed,02/Sep/11 18:23,16/Apr/19 09:32
Bug,CASSANDRA-3131,12521228,cqlsh doesn't work on windows (no readline),"Saulius Menkevicius reports in CASSANDRA-3010 that {{cqlsh}} doesn't start on Windows because the readline module is not present.

{{cqlsh}} should be fixed to only use readline if it is present.",thepaul,urandom,Low,Resolved,Fixed,02/Sep/11 20:40,16/Apr/19 09:32
Bug,CASSANDRA-3138,12521372,PropertyFileSnitch's ResourceWatcher fails because it uses FBUtilities.resourceToFile(..) while PropertyFileSnitch uses classloader.getResourceAsStream(..),"Resource files are not necessarily plain files. They could be inside a jar file. See CASSANDRA-2036

This will cause {noformat}RROR 24:15,806 ResourceWatcher$WatchedResource: Timed run of class org.apache.cassandra.locator.PropertyFileSnitch$1 failed.
org.apache.cassandra.config.ConfigurationException: unable to locate cassandra-topology.properties
	at org.apache.cassandra.utils.FBUtilities.resourceToFile(FBUtilities.java:467)
	at org.apache.cassandra.utils.ResourceWatcher$WatchedResource.run(ResourceWatcher.java:57)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662){noformat}",mck,mck,Low,Resolved,Fixed,05/Sep/11 16:41,16/Apr/19 09:32
Bug,CASSANDRA-3139,12521380,Prevent users from creating keyspaces with LocalStrategy replication,,xedin,jbellis,Low,Resolved,Fixed,05/Sep/11 19:03,16/Apr/19 09:32
Bug,CASSANDRA-3142,12521408,CustomTThreadPoolServer should log TTransportException at DEBUG level,"Currently CustomTThreadPoolServer, like the Thrift TThreadPoolServer, silently ignores TTransportException in its run() method. This is appropriate in most cases because TTransportException occurs fairly often when client connections die. However TTransportException is also thrown when TFramedTransport encounters a frame that is larger than thrift_framed_transport_size_in_mb. In that case, silently exiting the run loop leads to a SocketException on the client side which can be both difficult to diagnose, in part because nothing is logged by Cassandra, and high-impact, because the client may respond by marking the server node down and retrying the too-large request on another node, where it also fails. This process repeated leads to the entire cluster being marked down (see https://github.com/rantav/hector/issues/212). I've filed two Thrift issues (https://issues.apache.org/jira/browse/THRIFT-1323 and https://issues.apache.org/jira/browse/THRIFT-1324), but in the meantime, I suggest that CustomTThreadPoolServer log the exception at DEBUG level in order to support easier troubleshooting.
",jancona,jancona,Low,Resolved,Fixed,06/Sep/11 02:10,16/Apr/19 09:32
Bug,CASSANDRA-3144,12521471,"trunk is unable to participate with an 0.8 ring, again","Title pretty much says it all, looks like a rehash of CASSANDRA-2818 to some degree.",brandon.williams,brandon.williams,Urgent,Resolved,Fixed,06/Sep/11 17:04,16/Apr/19 09:32
Bug,CASSANDRA-3145,12521505,IntervalTree could miscalculate its max,"The implementation of IntervalTree in trunk expects an ordered list of Interval objects as the argument to its constructor. It uses the ordering (only) to determine its minimum and maximum endpoints out of all Intervals stored in it. However, no ordering should be able to guarantee the first element has the set-wide minimum and that the last element has the set-wide maximum; you have to order by minima or maxima or some combination.

I propose that the requirement for ordered input to the IntervalTree constructor be dropped, seeing as how the elements will be sorted as necessary inside the IntervalNode object anyway. The set-wide minimum and maximum could be more straightforwardly calculated inside IntervalNode, and just exposed via IntervalTree.",thepaul,thepaul,Low,Resolved,Fixed,06/Sep/11 22:02,16/Apr/19 09:32
Bug,CASSANDRA-3152,12521828,Logic of AbstractNetworkTopologySnitch.compareEndpoints is wrong,"Current logic in ANTS.cE is to compare the rack and then compare the DC's, the problem is when we have the same rack name but the racks are in a diffrent DC's this logic breaks...

Example: 
""us-east,1a"", InetAddress.getByName(""127.0.0.1"")
""us-east,1b"", InetAddress.getByName(""127.0.0.2"")
""us-east,1c"", InetAddress.getByName(""127.0.0.3"")
""us-west,1a"", InetAddress.getByName(""127.0.0.4"")
""us-west,1b"", InetAddress.getByName(""127.0.0.5"")
""us-west,1c"", InetAddress.getByName(""127.0.0.6"")

Expected:
/127.0.0.1,/127.0.0.3,/127.0.0.2,/127.0.0.4,/127.0.0.5,/127.0.0.6

Current:
/127.0.0.1,/127.0.0.4,/127.0.0.3,/127.0.0.2,/127.0.0.5,/127.0.0.6",vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,07/Sep/11 22:22,16/Apr/19 09:32
Bug,CASSANDRA-3154,12521834,Bad equality check in ColumnFamilyStore.isCompleteSSTables(),The equality check in isCompleteSSTables() always fails because it tries to call equals() with a Set and a List. This might result in failure to purge tombstones in some cases.,tupshin,tupshin,Low,Resolved,Fixed,07/Sep/11 23:21,16/Apr/19 09:32
Bug,CASSANDRA-3155,12521842,Secondary index should report it's memory consumption,Non-CFS backed secondary indexes will consume RAM which should be reported back to Cassandra to be factored into it's flush by RAM amount.,tjake,jasonrutherglen,Low,Resolved,Fixed,08/Sep/11 02:12,16/Apr/19 09:32
Bug,CASSANDRA-3156,12521845,assertion error in RowRepairResolver,"Only seems to happen on a coordinator who does not have a copy of the data:

DEBUG 03:15:59,866 Processing response on a callback from 3840@/10.179.64.227
DEBUG 03:15:59,866 Preprocessed data response
DEBUG 03:15:59,866 Processing response on a callback from 3841@/10.179.111.137
DEBUG 03:15:59,866 Preprocessed digest response
DEBUG 03:15:59,865 Processing response on a callback from 3837@/10.179.111.137
DEBUG 03:15:59,865 Preprocessed data response
DEBUG 03:15:59,865 Preprocessed data response
DEBUG 03:15:59,867 Preprocessed digest response
DEBUG 03:15:59,867 resolving 2 responses
ERROR 03:15:59,866 Fatal exception in thread Thread[ReadRepairStage:526,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:77)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR 03:15:59,866 Fatal exception in thread Thread[ReadRepairStage:525,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:77)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
ERROR 03:15:59,867 Fatal exception in thread Thread[ReadRepairStage:528,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:77)
        at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
DEBUG 03:15:59,867 resolving 2 responses
DEBUG 03:15:59,867 resolving 2 responses
DEBUG 03:15:59,867 resolving 2 responses",jbellis,brandon.williams,Urgent,Resolved,Fixed,08/Sep/11 03:18,16/Apr/19 09:32
Bug,CASSANDRA-3157,12522259,"After a ""short read"", the wrong read command may be used","In fetchRows, there is this code:
{noformat}
    for (int i = 0; i < commandsToSend.size(); i++)
    {
        ReadCallback<Row> handler = readCallbacks.get(i);
        ReadCommand command = commands.get(i);
{noformat}
On the first iteration of fetchRows, commands == commandsToSend so this is ok, but on a short read, commandsToSend will only contain the command to retry so we'll pick up the wrong command on the last line.",slebresne,slebresne,Low,Resolved,Fixed,08/Sep/11 13:47,16/Apr/19 09:32
Bug,CASSANDRA-3158,12522263,Improve caching of same-version Messages on digest and repair paths,,jbellis,jbellis,Low,Resolved,Fixed,08/Sep/11 14:03,16/Apr/19 09:32
Bug,CASSANDRA-3159,12522269,Multiple classpath entries in the cassandra-all.jar,"from CASSANDRA-2936

{code}
Sep 8, 2011 8:47:45 AM java.util.jar.Attributes read
WARNING: Duplicate name in Manifest: Class-Path.
Ensure that the manifest does not have duplicate entries, and
that blank lines separate individual sections in both your
manifest and in the META-INF/MANIFEST.MF entry in the jar file.
{code}",urandom,tjake,Low,Resolved,Fixed,08/Sep/11 14:59,16/Apr/19 09:32
Bug,CASSANDRA-3160,12522273,PIG_OPTS bash variable interpolation doesn't work correctly when PIG_OPTS is set in the environment.,PIG_OPTS bash variable interpolation doesn't work correctly when PIG_OPTS is set in the environment due to variable preceding quotes.,,eldondev,Low,Resolved,Fixed,08/Sep/11 16:01,16/Apr/19 09:32
Bug,CASSANDRA-3163,12522316,SlabAllocator OOMs much faster than HeapAllocator,"SlabAllocator will OOM with stress at around 5.5M rows, which in this configuration is roughly 3.3M rows per node.  Reverting to the HeapAllocator allowed all 10M rows to finish.  Examining the SA heap dump in MAT just shows ~98% of the heap is in 'remainder'",jbellis,brandon.williams,Normal,Resolved,Fixed,08/Sep/11 20:43,16/Apr/19 09:32
Bug,CASSANDRA-3164,12522322,GCInspector still not avoiding divide by zero,"This is because Long objects need to be compared with .equals, not ==.

CASSANDRA-3076 is the original issue but we should use a new ticket for this since 0.7.9 and 0.8.5 are both released already.",jbellis,jbellis,Low,Resolved,Fixed,08/Sep/11 20:59,16/Apr/19 09:32
Bug,CASSANDRA-3166,12522418,Rolling upgrades from 0.7 to 0.8 not possible,"We are in the progress of upgrading to 0.8 and we need to do a rolling upgrade, this fails miserably and it is reproducible;

1. set up a 3 node cluster with 0.7.9 and rf=3, read and write, QUORUM
2. upgrade one of the nodes (i upped a seednode, not sure if that is important)
3. continue reading/writing
4. see logs on the 0.7 node fill up with: INFO 12:36:08,240 Received connection from newer protocol version. Ignorning message.


it does work if i start the 0.7.9 nodes *after* the 0.8.4 node which makes me think that it matters if it is the 0.8 node connecting to the 0.7 nodes or the other way round.

Debug logging on the 0.8 node shows:
/var/log/cassandra/system.log.9:DEBUG [pool-2-thread-82] 2011-09-09 11:55:06,067 StorageProxy.java (line 178) Write timeout java.util.concurrent.TimeoutException for one (or more) of: 
/var/log/cassandra/system.log.9:DEBUG [pool-2-thread-76] 2011-09-09 11:55:06,067 StorageProxy.java (line 584) Read timeout: java.util.concurrent.TimeoutException: Operation timed out - received only 1 responses from /193.182.3.92,  .

nothing except for the ""newer protocol version..."" in the 0.7-logs

i will continue to look at this issue but if anyone has a quick patch, let me know

",,marcuse,Normal,Resolved,Fixed,09/Sep/11 12:45,16/Apr/19 09:32
Bug,CASSANDRA-3168,12522427,Arena allocation causes excessive flushing on small heaps,"adding allocator.size() to Memtable.getLiveSize has two problems:

1) it double-counts allocated parts of regions
2) it makes the size of an empty memtable the size of a single region

(2) is a particular problem because flushing a nearly-empty memtable will not actually free up much memory -- we just trade one almost-empty region, for another.  In testing, I even saw this happening to the low-traffic system tables like LocationInfo.",jbellis,jbellis,Normal,Resolved,Fixed,09/Sep/11 13:52,16/Apr/19 09:32
Bug,CASSANDRA-3169,12522441,read repair: NEWS does not match actual behavior,"NEWS says:

{noformat}
    - Hinted Handoff is substantially more robust, with the result that
      when HH is enabled, repair only needs to be run if a node crashes.
    - Because of this, read repair is disabled now by default on newly
      created ColumnFamilies.
{noformat}

But default RR value is still 1.0.",jbellis,jbellis,Low,Resolved,Fixed,09/Sep/11 15:42,16/Apr/19 09:32
Bug,CASSANDRA-3171,12522467,AbstractCompactionIterable uses a 1MB buffer for every sstable,"This causes an OOM for any compaction task that needs to open all the sstables when you have a lot of them (repair, major, etc)",brandon.williams,brandon.williams,Normal,Resolved,Fixed,09/Sep/11 19:47,16/Apr/19 09:32
Bug,CASSANDRA-3176,12522664,Disabling hinted handoff counterintuitively continues to log handoff messages,"In order to test a theory, we tried to disable hinted handoff on our cluster.  We updated all of the yaml files and then restarted all the nodes in our cluster.  However we continue to get messages such as this in the logs after restarting:
{quote}
INFO [HintedHandoff:1] 2011-09-10 22:41:40,813 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.1.2.3
INFO [HintedHandoff:1] 2011-09-10 22:41:40,813 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.1.2.3
INFO [HintedHandoff:1] 2011-09-10 22:41:45,025 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.2.3.4
INFO [HintedHandoff:1] 2011-09-10 22:41:45,026 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.2.3.4
INFO [HintedHandoff:1] 2011-09-10 22:42:10,017 HintedHandOffManager.java (line 323) Started hinted handoff for endpoint /10.3.4.5
INFO [HintedHandoff:1] 2011-09-10 22:42:10,017 HintedHandOffManager.java (line 379) Finished hinted handoff of 0 rows to endpoint /10.3.4.5
{quote}

Also looking at the System.HintsColumnFamily in jmx there is activity there such as pending tasks that come and go.",jbellis,jeromatron,Low,Resolved,Fixed,10/Sep/11 23:20,16/Apr/19 09:32
Bug,CASSANDRA-3178,12522749,Counter shard merging is not thread safe,"The first part of the counter shard merging process is done during counter replication. This was done there because it requires that all replica are made aware of the merging (we could only rely on nodetool repair for that but that seems much too fragile, it's better as just a safety net). However this part isn't thread safe as multiple threads can do the merging for the same shard at the same time (which shouldn't really ""corrupt"" the counter value per se, but result in an incorrect context).

Synchronizing that part of the code would be very costly in term of performance, so instance I propose to move the part of the shard merging done during replication to compaction. It's a better place anyway. The only downside is that it means compaction will sometime send mutations to other node as a side effect, which doesn't feel very clean but is probably not a big deal either.",slebresne,slebresne,Normal,Resolved,Fixed,12/Sep/11 14:58,16/Apr/19 09:32
Bug,CASSANDRA-3179,12522755,JVM segfaults,Both with and without compressed OOPs enabled.  Seems to mostly happen during compaction+reads.  I'll attach some hs_err files shortly.,jbellis,brandon.williams,Normal,Resolved,Fixed,12/Sep/11 15:23,16/Apr/19 09:32
Bug,CASSANDRA-3181,12522759,Compaction fails to occur,"Compaction just stops running at some point.  To repro, insert like 20M rows with a 1G heap and you'll get around 1k sstables.  Restarting doesn't help, you have to invoke a major to get anything to happen.",jbellis,brandon.williams,Normal,Resolved,Fixed,12/Sep/11 15:51,16/Apr/19 09:32
Bug,CASSANDRA-3182,12522760,Remove special-cased maximum on sstables-to-compact for leveled strategy,With CASSANDRA-3171 fixed we don't need to be scared of large numbers of compaction candidates anymore.,jbellis,jbellis,Low,Resolved,Fixed,12/Sep/11 16:01,16/Apr/19 09:32
Bug,CASSANDRA-3184,12522777,Update the versions that are referenced in the generated POMs so that they match the versions in svn's lib folder,Update the versions before the release so that the release uses the same dependencies for Maven downloaded dependencies,stephenc,stephenc,Low,Resolved,Fixed,12/Sep/11 18:37,16/Apr/19 09:32
Bug,CASSANDRA-3186,12522798,nodetool should not NPE when rack/dc info is not yet available,"As the title says.  What happens is the persisted ring is loaded, but if those nodes are down and you're using a snitch like ec2 that gets rack/dc info from gossip, nodetool NPEs instead of showing that the node is down.",brandon.williams,brandon.williams,Low,Resolved,Fixed,12/Sep/11 20:15,16/Apr/19 09:32
Bug,CASSANDRA-3187,12522800,Return both listen_address and rpc_address through describe_ring,"CASSANDRA-1777 changed describe_ring to return the rpc address associated with a node instead of the listen_address. This allows using different interfaces for listen_address and rpc_address, but breaks when rpc_address is set to something like 0.0.0.0.

I think the describe_ring should just return both interfaces. We can add an optional field to the TokenRange struct that is 'listen_endpoints' or something similar and populate that with the listen addresses of nodes.",nickmbailey,nickmbailey,Low,Resolved,Fixed,12/Sep/11 20:22,16/Apr/19 09:32
Bug,CASSANDRA-3191,12522827,unable to start single node cluster when listen_address is not localhost,"Due to bootstrap default == true

{code}
INFO 20:07:51,839 Joining: waiting for ring and schema information
 INFO 20:08:21,839 Joining: getting bootstrap token
ERROR 20:08:21,843 Exception encountered during startup.
java.lang.RuntimeException: No other nodes seen!  Unable to bootstrap
	at org.apache.cassandra.dht.BootStrapper.getBootstrapSource(BootStrapper.java:168)
	at org.apache.cassandra.dht.BootStrapper.getBalancedToken(BootStrapper.java:150)
	at org.apache.cassandra.dht.BootStrapper.getBootstrapToken(BootStrapper.java:145)
	at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:528)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:450)
	at org.apache.cassandra.service.StorageService.initServer(StorageService.java:372)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:213)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:335)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Exception encountered during startup.
{code}",jbellis,tjake,Normal,Resolved,Fixed,13/Sep/11 00:10,16/Apr/19 09:32
Bug,CASSANDRA-3192,12522833,NPE in RowRepairResolver,"On a 3 node brisk cluster (running against C* 1.0 branch), I was running the java stress tool and the terasort concurrently in two sessions.  Eventually both jobs failed with TimedOutException.
  
From this point forward most additional activity will fail with a TimedOutException. 
* Java Stress Tool - 5 rows / 10 columns - Operation [0] retried 10 times - error inserting key 0 ((TimedOutException))
* Hive - show tables: FAILED: Error in metadata: com.datastax.bdp.hadoop.hive.metastore.CassandraHiveMetaStoreException: There was a problem with the Cassandra Hive MetaStore: Could not connect to Cassandra. Reason: Error connecting to node localhost

However, the Cassandra CLI appears to be happy
* Cassandra CLI: you can successfully insert and read using consistencylevel as ONE or ALL

The seed node has the following error repeatedly occurring in the logs.  The other two nodes have no errors.

{code}
ERROR [ReadRepairStage:15] 2011-09-13 00:44:25,971 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadRepairStage:15,5,main]
java.lang.RuntimeException: java.lang.NullPointerException
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.service.RowRepairResolver.resolve(RowRepairResolver.java:82)
	at org.apache.cassandra.service.AsyncRepairCallback$1.runMayThrow(AsyncRepairCallback.java:54)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
{code}",jbellis,cdaw,Normal,Resolved,Fixed,13/Sep/11 01:07,16/Apr/19 09:32
Bug,CASSANDRA-3193,12522843,Gossip teardown causes test failures,Just look at any recent jenkins or buildbot attempt for the semi-nonsensical NBHM.remove NPE,jbellis,brandon.williams,Normal,Resolved,Fixed,13/Sep/11 04:28,16/Apr/19 09:32
Bug,CASSANDRA-3194,12522858,repair streaming forwarding loop,"I am able to reproduce what appears to be a streaming forwarding loop when running repairs.  This affect only nodes using broadcast_address (ec2 external ip) & listen_address of 0.0.0.0. (Configuration is using property file snitch in a multi DC NTS where some DC's are EC2 and others are not).  The hosts in the other dc's not using broadcast_address do not experience this symptom.

on ec2 host dc1host1:
INFO [AntiEntropyStage:1] 2011-09-13 06:34:01,673 StreamingRepairTask.java (line 211) [streaming task #ce793c30-ddd1-11e0-0000-071a4b76fefb] Received task from /0.0.0.0 to stream 12259 ranges to /external.ec2.ip.dc1host3
 INFO [AntiEntropyStage:1] 2011-09-13 06:34:01,673 StreamingRepairTask.java (line 136) [streaming task #ce793c30-ddd1-11e0-0000-071a4b76fefb] Forwarding streaming repair of 12259 ranges to /external.ec2.ip.of.dc1host1 (to be streamed with /external.ip.of.host3)

The above appears to trigger another streaming task and results in saturating the network interfaces dc1host1.  The above log entries are repeated until cassandra is killed.",slebresne,awinter,Normal,Resolved,Fixed,13/Sep/11 08:02,16/Apr/19 09:32
Bug,CASSANDRA-3195,12522876,"Cassandra-CLI does not allow ""Config"" as column family name","""create column family Config"" does not work, ""create column family Configg"" does.

I suppose the intent is that column families can be named freely, that they have a namespace completely of their own, and separate from, say, Cassandra-CLI commands.",xedin,kalmatar,Low,Resolved,Fixed,13/Sep/11 10:13,16/Apr/19 09:32
Bug,CASSANDRA-3198,12522884,debian packaging installation problem when installing for the first time,"when installing cassandra through the debian packaging for the first time, there is permission problem when starting Cassandra.

Normally, the postinst script change owner of /var/log/cassandra and /var/lib/cassandra from root to cassandra user.

there is a problem with the test which verify if threre is a need to change the owner of these directory or not.

On a new install, the $2 parameter is not set and the the test is false and the owner is not changed.

(simply, i think replace ""&&"" with ""||"" might work)
",jsevellec,jsevellec,Normal,Resolved,Fixed,13/Sep/11 11:59,16/Apr/19 09:32
Bug,CASSANDRA-3202,12522911,CFMetata.getMergeShardChance returns readRepairChance,"The summary says it all. Note that CASSANDRA-3178 will hopefully make that option useless. But in the meantime, this breaks the tests in 1.0.0 since read repair chance was set to 0.1 by default (while the test assumes merge_shard_chance is 1), so let's fix that quickly.",,slebresne,Low,Resolved,Fixed,13/Sep/11 15:39,16/Apr/19 09:32
Bug,CASSANDRA-3203,12522964,Odd flush behavior,"Given the same workload against 0.8, trunk is creating more than twice the amount of sstables.  Even though a uniform stress workload is being generated, flush size degrades quickly:

{noformat}
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:22,878 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@2058235391(7741
035/110172631 serialized/live bytes, 151785 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:24,888 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1520390052(3887
220/72403158 serialized/live bytes, 76220 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:26,890 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1868496516(4097
085/76255481 serialized/live bytes, 80335 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:28,893 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@498232521(43513
20/80922269 serialized/live bytes, 85320 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:29,895 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1592308290(2310
810/44514839 serialized/live bytes, 45310 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:30,897 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@775439677(22684
80/64984390 serialized/live bytes, 44480 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:31,899 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@928217914(26741
85/76231422 serialized/live bytes, 52435 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:32,901 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@158103119(27511
95/77317732 serialized/live bytes, 53945 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:33,903 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@2035169258(3132
420/88934701 serialized/live bytes, 61420 ops)
 INFO [NonPeriodicTasks:1] 2011-09-09 18:24:34,905 ColumnFamilyStore.java (line 658) Enqueuing flush of Memtable-Standard1@1097314626(2979
675/83651699 serialized/live bytes, 58425 ops)
{noformat}

The serialized to live size ratio appears completely out of whack.",jbellis,brandon.williams,Urgent,Resolved,Fixed,13/Sep/11 21:49,16/Apr/19 09:32
Bug,CASSANDRA-3204,12522993,stress cannot set the compaction strategy,"stress can't set the compaction strategy, so testing leveldb-style compaction is more difficult than it should be, especially with lots of cluster setup/teardown.",xedin,brandon.williams,Normal,Resolved,Fixed,14/Sep/11 04:24,16/Apr/19 09:32
Bug,CASSANDRA-3205,12523056,ColumnFamily.cloneMeShallow doesn't respect the insertionOrdered flag (for ArrayBackedSortedColumns),,slebresne,slebresne,Low,Resolved,Fixed,14/Sep/11 14:18,16/Apr/19 09:32
Bug,CASSANDRA-3206,12523066,"increase file descriptor limit in deb, rpm packages","We can use a lot of file descriptors (one per socket, 5? per sstable).  People hit this regularly on the user list and it will get worse with Leveled compaction, which limits sstable size to a relatively low size (currently 5MB).",thepaul,jbellis,Low,Resolved,Fixed,14/Sep/11 16:04,16/Apr/19 09:32
Bug,CASSANDRA-3207,12523081,Log message at INFO when a global or keyspace level repair operation completes,"If JMX times out it's difficult to tell when repair completes.Right now we log at DEBUG for each column family but we need a way to tell when the repair operation completes as a whole.
",slebresne,bcoverston,Low,Resolved,Fixed,14/Sep/11 18:05,16/Apr/19 09:32
Bug,CASSANDRA-3208,12523104,USE <keyspace> doesn't work for numeric keyspaces,"In the CLI, {code}USE <keyspace>;{code} doesn't work for keyspaces' names that contain only digits.
The error I'm getting is:
{{Syntax error at position 4: mismatched input '20110914' expecting Identifier}}",xedin,liqweed,Low,Resolved,Fixed,14/Sep/11 21:03,16/Apr/19 09:32
Bug,CASSANDRA-3210,12523138,memtables do not need to be flushed on the Table.apply() path anymore after 2449,"2449 removes auto-flush from Table.apply(), but the data structure is still there, no harm, but better remove it:

in
https://github.com/apache/cassandra/blob/c7cdc317c9a14e29699f9842424388aee77d0e1a/src/java/org/apache/cassandra/db/Table.java

line 399 and 470",yangyangyyy,yangyangyyy,Low,Resolved,Fixed,15/Sep/11 01:24,16/Apr/19 09:32
Bug,CASSANDRA-3212,12523189,Accomodate missing encryption_options in IncomingTcpConnection.stream,,jbellis,jbellis,Low,Resolved,Fixed,15/Sep/11 13:09,16/Apr/19 09:32
Bug,CASSANDRA-3215,12523306,The word count example demonstrating hadoop integration fails in trunk,"The following stack traces after running, bin/hadoop in the trunk (0.8.2-dev-SNAPSHOT):

./bin/word_count
11/09/15 12:28:28 INFO WordCount: output reducer type: cassandra
11/09/15 12:28:29 INFO jvm.JvmMetrics: Initializing JVM Metrics with processName=JobTracker, sessionId=
11/09/15 12:28:30 INFO mapred.JobClient: Running job: job_local_0001
11/09/15 12:28:30 INFO mapred.MapTask: io.sort.mb = 100
11/09/15 12:28:30 INFO mapred.MapTask: data buffer = 79691776/99614720
11/09/15 12:28:30 INFO mapred.MapTask: record buffer = 262144/327680
11/09/15 12:28:30 WARN mapred.LocalJobRunner: job_local_0001
java.lang.RuntimeException: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:132)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getLocation(ColumnFamilyRecordReader.java:176)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:113)
	... 4 more
11/09/15 12:28:31 INFO mapred.JobClient:  map 0% reduce 0%
11/09/15 12:28:31 INFO mapred.JobClient: Job complete: job_local_0001
11/09/15 12:28:31 INFO mapred.JobClient: Counters: 0
11/09/15 12:28:31 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
11/09/15 12:28:32 INFO mapred.JobClient: Running job: job_local_0002
11/09/15 12:28:32 INFO mapred.MapTask: io.sort.mb = 100
11/09/15 12:28:32 INFO mapred.MapTask: data buffer = 79691776/99614720
11/09/15 12:28:32 INFO mapred.MapTask: record buffer = 262144/327680
11/09/15 12:28:32 WARN mapred.LocalJobRunner: job_local_0002
java.lang.RuntimeException: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:132)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getLocation(ColumnFamilyRecordReader.java:176)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:113)
	... 4 more
11/09/15 12:28:33 INFO mapred.JobClient:  map 0% reduce 0%
11/09/15 12:28:33 INFO mapred.JobClient: Job complete: job_local_0002
11/09/15 12:28:33 INFO mapred.JobClient: Counters: 0
11/09/15 12:28:33 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
11/09/15 12:28:34 INFO mapred.JobClient: Running job: job_local_0003
11/09/15 12:28:34 INFO mapred.MapTask: io.sort.mb = 100
11/09/15 12:28:34 INFO mapred.MapTask: data buffer = 79691776/99614720
11/09/15 12:28:34 INFO mapred.MapTask: record buffer = 262144/327680
11/09/15 12:28:34 WARN mapred.LocalJobRunner: job_local_0003
java.lang.RuntimeException: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:132)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getLocation(ColumnFamilyRecordReader.java:176)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:113)
	... 4 more
11/09/15 12:28:35 INFO mapred.JobClient:  map 0% reduce 0%
11/09/15 12:28:35 INFO mapred.JobClient: Job complete: job_local_0003
11/09/15 12:28:35 INFO mapred.JobClient: Counters: 0
11/09/15 12:28:35 INFO jvm.JvmMetrics: Cannot initialize JVM Metrics with processName=JobTracker, sessionId= - already initialized
11/09/15 12:28:36 INFO mapred.JobClient: Running job: job_local_0004
11/09/15 12:28:36 INFO mapred.MapTask: io.sort.mb = 100
11/09/15 12:28:37 INFO mapred.MapTask: data buffer = 79691776/99614720
11/09/15 12:28:37 INFO mapred.MapTask: record buffer = 262144/327680
11/09/15 12:28:37 WARN mapred.LocalJobRunner: job_local_0004
java.lang.RuntimeException: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:132)
	at org.apache.hadoop.mapred.MapTask$NewTrackingRecordReader.initialize(MapTask.java:418)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:620)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
	at org.apache.hadoop.mapred.LocalJobRunner$Job.run(LocalJobRunner.java:177)
Caused by: java.lang.UnsupportedOperationException: no local connection available
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.getLocation(ColumnFamilyRecordReader.java:176)
	at org.apache.cassandra.hadoop.ColumnFamilyRecordReader.initialize(ColumnFamilyRecordReader.java:113)
	... 4 more
11/09/15 12:28:37 INFO mapred.JobClient:  map 0% reduce 0%
11/09/15 12:28:37 INFO mapred.JobClient: Job complete: job_local_0004
11/09/15 12:28:37 INFO mapred.JobClient: Counters: 0",brandon.williams,mccloud35,Low,Resolved,Fixed,16/Sep/11 09:47,16/Apr/19 09:32
Bug,CASSANDRA-3216,12523332,A streamOutSession keeps sstables references forever if the remote end dies,"A streamOutSession acquire a reference on the sstable it will stream and release them as soon as each sstable has been fully streamed. However, since a stream session has currently no means to know when it failed, we'll keep references indefinitely (meaning until next restart) if their is a failure. One way a stream session could very easily fail is if the remote end dies. We must make sure we correctly release sstable references when that happens.

Note that it won't be bulletproof, there is probably other means by which a streaming could fail: a bug in the code throwing an exception, no space left on the receiving end, etc... But those are unlikely enough that I propose to care only for the case of a node dying for now and leave the bullet-proofing to CASSANDRA-3112. ",slebresne,slebresne,Low,Resolved,Fixed,16/Sep/11 13:36,16/Apr/19 09:32
Bug,CASSANDRA-3218,12523356,"Failure reading a erroneous/spurious AutoSavingCache file can result in a failed application of a migration, which can prevent a node from reaching schema agreement.","Failure reading a erroneous/spurious AutoSavingCache file can result in a failed application of a migration, which can prevent a node from reaching schema agreement. This is distinctly possible when a machine loses it's data partition, and attempts to recover the schema upon restart, and so has to apply all the migrations. The initial stack traces look like this:

Add column family: org.apache.cassandra.config.CFMetaData@38bcfee6[cfId=1000,ksName=someks,cfName=somecf,cfType=Standard,comparat
or=org.apache.cassandra.db.marshal.UTF8Type,subcolumncomparator=<null>, ...

Followed by:

ERROR 00:56:47,974 Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.RuntimeException: java.lang.NegativeArraySizeException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NegativeArraySizeException
        at org.apache.cassandra.cache.AutoSavingCache.readSaved(AutoSavingCache.java:130)
        at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:273)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:465)
        at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:435)
        at org.apache.cassandra.db.Table.initCf(Table.java:369)
        at org.apache.cassandra.db.migration.AddColumnFamily.applyModels(AddColumnFamily.java:93)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:153)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:73)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more

Ultimately, attempted changes to this keyspace/cf will fail like this:

ERROR 13:07:51,006 Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.RuntimeException: java.lang.IllegalArgumentException: Unknown CF 1000
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IllegalArgumentException: Unknown CF 1000
        at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:155)
        at org.apache.cassandra.db.Table.getColumnFamilyStore(Table.java:148)
        at org.apache.cassandra.db.migration.DropKeyspace.applyModels(DropKeyspace.java:63)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:153)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:73)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 6 more",eldondev,eldondev,Low,Resolved,Fixed,16/Sep/11 16:58,16/Apr/19 09:32
Bug,CASSANDRA-3219,12523364,Nodes started at the same time end up with the same token,"Since autoboostrap is defaulted to on when you start a cluster at once (http://screenr.com/5G6) you can end up with nodes being assigned the same token.

{code}
INFO 17:34:55,688 Node /67.23.43.14 is now part of the cluster
 INFO 17:34:55,698 InetAddress /67.23.43.14 is now UP
 INFO 17:34:55,698 Nodes /67.23.43.14 and tjake2/67.23.43.15 have the same token 8823900603000512634329811229926543166.  Ignoring /67.23.43.14
 INFO 17:34:55,698 Node /98.129.220.182 is now part of the cluster
 INFO 17:34:55,698 InetAddress /98.129.220.182 is now UP
 INFO 17:34:55,698 Nodes /98.129.220.182 and tjake2/67.23.43.15 have the same token 8823900603000512634329811229926543166.  Ignoring /98.129.220.182
{code}",slebresne,tjake,Normal,Resolved,Fixed,16/Sep/11 17:39,16/Apr/19 09:32
Bug,CASSANDRA-3222,12523391,cfhistograms is transposed/wrong again,"Read/write latencies are transposed, row size is always equal the column count.  I think we've fixed this at least twice before, but here it is again.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,16/Sep/11 20:58,16/Apr/19 09:32
Bug,CASSANDRA-3223,12523412,probably don't need to do full copy to row cache after un-mmap() change,"3179  changes from directly using the bytebuffer from mmap(), to copying that buffer,

CFS.cacheRow() https://github.com/apache/cassandra/blob/cassandra-1.0.0/src/java/org/apache/cassandra/db/ColumnFamilyStore.java   line 1126
says it makes a deep copy exactly to prevent issues from unmmap().

maybe this deep copy is not needed now given 3179


if so, maybe slightly better performance in both speed and memory",yangyangyyy,yangyangyyy,Low,Resolved,Fixed,17/Sep/11 00:31,16/Apr/19 09:32
Bug,CASSANDRA-3224,12523442,LeveledCompactionStrategy is too complacent,"As the title says, it barely does anything.  I inserted 50G worth of data with 1G heap and 99% overwrite ratio, and it only compacted twice:

{noformat}
 INFO [CompactionExecutor:1] 2011-09-16 22:29:54,572 CompactionTask.java (line 118) Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-1-Data.db')]
 INFO [CompactionExecutor:1] 2011-09-16 22:29:58,606 CompactionTask.java (line 220) Compacted to [/var/lib/cassandra/data/Keyspace1/Standard1-h-2-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-4-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-5-Data.db,].  12,595,811 to 12,595,811 (~100% of original) bytes for 40,501 keys at 3.058122MBPS.  Time: 3,928ms.
 INFO [CompactionExecutor:1] 2011-09-16 22:29:58,607 CompactionTask.java (line 222) CF Total Bytes Compacted: 12,595,811
 INFO [CompactionExecutor:3] 2011-09-16 22:29:58,889 CompactionTask.java (line 118) Compacting [SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-4-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-2-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-5-Data.db'), SSTableReader(path='/var/lib/cassandra/data/Keyspace1/Standard1-h-3-Data.db')]
 INFO [CompactionExecutor:3] 2011-09-16 22:30:06,900 CompactionTask.java (line 220) Compacted to [/var/lib/cassandra/data/Keyspace1/Standard1-h-7-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-9-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-11-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-12-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-14-Data.db,/var/lib/cassandra/data/Keyspace1/Standard1-h-15-Data.db,].  28,374,396 to 28,374,396 (~100% of original) bytes for 91,236 keys at 3.380379MBPS.  Time: 8,005ms.
 INFO [CompactionExecutor:3] 2011-09-16 22:30:06,901 CompactionTask.java (line 222) CF Total Bytes Compacted: 40,970,207
{noformat}

Resulting in the following levels:

{noformat}
L0: 4965
L1: 6
L2: 0
L3: 0
L4: 0
L5: 0
L6: 0
L7: 0
{noformat}

This is obviously going to result in extremely poor read performance.",jbellis,brandon.williams,Normal,Resolved,Fixed,17/Sep/11 18:10,16/Apr/19 09:32
Bug,CASSANDRA-3227,12523482,"cassandra-cli use micro second timestamp, but CQL use milli second","cassandra-cli set micro second timestamp by FBUtilities.timestampMicros. But CQL insert or update operation set milli second timestamp by AbstractModification.getTimestamp.

If you register data by cassandra-cli, you can't update data by CQL. Because CQL timestamp is judged as past time.",jbellis,sabro,Normal,Resolved,Fixed,18/Sep/11 15:55,16/Apr/19 09:32
Bug,CASSANDRA-3230,12523621,clientutil jar missing from artifacts,The new clientutil jar is not being included in binary release artifacts,urandom,urandom,Normal,Resolved,Fixed,19/Sep/11 21:13,16/Apr/19 09:32
Bug,CASSANDRA-3231,12523646,CQL does not throw an error when invalid hex is supplied,"As reported on irc, if you try to create an index on a CF with a default comparator of BytesType, but you supply invalid hex, weird things happen.  Namely if you try to create one on 'category' you instead get one on '\xca\xfe\xff\xff', which is 4 bytes that appears to coincide with attempting to interpret 'ca', 'te', 'go', 'ry' as hex.",xedin,brandon.williams,Low,Resolved,Fixed,20/Sep/11 01:06,16/Apr/19 09:32
Bug,CASSANDRA-3234,12523796,LeveledCompaction has several performance problems,"Two main problems:

- BF size calculation doesn't take into account LCS breaking the output apart into ""bite sized"" sstables, so memory use is much higher than predicted
- ManyToMany merging is slow.  At least part of this is from running the full reducer machinery against single input sources, which can be optimized away.",jbellis,jbellis,Normal,Resolved,Fixed,20/Sep/11 23:47,16/Apr/19 09:32
Bug,CASSANDRA-3235,12523869,OutboundTcpConnection throws RuntimeException,"Regression introduced in CASSANDRA-1788, as reported by liangfeng on the user list.",jbellis,jbellis,Low,Resolved,Fixed,21/Sep/11 14:22,16/Apr/19 09:32
Bug,CASSANDRA-3239,12524230,"can't use RackInferringSnitch and CQL JDBC's ""CREATE KEYSPACE"" with NetworkTopologyStrategy","If using the CQL JDBC driver, there's a problem with using RackInferringSnitch

1. With RackInferringSnitch, the datacenter names are numeric
2. With CQL and NetworkTopologyStrategy, the data center replicas are specified as strategy_options:<dc-name>=<#-of-replicas>
3. Using a number for <dc-name> fails
4. Using a quoted number for <dc-name> fails
",xedin,machenmusik@comcast.net,Low,Resolved,Fixed,22/Sep/11 03:24,16/Apr/19 09:32
Bug,CASSANDRA-3243,12524242,Node which was decommissioned and shut-down reappears on a single node,"I decommissioned a node several days ago. It was no longer in the ring list on any node in the ring. However, it was in the dead gossip list.

In an attempt to clean it out of the dead gossip list so I could truncate, I shut down the entire ring and bought it back up. Once the ring came back up, one node showed the decommissioned node as still in the ring in a state of 'Down'. No other node in the ring shows this info.

I successfully ran removetoken on the node to get that phantom node out. However, it is back in the dead gossip list, preventing me from truncating.

Where might the info on this decommissioned node be being stored? Is HH possibly trying to deliver to the removed node, thus putting it back in the ring on one node?

I find it extremely curious that none of the other nodes in the ring showed the phantom node. Shouldn't gossip have propagated the node everywhere, even if it was down?",brandon.williams,alienth,Low,Resolved,Fixed,22/Sep/11 07:19,16/Apr/19 09:32
Bug,CASSANDRA-3245,12524371,"Don't fail when numactl is installed, but NUMA policies are not supported","When numactl is installed but NUMA policies are not supported, trying to run cassandra gives only:

{noformat}
numactl: This system does not support NUMA policy
{noformat}

..and the startup script fails there.

We should probably fail a little more gracefully. Possibly the best way to tell if numactl will work is by using:

{noformat}
numactl --hardware
{noformat}

but I don't have ready access to a machine with proper NUMA support at the moment so I can't check how easy it is to tell the difference in the output.

It looks just as reliable (if possibly a bit more brittle) to check for the existence of the directory {{/sys/devices/system/node}}. If that directory doesn't exist, we shouldn't even try to use or run numactl.",scode,thepaul,Low,Resolved,Fixed,23/Sep/11 02:56,16/Apr/19 09:32
Bug,CASSANDRA-3246,12524380,memtable_total_space_in_mb does not accept the value 0 in Cassandra 1.0,"This affects 1.0 beta1.

From the key explanation in cassandra.yaml it looks like it should accept the value ""0""

# Total memory to use for memtables. Cassandra will flush the largest
# memtable when this much memory is used.
# If omitted, Cassandra will set it to 1/3 of the heap.
# If set to 0, only the old flush thresholds are used.
memtable_total_space_in_mb: 0

However in the code I could see the following:

if (conf.memtable_total_space_in_mb <= 0)
throw new ConfigurationException(""memtable_total_space_in_mb must be positive"");
logger.info(""Global memtable threshold is enabled at {}MB"", conf.memtable_total_space_in_mb);",jbellis,thobbs,Low,Resolved,Fixed,23/Sep/11 05:48,16/Apr/19 09:32
Bug,CASSANDRA-3247,12524396,sstableloader ignores option doesn't work correctly,The --ignores option is supposed to take an argument but it doesn't.,slebresne,slebresne,Low,Resolved,Fixed,23/Sep/11 09:07,16/Apr/19 09:32
Bug,CASSANDRA-3250,12524426,fsync the directory after new sstable or commit log segment are created,"The mannual of fsync said:
bq.   Calling  fsync()  does  not  necessarily  ensure  that  the entry in the directory containing the file has also reached disk.  For that an explicit fsync() on a file descriptor for the directory is also needed.

At least on ext4, syncing the directory is a must to have step, as described by [1]. Otherwise, the new sstables or commit logs could be missed after crash even if itself is synced. 

Unfortunately, JVM does not provide an approach to sync the directory...

[1] http://www.linuxfoundation.org/news-media/blogs/browse/2009/03/don%E2%80%99t-fear-fsync
",xedin,hanzhu,Low,Resolved,Fixed,23/Sep/11 14:39,16/Apr/19 09:32
Bug,CASSANDRA-3251,12524432,CassandraStorage uses comparator for both super column names and sub column names.,"The CassandraStorage class uses the same comparator for super and sub column names.

This is because it calls columnsToTuple recursively without any indication that the subsequent call is for sub columns.  Also, the getDefaultMarshallers method does not return the sub column name comparator.",xedin,danapsimer,Normal,Resolved,Fixed,23/Sep/11 16:18,16/Apr/19 09:32
Bug,CASSANDRA-3253,12524448,inherent deadlock situation in commitLog flush?,"after my system ran for a while, it consitently goes into frozen state where all the mutations stage threads are waiting
on the switchlock,

the reason is that the switchlock is held by commit log, as shown by the following thread dump:



""COMMIT-LOG-WRITER"" prio=10 tid=0x00000000010df000 nid=0x32d3 waiting on condition [0x00007f2d81557000]
   java.lang.Thread.State: WAITING (parking)
        at sun.misc.Unsafe.park(Native Method)
        - parking to wait for  <0x00007f3579eec060> (a java.util.concurrent.FutureTask$Sync)
        at java.util.concurrent.locks.LockSupport.park(LockSupport.java:186)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.parkAndCheckInterrupt(AbstractQueuedSynchronizer.java:838)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.doAcquireSharedInterruptibly(AbstractQueuedSynchronizer.java:998)
        at java.util.concurrent.locks.AbstractQueuedSynchronizer.acquireSharedInterruptibly(AbstractQueuedSynchronizer.java:1304)
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:248)
        at java.util.concurrent.FutureTask.get(FutureTask.java:111)
        at org.apache.cassandra.db.commitlog.CommitLog.getContext(CommitLog.java:386)
        at org.apache.cassandra.db.ColumnFamilyStore.maybeSwitchMemtable(ColumnFamilyStore.java:650)
        at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:722)
        at org.apache.cassandra.db.commitlog.CommitLog.createNewSegment(CommitLog.java:573)
        at org.apache.cassandra.db.commitlog.CommitLog.access$300(CommitLog.java:81)
        at org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:596)
        at org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.lang.Thread.run(Thread.java:679)


we can clearly see that the COMMIT-LOG-WRITER thread is running the regular appender , but the appender itself calls getContext(), which again submits a new Callable to be executed, and waits on the Callable. but the new Callable is never going to be executed since the executor has only *one* thread.


I believe this is a deterministic bug.



",jbellis,yangyangyyy,Urgent,Resolved,Fixed,23/Sep/11 17:53,16/Apr/19 09:32
Bug,CASSANDRA-3255,12524501,Sstable scrub status persists in compactionstats after scrub is complete,"When scrubbing the sstables on a node, the 'Scrub' info persists in the 'compactionstats' nodetool utility, even after the scrub is complete.",xedin,alienth,Low,Resolved,Fixed,24/Sep/11 04:37,16/Apr/19 09:32
Bug,CASSANDRA-3256,12524504,AssertionError when repairing a node,"When repairing a node, the following exception was thrown two times:

{code}
ERROR [AntiEntropyStage:2] 2011-09-23 23:00:24,016 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[AntiEntropyStage:2,5,main]
java.lang.AssertionError
        at org.apache.cassandra.service.AntiEntropyService.rendezvous(AntiEntropyService.java:170)
        at org.apache.cassandra.service.AntiEntropyService.access$100(AntiEntropyService.java:90)
        at org.apache.cassandra.service.AntiEntropyService$TreeResponseVerbHandler.doVerb(AntiEntropyService.java:518)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{code}

No other errors occurred on the node. From peeking at the code, this assertion appears to simply check if an existing repair session could be found. Interestingly, the repair did continue to run after this as evidenced by several other AntiEntropyService entires in the log.

8 node ring with an RF of 3, if that matters at all. No other nodes in the ring threw exceptions.",slebresne,alienth,Low,Resolved,Fixed,24/Sep/11 06:10,16/Apr/19 09:32
Bug,CASSANDRA-3257,12524571,Enabling SSL on a fairly light cluster leaks Open files.,"To reproduce:

Enable SSL encryption and let the server be idle for a day or so you will see the below....

[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ /usr/sbin/lsof |grep -i cassandra-app.jks |wc -l ;date
16333
Sun Sep 25 17:23:29 UTC 2011
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ java -jar cmdline-jmxclient-0.10.3.jar - localhost:7501 java.lang:type=Memory gc
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ /usr/sbin/lsof |grep -i cassandra-app.jks |wc -l ;date
64
Sun Sep 25 17:23:53 UTC 2011
[vijay_tcasstest@vijay_tcass--1c-i-1568885c ~]$ 

After running GC manually the issue goes away.",vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,25/Sep/11 17:25,16/Apr/19 09:32
Bug,CASSANDRA-3258,12524632,*.bat files fails when CASSANDRA_HOME contains a white space.,"Issues 2952 and 2237 fixed the issue for cassandra.bat. But the following bat files need the same fix:
json2sstable.bat
nodetool.bat
sstable2json.bat
sstablekeys.bat
",talmdal,talmdal,Normal,Resolved,Fixed,26/Sep/11 13:46,16/Apr/19 09:32
Bug,CASSANDRA-3259,12524654,Replace token leaves the old node state in tact causing problems in cli,"in the replace token patch we dont evict the node from the Gossip which will leave the node lingering around and causes issues in cli (UNReachable nodes)

As a part of the replace token if the token is replaced with another token we should remove the old nodes Gossip states.",vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,26/Sep/11 15:51,16/Apr/19 09:32
Bug,CASSANDRA-3260,12524658,MergeIterator assertion on sources != empty can be thrown,"MergeIterator.get assert that it don't get an empty list of sources. This seems to at least not be the case in the unit test for some of tests (this don't make any test fail however, but there is a few stack trace thrown). I think it's pretty unnatural to ""fail"" on an empty list of sources and would force every caller to first take the empty case into account, so I propose to just remove that assertion.",slebresne,slebresne,Low,Resolved,Fixed,26/Sep/11 16:11,16/Apr/19 09:32
Bug,CASSANDRA-3262,12524827,SimpleSnitch.compareEndpoints doesn't respect the intent of the snitch,"SimpleSnitch is supposed to not sort the input addresses, thus respecting the order of the partitioner. However, it's compareEndpoints instead uses IP addresses comparison. Note that this matter when the dynamicSnitch fall back to the wrapped snitch since it uses the compareEndpoint method then.",slebresne,slebresne,Low,Resolved,Fixed,27/Sep/11 09:48,16/Apr/19 09:32
Bug,CASSANDRA-3263,12524864,Whitespace in SimpleSeedProvider string makes seed ignored,"If a seeds given to SimpleSeedProvider contains whitespace, the seed will be ignored

for example ""1.2.3.4, 5.6.7.8"" will only make 5.6.7.8 a seed.

patch simply trim()s the host.",marcuse,marcuse,Low,Resolved,Fixed,27/Sep/11 13:49,16/Apr/19 09:32
Bug,CASSANDRA-3266,12524968,missed CQL term rename,The CQL grammar was missed in the rename of {{bytea}} to {{blob}}.,urandom,urandom,Normal,Resolved,Fixed,27/Sep/11 21:02,16/Apr/19 09:32
Bug,CASSANDRA-3268,12525073,Cannot read counter value from jdbc cql,"it appears on line #36 in src/java/org/apache/cassandra/cql/jdbc/TypesMap.java  (notice it's in the portion of code that sits in the main src dir not the drivers)

map.put(""org.apache.cassandra.db.marshal.ColumnCounterType"", JdbcCounterColumn.instance);

should be 

map.put(""org.apache.cassandra.db.marshal.CounterColumnType"", JdbcCounterColumn.instance);

Notice CounterColumnType is reversed.",coreyhulen,coreyhulen,Low,Resolved,Fixed,28/Sep/11 16:02,16/Apr/19 09:32
Bug,CASSANDRA-3269,12525098,possible early deletion of commit logs,"I ran my cluster for about 2 days. the cluster has 2 nodes. I restarted one box several times, and the other one was always running. the one always running ended up accumulating 100GB of commit logs.


this is 1.0.0 code from about Sept 15 in github. I kept the original setting for 
#commitlog_total_space_in_mb: 4096
i.e. commented out


here is some sample of the output:

-rw-r--r-- 1 yyang yyang 134217857 2011-09-28 03:51 CommitLog-1317181834810.log
-rw-r--r-- 1 yyang yyang 134217869 2011-09-28 03:50 CommitLog-1317181764105.log
-rw-r--r-- 1 yyang yyang 134217783 2011-09-28 03:49 CommitLog-1317181694633.log
-rw-r--r-- 1 yyang yyang 134217750 2011-09-28 02:39 CommitLog-1317176955102.log
yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ ls -lt /mnt/cass/lib//cassandra/commitlog/|wc -l
727
yyang@ip-10-71-21-46:/mnt/cass/log/cassandra$ du -s /mnt/cass/lib/cassandra/commitlog/ 
95095316        /mnt/cass/lib/cassandra/commitlog/
",slebresne,yangyangyyy,Urgent,Resolved,Fixed,28/Sep/11 20:27,16/Apr/19 09:32
Bug,CASSANDRA-3270,12525107,Error during multi-threaded compaction in 0.8,"I'm running 0.8.6 plus the multi-threaded compaction patch in issue 2901.  I'm getting an error compacting last night:


Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.ClassCastException: java.util.concurrent.ThreadPoolExecutor cannot be cast to org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.db.compaction.CompactionManager.performMajor(CompactionManager.java:278)
        at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1856)
        at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1447)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.GeneratedMethodAccessor28.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ClassCastException: java.util.concurrent.ThreadPoolExecutor cannot be cast to org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor$1.rejectedExecution(DebuggableThreadPoolExecutor.java:53)
        at java.util.concurrent.ThreadPoolExecutor.reject(ThreadPoolExecutor.java:767)
        at java.util.concurrent.ThreadPoolExecutor.execute(ThreadPoolExecutor.java:658)
        at java.util.concurrent.AbstractExecutorService.submit(AbstractExecutorService.java:92)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getCompactedRow(ParallelCompactionIterable.java:211)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getReduced(ParallelCompactionIterable.java:185)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Reducer.getReduced(ParallelCompactionIterable.java:146)
        at org.apache.cassandra.utils.ReducingIterator.computeNext(ReducingIterator.java:74)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:105)
        at org.apache.cassandra.db.compaction.ParallelCompactionIterable$Unwrapper.computeNext(ParallelCompactionIterable.java:92)
        at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
        at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
        at org.apache.commons.collections.iterators.FilterIterator.setNextObject(FilterIterator.java:183)
        at org.apache.commons.collections.iterators.FilterIterator.hasNext(FilterIterator.java:94)
        at org.apache.cassandra.db.compaction.CompactionManager.doCompactionWithoutSizeEstimation(CompactionManager.java:573)
        at org.apache.cassandra.db.compaction.CompactionManager.doCompaction(CompactionManager.java:507)
        at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:320)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
",jbellis,kmueller,Normal,Resolved,Fixed,28/Sep/11 21:40,16/Apr/19 09:32
Bug,CASSANDRA-3272,12525118,READ Operation with CL=EACH_QUORUM succeed when a DC is down (RF=3),"""READ EACH_QUORUM: 	Returns the record with the most recent timestamp once a quorum of replicas in each data center of the cluster has responded.""

In other words, if a DC is down and the QUORUM could not be reached on that DC, read should fail.

test case:
- Cassandra version 0.8.6:
INFO [main] 2011-09-28 22:26:24,297 StorageService.java (line 371) Cassandra version: 0.8.6

- 6-node cluster with 2 DC and 3 node each. RF=3 in each DC:
[default@Keyspace3] describe keyspace;
Keyspace: Keyspace3:
Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
Durable Writes: true
Options: [DC2:3, DC1:3]
Column Families:
ColumnFamily: test
Key Validation Class: org.apache.cassandra.db.marshal.BytesType
Default column value validator: org.apache.cassandra.db.marshal.BytesType
Columns sorted by: org.apache.cassandra.db.marshal.BytesType
Row cache size / save period in seconds: 0.0/0
Key cache size / save period in seconds: 200000.0/14400
Memtable thresholds: 1.0875/1440/232 (millions of ops/minutes/MB)
GC grace seconds: 864000
Compaction min/max thresholds: 4/32
Read repair chance: 1.0
Replicate on write: true
Built indexes: []

all nodes are up, insert a row:

$ nodetool -h localhost ring
Address DC Rack Status State Load Owns Token
141784319550391026443072753096570088106
10.34.79.179 DC1 RAC1 Up Normal 11.13 KB 16.67% 0
10.34.70.163 DC2 RAC1 Up Normal 11.14 KB 16.67% 28356863910078205288614550619314017621
10.35.81.147 DC1 RAC1 Up Normal 11.14 KB 16.67% 56713727820156410577229101238628035242
10.84.233.170 DC2 RAC1 Up Normal 11.14 KB 16.67% 85070591730234615865843651857942052864
10.195.201.236 DC1 RAC1 Up Normal 11.14 KB 16.67% 113427455640312821154458202477256070485
10.118.147.73 DC2 RAC1 Up Normal 11.14 KB 16.67% 141784319550391026443072753096570088106

- insert a value 

[default@Keyspace3] set test[utf8('test-key-1')][utf8('test-col')]=utf8('test-value');
Value inserted.

sanity check (cli connects to a node in DC1) :
[default@Keyspace3] consistencylevel as EACH_QUORUM;                                  
Consistency level is set to 'EACH_QUORUM'.
[default@Keyspace3] get test[utf8('test-key-1')];   
=> (column=746573742d636f6c, value=test-value, timestamp=1317249361722000)
Returned 1 results

shut down DC2:
$ nodetool -h localhost ring
Address         DC          Rack        Status State   Load            Owns    Token                                       
                                                                               141784319550391026443072753096570088106     
10.34.79.179    DC1         RAC1        Up     Normal  51.86 KB        16.67%  0                                           
10.34.70.163    DC2         RAC1        Down   Normal  51.88 KB        16.67%  28356863910078205288614550619314017621      
10.35.81.147    DC1         RAC1        Up     Normal  47.5 KB         16.67%  56713727820156410577229101238628035242      
10.84.233.170   DC2         RAC1        Down   Normal  51.88 KB        16.67%  85070591730234615865843651857942052864      
10.195.201.236  DC1         RAC1        Up     Normal  47.5 KB         16.67%  113427455640312821154458202477256070485     
10.118.147.73   DC2         RAC1        Down   Normal  51.88 KB        16.67%  141784319550391026443072753096570088106  

[default@Keyspace3] get test[utf8('test-key-1')];   
=> (column=746573742d636f6c, value=746573742d76616c7565, timestamp=1317249361722000)
Returned 1 results.

tried with pycassaShell:
>>> col_fam.get('test-key-1',read_consistency_level=pycassa.ConsistencyLevel.EACH_QUORUM)
OrderedDict([('test-col', 'test-value')])
",jbellis,cywjackson,Low,Resolved,Fixed,28/Sep/11 23:25,16/Apr/19 09:32
Bug,CASSANDRA-3273,12525134,FailureDetector can take a very long time to mark a host down,"There are two ways to trigger this:

* Bring a node up very briefly in a mixed-version cluster and then terminate it
* Bring a node up, terminate it for a very long time, then bring it back up and take it down again

In the first case, what can happen is a very short interval arrival time is recorded by the versioning logic which requires reconnecting and can happen very quickly. This can easily be solved by rejecting any intervals within a reasonable bound, for instance the gossiper interval.

The second instance is harder to solve, because what is happening is that an extremely large interval is recorded, which is the time the node was left dead the first time.  This throws off the mean of the intervals and causes it to take a much longer time than it should to mark it down the second time.
",brandon.williams,brandon.williams,Normal,Resolved,Fixed,29/Sep/11 04:21,16/Apr/19 09:32
Bug,CASSANDRA-3275,12525211,Make Cassandra compile under JDK 7,"Currently system won't compile under JDK 7 because of errors in CQL JDBC component.

{noformat}
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CResultSet.java:39: error: CResultSet is not abstract and does not override abstract method <T>getObject(String,Class<T>) in ResultSet
    [javac] class CResultSet extends AbstractResultSet implements CassandraResultSet
    [javac] ^
    [javac]   where T is a type-variable:
    [javac]     T extends Object declared in method <T>getObject(String,Class<T>)
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraConnection.java:81: error: CassandraConnection is not abstract and does not override abstract method getNetworkTimeout() in Connection
    [javac] class CassandraConnection extends AbstractCassandraConnection implements Connection
    [javac] ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDataSource.java:24: error: CassandraDataSource is not abstract and does not override abstract method getParentLogger() in CommonDataSource
    [javac] public class CassandraDataSource implements DataSource
    [javac]        ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDatabaseMetaData.java:32: error: CassandraDatabaseMetaData is not abstract and does not override abstract method generatedKeyAlwaysReturned() in DatabaseMetaData
    [javac] class CassandraDatabaseMetaData implements DatabaseMetaData
    [javac] ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraDriver.java:40: error: CassandraDriver is not abstract and does not override abstract method getParentLogger() in Driver
    [javac] public class CassandraDriver implements Driver
    [javac]        ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraStatement.java:50: error: CassandraStatement is not abstract and does not override abstract method isCloseOnCompletion() in Statement
    [javac] class CassandraStatement extends AbstractStatement implements Statement
    [javac] ^
    [javac] /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraPreparedStatement.java:61: error: CassandraPreparedStatement is not abstract and does not override abstract method isCloseOnCompletion() in Statement
    [javac] class CassandraPreparedStatement extends CassandraStatement implements PreparedStatement
    [javac] ^
    [javac] Note: /usr/src/cassandra/drivers/java/src/org/apache/cassandra/cql/jdbc/CassandraPreparedStatement.java uses or overrides a deprecated API.
{noformat}",satishbabu,xedin,Normal,Resolved,Fixed,29/Sep/11 16:59,16/Apr/19 09:32
Bug,CASSANDRA-3278,12525224,SSLFactory should not enable cipher suites that aren't supported,"The socket creation (server or otherwise) in SSLFactory.java calls [setEnabledCipherSuites|http://download.oracle.com/javase/6/docs/api/javax/net/ssl/SSLServerSocket.html#setEnabledCipherSuites(java.lang.String\[\])] with the values specified in EncryptionOptions.java:

{code}
public String[] cipherSuites = {
    ""TLS_RSA_WITH_AES_128_CBC_SHA"", 
    ""TLS_RSA_WITH_AES_256_CBC_SHA""
};
{code}

The call to [setEnabledCipherSuites|http://download.oracle.com/javase/6/docs/api/javax/net/ssl/SSLServerSocket.html#setEnabledCipherSuites(java.lang.String\[\])] fails on systems that don't have [Java Cryptography Extension (JCE) Unlimited Strength Jurisdiction Policy Files 6|http://www.oracle.com/technetwork/java/javase/downloads/jce-6-download-429243.html] because AES256 is not supported.

To avoid installing the unlimited strength policy file the code in SSLFactory.java should call [getSupportedCipherSuites|http://download.oracle.com/javase/6/docs/api/javax/net/ssl/SSLServerSocket.html#getSupportedCipherSuites()] to find out which of the suites specified are supported.

Thanks,
George",vijay2win@yahoo.com,gcristea,Low,Resolved,Fixed,29/Sep/11 18:21,16/Apr/19 09:32
Bug,CASSANDRA-3282,12525272,CLI does not support removing compression options from a ColumnFamily,This may be an issue with ThriftValidator as well - not accepting a null or empty compression properties map as a disable flag.,xedin,zznate,Low,Resolved,Fixed,30/Sep/11 06:08,16/Apr/19 09:32
Bug,CASSANDRA-3284,12525288,help create column family refers to outdated fields,"help create column family in cassandra-cli refers to old, unsupported options.

- memtable_operations: Number of operations in millions before the memtable
  is flushed. Default is memtable_throughput / 64 * 0.3

- memtable_throughput: Maximum size in MB to let a memtable get to before
  it is flushed. Default is to use 1/16 the JVM heap size.",hsn,hsn,Low,Resolved,Fixed,30/Sep/11 10:25,16/Apr/19 09:32
Bug,CASSANDRA-3285,12525291,Bootstrap is broken in 1.0.0-rc1,"The commit of #3219 introduced two bugs: the condition to bootstrap is that there *are* non-system tables instead, a _not_ is missing, and the setToken() was wrongly push up into the ""I'm not bootstrapping"" block so a boostrapping node was left in the joining state.",slebresne,slebresne,Urgent,Resolved,Fixed,30/Sep/11 10:53,16/Apr/19 09:32
Bug,CASSANDRA-3288,12525372,CfDef can default to an invalid id and fail during system_add_column_family,"The line from this commit:
https://github.com/apache/cassandra/commit/38e3e85b121ba6308ba3ceb26312d12ed0d609ec#L1R683

Introduced an issue in that some clients, particularly Hector, will send a CfDef with an ID having been set to 0. Done via the CfDef#setId, the isSetId bit is flipped to true, causing error if schemaId of 0 already exists, which given the use case, is likely. 

Since we know the context of a system_create_column_family, this can be sidestepped by just stepping on whatever ID is there (irrelevant on a create anyway) with the value returned from: Schema.instance.nextCFId()",jbellis,zznate,Urgent,Resolved,Fixed,30/Sep/11 22:12,16/Apr/19 09:32
Bug,CASSANDRA-3289,12525378,assert err on ArrayBackedSortedColumns.addColumn(ArrayBackedSortedColumns.java:126),"I have the following in trunk:

RowKey: b
=> (column=a, value=38383838383838383838, timestamp=1317421952793000)
=> (column=d, value=617364646661736466, timestamp=1317420968944000)
=> (column=e, value=38383838383838383838, timestamp=1317421096152000)
=> (column=f, value=33343334333433343334, timestamp=1317422838818000)
=> (column=g, value=33343334333433343334, timestamp=1317422565130000)
=> (column=i, value=33343334333433343334, timestamp=1317422879258000)
=> (column=j, value=33343334333433343334, timestamp=1317422814873000)
=> (column=o, value=33343334333433343334, timestamp=1317422867106000)
=> (column=x, value=33343334333433343334, timestamp=1317422394097000)
=> (column=z, value=38383838383838383838, timestamp=1317421982057000)

Keyspace: testks:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [168:1]
  Column Families:
    ColumnFamily: testcf
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Key cache size / save period in seconds: 200000.0/14400
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []
      Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy

every thing is flushed to the sstables, but not in the same sstables, and the columns are in some what 'random' form:

/var/lib/cassandra/data/testks/testcf-h-10-Data.db
{
""61"": [[""0c"",""76"",1317405903119000], [""0d"",""76"",1317405977002000], [""7a"",""38383838383838383838"",1317422276322000]],
""62"": [[""61"",""38383838383838383838"",1317421952793000], [""63"",""4e864303"",1317421827329000,""d""], [""64"",""617364646661736466"",1317420968944000], [""65"",""38383838383838383838"",1317421096152000], [""67"",""33343334333433343334"",1317422565130000], [""78"",""33343334333433343334"",1317422394097000], [""7a"",""38383838383838383838"",1317421982057000]]
}
/var/lib/cassandra/data/testks/testcf-h-12-Data.db
{
""62"": [[""6a"",""33343334333433343334"",1317422814873000]]
}
/var/lib/cassandra/data/testks/testcf-h-13-Data.db
{
""62"": [[""66"",""33343334333433343334"",1317422838818000]]
}
/var/lib/cassandra/data/testks/testcf-h-14-Data.db
{
""62"": [[""6f"",""33343334333433343334"",1317422867106000]]
}
/var/lib/cassandra/data/testks/testcf-h-15-Data.db
{
""62"": [[""69"",""33343334333433343334"",1317422879258000]]
}


then i basically make a call to get key=b with all the column names (yes included column names that didn't exist to save time):

ColumnFamilyResult<String, String> queryColumns = template.queryColumns(""b"", Arrays.asList(""a"",""b"",""c"",""d"",""e"",""f"",""g"",""h"",""i"",""j"",""k"",""l"",""m"",""n"",""o"",""p"",""q"",""r"",""s"",""t"",""u"",""v"",""w"",""x"",""y"",""z""));

(let me know if it would be easier to just upload the sstables to the ticket)",jbellis,cywjackson,Urgent,Resolved,Fixed,30/Sep/11 23:06,16/Apr/19 09:32
Bug,CASSANDRA-3291,12525391,1.0 needs to clean out old-style hints,(Only marking this Minor because the manual workaround of deleting hint files is trivial.),jbellis,jbellis,Low,Resolved,Fixed,01/Oct/11 02:28,16/Apr/19 09:32
Bug,CASSANDRA-3292,12525396,creating column family sets durable_writes to true,"[default@rapidshare] describe keyspace rapidshare;
Keyspace: rapidshare:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: *false*
    Options: [datacenter1:1]
  Column Families:
[default@rapidshare] create column family t1;
1ba19300-ebfa-11e0-0000-34912694d0bf
Waiting for schema agreement...
... schemas agree across the cluster
[default@rapidshare] describe keyspace rapidshare;
Keyspace: rapidshare:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: *true*
    Options: [datacenter1:1]
  Column Families:
    ColumnFamily: t1
      Key Validation Class: org.apache.cassandra.db.marshal.BytesType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.BytesType
      Row cache size / save period in seconds: 0.0/0
      Key cache size / save period in seconds: 200000.0/14400
      Memtable thresholds: 0.028124999999999997/1440/6 (millions of ops/minutes/MB)
      GC grace seconds: 864000
      Compaction min/max thresholds: 4/32
      Read repair chance: 1.0
      Replicate on write: true
      Built indexes: []",jbellis,hsn,Low,Resolved,Fixed,01/Oct/11 07:00,16/Apr/19 09:32
Bug,CASSANDRA-3293,12525436,Metered Flusher log message confusing," INFO [NonPeriodicTasks:1] 2011-10-01 17:34:32,652 MeteredFlusher.java (line 62) flushing high-traffic column family ColumnFamilyStore(+table='rapidshare',+ columnFamily='resultcache')

instead of table it should be *keyspace=*",,hsn,Low,Resolved,Fixed,01/Oct/11 15:41,16/Apr/19 09:32
Bug,CASSANDRA-3296,12525512,CsDef instead of CfDef in system_add_keyspace() function,"throw new InvalidRequestException(""CsDef ("" + cf.getName() +"") had a keyspace definition that did not match KsDef"");

The string starts with ""CsDef ("" when it should be ""CfDef ("".",,alexiswilke,Low,Resolved,Fixed,03/Oct/11 06:06,16/Apr/19 09:32
Bug,CASSANDRA-3297,12525557,truncate can still result in data being replayed after a restart,Our first stab at fixing this was CASSANDRA-2950.,jbellis,jbellis,Normal,Resolved,Fixed,03/Oct/11 18:02,16/Apr/19 09:32
Bug,CASSANDRA-3298,12525564,CompressedRandomAccessReaderTest fails on Windows,"    [junit] Testcase: testResetAndTruncate(org.apache.cassandra.io.compress.CompressedRandomAccessReaderTest):      FAILED
    [junit] expected:<43> but was:<49>
    [junit] junit.framework.AssertionFailedError: expected:<43> but was:<49>
    [junit]     at org.apache.cassandra.io.compress.CompressedRandomAccessReaderTest.testResetAndTruncate(CompressedRandomAccessReaderTest.java:81)
    [junit]     at org.apache.cassandra.io.compress.CompressedRandomAccessReaderTest.testResetAndTruncate(CompressedRandomAccessReaderTest.java:39)",xedin,jbellis,Normal,Resolved,Fixed,03/Oct/11 18:44,16/Apr/19 09:32
Bug,CASSANDRA-3299,12525590,clientutil depends on FBUtilities (bad),"clientutils' (indirect )dependency on FBUtilities (needed for tests) would result in huge numbers of classes being pulled in transitively.

The attached patch moves the {{bytesToHex}} and {{hexToBytes}} methods into a new class ({{o.a.c.utils.Hex}}), which has no external dependencies.

This should be pretty safe, but I've marked it fixfor-1.0.1 since we're so close to release, and because the JDBC driver can embed a snapshot jar in the meantime.",urandom,urandom,Normal,Resolved,Fixed,03/Oct/11 21:33,16/Apr/19 09:32
Bug,CASSANDRA-3301,12525608,Java Stress Tool:  COUNTER_GET reads from CounterSuper1 instead of SuperCounter1,"Output from stress tool - COUNTER_ADD works fine bug COUNTER_GET does not
{code}
./stress --operation=COUNTER_ADD --family-type=Super --num-keys=1 --consistency-level=TWO --replication-factor=3 --nodes=cathy1
Unable to create stress keyspace: Keyspace already exists.
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
1,0,0,0.0060,0
END


./stress --operation=COUNTER_GET --family-type=Super --num-keys=1 --consistency-level=QUORUM --nodes=cathy1
total,interval_op_rate,interval_key_rate,avg_latency,elapsed_time
Operation [0] retried 10 times - error reading counter key 0 ((InvalidRequestException): unconfigured columnfamily CounterSuper1)

0,0,0,NaN,0
END
{code}

The CF created is called *SuperCounter1* and not *CounterSuper1*
{code}
 INFO 00:34:21,344 ColumnFamilyStore(table='Keyspace1', columnFamily='SuperCounter1') liveRatio is 9.167798032786886 (just-counted was 9.167798032786886).  calculation took 1281ms for 9883 columns
{code}

",slebresne,cdaw,Normal,Resolved,Fixed,04/Oct/11 00:39,16/Apr/19 09:32
Bug,CASSANDRA-3302,12525624,stop Cassandra result in hang,"testing this under trunk via a hacked package (replacing jars from 0.8.6 deb installation)

When calling service cassandra stop, the Cassandra process hang:

http://aep.appspot.com/display/i6aIUCkt4kz0HG5l2VszMM7QvLo/

The following logs is observed in the C* log:

 INFO [main] 2011-10-03 23:20:46,434 AbstractCassandraDaemon.java (line 270) Cassandra shutting down...
 INFO [main] 2011-10-03 23:20:46,434 CassandraDaemon.java (line 218) Stop listening to thrift clients

Re-run this using 1.0.0 branch, (following the same ""hack"" procedure), C* stop properly, and the following is observed in the log:

 INFO [main] 2011-10-04 05:02:08,048 AbstractCassandraDaemon.java (line 270) Cassandra shutting down...
 INFO [main] 2011-10-04 05:02:08,049 CassandraDaemon.java (line 218) Stop listening to thrift clients
 INFO [Thread-2] 2011-10-04 05:02:08,318 MessagingService.java (line 482) Shutting down MessageService...
 INFO [Thread-2] 2011-10-04 05:02:08,319 MessagingService.java (line 497) Waiting for in-progress requests to complete
 INFO [ACCEPT-/10.83.77.171] 2011-10-04 05:02:08,319 MessagingService.java (line 637) MessagingService shutting down server thread.


could this be related to CASSANDRA-3261 ?",slebresne,cywjackson,Normal,Resolved,Fixed,04/Oct/11 05:08,16/Apr/19 09:32
Bug,CASSANDRA-3303,12525636,Short reads protection results in returning more columns than asked for,"When we detect a short read (in SP.fetchRows), we retry a new command created by:
{noformat}
logger.debug(""detected short read: expected {} columns, but only resolved {} columns"", sliceCommand.count, liveColumnsInRow);
int retryCount = sliceCommand.count + sliceCommand.count - liveColumnsInRow;
SliceFromReadCommand retryCommand = new SliceFromReadCommand(command.table,
                                                             command.key,
                                                             command.queryPath,
                                                             sliceCommand.start,
                                                             sliceCommand.finish,
                                                             sliceCommand.reversed,
                                                             retryCount);
{noformat}
That is, in that new command, the count is greater than what asked in the initial command. But we never cut back the result of that new retried query.",byronclark,slebresne,Low,Resolved,Fixed,04/Oct/11 09:04,16/Apr/19 09:32
Bug,CASSANDRA-3304,12525659,Missing fields in show schema output,"if you compare output of these 2 commands:

*show keyspaces*
Keyspace: test:
  Replication Strategy: org.apache.cassandra.locator.SimpleStrategy
  Durable Writes: true
    Options: [replication_factor:1]
  Column Families:
    ColumnFamily: sipdb
    ""phone calls routing information""
      Key Validation Class: org.apache.cassandra.db.marshal.IntegerType
      Default column value validator: org.apache.cassandra.db.marshal.BytesType
      Columns sorted by: org.apache.cassandra.db.marshal.AsciiType
      Row cache size / save period in seconds / keys to save : 0.0/0/all
      Key cache size / save period in seconds: 0.0/0
      GC grace seconds: 0
      Compaction min/max thresholds: 4/32
      Read repair chance: 0.0
      Replicate on write: false
      Built indexes: []
      Column Metadata:
        Column Name: kam
          Validation Class: org.apache.cassandra.db.marshal.AsciiType
      *Compaction Strategy: org.apache.cassandra.db.compaction.SizeTieredCompacti*

*show schema*
create column family sipdb
  with column_type = 'Standard'
  and comparator = 'AsciiType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'IntegerType'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and keys_cached = 0.0
  and key_cache_save_period = 0
  and read_repair_chance = 0.0
  and gc_grace = 0
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = false
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider'
  and comment = 'phone calls routing information'
  and column_metadata = [
    {column_name : 'kam',
    validation_class : AsciiType}];

You will discover that show schema is missing: 1. compaction strategy. 2. how many keys to save",xedin,hsn,Low,Resolved,Fixed,04/Oct/11 12:52,16/Apr/19 09:32
Bug,CASSANDRA-3306,12525677,Failed streaming may cause duplicate SSTable reference,"during stress testing, i always get this error making leveledcompaction strategy unusable. Should be easy to reproduce - just write fast.

ERROR [CompactionExecutor:6] 2011-10-04 15:48:52,179 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[CompactionExecutor:6,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.DataTracker$View.newSSTables(DataTracker.java:580)
	at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:546)
	at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:268)
	at org.apache.cassandra.db.DataTracker.replaceCompactedSSTables(DataTracker.java:232)
	at org.apache.cassandra.db.ColumnFamilyStore.replaceCompactedSSTables(ColumnFamilyStore.java:960)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:199)
	at org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:47)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:131)
	at org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:114)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)

and this is in json data for table:

{
  ""generations"" : [ {
    ""generation"" : 0,
    ""members"" : [ 459, 460, 461, 462, 463, 464, 465, 466, 467, 468, 469, 470, 471, 472, 473, 474, 475, 476, 477, 478, 479, 480, 481, 482, 483, 484 ]
  }, {
    ""generation"" : 1,
    ""members"" : [ ]
  }, {
    ""generation"" : 2,
    ""members"" : [ ]
  }, {
    ""generation"" : 3,
    ""members"" : [ ]
  }, {
    ""generation"" : 4,
    ""members"" : [ ]
  }, {
    ""generation"" : 5,
    ""members"" : [ ]
  }, {
    ""generation"" : 6,
    ""members"" : [ ]
  }, {
    ""generation"" : 7,
    ""members"" : [ ]
  } ]
}",yukim,hsn,Normal,Resolved,Fixed,04/Oct/11 14:39,16/Apr/19 09:32
Bug,CASSANDRA-3309,12525728,Nodetool Doesnt close the open JMX connection causing it to leak Threads,"When nodetool is used intensively we will see 1000's of ""JMX server connection timeout""

Fix is to close the connections when no longer needed.",vijay2win@yahoo.com,vijay2win@yahoo.com,Normal,Resolved,Fixed,04/Oct/11 20:22,16/Apr/19 09:32
Bug,CASSANDRA-3310,12525733,Update lib/jamm-0.2.5 to the version deployed to Maven central,"https://oss.sonatype.org/content/groups/public/com/github/stephenc/jamm/0.2.5/jamm-0.2.5.jar is the version that is being synced to Maven central

MD5: 79f6bf54227abd15d4277ff0fe7038cb",,stephenc,Low,Resolved,Fixed,04/Oct/11 21:08,16/Apr/19 09:32
Bug,CASSANDRA-3311,12525734,"cqlsh: Error running ""select *"" vs ""select all columns""","* Install cql 1.0.5 from [http://code.google.com/a/apache-extras.org/p/cassandra-dbapi2/downloads/detail?name=cql-1.0.5.tar.gz&can=2&q=]

* Query using ""select *""
{code}
cqlsh> select * from users;
Exception: 'utf8' codec can't decode byte 0xb4 in position 7: unexpected code byte
{code}

* Query selecting all columns
{code}
 select KEY, password, gender, session_token, state, birth_year from users;
   KEY |  password | gender | session_token | state | birth_year |
 user1 | ch@ngem3a |      f |          None |    TX |       1968 |
{code}

* Test Data
{code}
CREATE KEYSPACE ks1 with 
  strategy_class =  
    'org.apache.cassandra.locator.SimpleStrategy' 
  and strategy_options:replication_factor=1;
  
use ks1;

CREATE COLUMNFAMILY users (
  KEY varchar PRIMARY KEY, password varchar, gender varchar,
  session_token varchar, state varchar, birth_year bigint);
  
INSERT INTO users (KEY, password, gender, state, birth_year) VALUES ('user1', 'ch@ngem3a', 'f', 'TX', '1968');    
{code}",xedin,cdaw,Normal,Resolved,Fixed,04/Oct/11 21:08,16/Apr/19 09:32
Bug,CASSANDRA-3312,12525758,need initClause when catch Exception and throw new Exception in cli,"through CASSANDRA-2746 , we added initCause to the Cli such that we could see more meaningful exception stacktrace when certain exception is thrown.

However, there are still some other area, eg:

executeGetWithConditions(Tree)
executeSet(Tree)
executeIncr(Tree, long)
etc etc...

basically any time you do a
{code}
            {
                throw new RuntimeException(e.getMessage());
            }
{code}

the real exception is lost. The right approach should be:

{code}
        catch (Exception e)
        {
            throw new RuntimeException(e.getMessage(), e);
        }
{code}

eg: i was getting this:
null
java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:310)
        at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:217)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:345)
Caused by: java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeGetWithConditions(CliClient.java:815)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:208)
        ... 2 more


but i have no idea what the problem is with just the above stack trace.

with the fix, this would tell us more:
null
java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:310)
        at org.apache.cassandra.cli.CliMain.processStatement(CliMain.java:217)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:345)
Caused by: java.lang.RuntimeException
        at org.apache.cassandra.cli.CliClient.executeGetWithConditions(CliClient.java:815)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:208)
        ... 2 more
Caused by: UnavailableException()
        at org.apache.cassandra.thrift.Cassandra$get_indexed_slices_result.read(Cassandra.java:14065)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_get_indexed_slices(Cassandra.java:810)
        at org.apache.cassandra.thrift.Cassandra$Client.get_indexed_slices(Cassandra.java:782)
        at org.apache.cassandra.cli.CliClient.executeGetWithConditions(CliClient.java:806)
        ... 3 more",satishbabu,cywjackson,Low,Resolved,Fixed,05/Oct/11 02:22,16/Apr/19 09:32
Bug,CASSANDRA-3313,12525766,Cancelling index build throws assert error,"Canceling index build throws this, but checking log there was no compaction running in background.

INFO 08:46:41,343 Writing Memtable-IndexInfo@9480253(34/42 serialized/live byte
s, 1 ops)
ERROR 08:46:41,343 Fatal exception in thread Thread[CompactionExecutor:3,5,main]

java.lang.AssertionError
        at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates
(SecondaryIndexManager.java:397)
        at org.apache.cassandra.db.Table.indexRow(Table.java:534)
        at org.apache.cassandra.db.index.SecondaryIndexBuilder.build(SecondaryIn
dexBuilder.java:64)
        at org.apache.cassandra.db.compaction.CompactionManager$7.run(Compaction
Manager.java:856)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:662)
 INFO 08:46:41,531 Completed flushing \var\lib\cassandra\data\system\IndexInfo-h
-1-Data.db (88 bytes)",jbellis,hsn,Low,Resolved,Fixed,05/Oct/11 06:58,16/Apr/19 09:32
Bug,CASSANDRA-3314,12525770,Fail to delete -Index files if index is currently building,"If there is index building in progress, following errors are thrown if cassandra is trying to delete *-Index.db files. There is no problem with deleting -Data or -Filter.. files. CF is using leveled compaction but it is probably not related.

ERROR [NonPeriodicTasks:1] 2011-10-05 09:13:03,702 AbstractCassandraDaemon.java
(line 133) Fatal exception in thread Thread[NonPeriodicTasks:1,5,main]
java.lang.RuntimeException: java.io.IOException: Failed to delete C:\var\lib\cas
sandra\data\test\sipdb-h-772-Index.db
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:3
4)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:44
1)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.
access$301(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.
run(ScheduledThreadPoolExecutor.java:206)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExec
utor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor
.java:908)
        at java.lang.Thread.run(Thread.java:662)",jbellis,hsn,Low,Resolved,Fixed,05/Oct/11 07:31,16/Apr/19 09:32
Bug,CASSANDRA-3318,12525837,Unable to delete after running scrub,"Another problem with sstable deletions on 1.0. Running scrub produces lot of unable to delete messages on windows.

ERROR 16:16:37,562 Unable to delete \var\lib\cassandra\data\test\sipdb-h-711-Dat
a.db (it will be removed on server restart; we'll also retry after GC)
 INFO 16:16:37,577 Scrub of SSTableReader(path='\var\lib\cassandra\data\test\sip
db-h-711-Data.db') complete: 48396 rows in new sstable and 0 empty (tombstoned)
rows dropped",jbellis,hsn,Low,Resolved,Fixed,05/Oct/11 14:21,16/Apr/19 09:32
Bug,CASSANDRA-3320,12525954,pig_cassandra script errors when running against pig 0.9.1 tar ball because there are multiple jars.,"The pig_cassandra script in contrib/pig/bin assumes there is only one pig jar file in $PIG_HOME.  However, the latest release of pig 0.9.1 has two jar files: one for hadoop and one without hadoop.  See below:

bone@zen:~/tools/pig-0.9.1-> ls -al *.jar
-rw-r--r--  1 bone  staff   5130595 Sep 29 18:55 pig-0.9.1-withouthadoop.jar
-rw-r--r--  1 bone  staff  12430153 Sep 29 18:55 pig-0.9.1.jar


This breaks the shell script with:
bin/pig_cassandra: line 42: [: /Users/bone/tools/pig/pig-0.9.1-withouthadoop.jar: binary operator expected
Unrecognized option: -x

Attached is a patch for the shell script that takes the last jar file listed in the directory. This fixes the problem.  I also add an ""echo"" to notify the user which jar file they are using. 
",boneill,boneill,Low,Resolved,Fixed,05/Oct/11 20:07,16/Apr/19 09:32
Bug,CASSANDRA-3325,12526023,Compaction degrades key cache stats,"When ""compaction_preheat_key_cache"" is set to true, then during compaction, it keep tracks of cached keys to to re-cache their new position.
It does this by calling  the following method on every key of the compacted sstable :
sstable.getCachedPosition(row.key)
which also update cache stats, thus lowering hit rate

Below is an attached patch allowing to know if the key is cached, but without updating the stats.",frousseau,frousseau,Low,Resolved,Fixed,06/Oct/11 10:29,16/Apr/19 09:32
Bug,CASSANDRA-3327,12526104,Support TimeUUID in CassandraStorage,"Cassandra CLI:

{code}
grunt> raw = LOAD 'cassandra://TEST/CF'
>>     USING CassandraStorage()
>>     AS (
>>         key:chararray,
>>         columns:bag {
>>             column:tuple(
>>                 name,
>>                 value
>>             )
>>         });

grunt> describe raw;
raw: {key: chararray,columns: {(name: bytearray,value: bytearray)}}

log_test =
    FOREACH raw
    GENERATE
        (CHARARRAY) key,
        flatten(columns);

grunt> DUMP log_test;
{code}

Returns:

{code}
org.apache.pig.impl.logicalLayer.FrontendException: ERROR 1066: Unable to open iterator for alias log_test. Backend error : Unexpected data type java.util.UUID found in stream. Note only standard Pig type is supported when you output from UDF/LoadFunc
        at org.apache.pig.PigServer.openIterator(PigServer.java:890)
        at org.apache.pig.tools.grunt.GruntParser.processDump(GruntParser.java:655)
        at org.apache.pig.tools.pigscript.parser.PigScriptParser.parse(PigScriptParser.java:303)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:188)
        at org.apache.pig.tools.grunt.GruntParser.parseStopOnError(GruntParser.java:164)
        at org.apache.pig.tools.grunt.Grunt.run(Grunt.java:67)
        at org.apache.pig.Main.run(Main.java:487)
        at org.apache.pig.Main.main(Main.java:108)
Caused by: java.lang.RuntimeException: Unexpected data type java.util.UUID found in stream. Note only standard Pig type is supported when you output from UDF/LoadFunc
        at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:478)
        at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:542)
        at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
        at org.apache.pig.impl.io.InterRecordWriter.write(InterRecordWriter.java:73)
        at org.apache.pig.impl.io.InterStorage.putNext(InterStorage.java:87)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:138)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigOutputFormat$PigRecordWriter.write(PigOutputFormat.java:97)
        at org.apache.hadoop.mapred.MapTask$NewDirectOutputCollector.write(MapTask.java:498)
        at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapOnly$Map.collect(PigMapOnly.java:48)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:263)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:256)
        at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:58)
        at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
        at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:621)
        at org.apache.hadoop.mapred.MapTask.run(MapTask.java:305)
{code}

According to driftx on IRC the setTupleValue function in CassandraStorage needs to handle the uuid case and cast it to a DataByteArray.",brandon.williams,140am,Normal,Resolved,Fixed,06/Oct/11 19:41,16/Apr/19 09:32
Bug,CASSANDRA-3331,12526225,Apache Daemon missing from the binary tarball,"Apparently the tools used to run the binary release are missing from the binary tarball.

I will verify that they are in the 1.0 branch, then update the ticket so we can ensure that they are included.

Ben",bcoverston,bcoverston,Normal,Resolved,Fixed,07/Oct/11 15:25,16/Apr/19 09:32
Bug,CASSANDRA-3334,12526275,dropping index causes some inflight mutations to fail,"dropping index causes some inflight mutations to fail. hector on client side didnt throw any exception

 INFO [MigrationStage:1] 2011-10-07 23:11:53,742 Migration.java (line 119) Applying migration fb1a8540-f128-11e0-0000-23b38323f4da Update column family to org.apache.cassandra.config.CFMetaData@786669[cfId=1000,ksName=test,cfName=sipdb,cfType=Standard,comparator=org.apache.cassandra.db.marshal.AsciiType,subcolumncomparator=<null>,comment=phone calls routing information,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=0.0,replicateOnWrite=false,gcGraceSeconds=0,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.Int32Type,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@8bb33c,mergeShardsChance=0.1,keyAlias=java.nio.HeapByteBuffer[pos=461 lim=464 cap=466],column_metadata={java.nio.HeapByteBuffer[pos=0 lim=3 cap=3]=ColumnDefinition{name=6b616d, validator=org.apache.cassandra.db.marshal.AsciiType, index_type=null, index_name='null'}},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO [MigrationStage:1] 2011-10-07 23:11:53,805 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Migrations@27537043(7860/9825 serialized/live bytes, 1 ops)
 INFO [MigrationStage:1] 2011-10-07 23:11:53,820 ColumnFamilyStore.java (line 664) Enqueuing flush of Memtable-Schema@8340427(3320/4150 serialized/live bytes, 3 ops)
 INFO [FlushWriter:3] 2011-10-07 23:11:53,820 Memtable.java (line 237) Writing Memtable-Migrations@27537043(7860/9825 serialized/live bytes, 1 ops)
 INFO [FlushWriter:3] 2011-10-07 23:11:55,008 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Migrations-h-14-Data.db (7924 bytes)
 INFO [FlushWriter:3] 2011-10-07 23:11:55,008 Memtable.java (line 237) Writing Memtable-Schema@8340427(3320/4150 serialized/live bytes, 3 ops)
 INFO [CompactionExecutor:4] 2011-10-07 23:11:55,008 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-13-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-14-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-11-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Migrations-h-12-Data.db')]
 INFO [FlushWriter:3] 2011-10-07 23:11:56,430 Memtable.java (line 273) Completed flushing \var\lib\cassandra\data\system\Schema-h-14-Data.db (3470 bytes)
 INFO [CompactionExecutor:3] 2011-10-07 23:11:56,446 CompactionTask.java (line 119) Compacting [SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-13-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-14-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-12-Data.db'), SSTableReader(path='\var\lib\cassandra\data\system\Schema-h-11-Data.db')]
ERROR [MutationStage:56] 2011-10-07 23:11:56,508 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:56,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:51] 2011-10-07 23:11:56,539 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:51,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:38] 2011-10-07 23:11:56,633 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:38,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
ERROR [MutationStage:57] 2011-10-07 23:11:56,664 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:57,5,main]
java.lang.AssertionError
	at org.apache.cassandra.db.index.SecondaryIndexManager.applyIndexUpdates(SecondaryIndexManager.java:369)
	at org.apache.cassandra.db.Table.apply(Table.java:457)
	at org.apache.cassandra.db.RowMutation.apply(RowMutation.java:253)
	at org.apache.cassandra.service.StorageProxy$5.runMayThrow(StorageProxy.java:436)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1263)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",jbellis,hsn,Low,Resolved,Fixed,07/Oct/11 21:20,16/Apr/19 09:32
Bug,CASSANDRA-3335,12526280,ThreadPoolExecutor creates threads as non-daemon and will block on shutdown by default,"This is most obviously visible in OptionalTasks which should not block shutdown, but often does.",jbellis,brandon.williams,Low,Resolved,Fixed,07/Oct/11 21:40,16/Apr/19 09:32
Bug,CASSANDRA-3338,12526414,Uncompressed sizes are used to estimate space for compaction of compressed sstables,We are using the uncompressed data size when estimating if we have enough to compact sstables. This means we can easily refuse compaction when there is clearly enough room to compact.,slebresne,slebresne,Normal,Resolved,Fixed,10/Oct/11 08:30,16/Apr/19 09:32
Bug,CASSANDRA-3341,12526465,Fix for Build Error in contrib/pig to accomodate refactoring of hexToBytes,"hexToBytes moved to Hex from FBUtilities.
Need to update contrib/pig to accomodate that move.",,boneill,Low,Resolved,Fixed,10/Oct/11 16:25,16/Apr/19 09:32
Bug,CASSANDRA-3342,12526542,cassandra-cli allows setting min_compaction_threshold to 1,"{{
[root@Apture] update column family MagicLinks with min_compaction_threshold=1 and max_compaction_threshold=20;
b98e3b80-f3a3-11e0-0000-76abb4a6dbbf
Waiting for schema agreement...
... schemas agree across the cluster
}}

I'm told that a min_compaction_threshold of 1 is nonsensical.  I had a spell where my servers stopped doing compactions.  Once I upped the min_compaction_threshold, they started compacting again.  I'm unable to confirm for sure that this was the case.",xedin,amnorvend,Normal,Resolved,Fixed,11/Oct/11 01:01,16/Apr/19 09:32
Bug,CASSANDRA-3343,12526553,nodetool printing classpath,"* Get file from: [https://repository.apache.org/content/repositories/orgapachecassandra-046/org/apache/cassandra/apache-cassandra/1.0.0/apache-cassandra-1.0.0-bin.tar.gz]
* Install C* and start server
* Run: nodetool -h localhost ring

{code}
Cathy-Daws-MacBook-Pro:bin cathy$ ./nodetool -h localhost ring

./../conf:./../build/classes/main:./../build/classes/thrift:./../lib/antlr-3.2.jar:./../lib/apache-cassandra-1.0.0.jar:./../lib/apache-cassandra-clientutil-1.0.0.jar:./../lib/apache-cassandra-thrift-1.0.0.jar:./../lib/avro-1.4.0-fixes.jar:./../lib/avro-1.4.0-sources-fixes.jar:./../lib/commons-cli-1.1.jar:./../lib/commons-codec-1.2.jar:./../lib/commons-lang-2.4.jar:./../lib/compress-lzf-0.8.4.jar:./../lib/concurrentlinkedhashmap-lru-1.2.jar:./../lib/guava-r08.jar:./../lib/high-scale-lib-1.1.2.jar:./../lib/jackson-core-asl-1.4.0.jar:./../lib/jackson-mapper-asl-1.4.0.jar:./../lib/jamm-0.2.5.jar:./../lib/jline-0.9.94.jar:./../lib/json-simple-1.1.jar:./../lib/libthrift-0.6.jar:./../lib/log4j-1.2.16.jar:./../lib/servlet-api-2.5-20081211.jar:./../lib/slf4j-api-1.6.1.jar:./../lib/slf4j-log4j12-1.6.1.jar:./../lib/snakeyaml-1.6.jar:./../lib/snappy-java-1.0.3.jar
Address         DC          Rack        Status State   Load            Owns    Token                                       
127.0.0.1       datacenter1 rack1       Up     Normal  8.91 KB         100.00% 10597065753338857570408052040129979696      
{code}",slebresne,cdaw,Low,Resolved,Fixed,11/Oct/11 03:31,16/Apr/19 09:32
Bug,CASSANDRA-3344,12526640,Compaction throttling can be too slow,"Compaction throttling needs to know how many active compactions are running (to divide bandwith for each active compaction).

The way active compaction is counted can be broken because it counts the number of active threads in the executor BUT the thread starts by acquiring a lock.
If the lock can't be acquired immediately : the thread is seen as ""active"" but does not participate in IO operations.
The case can happen when major compaction are triggered (major compaction acquire a write lock, while minor compactions acquire a read lock).

Having compaction througput to 16Mb/s, we observed is the following  (two times) :
 - only 1 active compaction (a long one for a few hours) starting at 16Mb/s, then after some time running at 2Mb/s, thus taking a very long time to complete
 - many pending compactions

Using JMX and monitoring the stack trace of the compaction threads showed that :
 - 1 thread was effectively compacting
 - 1 thread was waiting to acquire the write lock (due to a major compaction)
 - 6 threads were waiting to acquire the read lock (probably due to the thread above trying to acquire the write lock)

Attached is a proposed patch (very simple, not yet tested) which counts only active compactions.

",slebresne,frousseau,Low,Resolved,Fixed,11/Oct/11 10:29,16/Apr/19 09:32
Bug,CASSANDRA-3345,12526654,Repair still streams unnecessary sstables,"Through rebases, CASSANDRA-2610 unfortunately got committed with the use of the wrong streaming method, the one that stream all the sstables of the keyspace.",slebresne,slebresne,Low,Resolved,Fixed,11/Oct/11 13:07,16/Apr/19 09:32
Bug,CASSANDRA-3346,12526665,HsHa broken at startup,"{noformat}

ERROR 09:10:21,781 Exception encountered during startup
java.lang.IllegalArgumentException
        at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:589)
        at java.util.concurrent.ThreadPoolExecutor.<init>(ThreadPoolExecutor.java:514)
        at org.apache.cassandra.concurrent.DebuggableThreadPoolExecutor.<init>(DebuggableThreadPoolExecutor.java:90)
        at org.apache.cassandra.concurrent.JMXEnabledThreadPoolExecutor.<init>(JMXEnabledThreadPoolExecutor.java:76)
        at org.apache.cassandra.thrift.CassandraDaemon$ThriftServer.<init>(CassandraDaemon.java:192)
        at org.apache.cassandra.thrift.CassandraDaemon.startServer(CassandraDaemon.java:75)
        at org.apache.cassandra.service.AbstractCassandraDaemon.startRPCServer(AbstractCassandraDaemon.java:281)
        at org.apache.cassandra.service.AbstractCassandraDaemon.start(AbstractCassandraDaemon.java:253)
        at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:350)
        at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
{noformat}",brandon.williams,brandon.williams,Normal,Resolved,Fixed,11/Oct/11 14:12,16/Apr/19 09:32
Bug,CASSANDRA-3349,12526757,NPE on malformed CQL,"It's not clear why, but the CQL grammar specification in Cql.g allows for an empty WHERE clause on DELETE, i.e.:

{noformat}
DELETE FROM someCF WHERE;
{noformat}

When this is used, with or without a column list, it causes an NPE on the node processing the CQL. Traceback on a recent 1.0.0 build:

{noformat}
ERROR [pool-2-thread-1] 2011-10-11 15:45:25,655 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.NullPointerException
        at org.apache.cassandra.cql.CqlParser.deleteStatement(CqlParser.java:1994)
        at org.apache.cassandra.cql.CqlParser.query(CqlParser.java:292)
        at org.apache.cassandra.cql.QueryProcessor.getStatement(QueryProcessor.java:984)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:500)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1268)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:680)
{noformat}

The CQL client gets an error with the message, ""Internal application error"".

It might be better to allow leaving off the ""WHERE"" as well as the condition, to match SQL semantics, although fixing that probably won't solve this problem.",xedin,thepaul,Normal,Resolved,Fixed,11/Oct/11 21:17,16/Apr/19 09:32
Bug,CASSANDRA-3350,12526768,Can't USE numeric keyspace names in CQL,"Cassandra allows keyspace names to start with a digit or an underscore (see o.a.c.db.migration.Migration.isLegalName), but CQL's {{USE}} statement only accepts a CQL identifier, which must start with a letter. So there's no way to use a keyspace named ""142"" or ""\_hi\_"" in CQL, for example.

The {{USE}} statement should accept string literals and integers as well as identifiers, and CQL identifiers ({{IDENT}}) should probably allow starting with the underscore.",xedin,thepaul,Low,Resolved,Fixed,11/Oct/11 22:10,16/Apr/19 09:32
Bug,CASSANDRA-3351,12526780,If node fails to join a ring it will stay in joining state indefinately,"While attempting to add a new node to my ring something went wrong and I had to terminate the node on ec2. After this the node keeps appearing in the ring command in ""joining"" state and never goes away. Per driftx on the Cassandra channel if I do a whole cluster restart it should go away, but since this is a production system this is not really possible. Additionally if I could join a node with same IP again this should go away, but being on ec2 this is not always easy. So not sure if this truly qualifies as a bug or more like a feature request, but I feel there should be a way to remove a node in any state if I wish without joining a node with same ip or doing a whole cluster restart.",brandon.williams,tuke,Low,Resolved,Fixed,12/Oct/11 00:34,16/Apr/19 09:32
Bug,CASSANDRA-3353,12526923,misc CQL doc fixes,See patch to follow for some minor documentation fixes (mostly formatting nits).,urandom,urandom,Low,Resolved,Fixed,12/Oct/11 20:16,16/Apr/19 09:32
Bug,CASSANDRA-3356,12526977,Add more data type checks when storing data from Pig to Cassandra,"Pre-CASSANDRA-2810, the decompose method was tried before assuming the data was of type DataByteArray.  It needs to have both - first a check to see if it can be decomposed.  If that doesn't work (exception) it can try the cast.  If that doesn't work, then we can't store the object - or as Brandon says, we need to put another special case in ObjToBB.  This is all based on a conversation in IRC with Brandon Williams, Jacob Perkins, and Jeremy Hanna about a problem arising when trying to store an object of type long to Cassandra.",,jeromatron,Low,Resolved,Fixed,13/Oct/11 05:21,16/Apr/19 09:32
Bug,CASSANDRA-3357,12527002,SSTableImport/Export don't handle tombstone well if value validation != BytesType,"SSTableImport/Export use the value validator even on tomstone, but for those the value is the local deletion time, so this don't necessarily validate (with UTF8Type for instance)",slebresne,slebresne,Low,Resolved,Fixed,13/Oct/11 10:46,16/Apr/19 09:32
Bug,CASSANDRA-3358,12527029,2GB row size limit in ColumnIndex offset calculation,"Index offset is calculated using int instead of long resulting in overflow at 2GB row size. As a result affected columns can not be retrieved. 

Fix: use long instead of int",tho,tho,Normal,Resolved,Fixed,13/Oct/11 14:36,16/Apr/19 09:32
Bug,CASSANDRA-3364,12527307,"[patch] use long math, if you want long results","Code calculates long values, using integer intermediate input, which can cause truncation errors, safer just to use long input.",,dbrosius@apache.org,Low,Resolved,Fixed,15/Oct/11 17:31,16/Apr/19 09:32
Bug,CASSANDRA-3368,12527358,'show schema' in cli does not show compression_options,"Hi,

using the cassandra-cli command line tool, I realized that a 'show schema' does not print out the compression_options I specified when creating them.
Both, the server and the cli tool, where version 1.0.0-rc2.


Example:

[default@Test] CREATE COLUMN FAMILY Response2 WITH key_validation_class=BytesType AND compression_options {sstable_compression:DeflateCompressor};


[default@Test] show schema;
create keyspace Test
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = [{replication_factor : 2}];
use Test;
create column family Response2
  with column_type = 'Standard'
  and comparator = 'BytesType'
  and default_validation_class = 'BytesType'
  and key_validation_class = 'BytesType'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and keys_cached = 200000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider';


Not really critical, but might be confusing for some people = me ;-)

kind regards,
Christian",xedin,christianmovi,Low,Resolved,Fixed,16/Oct/11 16:09,16/Apr/19 09:32
Bug,CASSANDRA-3369,12527360,"AssertionError when adding a node and doing repair, repair hangs","Hi again,

I was playing aroung with Cassandra 1.0.0-rc2 and got an AssertionError. The cluster I set up was two cassandra nodes on one laptop using different 127.0.0.* loopback devices. Both nodes have separate folders on the harddisk.


Here is what I did:


1. Started node1 and inserted some data into it using a simple singlethreaded testprogram (uses hector 0.8.0-2):
127.0.0.1       datacenter1 rack1       Up     Normal  583.55 MB       100.00% Token(bytes[63e5b6995466cd3221cba16646ae19ed])

2. I started another node, node 2 = 127.0.0.2:
127.0.0.2       datacenter1 rack1       Up     Normal  147.57 KB       50.00%  Token(bytes[4d6ccfeaa8bb59551751a2816fde9343])
127.0.0.1       datacenter1 rack1       Up     Normal  583.55 MB       50.00%  Token(bytes[63e5b6995466cd3221cba16646ae19ed])

3. I triggered a ""nodetool -h 127.0.0.1  repair"" on the first node that had the data from my test.


This repair does not seem to ever end. The nodetool is hanging now but my computer is idle. I get an AssertionError on the first node:
java.lang.AssertionError
	at org.apache.cassandra.service.AntiEntropyService$Validator.prepare(AntiEntropyService.java:283)
	at org.apache.cassandra.db.compaction.CompactionManager.doValidationCompaction(CompactionManager.java:825)
	at org.apache.cassandra.db.compaction.CompactionManager.access$600(CompactionManager.java:63)
	at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:432)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)


Update: I dont know if it is important but here is the schema of my test:
create keyspace Test with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options = {replication_factor:2};
CREATE COLUMN FAMILY Response WITH key_validation_class=BytesType AND compression_options={sstable_compression:DeflateCompressor};


kind regards,
Christian",slebresne,christianmovi,Normal,Resolved,Fixed,16/Oct/11 16:35,16/Apr/19 09:32
Bug,CASSANDRA-3370,12527375,Deflate Compression corrupts SSTables,"Hi,

it seems that the Deflate Compressor corrupts the SSTables. 3 out of 3 Installations were corrupt. Snappy works fine.

Here is what I did:

1. Start a single cassandra node (I was using ByteOrderedPartitioner)
2. Write data into cf that uses deflate compression - I think it has to be enough data so that the data folder contains some files.
3. When I now try to read (I did a range scan) from my application, it fails and the logs show corruptions:

Caused by: org.apache.cassandra.io.compress.CorruptedBlockException: (/home/cspriegel/Development/cassandra1/data/Test/Response-h-2-Data.db): corruption detected, chunk at 0 of length 65536.

regards,
Christian",slebresne,christianmovi,Normal,Resolved,Fixed,16/Oct/11 23:04,16/Apr/19 09:32
Bug,CASSANDRA-3371,12527421,Cassandra inferred schema and actual data don't match,"It's looking like there may be a mismatch between the schema that's being reported by the latest CassandraStorage.java, and the data that's actually returned. Here's an example:

rows = LOAD 'cassandra://Frap/PhotoVotes' USING CassandraStorage();
DESCRIBE rows;
rows: {key: chararray,columns: {(name: chararray,value: bytearray,photo_owner: chararray,value_photo_owner: bytearray,pid: chararray,value_pid: bytearray,matched_string: chararray,value_matched_string: bytearray,src_big: chararray,value_src_big: bytearray,time: chararray,value_time: bytearray,vote_type: chararray,value_vote_type: bytearray,voter: chararray,value_voter: bytearray)}}
DUMP rows;
(691831038_1317937188.48955,{(photo_owner,1596090180),(pid,6855155124568798560),(matched_string,),(src_big,),(time,Thu Oct 06 14:39:48 -0700 2011),(vote_type,album_dislike),(voter,691831038)})

getSchema() is reporting the columns as an inner bag of tuples, each of which contains 16 values. In fact, getNext() seems to return an inner bag containing 7 tuples, each of which contains two values. 

It appears that things got out of sync with this change:
http://svn.apache.org/viewvc/cassandra/branches/cassandra-0.8/contrib/pig/src/java/org/apache/cassandra/hadoop/pig/CassandraStorage.java?r1=1177083&r2=1177082&pathrev=1177083

See more discussion at:
http://cassandra-user-incubator-apache-org.3065146.n2.nabble.com/pig-cassandra-problem-quot-Incompatible-field-schema-quot-error-tc6882703.html
",brandon.williams,petewarden,Normal,Resolved,Fixed,17/Oct/11 10:25,16/Apr/19 09:32
Bug,CASSANDRA-3373,12527592,ReadResponseSerializer doesn't compute serialized size correctly,,slebresne,slebresne,Normal,Resolved,Fixed,18/Oct/11 13:42,16/Apr/19 09:32
Bug,CASSANDRA-3374,12527593,CQL can't create column with compression or that use leveled compaction,"Looking at CreateColumnFamilyStatement.java, it doesn't seem CQL can create compressed column families, nor define a compaction strategy.",xedin,slebresne,Low,Resolved,Fixed,18/Oct/11 13:48,16/Apr/19 09:32
Bug,CASSANDRA-3375,12527612,Avoid clock drift on some Windows machines,"Performing Thread.sleep() with non-rounded values increases the frequency of interrupts on Windows machines; this can cause performance problems, and on some machines even clock drift problems for the duration of the sleep.
Fixing the issue is trivial: lower the degree of randomness by allowing only ""rounded"" sleep periods.
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6464007
http://bugs.sun.com/bugdatabase/view_bug.do?bug_id=6435126",,flavio,Low,Resolved,Fixed,18/Oct/11 16:21,16/Apr/19 09:32
Bug,CASSANDRA-3377,12527620,correct dropped messages logging,"CASSANDRA-3004 switched MessagingService back to logging only ""recent"" dropped messages instead of server lifetime totals, but the log message was not updated.",jbellis,jbellis,Low,Resolved,Fixed,18/Oct/11 17:02,16/Apr/19 09:32
Bug,CASSANDRA-3381,12527672,StorageProxy does not log correctly when schema is not in agreement,"""logger.debug(""%s disagrees (%s)"", host, entry.getKey());""

that would literally log: 

DEBUG [pool-2-thread-359] 2011-10-18 10:34:45,376 StorageProxy.java (line 821) %s disagrees (%s)


simple fix: replace with %s with {} ... may want to consider logging better comment?",tommysdk,cywjackson,Low,Resolved,Fixed,18/Oct/11 22:52,16/Apr/19 09:32
Bug,CASSANDRA-3384,12527782,"it would be nice if ""describe keyspace"" in cli shows ""Cache Provider""",Describe keyspace in the cli doesn't show the cache provider it would be nice to show it to verify the settings.,xedin,vijay2win@yahoo.com,Low,Resolved,Fixed,19/Oct/11 16:55,16/Apr/19 09:32
Bug,CASSANDRA-3385,12527792,NPE in hinted handoff,"I'm using the current HEAD of 1.0.0 github branch, and I'm still seeing this error, not sure if it's  this bug or another one.



 INFO [HintedHandoff:1] 2011-10-19 12:43:17,674 HintedHandOffManager.java (line 263) Started hinted handoff for token: 11342745564
0312821154458202477256070484 with IP: /10.39.85.140
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,885 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHan
doff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more
ERROR [HintedHandoff:1] 2011-10-19 12:43:17,886 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[HintedHandoff:1,1,main]
java.lang.RuntimeException: java.lang.NullPointerException
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
        at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:289)
        at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
        at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:337)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        ... 3 more


this could possibly be related to #3291
",jbellis,yangyangyyy,Low,Resolved,Fixed,19/Oct/11 17:44,16/Apr/19 09:32
Bug,CASSANDRA-3387,12527830,unnecessary locking in hint path due to InetAddress.getLocalhost(),"in tests we found locking contention also due to the following




Stacks at 01:49:19 PM (uptime 10:54)


MutationStage:62 [BLOCKED] CPU time: 0:06
java.net.InetAddress.getLocalHost()
org.apache.cassandra.utils.UUIDGen.getClockSeqAndNode()
org.apache.cassandra.utils.UUIDGen.createTimeUUIDBytes(long)
org.apache.cassandra.utils.UUIDGen.getTimeUUIDBytes()
org.apache.cassandra.db.RowMutation.hintFor(RowMutation, ByteBuffer)
org.apache.cassandra.service.StorageProxy$4.run()
java.lang.Thread.run()





we can easily change every getLocalHost() call to use a cached value",yangyangyyy,yangyangyyy,Low,Resolved,Fixed,19/Oct/11 20:59,16/Apr/19 09:32
Bug,CASSANDRA-3388,12527840,StorageService.setMode() is used inconsistently,"{{StorageService.setMode()}}, which ends up setting the OperationMode attribute of the related mbean, is used inconsistently.  In most places, it's used like ""{{setMode(""MODE: details"")}}, but in a few places, it's used more like a normal log message.

To make this attribute more usable through JMX, {{setMode()}} should have a signature like {{setMode(mode, details)}}, where the mode parameter could be an enum (or even just a string, the main thing is just being consistent).  The OperationMode JMX attribute should definitely remain a string, though.",slebresne,thobbs,Low,Resolved,Fixed,19/Oct/11 22:45,16/Apr/19 09:32
Bug,CASSANDRA-3390,12528014,ReadResponseSerializer.serializedSize() calculation is wrong,"in ReadResponse.java


the following code

    public long serializedSize(ReadResponse response, int version)
    {
        int size = DBConstants.intSize;
        size += (response.isDigestQuery() ? response.digest() : ByteBufferUtil.EMPTY_BYTE_BUFFER).remaining();
        size += DBConstants.boolSize;
        if (response.isDigestQuery())
            size += response.digest().remaining();
        else
            size += Row.serializer().serializedSize(response.row(), version);
        return size;
    }


adds the digest size 2 times

this triggers assertion error in at least ReadVerbHandler


",jbellis,yangyangyyy,Normal,Resolved,Fixed,20/Oct/11 19:44,16/Apr/19 09:32
Bug,CASSANDRA-3391,12528043,CFM.toAvro() incorrectly serialises key_validation_class defn,"see http://www.mail-archive.com/user@cassandra.apache.org/msg18132.html

Repo with 

{code}
create keyspace Stats with placement_strategy = 'org.apache.cassandra.locator.SimpleStrategy' and strategy_options={replication_factor:1};

use Stats;

create column family Sample_Stats with default_validation_class=CounterColumnType
    and key_validation_class='CompositeType(UTF8Type,UTF8Type)'
    and comparator='CompositeType(UTF8Type, UTF8Type)'
    and replicate_on_write=true;

[default@Stats] describe cluster;
Cluster Information:
   Snitch: org.apache.cassandra.locator.SimpleSnitch
   Partitioner: org.apache.cassandra.dht.RandomPartitioner
   Schema versions: 
	1d39bbf0-fb60-11e0-0000-242d50cf1ffd: [127.0.0.1]
{code}

Stop and restart the node

{code:java}
ERROR 10:12:22,729 Exception encountered during startup
java.lang.RuntimeException: Could not inflate CFMetaData for {""keyspace"": ""Stats"", ""name"": ""Sample_Stats"", ""column_type"": ""Standard"", ""comparator_type"": ""org.apache.cassandra.db.marshal.CompositeType(org.apache.cassandra.db.marshal.UTF8Type,org.apache.cassandra.db.marshal.UTF8Type)"", ""subcomparator_type"": null, ""comment"": """", ""row_cache_size"": 0.0, ""key_cache_size"": 200000.0, ""read_repair_chance"": 1.0, ""replicate_on_write"": true, ""gc_grace_seconds"": 864000, ""default_validation_class"": ""org.apache.cassandra.db.marshal.CounterColumnType"", ""key_validation_class"": ""org.apache.cassandra.db.marshal.CompositeType"", ""min_compaction_threshold"": 4, ""max_compaction_threshold"": 32, ""row_cache_save_period_in_seconds"": 0, ""key_cache_save_period_in_seconds"": 14400, ""row_cache_keys_to_save"": 2147483647, ""merge_shards_chance"": 0.1, ""id"": 1000, ""column_metadata"": [], ""row_cache_provider"": ""org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider"", ""key_alias"": null, ""compaction_strategy"": ""org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy"", ""compaction_strategy_options"": {}, ""compression_options"": {}}
	at org.apache.cassandra.config.CFMetaData.fromAvro(CFMetaData.java:362)
	at org.apache.cassandra.config.KSMetaData.fromAvro(KSMetaData.java:193)
	at org.apache.cassandra.db.DefsTable.loadFromStorage(DefsTable.java:99)
	at org.apache.cassandra.config.DatabaseDescriptor.loadSchemas(DatabaseDescriptor.java:502)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:161)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Caused by: org.apache.cassandra.config.ConfigurationException: Invalid definition for comparator org.apache.cassandra.db.marshal.CompositeType.
	at org.apache.cassandra.db.marshal.TypeParser.getRawAbstractType(TypeParser.java:319)
	at org.apache.cassandra.db.marshal.TypeParser.getAbstractType(TypeParser.java:247)
	at org.apache.cassandra.db.marshal.TypeParser.parse(TypeParser.java:83)
	at org.apache.cassandra.db.marshal.TypeParser.parse(TypeParser.java:92)
	at org.apache.cassandra.config.CFMetaData.fromAvro(CFMetaData.java:358)
	... 6 more
Caused by: org.apache.cassandra.config.ConfigurationException: Nonsensical empty parameter list for CompositeType
	at org.apache.cassandra.db.marshal.CompositeType.getInstance(CompositeType.java:67)
	at org.apache.cassandra.db.marshal.CompositeType.getInstance(CompositeType.java:61)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.apache.cassandra.db.marshal.TypeParser.getRawAbstractType(TypeParser.java:307)
	... 10 more
{code}

Will post the patch in a minute. ",amorton,amorton,Low,Resolved,Fixed,20/Oct/11 22:26,16/Apr/19 09:32
Bug,CASSANDRA-3394,12528300,AssertionError in PrecompactedRow.write via CommutativeRowIndexer during bootstrap,"{noformat}
ERROR [CompactionExecutor:5] 2011-10-21 15:48:16,138 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[CompactionExecutor:5,1,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.PrecompactedRow.write(PrecompactedRow.java:107)
        at org.apache.cassandra.io.sstable.SSTableWriter$CommutativeRowIndexer.doIndexing(SSTableWriter.java:514)
        at org.apache.cassandra.io.sstable.SSTableWriter$RowIndexer.index(SSTableWriter.java:359)
        at org.apache.cassandra.io.sstable.SSTableWriter$Builder.build(SSTableWriter.java:314)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1118)
        at org.apache.cassandra.db.compaction.CompactionManager$9.call(CompactionManager.java:1109)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

{{<sylvain> The bug is that the compacted row was gcing *every* tombstones instead of none.}}",slebresne,thepaul,Normal,Resolved,Fixed,21/Oct/11 17:09,16/Apr/19 09:32
Bug,CASSANDRA-3395,12528316,Quorum returns incorrect results during hinted handoff,"In a 3 node cluster with RF=3 and using a single coordinator, if monotonically increasing columns are inserted into a row and the latest one sliced (both at QUORUM) during HH replay occasionally this column will not be seen.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,21/Oct/11 20:03,16/Apr/19 09:32
Bug,CASSANDRA-3397,12528394,Problem markers don't show up in Eclipse,"The generated Eclipse files install an Ant Builder to build Cassandra within Eclipse. This appears to mean that the default Java Builder is not present. This means that no problem markers show up in the Problem view or the Package Explorer etc when there are compiler errors or warnings  - you have to study the console output, then navigate manually to the sources of the problems, which is very tedious.

It seems to be possible to re-install the default Java Builder in parallel with the Ant Builder, getting the best of both worlds. I have documented this on the wiki at http://wiki.apache.org/cassandra/RunningCassandraInEclipse

I was wondering a) whether this can be done automatically by the generate-eclipse-files Ant target, and b) whether using both Builders will be problem if one is working on any of the generated code (Thrift, CQL etc). The Java Builder can be temporarily disabled if so by unticking it under Properties->Builders...

See also https://issues.apache.org/jira/browse/CASSANDRA-2854",dallsopp,dallsopp,Low,Resolved,Fixed,22/Oct/11 21:25,16/Apr/19 09:32
Bug,CASSANDRA-3399,12528489,Truncate disregards running compactions when deleting sstables,"All truncation do is `cfs.markCompacted(truncatedSSTables)` without holding any lock or anything. Which have the effect of actually deleting sstables that may be compacting. More precisely there is three problems:
# It removes those compacting sstables from the current set of active sstables for the cfs. But when they are done compacting, DataTracker.replaceCompactedSSTables() will be called and it assumes that the compacted sstable are parts of the current set of active sstables. In other words, we'll get an exception looking like the one of CASSANDRA-3306.
# The result of the compaction will be added as a new active sstable (actually no, because the code will throw an exception before because of the preceding point, but that's something we should probably deal with).
# Currently, compaction don't 'acquire references' on SSTR. That's because the code assumes we won't compact twice the same sstable and that compaction is the only mean to delete an sstable. With these two assumption, acquiring references is not necessary, but truncate break that first assumption.

As for solution, I see two possibilities:
# make the compaction lock be per-cf instead of global (which I think is easy and a good idea anyway) and grab the write lock to do the markCompacted call. The big downside is that truncation will potentially take much longer.
# had two phases: mark the sstable that are not compacting as compacted and set the dataTracker as 'truncated at', and let it deal with the other sstable when their compaction is done. A bit like what is proposed for CASSANDRA-3116 
",jbellis,slebresne,Normal,Resolved,Fixed,24/Oct/11 14:24,16/Apr/19 09:32
Bug,CASSANDRA-3400,12528521,ConcurrentModificationException during nodetool repair,"When running a nodetool repair, the following exception can be thrown:


ERROR [AntiEntropySessions:12] 2011-10-24 11:17:52,154 AbstractCassandraDaemon.java (line 139) Fatal exception in thread Thread[AntiEntropySessions:12,5,RMI Runtime]
java.lang.RuntimeException: java.util.ConcurrentModificationException
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
at java.util.concurrent.FutureTask.run(FutureTask.java:138)
at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
at java.lang.Thread.run(Thread.java:619)
Caused by: java.util.ConcurrentModificationException
at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
at java.util.HashMap$KeyIterator.next(HashMap.java:828)
at org.apache.cassandra.service.AntiEntropyService$RepairSession$RepairJob.sendTreeRequests(AntiEntropyService.java:784)
at org.apache.cassandra.service.AntiEntropyService$RepairSession.runMayThrow(AntiEntropyService.java:680)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
... 6 more
",slebresne,scottfines,Low,Resolved,Fixed,24/Oct/11 16:41,16/Apr/19 09:32
Bug,CASSANDRA-3403,12528813,describe_ring topology information is wrong/incomplete,"In CASSANDRA-2882, topology information was added to describe_ring, however it asks the gossiper for the DC information, and the gossiper can only have this with a gossip-enabled snitch, which currently means the Ec2Snitch.  Instead, it should be asking the snitch for the DC for each endpoint.

Also, the port information should just be removed: whatever port the client has connected to in order to call describe_ring is the right port to use for all endpoints.",patricioe,brandon.williams,Normal,Resolved,Fixed,26/Oct/11 02:46,16/Apr/19 09:32
Bug,CASSANDRA-3404,12528815,[patch] fix logging contexts,"a couple of places the logging context doesn't match the class, probably due to copy/paste bug.
fixed.",,dbrosius@apache.org,Low,Resolved,Fixed,26/Oct/11 03:40,16/Apr/19 09:32
Bug,CASSANDRA-3405,12528834,Row cache provider reported wrong in cassandra-cli,"When doing ""show schema;"" in the CLI, the row_cache_provider is reported as ConcurrentLinkedHashCacheProvider while it really is SerializingCacheProvider

Same goes for ""describe keyspace"" (after CASSANDRA-3384) on the 0.8 branch",marcuse,marcuse,Low,Resolved,Fixed,26/Oct/11 08:15,16/Apr/19 09:32
Bug,CASSANDRA-3407,12529058,Changing partitioner causes interval tree build failure before the change can be detected,"After installed 1.0.0 and changed config file cassandra.yaml, restart cassandra and got exception,

INFO 22:25:37,727 Opening /srv/opt/cassandra8/data/system/IndexInfo-g-121 (5428 bytes)
ERROR 22:25:37,753 Exception encountered during startup_type: 0},
java.lang.StackOverflowError, validation_class: UTF8Type, index_type: 0},
       at java.math.BigInteger.compareMagnitude(BigInteger.java:2477)
       at java.math.BigInteger.compareTo(BigInteger.java:2463)type: 0},
       at org.apache.cassandra.dht.BigIntegerToken.compareTo(BigIntegerToken.java:39)
       at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:83)
       at org.apache.cassandra.db.DecoratedKey.compareTo(DecoratedKey.java:38)
       at java.util.Arrays.mergeSort(Arrays.java:1144)dex_type: 0},
       at java.util.Arrays.sort(Arrays.java:1079)dex_type: 0},
       at java.util.Collections.sort(Collections.java:117)},
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.findMinMedianMax(IntervalNode.java:102)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:43)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
.....

       at org.apache.cassandra.utils.IntervalTree.IntervalNode.<init>(IntervalNode.java:51)
       at org.apache.cassandra.utils.IntervalTree.IntervalTree.<init>(IntervalTree.java:38)
       at org.apache.cassandra.db.DataTracker$View.buildIntervalTree(DataTracker.java:522)
       at org.apache.cassandra.db.DataTracker$View.replace(DataTracker.java:547)
       at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:268)
       at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
       at org.apache.cassandra.db.ColumnFamilyStore.<init>(ColumnFamilyStore.java:216)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:315)
       at org.apache.cassandra.db.ColumnFamilyStore.createColumnFamilyStore(ColumnFamilyStore.java:285)
       at org.apache.cassandra.db.Table.initCf(Table.java:372)
       at org.apache.cassandra.db.Table.<init>(Table.java:320)
       at org.apache.cassandra.db.Table.open(Table.java:121)
       at org.apache.cassandra.db.Table.open(Table.java:104)
       at org.apache.cassandra.db.SystemTable.checkHealth(SystemTable.java:215)
       at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:150)
       at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
       at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:106)
Exception encountered during startup: null
",yukim,apache.zli,Low,Resolved,Fixed,27/Oct/11 14:33,16/Apr/19 09:32
Bug,CASSANDRA-3408,12529064,LeveledCompactionTask is too fragile and can block compactions,"If any error happens during a LeveledCompactionTask, it will just block every compaction.",slebresne,slebresne,Normal,Resolved,Fixed,27/Oct/11 15:14,16/Apr/19 09:32
Bug,CASSANDRA-3409,12529068,CFS reloading of the compaction strategy is done for every metadata update and is not thread safe,"The reloading of the compaction strategy done during CFS.reload is not thread safe. In particular, this is a problem for leveled compactions. It could leads to some sstable not being added to the manifest and also breaks the 'only one leveledCompactionTask can run at any given time' assumption (which, at least without CASSANDRA-3408 can likely leads to blocking compactions completely).",slebresne,slebresne,Normal,Resolved,Fixed,27/Oct/11 15:50,16/Apr/19 09:32
Bug,CASSANDRA-3410,12529076,inconsistent rejection of CL.ANY on reads,,jbellis,jbellis,Low,Resolved,Fixed,27/Oct/11 16:32,16/Apr/19 09:32
Bug,CASSANDRA-3414,12529198,Not possible to change row_cache_provider on existing cf,"row_cache_provider is not possible to change using update column family xyz with row_cache_provider='something' in 0.8

It does work in 1.0.0

Reason is that the field is not added to the avro record, patch attached fixes that",marcuse,marcuse,Low,Resolved,Fixed,28/Oct/11 05:53,16/Apr/19 09:32
Bug,CASSANDRA-3415,12529245,show schema fails,"following command breaks ""show schema"" cli command with error ""A long is exactly 8 bytes: 5""

create column family resultcache with column_type = 'Super' and comparator = 'LongType' and  key_validation_class = 'UTF8Type' and subcomparator = 'AsciiType' and replicate_on_write = false and rows_cached = 700 and keys_cached = 30000 and key_cache_save_period = 0 and column_metadata = [ {column_name: id, validation_class: LongType}, {column_name: name, validation_class: 'AsciiType'}, {column_name: crc32, validation_class: LongType}, {column_name: size, validation_class: LongType} ];",jbellis,hsn,Low,Resolved,Fixed,28/Oct/11 13:13,16/Apr/19 09:32
Bug,CASSANDRA-3417,12529294,InvocationTargetException ConcurrentModificationException at startup,"I was starting up the new DataStax AMI where the seed starts first and 34 nodes would latch on together. So far things have been working decently for launching, but right now I just got this during startup.


{CODE}
ubuntu@ip-10-40-190-143:~$ sudo cat /var/log/cassandra/output.log 
 INFO 09:24:38,453 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26
 INFO 09:24:38,456 Heap size: 1936719872/1937768448
 INFO 09:24:38,457 Classpath: /usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.4.0.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/joda-time-1.6.2.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.6.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.3.jar:/usr/share/cassandra/apache-cassandra-1.0.0.jar:/usr/share/cassandra/apache-cassandra-thrift-1.0.0.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar
 INFO 09:24:39,891 JNA mlockall successful
 INFO 09:24:39,901 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 09:24:40,057 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:24:40,069 Global memtable threshold is enabled at 616MB
 INFO 09:24:40,159 EC2Snitch using region: us-east, zone: 1d.
 INFO 09:24:40,475 Creating new commitlog segment /raid0/cassandra/commitlog/CommitLog-1319793880475.log
 INFO 09:24:40,486 Couldn't detect any schema definitions in local storage.
 INFO 09:24:40,486 Found table data in data directories. Consider using the CLI to define your schema.
 INFO 09:24:40,497 No commitlog files found; skipping replay
 INFO 09:24:40,501 Cassandra version: 1.0.0
 INFO 09:24:40,502 Thrift API version: 19.18.0
 INFO 09:24:40,502 Loading persisted ring state
 INFO 09:24:40,506 Starting up server gossip
 INFO 09:24:40,529 Enqueuing flush of Memtable-LocationInfo@1388314661(190/237 serialized/live bytes, 4 ops)
 INFO 09:24:40,530 Writing Memtable-LocationInfo@1388314661(190/237 serialized/live bytes, 4 ops)
 INFO 09:24:40,600 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-1-Data.db (298 bytes)
 INFO 09:24:40,613 Ec2Snitch adding ApplicationState ec2region=us-east ec2zone=1d
 INFO 09:24:40,621 Starting Messaging Service on /10.40.190.143:7000
 INFO 09:24:40,628 Joining: waiting for ring and schema information
 INFO 09:24:43,389 InetAddress /10.194.29.156 is now dead.
 INFO 09:24:43,391 InetAddress /10.85.11.38 is now dead.
 INFO 09:24:43,392 InetAddress /10.34.42.28 is now dead.
 INFO 09:24:43,393 InetAddress /10.77.63.49 is now dead.
 INFO 09:24:43,394 InetAddress /10.194.22.191 is now dead.
 INFO 09:24:43,395 InetAddress /10.34.74.58 is now dead.
 INFO 09:24:43,395 Node /10.34.33.16 is now part of the cluster
 INFO 09:24:43,396 InetAddress /10.34.33.16 is now UP
 INFO 09:24:43,397 Enqueuing flush of Memtable-LocationInfo@1629818866(20/25 serialized/live bytes, 1 ops)
 INFO 09:24:43,398 Writing Memtable-LocationInfo@1629818866(20/25 serialized/live bytes, 1 ops)
 INFO 09:24:43,417 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-2-Data.db (74 bytes)
 INFO 09:24:43,418 InetAddress /10.202.67.43 is now dead.
 INFO 09:24:43,419 InetAddress /10.116.215.81 is now dead.
 INFO 09:24:43,420 InetAddress /10.99.39.242 is now dead.
 INFO 09:24:43,421 InetAddress /10.80.110.28 is now dead.
 INFO 09:24:43,422 InetAddress /10.118.233.198 is now dead.
 INFO 09:24:43,423 InetAddress /10.40.177.173 is now dead.
 INFO 09:24:43,424 InetAddress /10.205.23.34 is now dead.
 INFO 09:24:43,425 InetAddress /10.101.41.8 is now dead.
 INFO 09:24:43,669 InetAddress /10.118.230.219 is now dead.
 INFO 09:24:43,670 InetAddress /10.80.41.192 is now dead.
 INFO 09:24:43,671 InetAddress /10.40.22.224 is now dead.
 INFO 09:24:43,672 InetAddress /10.39.107.114 is now dead.
 INFO 09:24:46,164 InetAddress /10.118.185.68 is now dead.
 INFO 09:24:46,166 InetAddress /10.84.205.93 is now dead.
 INFO 09:24:46,167 InetAddress /10.116.134.183 is now dead.
 INFO 09:24:46,670 InetAddress /10.118.179.67 is now dead.
 INFO 09:24:46,671 InetAddress /10.116.241.250 is now dead.
 INFO 09:24:48,441 InetAddress /10.118.94.62 is now dead.
 INFO 09:24:48,442 InetAddress /10.99.86.251 is now dead.
 INFO 09:24:50,176 InetAddress /10.113.42.21 is now dead.
 INFO 09:24:50,177 InetAddress /10.34.159.72 is now dead.
 INFO 09:24:50,178 InetAddress /10.32.79.134 is now dead.
 INFO 09:24:50,179 InetAddress /10.80.210.38 is now dead.
 INFO 09:24:50,180 InetAddress /10.34.70.73 is now dead.
 INFO 09:24:50,181 InetAddress /10.196.79.240 is now dead.
 INFO 09:25:01,713 InetAddress /10.82.210.172 is now dead.
 INFO 09:25:06,202 InetAddress /10.80.110.28 is now UP
 INFO 09:25:06,908 InetAddress /10.99.39.242 is now UP
 INFO 09:25:07,696 InetAddress /10.118.233.198 is now UP
 INFO 09:25:07,697 InetAddress /10.205.23.34 is now UP
 INFO 09:25:08,704 InetAddress /10.194.22.191 is now UP
 INFO 09:25:08,705 InetAddress /10.40.177.173 is now UP
 INFO 09:25:08,706 InetAddress /10.101.41.8 is now UP
 INFO 09:25:09,489 InetAddress /10.202.67.43 is now UP
 INFO 09:25:09,698 InetAddress /10.77.63.49 is now UP
 INFO 09:25:10,628 Joining: getting bootstrap token
 INFO 09:25:10,631 Enqueuing flush of Memtable-LocationInfo@1733057335(36/45 serialized/live bytes, 1 ops)
 INFO 09:25:10,631 Writing Memtable-LocationInfo@1733057335(36/45 serialized/live bytes, 1 ops)
 INFO 09:25:10,647 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-3-Data.db (87 bytes)
 INFO 09:25:10,649 Joining: sleeping 30000 ms for pending range setup
 INFO 09:25:10,689 InetAddress /10.85.11.38 is now UP
 INFO 09:25:10,708 InetAddress /10.34.74.58 is now UP
 INFO 09:25:10,912 InetAddress /10.194.29.156 is now UP
 INFO 09:25:11,261 Applying migration bb843dd0-0146-11e1-0000-b877c09da5ff Add keyspace: OpsCenter, rep strategy:SimpleStrategy{org.apache.cassandra.config.CFMetaData@55e29b99[cfId=1000,ksName=OpsCenter,cfName=pdps,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=300.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@105585dc,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@5ec736e4[cfId=1004,ksName=OpsCenter,cfName=rollups86400,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=2,maxCompactionThreshold=8,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@68e4e358,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@b09dc35[cfId=1003,ksName=OpsCenter,cfName=rollups7200,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=2,maxCompactionThreshold=8,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@3458213c,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@5ee04fd[cfId=1002,ksName=OpsCenter,cfName=rollups300,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=16,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@4d898115,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@7e79b177[cfId=1005,ksName=OpsCenter,cfName=events,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=OpsCenter raw event storage,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=8,maxCompactionThreshold=12,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@67723c7f,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@540523be[cfId=1006,ksName=OpsCenter,cfName=events_timeline,cfType=Standard,comparator=org.apache.cassandra.db.marshal.LongType,subcolumncomparator=<null>,comment=OpsCenter event timelines,rowCacheSize=0.0,keyCacheSize=5.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=8,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@1d6dba0a,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@ed0f59e[cfId=1007,ksName=OpsCenter,cfName=settings,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=OpsCenter settings,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=8,maxCompactionThreshold=12,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@38ad5fab,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}], org.apache.cassandra.config.CFMetaData@7e63f09e[cfId=1001,ksName=OpsCenter,cfName=rollups60,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=50.0,readRepairChance=0.25,replicateOnWrite=true,gcGraceSeconds=864000,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=43200,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.ConcurrentLinkedHashCacheProvider@534a55e5,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]}, durable_writes: true
 INFO 09:25:11,273 Enqueuing flush of Memtable-Migrations@1767199109(12925/16156 serialized/live bytes, 1 ops)
 INFO 09:25:11,273 Writing Memtable-Migrations@1767199109(12925/16156 serialized/live bytes, 1 ops)
 INFO 09:25:11,274 Enqueuing flush of Memtable-Schema@1616586953(5820/7275 serialized/live bytes, 3 ops)
 INFO 09:25:11,358 Completed flushing /raid0/cassandra/data/system/Migrations-h-1-Data.db (12989 bytes)
 INFO 09:25:11,358 Writing Memtable-Schema@1616586953(5820/7275 serialized/live bytes, 3 ops)
 INFO 09:25:11,390 Completed flushing /raid0/cassandra/data/system/Schema-h-1-Data.db (5970 bytes)
 INFO 09:25:11,727 InetAddress /10.116.215.81 is now UP
 INFO 09:25:11,744 InetAddress /10.34.42.28 is now UP
 INFO 09:25:11,750 InetAddress /10.40.22.224 is now UP
 INFO 09:25:12,023 InetAddress /10.80.41.192 is now UP
 INFO 09:25:12,712 InetAddress /10.39.107.114 is now UP
 INFO 09:25:12,717 InetAddress /10.118.185.68 is now UP
 INFO 09:25:12,721 InetAddress /10.116.134.183 is now UP
 INFO 09:25:13,322 InetAddress /10.118.230.219 is now UP
 INFO 09:25:13,632 InetAddress /10.84.205.93 is now UP
 INFO 09:25:14,713 InetAddress /10.118.179.67 is now UP
 INFO 09:25:14,717 InetAddress /10.116.241.250 is now UP
 INFO 09:25:17,468 InetAddress /10.34.159.72 is now UP
 INFO 09:25:17,476 InetAddress /10.118.94.62 is now UP
 INFO 09:25:17,480 InetAddress /10.80.210.38 is now UP
 INFO 09:25:17,716 InetAddress /10.32.79.134 is now UP
 INFO 09:25:17,721 InetAddress /10.99.86.251 is now UP
 INFO 09:25:18,717 InetAddress /10.196.79.240 is now UP
 INFO 09:25:18,727 InetAddress /10.34.70.73 is now UP
 INFO 09:25:19,596 InetAddress /10.113.42.21 is now UP
 INFO 09:25:25,750 InetAddress /10.82.210.172 is now UP
 INFO 09:25:37,743 Enqueuing flush of Memtable-LocationInfo@288976631(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,744 Writing Memtable-LocationInfo@288976631(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,764 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-4-Data.db (89 bytes)
 INFO 09:25:37,773 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-1-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-3-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-4-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-2-Data.db')]
 INFO 09:25:37,776 Enqueuing flush of Memtable-LocationInfo@1950702248(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,777 Writing Memtable-LocationInfo@1950702248(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:37,821 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-5-Data.db (89 bytes)
 INFO 09:25:37,869 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-6-Data.db,].  548 to 443 (~80% of original) bytes for 3 keys at 0.006500MB/s.  Time: 65ms.
 INFO 09:25:38,740 Enqueuing flush of Memtable-LocationInfo@92917455(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,740 Writing Memtable-LocationInfo@92917455(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,757 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-8-Data.db (89 bytes)
 INFO 09:25:38,766 Enqueuing flush of Memtable-LocationInfo@1096488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,767 Writing Memtable-LocationInfo@1096488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,814 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-9-Data.db (89 bytes)
 INFO 09:25:38,816 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-6-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-9-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-8-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-5-Data.db')]
 INFO 09:25:38,823 Enqueuing flush of Memtable-LocationInfo@1734564525(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,823 Writing Memtable-LocationInfo@1734564525(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:38,893 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-10-Data.db (89 bytes)
 INFO 09:25:38,916 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-11-Data.db,].  710 to 548 (~77% of original) bytes for 3 keys at 0.005226MB/s.  Time: 100ms.
 INFO 09:25:39,538 Enqueuing flush of Memtable-LocationInfo@811507066(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,539 Writing Memtable-LocationInfo@811507066(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,555 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-13-Data.db (89 bytes)
 INFO 09:25:39,578 Enqueuing flush of Memtable-LocationInfo@1125690366(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,578 Writing Memtable-LocationInfo@1125690366(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,594 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-14-Data.db (89 bytes)
 INFO 09:25:39,596 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-11-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-10-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-14-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-13-Data.db')]
 INFO 09:25:39,613 Enqueuing flush of Memtable-LocationInfo@1870148830(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,614 Writing Memtable-LocationInfo@1870148830(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,652 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-16-Data.db (89 bytes)
 INFO 09:25:39,692 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-15-Data.db,].  815 to 653 (~80% of original) bytes for 3 keys at 0.006487MB/s.  Time: 96ms.
 INFO 09:25:39,731 Enqueuing flush of Memtable-LocationInfo@1279866611(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,731 Writing Memtable-LocationInfo@1279866611(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:39,747 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-18-Data.db (89 bytes)
 INFO 09:25:40,649 Starting to bootstrap...
 INFO 09:25:40,701 Finished streaming session 304272969286 from /10.205.23.34
 INFO 09:25:40,703 Enqueuing flush of Memtable-LocationInfo@1868577756(53/66 serialized/live bytes, 2 ops)
 INFO 09:25:40,703 Writing Memtable-LocationInfo@1868577756(53/66 serialized/live bytes, 2 ops)
 INFO 09:25:40,721 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-19-Data.db (163 bytes)
 INFO 09:25:40,722 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-19-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-15-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-18-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-16-Data.db')]
 INFO 09:25:40,726 Node /10.40.190.143 state jump to normal
 INFO 09:25:40,734 Enqueuing flush of Memtable-LocationInfo@641287650(35/43 serialized/live bytes, 1 ops)
 INFO 09:25:40,735 Writing Memtable-LocationInfo@641287650(35/43 serialized/live bytes, 1 ops)
java.lang.reflect.InvocationTargetException
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
    at java.lang.reflect.Method.invoke(Method.java:597)
    at org.apache.commons.daemon.support.DaemonLoader.load(DaemonLoader.java:160)
Caused by: java.util.ConcurrentModificationException
    at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
    at java.util.HashMap$EntryIterator.next(HashMap.java:834)
    at java.util.HashMap$EntryIterator.next(HashMap.java:832)
    at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:301)
    at com.google.common.collect.AbstractBiMap$EntrySet$1.next(AbstractBiMap.java:293)
    at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1127)
    at org.apache.cassandra.service.StorageService.calculatePendingRanges(StorageService.java:1084)
    at org.apache.cassandra.service.StorageService.handleStateNormal(StorageService.java:920)
    at org.apache.cassandra.service.StorageService.onChange(StorageService.java:805)
    at org.apache.cassandra.gms.Gossiper.doNotifications(Gossiper.java:880)
    at org.apache.cassandra.gms.Gossiper.addLocalApplicationState(Gossiper.java:1027)
    at org.apache.cassandra.service.StorageService.setToken(StorageService.java:226)
    at org.apache.cassandra.service.StorageService.joinTokenRing(StorageService.java:573)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:460)
    at org.apache.cassandra.service.StorageService.initServer(StorageService.java:381)
    at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:215)
    at org.apache.cassandra.service.AbstractCassandraDaemon.init(AbstractCassandraDaemon.java:238)
    ... 5 more
Cannot load daemon
Service exit with a return value of 3
 INFO 09:35:35,156 JVM vendor/version: Java HotSpot(TM) 64-Bit Server VM/1.6.0_26
 INFO 09:35:35,159 Heap size: 1936719872/1937768448
 INFO 09:35:35,160 Classpath: /usr/share/cassandra/lib/antlr-3.2.jar:/usr/share/cassandra/lib/avro-1.4.0-fixes.jar:/usr/share/cassandra/lib/avro-1.4.0-sources-fixes.jar:/usr/share/cassandra/lib/commons-cli-1.1.jar:/usr/share/cassandra/lib/commons-codec-1.2.jar:/usr/share/cassandra/lib/commons-lang-2.4.jar:/usr/share/cassandra/lib/compress-lzf-0.8.4.jar:/usr/share/cassandra/lib/concurrentlinkedhashmap-lru-1.2.jar:/usr/share/cassandra/lib/guava-r08.jar:/usr/share/cassandra/lib/high-scale-lib-1.1.2.jar:/usr/share/cassandra/lib/jackson-core-asl-1.4.0.jar:/usr/share/cassandra/lib/jackson-mapper-asl-1.4.0.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar:/usr/share/cassandra/lib/jline-0.9.94.jar:/usr/share/cassandra/lib/joda-time-1.6.2.jar:/usr/share/cassandra/lib/json-simple-1.1.jar:/usr/share/cassandra/lib/libthrift-0.6.jar:/usr/share/cassandra/lib/log4j-1.2.16.jar:/usr/share/cassandra/lib/servlet-api-2.5-20081211.jar:/usr/share/cassandra/lib/slf4j-api-1.6.1.jar:/usr/share/cassandra/lib/slf4j-log4j12-1.6.1.jar:/usr/share/cassandra/lib/snakeyaml-1.6.jar:/usr/share/cassandra/lib/snappy-java-1.0.3.jar:/usr/share/cassandra/apache-cassandra-1.0.0.jar:/usr/share/cassandra/apache-cassandra-thrift-1.0.0.jar:/usr/share/cassandra/apache-cassandra.jar:/usr/share/java/jna.jar:/etc/cassandra:/usr/share/java/commons-daemon.jar:/usr/share/cassandra/lib/jamm-0.2.5.jar
 INFO 09:35:36,626 JNA mlockall successful
 INFO 09:35:36,636 Loading settings from file:/etc/cassandra/cassandra.yaml
 INFO 09:35:36,757 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:35:36,769 Global memtable threshold is enabled at 616MB
 INFO 09:35:36,811 EC2Snitch using region: us-east, zone: 1d.
 INFO 09:35:37,030 Opening /raid0/cassandra/data/system/Schema-h-1 (5970 bytes)
 INFO 09:35:37,067 Opening /raid0/cassandra/data/system/Migrations-h-1 (12989 bytes)
 INFO 09:35:37,075 Opening /raid0/cassandra/data/system/LocationInfo-h-19 (163 bytes)
 INFO 09:35:37,075 Opening /raid0/cassandra/data/system/LocationInfo-h-18 (89 bytes)
 INFO 09:35:37,083 Opening /raid0/cassandra/data/system/LocationInfo-h-15 (653 bytes)
 INFO 09:35:37,085 Opening /raid0/cassandra/data/system/LocationInfo-h-16 (89 bytes)
 INFO 09:35:37,131 Loading schema version bb843dd0-0146-11e1-0000-b877c09da5ff
 INFO 09:35:37,372 Creating new commitlog segment /raid0/cassandra/commitlog/CommitLog-1319794537372.log
 INFO 09:35:37,384 Replaying /raid0/cassandra/commitlog/CommitLog-1319793880475.log
 INFO 09:35:37,416 Finished reading /raid0/cassandra/commitlog/CommitLog-1319793880475.log
 INFO 09:35:37,422 Enqueuing flush of Memtable-events@1830423861(164/205 serialized/live bytes, 5 ops)
 INFO 09:35:37,423 Writing Memtable-events@1830423861(164/205 serialized/live bytes, 5 ops)
 INFO 09:35:37,424 Enqueuing flush of Memtable-Versions@817138449(83/103 serialized/live bytes, 3 ops)
 INFO 09:35:37,472 Completed flushing /raid0/cassandra/data/OpsCenter/events-h-1-Data.db (230 bytes)
 INFO 09:35:37,479 Writing Memtable-Versions@817138449(83/103 serialized/live bytes, 3 ops)
 INFO 09:35:37,497 Completed flushing /raid0/cassandra/data/system/Versions-h-1-Data.db (247 bytes)
 INFO 09:35:37,497 Log replay complete, 4 replayed mutations
 INFO 09:35:37,509 Cassandra version: 1.0.0
 INFO 09:35:37,510 Thrift API version: 19.18.0
 INFO 09:35:37,510 Loading persisted ring state
 INFO 09:35:37,528 Starting up server gossip
 INFO 09:35:37,530 Enqueuing flush of Memtable-LocationInfo@1655441108(29/36 serialized/live bytes, 1 ops)
 INFO 09:35:37,530 Writing Memtable-LocationInfo@1655441108(29/36 serialized/live bytes, 1 ops)
 INFO 09:35:37,554 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-20-Data.db (80 bytes)
 INFO 09:35:37,555 Ec2Snitch adding ApplicationState ec2region=us-east ec2zone=1d
 INFO 09:35:37,562 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-16-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-18-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-19-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-20-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-15-Data.db')]
 INFO 09:35:37,566 Starting Messaging Service on /10.40.190.143:7000
 INFO 09:35:37,592 Using saved token 19444706681196483626478548996101040654
 INFO 09:35:37,593 Enqueuing flush of Memtable-LocationInfo@995684858(53/66 serialized/live bytes, 2 ops)
 INFO 09:35:37,593 Writing Memtable-LocationInfo@995684858(53/66 serialized/live bytes, 2 ops)
 INFO 09:35:37,616 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-22-Data.db (163 bytes)
 INFO 09:35:37,620 Node /10.40.190.143 state jump to normal
 INFO 09:35:37,639 Bootstrap/Replace/Move completed! Now serving reads.
 INFO 09:35:37,640 Will not load MX4J, mx4j-tools.jar is not in the classpath
 INFO 09:35:37,684 Binding thrift service to /0.0.0.0:9160
 INFO 09:35:37,687 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-21-Data.db,].  1,074 to 799 (~74% of original) bytes for 4 keys at 0.007620MB/s.  Time: 100ms.
 INFO 09:35:37,688 Using TFastFramedTransport with a max frame size of 15728640 bytes.
 INFO 09:35:37,692 Using synchronous/threadpool thrift server on /0.0.0.0 : 9160
 INFO 09:35:37,695 Listening for thrift clients...
 INFO 09:35:37,706 Node /10.118.230.219 is now part of the cluster
 INFO 09:35:37,707 InetAddress /10.118.230.219 is now UP
 INFO 09:35:37,708 Enqueuing flush of Memtable-LocationInfo@2035487037(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,709 Writing Memtable-LocationInfo@2035487037(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,725 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-24-Data.db (89 bytes)
 INFO 09:35:37,726 Node /10.34.42.28 is now part of the cluster
 INFO 09:35:37,727 InetAddress /10.34.42.28 is now UP
 INFO 09:35:37,729 Enqueuing flush of Memtable-LocationInfo@321887181(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,729 Writing Memtable-LocationInfo@321887181(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,747 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-25-Data.db (89 bytes)
 INFO 09:35:37,748 Node /10.77.63.49 has restarted, now UP
 INFO 09:35:37,748 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-24-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-22-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-25-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-21-Data.db')]
 INFO 09:35:37,748 InetAddress /10.77.63.49 is now UP
 INFO 09:35:37,749 Node /10.77.63.49 state jump to normal
 INFO 09:35:37,750 Node /10.34.70.73 is now part of the cluster
 INFO 09:35:37,750 InetAddress /10.34.70.73 is now UP
 INFO 09:35:37,752 Enqueuing flush of Memtable-LocationInfo@1354749546(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,752 Writing Memtable-LocationInfo@1354749546(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,789 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-26-Data.db,].  1,140 to 877 (~76% of original) bytes for 4 keys at 0.020399MB/s.  Time: 41ms.
 INFO 09:35:37,801 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-27-Data.db (89 bytes)
 INFO 09:35:37,801 Node /10.99.86.251 is now part of the cluster
 INFO 09:35:37,802 InetAddress /10.99.86.251 is now UP
 INFO 09:35:37,803 Enqueuing flush of Memtable-LocationInfo@793374785(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,804 Writing Memtable-LocationInfo@793374785(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,825 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-29-Data.db (89 bytes)
 INFO 09:35:37,826 Node /10.202.67.43 has restarted, now UP
 INFO 09:35:37,827 InetAddress /10.202.67.43 is now UP
 INFO 09:35:37,827 Node /10.202.67.43 state jump to normal
 INFO 09:35:37,828 Node /10.116.134.183 is now part of the cluster
 INFO 09:35:37,828 InetAddress /10.116.134.183 is now UP
 INFO 09:35:37,829 Enqueuing flush of Memtable-LocationInfo@1728699027(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,830 Writing Memtable-LocationInfo@1728699027(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,850 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-30-Data.db (89 bytes)
 INFO 09:35:37,852 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-30-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-27-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-26-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-29-Data.db')]
 INFO 09:35:37,853 Node /10.118.94.62 is now part of the cluster
 INFO 09:35:37,853 InetAddress /10.118.94.62 is now UP
 INFO 09:35:37,855 Enqueuing flush of Memtable-LocationInfo@2001229122(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,855 Writing Memtable-LocationInfo@2001229122(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,885 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-31-Data.db (89 bytes)
 INFO 09:35:37,886 Node /10.116.215.81 is now part of the cluster
 INFO 09:35:37,887 InetAddress /10.116.215.81 is now UP
 INFO 09:35:37,888 Enqueuing flush of Memtable-LocationInfo@1748800276(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,888 Writing Memtable-LocationInfo@1748800276(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,909 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-33-Data.db (89 bytes)
 INFO 09:35:37,910 Node /10.80.110.28 has restarted, now UP
 INFO 09:35:37,911 InetAddress /10.80.110.28 is now UP
 INFO 09:35:37,911 Node /10.80.110.28 state jump to normal
 INFO 09:35:37,912 Node /10.80.210.38 is now part of the cluster
 INFO 09:35:37,912 InetAddress /10.80.210.38 is now UP
 INFO 09:35:37,914 Enqueuing flush of Memtable-LocationInfo@1761382005(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,914 Writing Memtable-LocationInfo@1761382005(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,925 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-32-Data.db,].  1,144 to 982 (~85% of original) bytes for 4 keys at 0.014190MB/s.  Time: 66ms.
 INFO 09:35:37,927 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-35-Data.db (89 bytes)
 INFO 09:35:37,928 Node /10.40.177.173 has restarted, now UP
 INFO 09:35:37,929 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-31-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-32-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-33-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-35-Data.db')]
 INFO 09:35:37,929 InetAddress /10.40.177.173 is now UP
 INFO 09:35:37,929 Node /10.40.177.173 state jump to normal
 INFO 09:35:37,930 Node /10.101.41.8 has restarted, now UP
 INFO 09:35:37,931 InetAddress /10.101.41.8 is now UP
 INFO 09:35:37,931 Node /10.101.41.8 state jump to normal
 INFO 09:35:37,931 Node /10.205.23.34 has restarted, now UP
 INFO 09:35:37,932 InetAddress /10.205.23.34 is now UP
 INFO 09:35:37,932 Node /10.205.23.34 state jump to normal
 INFO 09:35:37,933 Node /10.118.185.68 is now part of the cluster
 INFO 09:35:37,933 InetAddress /10.118.185.68 is now UP
 INFO 09:35:37,934 Enqueuing flush of Memtable-LocationInfo@260440278(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,935 Writing Memtable-LocationInfo@260440278(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,970 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-36-Data.db (89 bytes)
 INFO 09:35:37,971 Node /10.116.241.250 is now part of the cluster
 INFO 09:35:37,972 InetAddress /10.116.241.250 is now UP
 INFO 09:35:37,973 Enqueuing flush of Memtable-LocationInfo@768673839(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:37,974 Writing Memtable-LocationInfo@768673839(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,003 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-38-Data.db (89 bytes)
 INFO 09:35:38,004 Node /10.113.42.21 is now part of the cluster
 INFO 09:35:38,005 InetAddress /10.113.42.21 is now UP
 INFO 09:35:38,007 Enqueuing flush of Memtable-LocationInfo@1610335061(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,008 Writing Memtable-LocationInfo@1610335061(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,014 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-37-Data.db,].  1,249 to 1,087 (~87% of original) bytes for 4 keys at 0.012196MB/s.  Time: 85ms.
 INFO 09:35:38,024 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-40-Data.db (89 bytes)
 INFO 09:35:38,024 Node /10.194.29.156 is now part of the cluster
 INFO 09:35:38,025 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-37-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-40-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-36-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-38-Data.db')]
 INFO 09:35:38,025 InetAddress /10.194.29.156 is now UP
 INFO 09:35:38,026 Enqueuing flush of Memtable-LocationInfo@1625488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,027 Writing Memtable-LocationInfo@1625488363(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,042 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-41-Data.db (89 bytes)
 INFO 09:35:38,043 Node /10.85.11.38 has restarted, now UP
 INFO 09:35:38,044 InetAddress /10.85.11.38 is now UP
 INFO 09:35:38,044 Node /10.85.11.38 state jump to normal
 INFO 09:35:38,045 Node /10.34.159.72 is now part of the cluster
 INFO 09:35:38,046 InetAddress /10.34.159.72 is now UP
 INFO 09:35:38,047 Enqueuing flush of Memtable-LocationInfo@747881713(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,048 Writing Memtable-LocationInfo@747881713(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,065 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-42-Data.db (89 bytes)
 INFO 09:35:38,067 Node /10.194.22.191 is now part of the cluster
 INFO 09:35:38,067 InetAddress /10.194.22.191 is now UP
 INFO 09:35:38,069 Enqueuing flush of Memtable-LocationInfo@709926392(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,069 Writing Memtable-LocationInfo@709926392(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,092 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-44-Data.db (89 bytes)
 INFO 09:35:38,093 Node /10.34.74.58 is now part of the cluster
 INFO 09:35:38,097 InetAddress /10.34.74.58 is now UP
 INFO 09:35:38,098 Enqueuing flush of Memtable-LocationInfo@1356841826(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,099 Writing Memtable-LocationInfo@1356841826(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,105 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-43-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-41-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-44-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-42-Data.db')]
 INFO 09:35:38,106 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-43-Data.db,].  1,354 to 1,192 (~88% of original) bytes for 4 keys at 0.014034MB/s.  Time: 81ms.
 INFO 09:35:38,144 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-46-Data.db (89 bytes)
 INFO 09:35:38,145 Node /10.40.22.224 is now part of the cluster
 INFO 09:35:38,146 InetAddress /10.40.22.224 is now UP
 INFO 09:35:38,147 Enqueuing flush of Memtable-LocationInfo@422797318(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,148 Writing Memtable-LocationInfo@422797318(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,155 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-47-Data.db,].  1,459 to 1,297 (~88% of original) bytes for 4 keys at 0.024738MB/s.  Time: 50ms.
 INFO 09:35:38,164 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-49-Data.db (89 bytes)
 INFO 09:35:38,165 Node /10.32.79.134 is now part of the cluster
 INFO 09:35:38,166 InetAddress /10.32.79.134 is now UP
 INFO 09:35:38,167 Enqueuing flush of Memtable-LocationInfo@1455093129(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,168 Writing Memtable-LocationInfo@1455093129(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,199 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-50-Data.db (89 bytes)
 INFO 09:35:38,200 Node /10.118.179.67 is now part of the cluster
 INFO 09:35:38,200 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-50-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-47-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-49-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-46-Data.db')]
 INFO 09:35:38,200 InetAddress /10.118.179.67 is now UP
 INFO 09:35:38,202 Enqueuing flush of Memtable-LocationInfo@1105436908(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,202 Writing Memtable-LocationInfo@1105436908(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,248 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-51-Data.db (89 bytes)
 INFO 09:35:38,249 Node /10.84.205.93 is now part of the cluster
 INFO 09:35:38,249 InetAddress /10.84.205.93 is now UP
 INFO 09:35:38,251 Enqueuing flush of Memtable-LocationInfo@1306980591(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,251 Writing Memtable-LocationInfo@1306980591(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,262 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-52-Data.db,].  1,564 to 1,402 (~89% of original) bytes for 4 keys at 0.021919MB/s.  Time: 61ms.
 INFO 09:35:38,294 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-54-Data.db (89 bytes)
 INFO 09:35:38,294 Node /10.34.33.16 has restarted, now UP
 INFO 09:35:38,295 InetAddress /10.34.33.16 is now UP
 INFO 09:35:38,296 Node /10.34.33.16 state jump to normal
 INFO 09:35:38,296 Node /10.39.107.114 is now part of the cluster
 INFO 09:35:38,297 InetAddress /10.39.107.114 is now UP
 INFO 09:35:38,298 Enqueuing flush of Memtable-LocationInfo@1038389338(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,299 Writing Memtable-LocationInfo@1038389338(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,311 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-55-Data.db (89 bytes)
 INFO 09:35:38,312 Node /10.196.79.240 is now part of the cluster
 INFO 09:35:38,312 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-52-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-55-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-54-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-51-Data.db')]
 INFO 09:35:38,313 InetAddress /10.196.79.240 is now UP
 INFO 09:35:38,314 Enqueuing flush of Memtable-LocationInfo@1850278722(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,315 Writing Memtable-LocationInfo@1850278722(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,354 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-56-Data.db (89 bytes)
 INFO 09:35:38,355 Node /10.99.39.242 has restarted, now UP
 INFO 09:35:38,356 InetAddress /10.99.39.242 is now UP
 INFO 09:35:38,356 Node /10.99.39.242 state jump to normal
 INFO 09:35:38,357 Node /10.118.233.198 has restarted, now UP
 INFO 09:35:38,358 InetAddress /10.118.233.198 is now UP
 INFO 09:35:38,358 Node /10.118.233.198 state jump to normal
 INFO 09:35:38,359 Node /10.82.210.172 is now part of the cluster
 INFO 09:35:38,359 InetAddress /10.82.210.172 is now UP
 INFO 09:35:38,364 Enqueuing flush of Memtable-LocationInfo@786665924(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,364 Writing Memtable-LocationInfo@786665924(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,439 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-58-Data.db (89 bytes)
 INFO 09:35:38,440 Node /10.80.41.192 is now part of the cluster
 INFO 09:35:38,440 InetAddress /10.80.41.192 is now UP
 INFO 09:35:38,442 Enqueuing flush of Memtable-LocationInfo@1647844754(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,442 Writing Memtable-LocationInfo@1647844754(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,451 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-57-Data.db,].  1,669 to 1,515 (~90% of original) bytes for 4 keys at 0.010470MB/s.  Time: 138ms.
 INFO 09:35:38,459 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-60-Data.db (89 bytes)
 INFO 09:35:38,459 Node /10.76.243.129 is now part of the cluster
 INFO 09:35:38,460 Compacting [SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-56-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-58-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-57-Data.db'), SSTableReader(path='/raid0/cassandra/data/system/LocationInfo-h-60-Data.db')]
 INFO 09:35:38,460 InetAddress /10.76.243.129 is now UP
 INFO 09:35:38,462 Enqueuing flush of Memtable-LocationInfo@585652261(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,462 Writing Memtable-LocationInfo@585652261(35/43 serialized/live bytes, 1 ops)
 INFO 09:35:38,478 Completed flushing /raid0/cassandra/data/system/LocationInfo-h-61-Data.db (89 bytes)
 INFO 09:35:38,486 Node /10.34.42.28 state jump to normal
 INFO 09:35:38,487 Node /10.34.70.73 state jump to normal
 INFO 09:35:38,488 Node /10.99.86.251 state jump to normal
 INFO 09:35:38,489 Node /10.118.94.62 state jump to normal
 INFO 09:35:38,489 Node /10.80.110.28 state jump to normal
 INFO 09:35:38,490 Node /10.80.210.38 state jump to normal
 INFO 09:35:38,491 Node /10.40.177.173 state jump to normal
 INFO 09:35:38,493 Node /10.101.41.8 state jump to normal
 INFO 09:35:38,493 Node /10.113.42.21 state jump to normal
 INFO 09:35:38,494 Node /10.85.11.38 state jump to normal
 INFO 09:35:38,495 Node /10.34.159.72 state jump to normal
 INFO 09:35:38,496 Node /10.34.74.58 state jump to normal
 INFO 09:35:38,497 Node /10.84.205.93 state jump to normal
 INFO 09:35:38,497 Node /10.118.179.67 state jump to normal
 INFO 09:35:38,498 Node /10.34.33.16 state jump to normal
 INFO 09:35:38,499 Node /10.196.79.240 state jump to normal
 INFO 09:35:38,500 Node /10.118.233.198 state jump to normal
 INFO 09:35:38,501 Node /10.80.41.192 state jump to normal
 INFO 09:35:38,502 Node /10.76.243.129 state jump to normal
 INFO 09:35:38,508 Node /10.118.185.68 state jump to normal
 INFO 09:35:38,524 Node /10.118.230.219 state jump to normal
 INFO 09:35:38,536 Compacted to [/raid0/cassandra/data/system/LocationInfo-h-62-Data.db,].  1,782 to 1,620 (~90% of original) bytes for 4 keys at 0.020328MB/s.  Time: 76ms.
 INFO 09:35:38,537 Node /10.80.110.28 state jump to normal
 INFO 09:35:38,537 Node /10.40.177.173 state jump to normal
 INFO 09:35:38,538 Node /10.101.41.8 state jump to normal
 INFO 09:35:38,539 Node /10.116.241.250 state jump to normal
 INFO 09:35:38,540 Node /10.194.29.156 state jump to normal
 INFO 09:35:38,540 Node /10.34.74.58 state jump to normal
 INFO 09:35:38,541 Node /10.40.22.224 state jump to normal
 INFO 09:35:38,542 Node /10.32.79.134 state jump to normal
 INFO 09:35:38,543 Node /10.39.107.114 state jump to normal
 INFO 09:35:38,543 Node /10.99.39.242 state jump to normal
 INFO 09:35:38,550 Node /10.77.63.49 state jump to normal
 INFO 09:35:38,550 Node /10.34.42.28 state jump to normal
 INFO 09:35:38,551 Node /10.116.134.183 state jump to normal
 INFO 09:35:38,553 Node /10.76.243.129 state jump to normal
 INFO 09:35:38,557 Node /10.202.67.43 state jump to normal
 INFO 09:35:38,558 Node /10.118.94.62 state jump to normal
 INFO 09:35:38,562 Node /10.116.215.81 state jump to normal
 INFO 09:35:38,563 Node /10.80.210.38 state jump to normal
 INFO 09:35:38,564 Node /10.205.23.34 state jump to normal
 INFO 09:35:38,565 Node /10.39.107.114 state jump to normal
{CODE}",scode,j.casares,Low,Resolved,Fixed,28/Oct/11 18:33,16/Apr/19 09:32
Bug,CASSANDRA-3418,12529307,Counter decrements require a space around the minus sign but not around the plus sign,"UPDATE validation_cf_counter SET test=test+1 WHERE id='test_key' => Success
UPDATE validation_cf_counter SET test=test + 1 WHERE id='test_key' => Success
UPDATE validation_cf_counter SET test=test - 1 WHERE id='test_key' => Success
UPDATE validation_cf_counter SET test=test-1 WHERE id='test_key' => Failure (line 1:38 no viable alternative at input 'test')

",xedin,kreynolds,Low,Resolved,Fixed,28/Oct/11 19:45,16/Apr/19 09:32
Bug,CASSANDRA-3421,12529365,Can create a table with DecimalType comparator but CQL explodes trying to actually use it.,"CREATE KEYSPACE CassandraCQLTestKeyspace WITH strategy_class='org.apache.cassandra.locator.SimpleStrategy' AND strategy_options:replication_factor=1

USE CassandraCQLTestKeyspace

CREATE COLUMNFAMILY comparator_cf_decimal (id text PRIMARY KEY) WITH comparator='DecimalType'

INSERT INTO comparator_cf_decimal (id, 15.333) VALUES ('test', 'test')

# This leads to failure
SELECT 15.333 FROM comparator_cf_decimal WHERE id = 'test' 

ERROR 12:22:56,909 Internal error processing execute_cql_query
java.nio.BufferUnderflowException
	at java.nio.Buffer.nextGetIndex(Buffer.java:480)
	at java.nio.HeapByteBuffer.getInt(HeapByteBuffer.java:336)
	at org.apache.cassandra.cql.jdbc.JdbcDecimal.compose(JdbcDecimal.java:90)
	at org.apache.cassandra.db.marshal.DecimalType.compose(DecimalType.java:43)
	at org.apache.cassandra.db.marshal.DecimalType.compare(DecimalType.java:38)
	at org.apache.cassandra.db.marshal.DecimalType.compare(DecimalType.java:30)
	at java.util.TreeMap.getEntryUsingComparator(TreeMap.java:351)
	at java.util.TreeMap.getEntry(TreeMap.java:322)
	at java.util.TreeMap.get(TreeMap.java:255)
	at org.apache.cassandra.db.TreeMapBackedSortedColumns.getColumn(TreeMapBackedSortedColumns.java:138)
	at org.apache.cassandra.db.AbstractColumnContainer.getColumn(AbstractColumnContainer.java:128)
	at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:627)
	at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1236)
	at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
	at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
	at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)
",ardot,kreynolds,Normal,Resolved,Fixed,29/Oct/11 16:25,16/Apr/19 09:32
Bug,CASSANDRA-3422,12529368,Can create a Column Family with comparator CounterColumnType which is subsequently unusable,"It's probably the case that this shouldn't be allowed at all but one is currently allowed to create a Column Family with comparator CounterColumnType which then appears unusable.

CREATE COLUMNFAMILY comparator_cf_counter (id text PRIMARY KEY) WITH comparator=CounterColumnType

# Fails
UPDATE comparator_cf_counter SET 1=1 + 1 WHERE id='test_key'

Error => invalid operation for non commutative columnfamily comparator_cf_counter",slebresne,kreynolds,Low,Resolved,Fixed,29/Oct/11 16:46,16/Apr/19 09:32
Bug,CASSANDRA-3427,12529483,"CompressionMetadata is not shared across threads, we create a new one for each read","The CompressionMetada holds the compressed block offsets in memory. Without being absolutely huge, this is still of non-negligible size as soon as you have a bit of data in the DB. Reallocating this for each read is a very bad idea.

Note that this only affect range queries, since ""normal"" queries uses CompressedSegmentedFile that does reuse a unique CompressionMetadata instance.

( Background: http://thread.gmane.org/gmane.comp.db.cassandra.user/21362 )",slebresne,slebresne,Normal,Resolved,Fixed,31/Oct/11 13:57,16/Apr/19 09:32
Bug,CASSANDRA-3433,12529528,Describe ring is broken,CASSANDRA-2882 broke describe_ring by causing the 'endpoints' field to contain the rpc address rather than the listen address. the rpc_address belongs in the 'rpc_endpoints' field. Hence the name.,nickmbailey,nickmbailey,Normal,Resolved,Fixed,31/Oct/11 18:30,16/Apr/19 09:32
Bug,CASSANDRA-3436,12529565,CQL Metadata has inconsistent schema nomenclature,"The dumped object below shows that the default_name_type and the default_value_type are referenced inconsistently .. default_name_type should probably use the shortened version like everything else.

--- !ruby/object:CassandraCQL::Thrift::CqlResult 
rows: 
- !ruby/object:CassandraCQL::Thrift::CqlRow 
  columns: 
  - !ruby/object:CassandraCQL::Thrift::Column 
    name: id
    timestamp: -1
    value: test string
  - !ruby/object:CassandraCQL::Thrift::Column 
    name: test_column
    timestamp: 1320097088551000
    value: test
  key: test string
schema: !ruby/object:CassandraCQL::Thrift::CqlMetadata 
  default_name_type: org.apache.cassandra.db.marshal.UTF8Type
  default_value_type: UTF8Type
  name_types: 
    id: AsciiType
  value_types: 
    id: AsciiType
    test_column: UTF8Type
type: 1",jbellis,kreynolds,Low,Resolved,Fixed,31/Oct/11 21:43,16/Apr/19 09:32
Bug,CASSANDRA-3437,12529618,invalidate / unregisterSSTables is confused,"invalidate doesn't call unregisterSSTables, and vice versa, making it easy to get yourself into a situation that ""shouldn't happen.""  This is causing test failures post-CASSANDRA-3116.",jbellis,jbellis,Normal,Resolved,Fixed,01/Nov/11 04:07,16/Apr/19 09:32
Bug,CASSANDRA-3438,12529656,sstableloader fails,"Ticket at the request of driftx in IRC.  

I've attached the files I'm attempting to load (this is fabricated test data for 100 keys).  I generated this using SSTableSimpleUnsortedWriter. I have four dedicated nodes (mine, not ec2 or other cloud host) that I'm using, we'll call them A,B,C, and D.  I first start cassandra on node A and create the test keyspace and CF:

{code}
create keyspace test
  with placement_strategy = 'SimpleStrategy'
  and strategy_options = {replication_factor : 1}
  and durable_writes = true;

use test;

create column family test
  with column_type = 'Super'
  and comparator = 'UTF8Type'
  and subcomparator = 'UTF8Type'
  and default_validation_class = 'UTF8Type'
  and key_validation_class = 'UTF8Type'
  and rows_cached = 0.0
  and row_cache_save_period = 0
  and row_cache_keys_to_save = 2147483647
  and keys_cached = 200000.0
  and key_cache_save_period = 14400
  and read_repair_chance = 1.0
  and gc_grace = 864000
  and min_compaction_threshold = 4
  and max_compaction_threshold = 32
  and replicate_on_write = true
  and row_cache_provider = 'ConcurrentLinkedHashCacheProvider'
  and compaction_strategy = 'org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy';
{code}

Then I perform a load using sstableloader from node D, which works fine with the following output:

{code}
Starting client (and waiting 30 seconds for gossip) ...
 INFO 09:12:17,660 Loading settings from file:/opt/apache-cassandra-1.0.1/conf/cassandra.yaml
 INFO 09:12:17,761 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:12:17,771 Global memtable threshold is enabled at 75MB
 INFO 09:12:17,917 Starting up client gossip
 INFO 09:12:17,934 Starting Messaging Service on port 7000
 INFO 09:12:19,941 Node /172.21.31.244 is now part of the cluster
 INFO 09:12:19,941 InetAddress /172.21.31.244 is now UP
 INFO 09:12:48,003 Opening test/test-h-1 (2884 bytes)
 INFO 09:12:48,017 JNA not found. Native methods will be disabled.
Streaming revelant part of test/test-h-1-Data.db to [/172.21.31.244]
 INFO 09:12:48,052 Stream context metadata [test/test-h-1-Data.db sections=1 progress=0/2884 - 0%], 1 sstables.
 INFO 09:12:48,053 Streaming to /172.21.31.244

progress: [/172.21.31.244 0/1 (0)] [total: 0 - 0MB/s (avg: 0MB/s)] INFO 09:12:48,103 Shutting down MessageService...
 INFO 09:12:48,103 Waiting for in-progress requests to complete
 INFO 09:12:48,104 MessagingService shutting down server thread.
{code}

Then I start over by shutting cassandra, deleting all of the data and commitlog dirs, starting cassandra on Node A and Node B and creating the same keyspace and CF.  When I run the loader against that, I get:

{code}
Starting client (and waiting 30 seconds for gossip) ...
 INFO 09:15:09,316 Loading settings from file:/opt/apache-cassandra-1.0.1/conf/cassandra.yaml
 INFO 09:15:09,417 DiskAccessMode 'auto' determined to be mmap, indexAccessMode is mmap
 INFO 09:15:09,427 Global memtable threshold is enabled at 75MB
 INFO 09:15:09,572 Starting up client gossip
 INFO 09:15:09,591 Starting Messaging Service on port 7000
 INFO 09:15:10,777 Node /172.21.31.244 is now part of the cluster
 INFO 09:15:10,778 InetAddress /172.21.31.244 is now UP
 INFO 09:15:10,780 Node /172.21.31.245 is now part of the cluster
 INFO 09:15:10,781 InetAddress /172.21.31.245 is now UP
 INFO 09:15:39,664 Opening test/test-h-1 (2884 bytes)
 INFO 09:15:39,691 JNA not found. Native methods will be disabled.
Streaming revelant part of test/test-h-1-Data.db to [/172.21.31.244, /172.21.31.245]
 INFO 09:15:39,730 Stream context metadata [test/test-h-1-Data.db sections=1 progress=0/274 - 0%], 1 sstables.
 INFO 09:15:39,731 Streaming to /172.21.31.244
 INFO 09:15:39,743 Stream context metadata [test/test-h-1-Data.db sections=2 progress=0/2610 - 0%], 1 sstables.
 INFO 09:15:39,743 Streaming to /172.21.31.245

progress: [/172.21.31.244 0/1 (0)] [/172.21.31.245 0/1 (0)] [total: 0 - 0MB/s (avg: 0MB/s)]Exception in thread ""MiscStage:1"" java.lang.AssertionError: Reference counter -1 for test/test-h-1-Data.db
	at org.apache.cassandra.io.sstable.SSTableReader.releaseReference(SSTableReader.java:715)
	at org.apache.cassandra.streaming.StreamOutSession.startNext(StreamOutSession.java:123)
	at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:59)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
Exception in thread ""MiscStage:2"" java.lang.AssertionError: Reference counter -2 for test/test-h-1-Data.db
	at org.apache.cassandra.io.sstable.SSTableReader.releaseReference(SSTableReader.java:715)
	at org.apache.cassandra.streaming.StreamOutSession.close(StreamOutSession.java:150)
	at org.apache.cassandra.streaming.StreamOutSession.close(StreamOutSession.java:132)
	at org.apache.cassandra.streaming.StreamReplyVerbHandler.doVerb(StreamReplyVerbHandler.java:67)
	at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
	at java.lang.Thread.run(Thread.java:619)
progress: [/172.21.31.244 1/1 (100)] [/172.21.31.245 1/1 (100)] [total: 100 - 0MB/s (avg: 0MB/s)]
Waiting for targets to rebuild indexes ...
{code}

After that, it never exists.  There are on errors in the logs on the server side for either node.  Additional tests with larger inputs that show the same general error show slightly different behavior, specifically the progress on all but the first node gets past 1/N.  For example, this is the last line on a test of a real data set that had 16 sstables: ""progress: [/172.21.31.244 16/16 (100)] [/172.21.31.245 1/16 (6)] [total: 19 - 0MB/s (avg: 3MB/s)]]""... and it never progresses from there, and avg drops to zero over time indicating nothing is happening.  

I haven't replicated on any previous versions. ",slebresne,jeremypinkham,Normal,Resolved,Fixed,01/Nov/11 13:35,16/Apr/19 09:32
Bug,CASSANDRA-3440,12529794,local writes timing out cause attempt to hint to self,"As reported by Ramash Natarajan on the mailing list:

{noformat}
We have a 8 node cassandra cluster running 1.0.1. After running a load
test for a day we are seeing this exception in system.log file.

ERROR [EXPIRING-MAP-TIMER-1] 2011-11-01 13:20:45,350
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[EXPIRING-MAP-TIMER-1,5,main]
java.lang.AssertionError: /10.19.102.12
       at org.apache.cassandra.service.StorageProxy.scheduleLocalHint(StorageProxy.java:339)
       at org.apache.cassandra.net.MessagingService.scheduleMutationHint(MessagingService.java:201)
       at org.apache.cassandra.net.MessagingService.access$500(MessagingService.java:64)
       at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:175)
       at org.apache.cassandra.net.MessagingService$2.apply(MessagingService.java:152)
       at org.apache.cassandra.utils.ExpiringMap$1.run(ExpiringMap.java:89)
       at java.util.TimerThread.mainLoop(Timer.java:512)
       at java.util.TimerThread.run(Timer.java:462)
{noformat}",jbellis,jbellis,Normal,Resolved,Fixed,02/Nov/11 02:11,16/Apr/19 09:32
Bug,CASSANDRA-3441,12529800,PerRowSecondaryIndexes skip the first column on update,PerRowSecondaryIndexes skip the initial field on apply,tjake,tjake,Normal,Resolved,Fixed,02/Nov/11 03:03,16/Apr/19 09:32
Bug,CASSANDRA-3445,12529869,"recognize that ""SELECT first ... *"" isn't really ""SELECT *""","QueryProcessor includes the row key in ""first *"" because it mistakenly thinks the full row is being requested.  ""first *"" should really be treated like a slice.",xedin,jbellis,Low,Resolved,Fixed,02/Nov/11 14:48,16/Apr/19 09:32
Bug,CASSANDRA-3446,12529885,Problem SliceByNamesReadCommand on super column family after flush operation,"I'm having a problem with doing a multiget_slice on a super column family
after its first flush. Updates to the column values work properly, but
trying to retrieve the updated values using a multiget_slice operation fail
to get the updated values. Instead they return the values from before the
flush. The problem is not apparent with standard column families.

I've seen this problem in Cassandra v1.0.0 and v1.0.1. The problem
is not present in Cassandra v0.7.6.

Steps to reproduce:

   1. Create one or more super column entries
   2. Verify the sub column values can be updated and that you can retrieve
   the new values
   3. Use nodetool to flush the column family or restart cassandra
   4. Update the sub column values
   5. Verify they have been updated using cassandra-cli
   6. Verify you *DO NOT* get the updated values when doing a
   multiget_slice; instead you get the old values from before the flush

You can get the most recent value by doing a flush followed by a major
compaction. However, future updates are not retrieved properly either.

With debug turned on, it looks like the multiget_slice query uses the
following command/consistency level:
SliceByNamesReadCommand(table='test_cassandra', key=666f6f,
columnParent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', columns=[foo,])/QUORUM.

Cassandra-cli uses the following command/consistency level for a get_slice:
SliceFromReadCommand(table='test_cassandra', key='666f6f',
column_parent='QueryPath(columnFamilyName='test', superColumnName='null',
columnName='null')', start='', finish='', reversed=false,
count=1000000)/QUORUM

Notice the test program gets 'bar2' for the column values and cassandra-cli
gets 'bar3' for the column values:

tcpdump from test program using hector-core:1.0-1

16:46:07.424562 IP iam.47158 > iam.9160: Flags [P.], seq 55:138, ack 30,
win 257, options [nop,nop,TS val 27474096 ecr 27474095], length 83
E....#@.@.PK.........6#.....].8......{.....
..8...8.........multiget_slice................foo..........test................foo.........
16:46:07.424575 IP iam.9160 > iam.47158: Flags [.], ack 138, win 256,
options [nop,nop,TS val 27474096 ecr 27474096], length 0
E..4..@.@.<.........#..6].8..........(.....
..8...8.
16:46:07.428771 IP iam.9160 > iam.47158: Flags [P.], seq 30:173, ack 138,
win 256, options [nop,nop,TS val 27474097 ecr 27474096], length 143
@.@.<&........#..6].8................
............foo...............foo...............foo1.......bar2
........6h........foo2.......bar2
........I.....


tcpdump of cassandra-cli:

16:30:55.945123 IP iam.47134 > iam.9160: Flags [P.], seq 370:479, ack 5310,
win 387, options [nop,nop,TS val 27246226 ecr 27241207], length 109
E.....@.@.9q..........#..n.X\
.............
................get_range_slices..............test.........................................................d.........
16:30:55.945152 IP iam.9160 > iam.47134: Flags [.], ack 479, win 256,
options [nop,nop,TS val 27246226 ecr 27246226], length 0
E..4..@.@."".........#...\
...n.......(.....
........
16:30:55.949245 IP iam.9160 > iam.47134: Flags [P.], seq 5310:5461, ack
479, win 256, options [nop,nop,TS val 27246227 ecr 27246226], length 151
E.....@.@.""V........#...\
...n.............
....................get_range_slices...................foo..................foo...............foo1.......bar3
........&.........foo2.......bar3
........: .....",jbellis,rheostat,Urgent,Resolved,Fixed,02/Nov/11 16:15,16/Apr/19 09:32
Bug,CASSANDRA-3449,12529992,Missing null check in digest retry part of read path,,jbellis,jbellis,Normal,Resolved,Fixed,03/Nov/11 03:10,16/Apr/19 09:32
Bug,CASSANDRA-3450,12530083,maybeInit in ColumnFamilyRecordReader can cause rows to be empty but not null,"1) In {{ColumnFamilyRecordReader}} {{isPredicateEmpty}} needs bracing to correctly place the {{else if}} to the properly controlling {{if}}.

1a) {{isPredicateEmpty}} should use an || in the getSlice_range predicate rather than &&.

2) In {{ColumnFamilyRecordReader}} {{computeNext()}} calls {{maybeInit()}} and then if {{ros}} is not null it is indexed into.  {{maybeInit()}} could fetch new data, determine the associated slice predicate is empty, and end up removing all the rows if all columns turned out to be empty.  There is no check for {{rows.isEmpty()}} after the possible removal of all rows.

",lannyripple,lannyripple,Normal,Resolved,Fixed,03/Nov/11 17:20,16/Apr/19 09:32
Bug,CASSANDRA-3451,12530100,estimated row sizes regression,"CASSANDRA-2753 broke the histogram collection; it got the histogram for column count (which can go up to 2B) switched with the one for row size in bytes (which goes up to ~1.5PB).  So any row over 2GB, will break things.",jbellis,jbellis,Normal,Resolved,Fixed,03/Nov/11 19:15,16/Apr/19 09:32
Bug,CASSANDRA-3457,12530367,Make cqlsh look for a suitable python version,"On RHEL 5, which I guess we still want to support, the default ""python"" in the path is still 2.4. cqlsh does use a fair number of python features introduced in 2.5, like collections.defaultdict, functools.partial, generators. We can require RHEL 5 users to install a later python from EPEL, but we'd have to call it as 'python2.5', or 'python2.6', etc.

So rather than take the time to vet everything against python2.4, we may want to make a wrapper script for cqlsh that checks for the existence of python2.7, 2.6, and 2.5, and calls the appropriate one to run the real cqlsh.",thepaul,thepaul,Low,Resolved,Fixed,04/Nov/11 19:39,16/Apr/19 09:32
Bug,CASSANDRA-3466,12530617,Hinted handoff not working after rolling upgrade from 0.8.7 to 1.0.2,"While testing rolling upgrades from 0.8.7 to 1.0.2 on a test cluster I've noticed that hinted hand-off didn't always work properly. Hints generated on an upgraded node does not seem to be delivered to other newly upgraded nodes once they rejoin the ring. They only way I've found to get a node to deliver its hints is to restart it.

Here's some steps to reproduce this issue:

1. Install cassandra 0.8.7 on node1 and node2 using default settings.
2. Create keyspace foo with {replication_factor: 2}. Create column family bar
3. Shutdown node2 
4. Insert data into bar and verify that HintsColumnFamily on node2 contains hints
5. Start node2 and verify that hinted handoff is performed and HintsColumnFamily becomes empty again.

6. Upgrade and restart node1
7. Shutdown node2 
8. Insert data into bar and verify that HintsColumnFamily on node2 contains hints
9. Upgrade and start node2
10. Notice that hinted handoff is *not* performed when ""node2"" comes back. (Only if node1 is restarted)
",jbellis,jborgstrom,Normal,Resolved,Fixed,07/Nov/11 19:16,16/Apr/19 09:32
Bug,CASSANDRA-3467,12530627,"get_slice, super column family with UUIDType as column comparator","get_slice with more than one column selected by predicate fails, when comparator is set to (Lexical|Time)UUIDType and more than one column is being selected.

{code}
// Sample data:
create column family A with column_type = Super and comparator = LexicalUUIDType and subcomparator = UTF8Type and default_validation_class = UTF8Type;
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['a'] = 'A';
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['b'] = 'B';
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['c'] = 'C';
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['d'] = 'D';
set A[ascii('key')][lexicaluuid('b139337e-fb6d-41e1-a868-1db7f2a52a42')]['e'] = 'E';

// Failed call
$client->get_slice(
    'key', 
    new ColumnParent(array(
        'column_family'=>'A', 
        'super_column'=>base64_decode('sTkzfvttQeGoaB238qUqQg==')
    )), 
    new SlicePredicate(array(
        'column_names'=>array('a', 'b')
    )), 
    1
);

// Exception thrown
ERROR [ReadStage:302] 2011-11-07 21:29:30,339 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[ReadStage:302,5,main]
java.lang.RuntimeException: java.lang.IndexOutOfBoundsException
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1269)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.IndexOutOfBoundsException
	at java.nio.Buffer.checkIndex(Buffer.java:520)
	at java.nio.HeapByteBuffer.getLong(HeapByteBuffer.java:391)
	at org.apache.cassandra.utils.UUIDGen.getUUID(UUIDGen.java:67)
	at org.apache.cassandra.db.marshal.LexicalUUIDType.compare(LexicalUUIDType.java:58)
	at org.apache.cassandra.db.marshal.LexicalUUIDType.compare(LexicalUUIDType.java:31)
	at java.util.TreeMap.put(TreeMap.java:530)
	at java.util.TreeSet.add(TreeSet.java:238)
	at java.util.AbstractCollection.addAll(AbstractCollection.java:305)
	at java.util.TreeSet.addAll(TreeSet.java:295)
	at org.apache.cassandra.db.CollationController.collectTimeOrderedData(CollationController.java:98)
	at org.apache.cassandra.db.CollationController.getTopLevelColumns(CollationController.java:61)
	at org.apache.cassandra.db.ColumnFamilyStore.getTopLevelColumns(ColumnFamilyStore.java:1278)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1164)
	at org.apache.cassandra.db.ColumnFamilyStore.getColumnFamily(ColumnFamilyStore.java:1131)
	at org.apache.cassandra.db.Table.getRow(Table.java:378)
	at org.apache.cassandra.db.SliceByNamesReadCommand.getRow(SliceByNamesReadCommand.java:58)
	at org.apache.cassandra.service.StorageProxy$LocalReadRunnable.runMayThrow(StorageProxy.java:797)
	at org.apache.cassandra.service.StorageProxy$DroppableRunnable.run(StorageProxy.java:1265)
	... 3 more

// This works though
$client->get_slice(
    'key', 
    new ColumnParent(array(
        'column_family'=>'A', 
        'super_column'=>base64_decode('sTkzfvttQeGoaB238qUqQg==')
    )), 
    new SlicePredicate(array(
        'column_names'=>array('a')
    )), 
    1
);

// This works too
$client->get_slice('key', 
	new ColumnParent(array(
		'column_family'=>'A', 
		'super_column'=>base64_decode('sTkzfvttQeGoaB238qUqQg==')
	)), 
	new SlicePredicate(array(
		'slice_range'=>new SliceRange(array(
			'start'=>'', 
			'finish'=>'',
			'reversed'=>false,
			'count'=>100
		))
	)), 1);
{code}

Regards
ales",rbranson,alesl,Low,Resolved,Fixed,07/Nov/11 20:33,16/Apr/19 09:32
Bug,CASSANDRA-3472,12530704,Actually uses efficient cross DC writes,"CASSANDRA-2138 introduced the following code:
{noformat}
if (dataCenter.equals(localDataCenter) || StorageService.instance.useEfficientCrossDCWrites())
{
    // direct writes to local DC or old Cassadra versions
    for (InetAddress destination : messages.getValue())
        MessagingService.instance().sendRR(message, destination, handler);
}
else
{
    // Non-local DC. First endpoint in list is the destination for this group
{noformat}
A 'not' is missing on that useEfficientCrossDCWrites call (which does return true for any version >= 0.7.1).

A simple fix would be to add the missing !, but as said a comment, all this code should have been removed in 0.8 since it was detecting nodes before 0.7.1, but direct upgrade from pre-0.7.1 to 0.8+ is not supported. So let's just completely remove that code now.",slebresne,slebresne,Low,Resolved,Fixed,08/Nov/11 10:36,16/Apr/19 09:32
Bug,CASSANDRA-3473,12530737,Missing null check in CQL QueryProcessor,,slebresne,slebresne,Normal,Resolved,Fixed,08/Nov/11 17:12,16/Apr/19 09:32
Bug,CASSANDRA-3475,12530865,LoadBroadcaster never removes endpoints,As the title says.,brandon.williams,brandon.williams,Low,Resolved,Fixed,09/Nov/11 11:57,16/Apr/19 09:32
Bug,CASSANDRA-3481,12530929,"During repair, ""incorrect data size"" & ""Connection reset"" errors. Repair unable to complete.","This has been happening since 1.0.2. I wasn't on 1.0 for very long but I'm fairly certain repair was working ok. Repair worked decently for me in 0.8 (data bloat sucked). All my SSTables are version h.

On one node:

java.lang.AssertionError: incorrect row data size 596045 written to /mnt/cassandra/data/TRProd/Metrics1m-tmp-h-25036-Data.db; correct is 586675
	at org.apache.cassandra.io.sstable.SSTableWriter.appendFromStream(SSTableWriter.java:253)
	at org.apache.cassandra.streaming.IncomingStreamReader.streamIn(IncomingStreamReader.java:146)
	at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:87)
	at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
	at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)

On the other node:

4999 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-24953-Data.db sections=1707 progress=0/1513497639 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25000-Data.db sections=635 progress=0/53400713 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25002-Data.db sections=570 progress=0/709993 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25003-Data.db sections=550 progress=0/449498 - 0%, /mnt/cassandra/data/TRProd/Metrics1m-h-25005-Data.db sections=516 progress=0/316301 - 0%], 6 sstables.
 INFO [StreamStage:1] 2011-11-09 19:45:22,795 StreamOutSession.java (line 203) Streaming to /10.38.69.192
ERROR [Streaming:1] 2011-11-09 19:47:47,964 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Streaming:1,1,main]
java.lang.RuntimeException: java.net.SocketException: Connection reset
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
	at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:133)
	at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
	at com.ning.compress.lzf.LZFOutputStream.write(LZFOutputStream.java:97)
	at org.apache.cassandra.streaming.FileStreamTask.write(FileStreamTask.java:181)
	at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:145)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
ERROR [Streaming:1] 2011-11-09 19:47:47,970 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Streaming:1,1,main]
java.lang.RuntimeException: java.net.SocketException: Connection reset
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.net.SocketException: Connection reset
	at java.net.SocketOutputStream.socketWrite(SocketOutputStream.java:96)
	at java.net.SocketOutputStream.write(SocketOutputStream.java:136)
	at com.ning.compress.lzf.ChunkEncoder.encodeAndWriteChunk(ChunkEncoder.java:133)
	at com.ning.compress.lzf.LZFOutputStream.writeCompressedBlock(LZFOutputStream.java:203)
	at com.ning.compress.lzf.LZFOutputStream.write(LZFOutputStream.java:97)
	at org.apache.cassandra.streaming.FileStreamTask.write(FileStreamTask.java:181)
	at org.apache.cassandra.streaming.FileStreamTask.stream(FileStreamTask.java:145)
	at org.apache.cassandra.streaming.FileStreamTask.runMayThrow(FileStreamTask.java:91)
	at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
	... 3 more
",slebresne,efalcao,Normal,Resolved,Fixed,09/Nov/11 21:31,16/Apr/19 09:32
Bug,CASSANDRA-3482,12531051,Flush Assertion Error - CF size changed during serialization,"I have seen the following assert in the logs - there are no other suspicious or unexpected log messages.

INFO [FlushWriter:9] 2011-11-10 13:08:58,882 Memtable.java (line 237) Writing Memtable-UserData@1388955390(25676955/430716097 serialized/live bytes, 478913 ops)
ERROR [FlushWriter:9] 2011-11-10 13:08:59,513 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[FlushWriter:9,5,main]
java.lang.AssertionError: CF size changed during serialization: was 4 initially but 3 written
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeForSSTable(ColumnFamilySerializer.java:94)
        at org.apache.cassandra.db.ColumnFamilySerializer.serializeWithIndexes(ColumnFamilySerializer.java:112)
        at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:177)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:264)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:47)
        at org.apache.cassandra.db.Memtable$4.runMayThrow(Memtable.java:289)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)

Once the error occurs, further MemtablePostFlusher tasks are blocked:

nodetool tpstats:
  Pool Name                    Active   Pending      Completed   Blocked  All time blocked
  MemtablePostFlusher               1        18             16         0                 0

It *seems* that all further flushed for the particular CF (in this case UserData) will also result in the same assertion error. Restarting the node fixes the problem.",jbellis,dhendry,Urgent,Resolved,Fixed,10/Nov/11 21:05,16/Apr/19 09:32
Bug,CASSANDRA-3483,12531095,Support bringing up a new datacenter to existing cluster without repair,"Was talking to Brandon in irc, and we ran into a case where we want to bring up a new DC to an existing cluster. He suggested from jbellis the way to do it currently was set strategy options of dc2:0, then add the nodes. After the nodes are up, change the RF of dc2, and run repair. 

I'd like to avoid a repair as it runs AES and is a bit more intense than how bootstrap works currently by just streaming ranges from the SSTables. Would it be possible to improve this functionality (adding a new DC to existing cluster) than the proposed method? We'd be happy to do a patch if we got some input on the best way to go about it.
",scode,lenn0x,Normal,Resolved,Fixed,11/Nov/11 04:59,16/Apr/19 09:32
Bug,CASSANDRA-3484,12531167,Bizarre Compaction Manager Behaviour,"It seems the CompactionManager has gotten itself into a bad state. My 1.0.2 node has been up for 20 hours now - checking via JMX, the compaction manager is reporting that it has completed 14,797,412,000 tasks. Yep, thats right 14 billion tasks and increasing at a rate of roughly 208,400/second. 

I should point out that I am currently running a major compaction on the node. My theory is that this problem was introduced by CASSANDRA-3363. It looks like SizeTieredCompactionStrategy.getBackgroundTasks() returns a set of task without consideration for any in-progress compactions. Compactions are only kicked off if task.markSSTablesForCompaction() returns true (CompactionManager line 127) but the task resubmission is based only on the task list not being empty (CompactionManager line 141). Should the logic not be to only reschedule if a task has actually been executed?

I am just waiting now for the major compaction to finish to see if the problem goes away as would be suggested by my theory.",,dhendry,Normal,Resolved,Fixed,11/Nov/11 19:04,16/Apr/19 09:32
Bug,CASSANDRA-3485,12531187,Gossiper.addSavedEndpoint should never add itself,"Somehow, people are running into a situation where nodes are adding themselves to the persisted ring cache.  Since SS is initialized after the Gossiper and calls addSavedEndpoint on it, which inits the nodes with a generation of zero, this ends up with nodes using a generation of zero and thus never being marked as alive.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,11/Nov/11 22:22,16/Apr/19 09:32
Bug,CASSANDRA-3489,12531262,EncryptionOptions should be instantiated,"As the title says, otherwise you get an NPE when the options are missing from the yaml.  It's included in my second patch on CASSANDRA-3045 and is a one line fix.",brandon.williams,brandon.williams,Low,Resolved,Fixed,13/Nov/11 16:32,16/Apr/19 09:32
Bug,CASSANDRA-3491,12531347,Recursion bug in CollationController,"The following stack trace seems to indicate a recursion bug in CollationController

Where the stats collection mutation is itself having stats collected on and so fourth

http://pastebin.com/raw.php?i=35Rt7ryB",jbellis,tjake,Normal,Resolved,Fixed,14/Nov/11 17:54,16/Apr/19 09:32
Bug,CASSANDRA-3492,12531368,Compression option chunk_length is not converted into KB as it should,,slebresne,slebresne,Urgent,Resolved,Fixed,14/Nov/11 20:58,16/Apr/19 09:32
Bug,CASSANDRA-3493,12531370,cqlsh complains when you try to do UPDATE with counter columns,"trying to do a counter column UPDATE in cqlsh causes an ""Invalid syntax"" error:

{noformat}
cqlsh:foo> update brongo SET boo = boo+1 where key='hi';
Invalid syntax at line 1, char 28
  update brongo SET boo = boo+1 where key='hi';
                             ^
{noformat}

This is cause cqlsh's lexer doesn't know that + and - are valid operators in CQL. Don't worry, I'm not trying to make cqlsh be able to parse all CQL with exactness- it tries, in order to provide the best tab completion, but when it fails to parse it can still pass on CQL text to the server. This case is different because it's the lexer that can't understand the operators, before we even get to the parser. We do need a working and correct lexer, along with at least minimal parsing capability, in order to reliably split up statements, tell when the user is changing the keyspace, or SELECTing on a columnfamily with ASSUMEd types.

Also, the parser should be tweaked in a manner similar to CASSANDRA-3418.",thepaul,thepaul,Low,Resolved,Fixed,14/Nov/11 21:09,16/Apr/19 09:32
Bug,CASSANDRA-3496,12531527,Load from `nodetool ring` does not update after cleanup.,"Repro:
Bring up a node.

Insert 1M rows:
127.0.0.1       datacenter1 rack1       Up     Normal  406.92 MB       100.00% 77747037169725419723056812679314618801
(Already looks wrong, 406.92 is higher than I'm used to seeing from a single run of stress)

Bootstrap a second node into the cluster:

162877269496252595336256012556853953561
127.0.0.1       datacenter1 rack1       Up     Normal  407.03 MB       49.96%  77747037169725419723056812679314618801
127.0.0.2       datacenter1 rack1       Up     Normal  157.91 MB       50.04%  162877269496252595336256012556853953561

Cleanup
162877269496252595336256012556853953561
127.0.0.1       datacenter1 rack1       Up     Normal  551.2 MB       49.96%  77747037169725419723056812679314618801
127.0.0.2       datacenter1 rack1       Up     Normal  157.91 MB       50.04%  162877269496252595336256012556853953561

Looks like each operation that adds and removes SSTables only adds to the total and doesn't remove the old sstables from the total size count.

",cywjackson,bcoverston,Normal,Resolved,Fixed,15/Nov/11 20:18,16/Apr/19 09:32
Bug,CASSANDRA-3500,12531818,cqlsh ASSUME doesn't work when using SELECT keyspace.cfname syntax,"After assigning an ASSUME type to some columnfamily CF in keyspace K, if a SELECT is subsequently done on CF while the session is using a different keyspace, the ASSUME does not take effect:

{noformat}
cqlsh> USE ks;
cqlsh:ks> CREATE COLUMNFAMILY cf (key int PRIMARY KEY, col int);
cqlsh:ks> INSERT INTO cf (key, col) VALUES (99, 1633837924);
cqlsh:ks> ASSUME cf(col) VALUES ARE ascii;
cqlsh:ks> SELECT * FROM cf;
 KEY |  col |
  99 | abcd |

cqlsh:ks> USE system;
cqlsh:system> SELECT * FROM ks.cf;
 KEY |        col |
  99 | 1633837924 |

{noformat}

the output from both {{SELECT}}s there should be the same.",thepaul,thepaul,Low,Resolved,Fixed,17/Nov/11 17:22,16/Apr/19 09:32
Bug,CASSANDRA-3501,12531820,Move allows you to move to tokens > 2**127,Currently you can move to tokens greater than what should be the max token in RP.,jbellis,nickmbailey,Low,Resolved,Fixed,17/Nov/11 17:55,16/Apr/19 09:32
Bug,CASSANDRA-3503,12531835,IncomingStreamReader uses socket.getRemoteSocketAddress() which might be diffrent than FB.getBroadcastAddress(),"We can add BCA to the streaming so the receiver can use this to StreamInSession.get(bca, sid)

Currently this causes the repairs to hang when the bca is diffrent than LocalAddress.",vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,17/Nov/11 19:57,16/Apr/19 09:32
Bug,CASSANDRA-3506,12531982,"cassandra.bat does not use SETLOCAL, can cause classpath issues","In bin/cassandra.bat, we don't use SETLOCAL (although we do use ENDLOCAL for some reason), so modifications to the classpath within the batch script persist.  This means that if you run cassandra-1.0.0/bin/cassandra.bat, kill the process, and then run cassandra-1.0.3/bin/cassandra.bat, the 1.0.0 jars will still be in the classpath. ",thobbs,thobbs,Low,Resolved,Fixed,18/Nov/11 19:06,16/Apr/19 09:32
Bug,CASSANDRA-3508,12531999,requiring --debug to see stack traces for failures in cassandra-cli is a terrible idea (aka silent failure is never a valid option),"this manifests itself in cassandra-cli by returning null to the user.  In order to see what the problem was (and in many cases, just to know there was a problem at all) requires running cassandra-cli with ""--debug""",xedin,mdennis,Low,Resolved,Fixed,18/Nov/11 21:47,16/Apr/19 09:32
Bug,CASSANDRA-3509,12532000,cassandra-cli shows org.apache.Cassandra.XXX in example help for replication strategy but it should be cassandra with a lowercase c,"copying and pasting the example doesn't result in a working example and noticing the ""C"" -v- ""c"" is something easy to overlook ",tommysdk,mdennis,Low,Resolved,Fixed,18/Nov/11 21:49,16/Apr/19 09:32
Bug,CASSANDRA-3510,12532118,Incorrect query results due to invalid SSTable.maxTimestamp,"related to CASSANDRA-3446

(sorry this is so long, took me a bit to work through it all and there is a lot of new code :) )
 
h1. Summary

SSTable.maxTimestamp for files created before 1.0 defaults to Long.MIN_VALUE, and this means the wrong data is returned from queries. 
 
h2. Details 

Noticed on a cluster that was upgraded from 0.8.X to 1.X, it then had trouble similar to CASSANDRA-3446. It was rolled back to 0.8 and the migrated to 1.0.3. 

4 Node cluster, all files upgraded to ""hb"" format. 

In a super CF there are situations where a get for a sub columns returns a different value than a get for the column. .e.g. 

{noformat}
[default@XXX] get Users[ascii('username')]['meta']['password'];
=> (column=password, value=3130323130343130, timestamp=1307352647576000)

[default@XX] get Users[ascii('username')]['meta'];     
(snip)       
=> (column=password, value=3034323131303034, timestamp=1319563673493000)
{noformat}

The correct value is the second one. 

I added logging after line 109 in o.a.c.db.CollectionController.collectTimeOrderedData() to log the sstable name and the file max timestamp, this is what I got:

{code:java}
for (SSTableReader sstable : view.sstables)
{
    long currentMaxTs = sstable.getMaxTimestamp();
    logger.debug(String.format(""Got sstable %s and max TS %d"", sstable, currentMaxTs));
    reduceNameFilter(reducedFilter, container, currentMaxTs);
{code}

{noformat}
DEBUG 14:08:46,012 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12348-Data.db') and max TS 1321824847534000
DEBUG 14:08:47,231 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12346-Data.db') and max TS 1321813380793000
DEBUG 14:08:49,879 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12330-Data.db') and max TS -9223372036854775808
DEBUG 14:08:49,880 Got sstable SSTableReader(path='/var/lib/cassandra/data/X/Users-hb-12325-Data.db') and max TS -9223372036854775808
{noformat}

The key I was reading is present in files 12330 and 12325, the first contains the *old / wrong* value with timestamp 1307352647576000 above. The second contains the *new / correct* value with timestamp 1319563673493000.

**Updated:** Incorrect, it was a later file that had the correct value, see the first comment. 

When CollectionController.collectTimeOrderedData() processes the 12325 file (after processing the 12330 file) while looping over the sstables the call to reduceNameFilter() removes the column  from the filter because the column read from the 12330 file has a time stamp of 1307352647576000 and the 12325 file incorrectly has a max time stamp of -9223372036854775808 .

SSTableMetadata is reading the max time stamp from the stats file, but it is Long.MIN_VALUE. I think this happens because scrub creates the SSTableWriter using cfs.createCompactionWriter() which sets the maxTimestamp in the meta data collector according to the maxTimestamp in the meta data for the file(s) that will be scrubbed / compacted. But for pre 1.0 format files the default in SSTableMetadata is Long.MIN_VALUE, (see SSTableMetaData.deserialize() and the ctor). So scrubbing a pre 1.0 file will write stats files that have maxTimestamp as Long.MIN_VALUE.

During scrubbing the SSTableWriter does not update the maxTimestamp because append(AbstractCompactedRow) is called which expects the that cfs.createCompactionWriter() was able to set the correct maxTimestamp on the meta data. Compaction also uses append(AbstractCompactedRow) so may create an SSTable with an incorrect maxTimestamp if one of the input files started life as a pre 1.0 file and has a bad maxTimestamp. 

It looks like the only time the maxTimestamp is calculated is when the SSTable is originally written. So the error from the old files will be carried along. 

e.g. If the files a,b and c have the maxTimestamps 10, 100 and Long.MIN_VALUE compaction will write a SSTable with maxTimestamp 100. However file c may actually contain columns with a timestamp > 100 which will be in the compacted file.

h1. Reproduce

1. Start a clean 0.8.7

2. Add a schema (details of the schema do not matter):
{noformat}
[default@unknown] create keyspace dev;   
5f834620-140b-11e1-0000-242d50cf1fdf
Waiting for schema agreement...
... schemas agree across the cluster
[default@unknown] 
[default@unknown] use dev;
Authenticated to keyspace: dev
[default@dev] 
[default@dev] create column family super_dev with column_type = 'Super' 
...	and key_validation_class = 'AsciiType' and comparator = 'AsciiType' and 
...	subcomparator = 'AsciiType' and default_validation_class = 'AsciiType';
60490720-140b-11e1-0000-242d50cf1fdf
Waiting for schema agreement...
... schemas agree across the cluster
{noformat}

3. Shutdown 0.8.7

4. Start 1.0.3 using the same data. Check the schema version loaded, example below shows the wrong schema is loaded. I stepped the code and the wrong value was read from Migration.getLastMigrationId() due to this bug. 

{noformat}
 INFO [main] 2011-11-21 19:39:08,546 DatabaseDescriptor.java (line 501) Loading schema version 5f834620-140b-11e1-0000-242d50cf1fdf
{noformat}

5. Check the schema using the 1.0.3 CLI 

{noformat}
[default@unknown] use dev;
Authenticated to keyspace: dev
[default@dev] describe;
Keyspace: dev:
  Replication Strategy: org.apache.cassandra.locator.NetworkTopologyStrategy
  Durable Writes: true
    Options: [datacenter1:1]
  Column Families:
[default@dev] 
{noformat}

6. I then did a 1.0.3 scrub and re-started. The correct schema version was read, but stepping the code both Schema SSTables had Long.MIN_VALUE as the maxTimestamp so I think it was only the random order of the files that made it work. 

{noformat}
DEBUG 19:52:30,744 Got sstable SSTableReader(path='/var/lib/cassandra/data/system/Schema-hb-4-Data.db') and max TS -9223372036854775808
DEBUG 19:52:30,744 Got sstable SSTableReader(path='/var/lib/cassandra/data/system/Schema-hb-3-Data.db') and max TS -9223372036854775808
{noformat}

h1. Fixes

Not sure, (wanted to get the ticket opened and find out if I was imagining things), guessing...

Use Long.MIN_VALUE as a magic maxTimestamp that means the value is not know. This would not fix issues where the incorrect maxTimestamp been included in compaction. 
 
Looking at making scrub re-calculate the maxTimestamp.

Also wondering if the maxTimestamp should default to Long.MAX_VALUE if read from a file format that does not support maxTimestamp ?
",slebresne,amorton,Urgent,Resolved,Fixed,21/Nov/11 07:15,16/Apr/19 09:32
Bug,CASSANDRA-3514,12532195,CounterColumnFamily Compaction error (ArrayIndexOutOfBoundsException),"On a single node, I'm seeing the following error when trying to compact a CounterColumnFamily. This appears to have started with version 1.0.3.

nodetool -h localhost compact TRProd MetricsAllTime
Error occured during compaction
java.util.concurrent.ExecutionException: java.lang.ArrayIndexOutOfBoundsException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performMaximal(CompactionManager.java:250)
	at org.apache.cassandra.db.ColumnFamilyStore.forceMajorCompaction(ColumnFamilyStore.java:1471)
	at org.apache.cassandra.service.StorageService.forceTableCompaction(StorageService.java:1523)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.GeneratedMethodAccessor24.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.ArrayIndexOutOfBoundsException
	at org.apache.cassandra.utils.ByteBufferUtil.arrayCopy(ByteBufferUtil.java:292)
	at org.apache.cassandra.db.context.CounterContext$ContextState.copyTo(CounterContext.java:792)
	at org.apache.cassandra.db.context.CounterContext.removeOldShards(CounterContext.java:709)
	at org.apache.cassandra.db.CounterColumn.removeOldShards(CounterColumn.java:260)
	at org.apache.cassandra.db.CounterColumn.mergeAndRemoveOldShards(CounterColumn.java:306)
	at org.apache.cassandra.db.CounterColumn.mergeAndRemoveOldShards(CounterColumn.java:271)
	at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:86)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:102)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:133)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:102)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:87)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.consume(MergeIterator.java:116)
	at org.apache.cassandra.utils.MergeIterator$ManyToOne.computeNext(MergeIterator.java:99)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:172)
	at org.apache.cassandra.db.compaction.CompactionManager$4.call(CompactionManager.java:277)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more",slebresne,efalcao,Normal,Resolved,Fixed,21/Nov/11 19:13,16/Apr/19 09:32
Bug,CASSANDRA-3519,12532284,ConcurrentModificationException in FailureDetector,"Noticed in a 2 DC cluster, error was on node in DC 2 streaming to a node in DC 1. 

{code:java}

INFO [GossipTasks:1] 2011-11-20 18:36:05,153 Gossiper.java (line 759) InetAddress /10.6.130.70 is now dead.
ERROR [GossipTasks:1] 2011-11-20 18:36:25,252 StreamOutSession.java (line 232) StreamOutSession /10.6.130.70 failed because {} died or was restarted/removed
ERROR [AntiEntropySessions:21] 2011-11-20 18:36:25,252 AntiEntropyService.java (line 688) [repair #7fb5b1b0-11f1-11e1-0000-baed0a2090fe] session completed with the following err
or
java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
ERROR [GossipTasks:1] 2011-11-20 18:36:25,256 Gossiper.java (line 172) Gossip error
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
        at java.util.AbstractList$Itr.next(AbstractList.java:343)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:190)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
ERROR [AntiEntropySessions:21] 2011-11-20 18:36:25,256 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[AntiEntropySessions:21,5,RMI Runtime]
java.lang.RuntimeException: java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        ... 3 more
ERROR [RMI TCP Connection(3634)-10.29.60.10] 2011-11-20 18:36:25,256 StorageService.java (line 1712) Repair session 7fb5b1b0-11f1-11e1-0000-baed0a2090fe failed.
java.util.concurrent.ExecutionException: java.lang.RuntimeException: java.io.IOException: Endpoint /10.6.130.70 died
        at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
        at java.util.concurrent.FutureTask.get(FutureTask.java:83)
        at org.apache.cassandra.service.StorageService.forceTableRepair(StorageService.java:1708)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
        at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
        at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
        at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
        at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
        at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
        at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
        at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1426)
        at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
        at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1264)
        at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1359)
        at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
        at sun.reflect.GeneratedMethodAccessor19.invoke(Unknown Source)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)
        at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
        at sun.rmi.transport.Transport$1.run(Transport.java:159)
        at java.security.AccessController.doPrivileged(Native Method)
        at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
        at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
        at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:885)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:907)
        at java.lang.Thread.run(Thread.java:619)
Caused by: java.lang.RuntimeException: java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        ... 3 more
Caused by: java.io.IOException: Endpoint /10.6.130.70 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:181)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:205)
        ... 3 more
 INFO [GossipStage:1] 2011-11-20 18:36:28,173 Gossiper.java (line 777) Node /10.6.130.70 has restarted, now UP
 INFO [GossipStage:1] 2011-11-20 18:36:28,175 Gossiper.java (line 745) InetAddress /10.6.130.70 is now UP
 INFO [GossipStage:1] 2011-11-20 18:36:28,175 StorageService.java (line 885) Node /10.6.130.70 state jump to normal
{code}

FailureDetector uses a normal ArrayList for the listeners.",amorton,amorton,Low,Resolved,Fixed,22/Nov/11 07:38,16/Apr/19 09:32
Bug,CASSANDRA-3520,12532289,Unit test are hanging on 0.8 branch,"As the summary says, the unit test on current 0.8 are just hanging after CliTest (it's apparently not the case on windows, but it is on Linux and MacOSX).
Not sure what's going on, but what I can tell is that it's enough to run CliTest to have it hang after the test successfully pass (i.e, JUnit just wait indefinitely for the VM to exit). Even weirder, it seems that it is the counter increment in the CliTest that make it hang, if you comment those statement, it stop hanging. However, nothing seems to go wrong with the increment itself (the test passes) and it doesn't even trigger anything (typically sendToHintedEndpoint is not called because there is only one node).
Looking at the stack when the VM is hanging (attached), there is nothing specific to counters in there, and nothing that struck me at odd (but I could miss something). There do is a few thrift thread running (CASSANDRA-3335), but why would that only be a problem for the tests in that situation is a mystery to me.",slebresne,slebresne,Normal,Resolved,Fixed,22/Nov/11 08:42,16/Apr/19 09:32
Bug,CASSANDRA-3521,12532297,sstableloader throwing exceptions when loading snapshot data from compressed CFs,"Loaded data from snapshot then enabled  `sstable_compression: org.apache.cassandra.io.compress.SnappyCompressor`
Then flush, scrub and compact. I can see actual CompressionRatio in JMX Console and access my data without problems..

Now I snapshot compressed keyspace and when trying to load snapshot to another single node or different Keyspace (the same super CF structure with compression options enabled, even try to truncate snapshoted CFs.) I cant retrieve any records . 


sstableloader command with debug mode dont throw any errors and shows its streaming 

{quote}
sstableloader-cassandra_2/bin/sstableloader --debug Impressions_compressed/
{quote}


Node logs contains repeating the errors bellow.

{quote}
ERROR [Thread-319] 2011-11-22 09:56:01,931 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Thread-319,5,main]
java.lang.AssertionError: attempted to delete non-existing file HidSaid-tmp-hb-260-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.streaming.IncomingStreamReader.retry(IncomingStreamReader.java:170)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:92)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)
 INFO [Thread-320] 2011-11-22 09:56:02,492 StreamInSession.java (line 120) Streaming of file Impressions_compressed/HidSaid-hb-9-Data.db sections=1 progress=0/5616749 - 0% from org.apache.cassandra.streaming.StreamInSession@3cc62c07 failed: requesting a retry.
ERROR [Thread-320] 2011-11-22 09:56:02,493 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Thread-320,5,main]
java.lang.AssertionError: attempted to delete non-existing file HidSaid-tmp-hb-261-Data.db
        at org.apache.cassandra.io.util.FileUtils.deleteWithConfirm(FileUtils.java:49)
        at org.apache.cassandra.streaming.IncomingStreamReader.retry(IncomingStreamReader.java:170)
        at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:92)
        at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
        at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)
{quote}

Hope its enough if you need more info just tell me what you need to reproduce this bug.",yukim,pvelas,Normal,Resolved,Fixed,22/Nov/11 09:33,16/Apr/19 09:32
Bug,CASSANDRA-3522,12532356,Fatal exception in thread Thread ,"Seeing this recurring exception on all machines in a 5 node cluster.  Recently upgraded to 1.0.0 from 0.7.2. 

ERROR [MutationStage:20] 2011-11-22 15:25:55,758 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[MutationStage:20,5,main]
java.lang.AssertionError
	at org.apache.cassandra.locator.TokenMetadata.getToken(TokenMetadata.java:273)
	at org.apache.cassandra.service.StorageProxy$4.run(StorageProxy.java:350)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
",brandon.williams,grosendorf,Low,Resolved,Fixed,22/Nov/11 15:55,16/Apr/19 09:32
Bug,CASSANDRA-3529,12532653,ConcurrentModificationException in Table.all(),"{noformat}
    [junit] java.util.ConcurrentModificationException
    [junit]     at java.util.HashMap$HashIterator.nextEntry(HashMap.java:793)
    [junit]     at java.util.HashMap$KeyIterator.next(HashMap.java:828)
    [junit]     at com.google.common.collect.Iterators$8.next(Iterators.java:750)
    [junit]     at org.apache.cassandra.db.ColumnFamilyStore.all(ColumnFamilyStore.java:1509)
    [junit]     at org.apache.cassandra.db.MeteredFlusher.countFlushingBytes(MeteredFlusher.java:118)
    [junit]     at org.apache.cassandra.db.MeteredFlusher.run(MeteredFlusher.java:45)
    [junit]     at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
    [junit]     at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
    [junit]     at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
    [junit]     at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    [junit]     at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    [junit]     at java.lang.Thread.run(Thread.java:662)
{noformat}",jbellis,jbellis,Low,Resolved,Fixed,24/Nov/11 20:36,16/Apr/19 09:32
Bug,CASSANDRA-3530,12532664,"Header class not thread safe, but mutated by multiple threads","With Cassandra 1.0.3 we are getting exceptions like,

Fatal exception in thread Thread[WRITE-/xx.xx.xx.xx,5,main]java.util.ConcurrentModificationException        
        at java.util.Hashtable$Enumerator.next(Unknown Source)
        at org.apache.cassandra.net.Header.serializedSize(Header.java:97)        
        at org.apache.cassandra.net.OutboundTcpConnection.messageLength(OutboundTcpConnection.java:164)
        at org.apache.cassandra.net.OutboundTcpConnection.write(OutboundTcpConnection.java:154)        
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:115)        
        at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:94)

and,

ERROR [WRITE-/xx.xx.xx.xx] 2011-11-24 22:08:28,981 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[WRITE-/10.30.12.79,5,main]java.lang.NullPointerException        
        at org.apache.cassandra.net.Header.serializedSize(Header.java:101)
        at org.apache.cassandra.net.OutboundTcpConnection.messageLength(OutboundTcpConnection.java:164)
        at org.apache.cassandra.net.OutboundTcpConnection.write(OutboundTcpConnection.java:154)
        at org.apache.cassandra.net.OutboundTcpConnection.writeConnected(OutboundTcpConnection.java:115)        
	at org.apache.cassandra.net.OutboundTcpConnection.run(OutboundTcpConnection.java:94)

It looks like Header is not thread safe, but the same header instance is modified concurrently while being sent to several threads in StorageProxy.sendMessages. 

This bug eventually causes the node to OOM, as it kills the OutboundTcpConnection thread, which means nothing is dequeing from queue.
",jbellis,sgbridges,Normal,Resolved,Fixed,25/Nov/11 00:11,16/Apr/19 09:32
Bug,CASSANDRA-3531,12532705,Fix crack-smoking in ConsistencyLevelTest,"First, let's note that this test fails in current 1.0 branch. It was ""broken"" (emphasis on the quotes) by CASSANDRA-3529. But it's not CASSANDRA-3529 fault, it's only that the use of NonBlockingHashMap changed the order of the tables returned by Schema.instance.getNonSystemTables(). *And*,  it turns out that ConsistencyLevelTest bails out as soon as it has found one keyspace with rf >= 2 due to a misplaced return. So it use to be that ConsistencyLevelTest was only ran for Keyspace5 (whose RF is 2) for which the test work. But for any RF > 2, the test fails.

The reason of this failing is that the test creates a 3 node cluster for whom only 1 node is alive as far as the failure detector is concerned. So for RF=3 and CL=QUORUM, the writes are unavailable (the failure detector is queried), while for reads we ""pretend"" two nodes are alive so we end up with a case where isWriteUnavailable != isReadUnavailable.
",slebresne,slebresne,Low,Resolved,Fixed,25/Nov/11 11:37,16/Apr/19 09:32
Bug,CASSANDRA-3532,12532766,Compaction cleanupIfNecessary costly when many files in data dir,"From what I can tell SSTableWriter.cleanupIfNecessary seems increasingly costly as the number of files in the data dir increases.
It calls SSTable.componentsFor(descriptor, Descriptor.TempState.TEMP) which lists all files in the data dir to find matching components.

Am I roughly correct that   (cleanupCost = SSTable count * data dir size)?


We had been doing write load testing with default compaction throttling (16MB/s) and LeveledCompaction.
Unfortunately we haven't been keeping tabs on sstable counts and it grew out of control.

On a system with 300,000 sstables (!) here is an example of our compaction rate.  Note that as you're probably aware cleanupIfNecessary is included in the timing:

 INFO [CompactionExecutor:48] 2011-11-25 22:25:30,353 CompactionTask.java (line 213) Compacted to [/data1/cassandra/data/MA_DDR/indexes_03-hc-5369-Data.db,].  5,821,590 to 5,306,354 (~91% of original) bytes for 123 keys at 0.163755MB/s.  Time: 30,903ms.

Here's a slightly larger one:
 INFO [CompactionExecutor:43] 2011-11-25 22:23:28,956 CompactionTask.java (line 213) Compacted to [/data1/cassandra/data/MA_DDR/indexes_03-hc-5336-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5337-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5338-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5339-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5340-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5341-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5342-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5343-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5344-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5345-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5346-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5347-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5348-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5349-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5350-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5351-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5352-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5353-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5354-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5355-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5356-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5357-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5358-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5359-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5360-Data.db,/data1/cassandra/data/MA_DDR/indexes_03-hc-5361-Data.db,].  140,706,512 to 137,990,868 (~98% of original) bytes for 2,181 keys at 0.338627MB/s.  Time: 388,623ms.


This is with compaction throttling set to 0 (Off).


So I believe because of this it's going to take a very long time to recover from having so many small sstables. 
It might be notable that we're using Solaris 10, possibly listFiles() is faster on other platforms?

Is it feasible to keep track of the temp files and just delete them rather than searching for them for each SSTable using SSTable.componentsFor()?



Here's the stack trace for the CompactionExecutor:14 thread that appears to be occupying the majority of the cpu time on this node:

Name: CompactionExecutor:14
State: RUNNABLE
Total blocked: 3  Total waited: 1,610,714

Stack trace: 
 java.io.UnixFileSystem.getBooleanAttributes0(Native Method)
java.io.UnixFileSystem.getBooleanAttributes(Unknown Source)
java.io.File.isDirectory(Unknown Source)
org.apache.cassandra.io.sstable.SSTable$3.accept(SSTable.java:204)
java.io.File.listFiles(Unknown Source)
org.apache.cassandra.io.sstable.SSTable.componentsFor(SSTable.java:200)
org.apache.cassandra.io.sstable.SSTableWriter.cleanupIfNecessary(SSTableWriter.java:289)
org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:189)
org.apache.cassandra.db.compaction.LeveledCompactionTask.execute(LeveledCompactionTask.java:57)
org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:134)
org.apache.cassandra.db.compaction.CompactionManager$1.call(CompactionManager.java:114)
java.util.concurrent.FutureTask$Sync.innerRun(Unknown Source)
java.util.concurrent.FutureTask.run(Unknown Source)
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(Unknown Source)
java.util.concurrent.ThreadPoolExecutor$Worker.run(Unknown Source)
java.lang.Thread.run(Unknown Source)

No matter where I click in the busy Compaction thread timeline in YourKit it's in Running state and showing this above trace, except for short periods of time where it's actually compacting :)

Thanks,
Eric",eparusel,eparusel,Normal,Resolved,Fixed,25/Nov/11 22:58,16/Apr/19 09:32
Bug,CASSANDRA-3536,12532941,Assertion error during bootstraping cassandra," I have a 3 node cassandra cluster. I have RF set to 3 and do reads
and writes using QUORUM.

Here is my initial ring configuration

[root@CAP4-CNode1 ~]# /root/cassandra/bin/nodetool -h localhost ring
Address         DC          Rack        Status State   Load
Owns    Token

       113427455640312821154458202477256070484
10.19.104.11    datacenter1 rack1       Up     Normal  1.66 GB
33.33%  0
10.19.104.12    datacenter1 rack1       Up     Normal  1.06 GB
33.33%  56713727820156410577229101238628035242
10.19.104.13    datacenter1 rack1       Up     Normal  1.61 GB
33.33%  113427455640312821154458202477256070484

I want to add 10.19.104.14 to the cluster.

I edited the 10.19.104.14 cassandra.yaml file and set the token to
127605887595351923798765477786913079296 and set auto_bootstrap to
true.

When I started cassandra I am getting Assertion Error.  

thanks
Ramesh




[root@CAP4-CNode4 cassandra]#  INFO 10:29:46,093 Logging initialized
 INFO 10:29:46,099 JVM vendor/version: Java HotSpot(TM) 64-Bit Server
VM/1.6.0_25
 INFO 10:29:46,100 Heap size: 8304721920/8304721920
 INFO 10:29:46,100 Classpath:
bin/../conf:bin/../build/classes/main:bin/../build/classes/thrift:bin/../lib/antlr-3.2.jar:bin/../lib/apache-cassandra-1.0.2.jar:bin/../lib/apache-cassandra-clientutil-1.0.2.jar:bin/../lib/apache-cassandra-thrift-1.0.2.jar:bin/../lib/avro-1.4.0-fixes.jar:bin/../lib/avro-1.4.0-sources-fixes.jar:bin/../lib/commons-cli-1.1.jar:bin/../lib/commons-codec-1.2.jar:bin/../lib/commons-lang-2.4.jar:bin/../lib/compress-lzf-0.8.4.jar:bin/../lib/concurrentlinkedhashmap-lru-1.2.jar:bin/../lib/guava-r08.jar:bin/../lib/high-scale-lib-1.1.2.jar:bin/../lib/jackson-core-asl-1.4.0.jar:bin/../lib/jackson-mapper-asl-1.4.0.jar:bin/../lib/jamm-0.2.5.jar:bin/../lib/jline-0.9.94.jar:bin/../lib/jna.jar:bin/../lib/json-simple-1.1.jar:bin/../lib/libthrift-0.6.jar:bin/../lib/log4j-1.2.16.jar:bin/../lib/mx4j-examples.jar:bin/../lib/mx4j-impl.jar:bin/../lib/mx4j.jar:bin/../lib/mx4j-jmx.jar:bin/../lib/mx4j-remote.jar:bin/../lib/mx4j-rimpl.jar:bin/../lib/mx4j-rjmx.jar:bin/../lib/mx4j-tools.jar:bin/../lib/servlet-api-2.5-20081211.jar:bin/../lib/slf4j-api-1.6.1.jar:bin/../lib/slf4j-log4j12-1.6.1.jar:bin/../lib/snakeyaml-1.6.jar:bin/../lib/snappy-java-1.0.4.1.jar:bin/../lib/jamm-0.2.5.jar
 INFO 10:29:48,713 JNA mlockall successful
 INFO 10:29:48,726 Loading settings from
file:/root/apache-cassandra-1.0.2/conf/cassandra.yaml
 INFO 10:29:48,883 DiskAccessMode 'auto' determined to be mmap,
indexAccessMode is mmap
 INFO 10:29:48,898 Global memtable threshold is enabled at 2640MB
 INFO 10:29:49,203 Couldn't detect any schema definitions in local storage.
 INFO 10:29:49,204 Found table data in data directories. Consider
using the CLI to define your schema.
 INFO 10:29:49,220 Creating new commitlog segment
/var/lib/cassandra/commitlog/CommitLog-1321979389220.log
 INFO 10:29:49,227 No commitlog files found; skipping replay
 INFO 10:29:49,230 Cassandra version: 1.0.2
 INFO 10:29:49,230 Thrift API version: 19.18.0
 INFO 10:29:49,230 Loading persisted ring state
 INFO 10:29:49,235 Starting up server gossip
 INFO 10:29:49,259 Enqueuing flush of
Memtable-LocationInfo@122130810(192/240 serialized/live bytes, 4 ops)
 INFO 10:29:49,260 Writing Memtable-LocationInfo@122130810(192/240
serialized/live bytes, 4 ops)
 INFO 10:29:49,317 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-1-Data.db (300 bytes)
 INFO 10:29:49,340 Starting Messaging Service on port 7000
 INFO 10:29:49,349 JOINING: waiting for ring and schema information
 INFO 10:29:50,759 Applying migration
4b0e20f0-1511-11e1-0000-c11bc95834d7 Add keyspace: MSA, rep
strategy:SimpleStrategy{}, durable_writes: true
 INFO 10:29:50,761 Enqueuing flush of
Memtable-Migrations@1507565381(6744/8430 serialized/live bytes, 1 ops)
 INFO 10:29:50,761 Writing Memtable-Migrations@1507565381(6744/8430
serialized/live bytes, 1 ops)
 INFO 10:29:50,761 Enqueuing flush of
Memtable-Schema@1498835564(2889/3611 serialized/live bytes, 3 ops)
 INFO 10:29:50,776 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-1-Data.db (6808 bytes)
 INFO 10:29:50,777 Writing Memtable-Schema@1498835564(2889/3611
serialized/live bytes, 3 ops)
 INFO 10:29:50,797 Completed flushing
/var/lib/cassandra/data/system/Schema-h-1-Data.db (3039 bytes)
 INFO 10:29:50,814 Applying migration
4b6f2cb0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@1639d811[cfId=1000,ksName=MSA,cfName=modseq,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=5000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@2f984f7d,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:50,815 Enqueuing flush of
Memtable-Migrations@948613108(7482/9352 serialized/live bytes, 1 ops)
 INFO 10:29:50,816 Writing Memtable-Migrations@948613108(7482/9352
serialized/live bytes, 1 ops)
 INFO 10:29:50,816 Enqueuing flush of
Memtable-Schema@421910828(3294/4117 serialized/live bytes, 3 ops)
 INFO 10:29:50,831 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-2-Data.db (7546 bytes)
 INFO 10:29:50,832 Writing Memtable-Schema@421910828(3294/4117
serialized/live bytes, 3 ops)
 INFO 10:29:50,846 Completed flushing
/var/lib/cassandra/data/system/Schema-h-2-Data.db (3444 bytes)
 INFO 10:29:50,854 Applying migration
4b8c9fc0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@1bd97d0d[cfId=1001,ksName=MSA,cfName=msgid,cfType=Standard,comparator=org.apache.cassandra.db.marshal.BytesType,subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@63a0eec3,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO 10:29:50,855 Enqueuing flush of
Memtable-Migrations@1520138062(7750/9687 serialized/live bytes, 1 ops)
 INFO 10:29:50,856 Writing Memtable-Migrations@1520138062(7750/9687
serialized/live bytes, 1 ops)
 INFO 10:29:50,856 Enqueuing flush of
Memtable-Schema@347459675(3630/4537 serialized/live bytes, 3 ops)
 INFO 10:29:50,878 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-3-Data.db (7814 bytes)
 INFO 10:29:50,879 Writing Memtable-Schema@347459675(3630/4537
serialized/live bytes, 3 ops)
 INFO 10:29:50,894 Completed flushing
/var/lib/cassandra/data/system/Schema-h-3-Data.db (3780 bytes)
 INFO 10:29:50,900 Applying migration
4ba1ae60-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@6a095b8a[cfId=1002,ksName=MSA,cfName=participants,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@c58f769,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.SizeTieredCompactionStrategy,compactionStrategyOptions={},compressionOptions={}]
 INFO 10:29:50,900 Enqueuing flush of
Memtable-Migrations@618337492(8194/10242 serialized/live bytes, 1 ops)
 INFO 10:29:50,901 Writing Memtable-Migrations@618337492(8194/10242
serialized/live bytes, 1 ops)
 INFO 10:29:50,902 Enqueuing flush of
Memtable-Schema@724860211(4020/5025 serialized/live bytes, 3 ops)
 INFO 10:29:50,917 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-4-Data.db (8258 bytes)
 INFO 10:29:50,918 Writing Memtable-Schema@724860211(4020/5025
serialized/live bytes, 3 ops)
 INFO 10:29:50,925 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-3-Data.db')]
 INFO 10:29:50,934 Completed flushing
/var/lib/cassandra/data/system/Schema-h-4-Data.db (4170 bytes)
 INFO 10:29:50,935 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-3-Data.db')]
 INFO 10:29:50,940 Applying migration
4bb4e840-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@318c69a9[cfId=1003,ksName=MSA,cfName=subinfo,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=5000.0,keyCacheSize=5000000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=14400,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@796cefa8,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:50,941 Enqueuing flush of
Memtable-Migrations@1682081063(8618/10772 serialized/live bytes, 1
ops)
 INFO 10:29:50,941 Writing Memtable-Migrations@1682081063(8618/10772
serialized/live bytes, 1 ops)
 INFO 10:29:50,941 Enqueuing flush of
Memtable-Schema@1083461053(4427/5533 serialized/live bytes, 3 ops)
 INFO 10:29:50,977 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-5-Data.db (8682 bytes)
 INFO 10:29:50,978 Writing Memtable-Schema@1083461053(4427/5533
serialized/live bytes, 3 ops)
 INFO 10:29:50,991 Compacted to
[/var/lib/cassandra/data/system/Schema-h-5-Data.db,].  14,433 to
14,106 (~97% of original) bytes for 5 keys at 0.269051MB/s.  Time:
50ms.
 INFO 10:29:50,995 Completed flushing
/var/lib/cassandra/data/system/Schema-h-7-Data.db (4577 bytes)
 INFO 10:29:51,000 Applying migration
4bc6e9a0-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@20b00ec2[cfId=1004,ksName=MSA,cfName=transactions,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=0.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=0,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@698f352,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:51,001 Enqueuing flush of
Memtable-Migrations@596545504(9027/11283 serialized/live bytes, 1 ops)
 INFO 10:29:51,002 Writing Memtable-Migrations@596545504(9027/11283
serialized/live bytes, 1 ops)
 INFO 10:29:51,003 Enqueuing flush of
Memtable-Schema@1686621532(4835/6043 serialized/live bytes, 3 ops)
 INFO 10:29:51,029 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-7-Data.db (9091 bytes)
 INFO 10:29:51,029 Writing Memtable-Schema@1686621532(4835/6043
serialized/live bytes, 3 ops)
 INFO 10:29:51,031 Compacted to
[/var/lib/cassandra/data/system/Migrations-h-6-Data.db,].  30,426 to
30,234 (~99% of original) bytes for 1 keys at 0.272013MB/s.  Time:
106ms.
 INFO 10:29:51,044 Completed flushing
/var/lib/cassandra/data/system/Schema-h-8-Data.db (4985 bytes)
 INFO 10:29:51,049 Applying migration
4bd76460-1511-11e1-0000-c11bc95834d7 Add column family:
org.apache.cassandra.config.CFMetaData@4ab4faeb[cfId=1005,ksName=MSA,cfName=uid,cfType=Standard,comparator=org.apache.cassandra.db.marshal.ReversedType(org.apache.cassandra.db.marshal.BytesType),subcolumncomparator=<null>,comment=,rowCacheSize=0.0,keyCacheSize=1500000.0,readRepairChance=1.0,replicateOnWrite=true,gcGraceSeconds=3600,defaultValidator=org.apache.cassandra.db.marshal.BytesType,keyValidator=org.apache.cassandra.db.marshal.BytesType,minCompactionThreshold=4,maxCompactionThreshold=32,rowCacheSavePeriodInSeconds=0,keyCacheSavePeriodInSeconds=14400,rowCacheKeysToSave=2147483647,rowCacheProvider=org.apache.cassandra.cache.SerializingCacheProvider@2fc5809e,mergeShardsChance=0.1,keyAlias=<null>,column_metadata={},compactionStrategyClass=class
org.apache.cassandra.db.compaction.LeveledCompactionStrategy,compactionStrategyOptions={sstable_size_in_mb=10},compressionOptions={}]
 INFO 10:29:51,050 Enqueuing flush of
Memtable-Migrations@1333730706(9421/11776 serialized/live bytes, 1
ops)
 INFO 10:29:51,050 Writing Memtable-Migrations@1333730706(9421/11776
serialized/live bytes, 1 ops)
 INFO 10:29:51,051 Enqueuing flush of
Memtable-Schema@577668356(5236/6545 serialized/live bytes, 3 ops)
 INFO 10:29:51,065 Completed flushing
/var/lib/cassandra/data/system/Migrations-h-9-Data.db (9485 bytes)
 INFO 10:29:51,066 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-6-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-9-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-7-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Migrations-h-5-Data.db')]
 INFO 10:29:51,066 Writing Memtable-Schema@577668356(5236/6545
serialized/live bytes, 3 ops)
 INFO 10:29:51,081 Completed flushing
/var/lib/cassandra/data/system/Schema-h-9-Data.db (5386 bytes)
 INFO 10:29:51,083 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-5-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-9-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-8-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/Schema-h-7-Data.db')]
 INFO 10:29:51,114 Compacted to
[/var/lib/cassandra/data/system/Schema-h-10-Data.db,].  29,054 to
28,727 (~98% of original) bytes for 8 keys at 0.913207MB/s.  Time:
30ms.
 INFO 10:29:51,144 Compacted to
[/var/lib/cassandra/data/system/Migrations-h-10-Data.db,].  57,492 to
57,300 (~99% of original) bytes for 1 keys at 0.700584MB/s.  Time:
78ms.
 INFO 10:29:51,410 Node /10.19.104.13 is now part of the cluster
 INFO 10:29:51,412 InetAddress /10.19.104.13 is now UP
 INFO 10:29:51,414 Enqueuing flush of
Memtable-LocationInfo@709342045(35/43 serialized/live bytes, 1 ops)
 INFO 10:29:51,415 Writing Memtable-LocationInfo@709342045(35/43
serialized/live bytes, 1 ops)
 INFO 10:29:51,428 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db (89 bytes)
 INFO 10:29:51,439 Node /10.19.104.12 is now part of the cluster
 INFO 10:29:51,439 InetAddress /10.19.104.12 is now UP
 INFO 10:29:51,441 Enqueuing flush of
Memtable-LocationInfo@1292444743(35/43 serialized/live bytes, 1 ops)
 INFO 10:29:51,441 Writing Memtable-LocationInfo@1292444743(35/43
serialized/live bytes, 1 ops)
 INFO 10:29:51,455 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db (89 bytes)
 INFO 10:29:51,456 Node /10.19.104.11 is now part of the cluster
 INFO 10:29:51,457 InetAddress /10.19.104.11 is now UP
 INFO 10:29:51,459 Enqueuing flush of
Memtable-LocationInfo@1891328597(20/25 serialized/live bytes, 1 ops)
 INFO 10:29:51,459 Writing Memtable-LocationInfo@1891328597(20/25
serialized/live bytes, 1 ops)
 INFO 10:29:51,471 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-4-Data.db (74 bytes)
 INFO 10:29:51,473 Compacting
[SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-2-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-4-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-1-Data.db'),
SSTableReader(path='/var/lib/cassandra/data/system/LocationInfo-h-3-Data.db')]
 INFO 10:29:51,497 Compacted to
[/var/lib/cassandra/data/system/LocationInfo-h-5-Data.db,].  552 to
444 (~80% of original) bytes for 3 keys at 0.018410MB/s.  Time: 23ms.
 INFO 10:30:19,349 JOINING: getting bootstrap token
 INFO 10:30:19,352 Enqueuing flush of
Memtable-LocationInfo@225265367(36/45 serialized/live bytes, 1 ops)
 INFO 10:30:19,353 Writing Memtable-LocationInfo@225265367(36/45
serialized/live bytes, 1 ops)
 INFO 10:30:19,364 Completed flushing
/var/lib/cassandra/data/system/LocationInfo-h-7-Data.db (87 bytes)
 INFO 10:30:19,374 JOINING: sleeping 30000 ms for pending range setup
 INFO 10:30:49,375 JOINING: Starting to bootstrap...
ERROR 10:31:13,444 Fatal exception in thread Thread[Thread-49,5,main]
java.lang.AssertionError
       at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:178)
       at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
       at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:466)
       at org.apache.cassandra.db.DataTracker.replace(DataTracker.java:275)
       at org.apache.cassandra.db.DataTracker.addSSTables(DataTracker.java:237)
       at org.apache.cassandra.db.DataTracker.addStreamedSSTable(DataTracker.java:242)
       at org.apache.cassandra.db.ColumnFamilyStore.addSSTable(ColumnFamilyStore.java:922)
       at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:141)
       at org.apache.cassandra.streaming.IncomingStreamReader.read(IncomingStreamReader.java:102)
       at org.apache.cassandra.net.IncomingTcpConnection.stream(IncomingTcpConnection.java:184)
       at org.apache.cassandra.net.IncomingTcpConnection.run(IncomingTcpConnection.java:81)",jbellis,ramesh25,Normal,Resolved,Fixed,28/Nov/11 17:44,16/Apr/19 09:32
Bug,CASSANDRA-3537,12533074,ExpiringMap timer is not exception-proof,"I have 4 cassandra nodes ,and I put about 30G data to db for every nodes . It's just 4 days before I start the cluster ,but now every 4 nodes have the same problem ,JVM heap is full  ,and  GC take no effect ,There must be some memory leak . Jmap the memory as follow:

Object Histogram:

num 	  #instances	#bytes	Class description
--------------------------------------------------------------------------
1:		15793606	758093088	java.nio.HeapByteBuffer
2:		2153811	320138208	java.lang.Object[]
3:		6163192	197222144	org.apache.cassandra.db.Column
4:		2543836	175890256	int[]
5:		2168816	155397192	long[]
6:		2078123	116374888	org.cliffc.high_scale_lib.ConcurrentAutoTable$CAT
7:		1847111	73884440	java.math.BigInteger
8:		1234243	59243664	java.util.Hashtable
9:		1770829	58233000	char[]
10:		1770627	56660064	java.lang.String
11:		1665886	39981264	org.apache.cassandra.db.DecoratedKey
12:		692706	38791536	org.cliffc.high_scale_lib.NonBlockingHashMap$CHM
13:		1234274	37172088	java.util.Hashtable$Entry[]
14:		1133541	36273312	java.net.Inet4Address
15:		738528	35449344	org.apache.cassandra.service.ReadCallback
16:		2078118	33249888	org.cliffc.high_scale_lib.Counter
17:		1373886	32973264	org.apache.cassandra.db.ReadResponse
18:		1234023	29616552	org.apache.cassandra.net.Message
19:		1234019	29616456	org.apache.cassandra.net.Header
20:		1846185	29538960	org.apache.cassandra.dht.BigIntegerToken
21:		891378	28524096	org.apache.cassandra.utils.ExpiringMap$CacheableObject
22:		692706	27708240	org.cliffc.high_scale_lib.NonBlockingHashMap
23:		1148252	27558048	java.util.Collections$SynchronizedSet
24:		541977	26014896	org.apache.cassandra.db.SliceFromReadCommand
25:		998001	23952024	java.util.concurrent.ConcurrentSkipListMap$Node
26:		928792	22291008	java.util.ArrayList
27:		692715	22166880	java.util.concurrent.atomic.AtomicReferenceFieldUpdater$AtomicReferenceFieldUpdaterImpl
28:		891378	21393072	org.apache.cassandra.net.CallbackInfo
29:		1148247	18371952	java.util.Hashtable$KeySet
30:		731859	17564616	org.apache.cassandra.db.Row
31:		529991	16959712	org.apache.cassandra.db.ArrayBackedSortedColumns
32:		691425	16594200	org.apache.cassandra.db.AbstractColumnContainer$DeletionInfo
33:		648580	15565920	org.apache.cassandra.db.filter.QueryPath
34:		648338	15560112	org.apache.cassandra.service.RowDigestResolver
35:		971376	15542016	java.util.concurrent.atomic.AtomicInteger
36:		837418	13398688	org.apache.cassandra.utils.SimpleCondition
37:		535614	12854736	org.apache.cassandra.db.ColumnFamily
38:		725634	11610144	java.util.concurrent.atomic.AtomicReference
39:		195117	9365616	org.apache.cassandra.db.ThreadSafeSortedColumns
40:		281921	9021472	java.util.concurrent.ConcurrentSkipListMap$HeadIndex
41:		277679	8885728	java.util.concurrent.locks.ReentrantLock$NonfairSync
42:		314424	7546176	java.util.concurrent.ConcurrentSkipListMap$Index
43:		275186	6604464	java.util.concurrent.locks.AbstractQueuedSynchronizer$ConditionObject
44:		270280	6486720	java.util.concurrent.LinkedBlockingQueue$Node
45:		219553	5269272	org.apache.cassandra.io.sstable.IndexSummary$KeyPosition
46:		106436	5108928	java.util.TreeMap
47:		122185	4887400	org.apache.cassandra.db.ExpiringColumn
48:		189968	4559232	org.apache.cassandra.db.SuperColumn
49:		275659	4410544	java.util.concurrent.locks.ReentrantLock
50:		90213	4330224	java.util.concurrent.LinkedBlockingQueue
51:		107026	4281040	java.util.TreeMap$Entry
52:		30501	4222056	* ConstMethodKlass",jbellis,jonma,Low,Resolved,Fixed,29/Nov/11 07:24,16/Apr/19 09:32
Bug,CASSANDRA-3539,12533131,Assertion error when forwarding to local nodes,"CASSANDRA-3530 introduces a regression as reported on irc:
{quote}
Started a rolling upgrade from 1.0.3 to 1.0.4 now all boxes are constantly spitting out this assert: at 
org.apache.cassandra.db.RowMutationVerbHandler.forwardToLocalNodes(RowMutationVerbHandler.java:71)
{quote}",slebresne,slebresne,Normal,Resolved,Fixed,29/Nov/11 15:59,16/Apr/19 09:32
Bug,CASSANDRA-3540,12533155,Wrong check of partitioner for secondary indexes,"CASSANDRA-3407 doesn't handle the fact that secondary indexes have a specific partitioner (LocalPartitioner). This result in the following error when starting nodes in 1.0.4:
{noformat}
java.lang.RuntimeException: Cannot open /var/lib/cassandra/data/Index/AttractionLocationCategoryDateIdx.AttractionLocationCategoryDateIdx_09partition_idx-h-1 because partitioner does not match org.apache.cassandra.dht.LocalPartitioner
{noformat}",yukim,slebresne,Urgent,Resolved,Fixed,29/Nov/11 17:57,16/Apr/19 09:32
Bug,CASSANDRA-3543,12533334,Commit Log Allocator deadlock after first start with empty commitlog directory,"While testing CASSANDRA-3541 at some point stress completely timed out.  I proceeded to shut the cluster down and 2/3 JVMs hang infinitely.  After a while, one of them logged:

{noformat}
WARN 19:07:50,133 Some hints were not written before shutdown.  This is not supposed to happen.  You should (a) run repair, and (b) file a bug report
{noformat}",rbranson,brandon.williams,Normal,Resolved,Fixed,30/Nov/11 19:22,16/Apr/19 09:32
Bug,CASSANDRA-3544,12533346,NPE on startup when there are permissions issues with directories,"If the directories used by cassandra for data, commitlog, and saved caches aren't readable due to permissions, you get an NPE on startup.  In particular, if none of them are readable, you'll see something like this:

{noformat}
ERROR 14:50:11,945 Exception encountered during startup
java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:391)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:147)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
java.lang.NullPointerException
	at org.apache.cassandra.db.ColumnFamilyStore.scrubDataDirectories(ColumnFamilyStore.java:391)
	at org.apache.cassandra.service.AbstractCassandraDaemon.setup(AbstractCassandraDaemon.java:147)
	at org.apache.cassandra.service.AbstractCassandraDaemon.activate(AbstractCassandraDaemon.java:337)
	at org.apache.cassandra.thrift.CassandraDaemon.main(CassandraDaemon.java:107)
Exception encountered during startup: null
{noformat}

This traceback happens when the saved_caches directory isn't readable, but you can get different ones if only the data or commitlog directories aren't readable.

We should check the permissions of these directories before trying to list their contents.",yukim,thobbs,Low,Resolved,Fixed,30/Nov/11 20:55,16/Apr/19 09:32
Bug,CASSANDRA-3546,12533423,Hinted handoffs isn't delivered if/when HintedHandOffManager ends up in invalid state.,"Running Cassandra 1.0.3.
I've done some testing with 2 nodes (node A, node B), replication factor 2.
I take node A down, writing some data to node B and then take node A up.
Sometimes hints aren't delivered when node A comes up.

I've done some debugging in org.apache.cassandra.db.HintedHandOffManager and sometimes node B ends up in a strange state in method org.apache.cassandra.db.HintedHandOffManager.deliverHints(final InetAddress to), where org.apache.cassandra.db.HintedHandOffManager.queuedDeliveries already has node A in it's Set and therefore no hints will ever be delivered to node A.

The only reason for this that I can see is that in org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(InetAddress endpoint) the hintStore.isEmpty() check returns true and the endpoint (node A)  isn't removed from org.apache.cassandra.db.HintedHandOffManager.queuedDeliveries. Then no hints will ever be delivered again until node B is restarted.

During what conditions will hintStore.isEmpty() return true?
Shouldn't the hintStore.isEmpty() check be inside the try {} finally{} clause, removing the endpoint from queuedDeliveries in the finally block?

{code}
public void deliverHints(final InetAddress to)
{
    logger_.debug(""deliverHints to {}"", to);
    if (!queuedDeliveries.add(to))
        return;
    .......
}
{code}

{code}
private void deliverHintsToEndpoint(InetAddress endpoint) 
    throws IOException, DigestMismatchException, InvalidRequestException, TimeoutException, InterruptedException
{
     ColumnFamilyStore hintStore = Table.open(Table.SYSTEM_TABLE).getColumnFamilyStore(HINTS_CF);
     if (hintStore.isEmpty())
         return; // nothing to do, don't confuse users by logging a no-op handoff
     try
     {
         ......
     }
     finally
     {
         queuedDeliveries.remove(endpoint);
     }
}
{code} ",slebresne,fredrikl74,Normal,Resolved,Fixed,01/Dec/11 10:42,16/Apr/19 09:32
Bug,CASSANDRA-3547,12533432,Race between cf flush and  its secondary indexes flush,"When a CF with indexes is flushed, it's indexes are flushed too. In particular their memtable is switched, but without making the old memtable frozen. This can conflict with a concurrent flush of the index itself, as reported on the user list by Michael Vaknine:
{noformat}
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 java.lang.AssertionError
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.maybeSwitchMemtable(ColumnFamilyStore.java:671)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.forceFlush(ColumnFamilyStore.java:745)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.ColumnFamilyStore.forceBlockingFlush(ColumnFamilyStore.java:750)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.keys.KeysIndex.forceBlockingFlush(KeysIndex.java:119)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.SecondaryIndexManager.flushIndexesBlocking(SecondaryIndexManager.java:258)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.db.index.SecondaryIndexManager.maybeBuildSecondaryIndexes(SecondaryIndexManager.java:123)
TST-Cass2 ERROR [Thread-58] 2011-11-30 20:40:17,449 at org.apache.cassandra.streaming.StreamInSession.closeIfFinished(StreamInSession.java:151)
{noformat}",slebresne,slebresne,Normal,Resolved,Fixed,01/Dec/11 11:19,16/Apr/19 09:32
Bug,CASSANDRA-3548,12533435,NPE in AntiEntropyService$RepairSession.completed(),"This may be related to CASSANDRA-3519 (cluster it was observed on is still 1.0.1), however i think there is still a race condition.

Observed on a 2 DC cluster, during a repair that spanned the DC's.  

{noformat}
INFO [AntiEntropyStage:1] 2011-11-28 06:22:56,225 StreamingRepairTask.java (line 136) [streaming task #69187510-1989-11e1-0000-5ff37d368cb6] Forwarding streaming repair of 8602 
ranges to /10.6.130.70 (to be streamed with /10.37.114.10)
...
 INFO [AntiEntropyStage:66] 2011-11-29 11:20:57,109 StreamingRepairTask.java (line 253) [streaming task #69187510-1989-11e1-0000-5ff37d368cb6] task succeeded
ERROR [AntiEntropyStage:66] 2011-11-29 11:20:57,109 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[AntiEntropyStage:66,5,main]
java.lang.NullPointerException
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.completed(AntiEntropyService.java:712)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession$Differencer$1.run(AntiEntropyService.java:912)
        at org.apache.cassandra.streaming.StreamingRepairTask$2.run(StreamingRepairTask.java:186)
        at org.apache.cassandra.streaming.StreamingRepairTask$StreamingRepairResponse.doVerb(StreamingRepairTask.java:255)
        at org.apache.cassandra.net.MessageDeliveryTask.run(MessageDeliveryTask.java:59)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
{noformat}

One of the nodes involved in the repair session failed, e.g. (Not sure if this is from the same repair session as the streaming task above, but it illustrates the issue)

{noformat}
ERROR [AntiEntropySessions:1] 2011-11-28 19:39:52,507 AntiEntropyService.java (line 688) [repair #2bf19860-197f-11e1-0000-5ff37d368cb6] session completed with the following error
java.io.IOException: Endpoint /10.29.60.10 died
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.failedNode(AntiEntropyService.java:725)
        at org.apache.cassandra.service.AntiEntropyService$RepairSession.convict(AntiEntropyService.java:762)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:192)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
ERROR [GossipTasks:1] 2011-11-28 19:39:52,507 StreamOutSession.java (line 232) StreamOutSession /10.29.60.10 failed because {} died or was restarted/removed
ERROR [GossipTasks:1] 2011-11-28 19:39:52,571 Gossiper.java (line 172) Gossip error
java.util.ConcurrentModificationException
        at java.util.ArrayList$Itr.checkForComodification(ArrayList.java:782)
        at java.util.ArrayList$Itr.next(ArrayList.java:754)
        at org.apache.cassandra.gms.FailureDetector.interpret(FailureDetector.java:190)
        at org.apache.cassandra.gms.Gossiper.doStatusCheck(Gossiper.java:559)
        at org.apache.cassandra.gms.Gossiper.access$700(Gossiper.java:62)
        at org.apache.cassandra.gms.Gossiper$GossipTask.run(Gossiper.java:167)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)

{noformat}

When a node is marked as failed AntiEntropyService.RepairSession.forceShutdown() clears the activejobs map. But the jobs to other nodes will continue, and will eventually call completed(). 

RepairSession.terminated should stop completed() from checking the map, but there is a race between the map been cleared and if there is an error in finally block it wont be set. 
",slebresne,amorton,Low,Resolved,Fixed,01/Dec/11 11:35,16/Apr/19 09:32
Bug,CASSANDRA-3551,12533530,Timeout exception for quorum reads after upgrade from 1.0.2 to 1.0.5,"I upgraded from 1.0.2 to 1.0.5. For some column families always got TimeoutException. I turned on debug and increase rpc_timeout to 1 minute, but still got timeout. I believe it is bug on 1.0.5.

ConsistencyLevel is QUORUM, replicate factor is 3. 

Here are partial logs. 


DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,717 StorageProxy.java (line 813) RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=SlicePre
dicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 00 0
2 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00 0C 
00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/999/99999], max_keys=1024}
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,718 StorageProxy.java (line 1012) restricted ranges for query [PROD/US/000/0,PROD/US/999/99999] are [[PROD/US/000/0,PROD/US/300/~], (PROD/US/300/~,PROD/
US/600/~], (PROD/US/600/~,PROD/US/999/99999]]
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,720 VoxeoStrategy.java (line 157) ReplicationFactor 3
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,720 VoxeoStrategy.java (line 33) PROD/US/300/~
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,721 VoxeoStrategy.java (line 96) End region for token PROD/US/300/~ PROD/US/300/~ 10.92.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,721 VoxeoStrategy.java (line 96) End region for token PROD/US/300/~ PROD/US/600/~ 10.72.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,721 VoxeoStrategy.java (line 96) End region for token PROD/US/300/~ PROD/US/999/~ 10.8.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,723 VoxeoStrategy.java (line 157) ReplicationFactor 3
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,724 ReadCallback.java (line 77) Blockfor/repair is 2/false; setting up requests to /10.92.208.103,/10.72.208.103
DEBUG [WRITE-/10.92.208.103] 2011-12-01 22:25:39,725 OutboundTcpConnection.java (line 206) attempting to connect to /10.92.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,726 StorageProxy.java (line 859) reading RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=
SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00
 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 7
2 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/300/~], max_keys=1024} from /10.92.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,726 StorageProxy.java (line 859) reading RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=
SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00
 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 7
2 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/300/~], max_keys=1024} from /10.72.208.103
DEBUG [WRITE-/10.8.208.103] 2011-12-01 22:25:39,727 OutboundTcpConnection.java (line 206) attempting to connect to /10.8.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,727 StorageProxy.java (line 859) reading RangeSliceCommand{keyspace='keyspaceLBSDATAPRODUS', column_family='dataProvider', super_column=null, predicate=
SlicePredicate(slice_range:SliceRange(start:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 72 00
 0C 00 02 0C 00 02 0B 00 01 00 00 00 00, finish:80 01 00 01 00 00 00 10 67 65 74 5F 72 61 6E 67 65 5F 73 6C 69 63 65 73 00 00 00 03 0C 00 01 0B 00 03 00 00 00 0C 64 61 74 61 50 72 6F 76 69 64 65 7
2 00 0C 00 02 0C 00 02 0B 00 01 00 00 00 00 0B 00 02 00 00 00 00, reversed:false, count:1024)), range=[PROD/US/000/0,PROD/US/300/~], max_keys=1024} from /10.8.208.103
DEBUG [ReadStage:1] 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 0 of 1024: active:false:1@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 1 of 1024: name:false:4@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 2 of 1024: providerData:false:2283@1321549067179000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,731 SliceQueryFilter.java (line 123) collecting 3 of 1024: providerID:false:1@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,732 SliceQueryFilter.java (line 123) collecting 4 of 1024: timestamp:false:13@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,732 SliceQueryFilter.java (line 123) collecting 5 of 1024: vendorData:false:2364@1322777621601000
DEBUG [ReadStage:1] 2011-12-01 22:25:39,733 ColumnFamilyStore.java (line 1331) scanned DecoratedKey(PROD/US/001/1, 50524f442f55532f3030312f31)
DEBUG [ReadStage:1] 2011-12-01 22:25:39,733 RangeSliceVerbHandler.java (line 55) Sending RangeSliceReply{rows=Row(key=DecoratedKey(PROD/US/001/1, 50524f442f55532f3030312f31), cf=ColumnFamily(dataP
rovider [active:false:1@1322777621601000,name:false:4@1322777621601000,providerData:false:2283@1321549067179000,providerID:false:1@1322777621601000,timestamp:false:13@1322777621601000,vendorData:f
alse:2364@1322777621601000,]))} to 72@/10.72.208.103
DEBUG [RequestResponseStage:1] 2011-12-01 22:25:39,734 ResponseVerbHandler.java (line 44) Processing response on a callback from 72@/10.72.208.103
DEBUG [RequestResponseStage:2] 2011-12-01 22:25:39,887 ResponseVerbHandler.java (line 44) Processing response on a callback from 71@/10.92.208.103
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,889 SliceQueryFilter.java (line 123) collecting 0 of 2147483647: active:false:1@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 1 of 2147483647: name:false:4@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 2 of 2147483647: providerData:false:2283@1321549067179000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 3 of 2147483647: providerID:false:1@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,890 SliceQueryFilter.java (line 123) collecting 4 of 2147483647: timestamp:false:13@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,891 SliceQueryFilter.java (line 123) collecting 5 of 2147483647: vendorData:false:2364@1322777621601000
DEBUG [pool-2-thread-1] 2011-12-01 22:25:39,892 StorageProxy.java (line 867) range slices read DecoratedKey(PROD/US/001/1, 50524f442f55532f3030312f31)
DEBUG [RequestResponseStage:3] 2011-12-01 22:25:39,936 ResponseVerbHandler.java (line 44) Processing response on a callback from 73@/10.8.208.103
DEBUG [ScheduledTasks:1] 2011-12-01 22:26:19,788 LoadBroadcaster.java (line 86) Disseminating load info ...
DEBUG [pool-2-thread-1] 2011-12-01 22:26:39,904 StorageProxy.java (line 874) Range slice timeout: java.util.concurrent.TimeoutException: Operation timed out.
",slebresne,apache.zli,Urgent,Resolved,Fixed,01/Dec/11 22:58,16/Apr/19 09:32
Bug,CASSANDRA-3553,12533553,value validator in the cli does not pick up,"the summary is probably confusing, so here is an example:

{noformat}
[default@testks] describe testcf;
    ColumnFamily: testcf
      Key Validation Class: org.apache.cassandra.db.marshal.UTF8Type
      Default column value validator: org.apache.cassandra.db.marshal.LongType
      Columns sorted by: org.apache.cassandra.db.marshal.LongType
{noformat}

notice both column and column value are under LongType

in the cli (without assume):

[default@testks] set testcf['foo'][1293843587]=30; 
null
InvalidRequestException(why:(Expected 8 or 0 byte long (2)) [testks][testcf][1293843587] failed validation)
        at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15198)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:858)
        at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:830)
        at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:902)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:216)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)

so the above so the value cannot be validated, so now lets try to change the column name also:

[default@testks] set testcf['foo'][30]=30;               
null
InvalidRequestException(why:(Expected 8 or 0 byte long (2)) [testks][testcf][30] failed validation)
        at org.apache.cassandra.thrift.Cassandra$insert_result.read(Cassandra.java:15198)
        at org.apache.cassandra.thrift.Cassandra$Client.recv_insert(Cassandra.java:858)
        at org.apache.cassandra.thrift.Cassandra$Client.insert(Cassandra.java:830)
        at org.apache.cassandra.cli.CliClient.executeSet(CliClient.java:902)
        at org.apache.cassandra.cli.CliClient.executeCLIStatement(CliClient.java:216)
        at org.apache.cassandra.cli.CliMain.processStatementInteractive(CliMain.java:220)
        at org.apache.cassandra.cli.CliMain.main(CliMain.java:346)

now lets set value with 8 characters:

[default@testks] set testcf['foo'][30]=12345678;  
Value inserted.

so that shows column is fine, only the value part is not fine. put it in assume or long() works:

[default@testks] assume testcf validator as long;
Assumption for column family 'testcf' added successfully.
[default@testks] set testcf['foo'][30]=30;       
Value inserted.


or (restart to a new session to un-assume):

[default@testks] set testcf['foo'][30]=long(30);         
Value inserted.",xedin,cywjackson,Low,Resolved,Fixed,02/Dec/11 00:49,16/Apr/19 09:32
Bug,CASSANDRA-3554,12533568,Hints are not replayed unless node was marked down,"If B drops a write from A because it is overwhelmed (but not dead), A will hint the write.  But it will never get notified that B is back up (since it was never down), so it will never attempt hint delivery.",jbellis,jbellis,Normal,Resolved,Fixed,02/Dec/11 05:46,16/Apr/19 09:32
Bug,CASSANDRA-3556,12533581,nodetool info reports inaccurate datacenter/rack for localhost,The datacenter & rack information provided by 'nodetool info' is incorrect when using 'nodetool -h localhost info'. This is because the IP address passed to the EndpointSnitch to determine the datacenter & rack is sourced from the host parameter provided to nodetool and not the actual endpoint address used in the ring.,rbranson,rbranson,Low,Resolved,Fixed,02/Dec/11 08:10,16/Apr/19 09:32
Bug,CASSANDRA-3557,12533585,Commit Log segments are not recycled,Cassandra never recycles segments created after recovery.,rbranson,rbranson,Normal,Resolved,Fixed,02/Dec/11 09:19,16/Apr/19 09:32
Bug,CASSANDRA-3558,12533603,Compression chunk_length_kb is not correctly returned for thrift/avro,"CASSANDRA-3492 fixed the interpretation of chunk_length_kb as a size in bytes but infortunately forgot to convert it back to kb when returning it for thrift/avro. In particular, this means that a {{describe cf}} would return things like {{chunk_length_kb: 65535}}.

I'm afraid that because migration uses Avro this is kind of a problem. One may have to issue an 'update column family' with the right chunk_length_kb to be sure to be in a safe place.",slebresne,slebresne,Normal,Resolved,Fixed,02/Dec/11 11:59,16/Apr/19 09:32
Bug,CASSANDRA-3563,12533686,Packaging should increase vm.max_map_count to accommodate leveled compaction,"As the title says, leveled can create a lot of files and you can run into an IOError trying to mmap all of them.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,02/Dec/11 20:54,16/Apr/19 09:32
Bug,CASSANDRA-3565,12533699,CQL CF creation skips most of the validation code,"Most validation is done by ThriftValidation.validateCfDef, which we call from QP when creating an index but not on CF creation.",jbellis,jbellis,Low,Resolved,Fixed,02/Dec/11 23:08,16/Apr/19 09:32
Bug,CASSANDRA-3566,12533701,Cache saving broken on windows,CASSANDRA-1740 broke cache saving on Windows.,slebresne,jbellis,Low,Resolved,Fixed,02/Dec/11 23:26,16/Apr/19 09:32
Bug,CASSANDRA-3573,12533891,"When Snappy compression is not available on the platform, trying to enable it introduces problems","I've tried to enable compression for some column families in my cluster using Snappy compression.
It does not work and I am having problems with schema updates to remove it (a lot of UNREACHABLE nodes during scema update).

In log I have the next:


ERROR [FlushWriter:961] 2011-12-05 17:16:33,383 AbstractCassandraDaemon.java (line 133) Fatal exception in thread Thread[Flu
shWriter:961,5,main]
java.lang.NoClassDefFoundError: Could not initialize class org.xerial.snappy.Snappy
        at org.apache.cassandra.io.compress.SnappyCompressor.initialCompressedBufferLength(SnappyCompressor.java:39)
        at org.apache.cassandra.io.compress.CompressedSequentialWriter.<init>(CompressedSequentialWriter.java:63)
        at org.apache.cassandra.io.compress.CompressedSequentialWriter.open(CompressedSequentialWriter.java:34)
        at org.apache.cassandra.io.sstable.SSTableWriter.<init>(SSTableWriter.java:91)
        at org.apache.cassandra.db.ColumnFamilyStore.createFlushWriter(ColumnFamilyStore.java:1850)
        at org.apache.cassandra.db.Memtable.writeSortedContents(Memtable.java:250)
        at org.apache.cassandra.db.Memtable.access$400(Memtable.java:47)
        at org.apache.cassandra.db.Memtable$4.runMayThrow(Memtable.java:291)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)

It looks like Snappy can't initialize because it does not have native library for my platform. It would be great if:
1) A check be done on schema update if Snappy can be used
2) If it is enabled and can't be used it would still work without compression writes (but may be outputting some errors to indicate the situation)
 ",xedin,tivv,Low,Resolved,Fixed,05/Dec/11 15:20,16/Apr/19 09:32
Bug,CASSANDRA-3574,12533950,AssertionError in DK,"When running the dtests:

{noformat}
ERROR [pool-2-thread-1] 2011-12-05 16:22:53,940 Cassandra.java (line 4082) Internal error processing execute_cql_query
java.lang.AssertionError
        at org.apache.cassandra.db.DecoratedKey.<init>(DecoratedKey.java:56)
        at org.apache.cassandra.dht.RandomPartitioner.decorateKey(RandomPartitioner.java:47)
        at org.apache.cassandra.cql.QueryProcessor.multiRangeSlice(QueryProcessor.java:161)
        at org.apache.cassandra.cql.QueryProcessor.process(QueryProcessor.java:549)
        at org.apache.cassandra.thrift.CassandraServer.execute_cql_query(CassandraServer.java:1249)
        at org.apache.cassandra.thrift.Cassandra$Processor$execute_cql_query.process(Cassandra.java:4072)
        at org.apache.cassandra.thrift.Cassandra$Processor.process(Cassandra.java:2889)
        at org.apache.cassandra.thrift.CustomTThreadPoolServer$WorkerProcess.run(CustomTThreadPoolServer.java:187)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
{noformat}

I suspect CASSANDRA-1034 is to blame.",slebresne,brandon.williams,Normal,Resolved,Fixed,05/Dec/11 22:42,16/Apr/19 09:32
Bug,CASSANDRA-3577,12533983,TimeoutException When using QuorumEach or ALL consistency on Multi-DC,"Currently we have 
1) StorageProxy.sendMessages() sending messages to the first node in the other DC...  
2) A node in the other DC will remove the ForwardHeader and sendRR (Adding a MessageID to the Queue).
3) The receiving node receives the mutation, updates and sends the response to the Original Co-ordinator.
4) Co-Ordinator now checks for the MessageID (which it never had)

All the Quorum_Each updates fail in the co-ordinator, this issue started showing up after CASSANDRA-3472 the code was introduced in CASSANDRA-2138 .

Simple Fix is to remove the optimization in 0.8 and fix it in 1.x because it seems to me like it needs a change to the Message service version.

Possible Solution: We might want send the message ID's to be used by the all the nodes in other DC (Which is currently generated by the node which receives the Forward request see: (2) ).",vijay2win@yahoo.com,vijay2win@yahoo.com,Normal,Resolved,Fixed,06/Dec/11 03:30,16/Apr/19 09:32
Bug,CASSANDRA-3579,12534022,AssertionError in hintedhandoff - 1.0.5,"We are running a 8 node cassandra cluster running cassandra 1.0.5.
All our CF use leveled compaction.  We ran a test where we did a lot
of inserts for 3 days. After that we started to run tests where some
of the reads could ask for information that was inserted a while back.
In this scenario we are seeing this assertion error in HintedHandoff.

ERROR [HintedHandoff:3] 2011-12-05 15:42:04,324
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[HintedHandoff:3,1,main]
java.lang.RuntimeException: java.lang.RuntimeException:
java.util.concurrent.ExecutionException: java.lang.AssertionError:
originally calculated column size of 470937164 but now it is 470294247
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException:
java.util.concurrent.ExecutionException: java.lang.AssertionError:
originally calculated column size of 470937164 but now it is 470294247
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:330)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
Caused by: java.util.concurrent.ExecutionException:
java.lang.AssertionError: originally calculated column size of
470937164 but now it is 470294247
       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
       at java.util.concurrent.FutureTask.get(FutureTask.java:83)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:326)
       ... 6 more
Caused by: java.lang.AssertionError: originally calculated column size
of 470937164 but now it is 470294247
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
       at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
       at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
       at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       ... 3 more
ERROR [HintedHandoff:3] 2011-12-05 15:42:04,333
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[HintedHandoff:3,1,main]
java.lang.RuntimeException: java.lang.RuntimeException:
java.util.concurrent.ExecutionException: java.lang.AssertionError:
originally calculated column size of 470937164 but now it is 470294247
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:34)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.RuntimeException:
java.util.concurrent.ExecutionException: java.lang.AssertionError:
originally calculated column size of 470937164 but now it is 470294247
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:330)
       at org.apache.cassandra.db.HintedHandOffManager.access$100(HintedHandOffManager.java:81)
       at org.apache.cassandra.db.HintedHandOffManager$2.runMayThrow(HintedHandOffManager.java:353)
       at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
       ... 3 more
Caused by: java.util.concurrent.ExecutionException:
java.lang.AssertionError: originally calculated column size of
470937164 but now it is 470294247
       at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
       at java.util.concurrent.FutureTask.get(FutureTask.java:83)
       at org.apache.cassandra.db.HintedHandOffManager.deliverHintsToEndpoint(HintedHandOffManager.java:326)
       ... 6 more
Caused by: java.lang.AssertionError: originally calculated column size
of 470937164 but now it is 470294247
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
       at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
       at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
       at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       ... 3 more
ERROR [CompactionExecutor:9931] 2011-12-05 15:42:04,333
AbstractCassandraDaemon.java (line 133) Fatal exception in thread
Thread[CompactionExecutor:9931,1,main]
java.lang.AssertionError: originally calculated column size of
470937164 but now it is 470294247
       at org.apache.cassandra.db.compaction.LazilyCompactedRow.write(LazilyCompactedRow.java:124)
       at org.apache.cassandra.io.sstable.SSTableWriter.append(SSTableWriter.java:160)
       at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:158)
       at org.apache.cassandra.db.compaction.CompactionManager$6.call(CompactionManager.java:275)
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)
       at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
       at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
       at java.lang.Thread.run(Thread.java:662)",slebresne,ramesh25,Normal,Resolved,Fixed,06/Dec/11 13:11,16/Apr/19 09:32
Bug,CASSANDRA-3580,12534054,Don't assume the Table instance has been open when dropping a keyspace,"DropKeyspace assumes that the Table had been open (rather, it checks that Table.clear() don't return null). It has been seen however that in the case of a fat client (the sstableloader in that case, but it would be true for any fat client) the Table may not have been open. Let's just remove that assertion.",slebresne,slebresne,Low,Resolved,Fixed,06/Dec/11 17:03,16/Apr/19 09:32
Bug,CASSANDRA-3582,12534138,UserInterruptedException is poorly encapsulated,,jbellis,jbellis,Low,Resolved,Fixed,07/Dec/11 05:05,16/Apr/19 09:32
Bug,CASSANDRA-3584,12534221,Check for 0.0.0.0 is incorrect,"As noted by Jake in the comments to CASSANDRA-3214, we are using == for a String comparison.",jbellis,jbellis,Low,Resolved,Fixed,07/Dec/11 17:36,16/Apr/19 09:32
Bug,CASSANDRA-3588,12534260,cqlsh: HELP for DELETE_USING and DELETE_COLUMNS broken,"type ""HELP DELETE_USING"" or ""HELP DELETE_COLUMNS"" in cqlsh. both of those are referred to by the HELP index, by tab-completion after ""HELP"", and in the help text for DELETE.

nothing is shown but a new command prompt.",thepaul,thepaul,Low,Resolved,Fixed,07/Dec/11 21:59,16/Apr/19 09:32
Bug,CASSANDRA-3596,12534401,cqlsh: DESCRIBE output for a columnfamily does not work as input to same C* instance,"The {{DESCRIBE COLUMNFAMILY}} cqlsh command produces output that is intended to be usable as valid CQL (at least, when given to another Cassandra instance of the same version). But the output yields errors when run:

{noformat}
cqlsh> USE blah;
cqlsh:blah> CREATE COLUMNFAMILY cf1 (c1 int PRIMARY KEY, c2 varchar);
cqlsh:blah> DESCRIBE COLUMNFAMILY cf1;

CREATE COLUMNFAMILY cf1 (
  c1 int PRIMARY KEY,
  c2 text
) WITH
  comment='' AND
  comparator=text AND
  row_cache_provider='ConcurrentLinkedHashCacheProvider' AND
  key_cache_size=200000.000000 AND
  row_cache_size=0.000000 AND
  read_repair_chance=0.100000 AND
  gc_grace_seconds=864000 AND
  default_validation=text AND
  min_compaction_threshold=4 AND
  max_compaction_threshold=32 AND
  row_cache_save_period_in_seconds=0 AND
  key_cache_save_period_in_seconds=14400 AND
  replication_on_write=True;

cqlsh:blah> CREATE COLUMNFAMILY cf1 (
        ...   c1 int PRIMARY KEY,
        ...   c2 text
        ... ) WITH
        ...   comment='' AND
        ...   comparator=text AND
        ...   row_cache_provider='ConcurrentLinkedHashCacheProvider' AND
        ...   key_cache_size=200000.000000 AND
        ...   row_cache_size=0.000000 AND
        ...   read_repair_chance=0.100000 AND
        ...   gc_grace_seconds=864000 AND
        ...   default_validation=text AND
        ...   min_compaction_threshold=4 AND
        ...   max_compaction_threshold=32 AND
        ...   row_cache_save_period_in_seconds=0 AND
        ...   key_cache_save_period_in_seconds=14400 AND
        ...   replication_on_write=True;
Bad Request: replication_on_write is not a valid keyword argument for CREATE COLUMNFAMILY
{noformat}

So it needs to do a better job of determining which CF attributes are valid for which C* versions.",thepaul,thepaul,Low,Resolved,Fixed,08/Dec/11 21:07,16/Apr/19 09:32
Bug,CASSANDRA-3598,12534412,Index Scan's will span across multiple DC's,"Looks like we send requests to all the nodes provided by StorageService.instance.getLiveNaturalEndpoints(keyspace, range.right);
We dont filter it based on blockedFor (Consistency levels).

In a multi DC setup this will cause unnecessary load on the other DC. And even within a DC we might query more nodes than needed.

",vijay2win@yahoo.com,vijay2win@yahoo.com,Low,Resolved,Fixed,08/Dec/11 22:47,16/Apr/19 09:32
Bug,CASSANDRA-3601,12534422,get_count NullPointerException with counters,get_count doesn't currently work for counter columns or super counter columns. The fix seems to be pretty simple.,,ghinkle,Normal,Resolved,Fixed,09/Dec/11 02:05,16/Apr/19 09:32
Bug,CASSANDRA-3603,12534529,CounterColumn and CounterContext use a log4j logger instead of using slf4j like the rest of the code base,"(Will submit patch but not now, no time.)",scode,scode,Low,Resolved,Fixed,09/Dec/11 19:29,16/Apr/19 09:32
Bug,CASSANDRA-3604,12534571,Bad code in org.apache.cassandra.cql.QueryProcessor,"line 206:
            if (rows.get(0).key.key.equals(startKey))
                rows.remove(0);

the equals will always return false because object of different types are compared",slebresne,zolyfarkas,Normal,Resolved,Fixed,10/Dec/11 01:26,16/Apr/19 09:32
Bug,CASSANDRA-3612,12534669,CQL inserting blank key.,"One of our application bug inserted blank key into cluster causing assertion error on key. After checking the root cause, I found it is the bug with CQL and reproducible. Client cassandra-node and cqlsh-1.0.6.
Blank key only work when one column provided.
 
{}
cqlsh> insert into login (KEY,email)values('','');
cqlsh> select * from login;
u'' | u'email',u'' 
cqlsh> insert into login (KEY,email,verified)values('','','');
Request did not complete within rpc_timeout.
cqlsh> insert into login (KEY,verified)values('','');
Request did not complete within rpc_timeout.
cqlsh> insert into login (KEY,email)values('','');
cqlsh> 
cqlsh> select * from login;
u'' | u'email',u'' | u'uid',None
cqlsh> select * from login;
u'' | u'email',u'' | u'uid',None
cqlsh> select * from login;
u'' | u'email',u'' | u'uid',None
cqlsh> 
cqlsh> select * from login;
u'' | u'email',u'' | u'uid',None
u'samalgorai@gmail.com' | u'email',u'samalgorai@gmail.com' | u'password',u'388ad1c312a488ee9e12998fe097f2258fa8d5ee' | u'uid',UUID('05ea41dc-241f-11e1-8521-3da59237b189') | u'verified',u'0'
cqlsh> quit;
{/}

http://pastebin.com/HJn5fHhH",thepaul,samal_,Low,Resolved,Fixed,11/Dec/11 18:01,16/Apr/19 09:32
Bug,CASSANDRA-3614,12534825,"Fatal exception in thread Thread[MigrationStage:1,5,main] (LeveledCompaction)","ERROR 19:29:39,712 Fatal exception in thread Thread[MigrationStage:1,5,main]
java.lang.AssertionError
        at org.apache.cassandra.db.compaction.LeveledManifest.promote(LeveledManifest.java:156)
        at org.apache.cassandra.db.compaction.LeveledCompactionStrategy.handleNotification(LeveledCompactionStrategy.java:141)
        at org.apache.cassandra.db.DataTracker.notifySSTablesChanged(DataTracker.java:488)
        at org.apache.cassandra.db.DataTracker.removeAllSSTables(DataTracker.java:257)
        at org.apache.cassandra.db.ColumnFamilyStore.invalidate(ColumnFamilyStore.java:267)
        at org.apache.cassandra.db.Table.unloadCf(Table.java:361)
        at org.apache.cassandra.db.Table.dropCf(Table.java:343)
        at org.apache.cassandra.db.migration.DropColumnFamily.applyModels(DropColumnFamily.java:87)
        at org.apache.cassandra.db.migration.Migration.apply(Migration.java:156)
        at org.apache.cassandra.db.DefinitionsUpdateVerbHandler$1.runMayThrow(DefinitionsUpdateVerbHandler.java:73)
        at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:662)
",jbellis,asuffield-bossa,Normal,Resolved,Fixed,12/Dec/11 19:36,16/Apr/19 09:32
Bug,CASSANDRA-3615,12534826,CommitLog BufferOverflowException,"Reported on mailing list http://mail-archives.apache.org/mod_mbox/cassandra-dev/201112.mbox/%3CCAJHHpg2Rw_BWFJ9DycRGSYkmwMwrJDK3%3Dzw3HwRoutWHbUcULw%40mail.gmail.com%3E

ERROR 14:07:31,215 Fatal exception in thread
Thread[COMMIT-LOG-WRITER,5,main]
java.nio.BufferOverflowException
at java.nio.Buffer.nextPutIndex(Buffer.java:501)
at java.nio.DirectByteBuffer.putInt(DirectByteBuffer.java:654)
at
org.apache.cassandra.db.commitlog.CommitLogSegment.write(CommitLogSegment.java:259)
at
org.apache.cassandra.db.commitlog.CommitLog$LogRecordAdder.run(CommitLog.java:568)
at
org.apache.cassandra.db.commitlog.PeriodicCommitLogExecutorService$1.runMayThrow(PeriodicCommitLogExecutorService.java:49)
at org.apache.cassandra.utils.WrappedRunnable.run(WrappedRunnable.java:30)
at java.lang.Thread.run(Thread.java:662)
 INFO 14:07:31,504 flushing high-traffic column family CFS(Keyspace='***',
ColumnFamily='***') (estimated 103394287 bytes)

It happened during a fairly standard load process using M/R.",rbranson,rbranson,Normal,Resolved,Fixed,12/Dec/11 19:41,16/Apr/19 09:32
Bug,CASSANDRA-3616,12534843,Temp SSTable and file descriptor leak,"Discussion about this started in CASSANDRA-3532.  It's on it's own ticket now.

Anyhow:
The nodes in my cluster are using a lot of file descriptors, holding open tmp files. A few are using 50K+, nearing their limit (on Solaris, of 64K).

Here's a small snippet of lsof:
java 828 appdeployer *162u VREG 181,65540 0 333884 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776518-Data.db
java 828 appdeployer *163u VREG 181,65540 0 333502 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776452-Data.db
java 828 appdeployer *165u VREG 181,65540 0 333929 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776527-Index.db
java 828 appdeployer *166u VREG 181,65540 0 333859 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776514-Data.db
java 828 appdeployer *167u VREG 181,65540 0 333663 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776480-Data.db
java 828 appdeployer *168u VREG 181,65540 0 333812 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776506-Index.db

I spot checked a few and found they still exist on the filesystem too:
rw-rr- 1 appdeployer appdeployer 0 Dec 12 07:16 /data1/cassandra/data/MA_DDR/messages_meta-tmp-hb-776506-Index.db


After more investigation, it seems to happen during a CompactionTask.
I waited until I saw some -tmp- files hanging around in the data dir:

-rw-r--r--   1 appdeployer appdeployer       0 Dec 12 21:47:10 2011 messages_meta-tmp-hb-788904-Data.db
-rw-r--r--   1 appdeployer appdeployer       0 Dec 12 21:47:10 2011 messages_meta-tmp-hb-788904-Index.db

and then found this in the logs:
 INFO [CompactionExecutor:18839] 2011-12-12 21:47:07,173 CompactionTask.java (line 113) Compacting [SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760408-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760413-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760409-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-788314-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760407-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760412-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760410-Data.db'), SSTableReader(path='/data1/cassandra/data/MA_DDR/messages_meta-hb-760411-Data.db')]

INFO [CompactionExecutor:18839] 2011-12-12 21:47:10,461 CompactionTask.java (line 218) Compacted to [/data1/cassandra/data/MA_DDR/messages_meta-hb-788896-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788897-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788898-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788899-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788900-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788901-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788902-Data.db,/data1/cassandra/data/MA_DDR/messages_meta-hb-788903-Data.db,].  83,899,295 to 83,891,657 (~99% of original) bytes for 75,662 keys at 24.332518MB/s.  Time: 3,288ms.

Note that the timestamp of the 2nd log line matches the last modified time of the files, and has IDs leading up to, *but not including 788904*.

I thought this might be relavent information, but I haven't found the specific cause yet.",slebresne,eparusel,Normal,Resolved,Fixed,12/Dec/11 22:07,16/Apr/19 09:32
Bug,CASSANDRA-3618,12534885,OpenBitSet can allocate more bytes than it needs,"CASSANDRA-2466 changed OpenBitSet to break big long arrays into pages. However, it always allocate full pages, each page being of size 4096 * 8 bytes. This means that we almost always allocate too much bytes, and for a row that has 1 column, the associated row bloom filter allocates 32760 more bytes than it should.

This has a significant impact on performance. In a small test using the SSTableSimpleUnsortedWriter to generate rows with 1 column, 0.8 is about twice as fast as 1.0 because of that (the difference shrink when there is more columns obviously).",slebresne,slebresne,Normal,Resolved,Fixed,13/Dec/11 00:52,16/Apr/19 09:32
Bug,CASSANDRA-3624,12535015,Hinted Handoff - related OOM,"One of our nodes had collected alot of hints for another node, so when the dead node came back and the row mutations were read back from disk, the node died with an OOM-exception (and kept dying after restart, even with increased heap (from 8G to 12G)). The heap dump contained alot of SuperColumns and our application does not use those (but HH does). 

I'm guessing that each mutation is big so that PAGE_SIZE*<mutation_size> does not fit in memory (will check this tomorrow)

A simple fix (if my assumption above is correct) would be to reduce the PAGE_SIZE in HintedHandOffManager.java to something like 10 (or even 1?) to reduce the memory pressure. The performance hit would be small since we are doing the hinted handoff throttle delay sleep before sending every *mutation* anyway (not every page), thoughts?

If anyone runs in to the same problem, I got the node started again by simply removing the HintsColumnFamily* files.",jbellis,marcuse,Normal,Resolved,Fixed,13/Dec/11 20:34,16/Apr/19 09:32
Bug,CASSANDRA-3626,12535056,"Nodes can get stuck in UP state forever, despite being DOWN","This is a proposed phrasing for an upstream ticket named ""Newly discovered nodes that are down get stuck in UP state forever"" (will edit w/ feedback until done):

We have a observed a problem with gossip which, when you are bootstrapping a new node (or replacing using the replace_token support), any node in the cluster which is Down at the time the node is started, will be assumed to be Up and then *never ever* flapped back to Down until you restart the node.

This has at least two implications to replacing or bootstrapping new nodes when there are nodes down in the ring:

* If the new node happens to select a node listed as (UP but in reality is DOWN) as a stream source, streaming will sit there hanging forever.
* If that doesn't happen (by picking another host), it will instead finish bootstrapping correctly, and begin servicing requests all the while thinking DOWN nodes are UP, and thus routing requests to them, generating timeouts.

The way to get out of this is to restart the node(s) that you bootstrapped.

I have tested and confirmed the symptom (that the bootstrapped node things other nodes are Up) using a fairly recent 1.0. The main debugging effort happened on 0.8 however, so all details below refer to 0.8 but are probably similar in 1.0.

Steps to reproduce:

* Bring up a cluster of >= 3 nodes. *Ensure RF is < N*, so that the cluster is operative with one node removed.
* Pick two random nodes A, and B. Shut them *both* off.
* Wait for everyone to realize they are both off (for good measure).
* Now, take node A and nuke it's data directories and re-start it, such that it comes up w/ normal bootstrap (or use replace_token; didn't test that but should not affect it).
* Watch how node A starts up, all the while believing node B is down, even though all other nodes in the cluster agree that B is down and B is in fact still turned off.

The mechanism by which it initially goes into Up state is that the node receives a gossip response from any other node in the cluster, and GossipDigestAck2VerbHandler.doVerb() calls Gossiper.applyStateLocally().

Gossiper.applyStateLocally() doesn't have any local endpoint state for the cluster, so the else statement at the end (""it's a new node"") gets triggered and handleMajorStateChange() is called. handleMajorStateChange() always calls markAlive(), unless the state is a dead state (but ""dead"" here does not mean ""not up"", but refers to joining/hibernate etc).

So at this point the node is up in the mind of the node you just bootstrapped.

Now, in each gossip round doStatusCheck() is called, which iterates over all nodes (including the one falsly Up) and among other things, calls FailureDetector.interpret() on each node.

FailureDetector.interpret() is meant to update its sense of Phi for the node, and potentially convict it. However there is a short-circuit at the top, whereby if we do not yet have any arrival window for the node, we simply return immediately.

Arrival intervals are only added as a result of a FailureDetector.report() call, which never happens in this case because the initial endpoint state we added, which came from a remote node that was up, had the latest version of the gossip state (so Gossiper.reportFailureDetector() will never call report()).

The result is that the node can never ever be convicted.

Now, let's ignore for a moment the problem that a node that is actually Down will be thought to be Up temporarily for a little while. That is sub-optimal, but let's aim for a fix to the more serious problem in this ticket - which is that is stays up forever.

Considered solutions:

* When interpret() gets called and there is no arrival window, we could add a faked arrival window far back in time to cause the node to have history and be marked down. This ""works"" in the particular test case. The problem is that since we are not ourselves actively trying to gossip to these nodes with any particular speed, it might take a significant time before we get any kind of confirmation from someone else that it's actually Up in cases where the node actually *is* Up, so it's not clear that this is a good idea.

* When interpret() gets called and there is no arrival window, we can simply convict it immediately. This has roughly similar behavior as the previous suggestion.

* When interpret() gets called and there is no arrival window, we can add a faked arrival window at the current time, which will allow it to be treated as Up until the usual time has passed before we exceed the Phi conviction threshold.

* When interpret() gets called and there is no arrival window, we can immediately convict it, *and* schedule it for immediate gossip on the next round in order to try to ensure they go Up quickly if they are indeed up. This has an effect of O(n) gossip traffic, as a special case once during node start-up. While theoretically a problem, I personally thing we can ignore it for now since it won't be a significant problem any time soon. However, this is more complicated since the way we queue up messages is asynchronously to background connection attempts. We'd have to make sure the initial gossip message actually gets sent on an open TCP connection (I haven't confirmed whether this will be the case or not).

The first three are simple to implement, possibly the fourth. But in all cases, I am worried about potential negative consequences that I am not seeing.

Thoughts?
",brandon.williams,scode,Normal,Resolved,Fixed,13/Dec/11 23:18,16/Apr/19 09:32
Bug,CASSANDRA-3627,12535058,IN (...) SELECTs don't honor KEY keyword,"The WHERE clause of a SELECT ... IN (...) will not work with the KEY keyword, (but does with named/aliased keys).",urandom,urandom,Normal,Resolved,Fixed,13/Dec/11 23:25,16/Apr/19 09:32
Bug,CASSANDRA-3629,12535105,Bootstrapping nodes don't ensure schema is ready before continuing,"A bootstrapping node will assume that after it has slept for RING_DELAY it has all of the schema migrations and can continue the bootstrap process.  However, with a large enough amount of migrations this is not sufficient and causes problems.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,14/Dec/11 07:40,16/Apr/19 09:32
Bug,CASSANDRA-3632,12535199,using an ant builder in Eclipse is painful,"The {{generate-eclipse-files}} target creates project files that use an Ant builder.  Besides being painfully slow (I've had the runs stack up behind frequent saves), many of Eclipses errors and warnings do not show unless an internal builder is used.",urandom,urandom,Low,Resolved,Fixed,14/Dec/11 20:09,16/Apr/19 09:32
Bug,CASSANDRA-3636,12535263,cassandra 1.0.x breakes APT on debian OpenVZ,"During upgrade from 1.0.6
{code}Setting up cassandra (1.0.6) ...
*error: permission denied on key 'vm.max_map_count'*
dpkg: error processing cassandra (--configure):
 subprocess installed post-installation script returned error exit status 255
Errors were encountered while processing:
 cassandra
{code}",thepaul,zenek_kraweznik0,Low,Resolved,Fixed,15/Dec/11 07:51,16/Apr/19 09:32
Bug,CASSANDRA-3638,12535297,rangeSlice may iterate the whole memtable while just query one row . This may seriously affect the  performance .,"RangeSliceVerbHandler may  just only query one row , but cassandra may iterate the whole memtable .
the problem is in ColumnFamilyStore.getRangeSlice() method .


{color:red} // this iterator may iterate the whole memtable!!{color}
{code:title=ColumnFamilyStore.java|borderStyle=solid}
 public List<Row> getRangeSlice(ByteBuffer superColumn, final AbstractBounds range, int maxResults, IFilter columnFilter)
    throws ExecutionException, InterruptedException
    {
    ...
        DecoratedKey startWith = new DecoratedKey(range.left, null);
        DecoratedKey stopAt = new DecoratedKey(range.right, null);

        QueryFilter filter = new QueryFilter(null, new QueryPath(columnFamily, superColumn, null), columnFilter);
        int gcBefore = (int)(System.currentTimeMillis() / 1000) - metadata.getGcGraceSeconds();

        List<Row> rows;
        ViewFragment view = markReferenced(startWith, stopAt);
        try
        {
            CloseableIterator<Row> iterator = RowIteratorFactory.getIterator(view.memtables, view.sstables, startWith, stopAt, filter, getComparator(), this);
            rows = new ArrayList<Row>();

            try
            {
                // pull rows out of the iterator
                boolean first = true;
                while (iterator.hasNext()) // this iterator may iterate the whole memtable!!               
               {
                    ....
                }
            }
          .....
        }
       .....
        return rows;
    }

{code} 

{color:red} // Just only query one row ,but returned a sublist of columnFamiles   {color}
{code:title=Memtable.java|borderStyle=solid}
// Just only query one row ,but returned a sublist of columnFamiles     
public Iterator<Map.Entry<DecoratedKey, ColumnFamily>> getEntryIterator(DecoratedKey startWith)
    {
        return columnFamilies.tailMap(startWith).entrySet().iterator();
    }
{code} 


{color:red} // entry.getKey() will never bigger or equal to startKey, and then iterate the whole sublist of memtable {color}             
{code:title=RowIteratorFactory.java|borderStyle=solid}
 public IColumnIterator computeNext()
        {
            while (iter.hasNext())
            {
                Map.Entry<DecoratedKey, ColumnFamily> entry = iter.next();
                IColumnIterator ici = filter.getMemtableColumnIterator(entry.getValue(), entry.getKey(), comparator);
                // entry.getKey() will never bigger or equal to startKey, and then iterate the whole sublist of memtable             
                if (pred.apply(ici))  
                    return ici;
            }
            return endOfData();
{code} ",slebresne,jonma,Low,Resolved,Fixed,15/Dec/11 13:12,16/Apr/19 09:32
Bug,CASSANDRA-3641,12535364,inconsistent/corrupt counters w/ broken shards never converge,"We ran into a case (which MIGHT be related to CASSANDRA-3070) whereby we had counters that were corrupt (hopefully due to CASSANDRA-3178). The corruption was that there would exist shards with the *same* node_id, *same* clock id, but *different* counts.

The counter column diffing and reconciliation code assumes that this never happens, and ignores the count. The problem with this is that if there is an inconsistency, the result of a reconciliation will depend on the order of the shards.

In our case for example, we would see the value of the counter randomly fluctuating on a CL.ALL read, but we would get consistent (whatever the node had) on CL.ONE (submitted to one of the nodes in the replica set for the key).

In addition, read repair would not work despite digest mismatches because the diffing algorithm also did not care about the counts when determining the differences to send.

I'm attaching patches that fixes this. The first patch is against our 0.8 branch, which is not terribly useful to people, but I include it because it is the well-tested version that we have used on the production cluster which was subject to this corruption.

The other patch is against trunk, and contains the same change.

What the patch does is:

* On diffing, treat as DISJOINT if there is a count discrepancy.
* On reconciliation, look at the count and *deterministically* pick the higher one, and:
** log the fact that we detected a corrupt counter
** increment a JMX observable counter for monitoring purposes

A cluster which is subject to such corruption and has this patch, will fix itself with and AES + compact (or just repeated compactions assuming the replicate-on-compact is able to deliver correctly).
",scode,scode,Normal,Resolved,Fixed,15/Dec/11 20:09,16/Apr/19 09:32
Bug,CASSANDRA-3644,12535605,parsing of chunk_length_kb silently overflows,"Not likely to trigger for ""real"" values; I noticed because some other bug caused the chunk length setting to be corrupted somehow and take on some huge value having nothing to do with what I asked for in my schema update (not yet identified why; separate issue).",scode,scode,Low,Resolved,Fixed,18/Dec/11 01:32,16/Apr/19 09:32
Bug,CASSANDRA-3650,12535787,"CASSANDRA-2335 was properly resolved - artifact:pom doesn't support the ""groupId"" attribute","CASSANDRA-2335 was about a build error that windows users will see saying ""artifact:pom doesn't support the ""groupId"" attribute"". The fix for this is described in http://wiki.apache.org/cassandra/RunningCassandraInEclipse#artifact:pom_error and was outlined in CASSANDRA-2335. This bug was originally filed in the mistaken belief that the issue wasn't noted, when it had been.",,somerandomperson,Low,Resolved,Fixed,20/Dec/11 06:02,16/Apr/19 09:32
Bug,CASSANDRA-3651,12535793,Truncate shouldn't rethrow timeouts as UA,"Truncate is a very easy operation to timeout, but the timeouts rethrow as UnavailableException which is somewhat confusing.  Instead it should throw TimedOutException.",brandon.williams,brandon.williams,Normal,Resolved,Fixed,20/Dec/11 07:02,16/Apr/19 09:32
Bug,CASSANDRA-3652,12535802,correct and improve stream protocol mismatch error,"The message (and code comment) claims it got a ""newer"" version despite the fact that the check only determines that it is non-equal.

Fix that, and also print the actual version gotten and expected.
",scode,scode,Low,Resolved,Fixed,20/Dec/11 08:38,16/Apr/19 09:32
Bug,CASSANDRA-3655,12535918,NPE when running upgradesstables,"Running a test upgrade from 0.7(version f sstables) to 1.0.
upgradesstables runs for about 40 minutes and then NPE's when trying to retrieve a key.

No files have been succesfully upgraded. Likely related is that scrub (without having run upgrade) consumes all RAM and OOMs.

Possible theory is that a lot of paths call IPartitioner's decorateKey, and, at least in the randompartitioner's implementation, if any of those callers pass a null ByteBuffer, they key will be null in the stack trace below.


java.util.concurrent.ExecutionException: java.lang.NullPointerException
	at java.util.concurrent.FutureTask$Sync.innerGet(FutureTask.java:222)
	at java.util.concurrent.FutureTask.get(FutureTask.java:83)
	at org.apache.cassandra.db.compaction.CompactionManager.performAllSSTableOperation(CompactionManager.java:203)
	at org.apache.cassandra.db.compaction.CompactionManager.performSSTableRewrite(CompactionManager.java:219)
	at org.apache.cassandra.db.ColumnFamilyStore.sstablesRewrite(ColumnFamilyStore.java:970)
	at org.apache.cassandra.service.StorageService.upgradeSSTables(StorageService.java:1540)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:93)
	at com.sun.jmx.mbeanserver.StandardMBeanIntrospector.invokeM2(StandardMBeanIntrospector.java:27)
	at com.sun.jmx.mbeanserver.MBeanIntrospector.invokeM(MBeanIntrospector.java:208)
	at com.sun.jmx.mbeanserver.PerInterface.invoke(PerInterface.java:120)
	at com.sun.jmx.mbeanserver.MBeanSupport.invoke(MBeanSupport.java:262)
	at com.sun.jmx.interceptor.DefaultMBeanServerInterceptor.invoke(DefaultMBeanServerInterceptor.java:836)
	at com.sun.jmx.mbeanserver.JmxMBeanServer.invoke(JmxMBeanServer.java:761)
	at javax.management.remote.rmi.RMIConnectionImpl.doOperation(RMIConnectionImpl.java:1427)
	at javax.management.remote.rmi.RMIConnectionImpl.access$200(RMIConnectionImpl.java:72)
	at javax.management.remote.rmi.RMIConnectionImpl$PrivilegedOperation.run(RMIConnectionImpl.java:1265)
	at javax.management.remote.rmi.RMIConnectionImpl.doPrivilegedOperation(RMIConnectionImpl.java:1360)
	at javax.management.remote.rmi.RMIConnectionImpl.invoke(RMIConnectionImpl.java:788)
	at sun.reflect.GeneratedMethodAccessor39.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at sun.rmi.server.UnicastServerRef.dispatch(UnicastServerRef.java:305)
	at sun.rmi.transport.Transport$1.run(Transport.java:159)
	at java.security.AccessController.doPrivileged(Native Method)
	at sun.rmi.transport.Transport.serviceCall(Transport.java:155)
	at sun.rmi.transport.tcp.TCPTransport.handleMessages(TCPTransport.java:535)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run0(TCPTransport.java:790)
	at sun.rmi.transport.tcp.TCPTransport$ConnectionHandler.run(TCPTransport.java:649)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: java.lang.NullPointerException
	at org.apache.cassandra.db.compaction.PrecompactedRow.removeDeletedAndOldShards(PrecompactedRow.java:65)
	at org.apache.cassandra.db.compaction.PrecompactedRow.<init>(PrecompactedRow.java:92)
	at org.apache.cassandra.db.compaction.CompactionController.getCompactedRow(CompactionController.java:137)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:102)
	at org.apache.cassandra.db.compaction.CompactionIterable$Reducer.getReduced(CompactionIterable.java:87)
	at org.apache.cassandra.utils.MergeIterator$OneToOne.computeNext(MergeIterator.java:200)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at com.google.common.collect.Iterators$7.computeNext(Iterators.java:614)
	at com.google.common.collect.AbstractIterator.tryToComputeNext(AbstractIterator.java:140)
	at com.google.common.collect.AbstractIterator.hasNext(AbstractIterator.java:135)
	at org.apache.cassandra.db.compaction.CompactionTask.execute(CompactionTask.java:172)
	at org.apache.cassandra.db.compaction.CompactionManager$4.perform(CompactionManager.java:229)
	at org.apache.cassandra.db.compaction.CompactionManager$2.call(CompactionManager.java:182)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	... 3 more
",jbellis,tupshin,Normal,Resolved,Fixed,20/Dec/11 22:33,16/Apr/19 09:32
Bug,CASSANDRA-3656,12535960,GC can take 0 ms,,jbellis,jbellis,Low,Resolved,Fixed,21/Dec/11 04:41,16/Apr/19 09:32
Bug,CASSANDRA-3658,12536150,Fix smallish problems find by FindBugs,"I've just run (the newly released) FindBugs 2 out of curiosity. Attaching a number of patches related to issue raised by it. There is nothing major at all so all patches are against trunk.

I've tried keep each issue to it's own patch with a self describing title. It far from covers all FindBugs alerts, but it's a picky tool so I've tried to address only what felt at least vaguely useful. Those are still mostly nits (only patch 2 is probably an actual bug).",slebresne,slebresne,Low,Resolved,Fixed,22/Dec/11 11:28,16/Apr/19 09:32
Bug,CASSANDRA-3659,12536158,Flush non-cfs backed secondary indexes along with CF,Non CFS backed secondary indexes currently don't get flushed alongside CF.  Only CFS backed ones do (i.e. KEYS),tjake,tjake,Low,Resolved,Fixed,22/Dec/11 14:04,16/Apr/19 09:32
Bug,CASSANDRA-3666,12536283,Changing compaction strategy from Leveled to SizeTiered logs millions of messages about nothing to compact,"When column family compaction strategy is changed from Leveled to SizeTiered and there're Leveled compaction tasks pending, Cassandra starting to flood in logs with thousands per sec messages:

Nothing to compact in ColumnFamily1.  Use forceUserDefinedCompaction if you wish to force compaction of single sstables (e.g. for tombstone collection)

As a result, log disk is full and system is down.",jbellis,vjevdokimov,Normal,Resolved,Fixed,23/Dec/11 13:09,16/Apr/19 09:32
Bug,CASSANDRA-3677,12536442,NPE during HH delivery when gossip turned off on target,"probably not important bug

ERROR [OptionalTasks:1] 2011-12-27 21:44:25,342 AbstractCassandraDaemon.java (line 138) Fatal exception in thread Thread[OptionalTasks:1,5,main]
java.lang.NullPointerException
        at org.cliffc.high_scale_lib.NonBlockingHashMap.hash(NonBlockingHashMap.java:113)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:553)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfMatch(NonBlockingHashMap.java:348)
        at org.cliffc.high_scale_lib.NonBlockingHashMap.putIfAbsent(NonBlockingHashMap.java:319)
        at org.cliffc.high_scale_lib.NonBlockingHashSet.add(NonBlockingHashSet.java:32)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleHintDelivery(HintedHandOffManager.java:371)
        at org.apache.cassandra.db.HintedHandOffManager.scheduleAllDeliveries(HintedHandOffManager.java:356)
        at org.apache.cassandra.db.HintedHandOffManager.access$000(HintedHandOffManager.java:84)
        at org.apache.cassandra.db.HintedHandOffManager$1.run(HintedHandOffManager.java:119)
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:471)
        at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:351)
        at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:178)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:165)
        at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:267)
        at java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1110)
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:603)
        at java.lang.Thread.run(Thread.java:679)
",brandon.williams,hsn,Low,Resolved,Fixed,27/Dec/11 20:49,16/Apr/19 09:32
Bug,CASSANDRA-3681,12536553,Multiple threads can attempt hint handoff to the same target,"HintedHandOffManager attempts to prevent multiple threads sending hints to the same target with the queuedDeliveries set, but the code is buggy.  If two handoffs *do* occur concurrently, the second thread can use an arbitrarily large amount of memory skipping tombstones when it starts paging from the beginning of the hint row, looking for the first live hint.  (This is not a problem with a single thread, since it always pages starting with the last-seen hint column name, effectively skipping the tombstones.  Then it compacts when it's done.)

Technically this bug is present in all older Cassandra releases, but it only causes problems in 1.0.x since the hint rows tend to be much larger (since there is one hint per write containing the entire mutation, instead of just one per row consisting of just the key).",jbellis,jbellis,Low,Resolved,Fixed,29/Dec/11 03:03,16/Apr/19 09:32
Bug,CASSANDRA-3684,12536603,Composite Column Support for PIG,"It appears that some changes need to be made to support CompositeColumns. Right now if you try to load and use a column family that utilizes composite columns you get the following exception[1].

It appears to me that we need to modify the storage handler for Pig to support this scenario.


[1]

================================================================================
Backend error message
---------------------
java.lang.RuntimeException: Unexpected data type -1 found in stream.
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:478)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeBag(BinInterSedes.java:522)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:361)
	at org.apache.pig.data.BinInterSedes.writeTuple(BinInterSedes.java:541)
	at org.apache.pig.data.BinInterSedes.writeDatum(BinInterSedes.java:357)
	at org.apache.pig.data.BinSedesTuple.write(BinSedesTuple.java:57)
	at org.apache.pig.impl.io.PigNullableWritable.write(PigNullableWritable.java:123)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:90)
	at org.apache.hadoop.io.serializer.WritableSerialization$WritableSerializer.serialize(WritableSerialization.java:77)
	at org.apache.hadoop.mapred.MapTask$MapOutputBuffer.collect(MapTask.java:1061)
	at org.apache.hadoop.mapred.MapTask$NewOutputCollector.write(MapTask.java:691)
	at org.apache.hadoop.mapreduce.TaskInputOutputContext.write(TaskInputOutputContext.java:80)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapReduce$Map.collect(PigMapReduce.java:116)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.runPipeline(PigMapBase.java:239)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:232)
	at org.apache.pig.backend.hadoop.executionengine.mapReduceLayer.PigMapBase.map(PigMapBase.java:53)
	at org.apache.hadoop.mapreduce.Mapper.run(Mapper.java:144)
	at org.apache.hadoop.mapred.MapTask.runNewMapper(MapTask.java:764)
	at org.apache.hadoop.mapred.MapTask.run(MapTask.java:370)
	at org.apache.hadoop.mapred.Child$4.run(Child.java:272)
	at java.security.AccessController.doPrivileged(Native Method)
	at javax.security.auth.Subject.doAs(Subject.java:396)
	at org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1059)
	at org.apache.hadoop.mapred.Child.main(Child.java:266)

Backend error message
---------------------
java.lang.Throwable: Child Error
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:271)
Caused by: java.io.IOException: Task process exit with nonzero status of 65.
	at org.apache.hadoop.mapred.TaskRunner.run(TaskRunner.java:258)",jalkanen,bcoverston,Normal,Resolved,Fixed,29/Dec/11 18:26,16/Apr/19 09:32
Bug,CASSANDRA-3686,12536702,Streaming retry is no longer performed,"CASSANDRA-3532 changed exception handling when processing incoming stream, but since it wraps all exception into RuntimeException, streaming retry which had been occurred when IOException is thrown no longer works.",yukim,yukim,Low,Resolved,Fixed,30/Dec/11 22:34,16/Apr/19 09:32
Bug,CAMEL-3479,12494486,CamelContinuationServlet - May produce NPE under heavy load,"CAMEL-2986 fixes some issue with the CamelContinationServlet.

However under extreme load and under some circumstances you can still get a NPE.

The Jetty guides for writing and using continuation at
http://wiki.eclipse.org/Jetty/Feature/Continuations

Shows a different style for suspend/resume than we currently have implemented. 
I think it's best practice that we refactor the code in camel-jetty to be aligned with the Jetty guide.

I will follow the _Suspend Resume Pattern_ style listed on the Jetty guide.",davsclaus,davsclaus,Major,Closed,Fixed,03/Jan/11 13:01,25/Oct/11 11:35
Bug,CAMEL-3483,12494540,csv unmarshal and maybe other components uses default encoding ,See discussion in Nabble: http://camel.465427.n5.nabble.com/csv-unmarshal-uses-default-encoding-td3325474.html,muellerc,muellerc,Major,Closed,Fixed,03/Jan/11 22:53,07/Feb/12 15:08
Bug,CAMEL-3484,12494553,Spring feature dependency error in features.xml,The spring feature won't load because the feature has the dependencies in the wrong order. Looks like it happened at revision 949956.,tjsnell,tjsnell,Major,Closed,Fixed,04/Jan/11 01:58,04/Jan/11 02:36
Bug,CAMEL-3487,12494622,camel-juel fails to validate due to missing import,Needed servlet-api,hadrian,tjsnell,Major,Closed,Fixed,04/Jan/11 17:49,25/Oct/11 11:36
Bug,CAMEL-3489,12494633, BindyCsvDataFormat broken for pipe delimited files,"Attempting to unmarshall a pipe delimited CSV file into a POJO using Bindy causese the first and last character the the line processed to be dropped.  It appears that the BindyCsvDataFormat class removes the first and the last character from the line read from the CSV if the seperator is > 1 characters in length (see below or line 162-165 in BindyCsvDataFormat).  For pipe delimited files, you need to specify \\| as the seperator, as | is not evaluated correctly as a java regex by the split fuction.  This leads to the first and last character for the line being parsed being dropped.  From the comments it appears a ""fix"" was added to remove the first and last character of the line when the seperator contains quotes or double quotes.  Making this determination using the length of the seperator, rather than evaluating using a regex seems to be a poor solution that breaks other CSV delimiters.

See Attached for an code example.


",davsclaus,steven.lewis,Major,Closed,Fixed,04/Jan/11 19:32,25/Oct/11 11:35
Bug,CAMEL-3493,12494663,ConcurrentModificationException in DefaultCamelContext.removeRouteDefinitions(),"I get the following exception in DefaultCamelContext.removeRouteDefinitions().  We are removing route definitions from multiple threads.  I see synchronization in various other methods, but not this one. 

Caused by: java.util.ConcurrentModificationException
	at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)
	at java.util.AbstractList$Itr.next(AbstractList.java:343)
	at java.util.AbstractCollection.removeAll(AbstractCollection.java:336)
	at org.apache.camel.impl.DefaultCamelContext.removeRouteDefinitions(DefaultCamelContext.java:588)
",davsclaus,jn,Major,Closed,Fixed,05/Jan/11 01:09,25/Oct/11 11:35
Bug,CAMEL-3497,12494696,"Splitter Component: Setting 'streaming=""true"" parallelProcessing=""true""' consumes large amounts > of heap space for big original messages","Setting 'streaming=""true"" parallelProcessing=""true""' consumes large amounts of heap space for big original messages. E.g. 1024m of heap is not enough to process an 80Mb with 500'000 lines, splitting it line by line.
The problem seems to be the ArrayList in MulticastProcessor line 224. It contains a Future<Exchange> object for every token delivered by the java.util.Scanner. The list is only cleared (going out of scope) after all Future objects have been completed.
",davsclaus,rsteppac,Major,Closed,Fixed,05/Jan/11 09:21,25/Oct/11 11:35
Bug,CAMEL-3498,12494700,"Splitter Component: Setting 'streaming = ""true""' breaks error handling","Setting 'streaming = ""true""' breaks error handling:
If an exception is thrown in a processor, the exception in the subExchange is copied to the original exchange in MulticastProcessor line 554. In Splitter line 140 the original exchange is copied, including the exception that was thrown while processing the previous exchange. This prevents all subsequent exchanges from being processed successfully.
",davsclaus,rsteppac,Major,Closed,Fixed,05/Jan/11 09:38,25/Oct/11 11:36
Bug,CAMEL-3499,12494703,Issue with camel-soap - <dataFormats> -  JAXBContext not created,"I would like to use camel-soap into the following camel route

   <cxf:cxfEndpoint id=""busServicesFinder""
                    address=""http://localhost:8282/cxf/serviceFinder""
                    serviceClass=""fr.client.proxy.Services"">
   </cxf:cxfEndpoint>

   <camelContext trace=""true"" xmlns=""http://camel.apache.org/schema/spring"">

       <dataFormats>
           <soapjaxb id=""soap"" contextPath=""fr.client.proxy""/>
       </dataFormats>

       <route>
           <from uri=""cxf:bean:busServicesFinder?dataFormat=MESSAGE""/>
           <log message=""WebService called"" loggingLevel=""INFO""/>
           <!-- <convertBodyTo type=""String""/> -->
           <unmarshal ref=""soap""/>
           ...

but I get the following error in karaf when CXF extract the content and parse it with JAXB

Additional info

>  * which jaxb bundle is/are deployed ?
[ 200] [Active     ] [            ] [       ] [   60] Apache
ServiceMix :: Bundles :: jaxb-impl (2.1.13.1)

>  * does the system bundle export the sun packages ?
No

200 com.sun.xml.bind.v2
  200 com.sun.xml.bind.v2.model.annotation
  200 com.sun.xml.bind.unmarshaller
  200 com.sun.xml.bind.api.impl
  200 com.sun.xml.bind.v2.schemagen.episode
  200 com.sun.xml.bind.v2.util
  200 com.sun.xml.bind.v2.runtime.unmarshaller
  200 com.sun.xml.bind.marshaller
  200 com.sun.xml.bind.v2.model.runtime
  200 com.sun.xml.bind.v2.runtime.reflect.opt
  200 com.sun.xml.bind.v2.schemagen.xmlschema
  200 com.sun.xml.bind
  200 com.sun.xml.bind.v2.model.core
  200 com.sun.xml.bind.v2.runtime.output
  200 com.sun.xml.bind.v2.bytecode
  200 com.sun.xml.bind.api
  200 com.sun.xml.bind.v2.model.impl
  200 com.sun.xml.bind.v2.model.nav
  200 com.sun.xml.bind.v2.schemagen
  200 com.sun.xml.bind.util
  200 com.sun.xml.bind.v2.runtime.reflect
  200 com.sun.xml.bind.v2.runtime
  200 com.sun.xml.bind.annotation
  200 com.sun.xml.bind.v2.runtime.property

>  * is there a boot delegation on the com.sun.* packages from the jre ?
Yes (karaf - 2.1-SNAPSHOT)

{code}
as thrown exception, unwinding now
org.apache.cxf.interceptor.Fault: Unable to create context
       at org.apache.camel.component.cxf.CxfConsumer$1.checkFailure(CxfConsumer.java:223)[600:org.apache.camel.camel-cxf:2.5.0]
       at org.apache.camel.component.cxf.CxfConsumer$1.setResponseBack(CxfConsumer.java:200)[600:org.apache.camel.camel-cxf:2.5.0]
       at org.apache.camel.component.cxf.CxfConsumer$1.asyncInvoke(CxfConsumer.java:113)[600:org.apache.camel.camel-cxf:2.5.0]
       at org.apache.camel.component.cxf.CxfConsumer$1.invoke(CxfConsumer.java:68)[600:org.apache.camel.camel-cxf:2.5.0]
       at org.apache.cxf.interceptor.ServiceInvokerInterceptor$1.run(ServiceInvokerInterceptor.java:58)[598:org.apache.cxf.bundle:2.2.11]
       at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)[:1.6.0_22]
       at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)[:1.6.0_22]
       at java.util.concurrent.FutureTask.run(FutureTask.java:138)[:1.6.0_22]
       at org.apache.cxf.workqueue.SynchronousExecutor.execute(SynchronousExecutor.java:37)[598:org.apache.cxf.bundle:2.2.11]
       at org.apache.cxf.interceptor.ServiceInvokerInterceptor.handleMessage(ServiceInvokerInterceptor.java:106)[598:org.apache.cxf.bundle:2.2.11]
       at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:243)[598:org.apache.cxf.bundle:2.2.11]
       at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:111)[598:org.apache.cxf.bundle:2.2.11]
       at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.serviceRequest(JettyHTTPDestination.java:311)[598:org.apache.cxf.bundle:2.2.11]
       at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.doService(JettyHTTPDestination.java:275)[598:org.apache.cxf.bundle:2.2.11]
       at org.apache.cxf.transport.http_jetty.JettyHTTPHandler.handle(JettyHTTPHandler.java:70)[598:org.apache.cxf.bundle:2.2.11]
       at org.mortbay.jetty.handler.ContextHandler.handle(ContextHandler.java:765)[569:org.apache.servicemix.bundles.jetty-bundle:6.1.22.1]
       at org.mortbay.jetty.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:230)[569:org.apache.servicemix.bundles.jetty-bundle:6.1.22.1]
       at org.mortbay.jetty.handler.HandlerWrapper.handle(HandlerWrapper.java:152)[569:org.apache.servicemix.bundles.jetty-bundle:6.1.22.1]
       at org.mortbay.jetty.Server.handle(Server.java:326)[569:org.apache.servicemix.bundles.jetty-bundle:6.1.22.1]
       at org.mortbay.jetty.HttpConnection.handleRequest(HttpConnection.java:542)[569:org.apache.servicemix.bundles.jetty-bundle:6.1.22.1]
       at org.mortbay.jetty.HttpConnection$RequestHandler.content(HttpConnection.java:938)[569:org.apache.servicemix.bundles.jetty-bundle:6.1.22.1]
       at org.mortbay.jetty.HttpParser.parseNext(HttpParser.java:755)[569:org.apache.servicemix.bundles.jetty-bundle:6.1.22.1]
       at org.mortbay.jetty.HttpParser.parseAvailable(HttpParser.java:218)[569:org.apache.servicemix.bundles.jetty-bundle:6.1.22.1]
       at org.mortbay.jetty.HttpConnection.handle(HttpConnection.java:404)[569:org.apache.servicemix.bundles.jetty-bundle:6.1.22.1]
       at org.mortbay.io.nio.SelectChannelEndPoint.run(SelectChannelEndPoint.java:409)[569:org.apache.servicemix.bundles.jetty-bundle:6.1.22.1]
       at org.mortbay.thread.QueuedThreadPool$PoolThread.run(QueuedThreadPool.java:582)[569:org.apache.servicemix.bundles.jetty-bundle:6.1.22.1]
Caused by: java.io.IOException: Unable to create context
       at org.apache.camel.util.IOHelper.createIOException(IOHelper.java:80)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.util.IOHelper.createIOException(IOHelper.java:72)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.converter.jaxb.JaxbDataFormat.unmarshal(JaxbDataFormat.java:151)[602:org.apache.camel.camel-jaxb:2.5.0]
       at org.apache.camel.dataformat.soap.SoapJaxbDataFormat.unmarshal(SoapJaxbDataFormat.java:226)[603:org.apache.camel.camel-soap:2.5.0]
       at org.apache.camel.processor.UnmarshalProcessor.process(UnmarshalProcessor.java:51)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.impl.converter.AsyncProcessorTypeConverter$ProcessorToAsyncProcessorBridge.process(AsyncProcessorTypeConverter.java:50)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:174)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:299)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:256)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.Pipeline.process(Pipeline.java:143)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.Pipeline.process(Pipeline.java:78)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:99)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[202:org.apache.camel.camel-core:2.5.0]
       at org.apache.camel.component.cxf.CxfConsumer$1.asyncInvoke(CxfConsumer.java:80)[600:org.apache.camel.camel-cxf:2.5.0]
       ... 23 more
Caused by: javax.xml.bind.JAXBException: Unable to create context
 - with linked exception:
[java.lang.NoSuchMethodException:
com.sun.xml.bind.v2.ContextFactory.createContext(java.lang.String,
java.lang.ClassLoader)]
       at javax.xml.bind.ContextFinder.find(ContextFinder.java:72)[198:org.apache.servicemix.specs.jaxb-api-2.1:1.5.0]
       at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:77)[198:org.apache.servicemix.specs.jaxb-api-2.1:1.5.0]
       at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:73)[198:org.apache.servicemix.specs.jaxb-api-2.1:1.5.0]
       at javax.xml.bind.JAXBContext.newInstance(JAXBContext.java:69)[198:org.apache.servicemix.specs.jaxb-api-2.1:1.5.0]
       at org.apache.camel.dataformat.soap.SoapJaxbDataFormat.createContext(SoapJaxbDataFormat.java:286)[603:org.apache.camel.camel-soap:2.5.0]
       at org.apache.camel.converter.jaxb.JaxbDataFormat.getContext(JaxbDataFormat.java:183)[602:org.apache.camel.camel-jaxb:2.5.0]
       at org.apache.camel.converter.jaxb.JaxbDataFormat.unmarshal(JaxbDataFormat.java:126)[602:org.apache.camel.camel-jaxb:2.5.0]
       ... 47 more
Caused by: java.lang.NoSuchMethodException:
com.sun.xml.bind.v2.ContextFactory.createContext(java.lang.String,
java.lang.ClassLoader)
       at java.lang.Class.getMethod(Class.java:1605)[:1.6.0_22]
       at javax.xml.bind.ContextFinder.find(ContextFinder.java:69)[198:org.apache.servicemix.specs.jaxb-api-2.1:1.5.0]
       ... 53 more
{code}",njiang,cmoulliard,Major,Closed,Fixed,05/Jan/11 10:07,06/Jan/11 10:08
Bug,CAMEL-3526,12495205,SplitterWithXqureyTest refereces Sun JDK specific classes,"
The SplitterWithXqureyTest references com.sun classes that are internal to the Sun JDK.  This makes it not build with an IBM jdk, but also causes errors in Eclipse as eclipse prohibits access to them.

Will attach patch.",njiang,dkulp,Minor,Closed,Fixed,11/Jan/11 03:51,25/Oct/11 11:35
Bug,CAMEL-3528,12495270,HybridSourceDataBinding not thread safe...,"
The HybridSourceDataBinding in camel-cxf is not thread safe.  The readers/writers in CXF hold state (like schemas and properties) and  must be created when needed.    

",hadrian,dkulp,Major,Closed,Fixed,11/Jan/11 16:44,25/Oct/11 11:35
Bug,CAMEL-3529,12495280,camel-atom - Issue with seda consumer,"This test fails
https://hudson.apache.org/hudson/job/Camel.trunk.fulltest/org.apache.camel$camel-atom/109/testReport/org.apache.camel.component.atom/AtomGoodBlogsTest/testFiltering/

There is an issue with camel-atom starting 2 consumers. I have to dig into this, next morning.",davsclaus,davsclaus,Major,Closed,Fixed,11/Jan/11 18:26,25/Oct/11 11:35
Bug,CAMEL-3531,12495334,scala - xpath not working together with choice/when,"When using the Scala DSL, xpath expressions inside when() do not work as expected. As an example:
{code:none}
     ""direct:a"" ==> {
     choice {
        when (xpath(""//hello"")) to (""mock:english"")
        when (xpath(""//hallo"")) {
          to (""mock:dutch"")
          to (""mock:german"")
        } 
        otherwise to (""mock:french"")
      }
    }

// Send messages
""direct:a"" ! (""<hello/>"", ""<hallo/>"", ""<hellos/>"")
{code}

Here we should receive 1 message in each of the mocks. For whatever reason, all 3 messages go to mock:english. Similar routes work as expected with the Java DSL. ",davsclaus,cpmcdaniel,Major,Closed,Fixed,12/Jan/11 05:17,25/Oct/11 11:35
Bug,CAMEL-3534,12495386,Stopping a route has side effects on adviced routes,"See nable:
http://camel.465427.n5.nabble.com/errorHandler-when-stopping-routes-in-test-td3335015.html",davsclaus,jfansler,Minor,Closed,Fixed,12/Jan/11 16:10,02/May/13 02:29
Bug,CAMEL-3535,12495418,Aggregation fails to call onComplete for exchanges if the aggregation is after a bean or process.,"When creating a route that contains an aggregation, if that aggregation is preceded by a bean or process, it will fail to call AggregateOnCompletion.onComplete(). I've attached a unit test that can show you the behavior. Trace level loggging will need to be enabled to see the difference. With the call to the bean, it won't show the following log entry:
{noformat}TRACE org.apache.camel.processor.aggregate.AggregateProcessor - Aggregated exchange onComplete: Exchange[Message: ab]{noformat}
If you remove the bean call, it'll start calling onComplete() again.

What I've noticed is that if this call is not made, it ends up in a memory leak since the inProgressCompleteExchanges HashSet in AggregateProcessor never has any exchange ID's removed.",davsclaus,bfeaver,Major,Closed,Fixed,12/Jan/11 20:43,25/Oct/11 11:36
Bug,CAMEL-3537,12495465,camel-snmp - Does not support tcp protocol,"See nabble
http://camel.465427.n5.nabble.com/camel-snmp-2-5-problems-tp3339373p3339373.html",davsclaus,davsclaus,Minor,Closed,Fixed,13/Jan/11 08:12,25/Oct/11 11:35
Bug,CAMEL-3540,12495503,Jt400DataQueueConsumer incorrectly implements timeout semantics (jt400 component),"Jt400DataQueueConsumer implementation of receive(long) passes the timeout argument directly to com.ibm.as400.access.DataQueue.read(int), not performing unit conversion. However, Jt400DataQueueConsumer.receive(long) accepts milliseconds, whereas DataQueue.read(int) accepts seconds as the time unit.

Also, invoking Jt400DataQueueConsumer.receive() results in a call to DataQueue.read(), which is not a blocking call; on the contrary, it will not wait for entries.

Code snippet below.

{code:title=Jt400DataQueueConsumer.java|borderStyle=solid}
        DataQueue queue = endpoint.getDataQueue();
        try {
            DataQueueEntry entry;
            if (timeout >= 0) {
                entry = queue.read((int)timeout);
            } else {
                entry = queue.read();
            }
{code}

Note that the submitted patch floors the timeout value when converting to seconds, but different rounding might be desired, which should be specified in the class documentation.",davsclaus,jloureiro,Minor,Closed,Fixed,13/Jan/11 15:53,25/Oct/11 11:36
Bug,CAMEL-3543,12495591,Loading routes using loadRouteDefinition from CamelContext lacks some logic from camel-core-xml,"We have shared logic in camel-core-xml to use when build / loading routes from XML using JAXB. The routes must be prepared before they are usable for creating the runtime processors.

The logic should be moved into camel-core in the model package so we have it centralized and its more easy to reuse

See nabble
http://camel.465427.n5.nabble.com/Loading-routes-from-XML-files-with-Camel-2-4-0-tp3340082p3340082.html",davsclaus,davsclaus,Minor,Closed,Fixed,14/Jan/11 14:25,25/Oct/11 11:36
Bug,CAMEL-3545,12495675,MethodCallExpression doesn't validate whether the method exists for all cases,"I tried to refactor

{code:title=org.apache.camel.model.language.MethodCallExpression.java}
    public Expression createExpression(CamelContext camelContext) {
        Expression answer;

        if (beanType != null) {            
            instance = ObjectHelper.newInstance(beanType);
            return new BeanExpression(instance, getMethod(), parameterType); // <--
        } else if (instance != null) {
            return new BeanExpression(instance, getMethod(), parameterType); // <--
        } else {
            String ref = beanName();
            // if its a ref then check that the ref exists
            BeanHolder holder = new RegistryBean(camelContext, ref);
            // get the bean which will check that it exists
            instance = holder.getBean();
            answer = new BeanExpression(ref, getMethod(), parameterType);
        }

        // validate method
        validateHasMethod(camelContext, instance, getMethod(), parameterType);

        return answer;
    }
{code}

to

{code:title=org.apache.camel.model.language.MethodCallExpression.java}
    public Expression createExpression(CamelContext camelContext) {
        Expression answer;

        if (beanType != null) {            
            instance = ObjectHelper.newInstance(beanType);
            answer = new BeanExpression(instance, getMethod(), parameterType); // <--
        } else if (instance != null) {
            answer = new BeanExpression(instance, getMethod(), parameterType); // <--
        } else {
            String ref = beanName();
            // if its a ref then check that the ref exists
            BeanHolder holder = new RegistryBean(camelContext, ref);
            // get the bean which will check that it exists
            instance = holder.getBean();
            answer = new BeanExpression(ref, getMethod(), parameterType);
        }

        // validate method
        validateHasMethod(camelContext, instance, getMethod(), parameterType);

        return answer;
    }
{code}

so that the created BeanExpression is also validate if you provide the bean type or an instance. With this change, some tests in org.apache.camel.language.SimpleTest fails.
I'm not sure whether the tests are faulty or if it's a bug.
Also not sure whether this should fixed in 2.6. ",davsclaus,muellerc,Major,Closed,Fixed,15/Jan/11 14:29,25/Oct/11 11:36
Bug,CAMEL-3559,12495901,Aggregator - The completionFromBatchConsumer option dont aggregate the last incoming exchange,"See nabble
http://camel.465427.n5.nabble.com/Last-Aggregated-Exchange-lost-Aggregator-with-a-Batch-Consumer-and-persistent-AggregationRepository-tp3346214p3346214.html",davsclaus,davsclaus,Major,Closed,Fixed,18/Jan/11 14:31,25/Oct/11 11:36
Bug,CAMEL-3560,12495996,Detect uncaught exceptions in UoWProcessor to ensure UoW is done even for those uncaught exceptions,"The Camel routing engines will handle this in 99.9% of the cases, but when you shutdown a Spring AC then Spring may stop beans in whatever order and this can cause those beans to fail operating during a graceful shutdown. And in worst case exceptions is thrown in situations where they are not normally done.

To cater for that and other situations the UoWProcessor should detect this and act accordingly.

This ensure the in flight registry will be tracked and we are not stuck with a missing inflight message, causing Camel to wait for the 300 sec timeout to shutdown.

For example just try hitting ctrl + c in that camel-example-management and you can see such an example.",davsclaus,davsclaus,Major,Closed,Fixed,19/Jan/11 11:05,25/Oct/11 11:35
Bug,CAMEL-3569,12496228,Fix compiler error in code generated by camel-archetype-component,"Sample output:
{code}

[INFO] --- maven-compiler-plugin:2.3.2:compile (default-compile) @ camel-xxx ---
[WARNING] File encoding has not been set, using platform encoding MacRoman, i.e. build is platform dependent!
[INFO] Compiling 4 source files to /w1/apache/camel/projects/camel-xxx/target/classes
[INFO] -------------------------------------------------------------
[ERROR] COMPILATION ERROR : 
[INFO] -------------------------------------------------------------
[ERROR] /w1/apache/camel/projects/camel-xxx/src/main/java/org/apache/camel/example/HelloWorldConsumer.java:[28,7] org.apache.camel.example.HelloWorldConsumer is not abstract and does not override abstract method poll() in org.apache.camel.impl.ScheduledPollConsumer
[ERROR] /w1/apache/camel/projects/camel-xxx/src/main/java/org/apache/camel/example/HelloWorldConsumer.java:[38,19] poll() in org.apache.camel.example.HelloWorldConsumer cannot override poll() in org.apache.camel.impl.ScheduledPollConsumer; attempting to use incompatible return type
found   : void
required: int
[ERROR] /w1/apache/camel/projects/camel-xxx/src/main/java/org/apache/camel/example/HelloWorldConsumer.java:[36,4] method does not override or implement a method from a supertype
[INFO] 3 errors 
{code}",hadrian,hadrian,Major,Closed,Fixed,21/Jan/11 04:45,25/Oct/11 11:35
Bug,CAMEL-3572,12496255,Camel transport for cxf does not forward exception to camel exchange,"I encountered a problem that happens if you run a route like
from(""servlet:///test"").to(""direct:cxfendpoint"")

Behind the direct endpoint there is a cxf jax-rs endpoint that is secured by spring-security. If spring security denies access based on method annotations it throws an AccessDeniedException. As JAX-RS does not handle it the exception hits the CamelDestination. There it is only logged and swallowed. As the client then gets no error it tries to handle the reponse with jaxb which fails. (See below)

21.01.2011 11:19:51 org.apache.cxf.jaxrs.provider.AbstractJAXBProvider handleJAXBException
WARNUNG: javax.xml.bind.UnmarshalException
 - with linked exception:
[com.ctc.wstx.exc.WstxEOFException: Unexpected EOF in prolog
 at [row,col {unknown-source}]: [1,0]]
	at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallerImpl.handleStreamException(UnmarshallerImpl.java:426)
	at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallerImpl.unmarshal0(UnmarshallerImpl.java:362)
	at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallerImpl.unmarshal(UnmarshallerImpl.java:332)
	at org.apache.cxf.jaxrs.provider.JAXBElementProvider.unmarshalFromInputStream(JAXBElementProvider.java:229)
	at org.apache.cxf.jaxrs.provider.JAXBElementProvider.doUnmarshal(JAXBElementProvider.java:195)
	at org.apache.cxf.jaxrs.provider.JAXBElementProvider.readFrom(JAXBElementProvider.java:166)
	at org.apache.cxf.jaxrs.client.AbstractClient.readBody(AbstractClient.java:445)
	at org.apache.cxf.jaxrs.client.ClientProxyImpl.handleResponse(ClientProxyImpl.java:463)
	at org.apache.cxf.jaxrs.client.ClientProxyImpl.doChainedInvocation(ClientProxyImpl.java:445)
	at org.apache.cxf.jaxrs.client.ClientProxyImpl.invoke(ClientProxyImpl.java:177)
	at $Proxy18.getUsers(Unknown Source)
	at client.RESTClient.sayHelloAsUser(RESTClient.java:63)
	at client.RESTClient.main(RESTClient.java:34)
Caused by: com.ctc.wstx.exc.WstxEOFException: Unexpected EOF in prolog
 at [row,col {unknown-source}]: [1,0]
	at com.ctc.wstx.sr.StreamScanner.throwUnexpectedEOF(StreamScanner.java:682)
	at com.ctc.wstx.sr.BasicStreamReader.handleEOF(BasicStreamReader.java:2090)
	at com.ctc.wstx.sr.BasicStreamReader.nextFromProlog(BasicStreamReader.java:1996)
	at com.ctc.wstx.sr.BasicStreamReader.next(BasicStreamReader.java:1100)
	at com.sun.xml.bind.v2.runtime.unmarshaller.StAXStreamConnector.bridge(StAXStreamConnector.java:160)
	at com.sun.xml.bind.v2.runtime.unmarshaller.UnmarshallerImpl.unmarshal0(UnmarshallerImpl.java:360)
	... 11 more",cschneider,cschneider,Major,Closed,Fixed,21/Jan/11 10:41,25/Oct/11 11:35
Bug,CAMEL-3575,12496322,"allow ScheduledRoutePolicy to handle more than one action (start, stop, resume, pause)","The current implementation of ScheuledRoutePolicy only supports one type of action (start,stop,resume,pause) for only one route.  If, for example,  you attempt to configure a CronScheduleRoutePolicy with a routeStartTime and routeStopTime, only the one will by handled because ScheduleRoutePolicy (super-class of CronScheduleRoutePolicy) only store one action and one route in the SchedulerContext:

 protected void loadCallbackDataIntoSchedulerContext(Action action, Route route) throws SchedulerException {
       getScheduler().getContext().put(SCHEDULED_ACTION, action);
        getScheduler().getContext().put(SCHEDULED_ROUTE, route);
}

with the effect of creating two timers (one for startTime, the other for stopTime) that execute the same action (the last call to scheduleRoute).

",akarpe,rveguilla,Major,Closed,Fixed,21/Jan/11 19:37,25/Oct/11 11:35
Bug,CAMEL-3581,12496537,Camel CXF does not forward the response code from CXF to Camel,"When calling a CXF service from a camel route the reponse code from CXF should be set on the camel exachange.

Currently this is not done. So when CXF sets the reponse code to e.g. 403 camel still sends 200 and the client will not be able to work with this reponse.
",cschneider,cschneider,Major,Closed,Fixed,24/Jan/11 13:38,25/Oct/11 11:36
Bug,CAMEL-3584,12496560,Concurrent writes to the same file has race condition,"See nabble
http://camel.465427.n5.nabble.com/Using-Splitter-and-file-producer-with-option-fileExist-Append-failed-tp3353618p3353618.html

We need to synchronize on the file name to avoid concurrent writes to the same file.",davsclaus,davsclaus,Major,Closed,Fixed,24/Jan/11 15:17,25/Oct/11 11:35
Bug,CAMEL-3595,12496943,FTP rename fails,"(This is related to the user group discussion http://camel.465427.n5.nabble.com/Problem-renaming-existing-file-in-FTP-td3307670.html)

When using a tmpPrefix when uploading, the rename from the tmp-file to the destination file fails, if the destination file is already present, even though the option fileExist=Override is used.

We have the problem on just a few destination servers which all seems to be windows machines(?).

I have fetched the 2.4 branch of camel-ftp to find out why it does not work, since the unit test you refer to in your previous post do excercise the problem.

It appears that the cause of the problem is, that the FtpOperations.existsFile fails to return true, which then causes the code not to enter the part where it deletes the exisiting file.

The reason for the FtpOperations.existsFile to return false, is, that the list of existing files has the directoryname prepended, e.g. I'm trying to upload Test.REN to folder test/rename, and in the code 
""if (existing.equals(onlyName))"" 
existing=test/rename\Test.REN and onlyName=Test.REN - which returns false - but causes the later rename to fail as the target folder did contain the Test.REN. 

My guess is that it is server dependent whether the returned list of names in the directory include the directory name or not?

To fix this I have added the following line of code before the if-statement
existing = FileUtil.stripPath(FileUtil.normalizePath(existing));
So it will strip the path part of the file name before comparing it to onlyName.",davsclaus,mikaelfj,Major,Closed,Fixed,27/Jan/11 14:08,25/Oct/11 11:36
Bug,CAMEL-3608,12497334,HttpProducer should not eat up the content-type when working in proxy mode.,"Here is the user story about it.
http://stackoverflow.com/questions/4810050/how-do-i-implement-a-pass-through-jax-rs-proxy-using-apache-camel",njiang,njiang,Major,Closed,Fixed,01/Feb/11 13:26,25/Oct/11 11:35
Bug,CAMEL-3614,12497440,"When the OsgiTypeConverter looks for TypeConverterLoader services in the OSGi registry, it should use its own BundleContext instead of the one from the client bundle so that class space consistency can be fully enforced",,gnodet,gnodet,Major,Closed,Fixed,02/Feb/11 09:51,25/Oct/11 11:36
Bug,CAMEL-3616,12497470,camel-josql doesnot export org.apache.camel.language.sql.* ,,rajdavies,rajdavies,Major,Closed,Fixed,02/Feb/11 15:04,25/Oct/11 11:35
Bug,CAMEL-3617,12497561,Inconsistent filename value when move attribute is used with File component,"Unless I miss a point, when I use the following endpoint, the file:name value is incorrect and is equal to file:absolute.path

<endpoint id=""fileEndpoint"" uri=""file:${queue.input.folder}?recursive=true&amp;include=.*\.dat&amp;move=${queue.done.folder}/$simple{file:name}&amp;moveFailed=${queue.failed.folder}/$simple{file:name}"" />

${queue.input.folder}, ${queue.done.folder} and ${queue.failed.folder} are absolute paths resolved by Spring.

In fact, Camel tries to move the file to ${queue.done.folder}/${queue.input.folder}/$simple{file:name}
I've also tried using $simple{header.CamelFileName} instead of $simple{file:name} and it gives the same result.

For now, I've found a workaround using a processor which put the CamelFileName header value into a ""destFile"" property 
<endpoint id=""fileEndpoint"" uri=""file:${queue.input.folder}?recursive=true&amp;include=.*\.dat&amp;move=${queue.done.folder}/$simple{property.destFile}&amp;moveFailed=${queue.failed.folder}/$simple{property.destFile}"" />
",davsclaus,jmmorel,Major,Closed,Fixed,03/Feb/11 14:00,25/Oct/11 11:35
Bug,CAMEL-3620,12497618,Blueprint container goes 'GracePeriod' if component is defined in the same XML file,"When a Blueprint file contains both a route and a component bean definition, the Camel routes get started correct but the Blueprint container will go to status 'GracePeriod'.  

An example:
{code:xml}
<?xml version=""1.0"" encoding=""UTF-8""?>
<blueprint
    xmlns=""http://www.osgi.org/xmlns/blueprint/v1.0.0""
    xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
    xsi:schemaLocation=""
      http://www.osgi.org/xmlns/blueprint/v1.0.0
      http://www.osgi.org/xmlns/blueprint/v1.0.0/blueprint.xsd"">

    <camelContext xmlns=""http://camel.apache.org/schema/blueprint"">
      <route>
        <from uri=""file:activemq/input""/>
        <to uri=""file:activemq/output""/>

        <setBody>
          <simple>
            FileMovedEvent(file: ${file:name}, timestamp: ${date:now:hh:MM:ss.SSS})
          </simple>
        </setBody>
        <to uri=""amq://events"" />
      </route>
    </camelContext>

    <bean id=""amq"" class=""org.apache.activemq.camel.component.ActiveMQComponent"">
      <property name=""brokerURL"" value=""vm://default?create=false&amp;waitForStart=10000""/>
    </bean>

</blueprint>
{code}

After the 5 minute time-out period, the routes are stopped and we end up with this message in the log file.
{noformat}
Unable to start blueprint container for bundle activemq2.xml due to unresolved dependencies 
  [(&(component=log)(objectClass=org.apache.camel.spi.ComponentResolver)), 
   (&(component=amq)(objectClass=org.apache.camel.spi.ComponentResolver))]
{noformat}",gnodet,gertvanthienen,Major,Closed,Fixed,03/Feb/11 20:49,25/Oct/11 11:35
Bug,CAMEL-3637,12497816,Mistake in camel-eventAdmin feature,"The Camel Karaf feature camel-eventAdmin is not correct.

The features is described as follow:
<feature name=""camel-eventAdmin"" version=""2.6.0"">
  <feature version=""2.6.0"">camel-core</feature>
  <bundle>mvn:org.apache.camel/camel-eventAdmin/2.6.0</bundle>
</feature>

but the camel-eventAdmin artifact correct name is camel-eventadmin:
http://repo2.maven.org/maven2/org/apache/camel/camel-eventadmin/2.6.0/camel-eventadmin-2.6.0.jar

This typo mistake provides:
Downloading: http://repo1.maven.org/maven2/org/apache/camel/camel-eventAdmin/2.6.0/camel-eventAdmin-2.6.0.jar
[INFO] Unable to find resource 'org.apache.camel:camel-eventAdmin:jar:2.6.0' in repository central (http://repo1.maven.org/maven2)
[INFO] ------------------------------------------------------------------------
[ERROR] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Can't resolve bundle org.apache.camel:camel-eventAdmin:jar:2.6.0
[INFO] ------------------------------------------------------------------------

I'm gonna submit a patch to fix that.
",jbonofre,jbonofre,Major,Closed,Fixed,07/Feb/11 07:47,25/Oct/11 11:35
Bug,CAMEL-3640,12497964,camel-lucene - Should use Camel type converter when casting headers to String,"See nabble
http://camel.465427.n5.nabble.com/Bug-in-camel-lucene-component-tp3369088p3369088.html",davsclaus,davsclaus,Minor,Closed,Fixed,08/Feb/11 08:51,25/Oct/11 11:35
Bug,CAMEL-3642,12498088,camel-ftp - Should import bundle from camel-core to load file strategies,"See forum
http://fusesource.com/forums/thread.jspa?threadID=2654&tstart=0",davsclaus,davsclaus,Major,Closed,Fixed,09/Feb/11 07:57,25/Oct/11 11:36
Bug,CAMEL-3650,12498227,SMSC initiated unbind spawns exponential amounts of reconnect threads,"It took me a bit of time to put my finger on this problem.

When an unbind PDU is sent from the SMSC side to a camel-smpp endpoint, a cascading amount reconnect threads are spawned by the reconnect code. I can reliably re-produce this problem having a simple smpp consumer to a SMSC / simulator and issuing an unbind from the SMSC / simulator.

Here are some logs (I have added additional debug messages to the SmppConsumer to show the object references and state of the SmppSession objects):

{code}
2011-02-10 12:23:05,931 [ing.Main.main()] INFO  MainSupport          - Apache Camel 2.6.0 starting
2011-02-10 12:23:06,338 [ing.Main.main()] INFO  amelNamespaceHandler - OSGi environment not detected.
2011-02-10 12:23:06,338 [ing.Main.main()] DEBUG amelNamespaceHandler - Using org.apache.camel.spring.CamelContextFactoryBean as CamelContextBeanDefinitionParser
2011-02-10 12:23:06,764 [ing.Main.main()] DEBUG amelNamespaceHandler - Registered default: org.apache.camel.spring.CamelProducerTemplateFactoryBean with id: template on camel context: camelTestRouteBuilderContext
2011-02-10 12:23:06,764 [ing.Main.main()] DEBUG amelNamespaceHandler - Registered default: org.apache.camel.spring.CamelConsumerTemplateFactoryBean with id: consumerTemplate on camel context: camelTestRouteBuilderContext
2011-02-10 12:23:06,965 [ing.Main.main()] DEBUG SpringCamelContext   - Set the application context classloader to: java.net.URLClassLoader@16e7eec9
2011-02-10 12:23:06,984 [ing.Main.main()] DEBUG elContextFactoryBean - Found JAXB created routes: []
2011-02-10 12:23:06,990 [ing.Main.main()] DEBUG SpringCamelContext   - Adding routes from builder: Routes: []
2011-02-10 12:23:07,059 [ing.Main.main()] DEBUG SpringCamelContext   - onApplicationEvent: org.springframework.context.event.ContextRefreshedEvent[source=org.springframework.context.support.ClassPathXmlApplicationContext@4c9549af: startup date [Thu Feb 10 12:23:05 SAST 2011]; root of context hierarchy]
2011-02-10 12:23:07,059 [ing.Main.main()] INFO  SpringCamelContext   - Apache Camel 2.6.0 (CamelContext: camelTestRouteBuilderContext) is starting
2011-02-10 12:23:07,059 [ing.Main.main()] INFO  SpringCamelContext   - JMX enabled. Using ManagedManagementStrategy.
2011-02-10 12:23:07,106 [ing.Main.main()] DEBUG faultManagementAgent - Starting JMX agent on server: com.sun.jmx.mbeanserver.JmxMBeanServer@7e3b014c
2011-02-10 12:23:07,168 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=context,name=""camelTestRouteBuilderContext""
2011-02-10 12:23:07,194 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=services,name=DefaultTypeConverter(0x61e481c1)
2011-02-10 12:23:07,194 [ing.Main.main()] DEBUG DefaultTypeConverter - Loading type converters ...
2011-02-10 12:23:07,194 [ing.Main.main()] INFO  nTypeConverterLoader - Loading file META-INF/services/org/apache/camel/TypeConverter to retrieve list of packages, from url: jar:file:/home/jacovt/.m2/repository/org/apache/camel/camel-core/2.6.0/camel-core-2.6.0.jar!/META-INF/services/org/apache/camel/TypeConverter
2011-02-10 12:23:07,195 [ing.Main.main()] INFO  nTypeConverterLoader - Loading file META-INF/services/org/apache/camel/TypeConverter to retrieve list of packages, from url: jar:file:/home/jacovt/.m2/repository/org/apache/activemq/activemq-camel/5.3.1-fuse-02-00/activemq-camel-5.3.1-fuse-02-00.jar!/META-INF/services/org/apache/camel/TypeConverter
2011-02-10 12:23:07,196 [ing.Main.main()] DEBUG ageScanClassResolver - Searching for annotations of org.apache.camel.Converter in packages: [org.apache.camel.component.file, org.apache.camel.component.bean, org.apache.camel.converter, org.apache.activemq.camel.converter]
2011-02-10 12:23:07,474 [ing.Main.main()] DEBUG ageScanClassResolver - Found: [class org.apache.camel.component.file.GenericFileConverter, class org.apache.camel.component.bean.BeanConverter, class org.apache.camel.converter.CamelConverter, class org.apache.camel.converter.CollectionConverter, class org.apache.camel.converter.IOConverter, class org.apache.camel.converter.NIOConverter, class org.apache.camel.converter.ObjectConverter, class org.apache.camel.converter.TimePatternConverter, class org.apache.camel.converter.jaxp.DomConverter, class org.apache.camel.converter.jaxp.StaxConverter, class org.apache.camel.converter.jaxp.StreamSourceConverter, class org.apache.camel.converter.jaxp.XmlConverter, class org.apache.camel.converter.stream.StreamCacheConverter, class org.apache.activemq.camel.converter.ActiveMQConverter, class org.apache.activemq.camel.converter.ActiveMQMessageConverter]
2011-02-10 12:23:07,474 [ing.Main.main()] INFO  nTypeConverterLoader - Found 4 packages with 15 @Converter classes to load
2011-02-10 12:23:07,474 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.component.file.GenericFileConverter
2011-02-10 12:23:07,479 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.component.bean.BeanConverter
2011-02-10 12:23:07,479 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.converter.CamelConverter
2011-02-10 12:23:07,482 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.converter.CollectionConverter
2011-02-10 12:23:07,483 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.converter.IOConverter
2011-02-10 12:23:07,488 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.converter.NIOConverter
2011-02-10 12:23:07,490 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.converter.ObjectConverter
2011-02-10 12:23:07,491 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.converter.TimePatternConverter
2011-02-10 12:23:07,491 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.converter.jaxp.DomConverter
2011-02-10 12:23:07,492 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.converter.jaxp.StaxConverter
2011-02-10 12:23:07,494 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.converter.jaxp.StreamSourceConverter
2011-02-10 12:23:07,494 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.converter.jaxp.XmlConverter
2011-02-10 12:23:07,499 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.camel.converter.stream.StreamCacheConverter
2011-02-10 12:23:07,499 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.activemq.camel.converter.ActiveMQConverter
2011-02-10 12:23:07,501 [ing.Main.main()] DEBUG nTypeConverterLoader - Loading converter class: org.apache.activemq.camel.converter.ActiveMQMessageConverter
2011-02-10 12:23:07,507 [ing.Main.main()] DEBUG DefaultTypeConverter - Loading type converters done
2011-02-10 12:23:07,508 [ing.Main.main()] INFO  DefaultTypeConverter - Loaded 150 type converters in 0.313 seconds
2011-02-10 12:23:07,516 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=services,name=DefaultExecutorServiceStrategy(0x4d12ee4f)
2011-02-10 12:23:07,525 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=services,name=SharedProducerServicePool(0x15dbac11)
2011-02-10 12:23:07,525 [ing.Main.main()] DEBUG dProducerServicePool - Starting service pool: org.apache.camel.impl.SharedProducerServicePool@15dbac11
2011-02-10 12:23:07,534 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=services,name=DefaultInflightRepository(0xf7b650a)
2011-02-10 12:23:07,542 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=services,name=DefaultShutdownStrategy(0x6e267b76)
2011-02-10 12:23:07,555 [ing.Main.main()] DEBUG ultComponentResolver - Found component: smpp via type: org.apache.camel.component.smpp.SmppComponent via: META-INF/services/org/apache/camel/component/smpp
2011-02-10 12:23:07,590 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=components,name=""smpp""
2011-02-10 12:23:07,591 [ing.Main.main()] DEBUG DefaultComponent     - Creating endpoint uri=[smpp://ubankmobile01@127.0.0.1:2775?amp%3BenquireLinkTimer=55000&amp%3BsystemType=ubankmobile01&amp%3BtransactionTimer=15000&password=u%24%40nk01], path=[ubankmobile01@127.0.0.1:2775], parameters=[{amp;enquireLinkTimer=55000, amp;systemType=ubankmobile01, amp;transactionTimer=15000, password=u$@nk01}]
2011-02-10 12:23:07,605 [ing.Main.main()] DEBUG SpringCamelContext   - smpp://ubankmobile01@127.0.0.1:2775?amp%3BenquireLinkTimer=55000&amp%3BsystemType=ubankmobile01&amp%3BtransactionTimer=15000&password=u%24%40nk01 converted to endpoint: Endpoint[smpp://ubankmobile01@127.0.0.1:2775?amp%3BenquireLinkTimer=55000&amp%3BsystemType=ubankmobile01&amp%3BtransactionTimer=15000&password=******] by component: org.apache.camel.component.smpp.SmppComponent@517c804b
2011-02-10 12:23:07,615 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=endpoints,name=""smpp://ubankmobile01@127.0.0.1:2775""
2011-02-10 12:23:07,655 [ing.Main.main()] DEBUG ultComponentResolver - Found component: log via type: org.apache.camel.component.log.LogComponent via: META-INF/services/org/apache/camel/component/log
2011-02-10 12:23:07,661 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=components,name=""log""
2011-02-10 12:23:07,661 [ing.Main.main()] DEBUG DefaultComponent     - Creating endpoint uri=[log://SmppIn], path=[SmppIn], parameters=[{}]
2011-02-10 12:23:07,666 [ing.Main.main()] DEBUG SpringCamelContext   - log://SmppIn converted to endpoint: Endpoint[log://SmppIn] by component: org.apache.camel.component.log.LogComponent@4633c1aa
2011-02-10 12:23:07,669 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=endpoints,name=""log://SmppIn""
2011-02-10 12:23:07,712 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=tracer,name=Tracer(0x3761f888)
2011-02-10 12:23:07,747 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=errorhandlers,name=""DefaultErrorHandlerBuilder(ref:CamelDefaultErrorHandlerBuilder)""
2011-02-10 12:23:07,754 [ing.Main.main()] DEBUG SpringCamelContext   - Warming up route id: SmppConsumerRoute having autoStartup=true
2011-02-10 12:23:07,772 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=services,name=ProducerCache(0x3a67ad79)
2011-02-10 12:23:07,774 [ing.Main.main()] DEBUG ProcessorEndpoint$1  - Starting producer: Producer[log://SmppIn]
2011-02-10 12:23:07,775 [ing.Main.main()] DEBUG ProducerCache        - Adding to producer cache with key: Endpoint[log://SmppIn] for producer: Producer[log://SmppIn]
2011-02-10 12:23:07,797 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=threadpools,name=DefaultErrorHandler(0xa62b39f)
2011-02-10 12:23:07,797 [ing.Main.main()] DEBUG cutorServiceStrategy - Created new scheduled thread pool for source: DefaultErrorHandler[TraceInterceptor[To[log:SmppIn]]] with name: ErrorHandlerRedeliveryTask. [poolSize=10]. -> java.util.concurrent.ScheduledThreadPoolExecutor@55fe910c
2011-02-10 12:23:07,844 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=processors,name=""to1""
2011-02-10 12:23:07,873 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=routes,name=""SmppConsumerRoute""
2011-02-10 12:23:07,873 [ing.Main.main()] DEBUG SpringCamelContext   - Route: SmppConsumerRoute >>> EventDrivenConsumerRoute[Endpoint[smpp://ubankmobile01@127.0.0.1:2775?amp%3BenquireLinkTimer=55000&amp%3BsystemType=ubankmobile01&amp%3BtransactionTimer=15000&password=******] -> Instrumentation:route[UnitOfWork(Channel[sendTo(Endpoint[log://SmppIn])])]]
2011-02-10 12:23:07,873 [ing.Main.main()] DEBUG SpringCamelContext   - Starting consumer (order: 1000) on route: SmppConsumerRoute
2011-02-10 12:23:07,884 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=consumers,name=SmppConsumer(0x59c27402)
2011-02-10 12:23:07,884 [ing.Main.main()] DEBUG SmppConsumer         - Connecting to: smpp://ubankmobile01@127.0.0.1:2775...
2011-02-10 12:23:07,884 [ing.Main.main()] DEBUG SmppConsumer         - Starting consumer: SmppConsumer[smpp://ubankmobile01@127.0.0.1:2775]
2011-02-10 12:23:07,922 [ing.Main.main()] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:23:07,923 [ing.Main.main()] INFO  SMPPSession          - Connected
2011-02-10 12:23:07,923 [ing.Main.main()] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:23:07,925 [Thread-6       ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:23:10,913 [pool-1-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:23:10,913 [pool-1-thread-1] DEBUG SMPPSessionOpen      - Bind Response header (30, 80000001, 00000000, 1)
2011-02-10 12:23:10,915 [ing.Main.main()] DEBUG AbstractSession      - bind response received
2011-02-10 12:23:10,915 [ing.Main.main()] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- OPEN --> BOUND_RX[org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:23:10,916 [ing.Main.main()] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: BOUND_RX
2011-02-10 12:23:10,916 [Thread-7       ] INFO  SMPPSession          - Starting EnquireLinkSender
2011-02-10 12:23:10,916 [ing.Main.main()] INFO  SmppConsumer         - Connected to: smpp://ubankmobile01@127.0.0.1:2775
2011-02-10 12:23:10,917 [ing.Main.main()] INFO  SpringCamelContext   - Route: SmppConsumerRoute started and consuming from: Endpoint[smpp://ubankmobile01@127.0.0.1:2775?amp%3BenquireLinkTimer=55000&amp%3BsystemType=ubankmobile01&amp%3BtransactionTimer=15000&password=******]
2011-02-10 12:23:10,918 [ing.Main.main()] DEBUG DefaultComponent     - Creating endpoint uri=[spring-event://default], path=[default], parameters=[{}]
2011-02-10 12:23:10,919 [ing.Main.main()] DEBUG SpringCamelContext   - spring-event://default converted to endpoint: Endpoint[spring-event://default] by component: org.apache.camel.component.event.EventComponent@774e1f2b
2011-02-10 12:23:10,925 [ing.Main.main()] DEBUG faultManagementAgent - Registered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=endpoints,name=""spring-event://default""
2011-02-10 12:23:10,926 [ing.Main.main()] INFO  SpringCamelContext   - Total 1 routes, of which 1 is started.
2011-02-10 12:23:10,926 [ing.Main.main()] INFO  SpringCamelContext   - Apache Camel 2.6.0 (CamelContext: camelTestRouteBuilderContext) started in 3.866 seconds
2011-02-10 12:23:10,935 [ing.Main.main()] DEBUG MainSupport          - Starting Spring ApplicationContext: org.springframework.context.support.ClassPathXmlApplicationContext@4c9549af
2011-02-10 12:23:10,936 [ing.Main.main()] DEBUG SpringCamelContext   - onApplicationEvent: org.springframework.context.event.ContextStartedEvent[source=org.springframework.context.support.ClassPathXmlApplicationContext@4c9549af: startup date [Thu Feb 10 12:23:05 SAST 2011]; root of context hierarchy]
2011-02-10 12:23:11,408 [pool-1-thread-2] DEBUG stractSessionContext - Activity notified
2011-02-10 12:23:11,408 [pool-1-thread-2] DEBUG SMPPSession          - Sending enquire_link_resp
2011-02-10 12:23:16,413 [Thread-6       ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:23:16,413 [Thread-6       ] DEBUG SMPPSession          - Sending enquire link notify
2011-02-10 12:23:18,851 [pool-1-thread-3] DEBUG stractSessionContext - Activity notified
2011-02-10 12:23:18,852 [Thread-7       ] DEBUG AbstractSession      - enquire_link response received
2011-02-10 12:23:23,853 [Thread-6       ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:23:23,853 [Thread-6       ] DEBUG SMPPSession          - Sending enquire link notify
2011-02-10 12:23:24,854 [pool-1-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:23:24,854 [Thread-7       ] DEBUG AbstractSession      - enquire_link response received
2011-02-10 12:23:29,859 [Thread-6       ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:23:29,859 [Thread-6       ] DEBUG SMPPSession          - Sending enquire link notify
2011-02-10 12:23:30,938 [pool-1-thread-2] DEBUG stractSessionContext - Activity notified
2011-02-10 12:23:30,938 [Thread-7       ] DEBUG AbstractSession      - enquire_link response received
2011-02-10 12:23:35,943 [Thread-6       ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:23:35,943 [Thread-6       ] DEBUG SMPPSession          - Sending enquire link notify
2011-02-10 12:23:37,901 [pool-1-thread-3] DEBUG stractSessionContext - Activity notified
2011-02-10 12:23:37,901 [Thread-7       ] DEBUG AbstractSession      - enquire_link response received
2011-02-10 12:23:42,906 [Thread-6       ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:23:42,906 [Thread-6       ] DEBUG SMPPSession          - Sending enquire link notify
2011-02-10 12:23:45,256 [pool-1-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:23:45,256 [Thread-7       ] DEBUG AbstractSession      - enquire_link response received
2011-02-10 12:23:50,261 [Thread-6       ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:23:50,261 [Thread-6       ] DEBUG SMPPSession          - Sending enquire link notify
2011-02-10 12:23:51,232 [pool-1-thread-2] DEBUG stractSessionContext - Activity notified
2011-02-10 12:23:51,232 [Thread-7       ] DEBUG AbstractSession      - enquire_link response received
2011-02-10 12:23:56,237 [Thread-6       ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:23:56,237 [Thread-6       ] DEBUG SMPPSession          - Sending enquire link notify
2011-02-10 12:23:57,208 [pool-1-thread-3] DEBUG stractSessionContext - Activity notified
2011-02-10 12:23:57,208 [Thread-7       ] DEBUG AbstractSession      - enquire_link response received
2011-02-10 12:24:02,211 [Thread-6       ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:24:02,211 [Thread-6       ] DEBUG SMPPSession          - Sending enquire link notify
2011-02-10 12:24:03,277 [pool-1-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:24:03,277 [Thread-7       ] DEBUG AbstractSession      - enquire_link response received
2011-02-10 12:24:08,281 [Thread-6       ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:24:08,281 [Thread-6       ] DEBUG SMPPSession          - Sending enquire link notify
2011-02-10 12:24:09,436 [pool-1-thread-2] DEBUG stractSessionContext - Activity notified
2011-02-10 12:24:09,437 [Thread-7       ] DEBUG AbstractSession      - enquire_link response received
2011-02-10 12:24:09,821 [pool-1-thread-3] DEBUG stractSessionContext - Activity notified
2011-02-10 12:24:09,821 [pool-1-thread-3] INFO  ericSMPPSessionBound - Receving unbind request
2011-02-10 12:24:09,822 [pool-1-thread-3] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- BOUND_RX --> UNBOUND[org.jsmpp.session.SMPPSession@501b2cb9]
{code}

Everything is perfectly fine up to this point. It is here that the SMSC / simulator issues an unbind request towards the camel-smpp consumer.

{code}
2011-02-10 12:24:09,937 [Thread-7       ] INFO  SMPPSession          - EnquireLinkSender stop
2011-02-10 12:24:14,826 [Thread-6       ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:24:14,827 [Thread-6       ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- UNBOUND --> CLOSED[org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:14,827 [Thread-6       ] WARN  SmppConsumer         - Loost connection to: smpp://ubankmobile01@127.0.0.1:2775 - trying to reconnect...
2011-02-10 12:24:14,827 [Thread-6       ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:14,829 [Thread-8       ] INFO  SmppConsumer         - Schedule reconnect after 5000 millis
2011-02-10 12:24:19,829 [Thread-8       ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #1...
2011-02-10 12:24:19,830 [Thread-8       ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:19,830 [Thread-8       ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:19,830 [Thread-8       ] INFO  SMPPSession          - Connected
2011-02-10 12:24:19,830 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@1b6b7f83]
2011-02-10 12:24:19,831 [Thread-9       ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:22,834 [pool-2-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:24:22,834 [pool-2-thread-1] DEBUG SMPPSessionOpen      - Bind Response header (30, 80000001, 0000000d, 1)
2011-02-10 12:24:22,835 [Thread-8       ] DEBUG AbstractSession      - bind response received
2011-02-10 12:24:22,837 [Thread-8       ] ERROR SMPPSession          - Receive negative bind response
org.jsmpp.extra.NegativeResponseException: Negative response 0000000d found
	at org.jsmpp.session.AbstractSession.validateResponse(AbstractSession.java:215)
	at org.jsmpp.session.AbstractSession.executeSendCommand(AbstractSession.java:271)
	at org.jsmpp.session.SMPPSession.sendBind(SMPPSession.java:294)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:232)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:200)
	at org.apache.camel.component.smpp.SmppConsumer.createSession(SmppConsumer.java:161)
	at org.apache.camel.component.smpp.SmppConsumer.access$500(SmppConsumer.java:54)
	at org.apache.camel.component.smpp.SmppConsumer$3.run(SmppConsumer.java:223)
2011-02-10 12:24:22,838 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- OPEN --> CLOSED[org.jsmpp.session.SMPPSession@1b6b7f83]
{code}

The above negative bind responseis due to the fact that only a single smpp RX bind is allowed to the SMSC / simulator. I suspect the underlying problem is how the exception is handled here, but I might be wrong.

{code}
2011-02-10 12:24:22,838 [Thread-8       ] WARN  SmppConsumer         - Loost connection to: smpp://ubankmobile01@127.0.0.1:2775 - trying to reconnect...
2011-02-10 12:24:22,838 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:22,838 [Thread-10      ] INFO  SmppConsumer         - Schedule reconnect after 5000 millis
2011-02-10 12:24:22,838 [Thread-8       ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:22,838 [Thread-8       ] INFO  SmppConsumer         - Failed to reconnect to smpp://ubankmobile01@127.0.0.1:2775, sessionState == CLOSED
2011-02-10 12:24:22,838 [Thread-9       ] INFO  SMPPSession          - PDUReaderWorker stop
2011-02-10 12:24:22,838 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:27,838 [Thread-10      ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #1...
2011-02-10 12:24:27,839 [Thread-10      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:27,839 [Thread-10      ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:27,839 [Thread-8       ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #2...
2011-02-10 12:24:27,839 [Thread-8       ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:27,839 [Thread-10      ] INFO  SMPPSession          - Connected
2011-02-10 12:24:27,839 [Thread-8       ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:27,839 [Thread-10      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@62bc184]
2011-02-10 12:24:27,840 [Thread-11      ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:27,840 [Thread-8       ] INFO  SMPPSession          - Connected
2011-02-10 12:24:27,840 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@3e30e173]
{code}

OK, wait, we have 2 SMPPSession object references now? Seems like we have 2 Threads running that tries to re-connect...

{code}
2011-02-10 12:24:27,841 [Thread-12      ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:31,035 [pool-3-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:24:31,035 [pool-3-thread-1] DEBUG SMPPSessionOpen      - Bind Response header (30, 80000001, 0000000d, 1)
2011-02-10 12:24:31,035 [Thread-10      ] DEBUG AbstractSession      - bind response received
2011-02-10 12:24:31,035 [Thread-10      ] ERROR SMPPSession          - Receive negative bind response
org.jsmpp.extra.NegativeResponseException: Negative response 0000000d found
	at org.jsmpp.session.AbstractSession.validateResponse(AbstractSession.java:215)
	at org.jsmpp.session.AbstractSession.executeSendCommand(AbstractSession.java:271)
	at org.jsmpp.session.SMPPSession.sendBind(SMPPSession.java:294)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:232)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:200)
	at org.apache.camel.component.smpp.SmppConsumer.createSession(SmppConsumer.java:161)
	at org.apache.camel.component.smpp.SmppConsumer.access$500(SmppConsumer.java:54)
	at org.apache.camel.component.smpp.SmppConsumer$3.run(SmppConsumer.java:223)
2011-02-10 12:24:31,036 [Thread-10      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- OPEN --> CLOSED[org.jsmpp.session.SMPPSession@62bc184]
2011-02-10 12:24:31,036 [Thread-10      ] WARN  SmppConsumer         - Loost connection to: smpp://ubankmobile01@127.0.0.1:2775 - trying to reconnect...
2011-02-10 12:24:31,036 [Thread-10      ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:31,036 [Thread-10      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:31,036 [Thread-10      ] INFO  SmppConsumer         - Failed to reconnect to smpp://ubankmobile01@127.0.0.1:2775, sessionState == CLOSED
2011-02-10 12:24:31,036 [Thread-11      ] INFO  SMPPSession          - PDUReaderWorker stop
2011-02-10 12:24:31,036 [Thread-10      ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:31,037 [Thread-13      ] INFO  SmppConsumer         - Schedule reconnect after 5000 millis
2011-02-10 12:24:32,846 [Thread-12      ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:24:32,982 [pool-4-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:24:32,982 [pool-4-thread-1] DEBUG SMPPSessionOpen      - Bind Response header (30, 80000001, 0000000d, 1)
2011-02-10 12:24:32,982 [Thread-8       ] DEBUG AbstractSession      - bind response received
2011-02-10 12:24:32,982 [Thread-8       ] ERROR SMPPSession          - Receive negative bind response
org.jsmpp.extra.NegativeResponseException: Negative response 0000000d found
	at org.jsmpp.session.AbstractSession.validateResponse(AbstractSession.java:215)
	at org.jsmpp.session.AbstractSession.executeSendCommand(AbstractSession.java:271)
	at org.jsmpp.session.SMPPSession.sendBind(SMPPSession.java:294)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:232)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:200)
	at org.apache.camel.component.smpp.SmppConsumer.createSession(SmppConsumer.java:161)
	at org.apache.camel.component.smpp.SmppConsumer.access$500(SmppConsumer.java:54)
	at org.apache.camel.component.smpp.SmppConsumer$3.run(SmppConsumer.java:223)
2011-02-10 12:24:32,982 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- OPEN --> CLOSED[org.jsmpp.session.SMPPSession@3e30e173]
2011-02-10 12:24:32,982 [Thread-8       ] WARN  SmppConsumer         - Loost connection to: smpp://ubankmobile01@127.0.0.1:2775 - trying to reconnect...
2011-02-10 12:24:32,982 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:32,983 [Thread-8       ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:32,983 [Thread-12      ] INFO  SMPPSession          - PDUReaderWorker stop
2011-02-10 12:24:32,983 [Thread-14      ] INFO  SmppConsumer         - Schedule reconnect after 5000 millis
2011-02-10 12:24:32,983 [Thread-8       ] INFO  SmppConsumer         - Failed to reconnect to smpp://ubankmobile01@127.0.0.1:2775, sessionState == CLOSED
2011-02-10 12:24:32,983 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:36,037 [Thread-10      ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #2...
2011-02-10 12:24:36,037 [Thread-13      ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #1...
{code}

Some more re-connect threads...

{code}
2011-02-10 12:24:36,038 [Thread-10      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:36,038 [Thread-13      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:36,038 [Thread-10      ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:36,038 [Thread-13      ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:36,038 [Thread-10      ] INFO  SMPPSession          - Connected
2011-02-10 12:24:36,038 [Thread-13      ] INFO  SMPPSession          - Connected
2011-02-10 12:24:36,038 [Thread-10      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@782bbb7b]
2011-02-10 12:24:36,039 [Thread-13      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@7f21c5df]
2011-02-10 12:24:36,039 [Thread-15      ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:36,039 [Thread-16      ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:37,983 [Thread-14      ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #1...
2011-02-10 12:24:37,984 [Thread-14      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:37,984 [Thread-8       ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #3...
2011-02-10 12:24:37,984 [Thread-8       ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:37,984 [Thread-14      ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:37,984 [Thread-8       ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:37,984 [Thread-14      ] INFO  SMPPSession          - Connected
2011-02-10 12:24:37,984 [Thread-8       ] INFO  SMPPSession          - Connected
2011-02-10 12:24:37,985 [Thread-14      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@6a754384]
2011-02-10 12:24:37,985 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@38a3c5b6]
2011-02-10 12:24:37,985 [Thread-17      ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:37,987 [Thread-18      ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:38,831 [pool-6-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:24:38,831 [pool-6-thread-1] DEBUG SMPPSessionOpen      - Bind Response header (30, 80000001, 0000000d, 1)
2011-02-10 12:24:38,831 [Thread-13      ] DEBUG AbstractSession      - bind response received
2011-02-10 12:24:38,831 [Thread-13      ] ERROR SMPPSession          - Receive negative bind response
org.jsmpp.extra.NegativeResponseException: Negative response 0000000d found
	at org.jsmpp.session.AbstractSession.validateResponse(AbstractSession.java:215)
	at org.jsmpp.session.AbstractSession.executeSendCommand(AbstractSession.java:271)
	at org.jsmpp.session.SMPPSession.sendBind(SMPPSession.java:294)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:232)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:200)
	at org.apache.camel.component.smpp.SmppConsumer.createSession(SmppConsumer.java:161)
	at org.apache.camel.component.smpp.SmppConsumer.access$500(SmppConsumer.java:54)
	at org.apache.camel.component.smpp.SmppConsumer$3.run(SmppConsumer.java:223)
2011-02-10 12:24:38,832 [Thread-13      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- OPEN --> CLOSED[org.jsmpp.session.SMPPSession@7f21c5df]
2011-02-10 12:24:38,832 [Thread-13      ] WARN  SmppConsumer         - Loost connection to: smpp://ubankmobile01@127.0.0.1:2775 - trying to reconnect...
2011-02-10 12:24:38,832 [Thread-13      ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:38,832 [Thread-13      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:38,832 [Thread-19      ] INFO  SmppConsumer         - Schedule reconnect after 5000 millis
2011-02-10 12:24:38,832 [Thread-13      ] INFO  SmppConsumer         - Failed to reconnect to smpp://ubankmobile01@127.0.0.1:2775, sessionState == CLOSED
2011-02-10 12:24:38,832 [Thread-16      ] INFO  SMPPSession          - PDUReaderWorker stop
2011-02-10 12:24:38,833 [Thread-13      ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:40,957 [pool-5-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:24:40,957 [pool-5-thread-1] DEBUG SMPPSessionOpen      - Bind Response header (30, 80000001, 0000000d, 1)
2011-02-10 12:24:40,957 [Thread-10      ] DEBUG AbstractSession      - bind response received
2011-02-10 12:24:40,957 [Thread-10      ] ERROR SMPPSession          - Receive negative bind response
org.jsmpp.extra.NegativeResponseException: Negative response 0000000d found
	at org.jsmpp.session.AbstractSession.validateResponse(AbstractSession.java:215)
	at org.jsmpp.session.AbstractSession.executeSendCommand(AbstractSession.java:271)
	at org.jsmpp.session.SMPPSession.sendBind(SMPPSession.java:294)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:232)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:200)
	at org.apache.camel.component.smpp.SmppConsumer.createSession(SmppConsumer.java:161)
	at org.apache.camel.component.smpp.SmppConsumer.access$500(SmppConsumer.java:54)
	at org.apache.camel.component.smpp.SmppConsumer$3.run(SmppConsumer.java:223)
2011-02-10 12:24:40,958 [Thread-10      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- OPEN --> CLOSED[org.jsmpp.session.SMPPSession@782bbb7b]
2011-02-10 12:24:40,958 [Thread-10      ] WARN  SmppConsumer         - Loost connection to: smpp://ubankmobile01@127.0.0.1:2775 - trying to reconnect...
2011-02-10 12:24:40,958 [Thread-10      ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:40,958 [Thread-20      ] INFO  SmppConsumer         - Schedule reconnect after 5000 millis
2011-02-10 12:24:40,958 [Thread-15      ] INFO  SMPPSession          - PDUReaderWorker stop
2011-02-10 12:24:40,958 [Thread-10      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:40,958 [Thread-10      ] INFO  SmppConsumer         - Failed to reconnect to smpp://ubankmobile01@127.0.0.1:2775, sessionState == CLOSED
2011-02-10 12:24:40,959 [Thread-10      ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:42,821 [pool-8-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:24:42,821 [pool-8-thread-1] DEBUG SMPPSessionOpen      - Bind Response header (30, 80000001, 0000000d, 1)
2011-02-10 12:24:42,821 [Thread-8       ] DEBUG AbstractSession      - bind response received
2011-02-10 12:24:42,822 [Thread-8       ] ERROR SMPPSession          - Receive negative bind response
org.jsmpp.extra.NegativeResponseException: Negative response 0000000d found
	at org.jsmpp.session.AbstractSession.validateResponse(AbstractSession.java:215)
	at org.jsmpp.session.AbstractSession.executeSendCommand(AbstractSession.java:271)
	at org.jsmpp.session.SMPPSession.sendBind(SMPPSession.java:294)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:232)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:200)
	at org.apache.camel.component.smpp.SmppConsumer.createSession(SmppConsumer.java:161)
	at org.apache.camel.component.smpp.SmppConsumer.access$500(SmppConsumer.java:54)
	at org.apache.camel.component.smpp.SmppConsumer$3.run(SmppConsumer.java:223)
2011-02-10 12:24:42,822 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- OPEN --> CLOSED[org.jsmpp.session.SMPPSession@38a3c5b6]
2011-02-10 12:24:42,822 [Thread-8       ] WARN  SmppConsumer         - Loost connection to: smpp://ubankmobile01@127.0.0.1:2775 - trying to reconnect...
2011-02-10 12:24:42,822 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:42,822 [Thread-21      ] INFO  SmppConsumer         - Schedule reconnect after 5000 millis
2011-02-10 12:24:42,822 [Thread-8       ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:42,822 [Thread-8       ] INFO  SmppConsumer         - Failed to reconnect to smpp://ubankmobile01@127.0.0.1:2775, sessionState == CLOSED
2011-02-10 12:24:42,822 [Thread-18      ] INFO  SMPPSession          - PDUReaderWorker stop
2011-02-10 12:24:42,823 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:42,988 [Thread-17      ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:24:43,832 [Thread-19      ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #1...
2011-02-10 12:24:43,833 [Thread-19      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:43,833 [Thread-19      ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:43,833 [Thread-13      ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #2...
2011-02-10 12:24:43,833 [Thread-13      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:43,833 [Thread-19      ] INFO  SMPPSession          - Connected
2011-02-10 12:24:43,833 [Thread-13      ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:43,833 [Thread-19      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@18dd7404]
2011-02-10 12:24:43,834 [Thread-13      ] INFO  SMPPSession          - Connected
2011-02-10 12:24:43,834 [Thread-22      ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:43,834 [Thread-13      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@53606bf5]
2011-02-10 12:24:43,835 [Thread-23      ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:44,972 [pool-7-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:24:44,972 [pool-7-thread-1] DEBUG SMPPSessionOpen      - Bind Response header (30, 80000001, 0000000d, 1)
2011-02-10 12:24:44,972 [Thread-14      ] DEBUG AbstractSession      - bind response received
2011-02-10 12:24:44,972 [Thread-14      ] ERROR SMPPSession          - Receive negative bind response
org.jsmpp.extra.NegativeResponseException: Negative response 0000000d found
	at org.jsmpp.session.AbstractSession.validateResponse(AbstractSession.java:215)
	at org.jsmpp.session.AbstractSession.executeSendCommand(AbstractSession.java:271)
	at org.jsmpp.session.SMPPSession.sendBind(SMPPSession.java:294)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:232)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:200)
	at org.apache.camel.component.smpp.SmppConsumer.createSession(SmppConsumer.java:161)
	at org.apache.camel.component.smpp.SmppConsumer.access$500(SmppConsumer.java:54)
	at org.apache.camel.component.smpp.SmppConsumer$3.run(SmppConsumer.java:223)
2011-02-10 12:24:44,972 [Thread-14      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- OPEN --> CLOSED[org.jsmpp.session.SMPPSession@6a754384]
2011-02-10 12:24:44,973 [Thread-14      ] WARN  SmppConsumer         - Loost connection to: smpp://ubankmobile01@127.0.0.1:2775 - trying to reconnect...
2011-02-10 12:24:44,973 [Thread-14      ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:44,973 [Thread-14      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:44,973 [Thread-24      ] INFO  SmppConsumer         - Schedule reconnect after 5000 millis
2011-02-10 12:24:44,973 [Thread-17      ] INFO  SMPPSession          - PDUReaderWorker stop
2011-02-10 12:24:44,973 [Thread-14      ] INFO  SmppConsumer         - Failed to reconnect to smpp://ubankmobile01@127.0.0.1:2775, sessionState == CLOSED
2011-02-10 12:24:44,974 [Thread-14      ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:45,958 [Thread-20      ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #1...
2011-02-10 12:24:45,958 [Thread-20      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:45,959 [Thread-20      ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:45,959 [Thread-10      ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #3...
2011-02-10 12:24:45,959 [Thread-10      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:45,959 [Thread-20      ] INFO  SMPPSession          - Connected
2011-02-10 12:24:45,959 [Thread-10      ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:45,959 [Thread-20      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@684be8b8]
2011-02-10 12:24:45,960 [Thread-10      ] INFO  SMPPSession          - Connected
2011-02-10 12:24:45,960 [Thread-25      ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:45,960 [Thread-10      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@79b7d13e]
2011-02-10 12:24:45,960 [Thread-26      ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:47,018 [pool-9-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:24:47,018 [pool-9-thread-1] DEBUG SMPPSessionOpen      - Bind Response header (30, 80000001, 0000000d, 1)
2011-02-10 12:24:47,018 [Thread-19      ] DEBUG AbstractSession      - bind response received
2011-02-10 12:24:47,018 [Thread-19      ] ERROR SMPPSession          - Receive negative bind response
org.jsmpp.extra.NegativeResponseException: Negative response 0000000d found
	at org.jsmpp.session.AbstractSession.validateResponse(AbstractSession.java:215)
	at org.jsmpp.session.AbstractSession.executeSendCommand(AbstractSession.java:271)
	at org.jsmpp.session.SMPPSession.sendBind(SMPPSession.java:294)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:232)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:200)
	at org.apache.camel.component.smpp.SmppConsumer.createSession(SmppConsumer.java:161)
	at org.apache.camel.component.smpp.SmppConsumer.access$500(SmppConsumer.java:54)
	at org.apache.camel.component.smpp.SmppConsumer$3.run(SmppConsumer.java:223)
2011-02-10 12:24:47,018 [Thread-19      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- OPEN --> CLOSED[org.jsmpp.session.SMPPSession@18dd7404]
2011-02-10 12:24:47,019 [Thread-19      ] WARN  SmppConsumer         - Loost connection to: smpp://ubankmobile01@127.0.0.1:2775 - trying to reconnect...
2011-02-10 12:24:47,019 [Thread-19      ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:47,019 [Thread-19      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:47,019 [Thread-19      ] INFO  SmppConsumer         - Failed to reconnect to smpp://ubankmobile01@127.0.0.1:2775, sessionState == CLOSED
2011-02-10 12:24:47,019 [Thread-22      ] INFO  SMPPSession          - PDUReaderWorker stop
2011-02-10 12:24:47,019 [Thread-19      ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:47,020 [Thread-27      ] INFO  SmppConsumer         - Schedule reconnect after 5000 millis
{code}

Even more re-connect threads being spawned...

{code}
2011-02-10 12:24:47,823 [Thread-21      ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #1...
2011-02-10 12:24:47,823 [Thread-21      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:47,823 [Thread-21      ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:47,823 [Thread-8       ] INFO  SmppConsumer         - Trying to reconnect to smpp://ubankmobile01@127.0.0.1:2775 - attempt #4...
2011-02-10 12:24:47,823 [Thread-8       ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:47,823 [Thread-21      ] INFO  SMPPSession          - Connected
2011-02-10 12:24:47,823 [Thread-8       ] DEBUG SMPPSession          - Connect and bind to 127.0.0.1 port 2775
2011-02-10 12:24:47,823 [Thread-21      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@490eb6ae]
2011-02-10 12:24:47,824 [Thread-8       ] INFO  SMPPSession          - Connected
2011-02-10 12:24:47,824 [Thread-28      ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:47,824 [Thread-8       ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- CLOSED --> OPEN[org.jsmpp.session.SMPPSession@3b4d82e1]
2011-02-10 12:24:47,825 [Thread-29      ] INFO  SMPPSession          - Starting PDUReaderWorker with processor degree:3 ...
2011-02-10 12:24:48,838 [Thread-23      ] DEBUG SMPPSession          - No activity notified
2011-02-10 12:24:48,962 [ool-10-thread-1] DEBUG stractSessionContext - Activity notified
2011-02-10 12:24:48,962 [ool-10-thread-1] DEBUG SMPPSessionOpen      - Bind Response header (30, 80000001, 0000000d, 1)
2011-02-10 12:24:48,962 [Thread-13      ] DEBUG AbstractSession      - bind response received
2011-02-10 12:24:48,962 [Thread-13      ] ERROR SMPPSession          - Receive negative bind response
org.jsmpp.extra.NegativeResponseException: Negative response 0000000d found
	at org.jsmpp.session.AbstractSession.validateResponse(AbstractSession.java:215)
	at org.jsmpp.session.AbstractSession.executeSendCommand(AbstractSession.java:271)
	at org.jsmpp.session.SMPPSession.sendBind(SMPPSession.java:294)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:232)
	at org.jsmpp.session.SMPPSession.connectAndBind(SMPPSession.java:200)
	at org.apache.camel.component.smpp.SmppConsumer.createSession(SmppConsumer.java:161)
	at org.apache.camel.component.smpp.SmppConsumer.access$500(SmppConsumer.java:54)
	at org.apache.camel.component.smpp.SmppConsumer$3.run(SmppConsumer.java:223)
2011-02-10 12:24:48,963 [Thread-13      ] DEBUG SmppConsumer         - >>>>>>>>>>>>>>>>>>> STATE CHANGE -- OPEN --> CLOSED[org.jsmpp.session.SMPPSession@53606bf5]
2011-02-10 12:24:48,963 [Thread-13      ] WARN  SmppConsumer         - Loost connection to: smpp://ubankmobile01@127.0.0.1:2775 - trying to reconnect...
2011-02-10 12:24:48,963 [Thread-13      ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:48,963 [Thread-13      ] DEBUG SmppConsumer         - Session info: [org.jsmpp.session.SMPPSession@501b2cb9], session state: CLOSED
2011-02-10 12:24:48,963 [Thread-13      ] INFO  SmppConsumer         - Failed to reconnect to smpp://ubankmobile01@127.0.0.1:2775, sessionState == CLOSED
2011-02-10 12:24:48,963 [Thread-23      ] INFO  SMPPSession          - PDUReaderWorker stop
2011-02-10 12:24:48,963 [Thread-13      ] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:48,964 [Thread-30      ] INFO  SmppConsumer         - Schedule reconnect after 5000 millis
^C2011-02-10 12:24:49,726 [Thread-5       ] INFO  rt$HangupInterceptor - Received hang up - stopping the main instance.
2011-02-10 12:24:49,726 [Thread-5       ] INFO  MainSupport          - Apache Camel 2.6.0 stopping
2011-02-10 12:24:49,727 [Thread-5       ] DEBUG MainSupport          - Stopping Spring ApplicationContext: org.springframework.context.support.ClassPathXmlApplicationContext@4c9549af
2011-02-10 12:24:49,733 [Thread-5       ] DEBUG SpringCamelContext   - onApplicationEvent: org.springframework.context.event.ContextClosedEvent[source=org.springframework.context.support.ClassPathXmlApplicationContext@4c9549af: startup date [Thu Feb 10 12:23:05 SAST 2011]; root of context hierarchy]
2011-02-10 12:24:49,735 [Thread-5       ] DEBUG ledConnectionFactory - Stop the PooledConnectionFactory, number of connections in cache: 0
2011-02-10 12:24:49,736 [Thread-5       ] INFO  SpringCamelContext   - Apache Camel 2.6.0 (CamelContext:camelTestRouteBuilderContext) is shutting down
2011-02-10 12:24:49,737 [Thread-5       ] INFO  aultShutdownStrategy - Starting to graceful shutdown 1 routes (timeout 300 seconds)
2011-02-10 12:24:49,738 [Thread-5       ] DEBUG cutorServiceStrategy - Created new single thread pool for source: org.apache.camel.impl.DefaultShutdownStrategy@6e267b76 with name: ShutdownTask. -> java.util.concurrent.Executors$FinalizableDelegatedExecutorService@6ea53502
2011-02-10 12:24:49,742 [ - ShutdownTask] DEBUG aultShutdownStrategy - There are 1 routes to shutdown
2011-02-10 12:24:49,742 [ - ShutdownTask] DEBUG SmppConsumer         - Disconnecting from: smpp://ubankmobile01@127.0.0.1:2775...
2011-02-10 12:24:49,742 [ - ShutdownTask] DEBUG SmppConsumer         - Closing session: org.jsmpp.session.SMPPSession@501b2cb9
2011-02-10 12:24:49,742 [ - ShutdownTask] DEBUG SmppConsumer         - >>>>>>>>>> Closing SMPP session [org.jsmpp.session.SMPPSession@501b2cb9]
2011-02-10 12:24:49,742 [ - ShutdownTask] DEBUG SmppConsumer         - calling super.doStop()
2011-02-10 12:24:49,742 [ - ShutdownTask] DEBUG SmppConsumer         - Stopping consumer: SmppConsumer[smpp://ubankmobile01@127.0.0.1:2775]
2011-02-10 12:24:49,742 [ - ShutdownTask] DEBUG ProcessorEndpoint$1  - Stopping producer: Producer[log://SmppIn]
2011-02-10 12:24:49,743 [ - ShutdownTask] INFO  SmppConsumer         - Disconnected from: smpp://ubankmobile01@127.0.0.1:2775
2011-02-10 12:24:49,743 [ - ShutdownTask] DEBUG aultShutdownStrategy - Shutdown complete for: SmppConsumer[smpp://ubankmobile01@127.0.0.1:2775]
2011-02-10 12:24:49,743 [ - ShutdownTask] INFO  aultShutdownStrategy - Route: SmppConsumerRoute shutdown complete, was consuming from: Endpoint[smpp://ubankmobile01@127.0.0.1:2775?amp%3BenquireLinkTimer=55000&amp%3BsystemType=ubankmobile01&amp%3BtransactionTimer=15000&password=******]
2011-02-10 12:24:49,744 [Thread-5       ] INFO  aultShutdownStrategy - Graceful shutdown of 1 routes completed in 0 seconds
2011-02-10 12:24:49,745 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=routes,name=""SmppConsumerRoute""
2011-02-10 12:24:49,745 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=consumers,name=SmppConsumer(0x59c27402)
2011-02-10 12:24:49,745 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=processors,name=""to1""
2011-02-10 12:24:49,747 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=context,name=""camelTestRouteBuilderContext""
2011-02-10 12:24:49,747 [Thread-5       ] DEBUG cutorServiceStrategy - ShutdownNow ExecutorService: java.util.concurrent.Executors$FinalizableDelegatedExecutorService@6ea53502
2011-02-10 12:24:49,747 [Thread-5       ] INFO  ltInflightRepository - Shutting down with no inflight exchanges.
2011-02-10 12:24:49,747 [Thread-5       ] DEBUG dProducerServicePool - Stopping service pool: org.apache.camel.impl.SharedProducerServicePool@15dbac11
2011-02-10 12:24:49,748 [Thread-5       ] DEBUG cutorServiceStrategy - ShutdownNow ExecutorService: java.util.concurrent.ScheduledThreadPoolExecutor@55fe910c
2011-02-10 12:24:49,748 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=threadpools,name=DefaultErrorHandler(0xa62b39f)
2011-02-10 12:24:49,749 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=components,name=""smpp""
2011-02-10 12:24:49,749 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=endpoints,name=""spring-event://default""
2011-02-10 12:24:49,749 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=services,name=DefaultExecutorServiceStrategy(0x4d12ee4f)
2011-02-10 12:24:49,749 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=endpoints,name=""log://SmppIn""
2011-02-10 12:24:49,749 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=services,name=ProducerCache(0x3a67ad79)
2011-02-10 12:24:49,749 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=services,name=DefaultTypeConverter(0x61e481c1)
2011-02-10 12:24:49,749 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=services,name=SharedProducerServicePool(0x15dbac11)
2011-02-10 12:24:49,749 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=tracer,name=Tracer(0x3761f888)
2011-02-10 12:24:49,749 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=services,name=DefaultShutdownStrategy(0x6e267b76)
2011-02-10 12:24:49,749 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=components,name=""log""
2011-02-10 12:24:49,750 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=services,name=DefaultInflightRepository(0xf7b650a)
2011-02-10 12:24:49,750 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=endpoints,name=""smpp://ubankmobile01@127.0.0.1:2775""
2011-02-10 12:24:49,750 [Thread-5       ] DEBUG faultManagementAgent - Unregistered MBean with objectname: org.apache.camel:context=jacovt-notebook/camelTestRouteBuilderContext,type=errorhandlers,name=""DefaultErrorHandlerBuilder(ref:CamelDefaultErrorHandlerBuilder)""
2011-02-10 12:24:49,750 [Thread-5       ] INFO  SpringCamelContext   - Uptime: 1 minute
2011-02-10 12:24:49,750 [Thread-5       ] INFO  SpringCamelContext   - Apache Camel 2.6.0 (CamelContext: camelTestRouteBuilderContext) is shutdown in 0.014 seconds
{code}


The above thread-spawning quickly gets out of hand with thousands being created.

Note however that this problem does not seem to happen when the smsc goes down (session is closed and a connection refused pops up), but only when an unbind happens and then a negative bind.",muellerc,jacovt,Critical,Closed,Fixed,10/Feb/11 10:49,25/Oct/11 11:35
Bug,CAMEL-3664,12498521,Sampling EIP - NPE in toString,The toString in SamplingDefinition should cater for units may be null.,davsclaus,davsclaus,Trivial,Closed,Fixed,14/Feb/11 12:07,25/Oct/11 11:35
Bug,CAMEL-3666,12498589,Make Content-Type HTTP Header Optional,"As per section 7.2.1 of RFC 2616, any HTTP/1.1 message containing an entity-body SHOULD (not MUST) include a Content- Type header field defining the media type of that body.

The populateExchangeFromRestletRequest method of the DefaultRestletBinding class, unlike others in the same class, needs to be changed:

{code:Java}if (request.getEntity().getMediaType().equals(MediaType.APPLICATION_WWW_FORM)) {
  ...
} else {
  inMessage.setBody(request.getEntity().getStream());
}{code}",davsclaus,fribeiro,Major,Closed,Fixed,14/Feb/11 23:31,25/Oct/11 11:36
Bug,CAMEL-3670,12498726,bean component - Thread safey issue when selecting method to use in BeanInfo,"See nabble
http://camel.465427.n5.nabble.com/Bean-component-BeanInfo-thread-safety-tp3387197p3387197.html",davsclaus,davsclaus,Major,Closed,Fixed,16/Feb/11 07:33,25/Oct/11 11:35
Bug,CAMEL-3675,12498812,<jmxAgent>'s disabled attribute does not support property placeholders,"The Camel Properties web page (http://camel.apache.org/properties.html) states that property placeholder are supported on the <jmxAgent> tag.  However, the ""disabled"" attribute on this tag doesn't support property placeholders.

The AbstractCamelContextFactoryBean.initJMXAgent() method needs to call CamelContextHelper.parseBoolean() passing in the camelJMXAgent.isAgentDisabled() value to get property placeholders to work with this attribute.",davsclaus,marbor,Minor,Closed,Fixed,16/Feb/11 23:25,25/Oct/11 11:36
Bug,CAMEL-3677,12498874,"When splitting inside another split, the custom aggregationStrategy is not used.","When splitting inside another split, the custom aggregationStrategy is not used. For example in the route:

{code:xml}
	    <route id=""DoubleSplitRoute"">
	    	<from uri=""direct:in"" />
	    	<setBody>
	    		<constant>
&lt;a&gt;
	&lt;b&gt;
		&lt;c&gt;Hello&lt;/c&gt;
		&lt;c&gt;World&lt;/c&gt;
	&lt;/b&gt;
	&lt;b&gt;
		&lt;c&gt;Hello&lt;/c&gt;
		&lt;c&gt;again&lt;/c&gt;
	&lt;/b&gt;
&lt;/a&gt;
	    		</constant>
	    	</setBody>
			<split>
				<xpath>a/b</xpath>
				<split strategyRef=""concatWithSpaceStrategy"">
					<xpath>b/c/text()</xpath>
					<setBody><simple>${bodyAs(java.lang.String)}</simple></setBody>
					<log message=""Got a part: ${body}""/>
				</split>
				<log message=""Got a result: ${body}""/>
			</split>
		</route>
{code}

(where the {{concatWithSpaceStrategy}} does nothing more than to concat the bodies with a space inbetween.)

The expected results would be:

{code}
Got a result: Hello World
{code}

and 

{code}
Got a result: Hello again
{code}

But that is not what happens. The actual results are two times the same:

{code}
Got a result: 
<a>
	<b>
		<c>Hello</c>
		<c>World</c>
	</b>
	<b>
		<c>Hello</c>
		<c>again</c>
	</b>
</a>
{code}

The reason is, that the strategy is not used. In the class {{org.apache.camel.processor.MulticastProcessor}}, in the method {{protected AggregationStrategy getAggregationStrategy(Exchange exchange)}}, the first step is to find an aggregationStrategy in the Exchange. This is set to {{UseOriginalAggregationStrategy}}, and because it is not null, this aggregation strategy will be used, not the one declared for the splitter.

---

A workaround would be to remove the AggregationStrategy of the Exchange, before it is aggregated, by using a processor with the following process method:

{code:java}
	public void process(Exchange exchange) throws Exception {
		if (exchange != null) {
			exchange.removeProperty(Exchange.AGGREGATION_STRATEGY);
		}
	}
{code}

After integrating this in my route, I got the desired results.",,ojelinski,Minor,Closed,Fixed,17/Feb/11 12:01,17/Feb/11 12:47
Bug,CAMEL-3685,12499114,Splitter in parallel mode blocks when empty body to split,"This is only in the parallel mode. Works fine in non parallel.

See nabble:
http://camel.465427.n5.nabble.com/Trying-to-split-an-empty-List-blocks-a-Quartz-initiated-thread-indefinitely-td3391736.html#a3392166",davsclaus,davsclaus,Major,Closed,Fixed,19/Feb/11 07:35,25/Oct/11 11:36
Bug,CAMEL-3687,12499124,The camel-javaspaces component doesn't work with the latest version of Gigaspaces,I upgraded the camel-javaspaces to work with the latest 2.7-SNAPSHOT. Tested with Gigaspaces only,davsclaus,dgreco,Major,Closed,Fixed,19/Feb/11 15:15,25/Oct/11 11:36
Bug,CAMEL-3690,12499198,Endpoints may be shutdown twice as they are tracked in two lists in CamelContext,"Endpoint is a Service which means they are listed in both a endpoint and service list. They should only be listed in the endpoint list.

This avoids issues with endpoints may be shutdown twice when Camel shutdown.

See nabble
http://camel.465427.n5.nabble.com/QuartzComponent-do-not-delete-quartz-worker-threads-when-shutdown-Camel-tp3393728p3393728.html",davsclaus,davsclaus,Minor,Closed,Fixed,21/Feb/11 08:16,25/Oct/11 11:36
Bug,CAMEL-3702,12499334,Example camel-example-cxf-tomcat does not work on Tomcat 7,"I am trying to get the example camel-example-cxf-tomcat to work on Tomcat 7.0.8. This example contains a CxfEndpoint that is configured as being asynchronous (by default). Because of this asynchronous setting I was notified by an Exception that I should set the async-supported property of the CXFServlet in the web.xml. After I set this property to true, I see that invocations are received and processed by the Camel route, however the result is not returned: the connection is closed directly.
I assume this issue is not directly related to the example itself (apart from the missing async-supported property), however I was not able to pinpoint which component is triggering the connection to be closed before the result is written.",njiang,arnoud,Major,Closed,Fixed,22/Feb/11 12:39,25/Oct/11 11:35
Bug,CAMEL-3709,12499477,interceptFrom and from(Endpoint) don't work together,"When using interceptFrom(String) together with from(Endpoint), the below Exception occurs during the routes building process. Looking at RoutesDefinition.java:217 reveals, that the FromDefintion just created has no URI. That causes the comparison to all the interceptFroms' URIs to fail. As far as I can tell, the way to fix this would be to add {{setUri(myEndpoint.getEndpointUri())}} in the constructor {{FromDefinition(Endpoint endpoint)}}.

Below the stack trace, there is a unit test that demonstrates the issue. Until it if fixed, it can be easily circumvented by adding the commented-out line, and then change to {{from(""myEndpoint"")}}.
{code}
org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: null due to: null
	at org.apache.camel.util.EndpointHelper.matchEndpoint(EndpointHelper.java:109)
	at org.apache.camel.model.RoutesDefinition.route(RoutesDefinition.java:217)
	at org.apache.camel.model.RoutesDefinition.from(RoutesDefinition.java:167)
	at org.apache.camel.builder.RouteBuilder.from(RouteBuilder.java:101)
	at dk.mobilethink.adc2.endpoint.UnsetUriTest$1.configure(UnsetUriTest.java:18)
	at org.apache.camel.builder.RouteBuilder.checkInitialized(RouteBuilder.java:318)
	at org.apache.camel.builder.RouteBuilder.configureRoutes(RouteBuilder.java:273)
	at org.apache.camel.builder.RouteBuilder.addRoutesToCamelContext(RouteBuilder.java:259)
	at org.apache.camel.impl.DefaultCamelContext.addRoutes(DefaultCamelContext.java:612)
	at org.apache.camel.test.CamelTestSupport.setUp(CamelTestSupport.java:111)
	at junit.framework.TestCase.runBare(TestCase.java:132)
	at org.apache.camel.test.TestSupport.runBare(TestSupport.java:65)
	at junit.framework.TestResult$1.protect(TestResult.java:110)
	at junit.framework.TestResult.runProtected(TestResult.java:128)
	at junit.framework.TestResult.run(TestResult.java:113)
	at junit.framework.TestCase.run(TestCase.java:124)
	at junit.framework.TestSuite.runTest(TestSuite.java:232)
	at junit.framework.TestSuite.run(TestSuite.java:227)
	at org.junit.internal.runners.JUnit38ClassRunner.run(JUnit38ClassRunner.java:83)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)
Caused by: java.lang.NullPointerException
	at org.apache.camel.util.UnsafeUriCharactersEncoder.encode(UnsafeUriCharactersEncoder.java:56)
	at org.apache.camel.util.URISupport.normalizeUri(URISupport.java:162)
	at org.apache.camel.util.EndpointHelper.matchEndpoint(EndpointHelper.java:107)
	... 24 more
{code}

{code}
package dk.mobilethink.adc2.endpoint;

import org.apache.camel.Endpoint;
import org.apache.camel.builder.RouteBuilder;
import org.apache.camel.test.CamelTestSupport;

public class UnsetUriTest extends CamelTestSupport {
	@Override
	protected RouteBuilder createRouteBuilder() throws Exception {

		return new RouteBuilder() {
			public void configure() throws Exception {
				interceptFrom(""URI1"").to(""irrelevantURI"");

				Endpoint myEndpoint = getContext().getComponent(""direct"").createEndpoint(""ignoredURI"");
				
//				getContext().addEndpoint(""myEndpoint"", myEndpoint);
				from(myEndpoint)
					.inOnly(""log:foo"");
			}
		};
	}

	public void testNothing() { }
}
{code}
",davsclaus,sm,Minor,Closed,Fixed,23/Feb/11 12:12,25/Oct/11 11:35
Bug,CAMEL-3711,12499504,OnException - The order they are defined in the RouteBuilder / XML should be preserved when you have multiple of the same exception type,"When you have 2+ of {{onException(MyExcpetion.class)}} then the order may get reversed, in the logic which prepares the routes. The logic should cater for clashes and ensure orders is preserved.

This currently only occurs for XML based routes. The Java DSL works.",davsclaus,davsclaus,Major,Closed,Fixed,23/Feb/11 17:22,25/Oct/11 11:36
Bug,CAMEL-3713,12499544,Duplicate mail attachment names breaks the Message.getAttachments() interface contract,"If the camel-mail component consumes a Multipart email message that contains two parts with the same filename, subsequent calls to {{DataHandler dh = Message.getAttachments().get(""filename"")}} result in a ClassCastException.  This occurs because {{MailBinding.extractAttachmentsFromMultipart()}} calls {{CollectionHelper.appendValue(map, fileName, part.getDataHandler())}} to add an attachment to the Message attachments map.  If appendValue is passed a duplicate key, it converts the map value to an ArrayList and adds the existing value and the new value to the list.  The result is a Map that contains a mix of <String,DataSource> and <String,ArrayList>.

Fix is to NOT use CollectionHelper and to check for duplicate keys prior to adding an attachment.",hadrian,pegli,Major,Closed,Fixed,23/Feb/11 23:46,25/Oct/11 11:35
Bug,CAMEL-3715,12499570,Sending to http endpoint may double encoding parameters,"See nabble


The workaround is to use either HTTP_URI or HTTP_QUERY as a header with the parameters. Then they are not double encoded.

Also the HTTP_URI should be placeholder resolved, so we can use {{port}} or the likes to inject the port number dynamic.",davsclaus,davsclaus,Major,Closed,Fixed,24/Feb/11 09:24,25/Oct/11 11:35
Bug,CAMEL-3716,12499584,Archetype camel-archetype-war generates well the project but no web console is available in the browser,The archetype camel-archetype-war is not longer synchronized with last modifications made with component camel-web and so the camel web console does not appear into the web browser,davsclaus,cmoulliard,Minor,Closed,Fixed,24/Feb/11 11:32,25/Oct/11 11:36
Bug,CAMEL-3718,12499673,"OracleAQ does not support JMSReplyTo, so ensure try catch is used when accessing it","See nabble
http://camel.465427.n5.nabble.com/JmsComponent-changes-between-2-4-0-and-2-6-0-tp3398446p3398446.html",davsclaus,davsclaus,Major,Closed,Fixed,25/Feb/11 07:04,25/Oct/11 11:35
Bug,CAMEL-3720,12499753,camel-kestrel: concurrent gets optimized into a single get,"Please add camel-kestrel as a component in JIRA...thx.

The bug is that spymemcached is optimizing concurrent gets for the same key into a single get.  The fix is in the attached patch.",davsclaus,dcheckoway,Major,Closed,Fixed,25/Feb/11 20:15,25/Oct/11 11:35
Bug,CAMEL-3727,12499913,Recipient list with parallel processing doesn't reuse aggregation threads,"When I'm using recipient list in parallel mode {{aggregateExecutorService}} in {{MulticastProcessor}} doesn't reuse threads and is creating one new thread per each request.

To reproduce this bug simply add a loop to {{RecipientListParallelTest.testRecipientListParallel()}} test:
{code:title=RecipientListParallelTest.java|borderStyle=solid}
    public void testRecipientListParallel() throws Exception {
        for (int i = 0; i < 10000; i++) {
            MockEndpoint mock = getMockEndpoint(""mock:result"");
            mock.reset();
            mock.expectedBodiesReceivedInAnyOrder(""c"", ""b"", ""a"");
            template.sendBodyAndHeader(""direct:start"", ""Hello World"", ""foo"", ""direct:a,direct:b,direct:c"");
            assertMockEndpointsSatisfied();
        }
    }
{code}

In the logs you can find:
{code}
2011-02-28 13:22:30,984 [) thread #0 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.
2011-02-28 13:22:31,984 [) thread #4 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.
2011-02-28 13:22:32,984 [) thread #8 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.
2011-02-28 13:22:34,000 [ thread #12 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.
2011-02-28 13:22:35,000 [ thread #14 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.
2011-02-28 13:22:36,000 [ thread #15 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.
2011-02-28 13:22:37,015 [ thread #16 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.
2011-02-28 13:22:38,015 [ thread #17 - RecipientListProcessor-AggregateTask] DEBUG MulticastProcessor             - Done aggregating 3 exchanges on the fly.
{code}
",davsclaus,marcin kolda,Major,Closed,Fixed,28/Feb/11 12:28,25/Oct/11 11:36
Bug,CAMEL-3729,12499963,JAXB marshaling broken in 2.6.0,"This used to work in 2.5.0 but in 2.6.0 it throws:

{noformat}
Caused by: javax.xml.bind.JAXBException: class com.ask.ugc.camel.Jask3061$Foo nor any of its super class is known to this context.
	at com.sun.xml.bind.v2.runtime.JAXBContextImpl.getBeanInfo(JAXBContextImpl.java:594)
	at com.sun.xml.bind.v2.runtime.XMLSerializer.childAsRoot(XMLSerializer.java:482)
	at com.sun.xml.bind.v2.runtime.MarshallerImpl.write(MarshallerImpl.java:315)
	at com.sun.xml.bind.v2.runtime.MarshallerImpl.marshal(MarshallerImpl.java:244)
	at javax.xml.bind.helpers.AbstractMarshallerImpl.marshal(AbstractMarshallerImpl.java:75)
	at org.apache.camel.converter.jaxb.JaxbDataFormat.marshal(JaxbDataFormat.java:117)
	at org.apache.camel.converter.jaxb.JaxbDataFormat.marshal(JaxbDataFormat.java:96)
...
{noformat}


{code:java}
@ContextConfiguration(
        locations = ""foo.bar.JaxbBug$ContextConfig"",
        loader = JavaConfigContextLoader.class)
public class JaxbBug extends AbstractTestNGSpringContextTests {

    @EndpointInject(uri = ""mock:end"")
    protected MockEndpoint mockEndpoint;

    @Produce(uri = ""direct:test"")
    protected ProducerTemplate producer;

    @Test
    @DirtiesContext
    public void testJaxbMarshalling() throws InterruptedException {
        mockEndpoint.expectedMessageCount(1);

        producer.sendBody(new Foo());

        mockEndpoint.assertIsSatisfied();

    }

    @Configuration
    public static class ContextConfig extends SingleRouteCamelConfiguration {
        @Bean
        public RouteBuilder route() {
            return new RouteBuilder() {
                @Override
                public void configure() throws Exception {
                    JaxbDataFormat jaxb = new JaxbDataFormat(JAXBContext.newInstance(Foo.class));
                    from(""direct:test"").marshal(jaxb).to(""mock:end"");


                }
            };
        }
    }

    @XmlRootElement
    public static class Foo {
        String bar;

        public String getBar() {
            return bar;
        }

        public void setBar(String bar) {
            this.bar = bar;
        }
    }

}
{code}",davsclaus,dragisak,Major,Closed,Fixed,28/Feb/11 18:46,25/Oct/11 11:36
Bug,CAMEL-3749,12500323,sftp producer runs in exception fileName does not contain any path information,"When the fileName uri option does not contain any folder information, ie: ""fileName=filename.txt"" the producer runs in exception

The exception is thrown when the producer calls ls method of jsch ChannelSftp passing an empty string as directory parameter in org.apache.camel.component.file.remote.SftpOperations.existsFile line 704

sample URI: 
sftp://user@hostname:22?disconnect=true&eagerDeleteTargetFile=false%3Bstepwise%3Dfalse&fileName=SYSTEM01&password=******&soTimeout=30000&tempFileName=%24%7Bfile%3Aname.noext%7D.tmp

Stack trace:
{code}
[2011-03-03 14:40:40,827][RecipientList (thread #4)][ERROR][org.apache.camel.processor.DefaultErrorHandler][] Failed delivery for exchangeId: ID-******-33331-1299163043684-0-10. Exhausted after delivery attempt: 1 caught: org.apache.camel.component.file.GenericFileOperationFailedExcept on: 
org.apache.camel.component.file.GenericFileOperationFailedException: 
        at org.apache.camel.component.file.remote.SftpOperations.existsFile(SftpOperations.java:727) 
        at org.apache.camel.component.file.GenericFileProducer.processExchange(GenericFileProducer.java:130) 
        at org.apache.camel.component.file.remote.RemoteFileProducer.process(RemoteFileProducer.java:50) 
        at org.apache.camel.impl.converter.AsyncProcessorTypeConverter$ProcessorToAsyncProcessorBridge.process(AsyncProcessorTypeConverter.java:50) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77) 
        at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:299) 
        at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208) 
        at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:109) 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:103) 
        at org.apache.camel.processor.MulticastProcessor.doProcessParallel(MulticastProcessor.java:716) 
        at org.apache.camel.processor.MulticastProcessor.access$100(MulticastProcessor.java:78) 
        at org.apache.camel.processor.MulticastProcessor$1.call(MulticastProcessor.java:282) 
        at org.apache.camel.processor.MulticastProcessor$1.call(MulticastProcessor.java:274) 
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) 
        at java.util.concurrent.FutureTask.run(FutureTask.java:138) 
        at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) 
        at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) 
        at java.util.concurrent.FutureTask.run(FutureTask.java:138) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) 
        at java.lang.Thread.run(Thread.java:619) 
Caused by: 4: 
        at com.jcraft.jsch.ChannelSftp.ls(ChannelSftp.java:1353) 
        at org.apache.camel.component.file.remote.SftpOperations.existsFile(SftpOperations.java:704) 
        ... 20 more 
Caused by: java.lang.StringIndexOutOfBoundsException: String index out of range: 0 
        at java.lang.String.charAt(String.java:686) 
        at com.jcraft.jsch.ChannelSftp.remoteAbsolutePath(ChannelSftp.java:2367) 
        at com.jcraft.jsch.ChannelSftp.ls(ChannelSftp.java:1185) 
        ... 21 more
{code}
",davsclaus,crive,Major,Closed,Fixed,03/Mar/11 16:58,25/Oct/11 11:36
Bug,CAMEL-3753,12500499,Relax check for clientId must be set when using durable subscribers,"Using durable subscribers, the clientId must be configured as well, so the broker knows who the client is.
However some JMS providers mandates the client id to be configured on the JMS ConnectionFactory instead.

So we should relax this check in camel-jms

See nabble
http://camel.465427.n5.nabble.com/Camel-GlassFish-and-durable-subscriber-tp3408634p3408634.html",davsclaus,davsclaus,Minor,Closed,Fixed,05/Mar/11 07:36,25/Oct/11 11:35
Bug,CAMEL-3757,12500510,Auto mock endpoints should strip parameters to avoid confusing when accessing the mocked endpoint,"If you use mocking existing endpoints, which is detailed here
http://camel.apache.org/mock.html

We should stip parameters of the mocked endpoint, eg {{file:xxxx?noop=true}}. eg so the mocked endpoint would be {{mock:file:xxxx}} without any of the parameters.

Otherwise the mock endpoint expects those parameters is part of the mock endpoint and will fail creating the mock endpoint.",davsclaus,davsclaus,Minor,Closed,Fixed,05/Mar/11 12:29,25/Oct/11 11:35
Bug,CAMEL-3760,12500536,ManagementNamingStrategy - Should normalize ObjectName to avoid using illegal characters,"For example when using JMS in the loanbroaker example. There us a colon in the JMS queue name which is invalid char in JMX.

2011-03-06 08:26:55,859 [main           ] WARN  ManagedManagementStrategy      - Cannot check whether the managed object is registered. This exception will be ignored.
javax.management.MalformedObjectNameException: Could not create ObjectName from: org.apache.camel:context=vesta.apache.org/camel-1,type=threadpools,name=JmsReplyManagerTimeoutChecker[queue2:parallelLoanRequestQueue]. Reason: javax.management.MalformedObjectNameException: Invalid character ':' in value part of property
	at org.apache.camel.management.DefaultManagementNamingStrategy.createObjectName(DefaultManagementNamingStrategy.java:315)[camel-core-2.7-SNAPSHOT.jar:2.7-SNAPSHOT]


",davsclaus,davsclaus,Minor,Closed,Fixed,06/Mar/11 08:43,25/Oct/11 11:35
Bug,CAMEL-3769,12500780,Mail component issue with starttls option,"The problem occurs when I read from a pop3 endpoint and send to an smtp endpoint in the same camel context with the mail.pop3.starttls.enable=true and mail.smtp.starttls.enable=true options.

Required Java options for starttls are set:
-Djavax.net.ssl.trustStore=D:\test\xxx.jks
-Djavax.net.ssl.trustStorePassword=yyy


When I only configure one of either route, everything works fine. When I configure both, I get the following exception:

AUTH LOGIN
C: STAT
530 Must issue STARTTLS command first
STARTTLS
S: +OK 0 0
C: NOOP
220 begin TLS negotiation
S: +OK
C: RSET
S: +OK
C: QUIT
AUTH LOGIN
S: +OK POP3 server closing connection
503 wrong state for AUTH command
2011-03-03 10:08:36,797 [foo] ERROR DefaultErrorHandler - Failed delivery for exchangeId: ID-E6500-ahi-61446-1299143304838-0-2. Exhausted after delivery attempt: 1 caught: org.springframework.mail.MailAuthenticationException: Authentication failed; nested exception is javax.mail.AuthenticationFailedException: 503 wrong state for AUTH command

org.springframework.mail.MailAuthenticationException: Authentication failed; nested exception is javax.mail.AuthenticationFailedException: 503 wrong state for AUTH command

        at org.springframework.mail.javamail.JavaMailSenderImpl.doSend(JavaMailSenderImpl.java:392)[org.springframework.context.support-3.0.5.RELEASE.jar:3.0.5.RELEASE]
        at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:340)[org.springframework.context.support-3.0.5.RELEASE.jar:3.0.5.RELEASE]
        at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:355)[org.springframework.context.support-3.0.5.RELEASE.jar:3.0.5.RELEASE]
        at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:344)[org.springframework.context.support-3.0.5.RELEASE.jar:3.0.5.RELEASE]
        at org.apache.camel.component.mail.MailProducer.process(MailProducer.java:44)[camel-mail-2.6.0.jar:2.6.0]
        at org.apache.camel.impl.converter.AsyncProcessorTypeConverter$ProcessorToAsyncProcessorBridge.process(AsyncProcessorTypeConverter.java:50)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:104)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:272)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:98)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:299)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:269)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.Pipeline.process(Pipeline.java:125)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.Pipeline.process(Pipeline.java:80)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:102)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.RoutePolicyProcessor.process(RoutePolicyProcessor.java:75)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:91)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:85)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.component.timer.TimerConsumer.sendTimerExchange(TimerConsumer.java:104)[camel-core-2.6.0.jar:2.6.0]
        at org.apache.camel.component.timer.TimerConsumer$1.run(TimerConsumer.java:49)[camel-core-2.6.0.jar:2.6.0]
        at java.util.TimerThread.mainLoop(Unknown Source)[:1.5.0_22]
        at java.util.TimerThread.run(Unknown Source)[:1.5.0_22]



The problem seems to be in org.apache.camel.component.mail.MailConfiguration.createJavaMailSender() where it uses the same mail session for both connection; see Session.getDefaultInstance below

        if (session != null) {
            answer.setSession(session);
        } else {
            // use our authenticator that does no live user interaction but returns the already configured username and password
            Session session;
            try {
                session = Session.getDefaultInstance(answer.getJavaMailProperties(), getAuthenticator());
            } catch (Throwable t) {
                // fallback as default instance may not be allowed on some systems
                session = Session.getInstance(answer.getJavaMailProperties(), getAuthenticator());
            }
            answer.setSession(session);
        }


This is because getDefaultInstance creates a Session object the first time it is called. Then it caches that Session and returns it for all subsequent calls. It also ignores the new and different properties for the second route.


See also http://camel.465427.n5.nabble.com/Mail-component-with-starttls-td3409505.html 
",davsclaus,ahiebl,Major,Closed,Fixed,08/Mar/11 16:08,25/Oct/11 11:35
Bug,CAMEL-3779,12501095,HttpProducer drops authentification parameters.,"Hi, 

I run into the following problem with Camel 2.6.0:

An HTML file contain an URI lists of files to be downloaded from a web server to the local file system.
The HTML file is parsed via a Java Bean Splitter. The Java Bean Splitter produces a Set of URI strings.
These files should be downloaded from a web server to the local file system. The files are protected with BASIC authentification.

The extraction and the splitting of the download URIs works quite well with the Java Bean Splitter.
The Java Bean Splitter produces URI like: 
{code}http4://download.acme.com/file_1.txt?username=foo&password=baa{code}

Here's the setup:
{code}
<route>
   <from uri=""file:///tmp/files_to_retrieve/"" />
   <method bean=""prepareDownLoadUri"" method=""parseIndexHtml"" />
   <setHeader headerName=""CamelHttpMethod"">
     <constant>GET</constant>
   </setHeader> 
   <setHeader headerName=""CamelHttpUri"">
     <simple>${body}</simple>
   </setHeader>
   <to uri=""http4://download.acme.com/"" />
   <to uri=""bean:saveFileProcessor"" />
</route>
<bean id=""prepareDownLoadUri"" class=""com.acme.PrepareDownLoadUri"" />
<bean id=""saveFileProcessor"" class=""com.acme.SaveFileProcessor"" />
{code}
The injection of the URIs from the Splitter into the HttpProducer, works quite well. 
I debugged into the HttpProducer and it seems, that the HttpProducer does not provide the unterlying http client (in this case Apache HttpClient 4) with  the authentification settings from the URI.

At first, the queryString is extracted from the Exchange-Header (if provided)
{code:title=HttpProducer.createMethod(), line 273}
String url = HttpHelper.createURL(exchange, getEndpoint());
{code}
The url string contains the URI produces by the splitter including the authentification parameters: 
{code}http4://download.acme.com/file_1.txt?username=foo&password=baa{code}

Then the HttpProducer assembles a new URI for the requests. The new URI is assembled from parts of the string url (line 273)  
and other parameters from the Exchange Header: 
{code:title=HttpProducer.createMethod(), lines 285-300}
        // is a query string provided in the endpoint URI or in a header (header overrules endpoint)
        String queryString = exchange.getIn().getHeader(Exchange.HTTP_QUERY, String.class);
        if (queryString == null) {
            queryString = getEndpoint().getHttpUri().getRawQuery();
        }

        StringBuilder builder = new StringBuilder(uri.getScheme()).append(""://"").append(uri.getHost());

        if (uri.getPort() != -1) {
            builder.append("":"").append(uri.getPort());
        }

        if (uri.getPath() != null) {
            builder.append(uri.getRawPath());
        }

        if (queryString != null) {
            builder.append('?');
            builder.append(queryString);
        }

        HttpRequestBase httpRequest = methodToUse.createMethod(builder.toString());
{code}

The problem is, in the assembling of the new URI, the authentification parameters are dropped.  

One possible solution could be: 
# check for authentification parameters in the url (line 273) and extract them.
# build the credentials, AuthScheme and provide it to the underlying http client.

thanks in advance,
ben


",njiang,ben@micromata.de,Major,Resolved,Fixed,11/Mar/11 01:36,09/Jun/11 08:48
Bug,CAMEL-3788,12501723,HTTP Producer seems to send an empty body request on redelivery when the original request fails due to a connection timeout,"Camel HTTP Producer is throwing an exception when it encounters a connection timeout. However instead of preserving the original message it is treating it processing the exception output. When connected to a redelivery meachanism, this results in an exception message being sent as input to the HTTP Consumer. The older versions of Camel behaved differently on read timeout. In Camel 1.x, in a read timeout, the Exception is thrown and the input message to the producer is passed through the pipeline. Now there is a difference: An exception is thrown and the output of the HTTP Producer is passed through the pipeline. 

Need to study why and what changed since Camel 1.x and ensure that the above does not happen.

For more details, please check out the following nabble entry

http://camel.465427.n5.nabble.com/Http-route-returning-with-200-but-also-getting-read-timed-out-td3741631.html#a3897410

Cheers,

Ashwin...",njiang,akarpe,Major,Closed,Fixed,17/Mar/11 19:47,25/Oct/11 11:36
Bug,CAMEL-3789,12501793,org.apache.camel.component.file.strategy.MarkerFileExclusiveReadLockStrategy is not thread-safe,"MarkerFileExclusiveReadLockStrategy is not thread-safe. When I run  a File endpoint with more than one thread the MarkerFileExclusiveReadLockStrategy only deletes the last file to start being processed. 

The MarkerFileExclusiveReadLockStrategy uses global variables: 
private File lock; 
private String lockFileName; 
and gives them values on the acquireExclusiveReadLock method. When another thread calls the releaseExclusiveReadLock method it uses the global variables to delete the locked file. That means that if another thread came and called the acquireExclusiveReadLock it would have changed the values on the global variables. 

If lock and lockFileName are not global variables the problem seems to disappear and I can a multithreaded File endpoint and not locked file is left undeleted. 
",davsclaus,maria.iracheta@gmail.com,Major,Closed,Fixed,18/Mar/11 14:09,25/Oct/11 11:36
Bug,CAMEL-3791,12501926,Camel should reset the stream cache if the useOriginalInMessage option is true,"{code}
--- src/main/java/org/apache/camel/processor/RedeliveryErrorHandler.java	(revision 1083672)
+++ src/main/java/org/apache/camel/processor/RedeliveryErrorHandler.java	(working copy)
@@ -591,18 +591,23 @@
         // is the a failure processor to process the Exchange
         if (processor != null) {
 
-            // reset cached streams so they can be read again
-            MessageHelper.resetStreamCache(exchange.getIn());
-
             // prepare original IN body if it should be moved instead of current body
             if (data.useOriginalInMessage) {
                 if (log.isTraceEnabled()) {
                     log.trace(""Using the original IN message instead of current"");
                 }
                 Message original = exchange.getUnitOfWork().getOriginalInMessage();
                 exchange.setIn(original);
             }

+            // reset cached streams so they can be read again
+            MessageHelper.resetStreamCache(exchange.getIn());
{code}",njiang,njiang,Major,Resolved,Fixed,21/Mar/11 07:58,28/Mar/11 00:18
Bug,CAMEL-3795,12501970,java.util.concurrent.RejectedExecutionException using interceptSendToEndpoint,in some circumstances interceptSendToEndpoint in conjunction with recipientList is running in exception. this seems to be the case when an http endpoint is sent in the recipient list after having processed and ftp in the previous exchange.,davsclaus,crive,Major,Closed,Fixed,21/Mar/11 18:48,25/Oct/11 11:36
Bug,CAMEL-3797,12502042,Web Console: NPE when browsing routes with no description,"When some of the routes does not have a description NPE will be thrown, because of the code

{code}${i.getDescription.getText}{code}

To prevent it, just use

{code}${i.getDescriptionText}{code}",janstey,dejanb,Major,Closed,Fixed,22/Mar/11 12:42,25/Oct/11 11:36
Bug,CAMEL-3799,12502126,maven eclipse goal failure,"The error is:

{code}
[ERROR] BUILD ERROR
[INFO] ------------------------------------------------------------------------
[INFO] Request to merge when 'filtering' is not identical. Original=resource src/main/resources: output=target/classes, include=[], exclude=[**/*.java], test=false, filtering=false, merging with=resource src/main/resources: output=target/classes, include=[**/archetype-metadata.xml], exclude=[**/*.java], test=false, filtering=true
{code}

and it seems to be resolved in the upcoming maven-eclipse-plugin:2.9.0 (see [MECLIPSE-576|http://jira.codehaus.org/browse/MECLIPSE-576]). An update to the new release of the plugin, once available, should resolve this.

",hadrian,hadrian,Minor,Resolved,Fixed,23/Mar/11 02:46,20/Apr/11 19:48
Bug,CAMEL-3802,12502268,Simple language and OGNL does not support dots in key name,"It is not possible in a simple expression to have access to a key of a Map when the key name contains dots

ex : <simple>${body[com.fusesource.webinars.persistence.model.Incident]}</simple>
where com.fusesource.webinars.persistence.model.Incident is the key name

Camel generates the error :

{code}
Caused by: org.apache.camel.component.bean.MethodNotFoundException: Method with name: [com not found on bean: [{com.fusesource.webinars.persistence.model.Incident=com.fusesource.webinars.persistence.model.Incident@7fec0e19[
{code}",cmoulliard,cmoulliard,Minor,Closed,Fixed,24/Mar/11 09:37,25/Oct/11 11:36
Bug,CAMEL-3805,12502372,Setting null body on JmsMessage should not re-initialize JMS message again,"See nabble
http://camel.465427.n5.nabble.com/choice-when-check-BodyType-null-Body-null-tp4259599p4259599.html

We should use a boolean to keep track if the message body has been explict set to null using setBody(null) to avoid re-initialize the message body on subsequent accesses",davsclaus,davsclaus,Minor,Closed,Fixed,25/Mar/11 11:34,25/Oct/11 11:36
Bug,CAMEL-3806,12502380,SOAPAction HTTP header value not copied correctly to from CXF message to Camel message header,"When using camel-cxf consumer endpoint with PAYLOAD data format, the SoapActionInInterceptor correctly strips off any double quotes surrounding the SOAPAction header value, e.g:
{code}
SOAPAction: ""http://apache.org/hello_world_soap_http/greetMe""
{code}

and then sets this header on the message again:

{code:title=SoapActionInInterceptor.java}
public void handleMessage(SoapMessage message) throws Fault {
        String action = getSoapAction(message);
        if (!StringUtils.isEmpty(action)) {
            getAndSetOperation(message, action);
        }
    }
{code}

After the execution of getAndSetOperation(message, action), there are two SOAPAction headers set on the message. The original SOAPAction header value (with surrounding quotes) is part of the org.apache.cxf.message.Message.PROTOCOL_HEADERS. In addition the getAndSetOperation() method also adds 
SOAPAction=http://apache.org/hello_world_soap_http/greetMe header (without surrounding quotes). 

Later in org.apache.camel.component.cxf.DefaultCxfBinding.propagateHeadersFromCxfToCamel(), when the CXF message headers get copied to the Camel message, it only retrieves the org.apache.cxf.message.Message.PROTOCOL_HEADERS:

{code}
Map<String, List<String>> cxfHeaders = (Map)cxfMessage.get(Message.PROTOCOL_HEADERS);
{code}

The SOAPAction header in PROTOCOL_HEADERS wasn't changed by the SoapActionInInterceptor and hence still has enclosed double quotes. These headers then get copied to the Camel In message headers. The header that was set by the SoapActionInInterceptor SOAPAction=http://apache.org/hello_world_soap_http/greetMe does not get copied!

If later in a Camel route a custom Camel Processor tries to resolve this SOAPAction header, it gets the the value with surrounding double quotes, but these should actually have been stripped off. 

This behavior is a bug.

",njiang,tmielke,Major,Closed,Fixed,25/Mar/11 14:06,25/Oct/11 11:35
Bug,CAMEL-3808,12502407,Restlet Producer to add query to request URI based on runtime information (e.g. Camel header),"Currently, custom queries to be sent in the request URI must be defined in the route.  It is more usable to set the request URI queries at runtime. ",wtam,wtam,Minor,Closed,Fixed,25/Mar/11 19:12,25/Oct/11 11:35
Bug,CAMEL-3810,12502491,"At payload model, set soap body element to header uncorrectly","if wsdl's operatin's has multi parts and the parts use same xml-element. Camel CXF Payload model, will mixup the xml element in body with header",njiang,ext2xhb,Major,Closed,Fixed,27/Mar/11 05:47,11/Mar/15 13:31
Bug,CAMEL-3812,12502543,BeanInfo not reset when new bean looked up,"In camel-bean, when a bean is looked-up and it is not the same bean as the previous one beanInfo has to be reset.
This is done in getBean() method of org.apache.camel.component.bean.RegistryBean but the test is placed after setting bean=value so beanInfo is never reset :

if (value != bean) { 
    bean = value; 
    processor = null; 
    if (!ObjectHelper.equal(ObjectHelper.type(bean), ObjectHelper.type(value))) { 
        beanInfo = null; 
    }",davsclaus,maximilien,Major,Closed,Fixed,28/Mar/11 07:26,25/Oct/11 11:36
Bug,CAMEL-3816,12502668,ArrayIndexOutOfBoundsException in DefaultHttpBinding,"See nabble
http://servicemix.396122.n5.nabble.com/http-post-to-camel-jetty-smx-4-3-ArrayIndexOutOfBoundsException-tp4258334p4258334.html",njiang,davsclaus,Major,Resolved,Fixed,29/Mar/11 06:54,30/Mar/11 08:52
Bug,CAMEL-3826,12503100,Hawtdb - File store keeps growing if index are unique,"If you use unique correlation expressions for aggregator with hawtdb, but uses timeout for completion, the indexes are properly removed using the hawtdb API. But the file store keeps growing.
",davsclaus,davsclaus,Major,Closed,Fixed,01/Apr/11 08:08,25/Oct/11 11:36
Bug,CAMEL-3828,12503160,camel-jms default cache level should be CACHE_NONE when transaction is enabled,"I lost a huge amount of time chasing the problem as the default value just screw the whole transaction (the session is created outside the transaction and cached, so usually not enlisted in the tx).
",davsclaus,gnodet,Major,Resolved,Fixed,01/Apr/11 18:40,08/Apr/11 14:01
Bug,CAMEL-3834,12503340,SmppProducer sends incorrect data_coding parameter,"The dataCoding parameter on the SMPP uri allows for a value of 0,4, or 8.  Our aggregator expects the data_coding parameter on the SMPP request from the SmppProducer to be 0=SMSC Default Alphabet.  However, when 0, 4, or 8 is specified for the dataCoding parameter, the actual value sent on the smpp request is 1, 5, or 9.  It appears the MessageClass.Class1 on the GeneralDataCoding object in SmppProducer is masking an additional 1 to the byte value.

It seems the dataCoding parameter should allow you to specify the actual value that is needed as defined by the SMPP Protocol specification.",muellerc,sgansemer@mfoundry.com,Major,Closed,Fixed,04/Apr/11 20:37,25/Oct/11 11:35
Bug,CAMEL-3835,12503366,camel-cache osgi metadata isn't correct,"1. export package org.apache.camel.component.cache doesn't has version
2. both import and export org.apache.camel.processor.cache package",davsclaus,ffang,Major,Closed,Fixed,05/Apr/11 05:14,25/Oct/11 11:35
Bug,CAMEL-3842,12503631,keys in cxf's protocol_headers must be handled case-insenstively,"There was an issue in CXF, where the protocol headers were not stored in a case-insensitive manor. This lead to the problem of not being able to find some headers or finding some headers in duplicates at some CXF interceptors. This issue was fixed in CXF-3367 for CXF 2.3.4 and 2.4. It was fixed by not using the plain HashMap for storing the protocol headers. 

Currently (in trunk), camel-cxf's CxfHeaderHelper and DefaultCxfBinding seem to be using HashMap to create a new CXF's message instance and this can lead to the same issue.

In CXF, this issue was fixed by replacing the usage of HashMap<String, List<String>)() with that of TreeMap<String, List<String>>(String.CASE_INSENSITIVE_ORDER). This approach was chosen over an alternative approach that converts the header names into lowercase and store them a hashmap because there are typically a small number of entries and using a binary search in TreeMap seems appropriate in this use case.

I have prepared a patch for a similar change in camel-cxf's CxfHeaderHelper and DefaultCxfBinding and also prepared some unit test cases to verify the behavior.

Please take a look at this suggestion.

Thanks.
Regards, Aki
",davsclaus,ay,Major,Closed,Fixed,07/Apr/11 12:45,25/Oct/11 11:36
Bug,CAMEL-3847,12503722,Adding type converter should clear misses map for the given type,"See nabble
http://camel.465427.n5.nabble.com/addTypeConverter-does-not-clear-misses-in-BaseTypeConverterRegistry-tp4288871p4288871.html",davsclaus,davsclaus,Minor,Closed,Fixed,08/Apr/11 10:19,25/Oct/11 11:36
Bug,CAMEL-3849,12503740,OSGi - No possibility to enable ehcache clustering as: ehcache.xml is ignored and there is no possibility to set cacheManagerFactory described camel-cache docs,"This bug is caused by more than one problem as:
1. camel-cache bundle provides ehcache.xml config which is ignored so then default ehcache-failsafe.xml is taken by ehcache bundle.
2. according to camel-cache docs there is a way to set cacheManagerFactory but it seems to be not implemented as
a) default CacheManagerFactory instance is created when when CacheComponent is instantiated.
b) setter method: public void setCacheManagerFactory(CacheManagerFactory cacheManagerFactory) is used just nowhere.
c) there is no suitable filed in class CacheConfiguration

All above problems cause camel-cache to be not able to work in java- clustered world as default failsafe config. is really failsafe- w/o clustering.
",davsclaus,nannou9,Major,Closed,Fixed,08/Apr/11 14:13,25/Oct/11 11:35
Bug,CAMEL-3850,12503817,Splitter uses a to low configured aggregation task thread pool,"See nabble
http://camel.465427.n5.nabble.com/Camel-split-aggregate-parallelProcessing-hiccup-td4288393.html#a4292610

We should increase the thread pool to use a thread pool profile which has 10 threads by default.",davsclaus,davsclaus,Major,Closed,Fixed,09/Apr/11 09:04,25/Oct/11 11:36
Bug,CAMEL-3856,12504134,org.apache.camel.component.quartz package export not versioned Quartz component,"The Quartz packaging for OSGi uses the following configuration:

{code:xml}
<camel.osgi.export.pkg>
org.apache.camel.component.quartz.*,
org.apache.camel.routepolicy.quartz.*
</camel.osgi.export.pkg>
{code}

This gets concatenated as ${camel.osgi.export.pkg};${camel.osgi.version} and then fed to the Felix Bundle Plug-in.  The issue I am seeing is that only the second package gets versioned in the bundle manifest.  org.apache.camel.component.quartz is exported without a version.",davsclaus,davaleri,Minor,Closed,Fixed,13/Apr/11 04:14,25/Oct/11 11:35
Bug,CAMEL-3860,12504224,Camel cxfrs producer should support the configure the query parameter from the Exchange.HTTP_QUERY message header,"As CXFRS producer can't deal with the HttpQuery from the Exchange.HTTP_QUERY message header, the camel-cxfrs router can't route the http get request with query rightly.
Here is the mail thread[1] which talks about it.

[1]http://camel.465427.n5.nabble.com/CXF-RS-QueryParam-and-HeaderParam-tp4301808p4301808.html
",njiang,njiang,Major,Closed,Fixed,14/Apr/11 04:31,25/Oct/11 11:36
Bug,CAMEL-3861,12504242,camel-web - Cannot browse endpoints if messages was consumed from jms queue,"You get a NPE if you brows the endpoint.

Caused by: 
java.lang.NullPointerException
        at org.apache.camel.web.resources.$_scalate_$ExchangeResource_index_ssp$$anonfun$$_scalate_$render$2.apply(ExchangeResource.index.ssp.scala:46)
        at org.apache.camel.web.resources.$_scalate_$ExchangeResource_index_ssp$$anonfun$$_scalate_$render$2.apply(ExchangeResource.index.ssp.scala:40)
        at scala.collection.Iterator$class.foreach(Iterator.scala:631)",davsclaus,davsclaus,Minor,Closed,Fixed,14/Apr/11 08:54,25/Oct/11 11:35
Bug,CAMEL-3862,12504287,camel-cache- ehcache cache replication data not distributed across nodes,Cache replication across cluster nodes doesn't work using jms or rmi replication.,davsclaus,nannou9,Major,Closed,Fixed,14/Apr/11 16:51,25/Oct/11 11:36
Bug,CAMEL-3873,12504446,"camel-xmpp - Should stop and clean session, so hot-deploy works ","See this stacktrace


{code}
14:10:55,806 | ERROR | ExtenderThread-8 | ContextLoaderListener            | 72 - org.springframework.osgi.extender - 1.2.0 | Application context refresh failed (OsgiBundleXmlApplicationContext(bundle=qbus-camel-osgi, config=osgibundle:/META-INF/spring/*.xml))
org.apache.camel.FailedToCreateProducerException: Failed to create Producer for endpoint: Endpoint[xmpp://articleRoute@localhost?password=******&room=mechatron%40conference.mw-pc]. Reason: java.lang.NullPointerException
	at org.apache.camel.impl.ProducerCache.doGetProducer(ProducerCache.java:362)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ProducerCache.acquireProducer(ProducerCache.java:93)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ProducerCache.startProducer(ProducerCache.java:124)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.processor.SendProcessor.doStart(SendProcessor.java:146)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:65)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:52)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:60)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startServices(ServiceHelper.java:74)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.processor.DelegateAsyncProcessor.doStart(DelegateAsyncProcessor.java:77)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:65)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:52)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:60)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startServices(ServiceHelper.java:74)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.processor.DelegateAsyncProcessor.doStart(DelegateAsyncProcessor.java:77)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.processor.interceptor.TraceInterceptor.doStart(TraceInterceptor.java:429)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:65)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:52)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:60)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startServices(ServiceHelper.java:74)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.processor.RedeliveryErrorHandler.doStart(RedeliveryErrorHandler.java:847)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:65)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:52)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:60)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startServices(ServiceHelper.java:74)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.processor.DefaultChannel.doStart(DefaultChannel.java:146)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:65)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:52)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:60)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startServices(ServiceHelper.java:86)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.processor.MulticastProcessor.doStart(MulticastProcessor.java:893)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:65)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:52)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:60)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startServices(ServiceHelper.java:74)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.processor.DelegateAsyncProcessor.doStart(DelegateAsyncProcessor.java:77)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:65)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:52)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:60)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startServices(ServiceHelper.java:74)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.processor.DelegateAsyncProcessor.doStart(DelegateAsyncProcessor.java:77)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:65)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:52)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:60)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.RouteService.startChildService(RouteService.java:250)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.RouteService.warmUp(RouteService.java:146)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.DefaultCamelContext.doWarmUpRoutes(DefaultCamelContext.java:1800)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.DefaultCamelContext.safelyStartRouteServices(DefaultCamelContext.java:1726)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.DefaultCamelContext.doStartOrResumeRoutes(DefaultCamelContext.java:1528)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.DefaultCamelContext.doStartCamel(DefaultCamelContext.java:1420)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.DefaultCamelContext.doStart(DefaultCamelContext.java:1321)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.spring.SpringCamelContext.doStart(SpringCamelContext.java:164)[75:org.apache.camel.camel-spring:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:65)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:52)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:1299)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.spring.SpringCamelContext.maybeStart(SpringCamelContext.java:203)[75:org.apache.camel.camel-spring:2.6.0.fuse-01-09]
	at org.apache.camel.spring.SpringCamelContext.onApplicationEvent(SpringCamelContext.java:101)[75:org.apache.camel.camel-spring:2.6.0.fuse-01-09]
	at org.apache.camel.spring.CamelContextFactoryBean.onApplicationEvent(CamelContextFactoryBean.java:238)[75:org.apache.camel.camel-spring:2.6.0.fuse-01-09]
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:97)[61:org.springframework.context:3.0.5.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:303)[61:org.springframework.context:3.0.5.RELEASE]
	at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:911)[61:org.springframework.context:3.0.5.RELEASE]
	at org.springframework.osgi.context.support.AbstractOsgiBundleApplicationContext.finishRefresh(AbstractOsgiBundleApplicationContext.java:235)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.osgi.context.support.AbstractDelegatedExecutionApplicationContext$4.run(AbstractDelegatedExecutionApplicationContext.java:358)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.osgi.util.internal.PrivilegedUtils.executeWithCustomTCCL(PrivilegedUtils.java:85)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.osgi.context.support.AbstractDelegatedExecutionApplicationContext.completeRefresh(AbstractDelegatedExecutionApplicationContext.java:320)[71:org.springframework.osgi.core:1.2.0]
	at org.springframework.osgi.extender.internal.dependencies.startup.DependencyWaiterApplicationContextExecutor$CompleteRefreshTask.run(DependencyWaiterApplicationContextExecutor.java:136)[72:org.springframework.osgi.extender:1.2.0]
	at java.lang.Thread.run(Thread.java:619)[:1.6.0_14]
Caused by: java.lang.NullPointerException
	at org.jivesoftware.smackx.muc.MultiUserChat$1.connectionCreated(MultiUserChat.java:84)[247:org.apache.servicemix.bundles.smack:3.1.0.2]
	at org.jivesoftware.smack.XMPPConnection.initConnection(XMPPConnection.java:957)[247:org.apache.servicemix.bundles.smack:3.1.0.2]
	at org.jivesoftware.smack.XMPPConnection.connectUsingConfiguration(XMPPConnection.java:904)[247:org.apache.servicemix.bundles.smack:3.1.0.2]
	at org.jivesoftware.smack.XMPPConnection.connect(XMPPConnection.java:1415)[247:org.apache.servicemix.bundles.smack:3.1.0.2]
	at org.apache.camel.component.xmpp.XmppEndpoint.createConnection(XmppEndpoint.java:140)[248:org.apache.camel.camel-xmpp:2.6.0.fuse-01-09]
	at org.apache.camel.component.xmpp.XmppGroupChatProducer.doStart(XmppGroupChatProducer.java:76)[248:org.apache.camel.camel-xmpp:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:65)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:52)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:60)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.ProducerCache.doGetProducer(ProducerCache.java:360)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	... 65 more
{code}",davsclaus,davsclaus,Minor,Closed,Fixed,16/Apr/11 12:45,25/Oct/11 11:35
Bug,CAMEL-3877,12504473,AdviceWith - Spring XML using a context scoped error handler causes adviceWith to not work,"See nabble
http://camel.465427.n5.nabble.com/Error-Handling-with-Spring-XML-tp4296920p4296920.html

I have reproduced the issue. Its only when you use a context scoped error handler. It works in all other situations. And only for XML DSL.",davsclaus,davsclaus,Minor,Closed,Fixed,17/Apr/11 08:44,25/Oct/11 11:35
Bug,CAMEL-3878,12504482,Stopping a route should not stop context scoped error handler,"When stopping a route using .stopRoute from CamelContext or JMX etc. then the error handler should not be stopped if its a context scoped error handler, as it would be re-used.

We should defer stopping those resources till Camel is shutting down.",davsclaus,davsclaus,Major,Closed,Fixed,17/Apr/11 11:19,02/May/13 02:29
Bug,CAMEL-3881,12504714,camel-spring doesn't import into eclipse,"
camel-spring no longer imports into eclipse as OsgiSpringCamelContext references OsgiTypeConverter which implements ServiceTrackerCustomizer.   The org.osgi.compendium jar isn't a dependency of camel-spring so the class cannot be found and eclipse gives an error.

Simple fix is to add a dependency to the pom:

{code:xml}
    <dependency>
      <groupId>org.osgi</groupId>
      <artifactId>org.osgi.compendium</artifactId>
      <scope>provided</scope>
      <optional>true</optional>
    </dependency>
{code}



",hadrian,dkulp,Major,Resolved,Fixed,19/Apr/11 21:04,20/Apr/11 03:04
Bug,CAMEL-3885,12504789,Move the org.apache.camel.language.SpEL.java into camel-core to avoid export package org.apache.camel.language twice in camel-spring,"camel-spring and camel-core both export the org.apache.camel.language package which will cause some truble when OSGi resolve the bundle which imports this package. 
As the SpEL.java is not import any spring related class, an simple fixing is moving the SpEL.java into camel-core.",njiang,njiang,Major,Resolved,Fixed,20/Apr/11 13:43,20/Apr/11 14:11
Bug,CAMEL-3890,12504938,Camel Syslog Converter Not Selected Automatically,"When creating a route that sends text data from a file to the Camel Syslog unmarshal data format, the following error is returned.
Looking at the org.apache.camel.component.syslog.Rfc3164SyslogConverter reveals that the @Converter annotation is missing at the class level.
The attached test case illustrates the problem. With modifications to the pom.xml to point to the modified version of the camel-syslog the test works. See the comment: CHANGE TO POINT TO MODIFIED CAMEL-SYSLOG

org.apache.camel.NoTypeConversionAvailableException: No type converter available to convert from type: java.lang.String to the required type: org.apache.camel.component.syslog.SyslogMessage with value <165>Aug  4 05:34:00 mymachine myproc[10]: %% It's\n         time to make the do-nuts.  %%  Ingredients: Mix=OK, Jelly=OK #\n         Devices: Mixer=OK, Jelly_Injector=OK, Frier=OK # Transport:\n         Conveyer1=OK, Conveyer2=OK # %%

	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.mandatoryConvertTo(BaseTypeConverterRegistry.java:143)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.util.ExchangeHelper.convertToMandatoryType(ExchangeHelper.java:142)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.component.syslog.Rfc3164SyslogDataFormat.marshal(Rfc3164SyslogDataFormat.java:29)[camel-syslog-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.MarshalProcessor.process(MarshalProcessor.java:57)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.impl.converter.AsyncProcessorTypeConverter$ProcessorToAsyncProcessorBridge.process(AsyncProcessorTypeConverter.java:50)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:299)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:269)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:125)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:80)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:109)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.component.file.GenericFileConsumer.processExchange(GenericFileConsumer.java:330)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.component.file.GenericFileConsumer.processBatch(GenericFileConsumer.java:157)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.component.file.GenericFileConsumer.poll(GenericFileConsumer.java:121)[camel-core-2.7.1.jar:2.7.1]
	at org.apache.camel.impl.ScheduledPollConsumer.run(ScheduledPollConsumer.java:97)[camel-core-2.7.1.jar:2.7.1]
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)[:1.6.0_24]
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)[:1.6.0_24]
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)[:1.6.0_24]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)[:1.6.0_24]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)[:1.6.0_24]
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)[:1.6.0_24]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)[:1.6.0_24]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)[:1.6.0_24]
	at java.lang.Thread.run(Thread.java:680)[:1.6.0_24]
",hadrian,rrojas-chariot,Major,Closed,Fixed,22/Apr/11 00:54,22/Nov/11 09:57
Bug,CAMEL-3902,12505181,Setting fault message after doing JMS request/reply fail propagating the fault flag,"If you do something like

from X
  inOut JMS
  setFaultBody

Then the fault flag is not propagated back when the consumer on X sees the result.
The issue is in JmsMessage which overrides copyFrom, from DefaultMessage. And it dont propagate the fault flag.",davsclaus,davsclaus,Major,Closed,Fixed,26/Apr/11 11:14,25/Oct/11 11:36
Bug,CAMEL-3909,12505340,Configuring any Byte type header via Spring XML constant would throw java.lang.ClassCastException,"Configuring any Byte type header via Spring XML constant would throw java.lang.ClassCastException

{code:xml}
<camel:setHeader headerName=""CamelSmppDestAddrTon"">
  <camel:constant>0</camel:constant>
</camel:setHeader>
{code}

java.lang.ClassCastException: java.lang.String cannot be cast to java.lang.Byte
	at org.apache.camel.component.smpp.SmppBinding.createSubmitSm(SmppBinding.java:105)
	at org.apache.camel.component.smpp.SmppProducer.process(SmppProducer.java:128)
	at org.apache.camel.impl.converter.AsyncProcessorTypeConverter$ProcessorToAsyncProcessorBridge.process(AsyncProcessorTypeConverter.java:50)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)
	at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:104)
	at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:272)
	at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:98)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:299)
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208)
	at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:269)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:125)
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:80)
	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:109)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:103)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:85)
	at org.apache.camel.component.mina.MinaConsumer$ReceiveHandler.messageReceived(MinaConsumer.java:117)
	at org.apache.mina.common.support.AbstractIoFilterChain$TailFilter.messageReceived(AbstractIoFilterChain.java:570)
	at org.apache.mina.common.support.AbstractIoFilterChain.callNextMessageReceived(AbstractIoFilterChain.java:299)
	at org.apache.mina.common.support.AbstractIoFilterChain.access$1100(AbstractIoFilterChain.java:53)
	at org.apache.mina.common.support.AbstractIoFilterChain$EntryImpl$1.messageReceived(AbstractIoFilterChain.java:648)
	at org.apache.mina.filter.executor.ExecutorFilter.processEvent(ExecutorFilter.java:220)
	at org.apache.mina.filter.executor.ExecutorFilter$ProcessEventsRunnable.run(ExecutorFilter.java:264)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:680)",davsclaus,acodapella,Major,Closed,Fixed,27/Apr/11 18:49,25/Oct/11 11:36
Bug,CAMEL-3912,12505748,Consumer URI parameters ignored after Route Restart,"The URI parameters for a consumer appear to be ignored or disappear when the route where this consumer defined gets restarted.
One example could be JPA query passed in the URI and is used to filter the results from the consumer.
The attached test case shows this.
Look at the Readme.txt, camel-context.xml, and JPARouteRestartTest class for more information.

Also, took a stab at fixing the problem and provided a patch with a possible solution. I ran the unit tests after that change and all passed. :)",davsclaus,rrojas-chariot,Major,Closed,Fixed,29/Apr/11 03:39,25/Oct/11 11:35
Bug,CAMEL-3913,12505767,jms listener can cause class cast exception if message body type changed during routing,"See nabble
http://camel.465427.n5.nabble.com/error-handling-on-Http-POST-from-ActiveMQ-to-PHP-application-td4347185.html

",davsclaus,davsclaus,Major,Closed,Fixed,29/Apr/11 08:59,25/Oct/11 11:36
Bug,CAMEL-3915,12505798,Archtype components don't import into eclipse without errors,"

The archetypes are importing into eclipse as ""java"" projects which is trying to invoke the java compiler on them. That's generating issues with the templates and such.  They should have the ""javanature"" turned off on them so they are pretty much just plain projects.",hadrian,dkulp,Major,Resolved,Fixed,29/Apr/11 15:17,29/Apr/11 15:57
Bug,CAMEL-3917,12505801,CxfPayloadConverter may start failing to convert a CxfPayload to a Node,"There is an issue with CxfPayloadConverter that can lead to a failure in converting the CxfPayload into something which is actually convertible. 

This problem happens when you try to convert a CxfPayload instance that has an empty body list into a Node instance. This returns null and subsequently registers this type mapping key (CxfPayload->Node) in org.apache.camel.impl.converter.BaseTypeConverterRegistry's conversion-misses table. Because of this, a subsequent conversion using a CxfPayload instance with an non empty body list will permanently fail.

I am attaching a modified CxfPayloadConversionTest that includes testCxfPayloadToNode that illustrates this problem. Currently, the last assertion of this test is failing when this issue is present.

I think we can fix this problem by returning Void.TYPE instead of a null for this case.
In this way, the conversion is regarded as a success and there will be no entry in the conversion-misses table.

So, I am also attaching the modiefied CxfPayloadConverter class that can handle this test case.

",davsclaus,ay,Major,Closed,Fixed,29/Apr/11 15:45,25/Oct/11 11:36
Bug,CAMEL-3920,12505829,camel-dns creates a new exchange and drops headers,"From irc:

17:33  sproingie> i have a weird problem with the dns component... it seems to not copy headers properly
17:34  sproingie> .setHeader(""foo"", constant(""bar"")).to(""dns:ip"").log(LogLevel.INFO, ""foo=${in.header.foo}"")
17:34  sproingie> foo turns up blank


",joed,joed,Minor,Closed,Fixed,30/Apr/11 00:31,25/Oct/11 11:35
Bug,CAMEL-3925,12505916,Example 'camel-example-management': a lot of exceptions during shutdown,"I run example using
mvn camel:run
And stop the example with ctrl + c
Console output attached.",davsclaus,amarkevich,Minor,Closed,Fixed,02/May/11 10:56,25/Oct/11 11:35
Bug,CAMEL-3927,12505919,CXF's Provider service using the payload data-format may not transfer the payload correctly,"When using the Provider based service (i.e., one using the Provider<Source>'s invoke(Source m) signature), I observed that the payload content is not correctly extracted in the first call. The reason seems to be that the content is passed as a StaxSource instance to the camel's cxf binding component (DefaultCxfBinding) and this expects currently a DOMSource for the payload data format and therefore it not able to extract the payload content.

Some more information is described in a related mail thread at camel-user 
http://camel.465427.n5.nabble.com/Question-on-RemoveClassTypeInterceptor-in-Camel-Cxf-s-Payload-mode-td4358188.html

This problem can be reproduced using CxfConsumerProviderTest when this test is executed using the payload data format. I suppose this test should actually be executed in the payload data format mode. But this is currently not the case.

So, I am attaching the modified CxfConsumerProviderTest that uses the payload data format. This test will fail with the current trunk camel-cxf implementation. I am also attatching the patch file of the suggested change in DefaultCxfBinding that can resolve this issue and successfully pass this test.

I think we can eventually consider a different CxfPayload implementation that does not require a DOM object at all. But for now, I think we can live with this correction to avoid this payload extraction issue.

Thanks.
Regards, Aki


",wtam,ay,Major,Closed,Fixed,02/May/11 11:40,25/Oct/11 11:35
Bug,CAMEL-3928,12505924,custom interceptor - wrapProcessorInInterceptors - passing parent Description on Split,"In routes with a split step, when my wrapProcessorInInterceptors method is called, the  ProcessorDefinition<?> paremeter is referencing the Split definition instead of steps inside it.

For example, for the route:

from(""direct:a"").routeId(""route:a"")
        .split(xpath(""//a/b"")).id(""task:split.products"")
            .log(""*** splited ::: body : ${body} "").id(""task:handle.that"") (*)
            .setHeader(""insplit"",constant(""in-split"")) (*)
        .end()
        .log(""* aftersplit ::: body : ${body}"").id(""task:after.split"");

for steps inside the split (*) the ProcessorDefinition is referencing the splitdefinition, while i think it should point to the log and setHeader steps.

It can be easily fixed,  in class:::

org.apache.camel.processor.DefaultChannel

in the method initChannel, changing

target = strategy.wrapProcessorInInterceptors(routeContext.getCamelContext(), outputDefinition, target, next);

by this:

target = strategy.wrapProcessorInInterceptors(routeContext.getCamelContext(), traceDef, target, next);

",davsclaus,javier.arilos@gmail.com,Major,Closed,Fixed,02/May/11 12:55,25/Oct/11 11:36
Bug,CAMEL-3934,12506023,Camel XSLT does not work when used with camel-blueprint,"camel-xslt cannot found the xslt document when used with camel-blueprint, do to FileNotFoundException.
Apparently, the component uses spring resource abstraction, which is not applicable when used with camel-blueprint.",davsclaus,iocanel,Major,Resolved,Fixed,03/May/11 05:41,02/May/13 02:29
Bug,CAMEL-3936,12506078,JdbcAggregationRepository count (*) is not working with MySQL 5.0,"The JdbcAggregationRepository makes a {noformat} 'SELECT COUNT (*) FROM....'{noformat}  SQL request at line 131 in the source code. 

This appears to be incorrect with MySQL because theire is a space between COUNT and (. 

By replacing the request with {noformat} 'SELECT COUNT(*) FROM....'{noformat}  (without the white space) the bug is corrected for MySQL (I think it should be tested with other database without the space). 

This request can be tested with Mysql Query Browser (the returned error is the same as the one encountered when we execute the JdbcAggregationRepository with Camel). 

The exception returned is the following: 
{noformat} ERROR DefaultErrorHandler - Failed delivery for exchangeId: ID:COE-WRKST2-1749-1304443569312-2:2:1:88:1. Exhausted after delivery attempt: 1 caught: org.springframework.jdbc.BadSqlGrammarException: PreparedStatementCallback; bad SQL grammar [SELECT COUNT (*) FROM aggregation_repo_0 WHERE id = ?]; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '*) FROM aggregation_repo_0 WHERE id = '92'' at line 1
org.springframework.jdbc.BadSqlGrammarException: PreparedStatementCallback; bad SQL grammar [SELECT COUNT (*) FROM aggregation_repo_0 WHERE id = ?]; nested exception is com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '*) FROM aggregation_repo_0 WHERE id = '92'' at line 1
	at org.springframework.jdbc.support.SQLErrorCodeSQLExceptionTranslator.doTranslate(SQLErrorCodeSQLExceptionTranslator.java:233)
	at org.springframework.jdbc.support.AbstractFallbackSQLExceptionTranslator.translate(AbstractFallbackSQLExceptionTranslator.java:72)
	at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:602)
	at org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:636)
	at org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:665)
	at org.springframework.jdbc.core.JdbcTemplate.query(JdbcTemplate.java:673)
	at org.springframework.jdbc.core.JdbcTemplate.queryForObject(JdbcTemplate.java:728)
	at org.springframework.jdbc.core.JdbcTemplate.queryForObject(JdbcTemplate.java:744)
	at org.springframework.jdbc.core.JdbcTemplate.queryForInt(JdbcTemplate.java:775)
	at org.apache.camel.processor.aggregate.jdbc.JdbcAggregationRepository$1.doInTransaction(JdbcAggregationRepository.java:131)
	at org.apache.camel.processor.aggregate.jdbc.JdbcAggregationRepository$1.doInTransaction(JdbcAggregationRepository.java:114)
	at org.springframework.transaction.support.TransactionTemplate.execute(TransactionTemplate.java:130)
	at org.apache.camel.processor.aggregate.jdbc.JdbcAggregationRepository.add(JdbcAggregationRepository.java:114)
	at org.apache.camel.processor.aggregate.AggregateProcessor.doAggregation(AggregateProcessor.java:251)
	at org.apache.camel.processor.aggregate.AggregateProcessor.process(AggregateProcessor.java:189)
	at org.apache.camel.impl.converter.AsyncProcessorTypeConverter$ProcessorToAsyncProcessorBridge.process(AsyncProcessorTypeConverter.java:50)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:299)
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208)
	at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:269)
	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:109)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:103)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:85)
	at org.apache.camel.component.jms.EndpointMessageListener.onMessage(EndpointMessageListener.java:84)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doInvokeListener(AbstractMessageListenerContainer.java:560)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.invokeListener(AbstractMessageListenerContainer.java:498)
	at org.springframework.jms.listener.AbstractMessageListenerContainer.doExecuteListener(AbstractMessageListenerContainer.java:467)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.doReceiveAndExecute(AbstractPollingMessageListenerContainer.java:325)
	at org.springframework.jms.listener.AbstractPollingMessageListenerContainer.receiveAndExecute(AbstractPollingMessageListenerContainer.java:263)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.invokeListener(DefaultMessageListenerContainer.java:1058)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.executeOngoingLoop(DefaultMessageListenerContainer.java:1050)
	at org.springframework.jms.listener.DefaultMessageListenerContainer$AsyncMessageListenerInvoker.run(DefaultMessageListenerContainer.java:947)
	at java.lang.Thread.run(Thread.java:662)
Caused by: com.mysql.jdbc.exceptions.jdbc4.MySQLSyntaxErrorException: You have an error in your SQL syntax; check the manual that corresponds to your MySQL server version for the right syntax to use near '*) FROM aggregation_repo_0 WHERE id = '92'' at line 1
	at sun.reflect.GeneratedConstructorAccessor83.newInstance(Unknown Source)
	at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27)
	at java.lang.reflect.Constructor.newInstance(Constructor.java:513)
	at com.mysql.jdbc.Util.handleNewInstance(Util.java:409)
	at com.mysql.jdbc.Util.getInstance(Util.java:384)
	at com.mysql.jdbc.SQLError.createSQLException(SQLError.java:1054)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3566)
	at com.mysql.jdbc.MysqlIO.checkErrorPacket(MysqlIO.java:3498)
	at com.mysql.jdbc.MysqlIO.sendCommand(MysqlIO.java:1959)
	at com.mysql.jdbc.MysqlIO.sqlQueryDirect(MysqlIO.java:2113)
	at com.mysql.jdbc.ConnectionImpl.execSQL(ConnectionImpl.java:2568)
	at com.mysql.jdbc.PreparedStatement.executeInternal(PreparedStatement.java:2113)
	at com.mysql.jdbc.PreparedStatement.executeQuery(PreparedStatement.java:2275)
	at org.apache.commons.dbcp.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:96)
	at org.apache.commons.dbcp.DelegatingPreparedStatement.executeQuery(DelegatingPreparedStatement.java:96)
	at org.springframework.jdbc.core.JdbcTemplate$1.doInPreparedStatement(JdbcTemplate.java:643)
	at org.springframework.jdbc.core.JdbcTemplate.execute(JdbcTemplate.java:586)
	... 42 more{noformat} ",muellerc,bouba,Major,Closed,Fixed,03/May/11 17:38,03/May/11 20:49
Bug,CAMEL-3942,12506305,AnnotationTypeConverterLoader fails for folders containing spaces,"This problem appears when jar dependencies with type converters have path containing space characters.

In my case, the camel-mail-2.7.1.jar jar is located in my local maven repository.

I get this error during my project startup: 
{noformat}
[...]
Caused by: java.net.URISyntaxException: Illegal character in opaque part at index 22: jar:file:/D:/Documents and Settings/501915291/.m2/repository/org/apache/camel/camel-mail/2.7.1/camel-mail-2.7.1.jar!/META-INF/services/org/apache/camel/TypeConverter
        at java.net.URI$Parser.fail(URI.java:2809)
        at java.net.URI$Parser.checkChars(URI.java:2982)
        at java.net.URI$Parser.parse(URI.java:3019)
        at java.net.URI.<init>(URI.java:578)
        at java.net.URL.toURI(URL.java:918)
        at org.apache.camel.impl.converter.AnnotationTypeConverterLoader.findPackages(AnnotationTypeConverterLoader.java:118)
        at org.apache.camel.impl.converter.AnnotationTypeConverterLoader.findPackageNames(AnnotationTypeConverterLoader.java:108)
        at org.apache.camel.impl.converter.AnnotationTypeConverterLoader.load(AnnotationTypeConverterLoader.java:69)
        ... 63 more
{noformat}

FIX:
findPackages method in AnnotationTypeConverterLoader shouldn't use URIs to store already visited jars.",davsclaus,jcdelmas,Minor,Closed,Fixed,05/May/11 16:10,25/Oct/11 11:36
Bug,CAMEL-3948,12506357,Issue with the RSET command with POP3 servers,"Issue is discussed in this [thread|http://camel.465427.n5.nabble.com/POP3-mail-deletion-td4370158.html].

Note: Donald posted interesting and helpful findings.",davsclaus,hadrian,Major,Closed,Fixed,06/May/11 01:51,25/Oct/11 11:36
Bug,CAMEL-3950,12506417,"Route cannot be started within web-console, if autoStartup=""false"" is set in camel.xml","When a route has autoStartup=""false"" set in the camel.xml, it cannot be started anymore in the web- console.
In the logfile, an entry is generated, similar to this:
""Cannot start route route2 as its configured with autoStartup=false | org.apache.camel.spring.SpringCamelContext | qtp7446303-52""
If a route does not have autoStartup=""false"", it can be stopped and started normally (as before with ActiveMQ 5.4.2 / Camel 2.4.0)

Reproduce steps:
- Configure a route with ""<route autoStartup=""false"">""
- Start the ActiveMQ Service
- Navigate to ""http://server.domain.tld:8161/camel/routes
- try to start the route with the button ""Start""
- check the logfile ""activemq.log""
",davsclaus,jimmyrueedi,Minor,Closed,Fixed,06/May/11 14:46,25/Oct/11 11:35
Bug,CAMEL-3959,12506644,SpringTestSupport - debugBefore does not work,"You cannot use the debugBefore / debugAfter when using CamelSpringTestSupport from camel-test. The problem is that the Spring Framework triggers Camel to start before the unit test have setup the debugger and whatnot.

We should align this so it works like without spring, so its the CamelTestSupport that invokes start on CamelContext

See nabble
http://camel.465427.n5.nabble.com/CamelSpringTestSupport-and-debugBefore-tp4382259p4382259.html",davsclaus,davsclaus,Major,Resolved,Fixed,09/May/11 18:21,09/May/11 18:45
Bug,CAMEL-3965,12506935,ftp producer - If sending noop fails it should force re-connection attempt,"See nabble
http://camel.465427.n5.nabble.com/Limitations-or-bug-on-FTP2-with-FTPS-tp4372423p4372423.html

The RemoteFileProducer should force check if the connection is alive on the client, if the noop fails.",davsclaus,davsclaus,Major,Closed,Fixed,12/May/11 05:35,25/Oct/11 11:35
Bug,CAMEL-3967,12507016,Inject custom package scan class resolver asap to ensure loading of classpath works,"See nabble
http://camel.465427.n5.nabble.com/TypeConverter-loading-failes-in-JBoss-5-1-0-GA-tp4389726p4389726.html

The issue is when using JBoss and Camel 2.7

",davsclaus,davsclaus,Minor,Resolved,Fixed,12/May/11 16:59,12/May/11 17:10
Bug,CAMEL-3971,12507106,Dumping route to XML created by Java DSL using an expression may not output the actual used expression,"See this unit test
ManagedCamelContextDumpRoutesAsXmlTest

The route in Java DSL
{code}
                from(""seda:bar"").routeId(""myOtherRoute"")
                    .filter().header(""bar"")
                        .to(""mock:bar"")
                    .end();
{code}

And the output in XML
{code:xml}
  <route group=""org.apache.camel.management.ManagedCamelContextDumpRoutesAsXmlTest$1"" id=""myOtherRoute"">
        <from uri=""seda:bar""/>
        <filter id=""filter1"">
            <expressionDefinition/>
            <to uri=""mock:bar"" id=""to2""/>
        </filter>
    </route>
{code}

The same example created by Spring XML, outputs the XML correct


",davsclaus,davsclaus,Major,Resolved,Fixed,13/May/11 10:18,18/May/11 09:41
Bug,CAMEL-3974,12507187,Spring-WS producer doesn't propagate headers,"The Spring-WS producer only sets the body of the exchange, headers aren't propagated.

See Nabble: http://camel.465427.n5.nabble.com/spring-ws-component-not-propagating-in-headers-to-out-message-td4393731.html",rkettelerij,rkettelerij,Major,Closed,Fixed,13/May/11 21:53,25/Oct/11 11:35
Bug,CAMEL-3979,12507462,camel-jms - Potential ClassCastException if JmsMessage isn't set anymore,"See nabble
http://camel.465427.n5.nabble.com/Correct-way-to-handle-transactions-acroos-multiple-routes-joined-with-ActiveMQ-tp4402998p4402998.html",davsclaus,davsclaus,Minor,Closed,Fixed,17/May/11 08:58,25/Oct/11 11:35
Bug,CAMEL-3985,12507541,Name Part of the Sender & the Recipient Email Addresses are not encoded with the Exchange charset,"The name part of an email address can have any character, and it needs to be encoded with the correct charset provided for the exchange. ",davsclaus,karthz,Minor,Resolved,Fixed,17/May/11 23:09,29/Feb/12 14:15
Bug,CAMEL-3989,12507623,onException definition ignored when using route/@errorHandlerRef with the xml dsl,"When using a route definition like below:

{code}
<camelContext xmlns=""http://camel.apache.org/schema/spring"">
  <onException>
    <exception>org.apache.camel.MyException</exception>
    <handled><constant>false</constant></handled>
  </onException>
  <route errorHandlerRef=""errorHandler"">
    <from uri=""direct:start""/>
[...]
{code}

the onException definition is not propagated to the ref'd DLC and hence it will be ignored.",davsclaus,hadrian,Major,Resolved,Fixed,18/May/11 14:50,10/Jun/11 13:57
Bug,CAMEL-3991,12507699,Bad href links generated for certain endpoint uris in camel-web,"When new Endpoints are created via camel-web, some endpoint uris result in bad href links generated for the endpoint page.",hadrian,hadrian,Major,Closed,Fixed,19/May/11 01:26,25/Oct/11 11:36
Bug,CAMEL-4010,12508205,Camel FTP component fails to process multiple files at once from a specified folder,"Camel FTP component fails to process multiple files at once from a specified folder (let's assume the folder is /in).
When the consumer connects to a ftp account and finds a list of, let's say, 3 files, it will try to retrieve them in parallel but this is not handled properly.

As you can see from the below FTP server log, this is what Camel FTP does:
- it changes the current directory to /in
- retrieves the file
- it changes the current directory to /
- the second (probably) thread changes the directory to /in
- retreives the second file
- the first (probably) thread sends a delete command but it sends it as if it were in the root folder but it is actually in the /in folder
- the FTP gives an error that the file doesn't exist
- somehow the FTP Component blocks itself on the first file...

See my comments in the following log starting with *****

(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 257 ""/"" is current directory.
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> CWD in
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 250 CWD successful. ""/in"" is current directory.
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> PASV
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 227 Entering Passive Mode (192,168,0,25,228,206)
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> RETR WS_1001_1.xml
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 150 Connection accepted
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> SSL connection for data connection established
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 226 Transfer OK
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> CWD /
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 250 CWD successful. ""/"" is current directory.
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> PWD
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 257 ""/"" is current directory.
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> CWD in
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 250 CWD successful. ""/in"" is current directory.
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> PASV
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 227 Entering Passive Mode (192,168,0,25,228,207)
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> RETR WS_1001_2.xml
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 150 Connection accepted
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> DELE in/WS_1001_1.xml    ***** The location is already /in, should be DELE WS_1001_1.xml
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 550 File not found
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> PASV
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 227 Entering Passive Mode (192,168,0,25,228,208)
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> NLST in
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 550 Directory not found
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> PWD
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 257 ""/in"" is current directory.
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> CWD in            ***** Current directory is already /in
(002386)5/20/2011 17:07:03 PM - sftp-test (192.168.0.194)> 550 CWD failed. ""/in/in"": directory not found.

For the whole FTP server logs go here: http://pastie.org/private/kknnkxpbyzqmai7gsm8z5g
For the Servicemix/Camel(Trace) log: http://pastie.org/private/eulookxiepan3nkjyu41a

A workaround that seems to work is to use the maxMessagesPerPoll=1 option.",davsclaus,tricasoft,Major,Resolved,Fixed,24/May/11 15:16,06/Nov/11 11:57
Bug,CAMEL-4011,12508222,type converters should return NULL for Double.NaN values instead of 0,"see this discussion...http://camel.465427.n5.nabble.com/XPath-for-an-Integer-td4422095.html

Update the ObjectConverter.toXXX() methods to check for Double.NaN and return NULL instead of relying on Number.intValue()
",boday,boday,Trivial,Closed,Fixed,24/May/11 17:41,25/Oct/11 11:36
Bug,CAMEL-4018,12508452,Incorrect annotation parameter in examples,"In the ""Parameter Binding Annotations"" page at http://camel.apache.org/parameter-binding-annotations.html, a number of the examples include lines like;

{{public void doSomething(@Header(name = ""JMSCorrelationID"") String correlationID, @Body String body) {}}

The correct annotation parameter for @Header is actually ""value"", rather than ""name"".",njiang,stu.c,Minor,Resolved,Fixed,26/May/11 16:41,27/May/11 01:15
Bug,CAMEL-4021,12508528,ConcurrentModificationException at DefaultCamelContext.getRouteDefinition(),"Similar to CAMEL-3493 there is also a race in getRouteDefinition()
{noformat}
java.util.ConcurrentModificationException
        at java.util.AbstractList$Itr.checkForComodification(AbstractList.java:372)[:1.6.0_21]
        at java.util.AbstractList$Itr.next(AbstractList.java:343)[:1.6.0_21]
        at org.apache.camel.impl.DefaultCamelContext.getRouteDefinition(DefaultCamelContext.java:1098)[cih-core-1.0.jar:1.0]
        at com.tieto.cih.core.main.GatewayRouteSetup.startGatewayRoutes(GatewayRouteSetup.java:86)[cih-core-1.0.jar:1.0]
        at com.tieto.cih.core.main.GatewayRouteSetup.process(GatewayRouteSetup.java:73)[cih-core-1.0.jar:1.0]
{noformat}
As route count could be quite large, it would be nice to have them stored in synchronized map or something like that instead of list.",davsclaus,arkadi,Minor,Closed,Fixed,27/May/11 10:26,25/Oct/11 11:35
Bug,CAMEL-4022,12508547,Issue using errorBuilderRef with the xml dsl,"While fixing issues around the errorHandler I noticed that <onException> definitions defined in the camel context are ignored if a route specifies its own errorHandlerRef. The reason is that we set the onException definition on the default error handler. I have a fix for that, but I discovered a different issue (I think) for which I would like to discuss the solution.

When we have an onException definition that looks kinda like this:
{code}
<onException>
  <exception> java.lang.IllegalArgumentException</exception>
  <to uri=""mock:illegalArgumentException""/>
</onException>
{code}
... something happens, the IAE exception is caught, we do something, but in that process another exception is thrown. Currently, that would be caught by the default error handler, which may not be what we want.

What error handler (if any) should handle exceptions thrown while in onException?

The onException mechanism is somewhat similar to a try/catch. I don't think the exceptions thrown while handling onException should be handled by the same error handler configured for the route, or even the context scoped one. The processing should be very simple, predictable and immutable. Since the default ""CamelDefaultErrorHandlerBuilder"" can be replaced, it is not imho a solution and we need one global one that does as little as possible (the problem would be agreeing what that is: no redeliveries, logging or not, etc).

Thoughts?




",davsclaus,hadrian,Critical,Resolved,Fixed,27/May/11 13:27,10/Jun/11 08:42
Bug,CAMEL-4025,12508603,XPath namespaces not present when <xpath> element inside a <routeContext> element,"I have a route that is inside a {{<routeContext>}} element which uses XPath as part of a filter and a choice.  The input document has a default namespace that is not blank.  When I try to run a message through the route, I see the following error in the logs:

{code:none}
com.sun.org.apache.xpath.internal.domapi.XPathStylesheetDOM3Exception: Prefix must resolve to a namespace: foo
{code}

Adding the namespace declaration to the {{beans}} element as is done in the unit tests for camel-spring doesn't fix the problem, and neither does adding it to the {{<camelContext>}} element as described in the documentation.  I copied the {{SpringXPathFilterWithNamespaceTest}} class and created an input route configuration that uses a {{routeContext}}, and was able to reproduce the issue.",davsclaus,pegli,Major,Resolved,Fixed,27/May/11 22:49,29/May/11 13:49
Bug,CAMEL-4026,12508608,"Camel does not work out of the box in Java webstart, due classpath scanning is not possible","This is a regression.  Worked fine for us in Camel 2.5.0, fails in Camel 2.7.1.  

Easy to reproduce.  Run the following code in a webstart-deployed application:

CamelContext context = new DefaultCamelContext();
context.getTypeConverterRegistry(); 

Which produces the following exception:

2011-05-26 16:52:57,550 [javawsApplicationMain] ERROR test.TestCamelContext - Failed
org.apache.camel.RuntimeCamelException: org.apache.camel.TypeConverterLoaderException: Failed to load type converters because of: Cannot find any type converter classes from the following packages: [org.apache.camel.component.file, org.apache.camel.component.bean, org.apache.camel.converter]
    at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1139)
    at org.apache.camel.impl.DefaultCamelContext.getTypeConverter(DefaultCamelContext.java:986)
    at org.apache.camel.impl.DefaultCamelContext.getTypeConverterRegistry(DefaultCamelContext.java:1001)
    at test.TestCamelContext.test(TestCamelContext.java:54)
    at test.TestCamelContext.main(TestCamelContext.java:48)
    at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
    at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
    at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
    at java.lang.reflect.Method.invoke(Unknown Source)
    at com.sun.javaws.Launcher.executeApplication(Unknown Source)
    at com.sun.javaws.Launcher.executeMainClass(Unknown Source)
    at com.sun.javaws.Launcher.doLaunchApp(Unknown Source)
    at com.sun.javaws.Launcher.run(Unknown Source)
    at java.lang.Thread.run(Unknown Source)
Caused by: org.apache.camel.TypeConverterLoaderException: Failed to load type converters because of: Cannot find any type converter classes from the following packages: [org.apache.camel.component.file, org.apache.camel.component.bean, org.apache.camel.converter]
    at org.apache.camel.impl.converter.AnnotationTypeConverterLoader.load(AnnotationTypeConverterLoader.java:79)
    at org.apache.camel.impl.converter.BaseTypeConverterRegistry.loadTypeConverters(BaseTypeConverterRegistry.java:395)
    at org.apache.camel.impl.converter.DefaultTypeConverter.doStart(DefaultTypeConverter.java:41)
    at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:67)
    at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:54)
    at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:60)
    at org.apache.camel.impl.DefaultCamelContext.startServices(DefaultCamelContext.java:1613)
    at org.apache.camel.impl.DefaultCamelContext.addService(DefaultCamelContext.java:899)
    at org.apache.camel.impl.DefaultCamelContext.getTypeConverter(DefaultCamelContext.java:984)
    ... 12 more
",davsclaus,jn,Minor,Resolved,Fixed,28/May/11 01:16,23/Jul/11 11:07
Bug,CAMEL-4044,12509203,ChoiceProcessor : unable to find the mBeans processor,"Using the jconsole, we are not able to locate some processors after a ChoiceProcessor.

For example :

from(""direct:start"").to(""mock:test1"").id(""mock1"")
.choice()
.when(predicate).to(""mock:test2"").id(""test2"")
.otherwise().to(""mock:test3"").id(""test3"")
.end()
.to(""mock:outOfChoice"").id(""outOfChoice"");

The processors mock1 and outOfChoice are well found but not the processors test2 and test3.
",dkulp,rdubois,Major,Closed,Fixed,03/Jun/11 17:03,02/May/13 02:29
Bug,CAMEL-4047,12509215,javax.script dependency in camel-core,"
The OSGi imports for camel-core ends up with a non-optional dependency on javax.script due to it being used in the Activator.    However, the karaf feature for camel-core doesn't have a dependency on org.apache.servicemix.specs.scripting-api-1.0 .

One of two things should be done:
1) mark javax.script as optional.  This MAY require some updates to the Activator to work when it's not available.

2) Update the karaf features file to add:
{code:xml}
<bundle dependency=""true"">mvn:org.apache.servicemix.specs/org.apache.servicemix.specs.scripting-api-1.0/${servicemix-specs-version}</bundle>
{code}
",dkulp,dkulp,Major,Resolved,Fixed,03/Jun/11 17:56,08/Jun/11 03:05
Bug,CAMEL-4057,12509344,continued(true) doesn't work when is invoked for the second time in the Camel flow,"When Camel flow contains error handling as continued(true), it is not invoked when the error occurs for the second time (even in different onException).
What is more the exception is not handled even by global onException and Camel flow is interruped. ",boday,matihost,Major,Closed,Fixed,06/Jun/11 11:26,04/Apr/12 10:48
Bug,CAMEL-4062,12509416,Camel XMLSecurity encryption fails with a client-specified passPhrase,"Camel XMLSecurity produces an NPE when a client-provided passPhrase is included as a marshal().secureXml() parameter.

java.lang.NullPointerException
        at javax.crypto.spec.DESedeKeySpec.<init>(DashoA13*..)
        at javax.crypto.spec.DESedeKeySpec.<init>(DashoA13*..)
        at org.apache.camel.dataformat.xmlsecurity.XMLSecurityDataFormat.generateEncryptionKey(XMLSecurityDataFormat.java:176)
        at org.apache.camel.dataformat.xmlsecurity.XMLSecurityDataFormat.marshal(XMLSecurityDataFormat.java:94)
        at org.apache.camel.processor.MarshalProcessor.process(MarshalProcessor.java:57)
",hadrian,rnewcomb,Major,Resolved,Fixed,07/Jun/11 04:44,16/Jun/11 13:52
Bug,CAMEL-4063,12509417,XMLSecurity component does not decode element-level encrypted content,The camel-xmlsecurity component does not decode element-level encrypted data.,hadrian,rnewcomb,Major,Resolved,Fixed,07/Jun/11 04:52,16/Jun/11 13:53
Bug,CAMEL-4066,12509447,Test failure in bean validation BeanValidatorRouteTest ComparisonFailure: expected:<[may not be null]> but was:<[kann nicht null sein]>,"org.junit.ComparisonFailure: expected:<[may not be null]> but was:<[kann nicht null sein]>
	at org.junit.Assert.assertEquals(Assert.java:123)
	at org.junit.Assert.assertEquals(Assert.java:145)
	at org.apache.camel.component.bean.validator.BeanValidatorRouteTest.validateShouldFailWithImpliciteDefaultGroup(BeanValidatorRouteTest.java:89)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)

",cschneider,cschneider,Major,Closed,Fixed,07/Jun/11 11:49,07/Jun/11 11:52
Bug,CAMEL-4067,12509479,Test failures in camel-hdfs on windows as Streams are not closed correctly,"On windows some hdfs tests fail as the streams are not correctly being closed. 

The first stacktrace looks like below. This is a bit misleading as in fact the problem is that not all exchanges could be processed. The reason is that the file component can´t rename the file as it is still open. This is because an inputStream is opened for the file but never closed.

junit.framework.AssertionFailedError: expected:<40> but was:<12>
	at junit.framework.Assert.fail(Assert.java:47)
	at junit.framework.Assert.failNotEquals(Assert.java:283)
	at junit.framework.Assert.assertEquals(Assert.java:64)
	at junit.framework.Assert.assertEquals(Assert.java:195)
	at junit.framework.Assert.assertEquals(Assert.java:201)
	at org.apache.camel.component.hdfs.HdfsProducerFileWriteTest.testSimpleWriteFile(HdfsProducerFileWriteTest.java:72)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.junit.runners.model.FrameworkMethod$1.runReflectiveCall(FrameworkMethod.java:44)
	at org.junit.internal.runners.model.ReflectiveCallable.run(ReflectiveCallable.java:15)
	at org.junit.runners.model.FrameworkMethod.invokeExplosively(FrameworkMethod.java:41)
	at org.junit.internal.runners.statements.InvokeMethod.evaluate(InvokeMethod.java:20)
	at org.junit.internal.runners.statements.RunBefores.evaluate(RunBefores.java:28)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.rules.TestWatchman$1.evaluate(TestWatchman.java:48)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:76)
	at org.junit.runners.BlockJUnit4ClassRunner.runChild(BlockJUnit4ClassRunner.java:50)
	at org.junit.runners.ParentRunner$3.run(ParentRunner.java:193)
	at org.junit.runners.ParentRunner$1.schedule(ParentRunner.java:52)
	at org.junit.runners.ParentRunner.runChildren(ParentRunner.java:191)
	at org.junit.runners.ParentRunner.access$000(ParentRunner.java:42)
	at org.junit.runners.ParentRunner$2.evaluate(ParentRunner.java:184)
	at org.junit.internal.runners.statements.RunAfters.evaluate(RunAfters.java:31)
	at org.junit.runners.ParentRunner.run(ParentRunner.java:236)
	at org.eclipse.jdt.internal.junit4.runner.JUnit4TestReference.run(JUnit4TestReference.java:49)
	at org.eclipse.jdt.internal.junit.runner.TestExecution.run(TestExecution.java:38)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:467)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.runTests(RemoteTestRunner.java:683)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.run(RemoteTestRunner.java:390)
	at org.eclipse.jdt.internal.junit.runner.RemoteTestRunner.main(RemoteTestRunner.java:197)


",cschneider,cschneider,Major,Closed,Fixed,07/Jun/11 16:10,07/Jun/11 16:19
Bug,CAMEL-4068,12509489,Test failure in camel-spring-ws actual results come with windows line endings and the comparison fails,"Several tests like org.apache.camel.component.spring.ws.ConsumerEndpointMappingResponseHandlingRouteTest.testRootQName
fail with ComparisonFailure as the expected result has unix file endings and the actual result has windows file endings on windows",cschneider,cschneider,Major,Resolved,Fixed,07/Jun/11 17:32,07/Jun/11 17:34
Bug,CAMEL-4073,12509642,Misconfigured CXF Endpoint url results in NPE instead of actual exception.  ,"I'm new to Camel, trying to setup the JMS CXF service based on the [http://camel.apache.org/better-jms-transport-for-cxf-webservice-using-apache-camel.html], but with our own JMS services.  In the process I transposed some configuration and ended up with an NPE when CXF tried to start.

The URL passed to jaxws:endpoint/@address=""tcp://MyDomainServer1:2506,tcp://MyDomainServer1:2506"".  I know this wasn't what camel expected here, after debugging, but wanted to open a bug, since the actual meaningful exception is actually lost due to an NPE.

Expected Exception.  this was obtained with a debugger in CXF/camel before the NPE is thrown.
{code}
org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: MyDomainServer1://2506,tcp://MyDomainServer2:2506 due to: No component found with scheme: MyDomainServer1
	at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:457)
	at org.apache.camel.component.cxf.transport.CamelDestination.activate(CamelDestination.java:116)
	at org.apache.cxf.transport.AbstractObservable.setMessageObserver(AbstractObservable.java:48)
	at org.apache.cxf.binding.AbstractBindingFactory.addListener(AbstractBindingFactory.java:181)
	at org.apache.cxf.endpoint.ServerImpl.start(ServerImpl.java:127)
	at org.apache.cxf.jaxws.EndpointImpl.doPublish(EndpointImpl.java:334)
	at org.apache.cxf.jaxws.EndpointImpl.publish(EndpointImpl.java:239)
	at org.apache.cxf.jaxws.EndpointImpl.publish(EndpointImpl.java:489)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeCustomInitMethod(AbstractAutowireCapableBeanFactory.java:1544)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1485)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1417)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:519)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:291)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:288)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:190)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:580)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:895)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:425)
	at org.apache.cxf.bus.spring.BusApplicationContext.<init>(BusApplicationContext.java:91)
	at org.apache.cxf.bus.spring.SpringBusFactory.createApplicationContext(SpringBusFactory.java:102)
	at org.apache.cxf.bus.spring.SpringBusFactory.createBus(SpringBusFactory.java:93)
	at org.apache.cxf.bus.spring.SpringBusFactory.createBus(SpringBusFactory.java:86)
	at org.apache.cxf.bus.spring.SpringBusFactory.createBus(SpringBusFactory.java:64)
	at org.apache.cxf.bus.spring.SpringBusFactory.createBus(SpringBusFactory.java:53)
	at com.expedia.cc.container.remoting.prototype.Main.main(Main.java:37)

{code}

The Actual Exception was:
{code}
Exception in thread ""main"" java.lang.RuntimeException: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'JMS_prototype.Server': Invocation of init method failed; nested exception is javax.xml.ws.WebServiceException: java.lang.NullPointerException
	at org.apache.cxf.bus.spring.SpringBusFactory.createBus(SpringBusFactory.java:96)
	at org.apache.cxf.bus.spring.SpringBusFactory.createBus(SpringBusFactory.java:86)
	at org.apache.cxf.bus.spring.SpringBusFactory.createBus(SpringBusFactory.java:64)
	at org.apache.cxf.bus.spring.SpringBusFactory.createBus(SpringBusFactory.java:53)
	at com.expedia.cc.container.remoting.prototype.Main.main(Main.java:37)
Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'JMS_prototype.Server': Invocation of init method failed; nested exception is javax.xml.ws.WebServiceException: java.lang.NullPointerException
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1420)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:519)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:291)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:288)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:190)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:580)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:895)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:425)
	at org.apache.cxf.bus.spring.BusApplicationContext.<init>(BusApplicationContext.java:91)
	at org.apache.cxf.bus.spring.SpringBusFactory.createApplicationContext(SpringBusFactory.java:102)
	at org.apache.cxf.bus.spring.SpringBusFactory.createBus(SpringBusFactory.java:93)
	... 4 more
Caused by: javax.xml.ws.WebServiceException: java.lang.NullPointerException
	at org.apache.cxf.jaxws.EndpointImpl.doPublish(EndpointImpl.java:343)
	at org.apache.cxf.jaxws.EndpointImpl.publish(EndpointImpl.java:239)
	at org.apache.cxf.jaxws.EndpointImpl.publish(EndpointImpl.java:489)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
	at java.lang.reflect.Method.invoke(Method.java:597)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeCustomInitMethod(AbstractAutowireCapableBeanFactory.java:1544)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.invokeInitMethods(AbstractAutowireCapableBeanFactory.java:1485)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.initializeBean(AbstractAutowireCapableBeanFactory.java:1417)
	... 16 more
Caused by: java.lang.NullPointerException
	at org.apache.camel.FailedToCreateConsumerException.<init>(FailedToCreateConsumerException.java:31)
	at org.apache.camel.component.cxf.transport.CamelDestination.activate(CamelDestination.java:120)
	at org.apache.cxf.transport.AbstractObservable.setMessageObserver(AbstractObservable.java:48)
	at org.apache.cxf.binding.AbstractBindingFactory.addListener(AbstractBindingFactory.java:181)
	at org.apache.cxf.endpoint.ServerImpl.start(ServerImpl.java:127)
	at org.apache.cxf.jaxws.EndpointImpl.doPublish(EndpointImpl.java:334)
	... 25 more
{code}

Basically the problem is that the {{destinationEndpoint = getCamelContext().getEndpoint(camelDestinationUri);}} call fails, resulting the the FailedToCreateConsumerException first parameter to be null.  Since its internally expecting the endpoint to not be null, it NPE's.  Fix is most likely to make FailedToCreateConsumerException null safe.  Since it principly uses the endpoint to get the URI, you could make a constructor that took the URI that you were trying to configure, instead of relying on an already created endpoint.",dkulp,szetheli,Major,Closed,Fixed,08/Jun/11 21:06,25/Oct/11 11:36
Bug,CAMEL-4076,12509673,"FileIdempotentRepository fileStore does not get loaded on bootrapping, therefore files are processed again","I am using JVM based initialization of Camel using Spring.
I use the documented Camel initialization procedure of main.setApplicationContext(""my-spring-camel-config.xml"");
main.start()

I also use the documented FileIdempotentRepository that points to a FileStore.
However, when Camel initializes this filestore is not loaded up. Hence, the LRU cache contained within this is empty.
As a result, all my files are FTPed again from the remote server.

This happens everytime I start my JVM resulting in expensive FTP of files.",davsclaus,adkathuria@yahoo.com,Major,Closed,Fixed,09/Jun/11 07:49,25/Oct/11 11:35
Bug,CAMEL-4079,12509721,Bug in CxfConsumer.asyncInvoke that causes setResponseBack to be called twice,Bug in CxfConsumer.asyncInvoke that causes setResponseBack to be called twice.  The asyncInvoke should execute one of the conditions (continuation.isNew() or continuation.isResumed().,wtam,wtam,Major,Closed,Fixed,09/Jun/11 14:33,25/Oct/11 11:35
Bug,CAMEL-4083,12509836,MailBinding may drop characters from content type when using determineContentType option,"The method ""determineContentType"" in the MailBinding class may drop characters from the end of the Content-Type header (specifically ending quotes).  

Example:

Take the following SMINE enveloped data content type:
{code}
   Content-Type: application/pkcs7-mime; smime-type=enveloped-data; name=""smime.p7m""
{code}

When an incoming Exchange is processed by the mail Component with the above content type header, the resulting content type after the determineContentType is application/pkcs7-mime; smime-type=enveloped-data; name=""smime.p7m   (note the ending quote).  This results in a parsing error later on the processing chain.  The following simple unit test demonstrates the issue as it fails on the assertEquals call.

-----------------------------------------------------------------------------------------------------
{code}
package org.apache.camel.component.mail;

import org.apache.camel.CamelContext;
import org.apache.camel.impl.DefaultExchange;
import org.junit.Test;
import static org.junit.Assert.assertEquals;

public class ContentTypeTest
{
    @Test
    public void testDetermineContentType_endingWithQuotes_assertContentMatches()
    {
    	final String contentType = ""application/pkcs7-mime; smime-type=enveloped-data; name=\""smime.p7m\""""; 
    	
    	final MailConfiguration configuration = new MailConfiguration();
    	final MailBinding binding = new MailBinding();
    	
    	final DefaultExchange exchange = new DefaultExchange((CamelContext)null);
    	exchange.getIn().setHeader(""Content-Type"", contentType);
    	
    	
    	String determinedType = binding.determineContentType(configuration, exchange);
    	
    	assertEquals(contentType, determinedType);
    }
}
{code}
",davsclaus,gm2552,Minor,Closed,Fixed,10/Jun/11 14:12,25/Oct/11 11:35
Bug,CAMEL-4089,12509912,Setting pattern on <to> with a custom id causes a failure,"<to uri=""mock:result"" pattern=""InOut"" id=""crap""/>

{code}

org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'crap': Error setting property values; nested exception is org.springframework.beans.NotWritablePropertyException: Invalid property 'pattern' of bean class [org.apache.camel.spring.CamelEndpointFactoryBean]: Bean property 'pattern' is not writable or has an invalid setter method. Does the parameter type of the setter match the return type of the getter?
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1361)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory.java:1086)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory.java:517)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.java:456)
	at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:291)
	at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
	at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:288)
	at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:190)
	at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.java:563)
	at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.java:895)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:425)
	at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:139)
	at org.springframework.context.support.ClassPathXmlApplicationContext.<init>(ClassPathXmlApplicationContext.java:83)
	at org.apache.camel.spring.processor.SpringTestHelper.createSpringCamelContext(SpringTestHelper.java:37)
	at org.apache.camel.spring.processor.SpringSimpleResultTypeRouteTest.createCamelContext(SpringSimpleResultTypeRouteTest.java:46)
	at org.apache.camel.ContextTestSupport.setUp(ContextTestSupport.java:88)
	at org.apache.camel.TestSupport.runBare(TestSupport.java:62)
	at com.intellij.junit3.JUnit3IdeaTestRunner.doRun(JUnit3IdeaTestRunner.java:139)
	at com.intellij.junit3.JUnit3IdeaTestRunner.startRunnerWithArgs(JUnit3IdeaTestRunner.java:52)
	at com.intellij.rt.execution.junit.JUnitStarter.prepareStreamsAndStart(JUnitStarter.java:199)
	at com.intellij.rt.execution.junit.JUnitStarter.main(JUnitStarter.java:62)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
	at com.intellij.rt.execution.application.AppMain.main(AppMain.java:120)
Caused by: org.springframework.beans.NotWritablePropertyException: Invalid property 'pattern' of bean class [org.apache.camel.spring.CamelEndpointFactoryBean]: Bean property 'pattern' is not writable or has an invalid setter method. Does the parameter type of the setter match the return type of the getter?
	at org.springframework.beans.BeanWrapperImpl.setPropertyValue(BeanWrapperImpl.java:1024)
	at org.springframework.beans.BeanWrapperImpl.setPropertyValue(BeanWrapperImpl.java:900)
	at org.springframework.beans.AbstractPropertyAccessor.setPropertyValues(AbstractPropertyAccessor.java:76)
	at org.springframework.beans.AbstractPropertyAccessor.setPropertyValues(AbstractPropertyAccessor.java:58)
	at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBeanFactory.java:1358)
	... 34 more
{code}",davsclaus,davsclaus,Major,Closed,Fixed,11/Jun/11 12:33,25/Oct/11 11:35
Bug,CAMEL-4096,12510081,camel-http - Issue with reusing cached http producer with authenticated user,"See nabble
http://camel.465427.n5.nabble.com/Multiple-remote-connection-to-the-same-host-but-different-users-tp4309456p4309456.html

We should keep auth information in endpoint key, so using a different credential will not re-using cached with another credential.

This problem is most likely also in camel-http4.",boday,davsclaus,Major,Closed,Fixed,13/Jun/11 08:46,28/Jun/11 03:18
Bug,CAMEL-4099,12510093,Camel PropertyPlaceHolder throws an Error when using ${user.home} environment variable in Windows,"Error happens when I try to configure using properyconfigurer
<camel:propertyPlaceholder location=""file://${user.home}/aventurine-config/aventurine-config.properties"" id=""propertyPlaceHolder""/> and try to use an environment variable. THis happens only on windows environment. This works on Ubuntu, It seems that camel is removing the slashes in ${user.home}

org.apache.camel.RuntimeCamelException: org.apache.camel.FailedToCreateRouteException: Failed to create route areaFileRoute: Route[[From[file:{{upload.folder}}]] -> [DoTry[[process[ref:... because of Failed to resolve endpoint: file:{{upload.folder}} due to: C:Userscarloc\aventurine-config\aventurine-config.properties (The system cannot find the path specified)
	at org.apache.camel.util.ObjectHelper.wrapRuntimeCamelException(ObjectHelper.java:1139)
	at org.apache.camel.spring.SpringCamelContext.onApplicationEvent(SpringCamelContext.java:103)
	at org.apache.camel.spring.CamelContextFactoryBean.onApplicationEvent(CamelContextFactoryBean.java:238)
	at org.springframework.context.event.SimpleApplicationEventMulticaster.multicastEvent(SimpleApplicationEventMulticaster.java:97)
	at org.springframework.context.support.AbstractApplicationContext.publishEvent(AbstractApplicationContext.java:303)
	at org.springframework.context.support.AbstractApplicationContext.finishRefresh(AbstractApplicationContext.java:911)
	at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:428)
	at org.springframework.web.context.ContextLoader.createWebApplicationContext(ContextLoader.java:276)
	at org.springframework.web.context.ContextLoader.initWebApplicationContext(ContextLoader.java:197)
	at org.springframework.web.context.ContextLoaderListener.contextInitialized(ContextLoaderListener.java:47)
	at org.mortbay.jetty.handler.ContextHandler.startContext(ContextHandler.java:548)
	at org.mortbay.jetty.servlet.Context.startContext(Context.java:136)
	at org.mortbay.jetty.webapp.WebAppContext.startContext(WebAppContext.java:1239)
	at org.mortbay.jetty.handler.ContextHandler.doStart(ContextHandler.java:517)
	at org.mortbay.jetty.webapp.WebAppContext.doStart(WebAppContext.java:466)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at org.mortbay.jetty.handler.HandlerWrapper.doStart(HandlerWrapper.java:130)
	at org.mortbay.jetty.Server.doStart(Server.java:222)
	at org.mortbay.component.AbstractLifeCycle.start(AbstractLifeCycle.java:50)
	at com.ccti.StartUFM.main(StartUFM.java:35)
Caused by: org.apache.camel.FailedToCreateRouteException: Failed to create route areaFileRoute: Route[[From[file:{{upload.folder}}]] -> [DoTry[[process[ref:... because of Failed to resolve endpoint: file:{{upload.folder}} due to: C:Userscarloc\aventurine-config\aventurine-config.properties (The system cannot find the path specified)
	at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:170)
	at org.apache.camel.impl.DefaultCamelContext.startRoute(DefaultCamelContext.java:706)
	at org.apache.camel.impl.DefaultCamelContext.startRouteDefinitions(DefaultCamelContext.java:1643)
	at org.apache.camel.impl.DefaultCamelContext.doStartCamel(DefaultCamelContext.java:1432)
	at org.apache.camel.impl.DefaultCamelContext.doStart(DefaultCamelContext.java:1336)
	at org.apache.camel.spring.SpringCamelContext.doStart(SpringCamelContext.java:164)
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:67)
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:54)
	at org.apache.camel.impl.DefaultCamelContext.start(DefaultCamelContext.java:1314)
	at org.apache.camel.spring.SpringCamelContext.maybeStart(SpringCamelContext.java:203)
	at org.apache.camel.spring.SpringCamelContext.onApplicationEvent(SpringCamelContext.java:101)
	... 18 more
Caused by: org.apache.camel.ResolveEndpointFailedException: Failed to resolve endpoint: file:{{upload.folder}} due to: C:Userscarloc\aventurine-config\aventurine-config.properties (The system cannot find the path specified)
	at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:407)
	at org.apache.camel.util.CamelContextHelper.getMandatoryEndpoint(CamelContextHelper.java:47)
	at org.apache.camel.model.RouteDefinition.resolveEndpoint(RouteDefinition.java:180)
	at org.apache.camel.impl.DefaultRouteContext.resolveEndpoint(DefaultRouteContext.java:110)
	at org.apache.camel.impl.DefaultRouteContext.resolveEndpoint(DefaultRouteContext.java:116)
	at org.apache.camel.model.FromDefinition.resolveEndpoint(FromDefinition.java:72)
	at org.apache.camel.impl.DefaultRouteContext.getEndpoint(DefaultRouteContext.java:88)
	at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:793)
	at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:165)
	... 28 more
Caused by: java.io.FileNotFoundException: C:Userscarloc\aventurine-config\aventurine-config.properties (The system cannot find the path specified)
	at java.io.FileInputStream.open(Native Method)
	at java.io.FileInputStream.<init>(FileInputStream.java:106)
	at java.io.FileInputStream.<init>(FileInputStream.java:66)
	at org.apache.camel.component.properties.DefaultPropertiesResolver.loadPropertiesFromFilePath(DefaultPropertiesResolver.java:67)
	at org.apache.camel.component.properties.DefaultPropertiesResolver.resolveProperties(DefaultPropertiesResolver.java:49)
	at org.apache.camel.component.properties.PropertiesComponent.parseUri(PropertiesComponent.java:96)
	at org.apache.camel.component.properties.PropertiesComponent.parseUri(PropertiesComponent.java:83)
	at org.apache.camel.impl.DefaultCamelContext.resolvePropertyPlaceholders(DefaultCamelContext.java:963)
	at org.apache.camel.impl.DefaultCamelContext.getEndpoint(DefaultCamelContext.java:405)
	... 36 more",davsclaus,carloc,Minor,Closed,Fixed,13/Jun/11 12:00,25/Oct/11 11:35
Bug,CAMEL-4105,12510319,Thread controlling FTP Polling Process dies,"I have several threads each polling a different FTP Remote folder location.  This works fine for a couple of weeks then one or more FTP Threads die (disappear).  I'm going to attach a snippet from my log file below.  Notice that normally the connection to the FTP site does not exists.  The connection is then made and the folders are checked then the FTP process disconnects.  Some of the folders are on the same remote FTP site and I don't know if that causes a problem or not.

The first time I notice that the thread is dead is when the code starts checking the FTP threads to see if they are connected and I notices that not all of the thread are listed.

*** LOG SNIPPET ***
2011-06-13 11:40:00,531 | DEBUG | Not connected/logged in, connecting to: ftp://r0060207@ftp.inttraworks.inttra.com:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #1 - ftp://r0060207@ftp.inttraworks.inttra.com/outbound
2011-06-13 11:40:00,609 | DEBUG | Not connected/logged in, connecting to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #6 - ftp://bnsf@ftp.mercurygate.net/Outbound/Vendors
2011-06-13 11:40:00,609 | DEBUG | Not connected/logged in, connecting to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #5 - ftp://bnsf@ftp.mercurygate.net/Outbound/Customers
2011-06-13 11:40:00,609 | DEBUG | Not connected/logged in, connecting to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #3 - ftp://bnsf@ftp.mercurygate.net/Outbound/AP
2011-06-13 11:40:00,609 | DEBUG | Not connected/logged in, connecting to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #2 - ftp://bnsf@ftp.mercurygate.net/Outbound/TransportXML
2011-06-13 11:40:00,905 | INFO  | Connected and logged in to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #6 - ftp://bnsf@ftp.mercurygate.net/Outbound/Vendors
2011-06-13 11:40:00,911 | INFO  | Connected and logged in to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #2 - ftp://bnsf@ftp.mercurygate.net/Outbound/TransportXML
2011-06-13 11:40:00,914 | INFO  | Connected and logged in to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #3 - ftp://bnsf@ftp.mercurygate.net/Outbound/AP
2011-06-13 11:40:00,918 | INFO  | Connected and logged in to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #5 - ftp://bnsf@ftp.mercurygate.net/Outbound/Customers
2011-06-13 11:40:01,184 | INFO  | Connected and logged in to: ftp://r0060207@ftp.inttraworks.inttra.com:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #1 - ftp://r0060207@ftp.inttraworks.inttra.com/outbound
2011-06-13 11:40:01,470 | DEBUG | Took 0.565 seconds to poll: Outbound/Vendors | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #6 - ftp://bnsf@ftp.mercurygate.net/Outbound/Vendors
2011-06-13 11:40:01,470 | DEBUG | Disconnecting from: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #6 - ftp://bnsf@ftp.mercurygate.net/Outbound/Vendors
2011-06-13 11:40:01,562 | DEBUG | Took 0.650 seconds to poll: Outbound/TransportXML | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #2 - ftp://bnsf@ftp.mercurygate.net/Outbound/TransportXML
2011-06-13 11:40:01,562 | DEBUG | Disconnecting from: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #2 - ftp://bnsf@ftp.mercurygate.net/Outbound/TransportXML
2011-06-13 11:40:01,582 | DEBUG | Took 0.668 seconds to poll: Outbound/AP | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #3 - ftp://bnsf@ftp.mercurygate.net/Outbound/AP
2011-06-13 11:40:01,583 | DEBUG | Disconnecting from: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #3 - ftp://bnsf@ftp.mercurygate.net/Outbound/AP
2011-06-13 11:40:01,583 | DEBUG | Took 0.665 seconds to poll: Outbound/Customers | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #5 - ftp://bnsf@ftp.mercurygate.net/Outbound/Customers
2011-06-13 11:40:01,583 | DEBUG | Disconnecting from: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #5 - ftp://bnsf@ftp.mercurygate.net/Outbound/Customers
2011-06-13 11:40:01,877 | DEBUG | Took 0.693 seconds to poll: outbound | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #1 - ftp://r0060207@ftp.inttraworks.inttra.com/outbound
2011-06-13 11:40:01,877 | DEBUG | Disconnecting from: ftp://r0060207@ftp.inttraworks.inttra.com:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #1 - ftp://r0060207@ftp.inttraworks.inttra.com/outbound
2011-06-13 11:40:46,452 | INFO  | src=Ftp,srcTime=2011-06-13 11:39:03.418,dst=ROUTER,file=/esb/ToRouter/Binary/CARRIER/JBHLOW/TO_BNSF_DATA/HJBT110613113850.EDI,modified=2011-06-13 11:38:56.000,size=264 | Inbound | DefaultMessageListenerContainer-1
2011-06-13 11:40:46,465 | INFO  | src=Ftp,srcTime=2011-06-13 11:39:03.418,dst=Edi,file=Edi-CARRIER/JBHLOW/TO_BNSF_DATA/HJBT110613113850.EDI,modified=2011-06-13 11:38:56.000,size=264 | DstEdi | DefaultMessageListenerContainer-1
2011-06-13 11:41:00,524 | DEBUG | Not connected/logged in, connecting to: ftp://r0060207@ftp.inttraworks.inttra.com:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #1 - ftp://r0060207@ftp.inttraworks.inttra.com/outbound
2011-06-13 11:41:00,583 | DEBUG | Not connected/logged in, connecting to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #6 - ftp://bnsf@ftp.mercurygate.net/Outbound/Vendors
2011-06-13 11:41:00,594 | DEBUG | Not connected/logged in, connecting to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #5 - ftp://bnsf@ftp.mercurygate.net/Outbound/Customers
2011-06-13 11:41:00,594 | DEBUG | Not connected/logged in, connecting to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #3 - ftp://bnsf@ftp.mercurygate.net/Outbound/AP
2011-06-13 11:41:00,594 | DEBUG | Not connected/logged in, connecting to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #2 - ftp://bnsf@ftp.mercurygate.net/Outbound/TransportXML
2011-06-13 11:41:00,985 | INFO  | Connected and logged in to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #2 - ftp://bnsf@ftp.mercurygate.net/Outbound/TransportXML
2011-06-13 11:41:00,996 | INFO  | Connected and logged in to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #6 - ftp://bnsf@ftp.mercurygate.net/Outbound/Vendors
2011-06-13 11:41:01,043 | INFO  | Connected and logged in to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #3 - ftp://bnsf@ftp.mercurygate.net/Outbound/AP
2011-06-13 11:41:01,048 | INFO  | Connected and logged in to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #5 - ftp://bnsf@ftp.mercurygate.net/Outbound/Customers
2011-06-13 11:41:01,171 | INFO  | Connected and logged in to: ftp://r0060207@ftp.inttraworks.inttra.com:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #1 - ftp://r0060207@ftp.inttraworks.inttra.com/outbound
2011-06-13 11:41:01,527 | INFO  | src=Ftp,srcTime=2011-06-13 11:39:18.540,dst=ROUTER,file=/esb/ToRouter/Binary/CARRIER/JBHLOW/TO_BNSF_DATA/HJBT110613113857.EDI,modified=2011-06-13 11:39:12.000,size=257 | Inbound | DefaultMessageListenerContainer-1
2011-06-13 11:41:01,536 | INFO  | src=Ftp,srcTime=2011-06-13 11:39:18.540,dst=Edi,file=Edi-CARRIER/JBHLOW/TO_BNSF_DATA/HJBT110613113857.EDI,modified=2011-06-13 11:39:12.000,size=257 | DstEdi | DefaultMessageListenerContainer-1
2011-06-13 11:41:01,596 | DEBUG | Took 0.553 seconds to poll: Outbound/AP | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #3 - ftp://bnsf@ftp.mercurygate.net/Outbound/AP
2011-06-13 11:41:01,596 | DEBUG | Disconnecting from: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #3 - ftp://bnsf@ftp.mercurygate.net/Outbound/AP
2011-06-13 11:41:02,396 | DEBUG | Took 1.348 seconds to poll: Outbound/Customers | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #5 - ftp://bnsf@ftp.mercurygate.net/Outbound/Customers
2011-06-13 11:41:02,396 | DEBUG | Disconnecting from: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #5 - ftp://bnsf@ftp.mercurygate.net/Outbound/Customers
2011-06-13 11:41:02,973 | DEBUG | Took 1.802 seconds to poll: outbound | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #1 - ftp://r0060207@ftp.inttraworks.inttra.com/outbound
2011-06-13 11:41:02,973 | DEBUG | Disconnecting from: ftp://r0060207@ftp.inttraworks.inttra.com:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #1 - ftp://r0060207@ftp.inttraworks.inttra.com/outbound
2011-06-13 11:41:36,017 | ERROR | Could not accept connection : org.apache.activemq.transport.InactivityIOException: Channel was inactive for too long: /172.24.1.95:45512 | org.apache.activemq.broker.TransportConnector | ActiveMQ Task
2011-06-13 11:42:00,520 | DEBUG | Not connected/logged in, connecting to: ftp://r0060207@ftp.inttraworks.inttra.com:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #1 - ftp://r0060207@ftp.inttraworks.inttra.com/outbound
2011-06-13 11:42:00,572 | DEBUG | Not connected/logged in, connecting to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #3 - ftp://bnsf@ftp.mercurygate.net/Outbound/AP
2011-06-13 11:42:00,580 | DEBUG | Not connected/logged in, connecting to: ftp://bnsf@ftp.mercurygate.net:21 | org.apache.camel.component.file.remote.FtpConsumer | Camel (camel) thread #5 - ftp://bnsf@ftp.mercurygate.net/Outbound/Customers
",davsclaus,bbuzzard,Major,Closed,Fixed,14/Jun/11 15:02,25/Oct/11 11:35
Bug,CAMEL-4107,12510434,Error using multiple error handlers and onException (java.lang.IllegalStateException: SendProcessor has not been started),"When we use more than one errorHandler in our routes and onException, we face test failures with the following exception:
java.lang.IllegalStateException: SendProcessor has not been started: sendTo(Endpoint[mock://custom])
	at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:94)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:299)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:269)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.RedeliveryErrorHandler.deliverToFailureProcessor(RedeliveryErrorHandler.java:621)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:242)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:269)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:109)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.component.seda.SedaConsumer.sendToConsumers(SedaConsumer.java:189)[camel-core-2.7.2.jar:2.7.2]
	at org.apache.camel.component.seda.SedaConsumer.run(SedaConsumer.java:121)[camel-core-2.7.2.jar:2.7.2]
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)[:1.6.0_20]
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)[:1.6.0_20] ",davsclaus,nikosd23,Major,Resolved,Fixed,15/Jun/11 12:49,15/Jun/11 14:58
Bug,CAMEL-4124,12510978,camel-jms - Issue with using preserveMessageQos and JMS delivery mode not being propagated correctly,If doing a JMS -> JMS (preserveMessageQos) then the JMSDeliveryMode may not be correctly preserved. This problem is only if the route is a straight JMS -> JMS. If you add some steps in between then the JmsMessage may be touched and the property is then propagated correctly.,davsclaus,davsclaus,Minor,Closed,Fixed,20/Jun/11 16:30,25/Oct/11 11:36
Bug,CAMEL-4133,12511154,camel-blueprint doesn't handle elements with a namespace prefix,"Camel-blueprint only works if the camelContext and related elements are not using a namespace prefix.  Like:

{code:xml}
  <camelContext id=""camel"" xmlns=""http://camel.apache.org/schema/blueprint"">
    <routeBuilder ref=""reportIncidentRoutes""/>
  </camelContext>
{code}

If you try and use a prefixed form like:
{code:xml}
  <camel:camelContext id=""camel"" xmlns:camel=""http://camel.apache.org/schema/blueprint"">
    <camel:routeBuilder ref=""reportIncidentRoutes""/>
  </camel:camelContext>
{code}

it won't parse.

",dkulp,dkulp,Major,Closed,Fixed,21/Jun/11 20:16,25/Oct/11 11:35
Bug,CAMEL-4137,12511194,CxfNamespaceHandler should not just set the TCCL and not set it back,"Current CxfNamespaceHandler set the TCCL to parser the cxfEndpoint, but it never reset the TCCL back.
It will cause some trouble if there are lots of Blueprint Namespcehandlers are called by the same thread.",njiang,njiang,Minor,Resolved,Fixed,22/Jun/11 03:44,15/Aug/11 15:19
Bug,CAMEL-4138,12511212,camel-example-cxf-osgi can't be deployed with CXF 2.4.x,"As CXF 2.4.x remove the org.apache.cxf.transport.http_osgi package due to the refactoring of cxf http transport, current camel-example-cxf-osgi cannot be deployed into ServiceMix with CXF 2.4.x.
",njiang,njiang,Major,Resolved,Fixed,22/Jun/11 09:12,22/Jun/11 12:32
Bug,CAMEL-4143,12511324,CxfPayload to Node conversion is not working properly ,"We are using {{camel-cxf}} component and building it from the sources. Also we have slightly modified {{CxfPayloadConverterTest.testCxfPayloadToNode}} test, added some additional checks. Here are the changes that were made to the test:
{code:borderStyle=solid}
Index: CxfPayloadConverterTest.java
===================================================================
--- CxfPayloadConverterTest.java	(revision 6644)
+++ CxfPayloadConverterTest.java	(revision 6686)
@@ -49,6 +49,7 @@
         DocumentBuilderFactory documentBuilderFactory =
                 DocumentBuilderFactory.newInstance();
+        documentBuilderFactory.setNamespaceAware(true);
         DocumentBuilder documentBuilder = documentBuilderFactory.newDocumentBuilder();
         document = documentBuilder.parse(file);
         document.getDocumentElement().normalize();
@@ -118,5 +119,13 @@
         exchange.getIn().setBody(payload);
         node = exchange.getIn().getBody(Node.class);
         assertNotNull(node);
+        Element root = (Element) node;
+        assertEquals(""root element name"", ""root"", root.getNodeName());
+        assertEquals(""root element namespace"", ""http://www.test.org/foo"",
+                root.getNamespaceURI());
+        Element bar = (Element) root.getElementsByTagName(""bar"").item(0);
+        assertEquals(""child element name"", ""bar"", bar.getNodeName());
+        assertEquals(""child element namespace"", ""http://www.test.org/foo"",
+                bar.getNamespaceURI());
     }
 } 
{code}
Since there is no explicit converter from {{CxfPayload}} to {{Node}} the fallback converter from {{CxfPayloadConverter}} is used for this conversion. Fallback converter from {{CxfPayloadConverter}} under the hood uses converters from {{XmlConverter}}. There are two suitable converters in {{XmlConverter}} class which are randomly selected. Some times {{public Element toDOMElement(Node node)}} converter is used and some times {{public Document toDOMDocument(final Node node)}} converter is used. If {{public Document toDOMDocument(final Node node)}} converter is used then test fails with {{ClassCastException}}. To avoid this, explicit converter from {{CxfPayload}} to {{Node}} should be added. Patch that adds {{CxfPayload}} to {{Node}} converter is provided. 
",njiang,avenderov,Minor,Closed,Fixed,23/Jun/11 06:56,25/Oct/11 11:35
Bug,CAMEL-4149,12511461,ThrottlingInflightRoutePolicy can deadlock,"Using ThrottlingInflightRoutePolicy can deadlock a route in some situations. The unit test pasted in below shows one such situation.

What happens is that the bottom route processes its first exchange, then suspends. Since it is suspended it will not take the next exchange from the seda queue, and so it will never check whether it should re-enable the route.

Perhaps it will work by putting the check to re-enable the route in the onExchangeBegin method, if that is called even when the route is suspended?

{code}
import org.apache.camel.Exchange;
import org.apache.camel.Produce;
import org.apache.camel.ProducerTemplate;
import org.apache.camel.builder.RouteBuilder;
import org.apache.camel.component.mock.MockEndpoint;
import org.apache.camel.impl.DefaultInflightRepository;
import org.apache.camel.impl.ThrottlingInflightRoutePolicy;
import org.apache.camel.impl.ThrottlingInflightRoutePolicy.ThrottlingScope;
import org.apache.camel.test.CamelTestSupport;

public class ThrottleTest extends CamelTestSupport {

	@Produce(uri = ""direct:input"")
	protected ProducerTemplate input;
	protected MockEndpoint resultEndpoint;

	@Override
	protected RouteBuilder createRouteBuilder() throws Exception {
	    return new RouteBuilder() {
            public void configure() {
            	resultEndpoint = new MockEndpoint(""mock:result"");
            	resultEndpoint.setCamelContext(getContext());
            	
            	getContext().setInflightRepository(new DefaultInflightRepository() {
            		@Override
            	    public void add(Exchange exchange) {
            			super.add(exchange);
            			System.out.println(""                        add "" + this.size());
            	    }
            		@Override
           	        public void remove(Exchange exchange) {
            			super.remove(exchange);
            			System.out.println(""                     remove "" + this.size());
           	        }
            		
            	});
            	
            	ThrottlingInflightRoutePolicy throttler = new ThrottlingInflightRoutePolicy();
            	
            	throttler.setMaxInflightExchanges(1);
            	throttler.setScope(ThrottlingScope.Context);

            	from(""direct:input"")
            		.inOnly(""seda:hey"", ""seda:hey"", ""seda:hey"", ""seda:hey"", ""seda:hey"")
            		.delay(1000)
        		.inOnly(""log:inputDone"");
            	
            	from(""seda:hey"")
            		.routePolicy(throttler)
            		.inOut(""log:outputDone"")
            		.to(resultEndpoint);
            }
        };
	}
	
	public void testThatAllExchangesAreReceived() throws Exception {
		input.sendBody(""hello"");
		
		resultEndpoint.expectedMessageCount(5);
		resultEndpoint.assertIsSatisfied();
	}
}
{code}",davsclaus,sm,Minor,Closed,Fixed,24/Jun/11 11:37,25/Oct/11 11:35
Bug,CAMEL-4165,12512323,LoanBroker example have some issues when it is deployed into osgi container like ServiceMix,"When you deploy the camel-example-loanbroker bundle into OSGi container, the osgi container only load the WebService route because there are some confliction of the bean ids.
 ",njiang,njiang,Major,Resolved,Fixed,30/Jun/11 13:13,30/Jun/11 13:46
Bug,CAMEL-4166,12512332,"camel-smpp component prints ""1"" or ""2"" to console when sending sms messages.","in SmppBinding.java:
{code}
if (in.getHeaders().containsKey(DATA_CODING)) {
    System.out.println(""1"");
    submitSm.setDataCoding((Byte) in.getHeader(DATA_CODING));
} else {
    System.out.println(""2"");
    submitSm.setDataCoding(configuration.getDataCoding());
}
{code}
",janstey,justas,Minor,Closed,Fixed,30/Jun/11 14:59,25/Oct/11 11:35
Bug,CAMEL-4167,12512343,Label of marshal and unmarshal processors do not appear in trace output,"My route is defined as following:

        from(mqIn)
        .routeId(Helper.getRouteId(this.getClass().getName()))
        .errorHandler(deadLetterChannel(""log:DEAD"").maximumRedeliveries(1).retryAttemptedLogLevel(LoggingLevel.WARN))
        .onException(EAIException.class).maximumRedeliveries(0).handled(true).to(""bean:AuditBO?method=handleException"").end()
        //.transacted(""PROPAGATION_REQUIRES_NEW"")
        .setHeader(""UBS_ID"",new AuditId())
        .wireTap(""seda:auditNew"")
        .unmarshal(""SwiftDataFormat"")
        .setHeader(""MT"", getMT)
        .setHeader(""SWIFTAddress"", getSwiftAddress)
        .to(mqOut);

The tracing output is the following
[                          main] Tracer                         INFO  ID-w01b2bc3-3197-1309192772128-0-2 >>> (wmswift.in.IcgToCore) wireTap(seda://auditNew) -->  <<< Pattern:InOnly, Headers:{UBS_ID=5932bc56-2f7e-45cf-a2b4-ef851ff661d7}, BodyType:String, ...
[                          main] Tracer                         INFO  ID-w01b2bc3-3197-1309192772128-0-2 >>> (wmswift.in.IcgToCore)  --> setHeader[MT] <<< Pattern:InOnly, Headers:{UBS_ID=5932bc56-2f7e-45cf-a2b4-ef851ff661d7}, BodyType:com.ubs.eai.wmswift.SwiftDataMessage, Body:com.ubs.eai.wmswift.SwiftDataMessage@91b9b0
[                          main] Tracer                         INFO  ID-w01b2bc3-3197-1309192772128-0-2 >>> (wmswift.in.IcgToCore) setHeader[MT] --> setHeader[SWIFTAddress] <<< Pattern:InOnly, Headers:{MT=502, UBS_ID=5932bc56-2f7e-45cf-a2b4-ef851ff661d7}, BodyType:com.ubs.eai.wmswift.SwiftDataMessage, Body:com.ubs.eai.wmswift.SwiftDataMessage@91b9b0
[                          main] Tracer                         INFO  ID-w01b2bc3-3197-1309192772128-0-2 >>> (wmswift.in.IcgToCore) setHeader[SWIFTAddress] --> direct://core <<< Pattern:InOnly, Headers:{SWIFTAddress=ZYAMCHZ0XXXX, MT=502, UBS_ID=5932bc56-2f7e-45cf-a2b4-ef851ff661d7}, BodyType:com.ubs.eai.wmswift.SwiftDataMessage, Body:com.ubs.eai.wmswift.SwiftDataMessage@91b9b0
[                          main] Tracer                         INFO  ID-w01b2bc3-3197-1309192772128-0-2 >>> (wmswift.core.CoreToMQ) direct://core -->  <<< Pattern:InOnly, Headers:{SWIFTAddress=ZYAMCHZ0XXXX, MT=502, UBS_ID=5932bc56-2f7e-45cf-a2b4-ef851ff661d7}, BodyType:com.ubs.eai.wmswift.SwiftDataMessage, Body:com.ubs.eai.wmswift.SwiftDataMessage@91b9b0
[                          main] Tracer                         INFO  ID-w01b2bc3-3197-1309192772128-0-2 >>> (wmswift.core.CoreToMQ)  --> choice <<< Pattern:InOnly, Headers:{MT=502, UBS_ID=5932bc56-2f7e-45cf-a2b4-ef851ff661d7, SWIFTAddress=ZYAMCHZ0XXXX}, 

The bug I found is the missing label of the unmarshal processor.
",cmoulliard,lucien.schmitz@ubs.com,Minor,Resolved,Fixed,30/Jun/11 16:16,07/Jul/11 03:20
Bug,CAMEL-4170,12512438,camel-smpp component doesn't support sms messages longer than 254 symbols.,"Exception is thrown when trying to send sms messages longer than 254 characters. According to smpp specification:

The short message data
should be inserted in either
the short_message or
message_payload fields.
Both fields must not be used
simultaneously.

Camel-smpp producer always sets short_message field.
short_message max length is 254 octets, while message_payload can be up to 64K. camel smpp producer should set the correct smmp fields according to message length.",muellerc,justas,Major,Closed,Fixed,01/Jul/11 07:52,04/Jul/11 16:12
Bug,CAMEL-4171,12512458,Groovy language - classNotFoundException in OSGi environment,"The problem is in the following method of the GroovyLanguage

@SuppressWarnings(""unchecked"")
protected Class<Script> parseExpression(String expression) {
    return new GroovyClassLoader().parseClass(expression);
}

It uses the default constructor to create GroovyClassLoader, which uses Thread context classloader, which may not be equal to the classloader of the bundle where the camel context is defined.



Caused by: java.lang.NoClassDefFoundError: groovy/lang/Script
	at java.lang.ClassLoader.defineClass1(Native Method)[:1.6.0_24]
	at java.lang.ClassLoader.defineClassCond(ClassLoader.java:632)[:1.6.0_24]
	at java.lang.ClassLoader.defineClass(ClassLoader.java:616)[:1.6.0_24]
	at java.security.SecureClassLoader.defineClass(SecureClassLoader.java:141)[:1.6.0_24]
	at groovy.lang.GroovyClassLoader.access$300(GroovyClassLoader.java:55)
	at groovy.lang.GroovyClassLoader$ClassCollector.createClass(GroovyClassLoader.java:519)
	at groovy.lang.GroovyClassLoader$ClassCollector.onClassNode(GroovyClassLoader.java:536)
	at groovy.lang.GroovyClassLoader$ClassCollector.call(GroovyClassLoader.java:540)
	at org.codehaus.groovy.control.CompilationUnit$11.call(CompilationUnit.java:747)
	at org.codehaus.groovy.control.CompilationUnit.applyToPrimaryClassNodes(CompilationUnit.java:932)
	at org.codehaus.groovy.control.CompilationUnit.doPhaseOperation(CompilationUnit.java:509)
	at org.codehaus.groovy.control.CompilationUnit.processPhaseOperations(CompilationUnit.java:487)
	at org.codehaus.groovy.control.CompilationUnit.compile(CompilationUnit.java:464)
	at groovy.lang.GroovyClassLoader.doParseClass(GroovyClassLoader.java:306)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:287)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:267)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:214)
	at groovy.lang.GroovyClassLoader.parseClass(GroovyClassLoader.java:224)
	at org.apache.camel.language.groovy.GroovyLanguage.parseExpression(GroovyLanguage.java:44)
	at org.apache.camel.language.groovy.GroovyLanguage.createExpression(GroovyLanguage.java:38)
	at org.apache.camel.language.groovy.GroovyLanguage.createExpression(GroovyLanguage.java:27)
	at org.apache.camel.component.language.LanguageProducer.process(LanguageProducer.java:41)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	at org.apache.camel.impl.converter.AsyncProcessorTypeConverter$ProcessorToAsyncProcessorBridge.process(AsyncProcessorTypeConverter.java:50)[68:org.apache.camel.camel-core:2.6.0.fuse-01-09]
	... 92 more
Caused by: java.lang.ClassNotFoundException: groovy.lang.Script
	at java.net.URLClassLoader$1.run(URLClassLoader.java:202)[:1.6.0_24]
	at java.security.AccessController.doPrivileged(Native Method)[:1.6.0_24]
	at java.net.URLClassLoader.findClass(URLClassLoader.java:190)[:1.6.0_24]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:307)[:1.6.0_24]
	at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:696)[134:groovy-all:1.7.5]
	at groovy.lang.GroovyClassLoader$InnerLoader.loadClass(GroovyClassLoader.java:449)
	at groovy.lang.GroovyClassLoader.loadClass(GroovyClassLoader.java:793)[134:groovy-all:1.7.5]
	at java.lang.ClassLoader.loadClass(ClassLoader.java:248)[:1.6.0_24]
	... 115 more
",davsclaus,szhemzhitsky,Minor,Resolved,Fixed,01/Jul/11 12:00,22/Oct/12 12:44
Bug,CAMEL-4173,12512716,Nullpointer in camel-mail when body is null,"When trying to send a mail with an empty body (null) then we get the following NullPointer exception.
We should instead either complain about the null body or send the mail with an empty body.

13:27:27,010 | INFO  | tp1062262230-101 | ache.camel.processor.CamelLogger   87 | 55 - org.apache.camel.camel-core - 2.7.1 | Exchange[ExchangePattern:InOut, BodyType:org.apache.cxf.message.MessageContentsList, Body:[com.ier.ebo_servicerequest.ServiceRequestType@10e07658]]
13:27:27,013 | ERROR | tp1062262230-101 | ache.camel.processor.CamelLogger  232 | 55 - org.apache.camel.camel-core - 2.7.1 | Failed delivery for exchangeId: ID-MacBook-Pro-de-bahaaldine-local-51035-1309771358240-2-6. Exhausted after delivery attempt: 1 caught: org.springframework.mail.MailPreparationException: Could not prepare mail; nested exception is java.lang.NullPointerException
org.springframework.mail.MailPreparationException: Could not prepare mail; nested exception is java.lang.NullPointerException
            at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:367)[49:org.springframework.context.support:3.0.5.RELEASE]
            at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:344)[49:org.springframework.context.support:3.0.5.RELEASE]
            at org.apache.camel.component.mail.MailProducer.process(MailProducer.java:44)[281:org.apache.camel.camel-mail:2.7.1]
            at org.apache.camel.impl.converter.AsyncProcessorTypeConverter$ProcessorToAsyncProcessorBridge.process(AsyncProcessorTypeConverter.java:50)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:104)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:272)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:299)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:269)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.Pipeline.process(Pipeline.java:125)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.Pipeline.process(Pipeline.java:80)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:139)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:61)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:299)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:109)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:582)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:511)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.MulticastProcessor.process(MulticastProcessor.java:211)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.RecipientList.sendToRecipientList(RecipientList.java:134)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.RecipientList.process(RecipientList.java:102)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:269)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:109)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:103)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:85)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.component.cxf.CxfConsumer$1.syncInvoke(CxfConsumer.java:135)[125:org.apache.camel.camel-cxf:2.7.1]
            at org.apache.camel.component.cxf.CxfConsumer$1.invoke(CxfConsumer.java:77)[125:org.apache.camel.camel-cxf:2.7.1]
            at org.apache.cxf.interceptor.ServiceInvokerInterceptor$1.run(ServiceInvokerInterceptor.java:58)[124:org.apache.cxf.bundle:2.4.0]
            at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)[:1.6.0_24]
            at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)[:1.6.0_24]
            at java.util.concurrent.FutureTask.run(FutureTask.java:138)[:1.6.0_24]
            at org.apache.cxf.workqueue.SynchronousExecutor.execute(SynchronousExecutor.java:37)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.interceptor.ServiceInvokerInterceptor.handleMessage(ServiceInvokerInterceptor.java:106)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:263)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:118)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.http.AbstractHTTPDestination.invoke(AbstractHTTPDestination.java:208)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.servlet.ServletController.invokeDestination(ServletController.java:223)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.servlet.ServletController.invoke(ServletController.java:205)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.servlet.CXFNonSpringServlet.invoke(CXFNonSpringServlet.java:113)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.servlet.AbstractHTTPServlet.handleRequest(AbstractHTTPServlet.java:184)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.servlet.AbstractHTTPServlet.doPost(AbstractHTTPServlet.java:107)[124:org.apache.cxf.bundle:2.4.0]
            at javax.servlet.http.HttpServlet.service(HttpServlet.java:713)[72:org.apache.geronimo.specs.geronimo-servlet_2.5_spec:1.1.2]
            at org.apache.cxf.transport.servlet.AbstractHTTPServlet.service(AbstractHTTPServlet.java:163)[124:org.apache.cxf.bundle:2.4.0]
            at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:534)[79:org.eclipse.jetty.servlet:7.3.1.v20110307]
            at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:476)[79:org.eclipse.jetty.servlet:7.3.1.v20110307]
            at org.ops4j.pax.web.service.jetty.internal.HttpServiceServletHandler.doHandle(HttpServiceServletHandler.java:70)[84:org.ops4j.pax.web.pax-web-jetty:1.0.1]
            at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:119)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:517)[78:org.eclipse.jetty.security:7.3.1.v20110307]
            at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:934)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.ops4j.pax.web.service.jetty.internal.HttpServiceContext.doHandle(HttpServiceContext.java:116)[84:org.ops4j.pax.web.pax-web-jetty:1.0.1]
            at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:404)[79:org.eclipse.jetty.servlet:7.3.1.v20110307]
            at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:184)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:869)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.ops4j.pax.web.service.jetty.internal.JettyServerHandlerCollection.handle(JettyServerHandlerCollection.java:72)[84:org.ops4j.pax.web.pax-web-jetty:1.0.1]
            at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.server.Server.handle(Server.java:346)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:581)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.server.HttpConnection$RequestHandler.content(HttpConnection.java:1057)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:807)[75:org.eclipse.jetty.http:7.3.1.v20110307]
            at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:220)[75:org.eclipse.jetty.http:7.3.1.v20110307]
            at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:411)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:526)[74:org.eclipse.jetty.io:7.3.1.v20110307]
            at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:41)[74:org.eclipse.jetty.io:7.3.1.v20110307]
            at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:528)[73:org.eclipse.jetty.util:7.3.1.v20110307]
            at java.lang.Thread.run(Thread.java:680)[:1.6.0_24]
Caused by: java.lang.NullPointerException
            at javax.mail.util.ByteArrayDataSource.<init>(ByteArrayDataSource.java:137)[69:javax.mail:1.4.3]
            at org.apache.camel.component.mail.MailBinding.populateContentOnMimeMessage(MailBinding.java:205)[281:org.apache.camel.camel-mail:2.7.1]
            at org.apache.camel.component.mail.MailBinding.populateMailMessage(MailBinding.java:114)[281:org.apache.camel.camel-mail:2.7.1]
            at org.apache.camel.component.mail.MailProducer$1.prepare(MailProducer.java:46)[281:org.apache.camel.camel-mail:2.7.1]
            at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:352)[49:org.springframework.context.support:3.0.5.RELEASE]
            ... 98 more
13:27:27,023 | WARN  | tp1062262230-101 | ache.cxf.common.logging.LogUtils  371 |  -  -  | Interceptor for {http://www.ier.com/WS_ServiceRequest}WSServiceRequestService#{http://www.ier.com/WS_ServiceRequest}createServiceRequest has thrown exception, unwinding now
org.apache.cxf.interceptor.Fault: Could not prepare mail; nested exception is java.lang.NullPointerException
            at org.apache.camel.component.cxf.CxfConsumer$1.checkFailure(CxfConsumer.java:218)[125:org.apache.camel.camel-cxf:2.7.1]
            at org.apache.camel.component.cxf.CxfConsumer$1.setResponseBack(CxfConsumer.java:195)[125:org.apache.camel.camel-cxf:2.7.1]
            at org.apache.camel.component.cxf.CxfConsumer$1.syncInvoke(CxfConsumer.java:142)[125:org.apache.camel.camel-cxf:2.7.1]
            at org.apache.camel.component.cxf.CxfConsumer$1.invoke(CxfConsumer.java:77)[125:org.apache.camel.camel-cxf:2.7.1]
            at org.apache.cxf.interceptor.ServiceInvokerInterceptor$1.run(ServiceInvokerInterceptor.java:58)[124:org.apache.cxf.bundle:2.4.0]
            at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)[:1.6.0_24]
            at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)[:1.6.0_24]
            at java.util.concurrent.FutureTask.run(FutureTask.java:138)[:1.6.0_24]
            at org.apache.cxf.workqueue.SynchronousExecutor.execute(SynchronousExecutor.java:37)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.interceptor.ServiceInvokerInterceptor.handleMessage(ServiceInvokerInterceptor.java:106)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:263)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:118)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.http.AbstractHTTPDestination.invoke(AbstractHTTPDestination.java:208)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.servlet.ServletController.invokeDestination(ServletController.java:223)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.servlet.ServletController.invoke(ServletController.java:205)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.servlet.CXFNonSpringServlet.invoke(CXFNonSpringServlet.java:113)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.servlet.AbstractHTTPServlet.handleRequest(AbstractHTTPServlet.java:184)[124:org.apache.cxf.bundle:2.4.0]
            at org.apache.cxf.transport.servlet.AbstractHTTPServlet.doPost(AbstractHTTPServlet.java:107)[124:org.apache.cxf.bundle:2.4.0]
            at javax.servlet.http.HttpServlet.service(HttpServlet.java:713)[72:org.apache.geronimo.specs.geronimo-servlet_2.5_spec:1.1.2]
            at org.apache.cxf.transport.servlet.AbstractHTTPServlet.service(AbstractHTTPServlet.java:163)[124:org.apache.cxf.bundle:2.4.0]
            at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:534)[79:org.eclipse.jetty.servlet:7.3.1.v20110307]
            at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:476)[79:org.eclipse.jetty.servlet:7.3.1.v20110307]
            at org.ops4j.pax.web.service.jetty.internal.HttpServiceServletHandler.doHandle(HttpServiceServletHandler.java:70)[84:org.ops4j.pax.web.pax-web-jetty:1.0.1]
            at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:119)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.security.SecurityHandler.handle(SecurityHandler.java:517)[78:org.eclipse.jetty.security:7.3.1.v20110307]
            at org.eclipse.jetty.server.session.SessionHandler.doHandle(SessionHandler.java:226)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:934)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.ops4j.pax.web.service.jetty.internal.HttpServiceContext.doHandle(HttpServiceContext.java:116)[84:org.ops4j.pax.web.pax-web-jetty:1.0.1]
            at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:404)[79:org.eclipse.jetty.servlet:7.3.1.v20110307]
            at org.eclipse.jetty.server.session.SessionHandler.doScope(SessionHandler.java:184)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:869)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.ops4j.pax.web.service.jetty.internal.JettyServerHandlerCollection.handle(JettyServerHandlerCollection.java:72)[84:org.ops4j.pax.web.pax-web-jetty:1.0.1]
            at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:116)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.server.Server.handle(Server.java:346)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:581)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.server.HttpConnection$RequestHandler.content(HttpConnection.java:1057)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:807)[75:org.eclipse.jetty.http:7.3.1.v20110307]
            at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:220)[75:org.eclipse.jetty.http:7.3.1.v20110307]
            at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:411)[77:org.eclipse.jetty.server:7.3.1.v20110307]
            at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:526)[74:org.eclipse.jetty.io:7.3.1.v20110307]
            at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:41)[74:org.eclipse.jetty.io:7.3.1.v20110307]
            at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:528)[73:org.eclipse.jetty.util:7.3.1.v20110307]
            at java.lang.Thread.run(Thread.java:680)[:1.6.0_24]
Caused by: org.springframework.mail.MailPreparationException: Could not prepare mail; nested exception is java.lang.NullPointerException
            at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:367)[49:org.springframework.context.support:3.0.5.RELEASE]
            at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:344)[49:org.springframework.context.support:3.0.5.RELEASE]
            at org.apache.camel.component.mail.MailProducer.process(MailProducer.java:44)[281:org.apache.camel.camel-mail:2.7.1]
            at org.apache.camel.impl.converter.AsyncProcessorTypeConverter$ProcessorToAsyncProcessorBridge.process(AsyncProcessorTypeConverter.java:50)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:104)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:272)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:299)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:269)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.Pipeline.process(Pipeline.java:125)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.Pipeline.process(Pipeline.java:80)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:139)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.component.direct.DirectProducer.process(DirectProducer.java:61)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:299)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:109)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:582)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.MulticastProcessor.doProcessSequential(MulticastProcessor.java:511)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.MulticastProcessor.process(MulticastProcessor.java:211)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.RecipientList.sendToRecipientList(RecipientList.java:134)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.RecipientList.process(RecipientList.java:102)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:269)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:109)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:77)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:68)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:103)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:85)[55:org.apache.camel.camel-core:2.7.1]
            at org.apache.camel.component.cxf.CxfConsumer$1.syncInvoke(CxfConsumer.java:135)[125:org.apache.camel.camel-cxf:2.7.1]
            ... 41 more
Caused by: java.lang.NullPointerException
            at javax.mail.util.ByteArrayDataSource.<init>(ByteArrayDataSource.java:137)[69:javax.mail:1.4.3]
            at org.apache.camel.component.mail.MailBinding.populateContentOnMimeMessage(MailBinding.java:205)[281:org.apache.camel.camel-mail:2.7.1]
            at org.apache.camel.component.mail.MailBinding.populateMailMessage(MailBinding.java:114)[281:org.apache.camel.camel-mail:2.7.1]
            at org.apache.camel.component.mail.MailProducer$1.prepare(MailProducer.java:46)[281:org.apache.camel.camel-mail:2.7.1]
            at org.springframework.mail.javamail.JavaMailSenderImpl.send(JavaMailSenderImpl.java:352)[49:org.springframework.context.support:3.0.5.RELEASE]
            ... 98 more
",cschneider,cschneider,Major,Closed,Fixed,04/Jul/11 13:10,04/Jul/11 13:25
Bug,CAMEL-4174,12512731,The camel-blueprint does not register blueprint dependencies correctly,It means that components are not waited for correctly in OSGi and the start of camel context can fail if some components aren't available at that time.,gnodet,gnodet,Major,Resolved,Fixed,04/Jul/11 16:13,07/Jul/11 18:20
Bug,CAMEL-4176,12512937,camel-http4 component does not support proxy settings for HTTPS,"Camel-http4 does not support http proxy configuration for HTTPS (https4:// endpoints).

Scenario 1:

Camel -- HTTP --> Proxy -- HTTPS4 --> Destination

http.proxyHost & http.proxyPort are specified and recognized:

		String _authUsername=""WS_P2P_INTERNAL"";
		String _authPassword=""WS_P2P_INTERNAL"";
		
		from(""servlet:///proxy?matchOnUriPrefix=true"")
		.removeHeader(Exchange.HTTP_URI)
		.removeHeader(Exchange.HTTP_PATH)
		.removeHeader(Exchange.HTTP_QUERY)
		
		.to(""https4://aupdc-osb01d:9002/P2P_Creditors/P2P_CreditorWS?wsdl&authUsername=""+_authUsername+""&authPassword=""+_authPassword+""&x509HostnameVerifier=AllowAllVerifier&proxyAuthHost=aupdc00-revprx01t&proxyAuthPort=9001"");


Getting exception:

java.lang.IllegalStateException: Scheme 'http' not registered.
	at org.apache.http.conn.scheme.SchemeRegistry.getScheme(SchemeRegistry.java:71)
	at org.apache.http.impl.conn.DefaultClientConnectionOperator.openConnection(DefaultClientConnectionOperator.java:127)

Attaching the whole log - http_exception.log


Scenario 2:


Camel -- HTTPS --> Proxy -- HTTPS4 --> Destination

Camel does not seem top recognize and accept https.proxyHost and https.proxyPort settings:

getContext().getProperties().put(""https.proxyHost"", ""aupdc00-revprx01d"");
getContext().getProperties().put(""https.proxyPort"", ""9001"");



",davsclaus,doffe,Major,Resolved,Fixed,06/Jul/11 06:59,17/Jul/11 07:35
Bug,CAMEL-4177,12512968,dependency:go-offline fails in tooling/maven pom,"{code}
[INFO] Reactor Summary:
[INFO] 
[INFO] Camel :: Maven Plugins ............................ FAILURE [3.566s]
[INFO] Camel :: Maven Camel Plugin ....................... SKIPPED
[INFO] Camel :: Maven Guice Plugin ....................... SKIPPED
[INFO] Camel :: Html to Pdf Plugin ....................... SKIPPED
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 8.320s
[INFO] Finished at: Wed Jul 06 08:45:30 NDT 2011
[INFO] Final Memory: 10M/55M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.maven.plugins:maven-dependency-plugin:2.1:resolve-plugins (resolve-plugins) on project maven-plugins: Nested: Failure to find org.apache.maven.plugins:maven-plugin-plugin:jar:2.2.1 in http://repo1.maven.org/maven2 was cached in the local repository, resolution will not be reattempted until the update interval of central has elapsed or updates are forced
{code}",janstey,janstey,Minor,Resolved,Fixed,06/Jul/11 11:31,06/Jul/11 11:35
Bug,CAMEL-4184,12513119,Using adviceWith 2 times or more in Java DSL with a context scoped onException causing the 1st advised route to not work as expected,"See nabble
http://camel.465427.n5.nabble.com/intercept-using-adviceWith-tp4520468p4520468.html

",davsclaus,davsclaus,Minor,Resolved,Fixed,07/Jul/11 10:29,07/Jul/11 16:16
Bug,CAMEL-4187,12513169,The ProcessorFactory interface isn't usable in scala,,gnodet,gnodet,Major,Resolved,Fixed,07/Jul/11 18:17,07/Jul/11 18:19
Bug,CAMEL-4194,12513243,"JmsEndpoint that is a topic should not be singleton scoped, only queues should be","See nabble
http://camel.465427.n5.nabble.com/Removing-routes-programmatically-tp4556741p4556741.html

If you have multiple consumers on the *exact same* JMS endpoint uri, and if you remove on of the consumers (eg from a route), then the singleton JmsEndpoint will be stopped. This causes the other active consumers to not receive any messages anymore.

This problem is only if you remove the route. If you stop route then there is no problem.",davsclaus,davsclaus,Major,Closed,Fixed,08/Jul/11 10:31,25/Oct/11 11:35
Bug,CAMEL-4198,12513275,Bindy Parser not handling Null values in KVP,"Camel Bindy is not handling null values in KVP. It throws ArrayIndexOutOfBounds Exception.
From the source below if FIX message has a key value pair with null value i.e (eg: 43=1|63=|64=xyz) the key 63 has no value an while parsing it throws exception for keyValuePair[1].

           // Get KeyValuePair
            String[] keyValuePair = s.split(getKeyValuePairSeparator());

            // Extract Key
            int key = Integer.parseInt(keyValuePair[0]);

            // Extract key value
            String value = keyValuePair[1];",davsclaus,surya108,Minor,Resolved,Fixed,08/Jul/11 15:16,19/Sep/11 19:40
Bug,CAMEL-4211,12513761,URISupport - Normalize URI should support parameters with same key,"See nabble
http://camel.465427.n5.nabble.com/Problems-with-jetty-component-and-posts-with-more-then-one-value-for-a-field-tp4576908p4576908.html

The end user is using jetty producer component to send a HTTP POST/GET to some external client. In the endpoint uri he have the parameters, and there are 2 times {{to}} as parameter key. Currently Camel loses the 2nd {{to}} parameter. ",davsclaus,davsclaus,Major,Resolved,Fixed,12/Jul/11 07:09,12/Jul/11 09:23
Bug,CAMEL-4213,12513774,camel-jpa can't find entities when using blueprint in OSGi,"When using the following simple route, the entity classes can't be found:
{code}
public void configure() throws Exception {
    from(""direct:test-in"").to(""jpa:mypackage.MyEntity1"");
    from(""jpa:mypackage.MyEntity2"").to(""log:test-out"");
}
{code}
The error message is:
09:43:55,417 | WARN  | rint Extender: 3 | ObjectHelper                     | g.apache.camel.util.ObjectHelper  698 | 78 - org.apache.camel.camel-core - 2.7.2 | Cannot find class: mypackage.MyEntity1
09:43:55,467 | WARN  | rint Extender: 3 | ObjectHelper                     | g.apache.camel.util.ObjectHelper  698 | 78 - org.apache.camel.camel-core - 2.7.2 | Cannot find class: mypackage.MyEntity2

followed by many warnings about that the entityType is not known for the consumer.

The errors only occur when using blueprint to bootstrap the route. When using spring-dm everything works fine. The entity classes are located in the same bundle as the route. My guess is that it uses the wrong classloader.

JPA works fine otherwise. It is only the camel-jpa component I have problems with. camel-jpa is a very spring-centric module, using a spring TransactionManager and all, so I hope that my use case makes sense.

For completeness here is my blueprint-file:
{code}
<?xml version=""1.0"" encoding=""UTF-8""?>
<blueprint xmlns=""http://www.osgi.org/xmlns/blueprint/v1.0.0""
	default-activation=""eager"" xmlns:jpa=""http://aries.apache.org/xmlns/jpa/v1.1.0"">

	<camelContext xmlns=""http://camel.apache.org/schema/blueprint"">
		<package>mypackage</package>
	</camelContext>

	<bean id=""jpa"" class=""org.apache.camel.component.jpa.JpaComponent"">
		<property name=""entityManagerFactory"" ref=""entityManagerFactory"" />
		<property name=""transactionManager"" ref=""transactionManager"" />
	</bean>

	<bean id=""transactionManager"" class=""org.springframework.orm.jpa.JpaTransactionManager"">
		<property name=""entityManagerFactory"" ref=""entityManagerFactory"" />
	</bean>

	<reference id=""entityManagerFactory"" interface=""javax.persistence.EntityManagerFactory"" filter=""(osgi.unit.name=myunit)"" />

</blueprint>
{code}
The EntityManagerFactory is created by Aries Jpa, but it shouldn't be relevant for the question.",,ljb,Major,Resolved,Fixed,12/Jul/11 10:18,12/Sep/11 06:09
Bug,CAMEL-4215,12513810,FtpOperations Delete operation not respecting Stepwise Attribute,"Current implementation of deleteFile(String name) ignores the Endpoints 'stepwise' requirement. 

This was noticed when interfacing with a MVS ftp system with the operation failing silently as the resulting exception is swallowed by higher order code.

I've unfortunately been unable to create a test case that emulates the behavior, since the Apache FtpServer and associated mocks used within the test cases don't emulate the behavior of the MVS platform.

I believe other operations suffer the same problem (e.g rename).

I've attached a patch that corrects the issue. Existing test cases all pass, though someone will probably want to see if this implementation is the most efficient.",davsclaus,dsmw,Minor,Closed,Fixed,12/Jul/11 15:59,25/Oct/11 11:36
Bug,CAMEL-4219,12513891,Running the camel-example-spring-javaconfig fails on startup,This example fails when running mvn camel:run as stated in the readme.txt file.,davsclaus,davsclaus,Minor,Resolved,Fixed,13/Jul/11 08:57,13/Jul/11 09:09
Bug,CAMEL-4231,12514162,shouldn't call BaseTypeConverterRegistry.loadTypeConverters() in OSGi container,"the Activator in camel-core take care of loading the converters from bundles, we should avoid  calling BaseTypeConverterRegistry.loadTypeConverters() in OSGi container, otherwise due to no classloader was specified, see this code in OsgiTypeConverter
            @Override
            public Set<ClassLoader> getClassLoaders() {
                // we don't need any classloaders as we use osgi service tracker instead
                return Collections.emptySet();
            }

we'll get exception like
Caused by: org.apache.camel.TypeConverterLoaderException: Failed to load type converters because of: Cannot find any type converter classes from the following packages: [org.apache.servicemix.camel.converter]
	at org.apache.camel.impl.converter.AnnotationTypeConverterLoader.load(AnnotationTypeConverterLoader.java:90)
	at org.apache.camel.impl.converter.BaseTypeConverterRegistry.loadTypeConverters(BaseTypeConverterRegistry.java:406)
	at org.apache.camel.impl.converter.DefaultTypeConverter.doStart(DefaultTypeConverter.java:42)
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:67)
	at org.apache.camel.impl.ServiceSupport.start(ServiceSupport.java:54)
	at org.apache.camel.util.ServiceHelper.startService(ServiceHelper.java:56)
	at org.apache.camel.core.osgi.OsgiTypeConverter.createRegistry(OsgiTypeConverter.java:163)
",davsclaus,ffang,Blocker,Resolved,Fixed,15/Jul/11 06:00,15/Jul/11 08:40
Bug,CAMEL-4234,12514185,ProducerTemplate/MockEndpoint does not seem to trigger context exception handling.,"Hello,

I have a camel route that has ""onException"" clause.  When I run it on Tomcat it works pefectly, but in my integration test (JUnit4 + Spring Tests) the exception handling isn't triggered.  The unit tests uses a producer template and mock endpoint, so it seems that it is in one of those.

I'll try to reproduce it in a small test case.

Kind regards,
Bryan. ",davsclaus,xbbrck,Major,Resolved,Fixed,15/Jul/11 09:20,18/Jul/11 14:07
Bug,CAMEL-4235,12514199,CamelBlueprint2Test  testErrorHandler test is failed.,"org.apache.camel.itest.osgi.blueprint.CamelBlueprint2Test.testErrorHandler failed dure to the CircularDependencyException.

{code}
[Blueprint Extender: 1] ERROR org.apache.aries.blueprint.container.BlueprintContainerImpl - Unable to start blueprint container for bundle CamelBlueprintTestBundle14
org.osgi.service.blueprint.container.ComponentDefinitionException: Unable to intialize bean camel-1
	at org.apache.aries.blueprint.container.BeanRecipe.runBeanProcInit(BeanRecipe.java:638)
        at org.apache.aries.blueprint.container.BeanRecipe.internalCreate(BeanRecipe.java:724)
	at org.apache.aries.blueprint.di.AbstractRecipe.create(AbstractRecipe.java:64)
	at org.apache.aries.blueprint.container.BlueprintRepository.createInstances(BlueprintRepository.java:219)

......
Caused by: org.apache.camel.FailedToCreateRouteException: Failed to create route route1 at: >>> To[mock:result] <<< in route: Route[[From[direct:start]] -> [To[mock:result]]] because of org.apache.aries.blueprint.di.CircularDependencyException: [BeanRecipe[name='dlc'], BeanRecipe[name='.camelBlueprint.bean.factory.dlc'], BeanRecipe[name='camel-1'], BeanRecipe[name='dlc']]
	at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:794)
	at org.apache.camel.model.RouteDefinition.addRoutes(RouteDefinition.java:165)
	at org.apache.camel.impl.DefaultCamelContext.startRoute(DefaultCamelContext.java:705)
	at org.apache.camel.impl.DefaultCamelContext.startRouteDefinitions(DefaultCamelContext.java:1659)
{code}",njiang,njiang,Major,Resolved,Fixed,15/Jul/11 11:10,15/Jul/11 12:35
Bug,CAMEL-4237,12514244,Archetypes fail to build in offline mode,"If build tooling/archetypes with the --offline Maven argument you may see an error like

[ERROR]     Unresolveable build extension: Error resolving version for plugin 'org.apache.maven.archetype:archetype-packaging'

Seems the version for archetype-packaging in the dependencyManagement section defined in the parent pom is not propagating down to build extension... when Maven is not in offline mode it can simply grab the latest version from the metadata at central. I'm committing a simple fix shortly. ",janstey,janstey,Major,Closed,Fixed,15/Jul/11 16:20,25/Oct/11 11:35
Bug,CAMEL-4240,12514409,Unable to use XA JMS trabsaction on WebLogic,"There is problem with camel-jms component deployed on WebLogic 10.3.X and XA connection factory. 

As spring developers suggests, if we use XA transaction on WebLogic we must set SessionTransacted to false and specify transactionManager. See http://ourcraft.wordpress.com/2008/10/22/the-mysteriously-necessary-sessiontransactedtrue/ comment #9 by Juergen Hoeller.

But camel-jms setup transactionManager for MessageListener only when ""transacted"" property is true. So, it is not possible to use XA transactions with WebLogic JMS.
",davsclaus,ciand7,Major,Resolved,Fixed,18/Jul/11 08:34,23/Sep/11 13:49
Bug,CAMEL-4246,12514569,TraceInterceptor does not work correctly for AsyncProcessing,"TraceEventHandlers are not called correctly when a node is processed asynchronously.

From  org.apache.camel.processor.interceptor.TraceInterceptor:
            try {
                // special for interceptor where we need to keep booking how far we have routed in the intercepted processors
                if (node.getParent() instanceof InterceptDefinition && exchange.getUnitOfWork() != null) {
                    TracedRouteNodes traced = exchange.getUnitOfWork().getTracedRouteNodes();
                    traceIntercept((InterceptDefinition) node.getParent(), traced, exchange);
                }

                // process the exchange
                try {
                    sync = super.process(exchange, callback);
                } catch (Throwable e) {
                    exchange.setException(e);
                }
            } finally {
                // after (trace out)
                if (shouldLog && tracer.isTraceOutExchanges()) {
                    logExchange(exchange);
                    traceExchangeOut(exchange, traceState);
                }
            }

As it is this results in traceExchangeOut being called before the callback, which is wrong.
The call to super.process needs to wrap the callback to call traceExchangeOut (and the finally block shouldn't run if the process is asynch).

This isn't a regression, but the change to make more routes asynchronous makes it more noticeable.",davsclaus,yaytay,Minor,Resolved,Fixed,19/Jul/11 11:34,02/Sep/11 09:51
Bug,CAMEL-4252,12514759,camel-ftp - Using tempFileName option should use the separator option to allow control of path separators in the generated file name,"See nabble
http://camel.465427.n5.nabble.com/FTP-temp-file-not-being-created-correctly-on-Windows-Camel-2-7-2-tp4591581p4591581.html",davsclaus,davsclaus,Minor,Resolved,Fixed,20/Jul/11 08:20,02/Sep/11 09:55
Bug,CAMEL-4254,12514903,Missing setter or constructor-argument for elementNameStrategyRef in SoapJaxbDataFormat,"Because the variable ""elementNameStrategyRef"" in org.apache.camel.dataformat.soap.SoapJaxbDataFormat can't be set, it is always null. Therefore declaring the data format with spring-dsl leads to an automatic fallback to the default strategy ""TypeNameStrategy"" in org.apache.camel.dataformat.soap.SoapJaxbDataFormat.checkElementNameStrategy(Exchange exchange). This further leads to wrong results when marshalling.

So the following declararion does not work correctly:
...
    <bean id=""myStrategy"" class=""org.apache.camel.dataformat.soap.name.ServiceInterfaceStrategy"">
    	<constructor-arg value=""my.package.MyServiceInterface""/>
	<constructor-arg value=""true""/>
    </bean>
...
    <camelContext xmlns=""http://camel.apache.org/schema/spring"">
...
		<dataFormats>
			<soapjaxb id=""mySoap"" 
                              contextPath=""my.package"" 
                              elementNameStrategyRef=""myStrategy""/>
		</dataFormats>
...
                <route>
        	        <from uri=""cxf:/MyService?serviceClass=my.package.MyServiceInterface"" />
...
            		<convertBodyTo type=""my.package.MyType""/>
			<marshal ref=""mySoap""/>
...",njiang,ojelinski,Major,Closed,Fixed,21/Jul/11 12:59,23/Sep/11 12:32
Bug,CAMEL-4261,12514999,Properties component - Cached locations may add duplicates,"See nabble
http://camel.465427.n5.nabble.com/LRUCache-memory-leak-when-using-camel-properties-file-tp4621357p4621357.html

The internal cache should be a soft cache and we should use a key holder to ensure equals/hashCode",davsclaus,davsclaus,Major,Resolved,Fixed,22/Jul/11 07:15,08/Aug/11 18:02
Bug,CAMEL-4264,12515233,The routeContext stack of DefaultUnitOfWork should be thread safe,"Here is the stack trace
{code}
java.util.EmptyStackException
        at java.util.Stack.peek(Stack.java:85)
        at java.util.Stack.pop(Stack.java:67)
        at 
org.apache.camel.impl.DefaultUnitOfWork.popRouteContext(DefaultUnitOfWork.java:226)
        at 
org.apache.camel.processor.DefaultChannel$1.done(DefaultChannel.java:260)
        at 
org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:330)
        at 
org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:208)
        at 
org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:256)
        at 
org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:70)
        at org.apache.camel.processor.Pipeline.process(Pipeline.java:143)
        at org.apache.camel.processor.Pipeline.process(Pipeline.java:78)
        at 
org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:113)
        at 
org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:91)
        at 
org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:85)
        at 
org.apache.camel.processor.aggregate.AggregateProcessor$1.run(AggregateProcessor.java:397)
        at 
java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
        at 
java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
        at java.util.concurrent.FutureTask.run(FutureTask.java:138)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
        at 
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
        at java.lang.Thread.run(Thread.java:619)

{code}
You can find more information in this mail thread[1]
[1]http://camel.465427.n5.nabble.com/EmptyStackException-in-DefaultUnitOfWork-popRouteContext-td4630106.html",muellerc,njiang,Major,Resolved,Fixed,25/Jul/11 10:02,30/Aug/11 00:04
Bug,CAMEL-4265,12515292,Fail to close stream in DefaultPropertiesResolver,"    protected Properties loadPropertiesFromFilePath(CamelContext context, String path) throws IOException {
        if (path.startsWith(""file:"")) {
            path = ObjectHelper.after(path, ""file:"");
        }
        InputStream is = new FileInputStream(path);
        Properties answer = new Properties();
        answer.load(is);
        return answer;
    }

    Stream not closed before return, as well as in another method: loadPropertiesFromClasspath.    ",davsclaus,edge,Minor,Resolved,Fixed,25/Jul/11 17:55,02/Sep/11 09:46
Bug,CAMEL-4269,12515383,Connection to sftp server occasionally don't want to come back,"Problem appear if somebody kill logged on user on sftp server while you are connected with consumer.
Camel is not able to reconnect on next poll even if it says that it will do this.
RemoteFileConsumer is not connected but it thinks that it is still logged in.

I think ""loggedIn"" flag should be down after each disconnecting action like in attached patch.

{quote}
2011-07-24 22:35:14,848 WARN  [RemoteFilePollingConsumerPollStrategy:37] Trying to recover by disconnecting from remote server forcing a re-connect at next poll: sftp://test@host 
2011-07-24 22:35:14,849 WARN  [RemoteFilePollingConsumerPollStrategy:52] Consumer Consumer[sftp://test@host/test?delay=60000&delete=true&localWorkDirectory=%2Fvar%2Ftest%2Ftemp&password=xxxx&throwExceptionOnConnectFailed=true] could not poll endpoint: sftp://test@host/test?delay=60000&delete=true&localWorkDirectory=%2Fvar%2Ftest%2Ftemp&password=xxxx&throwExceptionOnConnectFailed=true caused by: Cannot change directory to: test 
org.apache.camel.component.file.GenericFileOperationFailedException: Cannot change directory to: test
	at org.apache.camel.component.file.remote.SftpOperations.doChangeDirectory(SftpOperations.java:408)
	at org.apache.camel.component.file.remote.SftpOperations.changeCurrentDirectory(SftpOperations.java:393)
	at org.apache.camel.component.file.remote.SftpConsumer.doPollDirectory(SftpConsumer.java:77)
	at org.apache.camel.component.file.remote.SftpConsumer.pollDirectory(SftpConsumer.java:49)
	at org.apache.camel.component.file.GenericFileConsumer.poll(GenericFileConsumer.java:83)
	at org.apache.camel.impl.ScheduledPollConsumer.run(ScheduledPollConsumer.java:97)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRunAndReset(FutureTask.java:317)
	at java.util.concurrent.FutureTask.runAndReset(FutureTask.java:150)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$101(ScheduledThreadPoolExecutor.java:98)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.runPeriodic(ScheduledThreadPoolExecutor.java:180)
	at java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:204)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)
Caused by: 4: 
	at com.jcraft.jsch.ChannelSftp.cd(ChannelSftp.java:285)
	at org.apache.camel.component.file.remote.SftpOperations.doChangeDirectory(SftpOperations.java:406)
	... 14 more
Caused by: java.io.IOException: Pipe closed
	at java.io.PipedInputStream.read(PipedInputStream.java:291)
	at java.io.PipedInputStream.read(PipedInputStream.java:361)
	at com.jcraft.jsch.ChannelSftp.fill(ChannelSftp.java:2333)
	at com.jcraft.jsch.ChannelSftp.header(ChannelSftp.java:2359)
	at com.jcraft.jsch.ChannelSftp._realpath(ChannelSftp.java:1819)
	at com.jcraft.jsch.ChannelSftp.cd(ChannelSftp.java:268)
	... 15 more
{quote}
",davsclaus,marekni,Major,Resolved,Fixed,26/Jul/11 11:53,02/Sep/11 09:49
Bug,CAMEL-4274,12515477,MDCUnitOfWork behaviour incorrect with nested routes,"MDCUnitOfWork.clear() removes any content set by MDCUnitOfWork from MDC.
If there are two separate exchange running at the same time in the same thread, as happens when a ProduceTemplate is used from Java, this is incorrect because the MDC of the parent is cleared after the child completes.
Conceptually the MDC should be a stack, but MDC only accepts strings so the simplest thing to do is for MDCUnitOfWork to replace the previous values in clear().",davsclaus,yaytay,Minor,Resolved,Fixed,27/Jul/11 06:11,02/Sep/11 09:47
Bug,CAMEL-4276,12515644,camel-cxf OSGi bundle uses an incorrect CXF version range,"The camel-cxf Karaf feature defines the following CXF version range:

<feature version=""[2.4,2.6)"">cxf</feature>

which is correct.

Unfortunately the camel-cxf bundle import statement doesn't define explicitly the CXF version range. The Felix maven-bundle-plugin determines the version range with the CXF version used and so define it to [2.4,2.5).

1/ it's not correct as it doesn't match the feature definition
2/ it means that you can't use Camel with CXF 2.5.0-SNAPSHOT",jbonofre,jbonofre,Major,Closed,Fixed,28/Jul/11 13:17,25/Oct/11 11:35
Bug,CAMEL-4283,12515781,dump as xml fails if message body is already xml,MessageHelper.dumpAsXml has a problem if the message body is already xml.,davsclaus,davsclaus,Minor,Resolved,Fixed,29/Jul/11 14:43,19/Sep/11 18:09
Bug,CAMEL-4287,12515914,camel-aws won't build offline,"If you build camel-aws in Maven's offline mode (-o), you may get the following error:

No versions available for org.codehaus.jackson:jackson-core-asl:jar:[1.4,) within specified range

This will occur even if you have the libs locally. This is happening because Maven cannot download the remote metadata to get a list of versions for jackson-core-asl. Specifying a hard version for this (not a range) gets around the issue. Committing a fix shortly.",janstey,janstey,Major,Resolved,Fixed,01/Aug/11 13:12,02/Sep/11 09:42
Bug,CAMEL-4293,12516889,@Bean method name is ignored,"Since 2.8.0 Exchange.BEAN_METHOD_NAME is always used instead of @Bean method name (see BeanInfo.createInvocation(Object pojo, Exchange exchange)). As a consequence, the wrong method is choosen when I use a beanref invokation on a method that uses @Bean 
parameter binding.

I built a simple test case as a zip file containing an Eclipse Helios maven project (see attachment). The test fails with camel 2.8.0 and succeeds with 2.7.3 and earlier (at least until 2.6.0).

regards 
Vincent MATHON",davsclaus,vmathon,Major,Resolved,Fixed,02/Aug/11 09:05,02/Sep/11 09:39
Bug,CAMEL-4294,12516924,Camel CXF Endpoint creates incomplete WSDL,"The dynamically created WSDL for a cxf:cxfEndpoint declaration is malformed when using a complex type as parameter.

Please see as well the discussion here.

http://camel.465427.n5.nabble.com/Camel-CXF-Endpoint-creating-incomplete-WSDL-td4642433.html

Use attached test project to run mvn test with version 2.8.0 and 2.7.2, this should show the error.",njiang,herriot1,Major,Resolved,Fixed,02/Aug/11 13:19,15/Aug/11 23:10
Bug,CAMEL-4295,12516936,camel-spring-ws bundle specify a wrong version of spring package,"Spring-ws bundle version is 2.0.2 but camel-spring-ws bundle imports spring package with the rang of [3,4).
So camel-spring-ws feature can not be installed rightly.",njiang,njiang,Major,Resolved,Fixed,02/Aug/11 14:35,05/Aug/11 09:00
Bug,CAMEL-4299,12517769,The cxfbean-component no longer works in OSGI (failure to auto create and NullPointerException),"Camel fails to auto create the cxfbean-component. I have tried both with blueprint and Spring-DM. For Spring-DM i get the following error:
Cannot auto create component: cxfbean

My Spring XML-file:
{code}
<?xml version=""1.0"" encoding=""UTF-8""?>
<beans xmlns=""http://www.springframework.org/schema/beans""
	xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance"" xmlns:camel=""http://camel.apache.org/schema/spring""
	xsi:schemaLocation=""http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
        http://camel.apache.org/schema/spring http://camel.apache.org/schema/spring/camel-spring.xsd"">

	<camel:camelContext>
		<camel:route>
			<camel:from uri=""jetty:http://0.0.0.0:8080?matchOnUriPrefix=true"" />
			<camel:to uri=""cxfbean:myBean"" />
		</camel:route>
	</camel:camelContext>

	<bean id=""myBean"" class=""mypackage.MyJaxRsAnnotatedBean"" />

</beans>
{code}

I have both the camel-cxf and camel-cxf-transport bundles installed and it seems that both of them  have a ComponentResolver for cxfbean published in the OSGI-registry. I can get my bundle to start if I create the cxfbean-component explicity in my Spring- or blueprint-XML file:
{code}
<bean id=""cxfbean"" class=""org.apache.camel.component.cxf.cxfbean.CxfBeanComponent"" />
{code}
But then I will get a NullPointerException when I visit a URL handled by my JAXRS-annotated bean:
java.lang.NullPointerException
	at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:96)
	at org.apache.camel.component.cxf.cxfbean.CxfBeanDestination.process(CxfBeanDestination.java:83)
	at org.apache.camel.impl.ProcessorEndpoint.onExchange(ProcessorEndpoint.java:102)
	at org.apache.camel.impl.ProcessorEndpoint$1.process(ProcessorEndpoint.java:72)
	at org.apache.camel.impl.converter.AsyncProcessorTypeConverter$ProcessorToAsyncProcessorBridge.process(AsyncProcessorTypeConverter.java:50)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)
	at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:114)
	at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:286)
	at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:109)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:69)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)
	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:318)
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:209)
	at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:305)
	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:102)
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)
	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98)
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89)
	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:69)
	at org.apache.camel.component.jetty.CamelContinuationServlet.service(CamelContinuationServlet.java:109)
	at javax.servlet.http.HttpServlet.service(HttpServlet.java:806)
	at org.eclipse.jetty.servlet.ServletHolder.handle(ServletHolder.java:538)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1352)
	at org.eclipse.jetty.servlets.MultiPartFilter.doFilter(MultiPartFilter.java:97)
	at org.apache.camel.component.jetty.CamelMultipartFilter.doFilter(CamelMultipartFilter.java:41)
	at org.eclipse.jetty.servlet.ServletHandler$CachedChain.doFilter(ServletHandler.java:1323)
	at org.eclipse.jetty.servlet.ServletHandler.doHandle(ServletHandler.java:476)
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:937)
	at org.eclipse.jetty.servlet.ServletHandler.doScope(ServletHandler.java:406)
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:871)
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:110)
	at org.eclipse.jetty.server.Server.handle(Server.java:346)
	at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:589)
	at org.eclipse.jetty.server.HttpConnection$RequestHandler.headerComplete(HttpConnection.java:1048)
	at org.eclipse.jetty.http.HttpParser.parseNext(HttpParser.java:601)
	at org.eclipse.jetty.http.HttpParser.parseAvailable(HttpParser.java:214)
	at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:411)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:535)
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:40)
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:529)
	at java.lang.Thread.run(Thread.java:680)
",njiang,ljb,Major,Resolved,Fixed,03/Aug/11 09:05,09/Aug/11 14:29
Bug,CAMEL-4305,12517903,xslt component - Cannot load xslt file from file system,"See nabble
http://camel.465427.n5.nabble.com/Custom-URIResolver-for-Camel-tp4665485p4665485.html

The Spring IO resource is a bit pants, as you ask for it to give an input stream, and then it fails doing that for a plain file url.
Instead it gives you an URL connection, and you get a unknown host exception.

",davsclaus,davsclaus,Minor,Resolved,Fixed,04/Aug/11 09:43,02/Sep/11 09:36
Bug,CAMEL-4311,12518098,Camel Bindy Parser One to Many KvP Unmarshall Issue,"Bindy KvP unmarshall issue. 

While unmarshalling FIX messages, camel-bindy causing inconsistencies if the incoming fix messages are defined to map using 'OneToMany' annotations.

In a given route for eg:

 BindyKeyValuePairDataFormat kvpBindyDataFormat = new BindyKeyValuePairDataFormat(""org.apache.camel.dataformat.bindy.model.fix.complex.onetomany"");
        public void configure() {
            from(URI_DIRECT_START).unmarshal(kvpBindyDataFormat).to(URI_MOCK_RESULT);
        }

kvpBindyDataFormat is single instance for multiple threads and with in 'BindyKeyValuePairFactory' class instance varialbe  below is shared across multiple threads causing data inconsistencies.

private Map<String, List<Object>> lists = new HashMap<String, List<Object>>(); 

Values from previous thread/messages are retained in the current thread causing issues.

For eg in the test case: public class BindyComplexOneToManyKeyValuePairUnMarshallTest extends CommonBindyTest 

if we send another message  with no repeating groups are sent.

String message2 = ""8=FIX 4.19=2034=135=049=INVMGR56=BRKR"" + ""1=BE.CHM.00111=CHM0001-0158=this is a camel - bindy test"" 
        				  + ""10=220"";

The output message has repeating group data from message processed in earlier thread are pouplated causing data inconsistencies.

Please suggest the fix considering multi-threading scenario. I am willing to volunteer for the fix.

Regards,
Surya



 ",davsclaus,surya108,Minor,Resolved,Fixed,06/Aug/11 01:41,02/Sep/11 09:32
Bug,CAMEL-4316,12518277,camel-cxf bundle should not ship the camel-cxf-transport packages.,"Camel-cxf bundle of camel 2.8.0 ships and exports the packages of camel-transport, it could cause some trouble when we install the camel-transport and camel-cxf bundles at the same time.",njiang,njiang,Major,Resolved,Fixed,09/Aug/11 02:57,09/Aug/11 14:22
Bug,CAMEL-4318,12518293,"Scheduled poll consumer from pollEnrich should not be polling when route starts, but on demand","See nabble
http://camel.465427.n5.nabble.com/Overriding-ScheduledPollConsumer-tp4676752p4676752.html

The problem is that the file consumer is started when the route starts
{code}
2011-08-09 10:11:05,648 [main           ] INFO  DefaultCamelContext            - Apache Camel  (CamelContext: camel-1) started in 0.284 seconds
2011-08-09 10:11:06,628 [rget/pollenrich] DEBUG FileConsumer                   - Took 0.000 seconds to poll: target/pollenrich
2011-08-09 10:11:07,130 [rget/pollenrich] DEBUG FileConsumer                   - Took 0.000 seconds to poll: target/pollenrich
2011-08-09 10:11:07,632 [rget/pollenrich] DEBUG FileConsumer                   - Took 0.000 seconds to poll: target/pollenrich
2011-08-09 10:11:08,133 [rget/pollenrich] DEBUG FileConsumer                   - Took 0.000 seconds to poll: target/pollenrich
2011-08-09 10:11:08,634 [rget/pollenrich] DEBUG FileConsumer                   - Took 0.000 seconds to poll: target/pollenrich
2011-08-09 10:11:09,135 [rget/pollenrich] DEBUG FileConsumer                   - Took 0.000 seconds to poll: target/pollenrich
2011-08-09 10:11:09,637 [rget/pollenrich] DEBUG FileConsumer                   - Took 0.000 seconds to poll: target/pollenrich
{code}

Instead it should only be polling on demand",davsclaus,davsclaus,Major,Resolved,Fixed,09/Aug/11 08:13,19/Sep/11 17:51
Bug,CAMEL-4319,12518319,Example feature: wrong spring version,"Feature
apache-camel-2.8.0.zip\apache-camel-2.8.0\examples\camel-example-etl\src\main\resources\features.xml 
contains
       <bundle>mvn:org.springframework/spring-jdbc/2.5.6.SEC01</bundle>
       <bundle>mvn:org.springframework/spring-orm/2.5.6.SEC01</bundle> 
Exception during 'hibernate-feauture' feature installation:
12:23:21,337 | INFO  | l Console Thread | araf.shell.console.jline.Console  258 | 36 - org.apache.karaf.shell.console - 2.2.1 | Exception caught while executing command java.lang.Exception: Could not start bundle mvn:org.springframework/spring-jdbc/2.5.6.SEC01 in feature(s) hibernate-feauture-0.0.0: The bundle ""org.springframework.jdbc_2.5.6.SEC01 [332]"" could not be resolved. Reason: Missing Constraint: Import-Package: org.springframework.dao; version=""[2.5.6.SEC01,2.5.6.SEC01]""
        at org.apache.karaf.features.internal.FeaturesServiceImpl.installFeatures(FeaturesServiceImpl.java:353)[38:org.apache.karaf.features.core:2.2.1]
...
Caused by: org.osgi.framework.BundleException: The bundle ""org.springframework.jdbc_2.5.6.SEC01 [332]"" could not be resolved. Reason: Missing Constraint: Import-Package: org.springframework.dao; version=""[2.5.6.SEC01,2.5.6.SEC01]""
        at org.eclipse.osgi.framework.internal.core.AbstractBundle.getResolverError(AbstractBundle.java:1317)[osgi-3.6.0.v20100517.jar:]
        at org.eclipse.osgi.framework.internal.core.AbstractBundle.getResolutionFailureException(AbstractBundle.java:1301)[osgi-3.6.0.v20100517.jar:]
        at org.eclipse.osgi.framework.internal.core.BundleHost.startWorker(BundleHost.java:319)[osgi-3.6.0.v20100517.jar:]
        at org.eclipse.osgi.framework.internal.core.AbstractBundle.start(AbstractBundle.java:284)[osgi-3.6.0.v20100517.jar:]
        at org.eclipse.osgi.framework.internal.core.AbstractBundle.start(AbstractBundle.java:276)[osgi-3.6.0.v20100517.jar:]
        at org.apache.karaf.features.internal.FeaturesServiceImpl.installFeatures(FeaturesServiceImpl.java:350)[38:org.apache.karaf.features.core:2.2.1]
        ... 15 more

Solution:
Change version for bundles org.springframework/spring-jdbc and org.springframework/spring-orm to 3.0.5.RELEASE instead 2.5.6.SEC01.",davsclaus,amarkevich,Trivial,Resolved,Fixed,09/Aug/11 14:15,02/Sep/11 09:23
Bug,CAMEL-4322,12518429,AnnotationTypeConverterLoader doesn't try all the classloaders to load the converter class,"User complained about cannot start the camel context with web container[1], after digging the code of AnnotationTypeConverterLoader, I found that AnnotationTypeConverterLoader doesn't try all the classloader to load the converter class.

[1]http://camel.465427.n5.nabble.com/TypeConverterLoaderException-in-WebApplication-on-ContextLoad-td4684853.html",njiang,njiang,Major,Resolved,Fixed,10/Aug/11 11:21,10/Feb/12 02:59
Bug,CAMEL-4324,12518447,Response body lost when an HTTP error code is returned,"When a REST service returns an HTTP error code (e.g. 404) with some content in the response body, this body is not correctly copied into the exception.
This results e.g. in ${exception.responseBody} having a value like ""org.restlet.data.Response@17bf0a7"" in the following route fragment :

<doTry>
    <to uri=""restlet:http://my.service.com/ws/{id}""/>
    <doCatch>
        <exception>org.apache.camel.CamelException</exception>
        <log message=""body is : ${exception.responseBody}""/>
    </doCatch>
</doTry>

The problem is probably due to the statement (class org.apache.camel.component.restlet.RestletProducer, method populateRestletProducerException) :
{code}
   String copy = response.toString();
{code}

that should be instead something like :
{code}
   String copy = null;
   if (response.getEntity() != null) {
       // get content text
       copy = response.getEntity().getText();
   }
{code}

",njiang,cedric.delaunois,Minor,Resolved,Fixed,10/Aug/11 14:01,19/Sep/11 18:12
Bug,CAMEL-4325,12518450,Wrong exception uri when an HTTP error code is returned,"When a REST service returns an HTTP error code (e.g. 404), the URI property of the exception is incorrect.
The URI is the URI of the origin endpoint of the route.
It should be instead the URI of the endpoint that threw the exception.

For instance, in the following route :
{code:xml}
<route id=""restproxy"">           
    <from uri=""restlet:http://localhost:9080/oe/ws/offre/{id}""/>           
    <doTry>
        <to uri=""restlet:http://bla.dot.com:8080/oe/ws/offre/{id}""/>
        <doCatch>
            <exception>org.apache.camel.CamelException</exception>
            <log message=""URI=${exception.uri}""/>
        </doCatch>
    </doTry>
</route>
{code}

The logged URI is ""restlet:http://localhost:9080/oe/ws/offre/{id}"" but should be instead the real URI used, e.g. ""restlet:http://bla.dot.com:8080/oe/ws/offre/1"".

This bug might be fixed by the following code change in class org.apache.camel.component.restlet.RestletProducer, method populateRestletProducerException() :
   - String uri = exchange.getFromEndpoint().getEndpointUri();
   + String uri = response.getRequest().getResourceRef()

",njiang,cedric.delaunois,Major,Resolved,Fixed,10/Aug/11 14:20,19/Sep/11 17:45
Bug,CAMEL-4326,12518453,NullPointerException when catching exceptions,"The following (blueprint) route produces a NullPointerException :

{code:xml}
<route id=""restproxy"">           
    <from uri=""restlet:http://localhost:9080/oe/ws/offre/{id}""/>
    <doTry>
        <to uri=""restlet:http://bla.dot.com:8080/oe/ws/offre/{id}""/>
        <doCatch>
            <exception>org.apache.camel.component.restlet.RestletOperationException</exception>
        </doCatch>
    </doTry>
</route>
{code}

This happens for when catching any exception classes that does not belong to camel-core or to the classpath (in a OSGI environment).

The NullPointerException is caused by the following statement in class org.apache.camel.model.CatchDefinition, method createExceptionClasses() :
{code}
Class<Exception> type = CastUtils.cast(ObjectHelper.loadClass(name, getClass().getClassLoader()));
{code}
i.e. the class is not found by the class loader.

A work-around is to activate dev:dynamic-import on the camel-core bundle.
",davsclaus,cedric.delaunois,Minor,Resolved,Fixed,10/Aug/11 14:36,10/Aug/11 17:31
Bug,CAMEL-4329,12518681,Username/password ignored when connectionFactory is set in JMS component,"There is a conflict between 2 properties: connectionFactory and username/password. If I set both the connectionFactory wins and jms endpoint uses connection factory that is unaware of username/password specified for the endpoint. 
We need to use both - we have different systems and use different connections factories with different username/passwords.

Let's assume I have a setup with 2 connection factories jmsConnectionFactory1 and jmsConnectionFactory2 and an endpoint jms:queue:myQueue?connectionFactory=jmsConnectionFactory2&username=myuser&password=secret, here is what happens:

I see that in JmsComponent:133 you retrieve the first ConnectionFactory (jmsConnectionFactory1) found in Spring.
Then in JmsComponent:414 you create a delegating ConnectionFactory which always uses username/password (with jmsConnectionFactory1) and set it to the endpoint.
Then in JmsComponent:434 you finally set the right ConnectionFactory (jmsConnectionFactory2) to the endpoint, though the username/password are lost forever.

I have solved it for our project by implementing a clone of JmsComponent with overriden createEndpoint. Move line 434 [setProperties(endpoint.getConfiguration(), parameters);] a the line immediately after 407 [String password = getAndRemoveParameter(parameters, ""password"", String.class);]
",davsclaus,anydoby,Major,Resolved,Fixed,12/Aug/11 09:28,13/Aug/11 08:47
Bug,CAMEL-4345,12519240,Synchronized code causes long delays and hangs for big applications especially with Blueprint,"The DefaultCamelContext uses synchronized ""endpoints"" which ends up ultimately extending a LinkedHashMap through the LRUCache.  The LinkedHashMap is obviously not thread safe, so it requires synchronized guards when accessing the endpoints object.  This especially happens in the getEndpoint(s) calls in the DefaultCamelContext.  In large systems with lots of routes and on multicore systems, dynamically created routes (and many routes) can cause long delays and hang for long times since route creation and the starting of the camel route can occur in unison with synchronization.  In a blueprint container, such as Karaf, this can cause timeouts on the bundle and camel routes will appear to hang indefinately.  Thread dumps show the hangs occur on the synchronized call in getEndpoint(s).  The fix for this is to use concurrent apis as much as possible and remove the synchronized code.  I refactored the LRUCache/LRUSoftCache to use Google's ConcurrentLinkedHashMap (ASL2 License http://code.google.com/p/concurrentlinkedhashmap) and removed the synchronized code that locks the endpoints object.  This should remove the hangs since the locks are no longer required.  Since COncurrentLinkedHashmap is not OSGi ready, I have shaded the classes in core.  On my executions, all unit tests pass with this refactoring using the concurrent code.  This should speed up Camel on multicore systems that have lots of routes.",hadrian,jgenender,Major,Resolved,Fixed,18/Aug/11 05:18,19/Sep/11 17:44
Bug,CAMEL-4352,12519319,"Ftp consumer using done file should cater for if stepwise has been enabled, and thus the done file path is to be calculated differently",The workaround is to use ${file:onlyname} or not using stepwise.,davsclaus,davsclaus,Minor,Resolved,Fixed,18/Aug/11 17:07,02/Sep/11 09:20
Bug,CAMEL-4354,12519401,header added using an EventNotifier is not present at AggregationStrategy for http endpoints,"A new header added using an EventNotifier is not present when the exchange is aggregated with an AggregationStrategy.
This is happening only if the enpoint type is http, ftp doesn't have this issue.

This was working with an early version of 2.8.0-SNAPSHOT

Following the EventNotifier code used.

{code:title=ExchangeSentEventNotifier.java|borderStyle=solid}
public class ExchangeSentEventNotifier extends EventNotifierSupport {

	@Override
	protected void doStart() throws Exception {
        /*
         *  filter out unwanted events
         *  we are interested only in ExchangeSentEvent
         */
        setIgnoreCamelContextEvents(true);
        setIgnoreServiceEvents(true);
        setIgnoreRouteEvents(true);
        setIgnoreExchangeCreatedEvent(true);
        setIgnoreExchangeCompletedEvent(true);
        setIgnoreExchangeFailedEvents(true);
        setIgnoreExchangeSentEvents(false);		
	}

	@Override
	protected void doStop() throws Exception {

	}

	@Override
	public boolean isEnabled(EventObject event) {
		return event instanceof ExchangeSentEvent;
	}

	@Override
	public void notify(EventObject event) throws Exception {
    	if(event.getClass() == ExchangeSentEvent.class){
            ExchangeSentEvent eventSent = (ExchangeSentEvent)event;
            
            log.debug(""Took "" + eventSent.getTimeTaken() + "" millis to send to: "" + eventSent.getEndpoint());

            //storing time taken to the custom header            
            eventSent.getExchange().getIn().setHeader(""x-time-taken"", eventSent.getTimeTaken());
            
    	}
		
	}

}
{code} ",davsclaus,crive,Major,Resolved,Fixed,19/Aug/11 11:24,02/Sep/11 09:15
Bug,CAMEL-4359,12519470,Exception thrown during JAXB marshalling will cause SedaConsumer to break,"Error handler tries to add message body to error log. If it tries to use JAXB type converter and marshalling throws exception, the exception is thrown all the way back to SedaConsumer. SedaConsumer will then retry to create error message in the same way until the point where exception is not handled any more and SedaConsumer loop breaks off.

One single bad message can stop Seda route.",davsclaus,dragisak,Major,Resolved,Fixed,19/Aug/11 20:59,20/Aug/11 10:52
Bug,CAMEL-4362,12519556,Bug in parsing of JPA uri,"According to http://camel.apache.org/jpa.html entityName is optional.
When configuring with the following uri (e.g. w/o entityName):
<camel:endpoint id=""jpaEndpoint"" uri=""jpa:?persistenceUnit=journalPersistenceUnit&amp;usePersist=true"" />
the following warning will be displayed
2011-08-21 18:57:11,381 [main][][][][][][][] WARN org.apache.camel.util.ObjectHelper - Cannot find class: persistenceUnitjournalPersistenceUnitusePersisttruepersistenceUnitjournalPersistenceUnitusePersisttrue
due to the incorrect if test in the JpaComponent:

{noformat}
// lets interpret the next string as a class
        if (path != null) {
            // provide the class loader of this component to work in OSGi environments as camel-jpa must be able
            // to resolve the entity classes
            Class<?> type = getCamelContext().getClassResolver().resolveClass(path, JpaComponent.class.getClassLoader());
            if (type != null) {
                endpoint.setEntityType(type);
            }
        }
{noformat}

path is not null, but it is the rest of the string and not the entityname. e.g it should not start with ? if it is an entityname - else it is the options.",muellerc,davidkarlsen@gmail.com,Major,Closed,Fixed,21/Aug/11 17:02,22/Nov/11 23:22
Bug,CAMEL-4365,12519691,Freemarker - Loading template from file system fails,"This only affects Camel 2.9.

",davsclaus,davsclaus,Minor,Resolved,Fixed,22/Aug/11 14:14,22/Aug/11 14:19
Bug,CAMEL-4367,12519740,"Camel-netty option ""encoder"" and ""decoder"" does not work","Contrary to the documentation, the ""encoder"" and ""decoder"" option is unusable as it's value is taken as-is instead of resolving the references using the registry.

This bug was introduced in the revision 960621 as part of CAMEL-2907.

Patch can be found at https://github.com/szabolcsberecz/camel/commit/b3bcc8dd284575c931048ea410c44577474e25dd",njiang,szabi,Minor,Resolved,Fixed,22/Aug/11 21:49,26/Aug/11 14:03
Bug,CAMEL-4370,12519790,It's hardly possible to use all expression of the Simple language to create file names in the file component,"Sometimes it can be necessary to use custom headers to create a file name.

For example, I declare my file endpoint in the following manner:

{code}
<route id=""fileReader"">
    <from uri=""file://rootFolder?move=.backup&amp;moveFailed=.error/${header.CustomHeader}"" />
    <to uri=""file://out""/>
</route>
{code}

The header ""CustomHeader"" cannot be read because of the following snippets of code in the org.apache.camel.component.file.GenericFile

{code}
/**
 * Bind this GenericFile to an Exchange
 */
public void bindToExchange(Exchange exchange) {
    exchange.setProperty(FileComponent.FILE_EXCHANGE_FILE, this);
    GenericFileMessage<T> in = new GenericFileMessage<T>(this);
    exchange.setIn(in);
    populateHeaders(in);
}

/**
 * Populates the {@link GenericFileMessage} relevant headers
 *
 * @param message the message to populate with headers
 */
public void populateHeaders(GenericFileMessage<T> message) {
    if (message != null) {
        message.setHeader(Exchange.FILE_NAME_ONLY, getFileNameOnly());
        message.setHeader(Exchange.FILE_NAME, getFileName());
        message.setHeader(""CamelFileAbsolute"", isAbsolute());
        message.setHeader(""CamelFileAbsolutePath"", getAbsoluteFilePath());

        if (isAbsolute()) {
            message.setHeader(Exchange.FILE_PATH, getAbsoluteFilePath());
        } else {
            // we must normalize path according to protocol if we build our own paths
            String path = normalizePathToProtocol(getEndpointPath() + File.separator + getRelativeFilePath());
            message.setHeader(Exchange.FILE_PATH, path);
        }

        message.setHeader(""CamelFileRelativePath"", getRelativeFilePath());
        message.setHeader(Exchange.FILE_PARENT, getParent());

        if (getFileLength() >= 0) {
            message.setHeader(""CamelFileLength"", getFileLength());
        }
        if (getLastModified() > 0) {
            message.setHeader(Exchange.FILE_LAST_MODIFIED, new Date(getLastModified()));
        }
    }
}
{code}

As you can see a new ""in"" message is created and not all the headers from the original message are copied to it.",davsclaus,szhemzhitsky,Major,Resolved,Fixed,23/Aug/11 06:17,24/Aug/11 08:49
Bug,CAMEL-4375,12519949,FilterCreateCamelContextPerClassTest is wrong configured,The FilterCreateCamelContextPerClassTest is wrong configured. The isCreateCamelContextPerClass() should return true as written in the comment but currently returns false. ,njiang,akuhtz,Major,Resolved,Fixed,24/Aug/11 11:58,26/Aug/11 10:19
Bug,CAMEL-4383,12520141,StreamResequencer logs unhandled Exceptions at DEBUG level,"If an unhandled Exception is caught by StreamResequencer.run(), it effectively logs the Exception at DEBUG level. A thrown Exception can indicate that a delivered message was not fully processed (as is the case in my route), so this means a message is dropped with a notification in the log only at DEBUG level. This is highly undesirable in a critical route.

It should at least be logged at WARN level, and preferably should be handled using the configured ErrorHandler for the route.",davsclaus,dgtombs,Major,Resolved,Fixed,25/Aug/11 18:50,30/Aug/11 00:04
Bug,CAMEL-4385,12520255,DefaultExchange.copy always creates a DefaultMessage even if the source exchange had a custom Message type,"We have a exchange with a custom MyMessage type as in message.
exchange.copy() creates an exchange with an in message of type DefaultMessage. 

Correct would be if the in message of the dest exchange would be of type MyMessage. 

I will first commit a test case that shows the problem and currently will fail.
",cschneider,cschneider,Major,Resolved,Fixed,26/Aug/11 14:42,29/Aug/11 15:51
Bug,CAMEL-4388,12520430,Exeptions cannot be propagated to the parent route when using LogEIP,"Here is unit test that demonstrates the problem.
For the unit test pass successfully it's necessary to delete LogEIP from the route.

{code}
package org.apache.camel.impl;

import org.apache.camel.Exchange;
import org.apache.camel.Processor;
import org.apache.camel.builder.RouteBuilder;
import org.apache.camel.test.junit4.CamelTestSupport;
import org.junit.Test;

public class PropagateExceptionTest extends CamelTestSupport {

    @Test
    public void failure() throws Exception {
        getMockEndpoint(""mock:handleFailure"").whenAnyExchangeReceived(new Processor() {
            @Override
            public void process(Exchange exchange) throws Exception {
                throw new RuntimeException(""TEST EXCEPTION"");
            }
        });

        getMockEndpoint(""mock:exceptionFailure"").expectedMessageCount(1);
        sendBody(""direct:startFailure"", ""Hello World"");
        assertMockEndpointsSatisfied();
    }

    @Test
    public void success() throws Exception {
        getMockEndpoint(""mock:handleSuccess"").whenAnyExchangeReceived(new Processor() {
            @Override
            public void process(Exchange exchange) throws Exception {
                throw new RuntimeException(""TEST EXCEPTION"");
            }
        });

        getMockEndpoint(""mock:exceptionSuccess"").expectedMessageCount(1);
        sendBody(""direct:startSuccess"", ""Hello World"");
        assertMockEndpointsSatisfied();
    }

    @Override
    protected RouteBuilder[] createRouteBuilders() throws Exception {
        return new RouteBuilder[] {
                new RouteBuilder() {
                    public void configure() throws Exception {
                        from(""direct:startFailure"")
                            .onException(Throwable.class)
                                .to(""mock:exceptionFailure"")
                                .end()
                            .to(""direct:handleFailure"")
                            .to(""mock:resultFailure"");

                        from(""direct:handleFailure"")
                            .errorHandler(noErrorHandler())
                            .log(""FAULTY LOG"")
                            .to(""mock:handleFailure"");
                    }
                },

                new RouteBuilder() {
                    public void configure() throws Exception {
                        from(""direct:startSuccess"")
                            .onException(Throwable.class)
                                .to(""mock:exceptionSuccess"")
                                .end()
                            .to(""direct:handleSuccess"")
                            .to(""mock:resultSuccess"");

                        from(""direct:handleSuccess"")
                            .errorHandler(noErrorHandler())
                            .to(""mock:handleSuccess"");
                    }
                }
        };
    }
}
{code}",davsclaus,szhemzhitsky,Major,Resolved,Fixed,27/Aug/11 10:52,27/Aug/11 16:31
Bug,CAMEL-4401,12520947,StreamResequencer poisoned by bad Exchange,"If a StreamResequencer's configured Expression returns null for an Exchange (e.g., if a header does not exist) and the resequencer's pipeline is currently empty, the bad Exchange will be added to the pipeline. ResequencerEnginer.insert() succeeds in calling sequence.add() with the bad Exchange, but throws an Exception when calling sequence.successor(). This results in the message ending up at the Error Handler but the bad Exchange still in the Engine's sequence. Probably insert() should be probably be more transactional.

After this happens:
1) Trying to add a further exchange (even a ""good"" one) results in an Exception when calling sequence.add().
2) ResequencerEngine.deliverNext() results in an Exception so no further messages will be delievered.

I will attach a unit test to reproduce when I get a chance.",davsclaus,dgtombs,Minor,Resolved,Fixed,31/Aug/11 17:15,14/Sep/11 09:31
Bug,CAMEL-4422,12521532,NPE when sending the SMS message less than 255.,"User complain about the NPE, when using the camel-smpp component to send the message which length is less than 255.
Here is the mail thread about it.
 http://camel.465427.n5.nabble.com/SMPP-2-8-component-null-pointers-tp4776354p4776354.html",njiang,njiang,Major,Resolved,Fixed,07/Sep/11 03:07,07/Sep/11 04:22
Bug,CAMEL-4433,12522435,Jms endpiont - option maximumBrowseSize is not in use,"The JmsEndpoint have a maximumBrowseSize option to limit the number of messages to browse back.
This option is not in use.",davsclaus,davsclaus,Minor,Resolved,Fixed,09/Sep/11 14:58,20/Sep/11 16:32
Bug,CAMEL-4436,12522539,CxfRsProducer clientFactoryBean's bus should be initialized,"This defect only manifests itself if an application hosts more than one CamelContext.  The first CamelContext contained by a Spring application context that does not have a custom bus (e.g. any http conduit configurations) while the second Spring application/Camel context does have a custom bus.

The symptom is when the application sends an message to the cxfrs endpoint in the second context (that contains custom bus), the cxfrs endpoint picks up the wrong bus.  It picks up the default cxf in the first application context.

Furthermore, this defect only affects cxfrs producer using URI address format. E.g.
  <to uri=""cxfrs://https://localhost:8181/CxfRsProducerTest/""/>

It does not affect cxfrs addresses defined as a bean.  E.g.
  <to uri=""cxfrs://bean://rsClientHttps""/>
  <cxf:rsClient id=""rsClientHttps"" address=""https://localhost:8181/CxfRsProducerTest/""/>

 
",wtam,wtam,Minor,Resolved,Fixed,10/Sep/11 03:12,10/Sep/11 16:30
Bug,CAMEL-4438,12522672,OnException using redeliveryPolicyRef from another XML file may not pickup that policy,"See nabble
http://camel.465427.n5.nabble.com/redeliveryPolicyProfile-and-onException-redeliveryPolicyRef-tp4738408p4738408.html
",davsclaus,davsclaus,Major,Resolved,Fixed,11/Sep/11 09:12,12/Sep/11 06:06
Bug,CAMEL-4439,12522701,Error in camel-restlet feature definition,"The current contents of the camel-features.xml file reads:

{code}
<feature name='camel-restlet' version='2.7.1-fuse-00-43' resolver='(obr)'>
  <feature version='2.7.1-fuse-00-43'>camel-core</feature>
  <bundle dependency=""true"">mvn:org.apache.camel/camel-restlet/2.7.1-fuse-00-43</bundle>
  <bundle>mvn:http://maven.restlet.org!org.restlet.jse/org.restlet/2.0.5</bundle>
</feature>
{code}

It actually should read http://fernandoribeiro.eti.br/2011/09/12/bug-in-fuse-4-4/ (Thanks to Fernando Ribeiro for the heads up!)",gertvanthienen,gertvanthienen,Minor,Resolved,Fixed,12/Sep/11 07:01,12/Sep/11 07:20
Bug,CAMEL-4444,12522736,Wrong slash in FTP component for doneFile,"I tried to use the doneFile with FTP component.
When I used it to my FTP on my windows machine everything goes fine.
When I tried to act the same with a FTP host on a Linux machine, I got an issue.

I sniff the packet to see what goes wrong and I see this :
the doneFile 
DELE public/Test/in\myFile.txt  
the file to process
DELE public/Test/in/myFile.xml


After a Quick look on the code:
org.apache.camel.component.file.GenericFileEndpoint 
line 682

 String answer = pattern;
        if (ObjectHelper.isNotEmpty(path) && ObjectHelper.isNotEmpty(pattern)) {
            // done file must always be in same directory as the real file name
            answer = path + File.separator + pattern;
        }

        if (getConfiguration().needToNormalize()) {
            // must normalize path to cater for Windows and other OS
            answer = FileUtil.normalizePath(answer);
        }

We use the ""File.Separator"" but in this case, it should get a kind of ""FTP.Separator"" or the FTP component should return true to the needToNormalize method and change the narmalizePath method to manage this case.




",davsclaus,oliches,Major,Resolved,Fixed,12/Sep/11 13:39,13/Sep/11 11:34
Bug,CAMEL-4446,12522919,camel-bindy - Marshaling using CSV will insert wrong char separator if using an escaped separator char such as a pipe,"When using bindy in CSV format with a separator in the model as ""\\|"" to denote a pipe character, then it does not work consistent for marshal and unmarshal. In marshal it will insert \ as separator, but it should use | instead.

See nabble
http://camel.465427.n5.nabble.com/bindy-different-behavior-between-marshal-and-unmarshal-with-pipe-separator-tp4798488p4798488.html",davsclaus,davsclaus,Minor,Resolved,Fixed,13/Sep/11 16:13,24/Jun/16 22:42
Bug,CAMEL-4447,12522947,thread safety issue with Exchange.getUnitOfWork(),"I'm having a somewhat rare issue with NullPointerExceptions in a multithreaded route. It seems that there is a race condition between one thread operating on the UnitOfWork and another removing it. I have gotten the following stacktraces so far:

{noformat}
ERROR [Camel0 - Resequencer Delivery] o.a.c.p.StreamResequencer - Caused by: [java.lang.NullPointerException - null]
java.lang.NullPointerException: null
	at org.apache.camel.processor.DefaultChannel$1.done(DefaultChannel.java:309) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:349) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:209) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:305) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:116) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:79) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:104) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:59) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.StreamResequencer.sendElement(StreamResequencer.java:165) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.StreamResequencer.sendElement(StreamResequencer.java:61) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.resequencer.ResequencerEngine.deliverNext(ResequencerEngine.java:254) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.resequencer.ResequencerEngine.deliver(ResequencerEngine.java:218) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.StreamResequencer$Delivery.run(StreamResequencer.java:212) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
{noformat}

And:

{noformat}
ERROR [Camel0 - Resequencer Delivery] o.a.c.p.StreamResequencer - Caused by: [java.lang.NullPointerException - null]
java.lang.NullPointerException: null
	at org.apache.camel.impl.DefaultExchange.isTransacted(DefaultExchange.java:331) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:57) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:116) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:79) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:104) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.Pipeline.process(Pipeline.java:59) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.StreamResequencer.sendElement(StreamResequencer.java:165) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.StreamResequencer.sendElement(StreamResequencer.java:61) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.resequencer.ResequencerEngine.deliverNext(ResequencerEngine.java:254) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.resequencer.ResequencerEngine.deliver(ResequencerEngine.java:218) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
	at org.apache.camel.processor.StreamResequencer$Delivery.run(StreamResequencer.java:212) ~[camel-core-2.8.1-SNAPSHOT.jar:2.8.1-SNAPSHOT]
{noformat}

Looking at the source, both NullPointerExceptions were thrown by code like:

{code}
return getUnitOfWork() != null && getUnitOfWork().isTransacted();
{code}

Oddly, I've only seen this happen soon after startup and only once per run.

The actual route is company confidential, but it's basically:

JMS -> Stream Resequencer -> Bean",davsclaus,dgtombs,Major,Resolved,Fixed,13/Sep/11 20:30,12/Dec/11 17:31
Bug,CAMEL-4449,12523019,NullPointerException when unmarshalling using serialization data format,"Spring configuration:
{code:xml}
    <!-- A custom CamelContext -->
    <camel:camelContext id=""camel"">
        <!-- JSON data format marshalling -->
        <camel:dataFormats>
            <camel:serialization id=""serialization""/>
        </camel:dataFormats>

        <!-- RawOffer input route -->
        <camel:route>
            <camel:from uri=""kestrel://etl01f/feeds""/>
            <camel:unmarshal ref=""serialization""/>
            <camel:to uri=""bean:shard?method=consume""/>
        </camel:route>
    </camel:camelContext>
{code}

Exception:
{code:java}
run:
     [java] log4j:WARN No appenders could be found for logger (org.springframework.context.support.ClassPathXmlApplicationContext).
     [java] log4j:WARN Please initialize the log4j system properly.
     [java] log4j:WARN See http://logging.apache.org/log4j/1.2/faq.html#noconfig for more info.
     [java] 13:28:07.587 [main] INFO  o.a.c.s.h.CamelNamespaceHandler - OSGi environment not detected.
     [java] 13:28:09.028 [main] INFO  o.a.camel.spring.SpringCamelContext - Apache Camel 2.8.0 (CamelContext: camel) is starting
     [java] 13:28:09.028 [main] INFO  o.a.camel.spring.SpringCamelContext - JMX enabled. Using ManagedManagementStrategy.
     [java] 13:28:09.415 [main] INFO  o.a.c.i.c.AnnotationTypeConverterLoader - Found 3 packages with 14 @Converter classes to load
     [java] 13:28:09.489 [main] INFO  o.a.c.i.c.DefaultTypeConverter - Loaded 153 core type converters (total 153 type converters)
     [java] 13:28:09.528 [main] INFO  o.a.c.i.c.DefaultTypeConverter - Loaded additional 0 type converters (total 153 type converters) in 0.003 seconds
     [java] 13:28:09.730 [main] INFO  o.a.c.c.kestrel.KestrelComponent - Creating endpoint for queue ""feeds"" on etl01f, parameters={}
     [java] 13:28:09.967 [main] INFO  o.a.c.c.kestrel.KestrelComponent - Creating MemcachedClient for etl01f/feeds
     [java] 2011-09-14 13:28:10.073 INFO net.spy.memcached.MemcachedConnection:  Added {QA sa=etl01f/95.108.229.218:22133, #Rops=0, #Wops=0, #iq=0, topRop=null, topWop=null, toWrite=0, interested=0} to connect queue
     [java] 2011-09-14 13:28:10.084 INFO net.spy.memcached.MemcachedConnection:  Connection state changed for sun.nio.ch.SelectionKeyImpl@711b50a1
     [java] 13:28:10.293 [main] INFO  o.a.c.c.kestrel.KestrelConsumer - Starting consumer for kestrel://etl01f/feeds
     [java] 13:28:10.302 [main] INFO  o.a.camel.spring.SpringCamelContext - Route: route1 started and consuming from: Endpoint[kestrel://etl01f/feeds]
     [java] 13:28:10.318 [main] INFO  o.a.camel.spring.SpringCamelContext - Total 1 routes, of which 1 is started.
     [java] 13:28:10.318 [main] INFO  o.a.camel.spring.SpringCamelContext - Apache Camel 2.8.0 (CamelContext: camel) started in 1.291 seconds
     [java] 13:28:12.858 [Camel (camel) thread #0 - Poller-kestrel://etl01f/feeds] ERROR o.a.c.processor.DefaultErrorHandler - Failed delivery for exchangeId: ID-incubos-osx-local-51787-1315992488896-0-1. Exhausted after delivery attempt: 1 caught: java.lang.NullPointerException
     [java] java.lang.NullPointerException: null
     [java] 	at org.apache.camel.impl.SerializationDataFormat.unmarshal(SerializationDataFormat.java:57) ~[camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.UnmarshalProcessor.process(UnmarshalProcessor.java:56) ~[camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.impl.converter.AsyncProcessorTypeConverter$ProcessorToAsyncProcessorBridge.process(AsyncProcessorTypeConverter.java:50) ~[camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:69) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99) ~[camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:318) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:209) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:305) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.Pipeline.process(Pipeline.java:116) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.Pipeline.process(Pipeline.java:79) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:102) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:69) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:104) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:85) [camel-core-2.8.0.jar:2.8.0]
     [java] 	at org.apache.camel.component.kestrel.KestrelConsumer$Poller.run(KestrelConsumer.java:230) [camel-kestrel-2.8.0.jar:2.8.0]
     [java] 	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441) [na:1.6.0_26]
     [java] 	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303) [na:1.6.0_26]
     [java] 	at java.util.concurrent.FutureTask.run(FutureTask.java:138) [na:1.6.0_26]
     [java] 	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_26]
     [java] 	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_26]
     [java] 	at java.lang.Thread.run(Thread.java:680) [na:1.6.0_26]
{code}

It seems that {{ExchangeHelper.convertToType()}} method returned {{null}} because {{camelContext.getTypeConverter()}} returned {{null}}. ",davsclaus,incubos,Major,Resolved,Fixed,14/Sep/11 09:34,17/Sep/11 19:26
Bug,CAMEL-4452,12523162,CXFConsumer may extract the request message as the response message and this can lead to problems,"CAMEL-4030 with Revision 1129070 in trunk changed the way how the response message is retrieved from the exchange and this is causing some issue.

In particular, the changed code may retrieve the request message as the response message when the call is oneway (when the condition camelExchange.getPattern().isOutCapable() is false).

Subsequently, this is leading to an NPE when the output operation is used to extract the payload body from this request message because there is no output operations in the oneway case at:

        for (MessagePartInfo partInfo : boi.getOutput().getMessageParts()) {

and resulting in:

java.lang.NullPointerException
	at org.apache.camel.component.cxf.DefaultCxfBinding.getResponsePayloadList(DefaultCxfBinding.java:394)
	at org.apache.camel.component.cxf.DefaultCxfBinding.populateCxfResponseFromExchange(DefaultCxfBinding.java:318)
	at org.apache.camel.component.cxf.CxfConsumer$1.setResponseBack(CxfConsumer.java:176)
	at org.apache.camel.component.cxf.CxfConsumer$1.syncInvoke(CxfConsumer.java:126)
	at org.apache.camel.component.cxf.CxfConsumer$1.invoke(CxfConsumer.java:71)
	at org.apache.cxf.interceptor.ServiceInvokerInterceptor$1.run(ServiceInvokerInterceptor.java:58)
	at java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:441)
	at java.util.concurrent.FutureTask$Sync.innerRun(FutureTask.java:303)
	at java.util.concurrent.FutureTask.run(FutureTask.java:138)
	at org.apache.cxf.workqueue.SynchronousExecutor.execute(SynchronousExecutor.java:37)
	at org.apache.cxf.interceptor.ServiceInvokerInterceptor.handleMessage(ServiceInvokerInterceptor.java:106)
	at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:263)
	at org.apache.cxf.phase.PhaseInterceptorChain.resume(PhaseInterceptorChain.java:232)
	at org.apache.cxf.interceptor.OneWayProcessorInterceptor$1.run(OneWayProcessorInterceptor.java:130)
	at org.apache.cxf.workqueue.AutomaticWorkQueueImpl$2.run(AutomaticWorkQueueImpl.java:353)
	at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
	at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
	at java.lang.Thread.run(Thread.java:662)


I see this change was introduced with CAMEL-4030 to support some sort of wire-tap short-cut:

from(""cxf:xxx"").inonly(""jms:xxx"").to(""xxx"")

I am not sure how this inbound/outbound switching operation relates to this use case.

But in any case, this new behavior can lead to this problem and  I think the old behavior (skipping the response message part if there is no response) should be reinstated.

I have a simple test case that can reproduce this problem, but the exception is thrown in an executor thread and only written to the log and the original test caller thread doesn't see the exception. So, it's not a useful automatic test case. Maybe, there is a way. Let me know, how you think.

thanks.
regards, aki


",njiang,ay,Major,Resolved,Fixed,15/Sep/11 08:03,16/Sep/11 11:39
Bug,CAMEL-4459,12523384,CAMEL-4407 has caused a regression in the features.xml,"
The scriptengine things that were added as part of CAMEL-4407 have made doing a features:install camel-script in a ""plain"" Karaf instance not work as those dependencies are not available in Central.   Thus, setups that worked in the past that did not need those no longer work.

Propose moving those three deps to a ""camel-script-optional"" feature that would not be pulled in by default.   If a user needs them, they would need to configure Karaf to pull from another repo.

",dkulp,dkulp,Major,Resolved,Fixed,16/Sep/11 20:09,17/Sep/11 04:30
Bug,CAMEL-4460,12523421,XStreamDataFormat has NPE when marshalling route to XML,"You can get a NPE when marshalling a route to XML which has <xstream> in the route

Caused by: java.lang.NullPointerException
	at org.apache.camel.model.dataformat.XStreamDataFormat$ConvertersAdapter.marshal(XStreamDataFormat.java:170)
	at org.apache.camel.model.dataformat.XStreamDataFormat$ConvertersAdapter.marshal(XStreamDataFormat.java:165)
	at com.sun.xml.internal.bind.v2.runtime.reflect.AdaptedAccessor.get(AdaptedAccessor.java:61)
	... 55 more",davsclaus,davsclaus,Minor,Resolved,Fixed,17/Sep/11 10:04,17/Sep/11 10:36
Bug,CAMEL-4467,12523700,LifecycleStrategy should be started/stopped when CamelContext is starting/stopping,"The LifecycleStrategy strategies is not start/stopped if they are a Service, such as the DefaultManagementLifecycleStrategy",davsclaus,davsclaus,Minor,Resolved,Fixed,20/Sep/11 12:09,20/Sep/11 12:16
Bug,CAMEL-4472,12523851,HazelcastComponentHelper. copyHeaders() creates an Out message,"Due to lazy creation of Out message in DefaultExchange.getOut() implementation, HazelcastComponentHelper.copyHeaders() method inadvertently creates an Out message if one has not been set yet, which then creates problems in CamelInvocationHandler.getBody() which then returns null as a result. 

There should be a check in copyHeaders() to see if Out exists, e.g.: 

{code}
// set out headers 
if (ex.hasOut()) { 
    ex.getOut().setHeaders(headers); 
} 
{code}",davsclaus,tmrkus,Major,Resolved,Fixed,21/Sep/11 12:05,21/Sep/11 15:52
Bug,CAMEL-4473,12523857,Unable to use cxf:binding configuration for the endpoint,"Caused by: org.springframework.beans.factory.BeanCreationException: Error creating bean with name 'service': Error setting property val
ues; nested exception is org.springframework.beans.NotWritablePropertyException: Invalid property 'bindingConfig' of bean class [org.ap
ache.camel.component.cxf.CxfSpringEndpoint]: Bean property 'bindingConfig' is not writable or has an invalid setter method. Does the pa
rameter type of the setter match the return type of the getter?
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBean
Factory.java:1361)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.populateBean(AbstractAutowireCapableBeanFactory
.java:1086)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.doCreateBean(AbstractAutowireCapableBeanFactory
.java:517)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.createBean(AbstractAutowireCapableBeanFactory.j
ava:456)
        at org.springframework.beans.factory.support.AbstractBeanFactory$1.getObject(AbstractBeanFactory.java:291)
        at org.springframework.beans.factory.support.DefaultSingletonBeanRegistry.getSingleton(DefaultSingletonBeanRegistry.java:222)
        at org.springframework.beans.factory.support.AbstractBeanFactory.doGetBean(AbstractBeanFactory.java:288)
        at org.springframework.beans.factory.support.AbstractBeanFactory.getBean(AbstractBeanFactory.java:190)
        at org.springframework.beans.factory.support.DefaultListableBeanFactory.preInstantiateSingletons(DefaultListableBeanFactory.jav
a:580)
        at org.springframework.context.support.AbstractApplicationContext.finishBeanFactoryInitialization(AbstractApplicationContext.ja
va:895)
        at org.springframework.context.support.AbstractApplicationContext.refresh(AbstractApplicationContext.java:425)
        at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:84)
        at org.springframework.test.context.support.AbstractGenericContextLoader.loadContext(AbstractGenericContextLoader.java:1)
        at org.springframework.test.context.TestContext.loadApplicationContext(TestContext.java:280)
        at org.springframework.test.context.TestContext.getApplicationContext(TestContext.java:304)
        ... 30 more
Caused by: org.springframework.beans.NotWritablePropertyException: Invalid property 'bindingConfig' of bean class [org.apache.camel.com
ponent.cxf.CxfSpringEndpoint]: Bean property 'bindingConfig' is not writable or has an invalid setter method. Does the parameter type o
f the setter match the return type of the getter?
        at org.springframework.beans.BeanWrapperImpl.setPropertyValue(BeanWrapperImpl.java:1024)
        at org.springframework.beans.BeanWrapperImpl.setPropertyValue(BeanWrapperImpl.java:900)
        at org.springframework.beans.AbstractPropertyAccessor.setPropertyValues(AbstractPropertyAccessor.java:76)
        at org.springframework.beans.AbstractPropertyAccessor.setPropertyValues(AbstractPropertyAccessor.java:58)
        at org.springframework.beans.factory.support.AbstractAutowireCapableBeanFactory.applyPropertyValues(AbstractAutowireCapableBean
Factory.java:1358)
        ... 44 more",njiang,szhemzhitsky,Major,Resolved,Fixed,21/Sep/11 12:54,22/Sep/11 07:05
Bug,CAMEL-4474,12523860,file: consumer does not create directory,"According to http://camel.apache.org/file2.html autoCreate is true by default and should for a consumer create the directory.
{noformat}
autoCreate 	true 	Automatically create missing directories in the file's pathname. For the file consumer, that means creating the starting directory. For the file producer, it means the directory the files should be written to. 
{noformat}
This does not happen and thus a route startup would fail.",taariql,davidkarlsen@gmail.com,Major,Closed,Fixed,21/Sep/11 13:04,25/Oct/11 11:35
Bug,CAMEL-4476,12524221,Camel splitter/aggregator 2x slower in 2.8.1 vs 2.7.3,"I noticed a sharp decline in the performance of my application when I upgraded from Camel 2.7.3 to 2.8.1. I discovered that the splitter/aggregator had become the bottleneck in my app. I have created a sample app that demonstrates the issue. The aggregator uses a lock, and I suspect lock contention may be at the root of this, but didn't notice any changes around that in the code diff between the versions.

I uploaded the sample app to http://www.vancameron.net/CamelPerf.zip. It uses gradle. Unzip to a directory then run ""gradle"" from the command line. To change the version from 2.8.1 to 2.7.3, edit the build.gradle file and run again.

The app loads 10 files with 100k lines each. It prints the current time in msec per batch.
",davsclaus,bryanck,Major,Resolved,Fixed,22/Sep/11 00:12,22/Sep/11 13:42
Bug,CAMEL-4477,12524271,"parseURI in org.apache.camel.component.printer.PrinterConfiguration leaves a prefixed ""/"" on the printername attribute which should be removed","in PrinterConfiguration.parseURI(), the line:

setPrintername(uri.getPath());

ends up leaving the starting ""/"" which is not really a part of the printer's name. When the PrinterProducer later searches for the matching PrintService, it attempts to recreate the printer path from the host name and the printer name with the following:

setPrinter(""\\\\"" + config.getHostname() + ""\\"" + config.getPrintername());

which ends up with a string like: \\<HostName>\/<printer> because the leading ""/"" was not removed from the printer name, and which leads to the printer never being found.",davsclaus,denis@denisrobert.net,Major,Closed,Fixed,22/Sep/11 12:10,25/Oct/11 11:36
Bug,CAMEL-4478,12524275,"Because local PrintService objects on Windows have a name which is not their UNC path, the PrinterProducer can never find them.","In org.apache.camel.component.printer.PrinterProducer, the line:

setPrinter(""\\\\"" + config.getHostname() + ""\\"" + config.getPrintername());

reconstructs a UNC path which is expected to match the name of a PrintService on the system. For local printers on Windows, the name of the PrintService is simply the name of the printer without the host and any backslashes, so the PrinterProducer is never able to find them.

So when config.getHostname() is ""localhost"", at least on Windows, the line should be simply:

setPrinter(config.getPrintername())

so that we'd have code like:

if (""localhost"".equalsIgnoreCase(config.getHostname())) {
  setPrinter(config.getPrintername());
} else {
  setPrinter(""\\\\"" + config.getHostname() + ""\\"" + config.getPrintername());
}

This can only work if issue CAMEL-4477 about the error in parseURI() in PrinterConfiguration is also fixed.
",davsclaus,denis@denisrobert.net,Major,Closed,Fixed,22/Sep/11 12:17,25/Oct/11 11:35
Bug,CAMEL-4480,12524405,"RuntimeCamelException ""Ambiguous Parameter Mapping"" must not be thrown, if there are parameters of the same type but with different QNames in ServiceInterfaceStrategy","In different WebMethods I have parameters of the same type, but with different QNames. The following if-clause (in ServiceInterfaceStrategy.analyzeServiceInterface(..)) throws an exeption in these cases (thrown if NOT QNameOfMethod1 equals QNameOfMethod2), but it should on the contrary throw an exception if the QNames ARE equal:

                if (inTypeNameToQName.containsKey(ti.getTypeName())
                    && (!(ti.getTypeName().equals(""javax.xml.ws.Holder"")))
                    && (!(inTypeNameToQName.get(ti.getTypeName()).equals(ti.getElName())))) {
                    throw new RuntimeCamelException(""Ambiguous parameter mapping. The type [ ""
                                                    + ti.getTypeName()
                                                    + "" ] is already mapped to a QName in this context."");
                }

The ""NOT"" must be deleted:

...
&& (inTypeNameToQName.get(ti.getTypeName()).equals(ti.getElName())))
...
",njiang,ojelinski,Major,Closed,Fixed,23/Sep/11 11:58,26/Oct/11 17:02
Bug,CAMEL-4482,12524509,"Using custom expression in Splitter EIP which throws exception, is not triggering onException","See nabble
http://camel.465427.n5.nabble.com/Global-exception-not-invoked-in-case-of-Exception-fired-while-iterating-through-File-Splitter-td4826097.html

We should detect exceptions occurred during evaluation of the expression, and then cause the splitter EIP to fail asap.",davsclaus,davsclaus,Major,Resolved,Fixed,24/Sep/11 07:49,27/Nov/11 14:25
Bug,CAMEL-4484,12524512,"Using custom expression in RecipientList EIP which throws exception, is not triggering onException",Similar issue as CAMEL-4482 but for the recipient list EIP,davsclaus,davsclaus,Major,Resolved,Fixed,24/Sep/11 11:07,08/Feb/12 06:01
Bug,CAMEL-4486,12524543,Exceptions are not propagated to the parent route when endpoint cannot be resolved in the RoutingSlip EIP,"Here is the unit test to reproduce the issue

{code}
package org.test;

import org.apache.camel.builder.RouteBuilder;
import org.apache.camel.component.mock.MockEndpoint;
import org.apache.camel.test.junit4.CamelTestSupport;
import org.junit.Test;

public class RecipientListTest extends CamelTestSupport {

    public static class Router {
        public String findEndpoint() {
            return ""unresolved://endpoint"";
        }
    }

    @Test
    public void recipientList() throws Exception {
        MockEndpoint endpoint = getMockEndpoint(""mock://error"");
        endpoint.expectedMessageCount(1);

        sendBody(""direct://parent"", ""Hello World!"");

        assertMockEndpointsSatisfied();
    }

    @Override
    protected RouteBuilder createRouteBuilder() throws Exception {
        return new RouteBuilder() {
            @Override
            public void configure() throws Exception {
                from(""direct://parent"")
                    .onException(Throwable.class)
                        .to(""mock://error"")
                    .end()
                    .to(""direct://child"");

                from(""direct://child"")
                    .errorHandler(noErrorHandler())
                    .routingSlip(bean(Router.class));
            }
        };
    }

}
{code}",davsclaus,szhemzhitsky,Major,Resolved,Fixed,25/Sep/11 00:12,25/Sep/11 17:39
Bug,CAMEL-4487,12524545,MockEndpoint should reset defaultProcessor,Thanks for the patch. I added unit test as well.,davsclaus,szhemzhitsky,Minor,Closed,Fixed,25/Sep/11 00:56,25/Oct/11 11:36
Bug,CAMEL-4489,12524630,camel-http4 sends out a 'http4' GET request,"When I try to do : from(""http4://www.google.com/"").to(""mock:results"") using apache camel-http4 scheme, a GET request is being sent out with 'http4' as the protocol - my proxy server fails to understand this as a protocol. 

This is what the log looks like: 

  DEBUG [Camel (camel-1) thread #0 - http4://www.google.com/] (RequestAddCookies.java132) - CookieSpec selected: best-match
  DEBUG [Camel (camel-1) thread #0 - http4://www.google.com/] (RequestAuthCache.java75) - Auth cache not set in the context
  DEBUG [Camel (camel-1) thread #0 - http4://www.google.com/] (DefaultRequestDirector.java643) - Attempt 1 to execute request
  DEBUG [Camel (camel-1) thread #0 - http4://www.google.com/] (DefaultClientConnection.java264) - Sending request: GET http4://www.google.com/ HTTP/1.1
  DEBUG [Camel (camel-1) thread #0 - http4://www.google.com/] (Wire.java63) - >> ""GET http4://www.google.com/ HTTP/1.1[\r][\n]""
  DEBUG [Camel (camel-1) thread #0 - http4://www.google.com/] (Wire.java63) - >> ""Host: www.google.com[\r][\n]""
  DEBUG [Camel (camel-1) thread #0 - http4://www.google.com/] (Wire.java63) - >> ""Proxy-Connection: Keep-Alive[\r][\n]""
  DEBUG [Camel (camel-1) thread #0 - http4://www.google.com/] (Wire.java63) - >> ""[\r][\n]""
  DEBUG [Camel (camel-1) thread #0 - http4://www.google.com/] (DefaultClientConnection.java268) - >> GET http4://www.google.com/ HTTP/1.1
  DEBUG [Camel (camel-1) thread #0 - http4://www.google.com/] (DefaultClientConnection.java271) - >> Host: www.google.com
  DEBUG [Camel (camel-1) thread #0 - http4://www.google.com/] (DefaultClientConnection.java271) - >> Proxy-Connection: Keep-Alive
  DEBUG [Camel (camel-1) thread #0 - http4://www.google.com/] (Wire.java63) - << ""HTTP/1.1 502 Proxy Error 



",njiang,fedexp,Critical,Closed,Fixed,26/Sep/11 13:23,25/Oct/11 11:35
Bug,CAMEL-4491,12524817,dataCoding not working properly in camel-smpp,"This is related to CAMEL-3093.

The way the dataCoding option works is incorrect (or at least very confusing). After reading the documentation I was under the impression that the given value would be used in the data_coding-part of the submit_sm PDU-packets, but this is not the case. When 0 is given as dataCoding 0x11 (hexadecimal) is sent in the PDU-packets. If 4 is specified 0x15 is sent, and if 8 is specified then 0x19 is sent. This is caused in SmppProducer by the use of the constructor with several parameters:
{code}
new GeneralDataCoding(
        false,
        true,
        MessageClass.CLASS1,
        Alphabet.valueOf(submitSm.getDataCoding()))
{code}
It constructs a DCS-value as defined in the GSM 03.38 specification, not as specified in the SMPP specification. 0x11 is for example a valid DCS-value in GSM 03.38, but is reserved in SMPP. I think that it would be better and more logical if the constructor with one parameter was used instead:
{code}
new GeneralDataCoding(submitSm.getDataCoding())
{code}

This is more flexibal (DCS-values can still be created manually) and is a better default value (0 instead of 0x11). With my SMS-center (this probably varies between centers though) a data_coding of 0x11 forces me to do the 7-bit GSM decoding myself. When 0 is specified (or 3) all I have to do is to pass it ISO-8859-1 encoded bytes.",muellerc,ljb,Major,Closed,Fixed,27/Sep/11 08:14,25/Oct/11 13:11
Bug,CAMEL-4503,12525193,Recipient List cannot vary destination by changing property in an expression,"This is probably due to producers being cached based on unresolved endpoint URI, rather than the resolved endpoint URI. Haven't tested this theory out though.

See message on user list http://markmail.org/message/sdysp6sspq5lomf4",njiang,janstey,Major,Resolved,Fixed,29/Sep/11 14:50,21/Oct/11 09:42
Bug,CAMEL-4509,12525539,Header not set after dead letter queue handles unmarshal error,"We have a route which unmarshals a soap msg into an object.  On that route is a dead letter queue error handler.  That DLQ sets headers on the message used later for error reporting.

If the error is thrown by the marshaller, the *first header* that we try to set is wiped out.  The 2nd header is set with no problem.  If an error is thrown by something other than the marshaller, the correct headers are set.

See attached project with failed test case (canSetHeadersOnBadXmlDeadLetter)",davsclaus,roytruelove@gmail.com,Major,Resolved,Fixed,03/Oct/11 15:37,05/Oct/11 09:46
Bug,CAMEL-4510,12525601,Camel flatpack component drops all headers from the original message,"In the simple example:

{code}
from(""file:/foo.csv"")
   .setHeader(""MyHeader"", constant(""Hello""))
   .to(""flatpack:foo"");

from(""flatpack:foo"")
   .to(""bean:importer"");
{code}

Header ""MyHeader"" is not being passed to the bean in the second route.

Looking at Camel Flatpack code I see that in {{FixedLengthEndpoint.processDataSet}} it creates an exchange object from scratch and, thus, looses any incoming headers.",muellerc,dragisak,Major,Closed,Fixed,03/Oct/11 23:30,23/Oct/11 10:28
Bug,CAMEL-4513,12525682,simple predicate fails to introspect the exception in an onException clause using onWhen,"The bug occured in the 2.6.0 version of Camel I'm using. I haven't test it against the latest version but I've checked the sources and it doesn't seem to have change since.

Given a camel route, with a onException clause like this :

{code}
this.onException(MyException.class)
    .onWhen(simple(""${exception.myExceptionInfo.aValue} == true""))
    ...
{code}

MyException is a customed exception like this :

{code:title=MyException.java}
public class MyException extends Exception {
   ....
   public MyExceptionInfo getMyExceptionInfo() {
     ...
   }
}
{code}

What I've observed is that when BeanExpression.OgnlInvokeProcessor.process iterate through the methods to calls, it does :
{code}
                // only invoke if we have a method name to use to invoke
                if (methodName != null) {
                    InvokeProcessor invoke = new InvokeProcessor(holder, methodName);
                    invoke.process(resultExchange);

                    // check for exception and rethrow if we failed
                    if (resultExchange.getException() != null) {
                        throw new RuntimeBeanExpressionException(exchange, beanName, methodName, resultExchange.getException());
                    }

                    result = invoke.getResult();
                }
{code}

It successfully invoke the method : invoke.process(resultExchange);
But it checks for exception in the exchange. Since we are in an exception clause, there is an actual exception (thrown by the application, but unrelated with the expression language search) and it fails

There is a simple workaround for that : writing his own predicate class to test wanted conditions",davsclaus,thomasgueze,Minor,Resolved,Fixed,04/Oct/11 15:04,10/Apr/12 11:56
Bug,CAMEL-4526,12526324,HttpProduder should not ignore the query part of HTTP_URI header,"Here is the mail thread discusses about it.
http://camel.465427.n5.nabble.com/Query-part-of-HTTP-URI-header-ignored-td4880040.html",njiang,njiang,Major,Resolved,Fixed,08/Oct/11 13:43,10/Oct/11 13:44
Bug,CAMEL-4530,12526371,"camel-jms - When useMessageIDAsCorrelationID an timeout occurs due no reply message, then old correlationID is not evicted from dynamic reply selector","If you use both fixed reply queues and have useMessageIDAsCorrelationID=true, then the dynamic MessageSelectorCreator may not evict the correlationID causing the JMSMessage selector to keep growing.",davsclaus,davsclaus,Major,Closed,Fixed,09/Oct/11 12:21,25/Oct/11 11:36
Bug,CAMEL-4533,12526511,NPE thrown when connection is null loses root cause,"When attempting to run a query against a datasource to which a connection could not be established, the attempt to rollback the transaction (which has not yet started) causes an NPE to be thrown which loses the root cause exception thrown by the driver",jbonofre,stu.c,Minor,Resolved,Fixed,10/Oct/11 20:34,12/Oct/11 07:48
Bug,CAMEL-4534,12526575,Bad endpoint in examples in documentation JMS Component,"Adress: http://camel.apache.org/jms.html

In section: ""Request-reply over JMS and using a shared fixed reply queue""
is: .inOut().to(""activemq:queue:foo?replyTo=bar?receiveTimeout=250"")
should be: .inOut().to(""activemq:queue:foo?replyTo=bar&receiveTimeout=250"")

The same in ""Request-reply over JMS and using an exclusive fixed reply queue""",iocanel,kborgul,Trivial,Resolved,Fixed,11/Oct/11 08:24,11/Oct/11 13:41
Bug,CAMEL-4536,12526679,Using AuthorizationPolicy on a Route prevents Processors from being exposed via JMX,"Using AuthorizationPolicy on a route (e.g., using .policy(myAuthPolicy) in a Java DSL) prevents that processors on this route are exposed via JMX. 

Steps to reproduce:

-) Start the Camel app in the attached test case (MyRouteBuilder)
-) Open JConsole
-) Connect to the corresponding local process
-) Under ""processors"" only the processors from the route without the policy are shown, but not the ones from the route where a policy is used",davsclaus,marcozapletal,Minor,Resolved,Fixed,11/Oct/11 15:51,23/Oct/11 13:37
Bug,CAMEL-4540,12526884,Unable to convert jms textmessage to bytemessage,"When receiving a jms textmessage and just forward it to a another queue as a jms bytemessage the original textmessage is sendt without being converted to a jms bytemessage. 

JmsBinding.makeJmsMessage seems to use the exchange headers to determine of a new jms message should be created or the original send.
Exchange headers seems to be populated if you do routing or set a dummy header.

See http://camel.465427.n5.nabble.com/Convert-jms-message-from-textmessage-to-bytemessage-tc4895362.html

/preben",davsclaus,preben,Major,Resolved,Fixed,12/Oct/11 18:00,15/Oct/11 10:57
Bug,CAMEL-4542,12526990,"Can't find splitter bean in registry using multiple camel contexts with ""vm"" endpoint","The splitter component can use a bean with a ""split method"". It seems that this ""split bean"" is handled as expression and resolved lately using Camel Context from current exchange.

If I send an exchange using a separate CamelContext (""client"")

<camelContext id=""client"" xmlns=""http://camel.apache.org/schema/spring"">
</camelContext>

to a route defined in another CamelContext (""server"") using in-memory transport like ""direct"" or ""vm""

<camelContext id=""server"" xmlns=""http://camel.apache.org/schema/spring"">

   <route id=""route02"" trace=""false"" streamCache=""false"">
     <from uri=""vm:route02""/>
     <split>
       <method bean   =""stringLineSplitter"" method=""split""/>
       <log    message=""before sending: ${body}""/>
       <inOut  uri    =""vm:route04""/>
       <log    message=""after sending""/>
     </split>
     <to uri=""mock:route02""/>
   </route>

</camelContext>

the test fails with 

""Cannot find class: stringLineSplitter"" (Camel 2.8.0). 
""org.apache.camel.NoSuchBeanException - No bean could be found in the registry for: stringLineSplitter"" (Camel 2.9-SNAPSHOT)

If I understood Camel right it fails
because it tries to resolve this bean based on client Camel Context
which is still set at the current exchange send from ""client"" to ""server"" but it
doesn't contain the bean.

If I send an exchange using same ""client"" CamelContext to another route in
""server"" CamelContext involving ""external"" components like ""jms"" (ActiveMQ)

<camelContext id=""server"" xmlns=""http://camel.apache.org/schema/spring"">

   <route id=""route03"" trace=""false"" streamCache=""false"">
     <from uri=""jms:queue:route03""/>
     <split>
       <method bean   =""stringLineSplitter"" method=""split""/>
       <log    message=""before sending: ${body}""/>
       <inOut  uri    =""vm:route04""/>
       <log    message=""after sending""/>
     </split>
     <to uri=""mock:route03""/>
   </route>

</camelContext>

the test passed successfully. It seems that ""jms"" component creates a
new exchange using ""server"" CamelContext.
",njiang,berndfischer63,Major,Resolved,Fixed,13/Oct/11 08:38,14/Nov/11 15:35
Bug,CAMEL-4545,12527191,"[WebConsole Archetype] Unable to run with ""mvn jetty:run""","If you install the camel-archetype-webconsole from the trunk (2.9-SNAPSHOT actually), and create a new project with it (""mvn archetype:generate -DarchetypeGroupId=org.apache.camel.archetypes -DarchetypeArtifactId=camel-archetype-webconsole -DarchetypeVersion=2.9-SNAPSHOT""), you can't run ""mvn jetty:run"".
There are conflicts with slf4j-api binding as you can see in the following logs

{code:xml} 
...
[main] ContextLoader ERROR Context initialization failed
java.lang.NoSuchMethodError: org.slf4j.helpers.MessageFormatter.format(Ljava/lang/String;Ljava/lang/Object;)Lorg/slf4j/helpers/FormattingTuple;
        at org.slf4j.impl.Log4jLoggerAdapter.info(Log4jLoggerAdapter.java:323)[slf4j-log4j12-1.6.1.jar:1.6.1]
...
{code} 

Running ""mvn dependencies:tree"" show that two versions of slf4j are loaded.

You can find with this issue a patch that exclude slf4j for activemq-camel, which solve the problem. 

But you then ran in the following problem

{code:xml}
... 
java.net.URISyntaxException: Illegal character in opaque part at index 22: jar:file:/mypath/.m2/repository/org/apache/camel/camel-web/2.9-SNAPSHOT/camel-web-2.9-SNAPSHOT.war!/WEB-INF/classes/
...
{code} 

Which can be solved by updating jetty-version to a more recent one (7.5.3.v20111011) in the parent pom ( issue [CAMEL-4544|https://issues.apache.org/jira/browse/CAMEL-4544] with patch)
",njiang,cexbrayat,Major,Resolved,Fixed,14/Oct/11 13:08,19/Oct/11 12:55
Bug,CAMEL-4553,12527443,camel-cxf endpoint should be able to determine soap version from incoming message,"more details from
http://camel.465427.n5.nabble.com/Camel-CXF-component-SOAP-1-1-amp-1-2-support-td4909456.html",ffang,ffang,Major,Resolved,Fixed,17/Oct/11 12:43,20/Oct/11 10:02
Bug,CAMEL-4556,12527524,NettyProducer creating new connection on every message,"Using a NettyProducer without the disconnect=true configuration is causing the route to block after 10 messages on the to(""netty://tcp...."") call.

It appears that a new socket connection is created for every message, and then after 10 connections no new connection is allowed (must be a default thread pool limit?).

Using the disconnect=true option fixes the problem as a socket is connected, message sent, then disconnected. But this does not seem viable for implementations where that overhead is undesirable or where more than one response is expected on a channel.

--

This is a small Unit Test that shows the problem (http://camel.465427.n5.nabble.com/Camel-Netty-Producer-creating-new-connection-on-every-message-td4844805.html#none) 



package netty; 

import java.util.Arrays; 
import java.util.Collection; 
import java.util.concurrent.TimeUnit; 
import java.util.concurrent.atomic.AtomicBoolean; 
import java.util.concurrent.atomic.AtomicInteger; 

import junit.framework.TestCase; 

import org.apache.camel.CamelContext; 
import org.apache.camel.Exchange; 
import org.apache.camel.ExchangePattern; 
import org.apache.camel.Processor; 
import org.apache.camel.builder.RouteBuilder; 
import org.apache.camel.impl.DefaultCamelContext; 
import org.junit.Before; 
import org.junit.BeforeClass; 
import org.junit.Test; 
import org.junit.runner.RunWith; 
import org.junit.runners.Parameterized; 
import org.junit.runners.Parameterized.Parameters; 
import org.slf4j.Logger; 
import org.slf4j.LoggerFactory; 

@RunWith(Parameterized.class) 
public class NettyTest extends TestCase 
{ 
    private final static Logger logger = LoggerFactory.getLogger(NettyTest.class); 
    private final static CamelContext serverContext = new DefaultCamelContext(); 

    private final CamelContext clientContext = new DefaultCamelContext(); 
    private final AtomicInteger responseCounter = new AtomicInteger(0); 
    private final AtomicBoolean passedTen = new AtomicBoolean(false); 

    private Boolean disconnectClient; 

    public NettyTest(Boolean disconnectClient) 
    { 
        this.disconnectClient = disconnectClient; 
    } 

    @Parameters 
    public static Collection<Object[]> configs() 
    { 
        return Arrays.asList(new Object[][] { { true }, { false } }); 
    } 

    @BeforeClass 
    public static void createServer() throws Exception 
    { 
        serverContext.addRoutes(new RouteBuilder() 
        { 
            @Override 
            public void configure() throws Exception 
            { 
                from(""netty:tcp://localhost:9000?sync=true&disconnectOnNoReply=false&allowDefaultCodec=true&tcpNoDelay=true&reuseAddress=true&keepAlive=false"") 
                        .setExchangePattern(ExchangePattern.InOut) 
                        .process(new Processor() { 

                            @Override 
                            public void process(Exchange exchange) throws Exception 
                            { 
                                Object body = exchange.getIn().getBody(); 
                                logger.info(""Request received : Value = {}"", body); 
                            } 
                            
                        }) 
                        .transform(constant(3)).stop(); 
            } 
        }); 

        serverContext.start(); 
    } 

    @Before 
    public void createClient() throws Exception 
    { 
        clientContext.addRoutes(new RouteBuilder() 
        { 
            @Override 
            public void configure() throws Exception 
            { 
                // Generate an Echo message and ensure a Response is sent 
                from(""timer://echoTimer?delay=1s&fixedRate=true&period=1s"") 
                        .setExchangePattern(ExchangePattern.InOut) 
                        .transform() 
                        .constant(2) 
                        .to(ExchangePattern.InOut, ""netty:tcp://localhost:9000?allowDefaultCodec=true&tcpNoDelay=true&reuseAddress=true&keepAlive=false&sync=true&disconnect="" + disconnectClient.toString()) 
                        .process(new Processor() 
                        { 
                            @Override 
                            public void process(Exchange exchange) throws Exception 
                            { 
                                Object body = exchange.getIn().getBody(); 
                                logger.info(""Response number {} : Value = {}"", 
                                        responseCounter.incrementAndGet(), body); 

                                if (responseCounter.get() > 10) { 
                                    passedTen.set(true); 
                                } 
                            } 

                        }).stop(); 
            } 
        }); 
    } 

    @Test 
    public void test() throws Exception 
    { 
        clientContext.getShutdownStrategy().setTimeout(1); 

        clientContext.start(); 

        logger.info(""Disconnect = {}"", this.disconnectClient); 

        Thread.sleep(TimeUnit.SECONDS.toMillis(15)); 

        clientContext.stop(); 

        assertTrue(""More than 10 responses have been received"", passedTen.get()); 
    } 
} ",davsclaus,maccamlc,Minor,Resolved,Fixed,18/Oct/11 00:49,08/Apr/12 12:55
Bug,CAMEL-4557,12527546,camel-archerype-blueprint creates the blueprint file with a wrong name space location,,cmoulliard,cmoulliard,Major,Resolved,Fixed,18/Oct/11 06:26,18/Oct/11 07:40
Bug,CAMEL-4559,12527718,Need to specify the sl4j-api version in the camel maven archetypes,"There are some third part dependencies will introduce other version of sl4j-api as CAMEL-4545 shows, we need to specify the sl4j-api in the pom.xml resource.",njiang,njiang,Major,Resolved,Fixed,19/Oct/11 09:36,19/Oct/11 12:54
Bug,CAMEL-4560,12527722,camel-file component does not close a file when it's used with camel-xslt,"Here are the steps to reproduce:

# Create the directory target/in.
# Put the file ""file.xml"" into the directory target/in. Note: the end tag in this file is not closed properly.
# Run the test.
# The test should fail.
# The file ""file.xml"" remains in the target/in but it should not.

{code:java|title=FileLockedTest.java}
package org.apache.camel;

import org.apache.camel.builder.RouteBuilder;
import org.apache.camel.component.mock.MockEndpoint;
import org.apache.camel.test.junit4.CamelTestSupport;
import org.junit.Test;

public class FileLockedTest extends CamelTestSupport {

    @Test
    public void testFileLocked() throws Exception {
        MockEndpoint result = getMockEndpoint(""mock:result"");
        result.setAssertPeriod(10000);
        result.expectedMessageCount(1);
        result.assertIsSatisfied();
    }

    @Override
    protected RouteBuilder createRouteBuilder() throws Exception {
        return new RouteBuilder() {
            @Override
            public void configure() throws Exception {
                from(""file:target/in?delay=100&moveFailed=.error"")
                    .onException(Throwable.class)
                        .to(""mock:result"")
                    .end()
                    .to(""xslt:FileLockedTest.xsl"");
            }
        };
    }

}
{code}

{code:xml|title=FileLockedTest.xsl}
<?xml version=""1.0"" encoding=""UTF-8""?>
<xsl:stylesheet
    xmlns:xsl=""http://www.w3.org/1999/XSL/Transform""
    xmlns=""http://schemas.computershare.ru/codex/2011/08/""
    version=""1.0"">

    <xsl:output method=""xml"" encoding=""UTF-8"" omit-xml-declaration=""yes"" />
    <xsl:preserve-space elements=""*""/>

    <xsl:template match=""/"">
        <xsl:apply-templates />
    </xsl:template>

    <xsl:template match=""@* | node()"">
        <xsl:copy>
            <xsl:apply-templates select=""@* | node()"" />
        </xsl:copy>
    </xsl:template>

</xsl:stylesheet>
{code}
{code:xml|title=file.xml; Note: the end tag is not closed properly}
<root></root
{code}",davsclaus,szhemzhitsky,Major,Resolved,Fixed,19/Oct/11 10:04,21/Oct/11 18:31
Bug,CAMEL-4561,12527878,CxfComponent should create a new CxfEndpoint instance if the instance is lookup from configuration registry,"CAMEL-4503 shows the bug, when we have uri parameters which can override the configuration of cxfEndpoint, the instance of cxfEndpoint from the configuration file could be changed with the uri parameters if the CxfComponnet doesn't return a new instance of the cxfEndpoint.",njiang,njiang,Major,Resolved,Fixed,20/Oct/11 06:11,20/Oct/11 15:02
Bug,CAMEL-4568,12528139,SLF4JLog does not work with JdbcDriver for Quickfix/J,"I discovered that to use SLF4J log, JdbcDriver cannot be present in the Quickfix/J config file. Otherwise, the default LogFactory - ScreenLogFactory will be used. We need JdbcDriver setting as it is used for persisting FIX messages into database for FIX session management.

After debugging the code, I found out that this could be fixed by swapping those two lines in QuickfixjEngine.java

298: isJdbcLog(settings, impliedLogFactories);
299: isSL4JLog(settings, impliedLogFactories);

Can we please have this fixed for the next release?",njiang,eliotc,Major,Resolved,Fixed,21/Oct/11 04:52,25/Oct/11 08:07
Bug,CAMEL-4573,12528347,camel-bindy BindyFixedLength parser is not Thread Safe while marshalling.,"Camel Fixed Length Parser. Not Threadsafe, as instance variable private Map<Integer, List<String>> results; is being used while marshalling/unbinding. Inconsistencies in multithreaded environment causing data from different threads being used.

Very similar to BUG 4311 ( for KeyValuePair Parser)",davsclaus,surya108,Major,Resolved,Fixed,22/Oct/11 02:32,22/Oct/11 08:41
Bug,CAMEL-4579,12528659,Add option on XSLT to allow StAX,"With the new StAX converters you may convert to a StAXSource which is not supported by the XSLT transformer, and you get this exception

Caused by: org.apache.xml.dtm.DTMException: Not supported: javax.xml.transform.stax.StAXSource@14d556e
	at org.apache.xml.dtm.ref.DTMManagerDefault.getDTM(DTMManagerDefault.java:477)
	at org.apache.xalan.transformer.TransformerImpl.transform(TransformerImpl.java:699)
	at org.apache.xalan.transformer.TransformerImpl.transform(TransformerImpl.java:1273)
	at org.apache.xalan.transformer.TransformerImpl.transform(TransformerImpl.java:1251)
	at org.apache.camel.builder.xml.XsltBuilder.process(XsltBuilder.java:107)

We should add option on XSLT component to explicit enable StAX if the end user wants it.",davsclaus,davsclaus,Major,Resolved,Fixed,25/Oct/11 12:22,25/Oct/11 13:21
Bug,CAMEL-4581,12528669,NPE using seda (with concurrentConsumers) and stream endopoints,"I found an issue with seda and stream endpoints. 
Using more then 1 concurrent consumers for seda and using ""stream:out"" as destination endopoint , I sometimes receive some NullPointerExceptions. 

This is how to reproduce the problem: 

context.addRoutes(new RouteBuilder() { 
  @Override 
   public void configure() throws Exception { 
        from(""seda:sendRequest?concurrentConsumers=10"") 
        .to(""stream:out""); 
   } 
}); 
context.start(); 

for ( int i = 0; i < 50000; i++ ) { 
        producer.sendBody(""seda:sendRequest"", String.format(""Message %d"", i)); 
} 

And this is one of the exceptions: 

2011-10-25 10:12:36,550 ERROR  [org.apache.camel.processor.DefaultErrorHandler] - Failed delivery for exchangeId: ID-localhost-51879-1319530353835-0-48591. Exhausted after delivery attempt: 1 caught: java.lang.NullPointerException 
java.lang.NullPointerException: null 
        at java.io.Writer.<init>(Writer.java:71) ~[na:1.6.0_26] 
        at java.io.OutputStreamWriter.<init>(OutputStreamWriter.java:113) ~[na:1.6.0_26] 
        at org.apache.camel.component.stream.StreamProducer.writeToStream(StreamProducer.java:135) ~[camel-stream-2.8.1.jar:2.8.1] 
        at org.apache.camel.component.stream.StreamProducer.process(StreamProducer.java:83) ~[camel-stream-2.8.1.jar:2.8.1] 
        at org.apache.camel.impl.converter.AsyncProcessorTypeConverter$ProcessorToAsyncProcessorBridge.process(AsyncProcessorTypeConverter.java:50) ~[camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.SendProcessor$2.doInAsyncProducer(SendProcessor.java:114) ~[camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.impl.ProducerCache.doInAsyncProducer(ProducerCache.java:284) ~[camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.SendProcessor.process(SendProcessor.java:109) ~[camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:69) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.interceptor.TraceInterceptor.process(TraceInterceptor.java:99) ~[camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.RedeliveryErrorHandler.processErrorHandler(RedeliveryErrorHandler.java:318) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.RedeliveryErrorHandler.process(RedeliveryErrorHandler.java:209) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.DefaultChannel.process(DefaultChannel.java:305) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.UnitOfWorkProcessor.process(UnitOfWorkProcessor.java:102) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.DelegateAsyncProcessor.processNext(DelegateAsyncProcessor.java:98) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:89) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.management.InstrumentationProcessor.process(InstrumentationProcessor.java:69) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.component.seda.SedaConsumer.sendToConsumers(SedaConsumer.java:217) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.component.seda.SedaConsumer.doRun(SedaConsumer.java:155) [camel-core-2.8.1.jar:2.8.1] 
        at org.apache.camel.component.seda.SedaConsumer.run(SedaConsumer.java:129) [camel-core-2.8.1.jar:2.8.1] 
        at java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886) [na:1.6.0_26] 
        at java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908) [na:1.6.0_26] 
        at java.lang.Thread.run(Thread.java:680) [na:1.6.0_26] 


Lorenzo",njiang,llt,Minor,Resolved,Fixed,25/Oct/11 13:03,27/Oct/11 12:19
Bug,CAMEL-4584,12528738,XQuery component does not work with StAXSource out of the box,"The new STaXSource type converters cause camel-saxon to fail unit tests as reported by CI servers

Caused by: net.sf.saxon.trans.XPathException: Unknown source class
	at net.sf.saxon.Configuration.buildDocument(Configuration.java:2892)
	at net.sf.saxon.query.StaticQueryContext.buildDocument(StaticQueryContext.java:211)
	at org.apache.camel.component.xquery.XQueryBuilder.createDynamicContext(XQueryBuilder.java:474)

We should implement the same optimized logic as we did today in XSLT to convert to Source if StAX is allowed, as well to avoid leaking resources we need to ensure if InputStream is in use, we should ensure it gets closed.",davsclaus,davsclaus,Minor,Resolved,Fixed,25/Oct/11 19:11,26/Oct/11 07:26
Bug,CAMEL-4591,12529036,XPath will lock File input in case of an exception during xpath evaluation,"We have a similar issue with XPath, as we had with XSLT and XQuery.

If the message is a java.io.File and an exception occurs during evaluation, then that file is locked on Windows platforms.
We need to close the FileInputStream to unlock the file, and allow Windows to access the file.",davsclaus,davsclaus,Major,Resolved,Fixed,27/Oct/11 11:54,27/Oct/11 13:23
Bug,CAMEL-4594,12529231,Using Saxon to do XML split and xpath evaluation causes XPathExpression exception,"If you do a route like
{code}
                from(""direct:start"")
                    .split().xpath(""/persons/person"")
                    .choice()
                        .when().xpath(""person/city = 'London'"")
                            .to(""mock:london"")
                        .when().xpath(""person/city = 'Paris'"")
                            .to(""mock:paris"")
                        .otherwise()
                            .to(""mock:other"");
{code}

Then Saxon throws this exception
{code}
Caused by: javax.xml.xpath.XPathExpressionException: Supplied node must be built using the same or a compatible Configuration
	at net.sf.saxon.xpath.XPathExpressionImpl.evaluate(XPathExpressionImpl.java:284)
	at org.apache.camel.builder.xml.XPathBuilder.doInEvaluateAs(XPathBuilder.java:677)
	... 77 more
{code}

",davsclaus,davsclaus,Major,Resolved,Fixed,28/Oct/11 11:02,28/Oct/11 14:14
Bug,CAMEL-4601,12529572,Malformed URI fragment copied twice,"In URISupport.createURIWithQuery if the uri parameter contains a fragment but no query, the fragment is added a second time in the returned URI.

Fairly easy to fix if anybody wants to take a stab at it :).",hadrian,hadrian,Minor,Resolved,Fixed,31/Oct/11 22:15,22/Dec/11 15:49
Bug,CAMEL-4605,12529819,Camel ftp read file after close connection,"There is an error with ftp server. 
Many files on the server. All files have no time to be processed. 
When the processing of the next file, the process stops. 
Error - The request is a file after closing the connection. 

Camel config route: 
{code}
<?xml version=""1.0"" encoding=""UTF-8""?>
<blueprint xmlns=""http://www.osgi.org/xmlns/blueprint/v1.0.0""
  xmlns:cm=""http://aries.apache.org/blueprint/xmlns/blueprint-cm/v1.0.0""
  xmlns:camel=""http://camel.apache.org/schema/blueprint""
  xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
  default-activation=""lazy"">

  <cm:property-placeholder persistent-id=""ccwe.exchange.oos"">
    <cm:default-properties>
      <cm:property name=""ftp.doc.url"" value=""ftp://mail01.zakupki.gov.ru/auto/organization/all/""/>
      <cm:property name=""ftp.doc.add_params"" value=""""/>
    </cm:default-properties>
  </cm:property-placeholder>
  
  <bean id=""readProcess"" class=""mypackage.docread.ReadProcess""/>

  <bean id=""routePolicy"" 
    class=""org.apache.camel.routepolicy.quartz.CronScheduledRoutePolicy"">
    <property name=""routeStartTime"" value=""* 0/10 * * * ?""/>
    <property name=""routeStopTime"" value=""* 4/10 * * * ?""/>
  </bean>

  <camelContext id=""exchange-read"" 
     xmlns=""http://camel.apache.org/schema/blueprint"" autoStartup=""false"">
     
    <threadPoolProfile id=""fooProfile"" 
       poolSize=""2"" maxPoolSize=""2"" maxQueueSize=""-1""/>
       
    <route id=""readRoute"" 
      routePolicyRef=""routePolicy""
      shutdownRunningTask= ""CompleteAllTasks"">
      
      <from uri=""{{ftp.doc.url}}?noop=true&amp;binary=true{{ftp.doc.add_params}}""/>
      <to uri=""bean:readProcess""/>
    </route>
  </camelContext>
</blueprint>
{code}

We have two streams
bq. <threadPoolProfile id=""fooProfile"" poolSize=""2"" maxPoolSize=""2"" maxQueueSize=""-1""/>


The first thread (thread#0) reading file from ftp server
{code}
2011-11-02 09:13:57,376 | TRACE | Camel (exchange-read) thread #0 - ftp://mail01.zakupki.gov.ru/auto/organization/all/ | FtpConsumer                      | mponent.file.GenericFileConsumer  261 | 94 - org.apache.camel.camel-core - 2.8.1 |
     Processing file: GenericFile[organization_all_20111023_010003_33.xml.zip]
2011-11-02 09:13:57,376 | TRACE | Camel (exchange-read) thread #0 - ftp://mail01.zakupki.gov.ru/auto/organization/all/ | FtpConsumer                      | mponent.file.GenericFileConsumer  293 | 94 - org.apache.camel.camel-core - 2.8.1 |
     Retrieving file: auto/organization/all/organization_all_20111023_010003_33.xml.zip from: Endpoint[ftp://mail01.zakupki.gov.ru/auto/organization/all/?
binary=true&connectTimeout=10000&delay=600000&disconnect=true&localWorkDirectory=%2Fhome%2Fuser%2Ftemp&noop=true&passiveMode=true&soTimeout=10000&timeout=15000]
{code}

The second stream (thread#1) is stopped
{code}
2011-11-02 09:14:00,004 | DEBUG | Camel (exchange-read) thread #1 - ShutdownTask | ScheduledPollConsumer            | camel.impl.ScheduledPollConsumer  265 | 94 - org.apache.camel.camel-core - 2.8.1 | 
     This consumer is stopping, so cancelling scheduled task: java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask@2bf8d0a4
2011-11-02 09:14:00,005 | DEBUG | Camel (exchange-read) thread #1 - ShutdownTask | FtpConsumer                      | pache.camel.impl.DefaultConsumer   83 | 94 - org.apache.camel.camel-core - 2.8.1 | 
     Stopping consumer: FtpConsumer[ftp://mail01.zakupki.gov.ru/auto/organization/all/?binary=true&connectTimeout=10000&delay=600000&disconnect=true&localWorkDirectory=%2Fhome%2Fuser%2Ftemp&noop=true&passiveMode=true&soTimeout=10000&timeout=15000]
2011-11-02 09:14:00,007 | DEBUG | Camel (exchange-read) thread #1 - ShutdownTask | ProcessorEndpoint$1              | pache.camel.impl.DefaultProducer   76 | 94 - org.apache.camel.camel-core - 2.8.1 | 
     Stopping producer: Producer[bean://readProcess]
2011-11-02 09:14:00,010 | DEBUG | Camel (exchange-read) thread #1 - ShutdownTask | FtpConsumer                      | t.file.remote.RemoteFileConsumer  102 | 94 - org.apache.camel.camel-core - 2.8.1 | 
     Disconnecting from: ftp://anonymous@mail01.zakupki.gov.ru:21
{code}

Warning!!!! ftp close connection. Completed method FTPClient.disconnect(). Parament _controlInput_ = null.

{code}
2011-11-02 09:14:03,182 | INFO  | Camel (exchange-read) thread #1 - ShutdownTask | DefaultShutdownStrategy          | ultShutdownStrategy$ShutdownTask  460 | 94 - org.apache.camel.camel-core - 2.8.1 | 
     Route: readRoute shutdown complete, was consuming from: Endpoint[...]
2011-11-02 09:14:03,182 | INFO  | DefaultQuartzScheduler-exchange-read_Worker-1 | DefaultShutdownStrategy          | mel.impl.DefaultShutdownStrategy  158 | 94 - org.apache.camel.camel-core - 2.8.1 | 
     Graceful shutdown of 1 routes completed in 3 seconds
{code}

The first thread (thread#0) continues reading the file [organization_all_20111023_010003_33.xml.zip].
{code}
2011-11-02 09:14:04,172 | ERROR | Camel (exchange-read) thread #0 - ftp://mail01.zakupki.gov.ru/auto/organization/all/ | FtpConsumer                      | ache.camel.processor.CamelLogger  232 | 94 - org.apache.camel.camel-core - 2.8.1 | Caused by: [java.lang.NullPointerException - null]
java.lang.NullPointerException
	at org.apache.commons.net.ftp.FTP.__getReply(FTP.java:295)[97:org.apache.commons.net:2.2]
	at org.apache.commons.net.ftp.FTP.getReply(FTP.java:622)[97:org.apache.commons.net:2.2]
	at org.apache.commons.net.ftp.FTPClient.completePendingCommand(FTPClient.java:1408)[97:org.apache.commons.net:2.2]
	at org.apache.commons.net.ftp.FTPClient.retrieveFile(FTPClient.java:1467)[97:org.apache.commons.net:2.2]
	at org.apache.camel.component.file.remote.FtpOperations.retrieveFileToFileInLocalWorkDirectory(FtpOperations.java:406)[99:org.apache.camel.camel-ftp:2.8.1]
{code}",davsclaus,alexey-s,Major,Resolved,Fixed,02/Nov/11 07:21,05/Nov/11 12:56
Bug,CAMEL-4611,12530085,source jars missing in 2.9.0-RC1,"There are no source jars in the latest staged release https://repository.apache.org/content/repositories/orgapachecamel-137/org/apache/camel/camel-core/2.9.0-RC1/

Found this warning in the logs

{code}
[INFO] --- maven-source-plugin:2.1.2:jar-no-fork (attach-sources) @ camel-core ---
[WARNING] NOT adding sources to artifacts with classifier as Maven only supports one classifier per artifact. Current artifact [org.apache.camel:camel-core:bundle:2.9-SNAPSHOT] has a [] classifier.
{code}

which led me to FELIX-3058. Gonna back our maven-bundle-plugin version down to 2.3.4 shortly which resolves the issue. ",janstey,janstey,Major,Resolved,Fixed,03/Nov/11 17:26,03/Nov/11 17:40
Bug,CAMEL-4619,12530418,"Stop or shutdown a route with a custom timeout value, will still log using the default timeout in the logs","If you stop a route with a custom time out value, then the graceful shutdown strategy will still use the default timeout value in the logs, so that information is not correct. For example if you have a short default value, but provide a higher timeout value, then you can have the log show negative timeout values.

INFO  DefaultShutdownStrategy        - Waiting as there are still 2 inflight and pending exchanges to complete, timeout in -14 seconds.",davsclaus,davsclaus,Minor,Resolved,Fixed,05/Nov/11 11:19,05/Nov/11 11:32
Bug,CAMEL-4647,12530741,Simple expression does not find method toString(),"In a route builder, we have a log definition: 

from(""direct:testSimple"")
 .log(""${body.toString}"");

The body contains an object, that implements a public toString() method. But the problem is, that the simple-expression does not find the method toString() and route processing stops without throwing an exception.

The funny thing is, that when we just rename the method, e.g. toInfoString(), then it works:
from(""direct:testSimple"")
 .log(""${body.toInfoString}"");

Why does simple expression have a problem with a method named ""toString""?
",davsclaus,romanstumm,Major,Resolved,Fixed,08/Nov/11 17:36,09/Nov/11 10:19
Bug,CAMEL-4649,12530844,NPE from DefaultCxfBinding when <soap:Body/> is null ,"when got soap message from CXF component like this:
<soap:Envelope xmlns:soap=""http://schemas.xmlsoap.org/soap/envelope/"">
<soap:Header>
......
</soap:Header>
<soap:Body/>
</soap:Envelope>

the NPE will be thrown: 

18:04:51,255 | WARN  | tp1212500935-153 | ache.cxf.common.logging.LogUtils  372 |  -  -  | Interceptor for {http://airportsoap.sopera.de}airport#{http://airportsoap.sopera.de}getAirportInformationByISOCountryCode has thrown exception, unwinding now
java.lang.NullPointerException
	at org.apache.camel.component.cxf.DefaultCxfBinding.getResponsePayloadList(DefaultCxfBinding.java:395)[163:org.apache.camel.camel-cxf:2.8.2]
	at org.apache.camel.component.cxf.DefaultCxfBinding.populateCxfResponseFromExchange(DefaultCxfBinding.java:318)[163:org.apache.camel.camel-cxf:2.8.2]
	at org.apache.camel.component.cxf.CxfConsumer$1.setResponseBack(CxfConsumer.java:176)[163:org.apache.camel.camel-cxf:2.8.2]
	at org.apache.camel.component.cxf.CxfConsumer$1.asyncInvoke(CxfConsumer.java:103)[163:org.apache.camel.camel-cxf:2.8.2]
	at org.apache.camel.component.cxf.CxfConsumer$1.invoke(CxfConsumer.java:68)[163:org.apache.camel.camel-cxf:2.8.2]
	at org.apache.cxf.interceptor.ServiceInvokerInterceptor$1.run(ServiceInvokerInterceptor.java:58)[141:org.apache.cxf.bundle:2.5.0]
	at org.apache.cxf.interceptor.ServiceInvokerInterceptor.handleMessage(ServiceInvokerInterceptor.java:93)[141:org.apache.cxf.bundle:2.5.0]
	at org.apache.cxf.phase.PhaseInterceptorChain.doIntercept(PhaseInterceptorChain.java:263)[141:org.apache.cxf.bundle:2.5.0]
	at org.apache.cxf.phase.PhaseInterceptorChain.resume(PhaseInterceptorChain.java:232)[141:org.apache.cxf.bundle:2.5.0]
	at org.apache.cxf.transport.ChainInitiationObserver.onMessage(ChainInitiationObserver.java:80)[141:org.apache.cxf.bundle:2.5.0]
	at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.serviceRequest(JettyHTTPDestination.java:323)[141:org.apache.cxf.bundle:2.5.0]
	at org.apache.cxf.transport.http_jetty.JettyHTTPDestination.doService(JettyHTTPDestination.java:289)[141:org.apache.cxf.bundle:2.5.0]
	at org.apache.cxf.transport.http_jetty.JettyHTTPHandler.handle(JettyHTTPHandler.java:72)[141:org.apache.cxf.bundle:2.5.0]
	at org.eclipse.jetty.server.handler.ContextHandler.doHandle(ContextHandler.java:939)[59:org.eclipse.jetty.server:7.4.5.v20110725]
	at org.eclipse.jetty.server.handler.ContextHandler.doScope(ContextHandler.java:875)[59:org.eclipse.jetty.server:7.4.5.v20110725]
	at org.eclipse.jetty.server.handler.ScopedHandler.handle(ScopedHandler.java:117)[59:org.eclipse.jetty.server:7.4.5.v20110725]
	at org.eclipse.jetty.server.handler.ContextHandlerCollection.handle(ContextHandlerCollection.java:185)[59:org.eclipse.jetty.server:7.4.5.v20110725]
	at org.eclipse.jetty.server.handler.HandlerWrapper.handle(HandlerWrapper.java:110)[59:org.eclipse.jetty.server:7.4.5.v20110725]
	at org.eclipse.jetty.server.Server.handleAsync(Server.java:391)[59:org.eclipse.jetty.server:7.4.5.v20110725]
	at org.eclipse.jetty.server.HttpConnection.handleRequest(HttpConnection.java:594)[59:org.eclipse.jetty.server:7.4.5.v20110725]
	at org.eclipse.jetty.server.HttpConnection.handle(HttpConnection.java:393)[59:org.eclipse.jetty.server:7.4.5.v20110725]
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint.handle(SelectChannelEndPoint.java:535)[54:org.eclipse.jetty.io:7.4.5.v20110725]
	at org.eclipse.jetty.io.nio.SelectChannelEndPoint$1.run(SelectChannelEndPoint.java:40)[54:org.eclipse.jetty.io:7.4.5.v20110725]
	at org.eclipse.jetty.util.thread.QueuedThreadPool$3.run(QueuedThreadPool.java:529)[53:org.eclipse.jetty.util:7.4.5.v20110725]
	at java.lang.Thread.run(Thread.java:662)[:1.6.0_24]


Looking into the source codes, before the line 395 of DefaultCxfBinding, the elements variable shoulde be checked if it's null.",njiang,xldai,Major,Resolved,Fixed,09/Nov/11 10:18,09/Nov/11 13:42
Bug,CAMEL-4650,12530849,NPE when using SEDA route and attaching an extra consumer,"I'm trying to construct a system for moving some seda queues over to
ActiveMQ during system shutdown. What I did was create a Route that
connects to some of my seda queues and then drains the queue to activemq.

Basicly I got two routes, the drainer:

from(""seda:"" + sedaId + ""?size=1000"")
                    .routeId(routeName +
sedaIs).noAutoStartup().to(activeMQFailuresQueue);


And the main route:

from(""seda:"" + sedaId + ""?size=1000"")
                    .routeId(routeName + sedaIs).to(SomeProcessor);

Now, sometimes the main route stalls for various reasons I need to
restart the jvm process it is running in, so I start the first route.
But when trying this in production, I got:

 java.lang.NullPointerException
    at
org.apache.camel.util.AsyncProcessorHelper.process(AsyncProcessorHelper.java:78)
    at
org.apache.camel.component.seda.SedaConsumer.sendToConsumers(SedaConsumer.java:210)
    at
org.apache.camel.component.seda.SedaConsumer.doRun(SedaConsumer.java:155)
    at
org.apache.camel.component.seda.SedaConsumer.run(SedaConsumer.java:129)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.runTask(ThreadPoolExecutor.java:886)
    at
java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:908)
    at java.lang.Thread.run(Thread.java:619)


Bvahdat added a comment:
just a very tiny pointer:

Looking at the source it seems that  the
SedaEndpoint.getConsumerMulticastProcessor() method returns 'null' causing
the NPE, as the condition:

multicastStarted == false || consumerMulticastProcessor == null

is true.
",davsclaus,tarjei@scanmine.com,Major,Resolved,Fixed,09/Nov/11 10:47,18/Nov/11 01:44
Bug,CAMEL-4655,12530968,Bindy does not support quoted value with separator char in CSV datasource,"i'm currently writing a tuto on Camel (2.8.2).

Showing HTTP4 component usage by downloading US Gov public data, i found a problem :


                from(""quartz://dataTimer?cron=0+*+*+*+*+?"").to(""direct:datas"");

                from(""direct:datas"")
                .to(""http4://earthquake.usgs.gov/earthquakes/catalogs/eqs7day-M1.txt"")
                .unmarshal(new BindyCsvDataFormat(""net.thejeearchitectcookbook.camel.earthquake""))
                .process(new Processor() {

                        public void process(Exchange exchange) throws Exception {

                                Message message = exchange.getIn();

                                // ...
                        }
                });

The data format is like and my separator char is "","":

nc,71678421,0,""Wednesday, November  9, 2011 14:53:13 UTC"",37.5727,-118.8170,1.3,6.60,14,""Central California""

I want to get all datas as java.lang.String but the date value raises exception :

java.lang.IllegalArgumentException: No position 11 defined for the field: 14, line: 2 must be specified]

My separator is "","" but some values are nested inside "" "". Unfortunately there are "","" inside the "" "".

Bindy get lost !
 
How can i get String values nested inside "" "" and containing "","" ?

I can note that Camel CSV component deals with it without any problems.


Here is my pojo :


package net.thejeearchitectcookbook.camel.earthquake;

import java.io.Serializable;


import org.apache.camel.dataformat.bindy.annotation.CsvRecord;
import org.apache.camel.dataformat.bindy.annotation.DataField;


@CsvRecord( separator = "","")
public class EarthquakeInfos implements Serializable {
   
        @DataField(pos = 1)
    private String src;
       
        @DataField(pos = 2)
    private String eqid;
       
        @DataField(pos = 3)
    private String version;
       
    @DataField(pos = 4)
    private String datetime;
   
        @DataField(pos = 5)
    private String lat;
       
        @DataField(pos = 6)
    private String lon;
   
    @DataField(pos = 7)
    private String magnitude;
   
        @DataField(pos = 8)
    private String depth;
       
        @DataField(pos = 9)
    private String nst;
       
    @DataField(pos = 10)
    private String place;
   



    public String getSrc() {
                return src;
        }

        public void setSrc(String src) {
                this.src = src;
        }

        public String getEqid() {
                return eqid;
        }

        public void setEqid(String eqid) {
                this.eqid = eqid;
        }

        public String getVersion() {
                return version;
        }

        public void setVersion(String version) {
                this.version = version;
        }

        public String getLat() {
                return lat;
        }

        public void setLat(String lat) {
                this.lat = lat;
        }

        public String getLon() {
                return lon;
        }

        public void setLon(String lon) {
                this.lon = lon;
        }

        public String getDepth() {
                return depth;
        }

        public void setDepth(String depth) {
                this.depth = depth;
        }

        public String getNst() {
                return nst;
        }

        public void setNst(String nst) {
                this.nst = nst;
        }

        public String getDatetime() {
        return datetime;
    }

    public void setDatetime(String datetime) {
        this.datetime = datetime;
    }

    public String getMagnitude() {
        return magnitude;
    }

    public void setMagnitude(String magnitude) {
        this.magnitude = magnitude;
    }

    public String getPlace() {
        return place;
    }

    public void setPlace(String place) {
        this.place = place;
    }
} 

I can find that CSV Camel component performs well with the same datasource :

from(""direct:datas"")
.to(""http4://earthquake.usgs.gov/earthquakes/catalogs/eqs7day-M1.txt"")
.unmarshal().csv().process(new Processor() {

	public void process(Exchange exchange) throws Exception {

		Message message = exchange.getIn();
		List<List<String>> datas = (List<List<String>>) message.getBody();
		// Skip header
		datas = datas.subList(1, datas.size() - 1);
		// Process my data
		for (List<String> row : datas) {
			// Process Row
			String datetime = row.get(3);
			String region = row.get(9);
			String magnitude = row.get(6);
					
		}
	}
});
",davsclaus,olivierursushorribilis,Major,Resolved,Fixed,10/Nov/11 07:35,12/Nov/11 14:11
Bug,CAMEL-4657,12530985,"camel-jms - Request/Reply - Leak in ActiveMQSessionPool causing it to eat up memory, when using fixed replyTo queue names","See nabble
http://camel.465427.n5.nabble.com/Possible-memory-leak-in-org-apache-activemq-pool-PooledSession-tp4964951p4964951.html

This bug is in ActiveMQ, but creating a ticket to get it resolved as the leak is apparent when using Spring DMLC with CACHE_SESSION, which Camel by default does when doing request/reply over JMS with fixed replyTo queues.

Then the consumer is not cached, and therefore created on each poll, but the ActiveMQSessionPool keeps growing in its internal list of created consumers, as the session is cached.

Most likely a patch is needed to fix this in the AMQ side",davsclaus,davsclaus,Critical,Resolved,Fixed,10/Nov/11 11:05,11/Nov/11 08:34
Bug,CAMEL-4658,12530999,Camel 2.9.0 SNAPSHOT not start on OSGI apache karaf,"Compiling my project is done using maven pluggins
maven-dependency-plugin and features-maven-plugin.
First unpack apache-karaf-2.2.3.zip.
The second component uses mvn:org.apache.camel.karaf/apache-camel/${ camel-version }/xml/features

karaf-version = 2.2.3
camel-version = 2.9.0-SNAPSHOT (build 2011-11-10)

Establish the following set of components
{code}
<feature version='${karaf-version}'>ssh</feature>
<feature version='${karaf-version}'>config</feature>
<feature version='${karaf-version}'>webconsole</feature>
<feature version='${karaf-version}'>wrapper</feature>

<feature version='${camel-version}'>camel-blueprint</feature>
<feature version='${camel-version}'>camel-ftp</feature>
<feature version='${camel-version}'>camel-http</feature>
<feature version='${camel-version}'>camel-http4</feature>
<feature version='${camel-version}'>camel-quartz</feature>
{code}
After starting karaf error log ${karaf.data}/log/karag.log
{code}
2011-11-10 16:19:51,128 | ERROR | Thread-6                                                           | FeaturesServiceImpl              | s.internal.Features
ServiceImpl$1  977 | 11 - org.apache.karaf.features.core - 2.2.3 | Error installing boot features
java.lang.Exception: Could not start bundle mvn:org.apache.camel.karaf/camel-karaf-commands/2.9.0-SNAPSHOT in feature(s) camel-core-2.9.0-SNAPSHOT: Unresolve
d constraint in bundle org.apache.camel.karaf.camel-karaf-commands [95]: Unable to resolve 95.0: missing requirement [95.0] package; (&(package=org.apache.fe
lix.gogo.commands)(version>=0.10.0)(!(version>=1.0.0)))
        at org.apache.karaf.features.internal.FeaturesServiceImpl.installFeatures(FeaturesServiceImpl.java:353)[11:org.apache.karaf.features.core:2.2.3]
        at org.apache.karaf.features.internal.FeaturesServiceImpl$1.run(FeaturesServiceImpl.java:975)[11:org.apache.karaf.features.core:2.2.3]
Caused by: org.osgi.framework.BundleException: Unresolved constraint in bundle org.apache.camel.karaf.camel-karaf-commands [95]: Unable to resolve 95.0: miss
ing requirement [95.0] package; (&(package=org.apache.felix.gogo.commands)(version>=0.10.0)(!(version>=1.0.0)))
{code}
We must add a dependency on a component in a file features.xml

Camel 2.8.1 and 2.7.0 to work correctly.",davsclaus,alexey-s,Major,Resolved,Fixed,10/Nov/11 13:49,02/May/13 02:29
Bug,CAMEL-4659,12531001,Open the range when importing org.apache.fe lix.gogo.commands package,"We need to import the package with [0.6,1) instead of [0.10,1) to be able to deploy on oder karaf versions (< 2.2.4)",njiang,gnodet,Major,Resolved,Fixed,10/Nov/11 13:53,02/May/13 02:29
Bug,CAMEL-4660,12531004,Hardcoded initial delay for completionInterval in AggregateProcessor,"When setting the completionInterval on an aggregator, the first completion will be triggered after 1000ms. The reason is the hardcoded initial delay in AggregateProcessor.

This behavior has not been documented and is not expected. The expected behavior is for the first completion to be triggered after the given interval. The first completion would then be triggered after completionInterval, the second after 2 * completionInterval, the third after 3 * completionInterval and so on.",davsclaus,okhofstad,Minor,Resolved,Fixed,10/Nov/11 14:33,11/Nov/11 09:13
Bug,CAMEL-4668,12531128,JpaConsumer - Should rollback if processing of an exchange failed,"If a JPA consumer pickup X messages and then one of those messages fails to be processed and throws an exception, then the JPA consumer should detect this and mark the TX as rollback.

Currently there is a flaw which causes the JPA consumer to commit the batch.

See nabble
http://camel.465427.n5.nabble.com/Misleading-jmx-statistics-on-jpa-component-tp4960503p4960503.html",davsclaus,davsclaus,Major,Resolved,Fixed,11/Nov/11 12:46,13/Nov/11 13:53
Bug,CAMEL-4670,12531177,The javax.xml.tranform.stax imported package should be optional,"It forbids camel to be deployed on 1.5, which can be problematic when using an OSGi container with some packages hidden.",gnodet,gnodet,Major,Resolved,Fixed,11/Nov/11 20:56,14/Nov/11 16:07
Bug,CAMEL-4672,12531229,Bindy - CSV format - Should be able to marshal with quotes,"See nabble
http://camel.465427.n5.nabble.com/Bindy-CSV-marshal-does-not-properly-quote-values-defined-by-the-CsvRecord-separator-td4809017.html

Its fairly common to marshal to/from with quotes in CSV format.
We should make this easier by adding an attribute to @CsvRecord where you can define a quote char to use, either single or double etc.",davsclaus,davsclaus,Minor,Resolved,Fixed,12/Nov/11 14:32,12/Nov/11 15:04
Bug,CAMEL-4679,12531390,Camel IRC component password and port configuration properties do not work,"If one sets a password in the URI for this component, the password is concatenated with the username when the URI is ""sanitized"" by IrcConfiguration#sanitize and is not properly parsed when reread by the IrcConfiguration#configure operation.

The result is that the concatenated combo is sent as the username and the colon is an illegal character.  The user's secret is also sent with their username and may inadvertently end up in log files.

If one specifies a port in the URI for this component, the port is not a URI parameter and is not handled by the IrcConfiguration#configure method.  The port is ignored and the default port range of 6667-6669 is attempted.",davaleri,davaleri,Major,Resolved,Fixed,15/Nov/11 00:40,15/Nov/11 16:19
Bug,CAMEL-4682,12531456,"When stopping CamelContext should not clear lifecycleStrategies, to make restart safely possible","We should not clear the lifecycleStrategies on CamelContext when stop() is invoked, as if we restart by invoking start(), the lifecycle strategies should be in use again.",davsclaus,davsclaus,Minor,Resolved,Fixed,15/Nov/11 13:14,18/Nov/11 01:43
Bug,CAMEL-4692,12531949,can't use same quartz scheduled route policy on two routes,"The following XML DSL defines two routes using the same route policy:

<beans xmlns=""http://www.springframework.org/schema/beans""
       xmlns:xsi=""http://www.w3.org/2001/XMLSchema-instance""
       xmlns:camel=""http://camel.apache.org/schema/spring""
       xsi:schemaLocation=""
       http://www.springframework.org/schema/beans http://www.springframework.org/schema/beans/spring-beans.xsd
       http://camel.apache.org/schema/spring http://camel.apache.org/schema/spring/camel-spring.xsd"">

  <camel:camelContext xmlns=""http://camel.apache.org/schema/spring"">
    <camel:propertyPlaceholder id=""properties"" location=""file:///etc/camel/css.properties""/>

    <camel:package>cdp_test</camel:package>
    <camel:route id=""r1"" routePolicyRef=""pol1"">
      <camel:from uri=""file://data/1""/>
      <camel:to uri=""stream:out""/>
    </camel:route>
    <camel:route id=""r2"" routePolicyRef=""pol1"">
      <camel:from uri=""file://data/2""/>
      <camel:to uri=""stream:out""/>
    </camel:route>

  </camel:camelContext>

  <bean id=""pol1"" class=""org.apache.camel.routepolicy.quartz.CronScheduledRoutePolicy"">
    <property name=""routeStartTime"" value=""00,30 * * * * ? *""/>
    <property name=""routeStopTime""  value=""10,40 * * * * ? *""/>
  </bean>

</beans>

This is the logging output. r2 does not follow the policy:

[pache.camel.spring.Main.main()] StdSchedulerFactory            INFO  Quartz scheduler 'DefaultQuartzScheduler-camel-1' initialized from an externally provided properties instance.
[pache.camel.spring.Main.main()] StdSchedulerFactory            INFO  Quartz scheduler version: 1.8.4
[pache.camel.spring.Main.main()] ScheduledRoutePolicy           INFO  Scheduled trigger: triggerGroup-r1.trigger-START-r1 for action: START on route:
[pache.camel.spring.Main.main()] ScheduledRoutePolicy           INFO  Scheduled trigger: triggerGroup-r1.trigger-STOP-r1 for action: STOP on route:
[pache.camel.spring.Main.main()] SpringCamelContext             INFO  Route: r1 started and consuming from: Endpoint[file://data/1]
[pache.camel.spring.Main.main()] SpringCamelContext             INFO  Route: r2 started and consuming from: Endpoint[file://data/2]
[pache.camel.spring.Main.main()] QuartzComponent                INFO  Starting Quartz scheduler: DefaultQuartzScheduler-camel-1
[pache.camel.spring.Main.main()] QuartzScheduler                INFO  Scheduler DefaultQuartzScheduler-camel-1_$_NON_CLUSTERED started.
[pache.camel.spring.Main.main()] SpringCamelContext             INFO  Total 2 routes, of which 2 is started.
[pache.camel.spring.Main.main()] SpringCamelContext             INFO  Apache Camel 2.8.2 (CamelContext: camel-1) started in 0.596 seconds
hello 1
[artzScheduler-camel-1_Worker-2] DefaultShutdownStrategy        INFO  Starting to graceful shutdown 1 routes (timeout 10000 milliseconds)
[el-1) thread #2 - ShutdownTask] DefaultShutdownStrategy        INFO  Route: r1 shutdown complete, was consuming from: Endpoint[file://data/1]
[artzScheduler-camel-1_Worker-2] DefaultShutdownStrategy        INFO  Graceful shutdown of 1 routes completed in 0 seconds
[artzScheduler-camel-1_Worker-2] SpringCamelContext             INFO  Route: r1 stopped, was consuming from: Endpoint[file://data/1]
hello 2
hello 2
[artzScheduler-camel-1_Worker-3] SpringCamelContext             INFO  Route: r1 started and consuming from: Endpoint[file://data/1]
hello 2
[artzScheduler-camel-1_Worker-4] DefaultShutdownStrategy        INFO  Starting to graceful shutdown 1 routes (timeout 10000 milliseconds)
[el-1) thread #2 - ShutdownTask] DefaultShutdownStrategy        INFO  Route: r1 shutdown complete, was consuming from: Endpoint[file://data/1]
[artzScheduler-camel-1_Worker-4] DefaultShutdownStrategy        INFO  Graceful shutdown of 1 routes completed in 0 seconds
[artzScheduler-camel-1_Worker-4] SpringCamelContext             INFO  Route: r1 stopped, was consuming from: Endpoint[file://data/1]

If I use two separate identical route policies it works as I expect.",davsclaus,julian.cable,Major,Resolved,Fixed,18/Nov/11 15:09,22/Nov/11 22:53
Bug,CAMEL-4695,12532007,Calling the camel-smpp component results in a NullPointerException,"When the camel-smpp component is called to submit a SMS, a NullPointerException is thrown:

{code:title=Stacktrace}
Caused by: java.lang.NullPointerException
       at org.jsmpp.util.PDUByteBuffer.appendAll(PDUByteBuffer.java:182)[149:org.apache.servicemix.bundles.jsmpp:2.1.0.3]
       at org.jsmpp.util.DefaultComposer.submitSm(DefaultComposer.java:238)[149:org.apache.servicemix.bundles.jsmpp:2.1.0.3]
       at org.jsmpp.DefaultPDUSender.sendSubmitSm(DefaultPDUSender.java:196)[149:org.apache.servicemix.bundles.jsmpp:2.1.0.3]
       at org.jsmpp.SynchronizedPDUSender.sendSubmitSm(SynchronizedPDUSender.java:192)[149:org.apache.servicemix.bundles.jsmpp:2.1.0.3]
       at org.jsmpp.session.SubmitSmCommandTask.executeTask(SubmitSmCommandTask.java:86)[149:org.apache.servicemix.bundles.jsmpp:2.1.0.3]
       at org.jsmpp.session.AbstractSession.executeSendCommand(AbstractSession.java:248)[149:org.apache.servicemix.bundles.jsmpp:2.1.0.3]
       at org.jsmpp.session.SMPPSession.submitShortMessage(SMPPSession.java:320)[149:org.apache.servicemix.bundles.jsmpp:2.1.0.3]
       at org.apache.camel.component.smpp.SmppSubmitSmCommand.execute(SmppSubmitSmCommand.java:56)
       ... 62 more
{code}",muellerc,muellerc,Major,Closed,Fixed,18/Nov/11 22:32,18/Jul/13 19:20
Bug,CAMEL-4696,12532015,"When a route is removed from camel, LifecycleStrategy is called twice ","DefaultCamelContext.removeRoute method has these lines among others

 if (getRouteStatus(routeId).isStopped()) {
    routeService.setRemovingRoutes(true);
    shutdownRouteService(routeService);
    removeRouteDefinition(routeId);
    ServiceHelper.stopAndShutdownServices(routeService);

I think the last line from this snippet is not needed, because the route should be already stopped in order to pass the IF statement and then it is shutdown through this call shutdownRouteService(routeService);

 ServiceHelper.stopAndShutdownServices(routeService) tries to stops again the route if not stopped, but the worse is it shutdown the rout again.
It seems to me like a bug, other thoughts?


",bibryam,bibryam,Minor,Resolved,Fixed,18/Nov/11 23:49,28/Nov/11 13:26
Bug,CAMEL-4698,12532104,Scala DSL does not support defining route-scoped error handlers,The Scala DSL does not allow for defining error handlers at the route scope. The 'errorHandler' method only sets the context-scoped error handler.,njiang,rvalk,Minor,Resolved,Fixed,20/Nov/11 22:35,21/Nov/11 07:52
Bug,CAMEL-4709,12532577,ClassNotFoundException thrown by hawtbuf codec decode in OSGi,"When running the HawtDBAggregateRouteTest from camel-itest-osgi, the following stack trace results. Looks like hawtdb is not instantiate object during readObject from an ObjectIndputStream.

{code}

Caused by: java.io.IOException: org.apache.camel.impl.DefaultExchangeHolder
        at org.fusesource.hawtbuf.codec.ObjectCodec.createIOException(ObjectCodec.java:57)
        at org.fusesource.hawtbuf.codec.ObjectCodec.decode(ObjectCodec.java:52)
        at org.apache.camel.component.hawtdb.HawtDBCamelCodec.unmarshallExchange(HawtDBCamelCodec.java:74)
        at org.apache.camel.component.hawtdb.HawtDBAggregationRepository.get(HawtDBAggregationRepository.java:161)
        ... 57 more
{code}

I am disabling the test for now.",njiang,hadrian,Major,Resolved,Fixed,24/Nov/11 04:58,09/Dec/11 15:09
Bug,CAMEL-4718,12532890,camel-example-reportincident-wssecurity shouldn't import org.apache.cxf.transport.http_osgi package anymore,org.apache.cxf.transport.http_osgi package won't exist since cxf 2.4,ffang,ffang,Major,Resolved,Fixed,28/Nov/11 10:56,28/Nov/11 11:34
Bug,CAMEL-4724,12533102,PropertiesComponent in vm:// uri,"when using vm:// uri and sender and receiver in different contexts properties searched in sender, but supposed to search in receiver, where route is defined.

[discussion|http://camel.465427.n5.nabble.com/using-vm-with-different-camelContexts-and-property-parser-td5022731.html]",njiang,akudrevatych,Major,Resolved,Fixed,29/Nov/11 11:49,02/Dec/11 12:46
Bug,CAMEL-4733,12533588,Dumping route to XML created by Java DSL using an expression with Transform EIP may not output the actual used expression,"Given this route
{code}
                from(""direct:start"").routeId(""myRoute"")
                    .transform(constant(""Hello World""))
                    .to(""mock:result"");
{code}

Will be dumped as XML as:
{code:xml}
<route group=""org.apache.camel.util.DumpModelAsXmlTransformRouteTest$1"" id=""myRoute"" xmlns=""http://camel.apache.org/schema/spring"">
    <from uri=""direct:start""/>
    <transform id=""transform1"">
        <expressionDefinition/>
    </transform>
    <to uri=""mock:result"" id=""to1""/>
</route>
{code}

The <transform> is wrong as it should contain the expression definition properly.",davsclaus,davsclaus,Major,Resolved,Fixed,02/Dec/11 09:55,03/Dec/11 08:26
Bug,CAMEL-4737,12533684,camel-netty component requires a URI parameter to set receiveBufferSizePredictorFactory for UDP consumers,"The Netty UDP consumer truncates received messages that are larger than 768 bytes. A number of higher level protocols and data-formatters have message sizes larger than 768. E.g. Syslog 1024 bytes

NOTE: There is a difference between receiveBufferSize receiveBufferSizePredictor see: http://lists.jboss.org/pipermail/netty-users/2010-January/001958.html  ",njiang,geemang,Major,Resolved,Fixed,02/Dec/11 20:27,04/Dec/11 14:46
Bug,CAMEL-4742,12533868,Tokenize pair - evaluated as predicate should close input stream,"If you read from a file and use the tokenize pair in a predicate such as a filter / content based router, then the input stream should be closed. Otherwise OS such as Windows keep a lock on the stream, and the file cannot be moved/deleted after processing is done.

See nabble
http://camel.465427.n5.nabble.com/Riding-on-org-apache-camel-language-TokenPairPredicateTest-tp5041981p5041981.html

Note that using tokenizePair as a predicate is must likely very unusual, as its meant for splitting a stream into tokens, and thus most often only used with the splitter EIP.",davsclaus,davsclaus,Minor,Resolved,Fixed,05/Dec/11 11:58,05/Dec/11 12:37
Bug,CAMEL-4748,12534159,camel-stream - Should close previous stream if re-initializing a new stream,"For example if scanStream=true then the previous stream should be closed, prior to re-initalizing a new stream.

For example on Windows this may cause a file to be locked by the JVM due the old InputStream is not closed.",davsclaus,davsclaus,Minor,Resolved,Fixed,07/Dec/11 10:03,07/Dec/11 10:12
Bug,CAMEL-4751,12534261,camel-guice osgi import version range in manifest.mf,"in Camel-Guice component's pom.xml we have:
 <properties>    
    <camel.osgi.export.pkg>org.apache.camel.guice.*</camel.osgi.export.pkg>
    <camel.osgi.import.additional>
        com.google.inject*;version=""[2,3)""        
    </camel.osgi.import.additional>
  </properties>

i.e imported version of google-guice package is [2,3). Unfortunately this in not compatible with any of the releases from Google-Guice (version 2 or 3). Guice seems to maintain 'package' level versioning. ""com.google.inject;version=1.3""

Suggested:fix:
Change the lower version from 2 to 1.2 .
This bug is a showstopper for us to integrate and deploy solution with camel+guice+peaberry on to karaf osgi.


We have from guice 3:
guice 
----------
Manifest-Version = 1.0
Archiver-Version = Plexus Archiver
Built-By = mcculls
Build-Jdk = 1.6.0_24
Created-By = Apache Maven

Bundle-Vendor = Google, Inc.
Bundle-RequiredExecutionEnvironment = J2SE-1.5,JavaSE-1.6
Bundle-Name = guice
Bundle-Copyright = Copyright (C) 2006 Google Inc.
Bundle-DocURL = http://code.google.com/p/google-guice/
Bundle-Description = Guice is a lightweight dependency injection framework for J
ava 5 and above
Bundle-SymbolicName = com.google.inject
Bundle-Version = 3.0.0
Bundle-License = http://www.apache.org/licenses/LICENSE-2.0.txt
Bundle-ManifestVersion = 2

Import-Package =
        javax.inject,
        org.aopalliance.intercept
Export-Package =
        com.google.inject.matcher;version=1.3,
        com.google.inject.name;version=1.3,
        com.google.inject.spi;version=1.3,
        com.google.inject.binder;version=1.3,
        com.google.inject.util;version=1.3,
        com.google.inject;version=1.3
",davsclaus,surya108,Minor,Resolved,Fixed,07/Dec/11 21:59,08/Dec/11 07:55
Bug,CAMEL-4754,12534359,The onException clause should make use of the correct logger name given through the log DSL ,"See http://camel.465427.n5.nabble.com/global-onException-clause-wrongly-identifies-route-in-which-exception-occurs-log-name-td5058304.html
",davsclaus,bvahdat,Minor,Resolved,Fixed,08/Dec/11 16:25,12/Dec/11 05:48
Bug,CAMEL-4756,12534426,camel-test will through NPE when setting the system property of skipStartingCamelContext to be true,"We will get a NPE error when setting skipStartingCamelContext system property to be true.
{code}
      java.lang.NullPointerException
        at org.apache.camel.spring.SpringCamelContext.springCamelContext(SpringCamelContext.java:75)
        at org.apache.camel.test.junit4.CamelSpringTestSupport.createCamelContext(CamelSpringTestSupport.java:180)
        at org.apache.camel.test.junit4.CamelTestSupport.doSetUp(CamelTestSupport.java:197)
        at org.apache.camel.test.junit4.CamelTestSupport.setUp(CamelTestSupport.java:167)
        at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
        at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:39)
        at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:25)
        at java.lang.reflect.Method.invoke(Method.java:597)

{code}",njiang,njiang,Major,Resolved,Fixed,09/Dec/11 03:58,09/Dec/11 14:56
Bug,CAMEL-4768,12534751,Extra request parameters sent by the camel-http4 component,"Described in http://camel.465427.n5.nabble.com/Extra-Request-Params-sent-in-camel-HTTP-component-GET-request-td5042808.html

Route 
{code}
from(""jetty:http://0.0.0.0:8080/myapp/myservice"") 
.to(""http4://www.google.com/?q=camel&bridgeEndpoint=true&throwExceptionOnFailure=false""); 
{code}
sends to the server the {{bridgeEndpoint=true&throwExceptionOnFailure=false}} parameters,
while route 
{code}
from(""jetty:http://0.0.0.0:8080/myapp/myservice"") 
.to(""http://www.google.com/?q=camel&bridgeEndpoint=true&throwExceptionOnFailure=false""); 
{code}
behaves properly and removes them",njiang,xverges,Minor,Resolved,Fixed,12/Dec/11 09:01,16/Jun/16 13:08
Bug,CAMEL-4769,12534755,camel-quartz - Using underscore in group name does not work,"The group name is not properly resolved if people specify a invalid host name, such as with underscores.
If so we should fallback and grab the group name by grabbing the string until the first path separator char.

See nabble
http://camel.465427.n5.nabble.com/Camel-Quartz-consumer-doesn-t-allow-underscores-tp5063734p5063734.html",davsclaus,davsclaus,Minor,Resolved,Fixed,12/Dec/11 09:32,12/Dec/11 12:11
Bug,CAMEL-4771,12534833,camel-castor feature is invalid,"Christian-Muellers-MacBook-Pro:karaf cmueller$ mvn clean install -Pvalidate
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Camel :: Platforms :: Apache Karaf
[INFO] Camel :: Platforms :: Apache Karaf :: Commands
[INFO] Camel :: Platforms :: Apache Karaf :: Features
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
...
Downloaded: http://repository.apache.org/snapshots/org/apache/camel/camel-castor/2.8.4-SNAPSHOT/camel-castor-2.8.4-20111212.023754-18.jar (17 KB at 21.9 KB/sec)
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Camel :: Platforms :: Apache Karaf ................ SUCCESS [2.195s]
[INFO] Camel :: Platforms :: Apache Karaf :: Commands .... SUCCESS [7.749s]
[INFO] Camel :: Platforms :: Apache Karaf :: Features .... FAILURE [29.685s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 40.617s
[INFO] Finished at: Mon Dec 12 21:54:41 CET 2011
[INFO] Final Memory: 16M/81M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.karaf.tooling:features-maven-plugin:2.2.4:validate (validate) on project apache-camel: Unable to validate /Users/cmueller/workspaceCamel/camel/platforms/karaf/features/target/classes/features.xml: mvn:commons-io/commons-io/1.3.2 is not an OSGi bundle -> [Help 1]",njiang,muellerc,Major,Resolved,Fixed,12/Dec/11 20:59,13/Dec/11 09:34
Bug,CAMEL-4772,12534837,Camel feature is invalid,"Christian-Muellers-MacBook-Pro:karaf cmueller$ mvn clean install -Pvalidate
[INFO] Scanning for projects...
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Build Order:
[INFO] 
[INFO] Camel :: Platforms :: Apache Karaf
[INFO] Camel :: Platforms :: Apache Karaf :: Features
[INFO]                                                                         
[INFO] ------------------------------------------------------------------------
[INFO] Building Camel :: Platforms :: Apache Karaf 2.7.5-SNAPSHOT
[INFO] ------------------------------------------------------------------------
[INFO] 
...
Downloading: http://repo1.maven.org/maven2/org/apache/servicemix/bundles/org.apache.servicemix.bundles.commons-pool/${commons-pool-bundle-version}/org.apache.servicemix.bundles.commons-pool-${commons-pool-bundle-version}.jar
[INFO] ------------------------------------------------------------------------
[INFO] Reactor Summary:
[INFO] 
[INFO] Camel :: Platforms :: Apache Karaf ................ SUCCESS [2.066s]
[INFO] Camel :: Platforms :: Apache Karaf :: Features .... FAILURE [1:17.335s]
[INFO] ------------------------------------------------------------------------
[INFO] BUILD FAILURE
[INFO] ------------------------------------------------------------------------
[INFO] Total time: 1:20.222s
[INFO] Finished at: Mon Dec 12 22:12:52 CET 2011
[INFO] Final Memory: 12M/81M
[INFO] ------------------------------------------------------------------------
[ERROR] Failed to execute goal org.apache.karaf.tooling:features-maven-plugin:2.2.2:validate (validate) on project apache-camel: Unable to validate /Users/cmueller/workspaceCamel/camel/platforms/karaf/features/target/classes/features.xml: Could not find artifact org.apache.servicemix.bundles:org.apache.servicemix.bundles.commons-pool:jar:${commons-pool-bundle-version} in central (http://repo1.maven.org/maven2)
[ERROR] ",njiang,muellerc,Major,Resolved,Fixed,12/Dec/11 21:19,13/Dec/11 06:33
Bug,CAMEL-4773,12534925,DynamicRouter EIP with bean endpoint should support method parameter with OGNL,"See nabble
http://camel.465427.n5.nabble.com/Dynamic-Routing-tp5068570p5068570.html",davsclaus,davsclaus,Major,Resolved,Fixed,13/Dec/11 10:47,13/Dec/11 14:27
Bug,CAMEL-4776,12535118,Camel Hazelcast Map consumer does not properly create an Exchange.,"The Hazelcast map consumer, when creating an Exchange fills the out message, instead of the in message.",iocanel,iocanel,Major,Resolved,Fixed,14/Dec/11 09:02,14/Dec/11 10:08
Bug,CAMEL-4785,12535394,camel-aws can allow SNAPSHOT transitive dependencies,The camel-aws component depends on aws-java-sdk which has several dependencies defined using version ranges. The problem with version ranges is that they can introduce SNAPSHOT versions if they are available on your system which is not so good... in my case I had this http://download.java.net/maven/2/com/sun/mail/javax.mail/1.4.5-SNAPSHOT/javax.mail-1.4.5-SNAPSHOT.jar jar in my local repo so camel-aws promptly used that SNAPSHOT lib.,janstey,janstey,Major,Resolved,Fixed,16/Dec/11 00:20,17/Dec/11 08:58
Bug,CAMEL-4787,12535463,Wrapped RouteContextProcessor could potential rewrap and cause an ever growing processor path,"There is a problem in the UnitOfWork processor that wraps using a RouteContextProcessor, where the latter is used to track the current route the exchange is being routed. Unfortunately the wrapping may re-wrap itself, causing every growing processor paths.

{code}
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	at org.apache.camel.processor.DelegateAsyncProcessor.process(DelegateAsyncProcessor.java:90)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	at org.apache.camel.processor.RouteContextProcessor.processNext(RouteContextProcessor.java:45)[file:/Users/davsclaus/workspace/camel/camel-core/target/classes/:]
	...
{code}",davsclaus,davsclaus,Blocker,Resolved,Fixed,16/Dec/11 14:33,16/Dec/11 14:35
Bug,CAMEL-4790,12535559,Using the CamelContext.stopRoute(routeId) should not remove the route from context,"Christian reported a camel route stop issue few months ago[1], I found this bug by running the unit test he submit.
{code}
public class RouteRemoveTest extends ContextTestSupport {
 
+    public void testStopRouteOnContext() throws Exception {
+        assertEquals(ServiceStatus.Started, ((DefaultRoute) context.getRoute(""foo"")).getStatus());
+        assertEquals(ServiceStatus.Started, context.getRouteStatus(""foo""));
+        
+        context.stopRoute(""foo"");
+        
+        assertEquals(ServiceStatus.Stopped, ((DefaultRoute) context.getRoute(""foo"")).getStatus());
+        assertEquals(ServiceStatus.Stopped, context.getRouteStatus(""foo""));
+    }
{code}

The route foo should not be removed from the camelContext.
[1] http://camel.465427.n5.nabble.com/Stop-a-route-for-unit-testing-tt4724550.html#a5011082

",njiang,njiang,Major,Resolved,Fixed,17/Dec/11 08:03,19/Dec/11 13:21
Bug,CAMEL-4794,12535649,After restarting route a camel-quartz component fires right away (and it shouldn't),"I am adding a new QuartzRouteRestartTest that demonstrates this issue. Not only a message is fired right after restart, but the scheduled times are messed up as well. I tried to keep the fire interval as low as possible, but it has to be large enough to demonstrate the problem. So I went down to 0/5 (fire every five sec), but it's more obvious with 0/10 or 0/15, the trigger starts firing at the wrong time.",hadrian,hadrian,Major,Resolved,Fixed,19/Dec/11 03:43,20/Dec/11 00:52
Bug,CAMEL-4795,12535669,Throttler and Delay EIP will invoke done callback twice if using async delayed,"If using async delayed on Throttler or Delay EIP, then a 2nd done on the callback is invoked, which should not happen. As the Camel routing engine, is invoking the done as well.

The 2nd done can cause what would appear as a routing error as a NPE occurs in the error handler, as the exchange is already done, and some cleanup work have occurred.
{code}
java.lang.NullPointerException
	at org.apache.camel.processor.RedeliveryErrorHandler.processAsyncErrorHandler(RedeliveryErrorHandler.java:387)
	at org.apache.camel.processor.RedeliveryErrorHandler$1.done(RedeliveryErrorHandler.java:336)
	at org.apache.camel.processor.interceptor.Debug$1$1.done(Debug.java:56)
	at org.apache.camel.processor.interceptor.TraceInterceptor$1.done(TraceInterceptor.java:188)
	at org.apache.camel.impl.ProducerCache$1.done(ProducerCache.java:307)
...
{code}",davsclaus,davsclaus,Major,Resolved,Fixed,19/Dec/11 08:50,19/Dec/11 12:39
Bug,CAMEL-4810,12536071,camel-eventadmin do not work,"Christian-Muellers-MacBook-Pro:bin cmueller$ ./karaf clean

karaf@root> features:addurl mvn:org.apache.camel.karaf/apache-camel/2.9.0/xml/features
karaf@root> features:install camel-eventadmin
Error executing command: Could not start bundle mvn:org.apache.camel/camel-eventadmin/2.9.0 in feature(s) camel-eventadmin-2.9.0: Unresolved constraint in bundle org.apache.camel.camel-eventadmin [59]: Unable to resolve 59.0: missing requirement [59.0] package; (&(package=org.osgi.service.event)(version>=1.2.0)(!(version>=2.0.0)))

We have to install the feature eventadmin (from Karaf) first",muellerc,muellerc,Major,Closed,Fixed,21/Dec/11 19:11,22/Dec/11 18:28
Bug,CAMEL-4811,12536084,camel-ognl featues doesn't work,"Christian-Muellers-MacBook-Pro:bin cmueller$ ./karaf clean

karaf@root> features:addurl mvn:org.apache.camel.karaf/apache-camel/2.9.0/xml/features
karaf@root> features:install camel-ognl 
Error executing command: Could not start bundle mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.ognl/3.0.2_1 in feature(s) camel-ognl-2.9.0: The bundle ""org.apache.servicemix.bundles.ognl_3.0.2.1 [59]"" could not be resolved. Reason: Missing Constraint: Import-Package: javassist; version=""[3.11.0,4.0.0)""

In our parent/pom.xml we define
<javassist-bundle-version>3.9.0.GA_1</javassist-bundle-version>
which doesn't fit the import range",muellerc,muellerc,Major,Closed,Fixed,21/Dec/11 20:49,22/Dec/11 17:51
Bug,CAMEL-4812,12536086,camel-servlet feature doesn't work,"Christian-Muellers-MacBook-Pro:bin cmueller$ ./karaf clean

karaf@root> features:addurl mvn:org.apache.camel.karaf/apache-camel/2.9.0/xml/features
karaf@root> features:install camel-servlet
Error executing command: Could not start bundle mvn:org.apache.camel/camel-servlet/2.9.0 in feature(s) camel-servlet-2.9.0: Unresolved constraint in bundle org.apache.camel.camel-servlet [80]: Unable to resolve 80.0: missing requirement [80.0] package; (&(package=org.osgi.service.http)(version>=1.2.0)(!(version>=2.0.0)))


We have to install the Karaf http feature first.",jbonofre,muellerc,Major,Resolved,Fixed,21/Dec/11 21:02,22/Dec/11 15:53
Bug,CAMEL-4815,12536166,camel-atom/camel-rss feature doesn't work,"camel-atom feature defines a dependency to abdera-core, which require woodstox dependency:

Error executing command: Could not start bundle mvn:org.apache.abdera/abdera-core/1.1.2 in feature(s) camel-atom-2.9.0, cxf-abdera-2.5.1: Unresolved constraint in bundle org.apache.abdera.core [62]: Unable to resolve 62.0: missing requirement [62.0] package; (&(package=javax.xml.stream)(version>=1.0.0)(!(version>=2.0.0)))
",jbonofre,jbonofre,Major,Resolved,Fixed,22/Dec/11 15:15,26/Dec/11 16:50
Bug,CAMEL-4818,12536205,camel-hdfs/camel-soap features require JAXB definition,"The camel-hdfs and camel-soap features don't install out of the box:

Error executing command: Could not start bundle mvn:org.apache.servicemix.specs/org.apache.servicemix.specs.jaxws-api-2.2/1.9.0 in feature(s) camel-soap-2.9-SNAPSHOT, camel-hdfs-2.9-SNAPSHOT, cxf-specs-2.5.1: Unresolved constraint in bundle org.apache.servicemix.specs.jaxws-api-2.2 [88]: Unable to resolve 88.0: missing requirement [88.0] package; (&(package=javax.xml.bind)(version>=2.2.0))

The ServiceMix JAXWS specs require a JAXB 2.2.x implementation available.",jbonofre,jbonofre,Major,Resolved,Fixed,22/Dec/11 20:11,26/Dec/11 16:47
Bug,CAMEL-4819,12536208,camel-jibx/camel-xstream features required woodstox dependency,"The camel-jibx and camel-xstream features don't install out of the box:
Error executing command: Could not start bundle mvn:org.apache.servicemix.bundles/org.apache.servicemix.bundles.dom4j/1.6.1_4 in feature(s) camel-xstream-2.9-SNAPSHOT, camel-jibx-2.9-SNAPSHOT: Unresolved constraint in bundle org.apache.servicemix.bundles.dom4j [65]: Unable to resolve 65.0: missing requirement [65.0] package; (&(package=javax.xml.stream)(version>=1.0.0)(!(version>=2.0.0)))

camel-jibx feature has a dependency to ServiceMix dom4j bundle.

This bundle requires javax.xml.stream (provided by woodstox).",jbonofre,jbonofre,Major,Resolved,Fixed,22/Dec/11 20:36,26/Dec/11 16:55
Bug,CAMEL-4828,12536312,xml-spec-api feature should reference woodstox bundle,"Several Camel features uses javax.xml.stream. If ServiceMix Spec provides the API, it doesn't provide an implementation.

The xml-specs-api feature should ship an implementation, woodstox.",jbonofre,jbonofre,Major,Resolved,Fixed,23/Dec/11 20:25,24/Dec/11 06:56
Bug,CAMEL-4841,12536588,URISupport: URI normalization duplicates query if path is empty,"URI with empty path, but non-empty query part is not properly normalized. The query part is in fact duplicated. E.g, URI ""foobar://?foo=1&bar=2"" is incorrectly normalized to ""foobar://?foo=1&bar=2?bar=2&foo=1"".

This is caused by incorrect identification of the path part in org.apache.camel.util.URISupport.normalizeUri(String):
        int idx = path.indexOf('?');
        if (idx > 0) {
            path = path.substring(0, idx);
        }

The comparison of ""idx > 0"" is incorrect, because this way, if '?' is the first character, the path is not set to empty string. The correct comparison should be ""idx > -1"" or ""idx >= 0"" or even just ""idx != -1"" since that's the only value that String.indexOf returns when the character is not found.",njiang,koscejev,Minor,Resolved,Fixed,29/Dec/11 16:00,30/Dec/11 08:49
Bug,CAMEL-4842,12536659,Remove ProducerCache from CamelContext service list when shutting down a route,"If people add routes dynamic and remove the routes later on, we need to remove services which may have been added to the CamelContext services list. For example the ProducerCache by SendProcessor etc.

This will ensure that the ProducerCache can be GC'ed.
",davsclaus,davsclaus,Major,Resolved,Fixed,30/Dec/11 13:35,05/Jan/12 11:11
